{
  "report_info": {
    "title": "PATENT CLASSIFICATION NLP - CITING PAPERS 论文引用上下文分析报告",
    "query": "Patent classification nlp - Citing Papers",
    "generated_at": "2025-09-11 22:59:58",
    "total_papers_queried": 80,
    "papers_with_contexts": 52,
    "papers_without_contexts": 27
  },
  "query_statistics": {
    "total_ids": 80,
    "found_ids": 52,
    "not_found_ids": [
      199088121,
      248022971,
      218489934,
      280086334,
      278994287,
      64481515,
      221883316,
      259370870,
      258045282,
      263236494,
      254206601,
      253333299,
      250287212,
      278465011,
      280589351,
      195792769,
      219445540,
      21663443,
      132799374,
      64850768,
      243904661,
      133605609,
      43859778,
      8139001,
      153789879,
      24708078,
      14012633
    ],
    "total_results": 577,
    "query_time_seconds": 1.1984353065490723
  },
  "papers_data": {
    "235262768": {
      "citing_paper_info": {
        "title": "PatentSBERTa: A deep NLP based hybrid model for patent distance and classification using augmented SBERT",
        "abstract": "",
        "year": 2021,
        "venue": "Technological forecasting & social change",
        "authors": [
          {
            "authorId": "2056771181",
            "name": "Hamid Bekamiri"
          },
          {
            "authorId": "47109088",
            "name": "D. Hain"
          },
          {
            "authorId": "3168776",
            "name": "Roman Jurowetzki"
          }
        ]
      },
      "citation_summary": {
        "citation_count": 3,
        "unique_cited_count": 3,
        "influential_count": 1,
        "detailed_records_count": 3
      },
      "cited_papers": [
        "16322186",
        "124360271",
        "199472638"
      ],
      "citation_details": [
        {
          "citedcorpusid": 16322186,
          "isinfluential": false,
          "contexts": [
            "This feature can be a significant strength of the model because multi-label classification still is challenging, and using the interpretable model as decision support can help to deploy the model (Zhang & Zhou, 2005)."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "A k-nearest neighbor based algorithm for multi-label classification",
            "abstract": "",
            "year": 2005,
            "venue": "IEEE International Conference on Granular Computing",
            "authors": [
              {
                "authorId": "3039887",
                "name": "Min-Ling Zhang"
              },
              {
                "authorId": "145624000",
                "name": "Zhi-Hua Zhou"
              }
            ]
          }
        },
        {
          "citedcorpusid": 124360271,
          "isinfluential": true,
          "contexts": [
            "Li et al. (2018) proposed DeepPatent as a deep learning algorithm for patent classification based on convolutional neural networks (CNN) and word vector embedding of patent title and abstract.",
            "In this study, PatentBERT and DeepPatent are used as the baseline to benchmark.",
            "This dataset has been used for similar tasks in previous research, such as DeepPatent (Li et al., 2018) and PatentBERT (Lee & Hsiang, 2020), making it a suitable dataset to benchmark our framework against previous work and the current state-of-the-art in deep NLP based patent classification.",
            "In this study, PatentBERT and DeepPatent are used as baselines to benchmark.",
            "Their study showed that DeepPatent achieved 73.88% classification precision by automatic feature extraction, which performed better than all existing algorithms that used the same information for training.",
            "They evaluated their model on USPTO-2M with 2,000,147 records after data cleaning of 2,679,443 USA raw utility patent documents in 637 categories at the IPC subclass level (Li et al., 2018).",
            "In Table 4, results of different approaches including ULMFiT, PatentBERT, and DeepPatent, were shown.",
            "The best result for F1 Top 5 score at subclass level in PatentBERT and DeepPatent was less than 45% and 43% respectively.",
            "Whereas, the existence of different approaches in the classification like IPC and CPC and diversity of information, both structured and unstructured such as Date, Claim, Abstract, and Description have added to this difficulty (Li et al., 2018)."
          ],
          "intents": [
            "['background']",
            "--",
            "['methodology']",
            "--",
            "--",
            "['methodology']",
            "--",
            "--",
            "['background']"
          ],
          "cited_paper_info": {
            "title": "A Hierarchical Feature Extraction Model for Multi-Label Mechanical Patent Classification",
            "abstract": "Various studies have focused on feature extraction methods for automatic patent classification in recent years. However, most of these approaches are based on the knowledge from experts in related domains. Here we propose a hierarchical feature extraction model (HFEM) for multi-label mechanical patent classification, which is able to capture both local features of phrases as well as global and temporal semantics. First, a n-gram feature extractor based on convolutional neural networks (CNNs) is designed to extract salient local lexical-level features. Next, a long dependency feature extraction model based on the bidirectional long–short-term memory (BiLSTM) neural network model is proposed to capture sequential correlations from higher-level sequence representations. Then the HFEM algorithm and its hierarchical feature extraction architecture are detailed. We establish the training, validation and test datasets, containing 72,532, 18,133, and 2679 mechanical patent documents, respectively, and then check the performance of HFEMs. Finally, we compared the results of the proposed HFEM and three other single neural network models, namely CNN, long–short-term memory (LSTM), and BiLSTM. The experimental results indicate that our proposed HFEM outperforms the other compared models in both precision and recall.",
            "year": 2018,
            "venue": "",
            "authors": [
              {
                "authorId": "145815844",
                "name": "Jie Hu"
              },
              {
                "authorId": "2124883266",
                "name": "Shaobo Li"
              },
              {
                "authorId": "50778791",
                "name": "Jianjun Hu"
              },
              {
                "authorId": "2394550",
                "name": "Guanci Yang"
              }
            ]
          }
        },
        {
          "citedcorpusid": 199472638,
          "isinfluential": false,
          "contexts": [
            "Transformers as new semantic-based methods have made the analysis of semantic-level information and retrieval from massive unstructured textual data for practical implementation possible (Sarica et al., 2020)."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Technology Knowledge Graph Based on Patent Data",
            "abstract": "",
            "year": 2019,
            "venue": "Expert systems with applications",
            "authors": [
              {
                "authorId": "144925091",
                "name": "Serhad Sarica"
              },
              {
                "authorId": "145990580",
                "name": "Jianxi Luo"
              },
              {
                "authorId": "143970744",
                "name": "K. Wood"
              }
            ]
          }
        }
      ]
    },
    "269149653": {
      "citing_paper_info": {
        "title": "A Survey on Patent Analysis: From NLP to Multimodal AI",
        "abstract": "Recent advances in Pretrained Language Models (PLMs) and Large Language Models (LLMs) have demonstrated transformative capabilities across diverse domains. The field of patent analysis and innovation is not an exception, where natural language processing (NLP) techniques presents opportunities to streamline and enhance important tasks -- such as patent classification and patent retrieval -- in the patent cycle. This not only accelerates the efficiency of patent researchers and applicants, but also opens new avenues for technological innovation and discovery. Our survey provides a comprehensive summary of recent NLP-based methods -- including multimodal ones -- in patent analysis. We also introduce a novel taxonomy for categorization based on tasks in the patent life cycle, as well as the specifics of the methods. This interdisciplinary survey aims to serve as a comprehensive resource for researchers and practitioners who work at the intersection of NLP, Multimodal AI, and patent analysis, as well as patent offices to build efficient patent systems.",
        "year": 2024,
        "venue": "Annual Meeting of the Association for Computational Linguistics",
        "authors": [
          {
            "authorId": "2147023466",
            "name": "Homaira Huda Shomee"
          },
          {
            "authorId": "2141990332",
            "name": "Zhu Wang"
          },
          {
            "authorId": "2256998683",
            "name": "Sathya Ravi"
          },
          {
            "authorId": "3390598",
            "name": "Sourav Medya"
          }
        ]
      },
      "citation_summary": {
        "citation_count": 67,
        "unique_cited_count": 53,
        "influential_count": 9,
        "detailed_records_count": 67
      },
      "cited_papers": [
        "229678542",
        "253167530",
        "235390652",
        "233856440",
        "46891022",
        "269688950",
        "270845577",
        "252756523",
        "272918346",
        "256411943",
        "269449308",
        "274762766",
        "253265366",
        "252428534",
        "259991612",
        "195791872",
        "220732566",
        "2871882",
        "273901256",
        "75974701",
        "269457191",
        "160025533",
        "263236494",
        "195069387",
        "273901571",
        "12740621",
        "238766042",
        "246917692",
        "221299629",
        "22139415",
        "237654988",
        "21663443",
        "52967399",
        "203566711",
        "132667166",
        "213012437",
        "235262768",
        "53277820",
        "18069615",
        "8923541",
        "233751708",
        "244525926",
        "257295154",
        "229406428",
        "218604766",
        "233583623",
        "247292305",
        "67464958",
        "195792769",
        "245343990",
        "236776781",
        "202765253",
        "225253208"
      ],
      "citation_details": [
        {
          "citedcorpusid": 2871882,
          "isinfluential": false,
          "contexts": [
            "The existing patent surveys in the literature (Gomez and Moens, 2014; Ali et al., 2024; Krestel et al., 2021; Hanbury et al., 2011; Casola and Lavelli, 2022) do not cover the recent studies in this area and fail to show the trends and methods in task specific manner."
          ],
          "intents": [
            "--"
          ],
          "cited_paper_info": {
            "title": "A Survey of Automated Hierarchical Classification of Patents",
            "abstract": "",
            "year": 2014,
            "venue": "Professional Search in the Modern World",
            "authors": [
              {
                "authorId": "144004546",
                "name": "J. Gómez"
              },
              {
                "authorId": "145446752",
                "name": "Marie-Francine Moens"
              }
            ]
          }
        },
        {
          "citedcorpusid": 8923541,
          "isinfluential": false,
          "contexts": [
            "Among other techniques, [20], [19] employ a deep metric learning framework with cross-entropy methods such as InfoNCE [43] and ArcFace [10]."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "ArcFace: Additive Angular Margin Loss for Deep Face Recognition",
            "abstract": "One of the main challenges in feature learning using Deep Convolutional Neural Networks (DCNNs) for large-scale face recognition is the design of appropriate loss functions that can enhance the discriminative power. Centre loss penalises the distance between deep features and their corresponding class centres in the Euclidean space to achieve intra-class compactness. SphereFace assumes that the linear transformation matrix in the last fully connected layer can be used as a representation of the class centres in the angular space and therefore penalises the angles between deep features and their corresponding weights in a multiplicative way. Recently, a popular line of research is to incorporate margins in well-established loss functions in order to maximise face class separability. In this paper, we propose an Additive Angular Margin Loss (ArcFace) to obtain highly discriminative features for face recognition. The proposed ArcFace has a clear geometric interpretation due to its exact correspondence to geodesic distance on a hypersphere. We present arguably the most extensive experimental evaluation against all recent state-of-the-art face recognition methods on ten face recognition benchmarks which includes a new large-scale image database with trillions of pairs and a large-scale video dataset. We show that ArcFace consistently outperforms the state of the art and can be easily implemented with negligible computational overhead. To facilitate future research, the code has been made available.",
            "year": 2018,
            "venue": "Computer Vision and Pattern Recognition",
            "authors": [
              {
                "authorId": "3234063",
                "name": "Jiankang Deng"
              },
              {
                "authorId": "3007274",
                "name": "J. Guo"
              },
              {
                "authorId": "1776444",
                "name": "S. Zafeiriou"
              }
            ]
          }
        },
        {
          "citedcorpusid": 12740621,
          "isinfluential": false,
          "contexts": [
            "(Chen et al., 2020) aim to solve entity identification and semantic relation extraction by BiLSTM-CRF (Huang et al., 2015) and BiGRU-HAN (Han et al., 2019), respectively."
          ],
          "intents": [
            "--"
          ],
          "cited_paper_info": {
            "title": "Bidirectional LSTM-CRF Models for Sequence Tagging",
            "abstract": "In this paper, we propose a variety of Long Short-Term Memory (LSTM) based models for sequence tagging. These models include LSTM networks, bidirectional LSTM (BI-LSTM) networks, LSTM with a Conditional Random Field (CRF) layer (LSTM-CRF) and bidirectional LSTM with a CRF layer (BI-LSTM-CRF). Our work is the first to apply a bidirectional LSTM CRF (denoted as BI-LSTM-CRF) model to NLP benchmark sequence tagging data sets. We show that the BI-LSTM-CRF model can efficiently use both past and future input features thanks to a bidirectional LSTM component. It can also use sentence level tag information thanks to a CRF layer. The BI-LSTM-CRF model can produce state of the art (or close to) accuracy on POS, chunking and NER data sets. In addition, it is robust and has less dependence on word embedding as compared to previous observations.",
            "year": 2015,
            "venue": "arXiv.org",
            "authors": [
              {
                "authorId": "3109481",
                "name": "Zhiheng Huang"
              },
              {
                "authorId": "145738410",
                "name": "W. Xu"
              },
              {
                "authorId": "2150332252",
                "name": "Kai Yu"
              }
            ]
          }
        },
        {
          "citedcorpusid": 18069615,
          "isinfluential": false,
          "contexts": [
            "The existing patent surveys in the literature (Gomez and Moens, 2014; Ali et al., 2024; Krestel et al., 2021; Hanbury et al., 2011; Casola and Lavelli, 2022) do not cover the recent studies in this area and fail to show the trends and methods in task specific manner."
          ],
          "intents": [
            "--"
          ],
          "cited_paper_info": {
            "title": "Patent image retrieval: a survey",
            "abstract": "",
            "year": 2011,
            "venue": "Patent Information Retrieval",
            "authors": [
              {
                "authorId": "1699657",
                "name": "A. Hanbury"
              },
              {
                "authorId": "144046232",
                "name": "Naeem Bhatti"
              },
              {
                "authorId": "144307089",
                "name": "M. Lupu"
              },
              {
                "authorId": "3246596",
                "name": "R. Mörzinger"
              }
            ]
          }
        },
        {
          "citedcorpusid": 21663443,
          "isinfluential": false,
          "contexts": [
            "Similarly, (Shalaby et al., 2018) use LSTM for IPC subclass level classification.",
            "Patent classification is an important but time-intensive task in the patent life cycle (Grawe et al., 2017; Shalaby et al., 2018; Risch and Krestel, 2018)."
          ],
          "intents": [
            "--",
            "--"
          ],
          "cited_paper_info": {
            "title": "An LSTM Approach to Patent Classification based on Fixed Hierarchy Vectors",
            "abstract": "",
            "year": 2018,
            "venue": "SDM",
            "authors": [
              {
                "authorId": "2113210651",
                "name": "M. Shalaby"
              },
              {
                "authorId": "3264870",
                "name": "J. Stutzki"
              },
              {
                "authorId": "39403212",
                "name": "Matthias Schubert"
              },
              {
                "authorId": "3075189",
                "name": "Stephan Günnemann"
              }
            ]
          }
        },
        {
          "citedcorpusid": 21663443,
          "isinfluential": false,
          "contexts": [
            "Similarly, [52] use LSTM for IPC subclass level classification."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "An LSTM Approach to Patent Classification based on Fixed Hierarchy Vectors",
            "abstract": "",
            "year": 2018,
            "venue": "SDM",
            "authors": [
              {
                "authorId": "2113210651",
                "name": "M. Shalaby"
              },
              {
                "authorId": "3264870",
                "name": "J. Stutzki"
              },
              {
                "authorId": "39403212",
                "name": "Matthias Schubert"
              },
              {
                "authorId": "3075189",
                "name": "Stephan Günnemann"
              }
            ]
          }
        },
        {
          "citedcorpusid": 22139415,
          "isinfluential": false,
          "contexts": [
            "Patent classification is an important but time-intensive task in the patent life cycle (Grawe et al., 2017; Shalaby et al., 2018; Risch and Krestel, 2018).",
            "One of the initial studies (Grawe et al., 2017) implements a single-layer LSTM to classify patents at the IPC subgroup level where the initial features are obtained by the Word2Vec method."
          ],
          "intents": [
            "--",
            "--"
          ],
          "cited_paper_info": {
            "title": "Automated Patent Classification Using Word Embedding",
            "abstract": "",
            "year": 2017,
            "venue": "International Conference on Machine Learning and Applications",
            "authors": [
              {
                "authorId": "35272389",
                "name": "Mattyws F. Grawe"
              },
              {
                "authorId": "144097599",
                "name": "C. A. Martins"
              },
              {
                "authorId": "2078352187",
                "name": "Andreia Gentil Bonfante"
              }
            ]
          }
        },
        {
          "citedcorpusid": 46891022,
          "isinfluential": false,
          "contexts": [
            "This complex task demands substantial human effort as well as expertise in various domains (Lin et al., 2018).",
            "(Chung and Sohn, 2020), (Lin et al., 2018) and (Aristodemou, 2021) apply a variety of neural networks such as CNN, Bi-LSTM, Attention-based (Lo et al., 2024) BLIP, GPT-4 Retrieval 2024 (Wang et al., 2024a) GPT-J, T5 Generation 2024 (Lee, 2024) GPT-J Generation 2024 (Jiang et al., 2024) Llama-3,…",
            "NLP-based representation learning methods can be useful in both tasks (Chung and Sohn, 2020; Lin et al., 2018)."
          ],
          "intents": [
            "--",
            "--",
            "--"
          ],
          "cited_paper_info": {
            "title": "Patent Quality Valuation with Deep Learning Models",
            "abstract": "",
            "year": 2018,
            "venue": "International Conference on Database Systems for Advanced Applications",
            "authors": [
              {
                "authorId": "2116456414",
                "name": "Hongjie Lin"
              },
              {
                "authorId": "2144219662",
                "name": "Hao Wang"
              },
              {
                "authorId": "30124005",
                "name": "Dongfang Du"
              },
              {
                "authorId": "2112252643",
                "name": "Han Wu"
              },
              {
                "authorId": "4027546",
                "name": "Biao Chang"
              },
              {
                "authorId": "2227868312",
                "name": "Enhong Chen"
              }
            ]
          }
        },
        {
          "citedcorpusid": 52967399,
          "isinfluential": true,
          "contexts": [
            "3.3.2 Pre-trained Language Models (PLMs) (Krant, 2023) proposes to use MSABERT to assess patent value based entirely on the textual data and use the OECD (Eurostat, O., 2005) quality indicators for evaluation.",
            "The growing complexity and volume of textual data across various domains have driven significant advancements in NLP, particularly through PLMs (Devlin et al., 2019) and LLMs (Radford et al., 2019).",
            "3.4.1 presents the studies with LLMs and PLMs for generating patent texts, and Sec.",
            "The use of PLMs and LLMs for automating patent generation has grown rapidly.",
            "PLMs could be powerful because of their pre-training step on a massive amount of data.",
            "The first study (Lee and Hsiang, 2020b) which involves PLMs, fine-tune the BERT model on the USPTO-2M dataset and introducing a new dataset, USPTO-3M at the subclass level to aid in future research.",
            "“NN”, “MMs”, “PLMs”, and “PatLMs” denote neural networks, multimodal models, pre-trained language models, and patent language models, respectively.",
            "More advanced techniques, including PLMs, have become popular over time.",
            "Table 4 shows the trend of using PLMs and LLMs to solve different patent tasks, and most patent-related tasks are shifting towards leveraging LLMs. Table 5 (see Appendix) shows the summary of patent generation.",
            "PLMs— such as BERT and RoBERTa—have significantly improved the performance to 0.82 (Roudsari et al., 2022).",
            "Thus, fine-tuning PLMs on patent datasets might be able to address some of these concerns by providing context-aware representations for the patent domain.",
            "PLMs are useful in many text-related tasks and patent retrieval is not an exception.",
            "Our survey also focuses on the recent advancements of PLMs and LLMs as well as their usefulness in the patent domain."
          ],
          "intents": [
            "--",
            "--",
            "--",
            "--",
            "--",
            "--",
            "--",
            "--",
            "--",
            "--",
            "--",
            "--",
            "--"
          ],
          "cited_paper_info": {
            "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
            "abstract": "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).",
            "year": 2019,
            "venue": "North American Chapter of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "39172707",
                "name": "Jacob Devlin"
              },
              {
                "authorId": "1744179",
                "name": "Ming-Wei Chang"
              },
              {
                "authorId": "2544107",
                "name": "Kenton Lee"
              },
              {
                "authorId": "3259253",
                "name": "Kristina Toutanova"
              }
            ]
          }
        },
        {
          "citedcorpusid": 53277820,
          "isinfluential": false,
          "contexts": [
            "(Benites et al., 2018) use SVM as a baseline method and experiment with various datasets, the number of features, and semi-supervised learning approaches."
          ],
          "intents": [
            "--"
          ],
          "cited_paper_info": {
            "title": "Classifying Patent Applications with Ensemble Methods",
            "abstract": "We present methods for the automatic classification of patent applications using an annotated dataset provided by the organizers of the ALTA 2018 shared task - Classifying Patent Applications. The goal of the task is to use computational methods to categorize patent applications according to a coarse-grained taxonomy of eight classes based on the International Patent Classification (IPC). We tested a variety of approaches for this task and the best results, 0.778 micro-averaged F1-Score, were achieved by SVM ensembles using a combination of words and characters as features. Our team, BMZ, was ranked first among 14 teams in the competition.",
            "year": 2018,
            "venue": "Australasian Language Technology Association Workshop",
            "authors": [
              {
                "authorId": "2043513",
                "name": "Fernando Benites"
              },
              {
                "authorId": "2854981",
                "name": "S. Malmasi"
              },
              {
                "authorId": "145130358",
                "name": "Marcos Zampieri"
              }
            ]
          }
        },
        {
          "citedcorpusid": 67464958,
          "isinfluential": false,
          "contexts": [
            "Patent Retrieval (PR) (Kravets et al., 2017; Kang et al., 2020; Chen et al., 2020; Setchi et al., 2021) focuses on developing methods to efficiently retrieve relevant patent documents and images based on specific search queries.",
            "(Kravets et al., 2017), (Jiang et al., 2021), and (Kucer et al., 2022) implement CNN, DUAL-VGG, and ResNet, respectively, to retrieve patent images based on a query image."
          ],
          "intents": [
            "--",
            "--"
          ],
          "cited_paper_info": {
            "title": "Patents Images Retrieval and Convolutional Neural Network Training Dataset Quality Improvement",
            "abstract": "The paper considers the problem of the analysis of patents’ figures for formalization of subjective opinions of the patent office experts that reviews applications for inventions. Drawings omission may indicate an incomplete description of the invention and entail the rejection of patent applications and other problems. Since patent images, even if one considers images of the same type, class, etc., are unique, different from each other. Nowadays for image processing are applied neural networks with different architectures. Neural network, Convolutional neural network, Siamese neural network were considered in the research. 4 libraries (Theano, TensorFlow, Caffe, and Keras) were studied. The main contributions of the paper are the new classification of patents’ imaged, training dataset formation and quality improvement approach, and the software implementation for CNN training.",
            "year": 2017,
            "venue": "",
            "authors": [
              {
                "authorId": "144523300",
                "name": "A. Kravets"
              },
              {
                "authorId": "2091911566",
                "name": "Nikita Lebedev"
              },
              {
                "authorId": "73321539",
                "name": "Maxim Legenchenko"
              }
            ]
          }
        },
        {
          "citedcorpusid": 75974701,
          "isinfluential": true,
          "contexts": [
            "The OECD index includes composite indicators and generality with other predominant indices.",
            "3.3.2 Pre-trained Language Models (PLMs) (Krant, 2023) proposes to use MSABERT to assess patent value based entirely on the textual data and use the OECD (Eurostat, O., 2005) quality indicators for evaluation."
          ],
          "intents": [
            "--",
            "--"
          ],
          "cited_paper_info": {
            "title": "Oslo Manual - Guidelines for Collecting and Interpreting Innovation Data, 3rd edition: Proposed Guidelines for Collecting and Interpreting Innovation Data",
            "abstract": "",
            "year": 2005,
            "venue": "",
            "authors": [
              {
                "authorId": "2076628927",
                "name": "P. S. Mortensen"
              },
              {
                "authorId": "26500849",
                "name": "C. Bloch"
              }
            ]
          }
        },
        {
          "citedcorpusid": 132667166,
          "isinfluential": false,
          "contexts": [
            "The earlier works on patent classification are mostly focused on simpler neural networks (Risch and Krestel, 2018, 2019).",
            "(Risch and Krestel, 2018) and (Risch and Krestel, 2019) focus on training fastText word embeddings on a corpus of 5 million patent documents, then use Bi-GRU for classification."
          ],
          "intents": [
            "--",
            "--"
          ],
          "cited_paper_info": {
            "title": "Domain-specific word embeddings for patent classification",
            "abstract": "\nPurpose\nPatent offices and other stakeholders in the patent domain need to classify patent applications according to a standardized classification scheme. The purpose of this paper is to examine the novelty of an application it can then be compared to previously granted patents in the same class. Automatic classification would be highly beneficial, because of the large volume of patents and the domain-specific knowledge needed to accomplish this costly manual task. However, a challenge for the automation is patent-specific language use, such as special vocabulary and phrases.\n\n\nDesign/methodology/approach\nTo account for this language use, the authors present domain-specific pre-trained word embeddings for the patent domain. The authors train the model on a very large data set of more than 5m patents and evaluate it at the task of patent classification. To this end, the authors propose a deep learning approach based on gated recurrent units for automatic patent classification built on the trained word embeddings.\n\n\nFindings\nExperiments on a standardized evaluation data set show that the approach increases average precision for patent classification by 17 percent compared to state-of-the-art approaches. In this paper, the authors further investigate the model’s strengths and weaknesses. An extensive error analysis reveals that the learned embeddings indeed mirror patent-specific language use. The imbalanced training data and underrepresented classes are the most difficult remaining challenge.\n\n\nOriginality/value\nThe proposed approach fulfills the need for domain-specific word embeddings for downstream tasks in the patent domain, such as patent classification or patent analysis.\n",
            "year": 2019,
            "venue": "Data Technologies and Applications",
            "authors": [
              {
                "authorId": "1695993",
                "name": "Julian Risch"
              },
              {
                "authorId": "3264110",
                "name": "Ralf Krestel"
              }
            ]
          }
        },
        {
          "citedcorpusid": 160025533,
          "isinfluential": false,
          "contexts": [
            "The growing complexity and volume of textual data across various domains have driven significant advancements in NLP, particularly through PLMs (Devlin et al., 2019) and LLMs (Radford et al., 2019)."
          ],
          "intents": [
            "--"
          ],
          "cited_paper_info": {
            "title": "Language Models are Unsupervised Multitask Learners",
            "abstract": "Natural language processing tasks, such as question answering, machine translation, reading comprehension, and summarization, are typically approached with supervised learning on taskspecific datasets. We demonstrate that language models begin to learn these tasks without any explicit supervision when trained on a new dataset of millions of webpages called WebText. When conditioned on a document plus questions, the answers generated by the language model reach 55 F1 on the CoQA dataset matching or exceeding the performance of 3 out of 4 baseline systems without using the 127,000+ training examples. The capacity of the language model is essential to the success of zero-shot task transfer and increasing it improves performance in a log-linear fashion across tasks. Our largest model, GPT-2, is a 1.5B parameter Transformer that achieves state of the art results on 7 out of 8 tested language modeling datasets in a zero-shot setting but still underfits WebText. Samples from the model reflect these improvements and contain coherent paragraphs of text. These findings suggest a promising path towards building language processing systems which learn to perform tasks from their naturally occurring demonstrations.",
            "year": 2019,
            "venue": "",
            "authors": [
              {
                "authorId": "38909097",
                "name": "Alec Radford"
              },
              {
                "authorId": "49387725",
                "name": "Jeff Wu"
              },
              {
                "authorId": "48422824",
                "name": "R. Child"
              },
              {
                "authorId": "150970919",
                "name": "D. Luan"
              },
              {
                "authorId": "2698777",
                "name": "Dario Amodei"
              },
              {
                "authorId": "1701686",
                "name": "I. Sutskever"
              }
            ]
          }
        },
        {
          "citedcorpusid": 195069387,
          "isinfluential": false,
          "contexts": [
            "Concurrently, [50] also fine-tune BERT, along with XLNet [58], and RoBERTa on the USPTO-2M dataset."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "XLNet: Generalized Autoregressive Pretraining for Language Understanding",
            "abstract": "With the capability of modeling bidirectional contexts, denoising autoencoding based pretraining like BERT achieves better performance than pretraining approaches based on autoregressive language modeling. However, relying on corrupting the input with masks, BERT neglects dependency between the masked positions and suffers from a pretrain-finetune discrepancy. In light of these pros and cons, we propose XLNet, a generalized autoregressive pretraining method that (1) enables learning bidirectional contexts by maximizing the expected likelihood over all permutations of the factorization order and (2) overcomes the limitations of BERT thanks to its autoregressive formulation. Furthermore, XLNet integrates ideas from Transformer-XL, the state-of-the-art autoregressive model, into pretraining. Empirically, under comparable experiment settings, XLNet outperforms BERT on 20 tasks, often by a large margin, including question answering, natural language inference, sentiment analysis, and document ranking.",
            "year": 2019,
            "venue": "Neural Information Processing Systems",
            "authors": [
              {
                "authorId": "2109512754",
                "name": "Zhilin Yang"
              },
              {
                "authorId": "3422912",
                "name": "Zihang Dai"
              },
              {
                "authorId": "35729970",
                "name": "Yiming Yang"
              },
              {
                "authorId": "143712374",
                "name": "J. Carbonell"
              },
              {
                "authorId": "145124475",
                "name": "R. Salakhutdinov"
              },
              {
                "authorId": "2827616",
                "name": "Quoc V. Le"
              }
            ]
          }
        },
        {
          "citedcorpusid": 195069387,
          "isinfluential": false,
          "contexts": [
            "Concurrently, (Roudsari et al., 2022) also fine-tune BERT, along with XLNet (Yang et al., 2019), and RoBERTa on the USPTO-2M dataset.",
            "They establish XLNet as the new state-of-the-art in classification performance, achieving the highest precision, recall, and f1 measure."
          ],
          "intents": [
            "--",
            "--"
          ],
          "cited_paper_info": {
            "title": "XLNet: Generalized Autoregressive Pretraining for Language Understanding",
            "abstract": "With the capability of modeling bidirectional contexts, denoising autoencoding based pretraining like BERT achieves better performance than pretraining approaches based on autoregressive language modeling. However, relying on corrupting the input with masks, BERT neglects dependency between the masked positions and suffers from a pretrain-finetune discrepancy. In light of these pros and cons, we propose XLNet, a generalized autoregressive pretraining method that (1) enables learning bidirectional contexts by maximizing the expected likelihood over all permutations of the factorization order and (2) overcomes the limitations of BERT thanks to its autoregressive formulation. Furthermore, XLNet integrates ideas from Transformer-XL, the state-of-the-art autoregressive model, into pretraining. Empirically, under comparable experiment settings, XLNet outperforms BERT on 20 tasks, often by a large margin, including question answering, natural language inference, sentiment analysis, and document ranking.",
            "year": 2019,
            "venue": "Neural Information Processing Systems",
            "authors": [
              {
                "authorId": "2109512754",
                "name": "Zhilin Yang"
              },
              {
                "authorId": "3422912",
                "name": "Zihang Dai"
              },
              {
                "authorId": "35729970",
                "name": "Yiming Yang"
              },
              {
                "authorId": "143712374",
                "name": "J. Carbonell"
              },
              {
                "authorId": "145124475",
                "name": "R. Salakhutdinov"
              },
              {
                "authorId": "2827616",
                "name": "Quoc V. Le"
              }
            ]
          }
        },
        {
          "citedcorpusid": 195791872,
          "isinfluential": false,
          "contexts": [
            "Lastly, recent advanced LLMs can generate accurate and technical language descriptions for patents and, thus, are useful to optimize human resources and precision in patent writing (Lee and Hsiang, 2020a).",
            "(Lee and Hsiang, 2020a) implement GPT-2 (Rad-ford et al., 2019) models to generate the independent claims in patents.",
            "The first study (Lee and Hsiang, 2020b) which involves PLMs, fine-tune the BERT model on the USPTO-2M dataset and introducing a new dataset, USPTO-3M at the subclass level to aid in future research."
          ],
          "intents": [
            "--",
            "--",
            "--"
          ],
          "cited_paper_info": {
            "title": "Patent Claim Generation by Fine-Tuning OpenAI GPT-2",
            "abstract": "",
            "year": 2019,
            "venue": "World Patent Information",
            "authors": [
              {
                "authorId": "2387987",
                "name": "Jieh-Sheng Lee"
              },
              {
                "authorId": "1798127",
                "name": "J. Hsiang"
              }
            ]
          }
        },
        {
          "citedcorpusid": 195792769,
          "isinfluential": false,
          "contexts": [
            "Lastly, recent advanced LLMs can generate accurate and technical language descriptions for patents and, thus, are useful to optimize human resources and precision in patent writing (Lee and Hsiang, 2020a).",
            "(Lee and Hsiang, 2020a) implement GPT-2 (Rad-ford et al., 2019) models to generate the independent claims in patents.",
            "The first study (Lee and Hsiang, 2020b) which involves PLMs, fine-tune the BERT model on the USPTO-2M dataset and introducing a new dataset, USPTO-3M at the subclass level to aid in future research."
          ],
          "intents": [
            "--",
            "--",
            "--"
          ],
          "cited_paper_info": {
            "title": "Patent classification by fine-tuning BERT language model",
            "abstract": "",
            "year": 2020,
            "venue": "World Patent Information",
            "authors": [
              {
                "authorId": "2108319166",
                "name": "Jieh-Sheng Lee"
              },
              {
                "authorId": "1798127",
                "name": "J. Hsiang"
              }
            ]
          }
        },
        {
          "citedcorpusid": 202765253,
          "isinfluential": false,
          "contexts": [
            "(Chen et al., 2020) aim to solve entity identification and semantic relation extraction by BiLSTM-CRF (Huang et al., 2015) and BiGRU-HAN (Han et al., 2019), respectively."
          ],
          "intents": [
            "--"
          ],
          "cited_paper_info": {
            "title": "OpenNRE: An Open and Extensible Toolkit for Neural Relation Extraction",
            "abstract": "OpenNRE is an open-source and extensible toolkit that provides a unified framework to implement neural models for relation extraction (RE). Specifically, by implementing typical RE methods, OpenNRE not only allows developers to train custom models to extract structured relational facts from the plain text but also supports quick model validation for researchers. Besides, OpenNRE provides various functional RE modules based on both TensorFlow and PyTorch to maintain sufficient modularity and extensibility, making it becomes easy to incorporate new models into the framework. Besides the toolkit, we also release an online system to meet real-time extraction without any training and deploying. Meanwhile, the online system can extract facts in various scenarios as well as aligning the extracted facts to Wikidata, which may benefit various downstream knowledge-driven applications (e.g., information retrieval and question answering). More details of the toolkit and online system can be obtained from http://github.com/thunlp/OpenNRE.",
            "year": 2019,
            "venue": "Conference on Empirical Methods in Natural Language Processing",
            "authors": [
              {
                "authorId": "48506411",
                "name": "Xu Han"
              },
              {
                "authorId": "4800645",
                "name": "Tianyu Gao"
              },
              {
                "authorId": "1390925224",
                "name": "Yuan Yao"
              },
              {
                "authorId": "50816334",
                "name": "Deming Ye"
              },
              {
                "authorId": "49293587",
                "name": "Zhiyuan Liu"
              },
              {
                "authorId": "1753344",
                "name": "Maosong Sun"
              }
            ]
          }
        },
        {
          "citedcorpusid": 203566711,
          "isinfluential": true,
          "contexts": [
            "The earlier works on patent classification are mostly focused on simpler neural networks (Risch and Krestel, 2018, 2019).",
            "From Table 8, the early works have a low precision of 0.53 on USPTO data (Risch and Krestel, 2018).",
            "Patent classification is an important but time-intensive task in the patent life cycle (Grawe et al., 2017; Shalaby et al., 2018; Risch and Krestel, 2018).",
            "(Risch and Krestel, 2018) and (Risch and Krestel, 2019) focus on training fastText word embeddings on a corpus of 5 million patent documents, then use Bi-GRU for classification."
          ],
          "intents": [
            "--",
            "--",
            "--",
            "--"
          ],
          "cited_paper_info": {
            "title": "Learning Patent Speak: Investigating Domain-Specific Word Embeddings",
            "abstract": "A patent examiner needs domain-specific knowledge to classify a patent application according to its field of invention. Standardized classification schemes help to compare a patent application to previously granted patents and thereby check its novelty. Due to the large volume of patents, automatic patent classification would be highly beneficial to patent offices and other stakeholders in the patent domain. However, a challenge for the automation of this costly manual task is the patent-specific language use. To facilitate this task, we present domain-specific pre-trained word embeddings for the patent domain. We trained our model on a very large dataset of more than 5 million patents to learn the language use in this domain. We evaluated the quality of the resulting embeddings in the context of patent classification. To this end, we propose a deep learning approach based on gated recurrent units for automatic patent classification built on the trained word embeddings. Experiments on a standardized evaluation dataset show that our approach increases average precision for patent classification by 17 percent compared to state-of-the-art approaches.",
            "year": 2018,
            "venue": "International Conference on Digital Information Management",
            "authors": [
              {
                "authorId": "1695993",
                "name": "Julian Risch"
              },
              {
                "authorId": "3264110",
                "name": "Ralf Krestel"
              }
            ]
          }
        },
        {
          "citedcorpusid": 213012437,
          "isinfluential": false,
          "contexts": [
            "(Trappey et al., 2019) use Deep Neural Networks with 11 quality indicators."
          ],
          "intents": [
            "--"
          ],
          "cited_paper_info": {
            "title": "Patent Value Analysis Using Deep Learning Models—The Case of IoT Technology Mining for the Manufacturing Industry",
            "abstract": "",
            "year": 2021,
            "venue": "IEEE transactions on engineering management",
            "authors": [
              {
                "authorId": "1761458",
                "name": "A. Trappey"
              },
              {
                "authorId": "1766308",
                "name": "C. Trappey"
              },
              {
                "authorId": "2645509",
                "name": "Usharani Hareesh Govindarajan"
              },
              {
                "authorId": "32675137",
                "name": "John J. H. Sun"
              }
            ]
          }
        },
        {
          "citedcorpusid": 218604766,
          "isinfluential": false,
          "contexts": [
            "In a separate study, (Lee, 2020) focuses on personalized claim generation by fine-tuning a pre-trained GPT-2 model with inventor-centric data to demonstrate greater relevance."
          ],
          "intents": [
            "--"
          ],
          "cited_paper_info": {
            "title": "PatentTransformer: A Framework for Personalized Patent Claim Generation",
            "abstract": ". This paper proposes the PatentTransformer framework to generate and measure personalized patent claims. The objective is to help inventors conceive better inventions by learning from relevant inventors. Patent claim generation is a way of “augmented inventing.” for inventors. Such patent claim generation leverages the recent transfer learning in the Deep Learning field, particularly the state-of-the-art Transformer-based models. In terms of system implementation, it is planned to build an \"auto-complete\" function for patent claim drafting. The auto-complete function is analyzed from four different perspectives: extent of generation, generative direction, proximity of generation, and constraint in generation. Technically, the PatentTransformer framework is composed of two Transformer models. One is for text generation and the other is for quality measurement. Specifically, the patent claim generation is based on GPT-2 model and the measurement of personalization is based on BERT model. The training data is inventor-centric and comes from the Inventors Endpoint API provided by the USPTO.",
            "year": 2019,
            "venue": "International Conference on Legal Knowledge and Information Systems",
            "authors": [
              {
                "authorId": "2387987",
                "name": "Jieh-Sheng Lee"
              }
            ]
          }
        },
        {
          "citedcorpusid": 220732566,
          "isinfluential": false,
          "contexts": [
            "Patent Retrieval (PR) (Kravets et al., 2017; Kang et al., 2020; Chen et al., 2020; Setchi et al., 2021) focuses on developing methods to efficiently retrieve relevant patent documents and images based on specific search queries.",
            "(Chen et al., 2020) aim to solve entity identification and semantic relation extraction by BiLSTM-CRF (Huang et al., 2015) and BiGRU-HAN (Han et al., 2019), respectively."
          ],
          "intents": [
            "--",
            "--"
          ],
          "cited_paper_info": {
            "title": "A deep learning based method for extracting semantic information from patent documents",
            "abstract": "",
            "year": 2020,
            "venue": "Scientometrics",
            "authors": [
              {
                "authorId": "2146033169",
                "name": "Liang Chen"
              },
              {
                "authorId": "50433328",
                "name": "Shuo Xu"
              },
              {
                "authorId": "48324516",
                "name": "Lijun Zhu"
              },
              {
                "authorId": "2155701047",
                "name": "Jing Zhang"
              },
              {
                "authorId": "2072811437",
                "name": "Xiao-ping Lei"
              },
              {
                "authorId": "31279648",
                "name": "Guancan Yang"
              }
            ]
          }
        },
        {
          "citedcorpusid": 221299629,
          "isinfluential": false,
          "contexts": [
            "Patent Retrieval (PR) (Kravets et al., 2017; Kang et al., 2020; Chen et al., 2020; Setchi et al., 2021) focuses on developing methods to efficiently retrieve relevant patent documents and images based on specific search queries.",
            "(Kang et al., 2020) use the BERT language model which includes the combinations of title, abstract, and claim."
          ],
          "intents": [
            "--",
            "--"
          ],
          "cited_paper_info": {
            "title": "Patent prior art search using deep learning language model",
            "abstract": "A patent is one of the essential indicators of new technologies and business processes, which becomes the main driving force of the companies and even the national competitiveness as well, that has recently been submitted and exploited in a large scale of quantities of information sources. Since the number of patent processing personnel, however, can hardly keep up with the increasing number of patents, and thus may have been worried about from deteriorating the quality of examinations. In this regard, the advancement of deep learning for the language processing capabilities has been developed significantly so that the prior art search by the deep learning models also can be accomplished for the labor-intensive and expensive patent document search tasks. The prior art search requires differentiation tasks, usually with the sheer volume of relevant documents; thus, the recall is much more important than the precision, which is the primary difference from the conventional search engines. This paper addressed a method to effectively handle the patent documents using BERT, one of the major deep learning-based language models. We proved through experiments that our model had outperformed the conventional approaches and the combinations of the key components with the recall value of up to '94.29%' from the real patent dataset.",
            "year": 2020,
            "venue": "International Database Engineering and Applications Symposium",
            "authors": [
              {
                "authorId": "1905879088",
                "name": "Dylan Myungchul Kang"
              },
              {
                "authorId": "1646750026",
                "name": "Charles Cheolgi Lee"
              },
              {
                "authorId": "2112280",
                "name": "Suan Lee"
              },
              {
                "authorId": "1728685",
                "name": "Wookey Lee"
              }
            ]
          }
        },
        {
          "citedcorpusid": 225253208,
          "isinfluential": false,
          "contexts": [
            "PCA DNN Accuracy ETSI and DI [22] Investor reaction, citations Many MAE Patentsview [9] Abstract, claims, predefined CNN, Bi-LSTM Precision, recall USPTO [2] 12 patent indices ANN APR, F1, FNR, MAE USPTO, OECD [12] 9 patent indices MLP Accuracy, Kappa, MAE USPTO [39] Maintenance period…",
            "[9], [40] and [2] apply a variety of neural networks such as CNN, Bi-LSTM, Attention-based CNN (ACNN), deep and wide Artificial Neural Networks (ANN), respectively.",
            "AI-based representation learning methods can be useful in both tasks [9, 40]."
          ],
          "intents": [
            "--",
            "['background']",
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Early detection of valuable patents using a deep learning model: Case of semiconductor industry",
            "abstract": "",
            "year": 2020,
            "venue": "",
            "authors": [
              {
                "authorId": "144921413",
                "name": "Park Chung"
              },
              {
                "authorId": "2210599",
                "name": "S. Sohn"
              }
            ]
          }
        },
        {
          "citedcorpusid": 225253208,
          "isinfluential": false,
          "contexts": [
            "(Chung and Sohn, 2020), (Lin et al., 2018) and (Aristodemou, 2021) apply a variety of neural networks such as CNN, Bi-LSTM, Attention-based (Lo et al., 2024) BLIP, GPT-4 Retrieval 2024 (Wang et al., 2024a) GPT-J, T5 Generation 2024 (Lee, 2024) GPT-J Generation 2024 (Jiang et al., 2024) Llama-3,…",
            "NLP-based representation learning methods can be useful in both tasks (Chung and Sohn, 2020; Lin et al., 2018)."
          ],
          "intents": [
            "--",
            "--"
          ],
          "cited_paper_info": {
            "title": "Early detection of valuable patents using a deep learning model: Case of semiconductor industry",
            "abstract": "",
            "year": 2020,
            "venue": "",
            "authors": [
              {
                "authorId": "144921413",
                "name": "Park Chung"
              },
              {
                "authorId": "2210599",
                "name": "S. Sohn"
              }
            ]
          }
        },
        {
          "citedcorpusid": 229406428,
          "isinfluential": false,
          "contexts": [
            "(Kravets et al., 2017), (Jiang et al., 2021), and (Kucer et al., 2022) implement CNN, DUAL-VGG, and ResNet, respectively, to retrieve patent images based on a query image."
          ],
          "intents": [
            "--"
          ],
          "cited_paper_info": {
            "title": "Deriving Design Feature Vectors for Patent Images Using Convolutional Neural Networks",
            "abstract": "\n The patent database is often used by designers to search for inspirational stimuli for innovative design opportunities because of the large size, extensive variety, and the massive quantity of design information contained in patent documents. Growing work on design-by-analogy has adopted various vectorization approaches for associating design documents. However, they only focused on text analysis and ignored visual information. Research in engineering design and cognitive psychology has shown that visual stimuli may benefit design ideation. In this study, we focus on visual design stimuli and automatically derive the vector space and the design feature vectors representing design images. The automatic vectorization approach uses a novel convolutional neural network architecture named Dual-Visual Geometry Group (VGG) aiming to accomplish two tasks: visual material-type prediction and international patent classification (IPC) section-label predictions. The derived feature vectors that embed both visual characteristics and technology-related knowledge can be potentially utilized to guide the retrieval and use of near-field and far-field design stimuli according to their vector distances. We report the accuracy of the training tasks and also use a case study to demonstrate the advantages of design image retrievals based on our model.",
            "year": 2020,
            "venue": "",
            "authors": [
              {
                "authorId": "2142580538",
                "name": "Shuo Jiang"
              },
              {
                "authorId": "145990580",
                "name": "Jianxi Luo"
              },
              {
                "authorId": "1414783177",
                "name": "Guillermo A. Ruiz-Pava"
              },
              {
                "authorId": "2143910587",
                "name": "Jie Hu"
              },
              {
                "authorId": "3357163",
                "name": "C. Magee"
              }
            ]
          }
        },
        {
          "citedcorpusid": 229678542,
          "isinfluential": false,
          "contexts": [
            "Patent documents require precise and technical language to accurately describe the invention and its claims (Risch et al., 2021)."
          ],
          "intents": [
            "--"
          ],
          "cited_paper_info": {
            "title": "PatentMatch: A Dataset for Matching Patent Claims & Prior Art",
            "abstract": "Patent examiners need to solve a complex information retrieval task when they assess the novelty and inventive step of claims made in a patent application. Given a claim, they search for prior art, which comprises all relevant publicly available information. This time-consuming task requires a deep understanding of the respective technical domain and the patent-domain-specific language. For these reasons, we address the computer-assisted search for prior art by creating a training dataset for supervised machine learning called PatentMatch. It contains pairs of claims from patent applications and semantically corresponding text passages of different degrees from cited patent documents. Each pair has been labeled by technically-skilled patent examiners from the European Patent Office. Accordingly, the label indicates the degree of semantic correspondence (matching), i.e., whether the text passage is prejudicial to the novelty of the claimed invention or not. Preliminary experiments using a baseline system show that PatentMatch can indeed be used for training a binary text pair classifier on this challenging information retrieval task. The dataset is available online: https://hpi.de/naumann/s/patentmatch.",
            "year": 2020,
            "venue": "arXiv.org",
            "authors": [
              {
                "authorId": "1695993",
                "name": "Julian Risch"
              },
              {
                "authorId": "2090084616",
                "name": "Nicolas Alder"
              },
              {
                "authorId": "2042640458",
                "name": "Christoph Hewel"
              },
              {
                "authorId": "3264110",
                "name": "Ralf Krestel"
              }
            ]
          }
        },
        {
          "citedcorpusid": 229678542,
          "isinfluential": false,
          "contexts": [
            "Patent documents require precise and technical language to accurately describe the invention and its claims [47]."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "PatentMatch: A Dataset for Matching Patent Claims & Prior Art",
            "abstract": "Patent examiners need to solve a complex information retrieval task when they assess the novelty and inventive step of claims made in a patent application. Given a claim, they search for prior art, which comprises all relevant publicly available information. This time-consuming task requires a deep understanding of the respective technical domain and the patent-domain-specific language. For these reasons, we address the computer-assisted search for prior art by creating a training dataset for supervised machine learning called PatentMatch. It contains pairs of claims from patent applications and semantically corresponding text passages of different degrees from cited patent documents. Each pair has been labeled by technically-skilled patent examiners from the European Patent Office. Accordingly, the label indicates the degree of semantic correspondence (matching), i.e., whether the text passage is prejudicial to the novelty of the claimed invention or not. Preliminary experiments using a baseline system show that PatentMatch can indeed be used for training a binary text pair classifier on this challenging information retrieval task. The dataset is available online: https://hpi.de/naumann/s/patentmatch.",
            "year": 2020,
            "venue": "arXiv.org",
            "authors": [
              {
                "authorId": "1695993",
                "name": "Julian Risch"
              },
              {
                "authorId": "2090084616",
                "name": "Nicolas Alder"
              },
              {
                "authorId": "2042640458",
                "name": "Christoph Hewel"
              },
              {
                "authorId": "3264110",
                "name": "Ralf Krestel"
              }
            ]
          }
        },
        {
          "citedcorpusid": 233583623,
          "isinfluential": false,
          "contexts": [
            "To streamline this, NLP techniques can be helpful, particularly in patent classification, retrieval, and quality analysis (Krestel et al., 2021).",
            "The existing patent surveys in the literature (Gomez and Moens, 2014; Ali et al., 2024; Krestel et al., 2021; Hanbury et al., 2011; Casola and Lavelli, 2022) do not cover the recent studies in this area and fail to show the trends and methods in task specific manner."
          ],
          "intents": [
            "--",
            "--"
          ],
          "cited_paper_info": {
            "title": "A survey on deep learning for patent analysis",
            "abstract": "",
            "year": 2021,
            "venue": "World Patent Information",
            "authors": [
              {
                "authorId": "3264110",
                "name": "Ralf Krestel"
              },
              {
                "authorId": "2047942384",
                "name": "Renukswamy Chikkamath"
              },
              {
                "authorId": "2042640458",
                "name": "Christoph Hewel"
              },
              {
                "authorId": "1695993",
                "name": "Julian Risch"
              }
            ]
          }
        },
        {
          "citedcorpusid": 233751708,
          "isinfluential": false,
          "contexts": [
            "As an example, patent citation has been considered as a proxy for patent valuation (Nandi et al., 2024; Hsu et al., 2020).",
            "(Hsu et al., 2020) predict forward citation and investor reaction to patent announcements implementing CNN-LSTM neural networks and various ML models."
          ],
          "intents": [
            "--",
            "--"
          ],
          "cited_paper_info": {
            "title": "Deep Learning, Text, and Patent Valuation",
            "abstract": "This paper uses deep learning and natural language processing (NLP) methods on the US patent corpus to evaluate their predictive power in estimating two measures of patent value: (i) investor reaction to patent announcements as measured in Kogan et al., 2017 and (ii) forward citations. While forward citations have traditionally been used as measures of economic value in the literature, their utility is mainly retrospective. Contemporaneous predictions of patent value, as embodied in investor reactions to patent grants, can be important for managers and policy-makers for prospective decision making. We compare the prediction performance of models using the structured features of the patent (number of citations, technology class, etc.) to deep learning and NLP methods. Relative to linear regression models using the same features, deep learning models reduce mean absolute error (MAE) by approximately 32%. Incorporating patent text further lowers the MAE by 13%.",
            "year": 2020,
            "venue": "Social Science Research Network",
            "authors": [
              {
                "authorId": "1777653",
                "name": "Po-Hsuan Hsu"
              },
              {
                "authorId": "2675022",
                "name": "Dokyun Lee"
              },
              {
                "authorId": "144249785",
                "name": "Prasanna Tambe"
              },
              {
                "authorId": "1696566",
                "name": "David H. Hsu"
              }
            ]
          }
        },
        {
          "citedcorpusid": 233751708,
          "isinfluential": false,
          "contexts": [
            "PCA DNN Accuracy ETSI and DI [22] Investor reaction, citations Many MAE Patentsview [9] Abstract, claims, predefined CNN, Bi-LSTM Precision, recall USPTO [2] 12 patent indices ANN APR, F1, FNR, MAE USPTO, OECD [12] 9 patent indices MLP Accuracy, Kappa, MAE USPTO [39] Maintenance period…",
            "[22] predict forward citation and investor reaction to patent announcements implementing CNN-LSTM neural networks and various ML models."
          ],
          "intents": [
            "--",
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Deep Learning, Text, and Patent Valuation",
            "abstract": "This paper uses deep learning and natural language processing (NLP) methods on the US patent corpus to evaluate their predictive power in estimating two measures of patent value: (i) investor reaction to patent announcements as measured in Kogan et al., 2017 and (ii) forward citations. While forward citations have traditionally been used as measures of economic value in the literature, their utility is mainly retrospective. Contemporaneous predictions of patent value, as embodied in investor reactions to patent grants, can be important for managers and policy-makers for prospective decision making. We compare the prediction performance of models using the structured features of the patent (number of citations, technology class, etc.) to deep learning and NLP methods. Relative to linear regression models using the same features, deep learning models reduce mean absolute error (MAE) by approximately 32%. Incorporating patent text further lowers the MAE by 13%.",
            "year": 2020,
            "venue": "Social Science Research Network",
            "authors": [
              {
                "authorId": "1777653",
                "name": "Po-Hsuan Hsu"
              },
              {
                "authorId": "2675022",
                "name": "Dokyun Lee"
              },
              {
                "authorId": "144249785",
                "name": "Prasanna Tambe"
              },
              {
                "authorId": "1696566",
                "name": "David H. Hsu"
              }
            ]
          }
        },
        {
          "citedcorpusid": 233856440,
          "isinfluential": false,
          "contexts": [
            "Patent Retrieval (PR) (Kravets et al., 2017; Kang et al., 2020; Chen et al., 2020; Setchi et al., 2021) focuses on developing methods to efficiently retrieve relevant patent documents and images based on specific search queries.",
            "(Setchi et al., 2021) describe five technical requirements to investigate the feasibility of AI for the task."
          ],
          "intents": [
            "--",
            "--"
          ],
          "cited_paper_info": {
            "title": "Artificial intelligence for patent prior art searching",
            "abstract": "",
            "year": 2021,
            "venue": "World Patent Information",
            "authors": [
              {
                "authorId": "2343276",
                "name": "R. Setchi"
              },
              {
                "authorId": "1750238",
                "name": "Irena Spasic"
              },
              {
                "authorId": "144223687",
                "name": "J. Morgan"
              },
              {
                "authorId": "2088582068",
                "name": "Christopher Harrison"
              },
              {
                "authorId": "98203083",
                "name": "Richard Corken"
              }
            ]
          }
        },
        {
          "citedcorpusid": 235262768,
          "isinfluential": false,
          "contexts": [
            "(Bekamiri et al., 2024) use Sentence BERT that takes into account entire sentences instead of word by word."
          ],
          "intents": [
            "--"
          ],
          "cited_paper_info": {
            "title": "PatentSBERTa: A deep NLP based hybrid model for patent distance and classification using augmented SBERT",
            "abstract": "",
            "year": 2021,
            "venue": "Technological forecasting & social change",
            "authors": [
              {
                "authorId": "2056771181",
                "name": "Hamid Bekamiri"
              },
              {
                "authorId": "47109088",
                "name": "D. Hain"
              },
              {
                "authorId": "3168776",
                "name": "Roman Jurowetzki"
              }
            ]
          }
        },
        {
          "citedcorpusid": 235390652,
          "isinfluential": false,
          "contexts": [
            "Patent classification can benefit from the AI-based multi-label classification tools for the hierarchical schemes: International Patent Classification (IPC) and the Cooperative Patent Classification [50, 1].",
            "[1] implement domain adaptive pre-trained Linguistically Informed Masking and show that SciBERT based representations perform better than BERT-based representations in patent classification."
          ],
          "intents": [
            "['methodology']",
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Linguistically Informed Masking for Representation Learning in the Patent Domain",
            "abstract": "Domain-specific contextualized language models have demonstrated substantial effectiveness gains for domain-specific downstream tasks, like similarity matching, entity recognition or information retrieval. However successfully applying such models in highly specific language domains requires domain adaptation of the pre-trained models. In this paper we propose the empirically motivated Linguistically Informed Masking (LIM) method to focus domain-adaptative pre-training on the linguistic patterns of patents, which use a highly technical sublanguage. We quantify the relevant differences between patent, scientific and general-purpose language and demonstrate for two different language models (BERT and SciBERT) that domain adaptation with LIM leads to systematically improved representations by evaluating the performance of the domain-adapted representations of patent language on two independent downstream tasks, the IPC classification and similarity matching. We demonstrate the impact of balancing the learning from different information sources during domain adaptation for the patent domain. We make the source code as well as the domain-adaptive pre-trained patent language models publicly available at https://github.com/sophiaalthammer/patent-lim.",
            "year": 2021,
            "venue": "arXiv.org",
            "authors": [
              {
                "authorId": "1620824465",
                "name": "Sophia Althammer"
              },
              {
                "authorId": "143682826",
                "name": "Mark Buckley"
              },
              {
                "authorId": "97393346",
                "name": "Sebastian Hofstätter"
              },
              {
                "authorId": "1699657",
                "name": "A. Hanbury"
              }
            ]
          }
        },
        {
          "citedcorpusid": 235390652,
          "isinfluential": false,
          "contexts": [
            "Patent classification can benefit from multi-label classification tools for the hierarchical schemes: International Patent Classification (IPC) and the Cooperative Patent Classification (Roudsari et al., 2022; Althammer et al., 2021)."
          ],
          "intents": [
            "--"
          ],
          "cited_paper_info": {
            "title": "Linguistically Informed Masking for Representation Learning in the Patent Domain",
            "abstract": "Domain-specific contextualized language models have demonstrated substantial effectiveness gains for domain-specific downstream tasks, like similarity matching, entity recognition or information retrieval. However successfully applying such models in highly specific language domains requires domain adaptation of the pre-trained models. In this paper we propose the empirically motivated Linguistically Informed Masking (LIM) method to focus domain-adaptative pre-training on the linguistic patterns of patents, which use a highly technical sublanguage. We quantify the relevant differences between patent, scientific and general-purpose language and demonstrate for two different language models (BERT and SciBERT) that domain adaptation with LIM leads to systematically improved representations by evaluating the performance of the domain-adapted representations of patent language on two independent downstream tasks, the IPC classification and similarity matching. We demonstrate the impact of balancing the learning from different information sources during domain adaptation for the patent domain. We make the source code as well as the domain-adaptive pre-trained patent language models publicly available at https://github.com/sophiaalthammer/patent-lim.",
            "year": 2021,
            "venue": "arXiv.org",
            "authors": [
              {
                "authorId": "1620824465",
                "name": "Sophia Althammer"
              },
              {
                "authorId": "143682826",
                "name": "Mark Buckley"
              },
              {
                "authorId": "97393346",
                "name": "Sebastian Hofstätter"
              },
              {
                "authorId": "1699657",
                "name": "A. Hanbury"
              }
            ]
          }
        },
        {
          "citedcorpusid": 236776781,
          "isinfluential": true,
          "contexts": [
            "Multimodal techniques have also been used in the information retrieval [44].",
            "…[5] BiLSTM-CRF, BiGRU-HAN USPTO supervised [25] DUAL-VGG CLEF-IP, USPTO supervised [51] SVM, Naive Bayes, Random Forest, MLP IPO supervised [44] RoBERTa, CLIP EPO pre-trained [53] Sentence-BERT, TransE USPTO pre-trained, unsupervised [33] (ImageNet, Sketchy) ResNet50 DeepPatent supervised,…",
            "[44] utilize CLIP for image embedding alongside RoBERTa for capturing textual features, and thus, enhances the search process by incorporating both visual and textual data."
          ],
          "intents": [
            "['methodology']",
            "['methodology']",
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "A Multimodal Approach for Semantic Patent Image Retrieval",
            "abstract": "Patent images such as technical drawings contain valuable information and are frequently used by experts to compare patents. However, current approaches to patent information retrieval are largely focused on textual information. Consequently, we review previous work on patent retrieval with a focus on illustrations in figures. In this paper, we report on work in progress for a novel approach for patent image retrieval that uses deep multimodal features. Scene text spotting and optical character recognition are employed to extract numerals from an image to subsequently identify references to corresponding sentences in the patent document. Furthermore, we use a neural state-of-the-art CLIP model to extract structural features from illustrations and additionally derive textual features from the related patent text using a sentence transformer model. To fuse our multimodal features for similarity search we apply re-ranking according to averaged or maximum scores. In our experiments, we compare the impact of different modalities on the task of similarity search for patent images. The experimental results suggest that patent image retrieval can be successfully performed using the proposed feature sets, while the best results are achieved when combining the features of both modalities.",
            "year": 2021,
            "venue": "",
            "authors": [
              {
                "authorId": "1404095227",
                "name": "Kader Pustu-Iren"
              },
              {
                "authorId": "2122351107",
                "name": "Gerrit Bruns"
              },
              {
                "authorId": "1738703",
                "name": "R. Ewerth"
              }
            ]
          }
        },
        {
          "citedcorpusid": 236776781,
          "isinfluential": false,
          "contexts": [
            "( Pustu-Iren et al., 2021) demonstrate that the image and text-based transformer models achieve the highest mean average precision in patent retrieval tasks."
          ],
          "intents": [
            "--"
          ],
          "cited_paper_info": {
            "title": "A Multimodal Approach for Semantic Patent Image Retrieval",
            "abstract": "Patent images such as technical drawings contain valuable information and are frequently used by experts to compare patents. However, current approaches to patent information retrieval are largely focused on textual information. Consequently, we review previous work on patent retrieval with a focus on illustrations in figures. In this paper, we report on work in progress for a novel approach for patent image retrieval that uses deep multimodal features. Scene text spotting and optical character recognition are employed to extract numerals from an image to subsequently identify references to corresponding sentences in the patent document. Furthermore, we use a neural state-of-the-art CLIP model to extract structural features from illustrations and additionally derive textual features from the related patent text using a sentence transformer model. To fuse our multimodal features for similarity search we apply re-ranking according to averaged or maximum scores. In our experiments, we compare the impact of different modalities on the task of similarity search for patent images. The experimental results suggest that patent image retrieval can be successfully performed using the proposed feature sets, while the best results are achieved when combining the features of both modalities.",
            "year": 2021,
            "venue": "",
            "authors": [
              {
                "authorId": "1404095227",
                "name": "Kader Pustu-Iren"
              },
              {
                "authorId": "2122351107",
                "name": "Gerrit Bruns"
              },
              {
                "authorId": "1738703",
                "name": "R. Ewerth"
              }
            ]
          }
        },
        {
          "citedcorpusid": 237654988,
          "isinfluential": false,
          "contexts": [
            "Similarly, [55] apply text mining techniques to extract key sections from patents, train Word2Vec, and then use multiple parallel LSTMs for the classification task."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Deep Learning based Pipeline with Multichannel Inputs for Patent Classication",
            "abstract": "",
            "year": 2021,
            "venue": "PatentSemTech@SEMANTiCS",
            "authors": [
              {
                "authorId": "2286136",
                "name": "Mustafa Sofean"
              }
            ]
          }
        },
        {
          "citedcorpusid": 238766042,
          "isinfluential": true,
          "contexts": [
            "(Chung and Sohn, 2020), (Lin et al., 2018) and (Aristodemou, 2021) apply a variety of neural networks such as CNN, Bi-LSTM, Attention-based (Lo et al., 2024) BLIP, GPT-4 Retrieval 2024 (Wang et al., 2024a) GPT-J, T5 Generation 2024 (Lee, 2024) GPT-J Generation 2024 (Jiang et al., 2024) Llama-3,…",
            "On the other hand, the generic quality analysis are based on well-known measures (Aristodemou, 2021; Erdogan et al., 2022).",
            "Businesses have shown great interest in evaluating patent value due to its significant impact in generating revenue and investment (Aristodemou, 2021).",
            "The works that use patent images are written in blue. a patent can be assessed using various measures, including the number of forward or backward citations, the number of claims, the grant lag, patent family size, the remaining lifetime of the patent (Aristodemou, 2021; Erdogan et al., 2022)."
          ],
          "intents": [
            "--",
            "--",
            "--",
            "--"
          ],
          "cited_paper_info": {
            "title": "Identifying Valuable Patents: A Deep Learning Approach",
            "abstract": "",
            "year": 2021,
            "venue": "",
            "authors": [
              {
                "authorId": "50859988",
                "name": "L. Aristodemou"
              }
            ]
          }
        },
        {
          "citedcorpusid": 238766042,
          "isinfluential": true,
          "contexts": [
            "On the other hand, the generic quality analysis are based on some well-known measures [2, 12].",
            "Quality analysis : Businesses have shown great interest in evaluating patent value due to its significant impact in generating revenue and investment [2].",
            "The quality of a patent can be assessed using various measures, including the number of forward or backward citations, the number of claims, the grant lag, patent family size, the remaining lifetime of the patent [2, 12].",
            "…Accuracy ETSI and DI [22] Investor reaction, citations Many MAE Patentsview [9] Abstract, claims, predefined CNN, Bi-LSTM Precision, recall USPTO [2] 12 patent indices ANN APR, F1, FNR, MAE USPTO, OECD [12] 9 patent indices MLP Accuracy, Kappa, MAE USPTO [39] Maintenance period BiLSTM-ATT-CRF…",
            "[9], [40] and [2] apply a variety of neural networks such as CNN, Bi-LSTM, Attention-based CNN (ACNN), deep and wide Artificial Neural Networks (ANN), respectively."
          ],
          "intents": [
            "['background']",
            "['background']",
            "['background']",
            "['background']",
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Identifying Valuable Patents: A Deep Learning Approach",
            "abstract": "",
            "year": 2021,
            "venue": "",
            "authors": [
              {
                "authorId": "50859988",
                "name": "L. Aristodemou"
              }
            ]
          }
        },
        {
          "citedcorpusid": 244525926,
          "isinfluential": false,
          "contexts": [
            "(Kravets et al., 2017), (Jiang et al., 2021), and (Kucer et al., 2022) implement CNN, DUAL-VGG, and ResNet, respectively, to retrieve patent images based on a query image."
          ],
          "intents": [
            "--"
          ],
          "cited_paper_info": {
            "title": "DeepPatent: Large scale patent drawing recognition and retrieval",
            "abstract": "We tackle the problem of analyzing and retrieving technical drawings. First, we introduce DeepPatent, a new large-scale dataset for recognition and retrieval of design patent drawings. The dataset provides more than 350,000 design patent drawings for the purpose of image retrieval. Unlike existing datasets, DeepPatent provides fine-grained image retrieval associations within the collection of drawings and does not rely on cross-domain associations for supervision. We develop a baseline deep learning model, named Patent-Net, based on best practices for training retrieval models for static images. We demonstrate the superior performance of PatentNet when trained on our fine-grained associations of DeepPatent against other deep learning approaches and classic computer vision descriptors. With the introduction of this new dataset, and benchmark algorithms, we demonstrate that the analysis and retrieval of technical drawings remains an open challenge in computer vision; and that patent drawing retrieval provides a real-world testbench to spur research.",
            "year": 2022,
            "venue": "IEEE Workshop/Winter Conference on Applications of Computer Vision",
            "authors": [
              {
                "authorId": "41052688",
                "name": "Michal Kucer"
              },
              {
                "authorId": "2864621",
                "name": "D. Oyen"
              },
              {
                "authorId": "1780793",
                "name": "J. Castorena"
              },
              {
                "authorId": "97569165",
                "name": "Jian Wu"
              }
            ]
          }
        },
        {
          "citedcorpusid": 245343990,
          "isinfluential": false,
          "contexts": [
            "Patent classification can benefit from the AI-based multi-label classification tools for the hierarchical schemes: International Patent Classification (IPC) and the Cooperative Patent Classification [50, 1].",
            "Concurrently, [50] also fine-tune BERT, along with XLNet [58], and RoBERTa on the USPTO-2M dataset."
          ],
          "intents": [
            "['methodology']",
            "['background']"
          ],
          "cited_paper_info": {
            "title": "PatentNet: multi-label classification of patent documents using deep learning based language understanding",
            "abstract": "Patent classification is an expensive and time-consuming task that has conventionally been performed by domain experts. However, the increase in the number of filed patents and the complexity of the documents make the classification task challenging. The text used in patent documents is not always written in a way to efficiently convey knowledge. Moreover, patent classification is a multi-label classification task with a large number of labels, which makes the problem even more complicated. Hence, automating this expensive and laborious task is essential for assisting domain experts in managing patent documents, facilitating reliable search, retrieval, and further patent analysis tasks. Transfer learning and pre-trained language models have recently achieved state-of-the-art results in many Natural Language Processing tasks. In this work, we focus on investigating the effect of fine-tuning the pre-trained language models, namely, BERT, XLNet, RoBERTa, and ELECTRA, for the essential task of multi-label patent classification. We compare these models with the baseline deep-learning approaches used for patent classification. We use various word embeddings to enhance the performance of the baseline models. The publicly available USPTO-2M patent classification benchmark and M-patent datasets are used for conducting experiments. We conclude that fine-tuning the pre-trained language models on the patent text improves the multi-label patent classification performance. Our findings indicate that XLNet performs the best and achieves a new state-of-the-art classification performance with respect to precision, recall, F1 measure, as well as coverage error, and LRAP.",
            "year": 2021,
            "venue": "Scientometrics",
            "authors": [
              {
                "authorId": "2197861164",
                "name": "Arousha Haghighian Roudsari"
              },
              {
                "authorId": "145653991",
                "name": "Jafar Afshar"
              },
              {
                "authorId": "1728685",
                "name": "Wookey Lee"
              },
              {
                "authorId": "2112280",
                "name": "Suan Lee"
              }
            ]
          }
        },
        {
          "citedcorpusid": 245343990,
          "isinfluential": true,
          "contexts": [
            "Concurrently, (Roudsari et al., 2022) also fine-tune BERT, along with XLNet (Yang et al., 2019), and RoBERTa on the USPTO-2M dataset.",
            "PLMs— such as BERT and RoBERTa—have significantly improved the performance to 0.82 (Roudsari et al., 2022).",
            "Patent classification can benefit from multi-label classification tools for the hierarchical schemes: International Patent Classification (IPC) and the Cooperative Patent Classification (Roudsari et al., 2022; Althammer et al., 2021)."
          ],
          "intents": [
            "--",
            "--",
            "--"
          ],
          "cited_paper_info": {
            "title": "PatentNet: multi-label classification of patent documents using deep learning based language understanding",
            "abstract": "Patent classification is an expensive and time-consuming task that has conventionally been performed by domain experts. However, the increase in the number of filed patents and the complexity of the documents make the classification task challenging. The text used in patent documents is not always written in a way to efficiently convey knowledge. Moreover, patent classification is a multi-label classification task with a large number of labels, which makes the problem even more complicated. Hence, automating this expensive and laborious task is essential for assisting domain experts in managing patent documents, facilitating reliable search, retrieval, and further patent analysis tasks. Transfer learning and pre-trained language models have recently achieved state-of-the-art results in many Natural Language Processing tasks. In this work, we focus on investigating the effect of fine-tuning the pre-trained language models, namely, BERT, XLNet, RoBERTa, and ELECTRA, for the essential task of multi-label patent classification. We compare these models with the baseline deep-learning approaches used for patent classification. We use various word embeddings to enhance the performance of the baseline models. The publicly available USPTO-2M patent classification benchmark and M-patent datasets are used for conducting experiments. We conclude that fine-tuning the pre-trained language models on the patent text improves the multi-label patent classification performance. Our findings indicate that XLNet performs the best and achieves a new state-of-the-art classification performance with respect to precision, recall, F1 measure, as well as coverage error, and LRAP.",
            "year": 2021,
            "venue": "Scientometrics",
            "authors": [
              {
                "authorId": "2197861164",
                "name": "Arousha Haghighian Roudsari"
              },
              {
                "authorId": "145653991",
                "name": "Jafar Afshar"
              },
              {
                "authorId": "1728685",
                "name": "Wookey Lee"
              },
              {
                "authorId": "2112280",
                "name": "Suan Lee"
              }
            ]
          }
        },
        {
          "citedcorpusid": 246917692,
          "isinfluential": false,
          "contexts": [
            "Recent work focuses on patent infringement, such as [6] develop a model with different deep learning methods, such as CNN and LSTM, to predict the possibility of a patent application being granted and classify the reason for a failed application."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Establish a patent risk prediction model for emerging technologies using deep learning and data augmentation",
            "abstract": "",
            "year": 2022,
            "venue": "Advanced Engineering Informatics",
            "authors": [
              {
                "authorId": "122359333",
                "name": "Y. Chi"
              },
              {
                "authorId": "1754004",
                "name": "Hei-Chia Wang"
              }
            ]
          }
        },
        {
          "citedcorpusid": 247292305,
          "isinfluential": false,
          "contexts": [
            "More specifically, [28] experiment with different word embedding techniques, whereas [27] focus on applying various partitioning techniques to enhance the performance of the proposed framework.",
            "Meanwhile, [27] and [28] both investigate ensemble models incorporating Bi-LSTM, Bi-GRU, LSTM, and GRU."
          ],
          "intents": [
            "['background']",
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Automated Single-Label Patent Classification using Ensemble Classifiers",
            "abstract": "Many thousands of patent applications arrive at patent offices around the world every day. One important task when a patent application is submitted is to assign one or more classification codes from the complex and hierarchical patent classification schemes that will enable routing of the patent application to a patent examiner who is knowledgeable about the specific technical field. This task is typically undertaken by patent professionals, however due to the large number of applications and the potential complexity of an invention, they are usually overwhelmed. Therefore, there is a need for this code assignment manual task to be supported or even fully automated by classification systems that will classify patent applications, hopefully with an accuracy close to patent professionals. Like in many other text analysis problems, in the last years, this intellectually demanding task has been studied using word embeddings and deep learning techniques. In this paper these research efforts are shortly reviewed and re-produced with similar deep learning techniques using different feature representations on automatic patent classification in the level of sub-classes. On top of that, an innovative method of ensemble classifiers trained with different parts of the patent document is proposed. To the best of our knowledge, this is the first time that an ensemble method was proposed for the patent classification problem. Our first results are quite promising showing that an ensemble architecture of classifiers significantly outperforms current state-of-the-art techniques using the same classifiers as standalone solutions.",
            "year": 2022,
            "venue": "International Conference on Machine Learning and Computing",
            "authors": [
              {
                "authorId": "2852041",
                "name": "Eleni Kamateri"
              },
              {
                "authorId": "119177158",
                "name": "Vasileios Stamatis"
              },
              {
                "authorId": "1705272",
                "name": "K. Diamantaras"
              },
              {
                "authorId": "1786622",
                "name": "M. Salampasis"
              }
            ]
          }
        },
        {
          "citedcorpusid": 252428534,
          "isinfluential": false,
          "contexts": [
            "Moreover, there are various studies on generating new ideas and evaluating novelty, such as identifying the inventive process of novel patent using BERT [15], and an explainable AI (XAI) model for novelty analysis via [24]."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Unveiling the inventive process from patents by extracting problems, solutions and advantages with natural language processing",
            "abstract": "",
            "year": 2023,
            "venue": "Expert systems with applications",
            "authors": [
              {
                "authorId": "2074687747",
                "name": "Vito Giordano"
              },
              {
                "authorId": "2787918",
                "name": "Giovanni Puccetti"
              },
              {
                "authorId": "3412161",
                "name": "F. Chiarello"
              },
              {
                "authorId": "1395617336",
                "name": "Tommaso Pavanello"
              },
              {
                "authorId": "2142606580",
                "name": "Fantoni Gualtiero"
              }
            ]
          }
        },
        {
          "citedcorpusid": 252756523,
          "isinfluential": false,
          "contexts": [
            "(Christofidellis et al., 2022) introduce the Patent Generative Transformer (PGT) that supports three tasks: part-of-patent generation, text infilling, and coherence evaluation."
          ],
          "intents": [
            "--"
          ],
          "cited_paper_info": {
            "title": "PGT: a prompt based generative transformer for the patent domain",
            "abstract": "Patents are a valuable source of knowledge, but drafting them is a time-consuming and expensive task. Methods that assist patent generation can provide a two-fold improvement as they can speed up the generation process and suggest to the inventor ideas and claims. Herein, influenced by recent advances in language modeling via multitask learning and prompt engineering, we present Patent Generative Transformer (PGT), a transformer-based language model trained to facilitate patent drafting. Specifically, the model supports three tasks: part-of-patent generation, text infilling, and patent coherence evaluation. PGT complements inventors and assures the fast and successful transition from their input to a coherent patent disclosure taking advantage of its multitasking nature. We show how the model out-performs a collection of task-specific baselines on relevant metrics. We further test the quality of the generated text via blind testing by subject matter experts. Finally, we explore a zero-shot extension of the model showing how to use PGT for generating domain-specific abstracts.",
            "year": 2022,
            "venue": "",
            "authors": [
              {
                "authorId": "2039675061",
                "name": "Dimitrios Christofidellis"
              },
              {
                "authorId": "2187177717",
                "name": "Antonio Berrios Torres"
              },
              {
                "authorId": "113880332",
                "name": "A. Dave"
              },
              {
                "authorId": "1744102",
                "name": "M. Roveri"
              },
              {
                "authorId": "2187167593",
                "name": "Kristin Schmidt"
              },
              {
                "authorId": "51149179",
                "name": "Sarath Swaminathan"
              },
              {
                "authorId": "144114375",
                "name": "Hans Vandierendonck"
              },
              {
                "authorId": "35312152",
                "name": "D. Zubarev"
              },
              {
                "authorId": "35904689",
                "name": "Matteo Manica"
              }
            ]
          }
        },
        {
          "citedcorpusid": 253167530,
          "isinfluential": false,
          "contexts": [
            "Patents are represented as nodes connected by edges such as citations in a citation network (Liu and Li, 2022)."
          ],
          "intents": [
            "--"
          ],
          "cited_paper_info": {
            "title": "Early Identification of Significant Patents Using Heterogeneous Applicant-Citation Networks Based on the Chinese Green Patent Data",
            "abstract": "With the deterioration of the environment and the acceleration of resource consumption, green patent innovation focusing on environmental protection fields has become a research hot-spot around the world. Previous researchers constructed homogeneous information networks to analyze the influence of patents based on citation ranking algorithms. However, a patent information network is a complex network containing multiple pieces of information (e.g., citation, applicant, inventor), and the use of a single information network will result in incomplete information or information loss, and the obtained results are biased. In addition, scholars constructed centrality indicators to assess the importance of patents with less consideration of the age bias problem of algorithms and models, and the results obtained are inaccurate. In this paper, based on the Chinese green patent (CNGP) dataset from 1985 to 2020, a CNGP heterogeneous applicant-citation network is constructed, and the rescaling method and normalization procedure are used to solve the age bias. The results illustrate that the method proposed in this paper is able to identify significant patents earlier, and the performance of the rescaled indegree (R_ID) works best such as the IR score is 17.32% in the top 5% of the rankings, and it is the best in the constructed dynamic heterogeneous networks as well. In addition, the constructed heterogeneous information network has better results compared with the traditional homogeneous information network, such as the NIR score of R_ID metrics can be improved by 2% under the same condition. Therefore, the analysis method proposed in this paper can reasonably evaluate the quality of patents and identify significant patents earlier, thus providing a new method for scientists to measure the quality of patents.",
            "year": 2022,
            "venue": "Sustainability",
            "authors": [
              {
                "authorId": "2154486518",
                "name": "Xipeng Liu"
              },
              {
                "authorId": "2108184350",
                "name": "Xinmiao Li"
              }
            ]
          }
        },
        {
          "citedcorpusid": 253265366,
          "isinfluential": false,
          "contexts": [
            "This knowledge graph can help with prior art searches, the identification of related patents, and identify valuable patents (e.g., patents with high citations) (Siddharth et al., 2022)."
          ],
          "intents": [
            "--"
          ],
          "cited_paper_info": {
            "title": "Enhancing patent retrieval using text and knowledge graph embeddings: a technical note",
            "abstract": "Patent retrieval influences several applications within engineering design research, education, and practice as well as applications that concern innovation, intellectual property, and knowledge management etc. In this article, we propose a method to retrieve patents relevant to an initial set of patents, by synthesising state-of-the-art techniques among natural language processing and knowledge graph embedding. Our method involves a patent embedding approach that captures text, citation, and inventor information, which individually represent different facets of knowledge communicated through a patent document. We obtain text embeddings through Sentence-BERT applied to titles and abstracts. We obtain citation and inventor embeddings through TransE that is trained using the corresponding knowledge graphs. We identify using a classification task that the concatenation of text, citation, and inventor embeddings offers a plausible representation of a patent. While the proposed patent embedding could be used to associate a pair of patents, we observe using a recall task that multiple initial patents could be associated with a target patent using mean cosine similarity, which could then be utilised to rank all target patents and retrieve the most relevant ones. We apply the proposed patent retrieval method to a set of patents corresponding to a product family and an inventor's portfolio.",
            "year": 2022,
            "venue": "Journal of engineering design",
            "authors": [
              {
                "authorId": "51471694",
                "name": "L. Siddharth"
              },
              {
                "authorId": "2151302471",
                "name": "Guangtong Li"
              },
              {
                "authorId": "145990580",
                "name": "Jianxi Luo"
              }
            ]
          }
        },
        {
          "citedcorpusid": 256411943,
          "isinfluential": false,
          "contexts": [
            "Moreover, there are various studies on generating new ideas and evaluating novelty, such as identifying the inventive process of novel patent using BERT [15], and an explainable AI (XAI) model for novelty analysis via [24]."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "An eXplainable AI (XAI) model for text-based patent novelty analysis",
            "abstract": "",
            "year": 2023,
            "venue": "Expert systems with applications",
            "authors": [
              {
                "authorId": "2143039747",
                "name": "Hyejin Jang"
              },
              {
                "authorId": "2109591986",
                "name": "Sunhye Kim"
              },
              {
                "authorId": "38717655",
                "name": "B. Yoon"
              }
            ]
          }
        },
        {
          "citedcorpusid": 257295154,
          "isinfluential": false,
          "contexts": [
            "[56] employ techniques such as random forest, Support Vector Regression, and Decision Trees to effectively merge the search findings."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Machine learning methods for results merging in patent retrieval",
            "abstract": "PurposeIn federated search, a query is sent simultaneously to multiple resources and each one of them returns a list of results. These lists are merged into a single list using the results merging process. In this work, the authors apply machine learning methods for results merging in federated patent search. Even though several methods for results merging have been developed, none of them were tested on patent data nor considered several machine learning models. Thus, the authors experiment with state-of-the-art methods using patent data and they propose two new methods for results merging that use machine learning models.Design/methodology/approachThe methods are based on a centralized index containing samples of documents from all the remote resources, and they implement machine learning models to estimate comparable scores for the documents retrieved by different resources. The authors examine the new methods in cooperative and uncooperative settings where document scores from the remote search engines are available and not, respectively. In uncooperative environments, they propose two methods for assigning document scores.FindingsThe effectiveness of the new results merging methods was measured against state-of-the-art models and found to be superior to them in many cases with significant improvements. The random forest model achieves the best results in comparison to all other models and presents new insights for the results merging problem.Originality/valueIn this article the authors prove that machine learning models can substitute other standard methods and models that used for results merging for many years. Our methods outperformed state-of-the-art estimation methods for results merging, and they proved that they are more effective for federated patent search.",
            "year": 2023,
            "venue": "Data Technologies and Applications",
            "authors": [
              {
                "authorId": "119177158",
                "name": "Vasileios Stamatis"
              },
              {
                "authorId": "1786622",
                "name": "M. Salampasis"
              },
              {
                "authorId": "1705272",
                "name": "K. Diamantaras"
              }
            ]
          }
        },
        {
          "citedcorpusid": 257295154,
          "isinfluential": false,
          "contexts": [
            "(Stamatis et al., 2023) employ techniques such as random forest, Support Vector Regression, and Decision Trees to merge the search findings effectively."
          ],
          "intents": [
            "--"
          ],
          "cited_paper_info": {
            "title": "Machine learning methods for results merging in patent retrieval",
            "abstract": "PurposeIn federated search, a query is sent simultaneously to multiple resources and each one of them returns a list of results. These lists are merged into a single list using the results merging process. In this work, the authors apply machine learning methods for results merging in federated patent search. Even though several methods for results merging have been developed, none of them were tested on patent data nor considered several machine learning models. Thus, the authors experiment with state-of-the-art methods using patent data and they propose two new methods for results merging that use machine learning models.Design/methodology/approachThe methods are based on a centralized index containing samples of documents from all the remote resources, and they implement machine learning models to estimate comparable scores for the documents retrieved by different resources. The authors examine the new methods in cooperative and uncooperative settings where document scores from the remote search engines are available and not, respectively. In uncooperative environments, they propose two methods for assigning document scores.FindingsThe effectiveness of the new results merging methods was measured against state-of-the-art models and found to be superior to them in many cases with significant improvements. The random forest model achieves the best results in comparison to all other models and presents new insights for the results merging problem.Originality/valueIn this article the authors prove that machine learning models can substitute other standard methods and models that used for results merging for many years. Our methods outperformed state-of-the-art estimation methods for results merging, and they proved that they are more effective for federated patent search.",
            "year": 2023,
            "venue": "Data Technologies and Applications",
            "authors": [
              {
                "authorId": "119177158",
                "name": "Vasileios Stamatis"
              },
              {
                "authorId": "1786622",
                "name": "M. Salampasis"
              },
              {
                "authorId": "1705272",
                "name": "K. Diamantaras"
              }
            ]
          }
        },
        {
          "citedcorpusid": 259991612,
          "isinfluential": false,
          "contexts": [
            "While the above methods heavily focus on texts, (Ghauri et al., 2023) classify patent images into distinct types of visualizations, such as graphs, block circuits, flowcharts, and technical drawings, along with various perspectives, including side, top, left, and perspective views."
          ],
          "intents": [
            "--"
          ],
          "cited_paper_info": {
            "title": "Classification of Visualization Types and Perspectives in Patents",
            "abstract": "Due to the swift growth of patent applications each year, information and multimedia retrieval approaches that facilitate patent exploration and retrieval are of utmost importance. Different types of visualizations (e.g., graphs, technical drawings) and perspectives (e.g., side view, perspective) are used to visualize details of innovations in patents. The classification of these images enables a more efficient search and allows for further analysis. So far, datasets for image type classification miss some important visualization types for patents. Furthermore, related work does not make use of recent deep learning approaches including transformers. In this paper, we adopt state-of-the-art deep learning methods for the classification of visualization types and perspectives in patent images. We extend the CLEF-IP dataset for image type classification in patents to ten classes and provide manual ground truth annotations. In addition, we derive a set of hierarchical classes from a dataset that provides weakly-labeled data for image perspectives. Experimental results have demonstrated the feasibility of the proposed approaches. Source code, models, and dataset will be made publicly available.",
            "year": 2023,
            "venue": "International Conference on Theory and Practice of Digital Libraries",
            "authors": [
              {
                "authorId": "2000247523",
                "name": "J. Ghauri"
              },
              {
                "authorId": "1404095219",
                "name": "Eric Müller-Budack"
              },
              {
                "authorId": "1738703",
                "name": "R. Ewerth"
              }
            ]
          }
        },
        {
          "citedcorpusid": 263236494,
          "isinfluential": false,
          "contexts": [
            "Meanwhile, (Kamateri et al., 2023) and (Kamateri et al., 2022) both investigate ensemble models incorporating Bi-LSTM, Bi-GRU, LSTM, and GRU.",
            "More specifically, (Kamateri et al., 2022) conduct experiments with different word embedding techniques, whereas (Kamateri et al., 2023) focus on applying various partitioning techniques to enhance the performance of the proposed framework."
          ],
          "intents": [
            "--",
            "--"
          ],
          "cited_paper_info": {
            "title": "An ensemble framework for patent classification",
            "abstract": "",
            "year": 2023,
            "venue": "World Patent Information",
            "authors": [
              {
                "authorId": "2248859386",
                "name": "Eleni Kamateri"
              },
              {
                "authorId": "2248869057",
                "name": "Michail Salampasis"
              },
              {
                "authorId": "2265503477",
                "name": "Konstantinos Diamantaras"
              }
            ]
          }
        },
        {
          "citedcorpusid": 269449308,
          "isinfluential": false,
          "contexts": [
            "…et al., 2024a) GPT-J, T5 Generation 2024 (Lee, 2024) GPT-J Generation 2024 (Jiang et al., 2024) Llama-3, Mistral, and PatentGPT-J Generation 2024 (Bai et al., 2024) Llama-2 and Mixtral Generation 2024 (Ren and Ma, 2024) Qwen2 Generation 2024 (Wang et al., 2024b) Qwen2, LLAMA3, GPT-4o, Mistral…",
            "(Bai et al., 2024) build a cost-effective LLMs for the intellectual property (IP) domain to handle domain-specific expertise and long-text processing."
          ],
          "intents": [
            "--",
            "--"
          ],
          "cited_paper_info": {
            "title": "PatentGPT: A Large Language Model for Intellectual Property",
            "abstract": "In recent years, large language models(LLMs) have attracted significant attention due to their exceptional performance across a multitude of natural language process tasks, and have been widely applied in various fields. However, the application of large language models in the Intellectual Property (IP) domain is challenging due to the strong need for specialized knowledge, privacy protection, processing of extremely long text in this field. In this technical report, we present for the first time a low-cost, standardized procedure for training IP-oriented LLMs, meeting the unique requirements of the IP domain. Using this standard process, we have trained the PatentGPT series models based on open-source pretrained models. By evaluating them on the open-source IP-oriented benchmark MOZIP, our domain-specific LLMs outperforms GPT-4, indicating the effectiveness of the proposed training procedure and the expertise of the PatentGPT models in the IP domain. Remarkably, our model surpassed GPT-4 on the 2019 China Patent Agent Qualification Examination, scoring 65 and matching human expert levels. Additionally, the PatentGPT model, which utilizes the SMoE architecture, achieves performance comparable to that of GPT-4 in the IP domain and demonstrates a better cost-performance ratio on long-text tasks, potentially serving as an alternative to GPT-4 within the IP domain.",
            "year": 2024,
            "venue": "arXiv.org",
            "authors": [
              {
                "authorId": "2298902592",
                "name": "Zilong Bai"
              },
              {
                "authorId": "2299340325",
                "name": "Ruiji Zhang"
              },
              {
                "authorId": "2108397315",
                "name": "Linqing Chen"
              },
              {
                "authorId": "2298899264",
                "name": "Qijun Cai"
              },
              {
                "authorId": "2299112746",
                "name": "Yuan Zhong"
              },
              {
                "authorId": "2299107366",
                "name": "Cong Wang"
              },
              {
                "authorId": "2298949551",
                "name": "Yan Fang"
              },
              {
                "authorId": "2299298800",
                "name": "Jie Fang"
              },
              {
                "authorId": "2298972263",
                "name": "Jing Sun"
              },
              {
                "authorId": "2299009862",
                "name": "Weikuan Wang"
              },
              {
                "authorId": "2298951217",
                "name": "Lizhi Zhou"
              },
              {
                "authorId": "2298953166",
                "name": "Haoran Hua"
              },
              {
                "authorId": "2298953107",
                "name": "Tian Qiu"
              },
              {
                "authorId": "2298975347",
                "name": "Chaochao Wang"
              },
              {
                "authorId": "2298937369",
                "name": "Cheng Sun"
              },
              {
                "authorId": "2298981604",
                "name": "Jianping Lu"
              },
              {
                "authorId": "2298937601",
                "name": "Yixin Wang"
              },
              {
                "authorId": "2284894360",
                "name": "Yu-Nong Xia"
              },
              {
                "authorId": "2298952229",
                "name": "Meng Hu"
              },
              {
                "authorId": "2298928031",
                "name": "Haowen Liu"
              },
              {
                "authorId": "2276416080",
                "name": "Peng Xu"
              },
              {
                "authorId": "2298944526",
                "name": "Licong Xu"
              },
              {
                "authorId": "2298902447",
                "name": "Fu Bian"
              },
              {
                "authorId": "2299108434",
                "name": "Xiaolong Gu"
              },
              {
                "authorId": "2298963920",
                "name": "Lisha Zhang"
              },
              {
                "authorId": "2249021711",
                "name": "Weilei Wang"
              },
              {
                "authorId": "2298904107",
                "name": "Changyang Tu"
              }
            ]
          }
        },
        {
          "citedcorpusid": 269457191,
          "isinfluential": false,
          "contexts": [
            "(Chung and Sohn, 2020), (Lin et al., 2018) and (Aristodemou, 2021) apply a variety of neural networks such as CNN, Bi-LSTM, Attention-based (Lo et al., 2024) BLIP, GPT-4 Retrieval 2024 (Wang et al., 2024a) GPT-J, T5 Generation 2024 (Lee, 2024) GPT-J Generation 2024 (Jiang et al., 2024) Llama-3,…",
            "(Chung and Sohn, 2020), (Lin et al., 2018) and (Aristodemou, 2021) apply a variety of neural networks such as CNN, Bi-LSTM, Attention-based (Lo et al., 2024) BLIP, GPT-4 Retrieval 2024 (Wang et al., 2024a) GPT-J, T5 Generation 2024 (Lee, 2024) GPT-J Generation 2024 (Jiang et al., 2024) Llama-3, Mistral, and PatentGPT-J Generation 2024 (Bai et al., 2024) Llama-2 and Mixtral Generation 2024 (Ren and Ma, 2024) Qwen2 Generation 2024 (Wang et al., 2024b) Qwen2, LLAMA3, GPT-4o, Mistral Generation 2024 CNN (ACNN), deep and wide Artificial Neural Networks (ANN), respectively."
          ],
          "intents": [
            "--",
            "--"
          ],
          "cited_paper_info": {
            "title": "Large Language Model Informed Patent Image Retrieval",
            "abstract": "In patent prosecution, image-based retrieval systems for identifying similarities between current patent images and prior art are pivotal to ensure the novelty and non-obviousness of patent applications. Despite their growing popularity in recent years, existing attempts, while effective at recognizing images within the same patent, fail to deliver practical value due to their limited generalizability in retrieving relevant prior art. Moreover, this task inherently involves the challenges posed by the abstract visual features of patent images, the skewed distribution of image classifications, and the semantic information of image descriptions. Therefore, we propose a language-informed, distribution-aware multimodal approach to patent image feature learning, which enriches the semantic understanding of patent image by integrating Large Language Models and improves the performance of underrepresented classes with our proposed distribution-aware contrastive losses. Extensive experiments on DeepPatent2 dataset show that our proposed method achieves state-of-the-art or comparable performance in image-based patent retrieval with mAP +53.3%, Recall@10 +41.8%, and MRR@10 +51.9%. Furthermore, through an in-depth user analysis, we explore our model in aiding patent professionals in their image retrieval efforts, highlighting the model's real-world applicability and effectiveness.",
            "year": 2024,
            "venue": "PatentSemTech@SIGIR",
            "authors": [
              {
                "authorId": "2282137855",
                "name": "Hao-Cheng Lo"
              },
              {
                "authorId": "2282226423",
                "name": "Jung-Mei Chu"
              },
              {
                "authorId": "2282137432",
                "name": "Jieh Hsiang"
              },
              {
                "authorId": "2282138644",
                "name": "Chun-Chieh Cho"
              }
            ]
          }
        },
        {
          "citedcorpusid": 269688950,
          "isinfluential": false,
          "contexts": [
            "…neural networks such as CNN, Bi-LSTM, Attention-based (Lo et al., 2024) BLIP, GPT-4 Retrieval 2024 (Wang et al., 2024a) GPT-J, T5 Generation 2024 (Lee, 2024) GPT-J Generation 2024 (Jiang et al., 2024) Llama-3, Mistral, and PatentGPT-J Generation 2024 (Bai et al., 2024) Llama-2 and Mixtral…",
            "(Lee, 2024) finetunes a pretrained model PatentGPT-J-6B using reinforcement learning from human feedback (RLHF) to align patent claim generation with drafting goals."
          ],
          "intents": [
            "--",
            "--"
          ],
          "cited_paper_info": {
            "title": "InstructPatentGPT: Training patent language models to follow instructions with human feedback",
            "abstract": "In this research, patent prosecution is conceptualized as a system of reinforcement learning from human feedback. The objective of the system is to increase the likelihood for a language model to generate patent claims that have a higher chance of being granted. To showcase the controllability of the language model, the system learns from granted patents and pre-grant applications with different rewards. The status of “granted” and “pre-grant” are perceived as labeled human feedback implicitly. In addition, specific to patent drafting, the experiments in this research demonstrate the model’s capability to learn from adjusting claim length and inclusion of limiting terms for narrowing claim scope. As proof of concept, the experiments focus on claim ones only and the training data originates from a patent dataset tailored specifically for artificial intelligence. Although the available human feedback in patent prosecution are limited and the quality of generated patent text requires improvement, the experiments following the 3-stage reinforcement learning from human feedback have demonstrated that generative language models are capable of reflecting the human feedback or intent in patent prosecution. To enhance the usability of language models, the implementation in this research utilizes modern techniques that enable execution on a single consumer-grade GPU. The demonstrated proof of concept, which reduces hardware requirements, will prove valuable in the future as more human feedback in patent prosecution become available for broader use, either within patent offices or in the public domain.",
            "year": 2024,
            "venue": "Artificial Intelligence and Law",
            "authors": [
              {
                "authorId": "2108319166",
                "name": "Jieh-Sheng Lee"
              }
            ]
          }
        },
        {
          "citedcorpusid": 270845577,
          "isinfluential": true,
          "contexts": [
            "Interestingly, general purpose models have outperformed domain specific models (Jiang et al., 2024) in this task.",
            "…Attention-based (Lo et al., 2024) BLIP, GPT-4 Retrieval 2024 (Wang et al., 2024a) GPT-J, T5 Generation 2024 (Lee, 2024) GPT-J Generation 2024 (Jiang et al., 2024) Llama-3, Mistral, and PatentGPT-J Generation 2024 (Bai et al., 2024) Llama-2 and Mixtral Generation 2024 (Ren and Ma, 2024) Qwen2…",
            "( Jiang et al., 2024) generate claims by incorporating descriptions instead of abstracts."
          ],
          "intents": [
            "--",
            "--",
            "--"
          ],
          "cited_paper_info": {
            "title": "Can Large Language Models Generate High-quality Patent Claims?",
            "abstract": "Large language models (LLMs) have shown exceptional performance across various text generation tasks but remain under-explored in the patent domain, which offers highly structured and precise language. This paper constructs a dataset to investigate the performance of current LLMs in patent claim generation. Our results demonstrate that generating claims based on patent descriptions outperforms previous research relying on abstracts. Interestingly, current patent-specific LLMs perform much worse than state-of-the-art general LLMs, highlighting the necessity for future research on in-domain LLMs. We also find that LLMs can produce high-quality first independent claims, but their performances markedly decrease for subsequent dependent claims. Moreover, fine-tuning can enhance the completeness of inventions' features, conceptual clarity, and feature linkage. Among the tested LLMs, GPT-4 demonstrates the best performance in comprehensive human evaluations by patent experts, with better feature coverage, conceptual clarity, and technical coherence. Despite these capabilities, comprehensive revision and modification are still necessary to pass rigorous patent scrutiny and ensure legal robustness.",
            "year": 2024,
            "venue": "North American Chapter of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "2148649304",
                "name": "Lekang Jiang"
              },
              {
                "authorId": "2294313959",
                "name": "Caiqi Zhang"
              },
              {
                "authorId": "2309007437",
                "name": "Pascal A Scherz"
              },
              {
                "authorId": "2290185595",
                "name": "Stephan Goetz"
              }
            ]
          }
        },
        {
          "citedcorpusid": 272918346,
          "isinfluential": false,
          "contexts": [
            "The existing patent surveys in the literature (Gomez and Moens, 2014; Ali et al., 2024; Krestel et al., 2021; Hanbury et al., 2011; Casola and Lavelli, 2022) do not cover the recent studies in this area and fail to show the trends and methods in task specific manner."
          ],
          "intents": [
            "--"
          ],
          "cited_paper_info": {
            "title": "Innovating Patent Retrieval: A Comprehensive Review of Techniques, Trends, and Challenges in Prior Art Searches",
            "abstract": "As the patent landscape continues to grow, so does the complexity of retrieving relevant “prior art”, “background art”, or “state of the art” from an expanding pool of publicly available patent data, a critical step in establishing novelty. However, retrieving this information presents significant challenges due to its volume and complexity. This systematic literature review surveys patent retrieval techniques over the past decade, focusing on ‘prior art’ and ‘novelty’ searches. Adhering to the PRISMA 2020 guidelines, our research includes 78 pertinent articles selected from a corpus of 1441, providing an in-depth overview of recent advancements, emerging trends, challenges, and future directions in the field of patent prior art retrieval. The review addresses six research questions: defining the current state of the art, evaluating the efficacy of various approaches, examining commonly used patent data collections, exploring the impact of semantic search and natural language processing (NLP) technologies, identifying frequently used components of patent documents, and discussing ongoing challenges in the domain of patent prior art search and retrieval. Our findings highlight the growing use of NLP to enhance the precision and comprehensiveness of patent searches, particularly on the Cross-Language Evaluation Forum for Intellectual Property (CLEF-IP) and the United States Patent and Trademark Office (USPTO) databases. Despite advancements, the specialized and technical nature of patent language continues to pose significant challenges in achieving high accuracy in patent retrieval.",
            "year": 2024,
            "venue": "Applied System Innovation",
            "authors": [
              {
                "authorId": "2323097236",
                "name": "Amna Ali"
              },
              {
                "authorId": "2866927",
                "name": "Ali Tufail"
              },
              {
                "authorId": "2075629368",
                "name": "L. D. De Silva"
              },
              {
                "authorId": "103638405",
                "name": "Pg Emeroylariffion Abas"
              }
            ]
          }
        },
        {
          "citedcorpusid": 273901256,
          "isinfluential": false,
          "contexts": [
            "As an example, patent citation has been considered as a proxy for patent valuation (Nandi et al., 2024; Hsu et al., 2020)."
          ],
          "intents": [
            "--"
          ],
          "cited_paper_info": {
            "title": "An Experimental Analysis on Evaluating Patent Citations",
            "abstract": "The patent citation count is a good indicator of patent quality. This often generates monetary value for the inventors and organizations. However, the factors that influence a patent receiving high citations over the year are still not well understood. With the patents over the past two decades, we study the problem of patent citation prediction and formulate this as a binary classification problem. We create a semantic graph of patents based on their semantic similarities, enabling the use of Graph Neural Network (GNN)-based approaches for predicting citations. Our experimental results demonstrate the effectiveness of our GNN-based methods when applied to the semantic graph, showing that they can accurately predict patent citations using only patent text. More specifically, these methods produce up to 94% recall for patents with high citations and outperform existing baselines. Furthermore, we leverage this constructed graph to gain insights and explanations for the predictions made by the GNNs.",
            "year": 2024,
            "venue": "Conference on Empirical Methods in Natural Language Processing",
            "authors": [
              {
                "authorId": "115999068",
                "name": "R. N. Nandi"
              },
              {
                "authorId": "2329735236",
                "name": "S. Maity"
              },
              {
                "authorId": "2329734988",
                "name": "Brian Uzzi"
              },
              {
                "authorId": "3390598",
                "name": "Sourav Medya"
              }
            ]
          }
        },
        {
          "citedcorpusid": 273901571,
          "isinfluential": false,
          "contexts": [
            "Patentformer (Wang et al., 2024a) generates detailed patent specifications by fine-tuning T5 and GPT-J language models on a dataset that includes claims, drawings, and descriptions.",
            "…2018) and (Aristodemou, 2021) apply a variety of neural networks such as CNN, Bi-LSTM, Attention-based (Lo et al., 2024) BLIP, GPT-4 Retrieval 2024 (Wang et al., 2024a) GPT-J, T5 Generation 2024 (Lee, 2024) GPT-J Generation 2024 (Jiang et al., 2024) Llama-3, Mistral, and PatentGPT-J Generation…"
          ],
          "intents": [
            "--",
            "--"
          ],
          "cited_paper_info": {
            "title": "Patentformer: A Novel Method to Automate the Generation of Patent Applications",
            "abstract": "In recent years, Large Language Models (LLMs) have demonstrated impressive performances across various NLP tasks. However, their potential for automating the task of writing patent documents remains relatively unexplored. To address this gap, in this work, we propose a novel method, Patentformer, for generating patent specification by fine-tuning the generative models with diverse sources of information, e.g., patent claims, drawing text, and brief descriptions of the drawings. To enhance the generative models’ comprehension of the complex task of writing patent specification, we introduce a new task, claim+drawing-to-specification, and release a new dataset. We evaluate our proposed method on thousands of patents from the USPTO and show that our method can generate human-like patent specification in legal writing style. Human evaluations by four patent experts further affirm that our proposed method has the potential to generate correct specification, and the quality of generated specification may sometimes be better than the actual specification.",
            "year": 2024,
            "venue": "Conference on Empirical Methods in Natural Language Processing",
            "authors": [
              {
                "authorId": "2165311295",
                "name": "Juanyan Wang"
              },
              {
                "authorId": "2329738956",
                "name": "Sai Mudhiganti"
              },
              {
                "authorId": "2330067131",
                "name": "Manali Sharma"
              }
            ]
          }
        },
        {
          "citedcorpusid": 274762766,
          "isinfluential": false,
          "contexts": [
            "…al., 2024) Llama-3, Mistral, and PatentGPT-J Generation 2024 (Bai et al., 2024) Llama-2 and Mixtral Generation 2024 (Ren and Ma, 2024) Qwen2 Generation 2024 (Wang et al., 2024b) Qwen2, LLAMA3, GPT-4o, Mistral Generation 2024 CNN (ACNN), deep and wide Artificial Neural Networks (ANN), respectively.",
            "A multi-agent framework for drafting patents using LLMs is introduced by (Wang et al., 2024b)."
          ],
          "intents": [
            "--",
            "--"
          ],
          "cited_paper_info": {
            "title": "AutoPatent: A Multi-Agent Framework for Automatic Patent Generation",
            "abstract": "As the capabilities of Large Language Models (LLMs) continue to advance, the field of patent processing has garnered increased attention within the natural language processing community. However, the majority of research has been concentrated on classification tasks, such as patent categorization and examination, or on short text generation tasks like patent summarization and patent quizzes. In this paper, we introduce a novel and practical task known as Draft2Patent, along with its corresponding D2P benchmark, which challenges LLMs to generate full-length patents averaging 17K tokens based on initial drafts. Patents present a significant challenge to LLMs due to their specialized nature, standardized terminology, and extensive length. We propose a multi-agent framework called AutoPatent which leverages the LLM-based planner agent, writer agents, and examiner agent with PGTree and RRAG to generate lengthy, intricate, and high-quality complete patent documents. The experimental results demonstrate that our AutoPatent framework significantly enhances the ability to generate comprehensive patents across various LLMs. Furthermore, we have discovered that patents generated solely with the AutoPatent framework based on the Qwen2.5-7B model outperform those produced by larger and more powerful LLMs, such as GPT-4o, Qwen2.5-72B, and LLAMA3.1-70B, in both objective metrics and human evaluations. We will make the data and code available upon acceptance at \\url{https://github.com/QiYao-Wang/AutoPatent}.",
            "year": 2024,
            "venue": "arXiv.org",
            "authors": [
              {
                "authorId": "2307111273",
                "name": "Qiyao Wang"
              },
              {
                "authorId": "2266469238",
                "name": "Shiwen Ni"
              },
              {
                "authorId": "2335620232",
                "name": "Huaren Liu"
              },
              {
                "authorId": "2307124302",
                "name": "Shule Lu"
              },
              {
                "authorId": "2316166706",
                "name": "Guhong Chen"
              },
              {
                "authorId": "2283976398",
                "name": "Xi Feng"
              },
              {
                "authorId": "2335834731",
                "name": "Chi Wei"
              },
              {
                "authorId": "2285319130",
                "name": "Qiang Qu"
              },
              {
                "authorId": "2306942431",
                "name": "Hamid Alinejad-Rokny"
              },
              {
                "authorId": "2335506457",
                "name": "Yuan Lin"
              },
              {
                "authorId": "2333904068",
                "name": "Min Yang"
              }
            ]
          }
        },
        {
          "citedcorpusid": null,
          "isinfluential": false,
          "contexts": [
            "The existing patent surveys in the literature (Gomez and Moens, 2014; Ali et al., 2024; Krestel et al., 2021; Hanbury et al., 2011; Casola and Lavelli, 2022) do not cover the recent studies in this area and fail to show the trends and methods in task specific manner."
          ],
          "intents": [
            "--"
          ],
          "cited_paper_info": {}
        },
        {
          "citedcorpusid": null,
          "isinfluential": false,
          "contexts": [
            "On the other hand, the generic quality analysis are based on well-known measures (Aristodemou, 2021; Erdogan et al., 2022).",
            "(Erdogan et al., 2022) apply an MLP-based approach for quality analysis, utilizing nine indices such as claim counts, forward citations, backward citations, the patent family size to measure the value of a patent, etc. (Li et al., 2022) classify patents based on their maintenance period in four…",
            "The works that use patent images are written in blue. a patent can be assessed using various measures, including the number of forward or backward citations, the number of claims, the grant lag, patent family size, the remaining lifetime of the patent (Aristodemou, 2021; Erdogan et al., 2022)."
          ],
          "intents": [
            "--",
            "--",
            "--"
          ],
          "cited_paper_info": {}
        },
        {
          "citedcorpusid": null,
          "isinfluential": true,
          "contexts": [
            "3.3.2 Pre-trained Language Models (PLMs) (Krant, 2023) proposes to use MSABERT to assess patent value based entirely on the textual data and use the OECD (Eurostat, O., 2005) quality indicators for evaluation."
          ],
          "intents": [
            "--"
          ],
          "cited_paper_info": {}
        },
        {
          "citedcorpusid": null,
          "isinfluential": false,
          "contexts": [
            "Nonetheless, it remains unclear which of these indices are associated with the actual value of the patent (e.g., generated revenue)."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {}
        }
      ]
    },
    "270967883": {
      "citing_paper_info": {
        "title": "Mining digital identity insights: patent analysis using NLP",
        "abstract": "The field of digital identity innovation has grown significantly over the last 30 years, with over 6000 technology patents registered worldwide. However, many questions remain about who controls and owns our digital identity and intellectual property and, ultimately, where the future of digital identity is heading. To investigate this further, this research mines digital identity patents and explores core themes such as identity, systems, privacy, security, and emerging fields like blockchain, financial transactions, and biometric technologies, utilizing natural language processing (NLP) methods including part-of-speech (POS) tagging, clustering, topic classification, noise reduction, and lemmatisation techniques. Finally, the research employs graph modelling and statistical analysis to discern inherent trends and forecast future developments. The findings significantly contribute to the digital identity landscape, identifying key players, emerging trends, and technological progress. This research serves as a valuable resource for academia and industry stakeholders, aiding in strategic decision-making and investment in emerging technologies and facilitating navigation through the dynamic realm of digital identity technologies.",
        "year": 2024,
        "venue": "EURASIP Journal on Information Security",
        "authors": [
          {
            "authorId": "2309763180",
            "name": "Matthew Comb"
          },
          {
            "authorId": "2309788010",
            "name": "Andrew Martin"
          }
        ]
      },
      "citation_summary": {
        "citation_count": 14,
        "unique_cited_count": 12,
        "influential_count": 3,
        "detailed_records_count": 14
      },
      "cited_papers": [
        "219792763",
        "157104256",
        "152760429",
        "24726740",
        "198229805",
        "49867601",
        "235366550",
        "52056126",
        "195345355",
        "11743567",
        "211458028",
        "10659969"
      ],
      "citation_details": [
        {
          "citedcorpusid": 10659969,
          "isinfluential": false,
          "contexts": [
            "Scikit-learn (sklearn) [46]: Our pipeline leveraged Scikit-learn extensively for implementing machine learning algorithms and statistical models."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Scikit-learn: Machine Learning in Python",
            "abstract": "Scikit-learn is a Python module integrating a wide range of state-of-the-art machine learning algorithms for medium-scale supervised and unsupervised problems. This package focuses on bringing machine learning to non-specialists using a general-purpose high-level language. Emphasis is put on ease of use, performance, documentation, and API consistency. It has minimal dependencies and is distributed under the simplified BSD license, encouraging its use in both academic and commercial settings. Source code, binaries, and documentation can be downloaded from http://scikit-learn.sourceforge.net.",
            "year": 2011,
            "venue": "Journal of machine learning research",
            "authors": [
              {
                "authorId": "2570016",
                "name": "Fabian Pedregosa"
              },
              {
                "authorId": "3025780",
                "name": "G. Varoquaux"
              },
              {
                "authorId": "1797840",
                "name": "Alexandre Gramfort"
              },
              {
                "authorId": "52200573",
                "name": "V. Michel"
              },
              {
                "authorId": "8493461",
                "name": "B. Thirion"
              },
              {
                "authorId": "2958756",
                "name": "O. Grisel"
              },
              {
                "authorId": "27257992",
                "name": "Mathieu Blondel"
              },
              {
                "authorId": "1881041",
                "name": "Gilles Louppe"
              },
              {
                "authorId": "2780213",
                "name": "P. Prettenhofer"
              },
              {
                "authorId": "2067827437",
                "name": "Ron Weiss"
              },
              {
                "authorId": "39571582",
                "name": "Ron J. Weiss"
              },
              {
                "authorId": "2081469",
                "name": "J. Vanderplas"
              },
              {
                "authorId": "144720379",
                "name": "Alexandre Passos"
              },
              {
                "authorId": "3084321",
                "name": "D. Cournapeau"
              },
              {
                "authorId": "2423884",
                "name": "M. Brucher"
              },
              {
                "authorId": "35243423",
                "name": "M. Perrot"
              },
              {
                "authorId": "1710398",
                "name": "E. Duchesnay"
              }
            ]
          }
        },
        {
          "citedcorpusid": 11743567,
          "isinfluential": false,
          "contexts": [
            "While the world is moving towards establishing ecosystems to foster digital identity-centric products and services [1, 3, 34, 42, 60], the fundamental question of digital identity ownership - who owns and controls the standards on which society’s digital identity solutions will be built - remains…"
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Transformation from Identity Stone Age to Digital Identity",
            "abstract": "Technological conversion, political interests and Business drivers has triggered a means, to establish individual characterization and personalization. People started raising concerns on multiple identities managed across various zones and hence various solutions were designed. Technological advancement has brought various issues and concerns around Identity assurance, privacy and policy enabled common Authentication framework. A compressive framework is needed to established common identity model to address national needs like standards, regulation and laws, minimum risk, interoperability and to provide user with a consistent context or user experience. \nThis document focuses on Transformation path of identity stone age to Identity as in state. It defines a digital identity zone model (DIZM) to showcase the Global Identity defined across the ecosystem. Also, provide insight of emerging Technology trend to enable Identity assurance, privacy and policy enabled common Authentication framework.",
            "year": 2011,
            "venue": "arXiv.org",
            "authors": [
              {
                "authorId": "2527942",
                "name": "Mohit Kohli"
              }
            ]
          }
        },
        {
          "citedcorpusid": 24726740,
          "isinfluential": false,
          "contexts": [
            "1) [17, 62–64]."
          ],
          "intents": [
            "--"
          ],
          "cited_paper_info": {
            "title": "SAO network analysis of patents for technology trends identification: a case study of polymer electrolyte membrane technology in proton exchange membrane fuel cells",
            "abstract": "",
            "year": 2011,
            "venue": "Scientometrics",
            "authors": [
              {
                "authorId": "7236231",
                "name": "Sungchul Choi"
              },
              {
                "authorId": "39508255",
                "name": "Janghyeok Yoon"
              },
              {
                "authorId": "24357608",
                "name": "Kwangsoo Kim"
              },
              {
                "authorId": "2155773090",
                "name": "Jae Yeol Lee"
              },
              {
                "authorId": "1728715",
                "name": "Cheol-Han Kim"
              }
            ]
          }
        },
        {
          "citedcorpusid": 49867601,
          "isinfluential": false,
          "contexts": [
            "Furthermore, despite increasing consumer demand to own their personal information [40, 47, 58], companies have registered over 6000 digital identity patents, demonstrating their perceived importance in shaping the future of digital services."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "A Survey on Essential Components of a Self-Sovereign Identity",
            "abstract": "",
            "year": 2018,
            "venue": "Computer Science Review",
            "authors": [
              {
                "authorId": "47543283",
                "name": "Alexander Mühle"
              },
              {
                "authorId": "69568501",
                "name": "Andreas Grüner"
              },
              {
                "authorId": "51136754",
                "name": "T. Gayvoronskaya"
              },
              {
                "authorId": "1708312",
                "name": "C. Meinel"
              }
            ]
          }
        },
        {
          "citedcorpusid": 52056126,
          "isinfluential": true,
          "contexts": [
            "4.2.6) relates to and reaffirms the emerging consensus regarding the advantageous integration of blockchain technologies within digital identity management solutions - a perspective steadily gaining ground in recent academic discussions [2, 8, 24, 56]."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Sora Identity: Secure, Digital Identity on the Blockchain",
            "abstract": "Digital identity is the cornerstone of a digital economy. However, proving identity remotely is difficult to do. To complicate things further, identity is usually not a global, absolute construct, but the information shared with different parties differs, based on the relationship to the user. Therefore, a viable solution for digital identity should enable users to have full control over their personal information and share only the information that they wish to share with each service. Blockchain technology can help to realize a self-sovereign identity that puts the user in control of her information, by enabling a decentralized way to handle public key infrastructure. In the current contribution, we present the Sora identity system, which is a mobile app that utilizes blockchain technology to create a secure protocol for storing encrypted personal information, as well as sharing verifiable claims about personal information.",
            "year": 2018,
            "venue": "Annual International Computer Software and Applications Conference",
            "authors": [
              {
                "authorId": "49868854",
                "name": "Makoto Takemiya"
              },
              {
                "authorId": "51197657",
                "name": "Bohdan Vanieiev"
              }
            ]
          }
        },
        {
          "citedcorpusid": 152760429,
          "isinfluential": false,
          "contexts": [
            "Digital identity - defined by Kim Cameron [15] as “a set of claims made by one digital subject (e.g., a user) about itself or another digital subject” - holds great significance in today’s fast-paced digital world.",
            "These laws are [15]: 1."
          ],
          "intents": [
            "['background']",
            "--"
          ],
          "cited_paper_info": {
            "title": "The Laws of Identity",
            "abstract": "This paper is about how we can prevent the loss of trust and go forward to give Internet users a deep sense of safety, privacy, and certainty about whom they are relating to in cyberspace. Nothing could be more essential if Web-based services and applications are to continue to move beyond \"cyber publication\" and encompass all kinds of interaction and services. Our approach has been to develop a formal understanding of the dynamics causing digital identity systems to succeed or fail in various contexts, expressed as the Laws of Identity. Taken together, these laws define a unifying identity metasystem that can offer the Internet the identity layer it so obviously requires.",
            "year": 2005,
            "venue": "",
            "authors": [
              {
                "authorId": "2285495457",
                "name": "Kim Cameron"
              },
              {
                "authorId": "2285481642",
                "name": "Andre Durand"
              },
              {
                "authorId": "2285503703",
                "name": "Bill Barnes"
              },
              {
                "authorId": "2285499232",
                "name": "Carl Ellison"
              },
              {
                "authorId": "97505757",
                "name": "Caspar Bowden"
              },
              {
                "authorId": "2285504343",
                "name": "Craig Burton"
              },
              {
                "authorId": "2285497263",
                "name": "Dan Blum"
              },
              {
                "authorId": "2285489997",
                "name": "Dave Kearns"
              },
              {
                "authorId": "2285502940",
                "name": "Dave Winer"
              },
              {
                "authorId": "2285502932",
                "name": "Dick Hardt"
              },
              {
                "authorId": "2285483066",
                "name": "Doc Searls"
              },
              {
                "authorId": "2904857",
                "name": "Drummond Reed"
              },
              {
                "authorId": "2285499250",
                "name": "Ellen Mcdermott"
              },
              {
                "authorId": "144904492",
                "name": "Eric Norlin"
              },
              {
                "authorId": "2285495114",
                "name": "Esther Dyson"
              },
              {
                "authorId": "151415147",
                "name": "Fen Labalme"
              },
              {
                "authorId": "2285490003",
                "name": "Woman Kaliya"
              },
              {
                "authorId": "2285495209",
                "name": "Jc Cannon"
              },
              {
                "authorId": "2285494992",
                "name": "James Kobielus"
              },
              {
                "authorId": "2714340",
                "name": "James Governor"
              },
              {
                "authorId": "2287814267",
                "name": "Jamie Lewis"
              },
              {
                "authorId": "2247397720",
                "name": "John Shewchuk"
              },
              {
                "authorId": "2285481633",
                "name": "Luke Razzell"
              },
              {
                "authorId": "2285502237",
                "name": "Marc Canter"
              },
              {
                "authorId": "2285498447",
                "name": "Mark Wahl"
              },
              {
                "authorId": "2286085394",
                "name": "Martin Taylor"
              },
              {
                "authorId": "2285541329",
                "name": "Mike Jones"
              },
              {
                "authorId": "2285505479",
                "name": "Phil Becker"
              },
              {
                "authorId": "2285481431",
                "name": "Radovan Janocek"
              },
              {
                "authorId": "2285497545",
                "name": "Ravi Pandya"
              },
              {
                "authorId": "97650808",
                "name": "R. Scoble"
              },
              {
                "authorId": "2285337814",
                "name": "Scott C Lem-On"
              },
              {
                "authorId": "2285505474",
                "name": "Simon Davies"
              },
              {
                "authorId": "2091358925",
                "name": "S. Brands"
              },
              {
                "authorId": "2285482584",
                "name": "Stuart Kwan"
              },
              {
                "authorId": "2285367769",
                "name": "William Heath"
              }
            ]
          }
        },
        {
          "citedcorpusid": 157104256,
          "isinfluential": false,
          "contexts": [
            "This feature is underpinned by a broad spectrum of academic explorations that spotlight the pivotal role of sound identity management infrastructures in contemporary digital ecosystems [9]."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Building Digital Identities: The Challenges, Risks and Opportunities of Collecting Behavioural Attributes for new Digital Identity Systems.",
            "abstract": "",
            "year": 2017,
            "venue": "",
            "authors": [
              {
                "authorId": "80730587",
                "name": "A. Beduschi"
              },
              {
                "authorId": "103432242",
                "name": "John M. Cinnamon"
              },
              {
                "authorId": "2087361749",
                "name": "J. Langford"
              },
              {
                "authorId": "2112754398",
                "name": "Chunbo Luo"
              },
              {
                "authorId": "49002519",
                "name": "D. Owen"
              }
            ]
          }
        },
        {
          "citedcorpusid": 195345355,
          "isinfluential": true,
          "contexts": [
            "4.2.6) relates to and reaffirms the emerging consensus regarding the advantageous integration of blockchain technologies within digital identity management solutions - a perspective steadily gaining ground in recent academic discussions [2, 8, 24, 56].",
            "It is a fundamental component in verifying and securing personal and transactional data [8, 14]."
          ],
          "intents": [
            "['background']",
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Towards a Blockchain based digital identity verification, record attestation and record sharing system",
            "abstract": "In this study, we investigated the utilization of Blockchain technology in identity systems. We reviewed the drawbacks and weaknesses of traditional identity systems, and discussed the potential benefits of employing Blockchain technology for a more efficient identity system. As a result, we proposed a Blockchain based digital identity verification, record attestation and record sharing system. When compared to traditional identity systems, it makes identity verification and identity based record sharing more efficient and secure, while respecting privacy of identity owners. By exploiting the trust fabric of Blockchain, a Blockchain based identity system eliminates the middle man and wait lines for authentication, authorization, and attestation. It allows individuals to decide what parts of their identity they want to share and with whom they want to share it.",
            "year": 2019,
            "venue": "arXiv.org",
            "authors": [
              {
                "authorId": "3315385",
                "name": "Mehmet Aydar"
              },
              {
                "authorId": "2460907",
                "name": "S. Ayvaz"
              }
            ]
          }
        },
        {
          "citedcorpusid": 198229805,
          "isinfluential": false,
          "contexts": [
            "SciPy [61]: Employed in tandem with NumPy, SciPy enabled high-level computations such as linear algebra and optimisation that were crucial in processing and analysing large volumes of patent data."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "SciPy 1.0: fundamental algorithms for scientific computing in Python",
            "abstract": "SciPy is an open-source scientific computing library for the Python programming language. Since its initial release in 2001, SciPy has become a de facto standard for leveraging scientific algorithms in Python, with over 600 unique code contributors, thousands of dependent packages, over 100,000 dependent repositories and millions of downloads per year. In this work, we provide an overview of the capabilities and development practices of SciPy 1.0 and highlight some recent technical developments. This Perspective describes the development and capabilities of SciPy 1.0, an open source scientific computing library for the Python programming language.",
            "year": 2019,
            "venue": "Nature Methods",
            "authors": [
              {
                "authorId": "2053486178",
                "name": "Pauli Virtanen"
              },
              {
                "authorId": "3912995",
                "name": "R. Gommers"
              },
              {
                "authorId": "35525979",
                "name": "T. Oliphant"
              },
              {
                "authorId": "2065380893",
                "name": "Matt Haberland"
              },
              {
                "authorId": "144896751",
                "name": "Tyler Reddy"
              },
              {
                "authorId": "3084321",
                "name": "D. Cournapeau"
              },
              {
                "authorId": "2143743195",
                "name": "Evgeni Burovski"
              },
              {
                "authorId": "143825690",
                "name": "Pearu Peterson"
              },
              {
                "authorId": "2214143",
                "name": "Warren Weckesser"
              },
              {
                "authorId": "2065294575",
                "name": "Jonathan Bright"
              },
              {
                "authorId": "2066718598",
                "name": "S. J. van der Walt"
              },
              {
                "authorId": "144082394",
                "name": "M. Brett"
              },
              {
                "authorId": "2115849250",
                "name": "Joshua Wilson"
              },
              {
                "authorId": "2061249",
                "name": "K. Millman"
              },
              {
                "authorId": "101711386",
                "name": "N. Mayorov"
              },
              {
                "authorId": "2072898697",
                "name": "Andrew R. J. Nelson"
              },
              {
                "authorId": "48376304",
                "name": "E. Jones"
              },
              {
                "authorId": "2066375342",
                "name": "Robert Kern"
              },
              {
                "authorId": "144752199",
                "name": "Eric Larson"
              },
              {
                "authorId": "144873258",
                "name": "C. Carey"
              },
              {
                "authorId": "49005409",
                "name": "İlhan Polat"
              },
              {
                "authorId": "2150672343",
                "name": "Yu Feng"
              },
              {
                "authorId": "2054115424",
                "name": "Eric W. Moore"
              },
              {
                "authorId": "2081469",
                "name": "J. Vanderplas"
              },
              {
                "authorId": "98592399",
                "name": "D. Laxalde"
              },
              {
                "authorId": "15571182",
                "name": "Josef Perktold"
              },
              {
                "authorId": "2772998",
                "name": "R. Cimrman"
              },
              {
                "authorId": "35265702",
                "name": "Ian Henriksen"
              },
              {
                "authorId": "153037053",
                "name": "E. Quintero"
              },
              {
                "authorId": "2065073027",
                "name": "Charles R. Harris"
              },
              {
                "authorId": "6402888",
                "name": "A. Archibald"
              },
              {
                "authorId": "19235619",
                "name": "Antônio H. Ribeiro"
              },
              {
                "authorId": "2570016",
                "name": "Fabian Pedregosa"
              },
              {
                "authorId": "1491359454",
                "name": "P. van Mulbregt"
              },
              {
                "authorId": "2064378280",
                "name": "Aditya Alessandro Pietro Alex Andreas Andreas Anthony Ant Vijaykumar Bardelli Rothberg Hilboll Kloeckner Sco"
              },
              {
                "authorId": "1491360442",
                "name": "A. Vijaykumar"
              },
              {
                "authorId": "46473152",
                "name": "Alessandro Pietro Bardelli"
              },
              {
                "authorId": "13044073",
                "name": "Alex Rothberg"
              },
              {
                "authorId": "5301477",
                "name": "A. Hilboll"
              },
              {
                "authorId": "117221049",
                "name": "Andre Kloeckner"
              },
              {
                "authorId": "2860725",
                "name": "A. Scopatz"
              },
              {
                "authorId": "2116599554",
                "name": "Antony Lee"
              },
              {
                "authorId": "2842990",
                "name": "Ariel S. Rokem"
              },
              {
                "authorId": "144291907",
                "name": "C. N. Woods"
              },
              {
                "authorId": "80845765",
                "name": "Chad Fulton"
              },
              {
                "authorId": "122327721",
                "name": "Charles Masson"
              },
              {
                "authorId": "1491360726",
                "name": "C. Häggström"
              },
              {
                "authorId": "73014178",
                "name": "Clark Fitzgerald"
              },
              {
                "authorId": "46347313",
                "name": "D. Nicholson"
              },
              {
                "authorId": "1491359271",
                "name": "David R. Hagen"
              },
              {
                "authorId": "1721034",
                "name": "D. Pasechnik"
              },
              {
                "authorId": "1759500",
                "name": "E. Olivetti"
              },
              {
                "authorId": "2151071148",
                "name": "Eric Martin"
              },
              {
                "authorId": "34422202",
                "name": "Eric Wieser"
              },
              {
                "authorId": "2110243690",
                "name": "Fabrice Silva"
              },
              {
                "authorId": "15628537",
                "name": "F. Lenders"
              },
              {
                "authorId": "2052594289",
                "name": "Florian Wilhelm"
              },
              {
                "authorId": "113071069",
                "name": "G. Young"
              },
              {
                "authorId": "2058961413",
                "name": "Gavin A. Price"
              },
              {
                "authorId": "40657747",
                "name": "G. Ingold"
              },
              {
                "authorId": "2059529796",
                "name": "Gregory E. Allen"
              },
              {
                "authorId": "87747838",
                "name": "Gregory R. Lee"
              },
              {
                "authorId": "40936461",
                "name": "H. Audren"
              },
              {
                "authorId": "35148114",
                "name": "I. Probst"
              },
              {
                "authorId": "144455209",
                "name": "J. Dietrich"
              },
              {
                "authorId": "2669435",
                "name": "J. Silterra"
              },
              {
                "authorId": "38847103",
                "name": "James T. Webber"
              },
              {
                "authorId": "31387499",
                "name": "J. Slavič"
              },
              {
                "authorId": "3083916",
                "name": "J. Nothman"
              },
              {
                "authorId": "2151427",
                "name": "J. Buchner"
              },
              {
                "authorId": "2562288",
                "name": "Johannes Kulick"
              },
              {
                "authorId": "3010882",
                "name": "Johannes L. Schönberger"
              },
              {
                "authorId": "1491360335",
                "name": "J. V. De Miranda Cardoso"
              },
              {
                "authorId": "145935219",
                "name": "J. Reimer"
              },
              {
                "authorId": "2068517007",
                "name": "J. Harrington"
              },
              {
                "authorId": "152794754",
                "name": "Juan Rodríguez"
              },
              {
                "authorId": "1398851518",
                "name": "Juan Nunez-Iglesias"
              },
              {
                "authorId": "48736174",
                "name": "Justin Kuczynski"
              },
              {
                "authorId": "50159131",
                "name": "K. Tritz"
              },
              {
                "authorId": "47049820",
                "name": "M. Thoma"
              },
              {
                "authorId": "144620446",
                "name": "M. Newville"
              },
              {
                "authorId": "2997408",
                "name": "Matthias Kümmerer"
              },
              {
                "authorId": "48393751",
                "name": "Maximilian Bolingbroke"
              },
              {
                "authorId": "2980014",
                "name": "Michael Tartre"
              },
              {
                "authorId": "36917288",
                "name": "M. Pak"
              },
              {
                "authorId": "2116828326",
                "name": "Nathaniel J. Smith"
              },
              {
                "authorId": "28955794",
                "name": "N. Nowaczyk"
              },
              {
                "authorId": "1491361121",
                "name": "Nikolay Shebanov"
              },
              {
                "authorId": "144208188",
                "name": "O. Pavlyk"
              },
              {
                "authorId": "96695635",
                "name": "P. A. Brodtkorb"
              },
              {
                "authorId": "2111215482",
                "name": "Perry Lee"
              },
              {
                "authorId": "144431879",
                "name": "R. McGibbon"
              },
              {
                "authorId": "3449704",
                "name": "Roman Feldbauer"
              },
              {
                "authorId": "2112242667",
                "name": "Sam Lewis"
              },
              {
                "authorId": "152330021",
                "name": "S. Tygier"
              },
              {
                "authorId": "34953991",
                "name": "Scott Sievert"
              },
              {
                "authorId": "1737624",
                "name": "S. Vigna"
              },
              {
                "authorId": "2053615130",
                "name": "Stefan Peterson"
              },
              {
                "authorId": "5891171",
                "name": "S. More"
              },
              {
                "authorId": "92169827",
                "name": "Tadeusz Pudlik"
              },
              {
                "authorId": "66273392",
                "name": "T. Oshima"
              },
              {
                "authorId": "1915727",
                "name": "T. Pingel"
              },
              {
                "authorId": "144512158",
                "name": "T. Robitaille"
              },
              {
                "authorId": "3419085",
                "name": "Thomas Spura"
              },
              {
                "authorId": "2646100",
                "name": "T. Jones"
              },
              {
                "authorId": "90587764",
                "name": "T. Cera"
              },
              {
                "authorId": "1491358355",
                "name": "Tim Leslie"
              },
              {
                "authorId": "2207047",
                "name": "Tiziano Zito"
              },
              {
                "authorId": "1491360459",
                "name": "Tom Krauss"
              },
              {
                "authorId": "10515643",
                "name": "U. Upadhyay"
              },
              {
                "authorId": "1722413",
                "name": "Y. Halchenko"
              },
              {
                "authorId": "1398440330",
                "name": "Y. Vázquez-Baeza"
              }
            ]
          }
        },
        {
          "citedcorpusid": 211458028,
          "isinfluential": false,
          "contexts": [
            "Alipay is used for “restaurants, taxis, school fees, cinema tickets and even to transfer money to each other” [12]."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Who Can You Trust?: How Technology Brought Us Together and Why It Might Drive Us Apart",
            "abstract": "If you can't trust those in charge, who can you trust? From government to business, banks to media, trust in institutions is at an all-time low. But this isn't the age of distrust--far from it. In this revolutionary book, world-renowned trust expert Rachel Botsman reveals that we are at the tipping point of one of the biggest social transformations in human history--with fundamental consequences for everyone. A new world order is emerging: we might have lost faith in institutions and leaders, but millions of people rent their homes to total strangers, exchange digital currencies, or find themselves trusting a bot. This is the age of \"distributed trust,\" a paradigm shift driven by innovative technologies that are rewriting the rules of an all-too-human relationship. If we are to benefit from this radical shift, we must understand the mechanics of how trust is built, managed, lost, and repaired in the digital age. In the first book to explain this new world, Botsman provides a detailed map of this uncharted landscape--and explores what's next for humanity.",
            "year": 2017,
            "venue": "",
            "authors": [
              {
                "authorId": "144319114",
                "name": "Rachel Botsman"
              }
            ]
          }
        },
        {
          "citedcorpusid": 219792763,
          "isinfluential": false,
          "contexts": [
            "NumPy [27]: This module played a crucial role in handling large arrays of numerical data intrinsic to the patent datasets."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Array programming with NumPy",
            "abstract": "Array programming provides a powerful, compact and expressive syntax for accessing, manipulating and operating on data in vectors, matrices and higher-dimensional arrays. NumPy is the primary array programming library for the Python language. It has an essential role in research analysis pipelines in fields as diverse as physics, chemistry, astronomy, geoscience, biology, psychology, materials science, engineering, finance and economics. For example, in astronomy, NumPy was an important part of the software stack used in the discovery of gravitational waves1 and in the first imaging of a black hole2. Here we review how a few fundamental array concepts lead to a simple and powerful programming paradigm for organizing, exploring and analysing scientific data. NumPy is the foundation upon which the scientific Python ecosystem is constructed. It is so pervasive that several projects, targeting audiences with specialized needs, have developed their own NumPy-like interfaces and array objects. Owing to its central position in the ecosystem, NumPy increasingly acts as an interoperability layer between such array computation libraries and, together with its application programming interface (API), provides a flexible framework to support the next decade of scientific and industrial analysis. NumPy is the primary array programming library for Python; here its fundamental concepts are reviewed and its evolution into a flexible interoperability layer between increasingly specialized computational libraries is discussed.",
            "year": 2020,
            "venue": "Nature",
            "authors": [
              {
                "authorId": "2065073027",
                "name": "Charles R. Harris"
              },
              {
                "authorId": "2061249",
                "name": "K. Millman"
              },
              {
                "authorId": "39888153",
                "name": "S. Walt"
              },
              {
                "authorId": "3912995",
                "name": "R. Gommers"
              },
              {
                "authorId": "2053486178",
                "name": "Pauli Virtanen"
              },
              {
                "authorId": "3084321",
                "name": "D. Cournapeau"
              },
              {
                "authorId": "34422202",
                "name": "Eric Wieser"
              },
              {
                "authorId": "2110004284",
                "name": "Julian Taylor"
              },
              {
                "authorId": "2069160663",
                "name": "Sebastian Berg"
              },
              {
                "authorId": "2116828326",
                "name": "Nathaniel J. Smith"
              },
              {
                "authorId": "2066375342",
                "name": "Robert Kern"
              },
              {
                "authorId": "1395072875",
                "name": "Matti Picus"
              },
              {
                "authorId": "7018631",
                "name": "Stephan Hoyer"
              },
              {
                "authorId": "40801125",
                "name": "M. Kerkwijk"
              },
              {
                "authorId": "144082394",
                "name": "M. Brett"
              },
              {
                "authorId": "3029849",
                "name": "A. Haldane"
              },
              {
                "authorId": "1752588468",
                "name": "Jaime Fern'andez del R'io"
              },
              {
                "authorId": "2037317966",
                "name": "Marcy Wiebe"
              },
              {
                "authorId": "143825690",
                "name": "Pearu Peterson"
              },
              {
                "authorId": "1752588457",
                "name": "Pierre G'erard-Marchant"
              },
              {
                "authorId": "46183258",
                "name": "Kevin Sheppard"
              },
              {
                "authorId": "144896751",
                "name": "Tyler Reddy"
              },
              {
                "authorId": "2214143",
                "name": "Warren Weckesser"
              },
              {
                "authorId": "153697803",
                "name": "Hameer Abbasi"
              },
              {
                "authorId": "102782888",
                "name": "C. Gohlke"
              },
              {
                "authorId": "35525979",
                "name": "T. Oliphant"
              }
            ]
          }
        },
        {
          "citedcorpusid": 235366550,
          "isinfluential": false,
          "contexts": [
            "While the world is moving towards establishing ecosystems to foster digital identity-centric products and services [1, 3, 34, 42, 60], the fundamental question of digital identity ownership - who owns and controls the standards on which society’s digital identity solutions will be built - remains…"
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Building Trust: Lessons from Canada’s Approach to Digital Identity",
            "abstract": "© 2020 Observer Research Foundation. All rights reserved. No part of this publication may be reproduced, copied, archived, retained or transmitted through print, speech or electronic media without prior written approval from ORF. Attribution: Sunil Abraham, “Building Trust: Lessons from Canada’s Approach to Digital Identity,” ORF Issue Brief No. 367, June 2020, Observer Research Foundation. ABSTRACT Both during times of normalcy and crises, governments depend on increasingly digitised identity systems. Such systems, however, have been considered controversial since the use of IBM machines to facilitate the Holocaust. Since then, more contemporary identity systems have tried to ensure that they do not violate citizens’ essential rights. This requires multi-stakeholder coordination, a network paradigm, a focus on open standards rather than specific technologies, clarity and predictability on intellectual property, an openness to the latest technological developments, and a commitment to interoperability and compatibility across institutions and entities. Most critically, successful digital identity projects need to build trust. This brief draws lessons from Canada’s experience of building a national identity ecosystem.",
            "year": 2020,
            "venue": "",
            "authors": [
              {
                "authorId": "31994702",
                "name": "S. Abraham"
              },
              {
                "authorId": "31994702",
                "name": "S. Abraham"
              }
            ]
          }
        },
        {
          "citedcorpusid": null,
          "isinfluential": false,
          "contexts": [
            "Oracle established its first digital identity patent (US-10063523-B2) titled ‘Crafted identities’ [16] in 2005, with a recent digital identity patent, ‘Service Discovery for a Multi-Tenant Identity’ [25] (US-2018041515-A1), filed in 2017."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {}
        },
        {
          "citedcorpusid": null,
          "isinfluential": true,
          "contexts": [
            "4.2.6) relates to and reaffirms the emerging consensus regarding the advantageous integration of blockchain technologies within digital identity management solutions - a perspective steadily gaining ground in recent academic discussions [2, 8, 24, 56]."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {}
        }
      ]
    },
    "216043471": {
      "citing_paper_info": {
        "title": "Multi-label Patent Classification using Attention-Aware Deep Learning Model",
        "abstract": "Patent classification is challenging and essential for any further patent analysis task. We tackle the classification task on lower level patent classification (subgroup level) by using AttentionXML. Recently, pretraining methods for Natural Language Processing (NLP), such as DistilBERT pre-trained model, have achieved state-of-the-art results on some NLP tasks such as text classification. In this work we focus on investigating the effect of applying DistilBERT pre-trained model and fine-tuning it for the important task of multi-label patent classification. Moreover, the large USPTO-3M dataset (3,050,625 patents) based on CPC subclass and subgroup level is used for the purpose of comparing previous deep learning related studies.",
        "year": 2020,
        "venue": "International Conference on Big Data and Smart Computing",
        "authors": [
          {
            "authorId": "9346492",
            "name": "A. H. Roudsari"
          },
          {
            "authorId": "145653991",
            "name": "Jafar Afshar"
          },
          {
            "authorId": "1646750026",
            "name": "Charles Cheolgi Lee"
          },
          {
            "authorId": "1728685",
            "name": "Wookey Lee"
          }
        ]
      },
      "citation_summary": {
        "citation_count": 5,
        "unique_cited_count": 5,
        "influential_count": 0,
        "detailed_records_count": 5
      },
      "cited_papers": [
        "52967399",
        "24708078",
        "64850768",
        "2871882",
        "146808575"
      ],
      "citation_details": [
        {
          "citedcorpusid": 2871882,
          "isinfluential": false,
          "contexts": [
            "tasks [2]."
          ],
          "intents": [
            "--"
          ],
          "cited_paper_info": {
            "title": "A Survey of Automated Hierarchical Classification of Patents",
            "abstract": "",
            "year": 2014,
            "venue": "Professional Search in the Modern World",
            "authors": [
              {
                "authorId": "144004546",
                "name": "J. Gómez"
              },
              {
                "authorId": "145446752",
                "name": "Marie-Francine Moens"
              }
            ]
          }
        },
        {
          "citedcorpusid": 24708078,
          "isinfluential": false,
          "contexts": [
            "Only the most relevant works is mentioned in this section, nevertheless some previous works are worth mentioning such as Chen and Chang [6] that focused on subgroup level patent classification and a more recent work by D’hondt et al."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "A three-phase method for patent classification",
            "abstract": "",
            "year": 2012,
            "venue": "Information Processing & Management",
            "authors": [
              {
                "authorId": "123331773",
                "name": "Yen-Liang Chen"
              },
              {
                "authorId": "2140047299",
                "name": "Yuan Chang"
              }
            ]
          }
        },
        {
          "citedcorpusid": 52967399,
          "isinfluential": false,
          "contexts": [
            "In [8], Lee and Hsiange took advantage of the pre-trained BERT model [9] and ﬁne-tuned it for patent classiﬁcation and compared their result with DeepPatent."
          ],
          "intents": [
            "['result']"
          ],
          "cited_paper_info": {
            "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
            "abstract": "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).",
            "year": 2019,
            "venue": "North American Chapter of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "39172707",
                "name": "Jacob Devlin"
              },
              {
                "authorId": "1744179",
                "name": "Ming-Wei Chang"
              },
              {
                "authorId": "2544107",
                "name": "Kenton Lee"
              },
              {
                "authorId": "3259253",
                "name": "Kristina Toutanova"
              }
            ]
          }
        },
        {
          "citedcorpusid": 64850768,
          "isinfluential": false,
          "contexts": [
            "in [4].",
            "Third, the unbalanced distribution of patents in different categories result in sparsity of available number of documents in each category and makes the training difficult [4]."
          ],
          "intents": [
            "--",
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Patent Classification on Subgroup Level Using Balanced Winnow",
            "abstract": "",
            "year": 2017,
            "venue": "",
            "authors": [
              {
                "authorId": "1447164726",
                "name": "E.K.L. D'hondt"
              },
              {
                "authorId": "1702730",
                "name": "S. Verberne"
              },
              {
                "authorId": "52109694",
                "name": "N. Oostdijk"
              },
              {
                "authorId": "1728633",
                "name": "L. Boves"
              }
            ]
          }
        },
        {
          "citedcorpusid": 146808575,
          "isinfluential": false,
          "contexts": [
            "On the other hand, two most recent works on XMTC are AttentionXML [10] and X-BERT [11]."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "A Modular Deep Learning Approach for Extreme Multi-label Text Classification",
            "abstract": "Extreme multi-label classification (XMC) aims to assign to an instance the most relevant subset of labels from a colossal label set. Due to modern applications that lead to massive label sets, the scalability of XMC has attracted much recent attention from both academia and industry. In this paper, we establish a three-stage framework to solve XMC efficiently, which includes 1) indexing the labels, 2) matching the instance to the relevant indices, and 3) ranking the labels from the relevant indices. This framework unifies many existing XMC approaches. Based on this framework, we propose a modular deep learning approach SLINMER: Semantic Label Indexing, Neural Matching, and Efficient Ranking. The label indexing stage of SLINMER can adopt different semantic label representations leading to different configurations of SLINMER. Empirically, we demonstrate that several individual configurations of SLINMER achieve superior performance than the state-of-the-art XMC approaches on several benchmark datasets. Moreover, by ensembling those configurations, SLINMER can achieve even better results. In particular, on a Wiki dataset with around 0.5 millions of labels, the precision@1 is increased from 61% to 67%.",
            "year": 2019,
            "venue": "arXiv.org",
            "authors": [
              {
                "authorId": "1702500",
                "name": "Wei-Cheng Chang"
              },
              {
                "authorId": "46185316",
                "name": "Hsiang-Fu Yu"
              },
              {
                "authorId": "144474630",
                "name": "Kai Zhong"
              },
              {
                "authorId": "35729970",
                "name": "Yiming Yang"
              },
              {
                "authorId": "1783667",
                "name": "I. Dhillon"
              }
            ]
          }
        }
      ]
    },
    "276250058": {
      "citing_paper_info": {
        "title": "Can AI Examine Novelty of Patents?: Novelty Evaluation Based on the Correspondence between Patent Claim and Prior Art",
        "abstract": "Assessing the novelty of patent claims is a critical yet challenging task traditionally performed by patent examiners. While advancements in NLP have enabled progress in various patent-related tasks, novelty assessment remains unexplored. This paper introduces a novel challenge by evaluating the ability of large language models (LLMs) to assess patent novelty by comparing claims with cited prior art documents, following the process similar to that of patent examiners done. We present the first dataset specifically designed for novelty evaluation, derived from real patent examination cases, and analyze the capabilities of LLMs to address this task. Our study reveals that while classification models struggle to effectively assess novelty, generative models make predictions with a reasonable level of accuracy, and their explanations are accurate enough to understand the relationship between the target patent and prior art. These findings demonstrate the potential of LLMs to assist in patent evaluation, reducing the workload for both examiners and applicants. Our contributions highlight the limitations of current models and provide a foundation for improving AI-driven patent analysis through advanced models and refined datasets.",
        "year": 2025,
        "venue": "arXiv.org",
        "authors": [
          {
            "authorId": "2344757616",
            "name": "Hayato Ikoma"
          },
          {
            "authorId": "2259321079",
            "name": "Teruko Mitamura"
          }
        ]
      },
      "citation_summary": {
        "citation_count": 5,
        "unique_cited_count": 5,
        "influential_count": 1,
        "detailed_records_count": 5
      },
      "cited_papers": [
        "259950998",
        "233583623",
        "259370554",
        "257291969",
        "250408264"
      ],
      "citation_details": [
        {
          "citedcorpusid": 233583623,
          "isinfluential": false,
          "contexts": [
            "This has led to the proposal of numerous tasks, including support tasks, classification, and retrieval tasks (Krestel et al., 2021)."
          ],
          "intents": [
            "--"
          ],
          "cited_paper_info": {
            "title": "A survey on deep learning for patent analysis",
            "abstract": "",
            "year": 2021,
            "venue": "World Patent Information",
            "authors": [
              {
                "authorId": "3264110",
                "name": "Ralf Krestel"
              },
              {
                "authorId": "2047942384",
                "name": "Renukswamy Chikkamath"
              },
              {
                "authorId": "2042640458",
                "name": "Christoph Hewel"
              },
              {
                "authorId": "1695993",
                "name": "Julian Risch"
              }
            ]
          }
        },
        {
          "citedcorpusid": 250408264,
          "isinfluential": false,
          "contexts": [
            "To improve compatibility with HUPD (Suzgun et al., 2024), every patent document collected is also contained in HUPD.",
            "In the research field of patentablity evaluation, Suz-gun et al. (2024) developed a large-scale patent dataset (HUPD) and one of the proposed task us-ing HUPD is evaluating the ability of predicting patentability from claim text or abstracts.",
            "In addition to it, some researches(e.g. Suzgun et al., 2024; Gao et al., 2022) reported that patentability can be predicted in a certain accuracy from only claim texts."
          ],
          "intents": [
            "--",
            "--",
            "--"
          ],
          "cited_paper_info": {
            "title": "The Harvard USPTO Patent Dataset: A Large-Scale, Well-Structured, and Multi-Purpose Corpus of Patent Applications",
            "abstract": "Innovation is a major driver of economic and social development, and information about many kinds of innovation is embedded in semi-structured data from patents and patent applications. Although the impact and novelty of innovations expressed in patent data are difficult to measure through traditional means, ML offers a promising set of techniques for evaluating novelty, summarizing contributions, and embedding semantics. In this paper, we introduce the Harvard USPTO Patent Dataset (HUPD), a large-scale, well-structured, and multi-purpose corpus of English-language patent applications filed to the United States Patent and Trademark Office (USPTO) between 2004 and 2018. With more than 4.5 million patent documents, HUPD is two to three times larger than comparable corpora. Unlike previously proposed patent datasets in NLP, HUPD contains the inventor-submitted versions of patent applications--not the final versions of granted patents--thereby allowing us to study patentability at the time of filing using NLP methods for the first time. It is also novel in its inclusion of rich structured metadata alongside the text of patent filings: By providing each application's metadata along with all of its text fields, the dataset enables researchers to perform new sets of NLP tasks that leverage variation in structured covariates. As a case study on the types of research HUPD makes possible, we introduce a new task to the NLP community--namely, binary classification of patent decisions. We additionally show the structured metadata provided in the dataset enables us to conduct explicit studies of concept shifts for this task. Finally, we demonstrate how HUPD can be used for three additional tasks: multi-class classification of patent subject areas, language modeling, and summarization.",
            "year": 2022,
            "venue": "Neural Information Processing Systems",
            "authors": [
              {
                "authorId": "51903517",
                "name": "Mirac Suzgun"
              },
              {
                "authorId": "2268317740",
                "name": "Luke Melas-Kyriazi"
              },
              {
                "authorId": "152970139",
                "name": "Suproteem K. Sarkar"
              },
              {
                "authorId": "1794750",
                "name": "S. Kominers"
              },
              {
                "authorId": "1692491",
                "name": "Stuart M. Shieber"
              }
            ]
          }
        },
        {
          "citedcorpusid": 257291969,
          "isinfluential": false,
          "contexts": [
            "Related to our approach to compare each element of claims to cited texts, Schmitt et al. (2023) suggested a mathematical-logical approach for assessing patentability by comparing the feature combinations of patent claims with the pertinent prior art."
          ],
          "intents": [
            "--"
          ],
          "cited_paper_info": {
            "title": "Assessment of patentability by means of semantic patent analysis – A mathematical-logical approach",
            "abstract": "",
            "year": 2023,
            "venue": "World Patent Information",
            "authors": [
              {
                "authorId": "2210411578",
                "name": "Valentin J. Schmitt"
              },
              {
                "authorId": "97601156",
                "name": "Lothar Walter"
              },
              {
                "authorId": "73505899",
                "name": "Frank C. Schnittker"
              }
            ]
          }
        },
        {
          "citedcorpusid": 259370554,
          "isinfluential": false,
          "contexts": [
            "In aspects of comparison between patent claims and patent descriptions, Hashimoto et al. (2023) proposed a new task to make comparisons between the claim and its description paragraphs to extract unclaimed embodiments."
          ],
          "intents": [
            "--"
          ],
          "cited_paper_info": {
            "title": "Hunt for Buried Treasures: Extracting Unclaimed Embodiments from Patent Specifications",
            "abstract": "Patent applicants write patent specificationsthat describe embodiments of inventions.Some embodiments are claimed for a patent,while others may be unclaimeddue to strategic considerations.Unclaimed embodiments may be extracted byapplicants later and claimed incontinuing applications togain advantages over competitors.Despite being essential for corporate intellectual property (IP) strategies,unclaimed embodiment extraction is conducted manually,and little research has been conducted on its automation.This paper presents a novel task ofunclaimed embodiment extraction (UEE)and a novel dataset for the task.Our experiments with Transformer-based modelsdemonstratedthat the task was challenging as it requiredconducting natural language inference onpatent specifications, which consisted oftechnical, long, syntactically and semanticallyinvolved sentences.We release the dataset and code to foster this new area of research.",
            "year": 2023,
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "1760686",
                "name": "Chikara Hashimoto"
              },
              {
                "authorId": "2186564515",
                "name": "Gautam Kumar"
              },
              {
                "authorId": "2066740552",
                "name": "Shuichiro Hashimoto"
              },
              {
                "authorId": "2221286688",
                "name": "Jun Suzuki"
              }
            ]
          }
        },
        {
          "citedcorpusid": 259950998,
          "isinfluential": true,
          "contexts": [
            "Longformer : Longformer is a Encoder-only model pretrained for MLM on long documents(Beltagy et al., 2020 dow of Llama2 is 4096, that of Llama3 is 8192, and they were pretrained with adequately long documents(Touvron et al., 2023).",
            "The results of claim-cited texts input suggest that models with smaller parameter, such as Llama2 13B and Llama3 8B, appear to predict insufficiently.",
            "Although the first approach is common for encoder-only models, it has been reported that the decoder-only models, such as Llama2 perform effectively in classification tasks using a classification head at the top(e.g., Li et al. (2023)) 5 .",
            "Most of other results of Llama2 13B and Llama3 8B are close to 0.5.",
            "This observation aligns with the experiment results that Llama2 models with the classification head outperform encoder-only models, such as BERT, RoBERTa, in classification tasks.",
            "We used Llama2 7B, 13B and Llama3 8B, 70B.",
            "The chosen models are: Long-former (Beltagy et al., 2020), Llama2(Touvron et al., 2023), Llama3 (Meta(2024)(Meta, 2024)), GPT-4o(openAI, 2024) to assess them with their architecture and parameter size."
          ],
          "intents": [
            "--",
            "--",
            "--",
            "--",
            "--",
            "--",
            "--"
          ],
          "cited_paper_info": {
            "title": "Llama 2: Open Foundation and Fine-Tuned Chat Models",
            "abstract": "In this work, we develop and release Llama 2, a collection of pretrained and fine-tuned large language models (LLMs) ranging in scale from 7 billion to 70 billion parameters. Our fine-tuned LLMs, called Llama 2-Chat, are optimized for dialogue use cases. Our models outperform open-source chat models on most benchmarks we tested, and based on our human evaluations for helpfulness and safety, may be a suitable substitute for closed-source models. We provide a detailed description of our approach to fine-tuning and safety improvements of Llama 2-Chat in order to enable the community to build on our work and contribute to the responsible development of LLMs.",
            "year": 2023,
            "venue": "arXiv.org",
            "authors": [
              {
                "authorId": "2113243762",
                "name": "Hugo Touvron"
              },
              {
                "authorId": "143792623",
                "name": "Louis Martin"
              },
              {
                "authorId": "2059203763",
                "name": "Kevin R. Stone"
              },
              {
                "authorId": "2214809450",
                "name": "Peter Albert"
              },
              {
                "authorId": "2634674",
                "name": "Amjad Almahairi"
              },
              {
                "authorId": "2223764353",
                "name": "Yasmine Babaei"
              },
              {
                "authorId": "2223756247",
                "name": "Niko-lay Bashlykov"
              },
              {
                "authorId": "47505161",
                "name": "Soumya Batra"
              },
              {
                "authorId": "51229603",
                "name": "Prajjwal Bhargava"
              },
              {
                "authorId": "2116473",
                "name": "Shruti Bhosale"
              },
              {
                "authorId": "2023469",
                "name": "D. Bikel"
              },
              {
                "authorId": "2040305955",
                "name": "Lukas Blecher"
              },
              {
                "authorId": "66286536",
                "name": "Cris-tian Cantón Ferrer"
              },
              {
                "authorId": "2108267192",
                "name": "Moya Chen"
              },
              {
                "authorId": "7153363",
                "name": "Guillem Cucurull"
              },
              {
                "authorId": "71039937",
                "name": "David Esiobu"
              },
              {
                "authorId": "2166312768",
                "name": "Jude Fernandes"
              },
              {
                "authorId": "2223974989",
                "name": "J. Fu"
              },
              {
                "authorId": "2223742000",
                "name": "Wenyin Fu"
              },
              {
                "authorId": "2223748737",
                "name": "Brian Fuller"
              },
              {
                "authorId": "2107063269",
                "name": "Cynthia Gao"
              },
              {
                "authorId": "28554843",
                "name": "Vedanuj Goswami"
              },
              {
                "authorId": "39589154",
                "name": "Naman Goyal"
              },
              {
                "authorId": "4305645",
                "name": "A. Hartshorn"
              },
              {
                "authorId": "2195458",
                "name": "Saghar Hosseini"
              },
              {
                "authorId": "2132302721",
                "name": "Rui Hou"
              },
              {
                "authorId": "2065277797",
                "name": "Hakan Inan"
              },
              {
                "authorId": "2059886128",
                "name": "Marcin Kardas"
              },
              {
                "authorId": "2190957318",
                "name": "Viktor Kerkez"
              },
              {
                "authorId": "2072010",
                "name": "Madian Khabsa"
              },
              {
                "authorId": "2207049",
                "name": "Isabel M. Kloumann"
              },
              {
                "authorId": "2294453195",
                "name": "A. Korenev"
              },
              {
                "authorId": "2146367061",
                "name": "Punit Singh Koura"
              },
              {
                "authorId": "114952298",
                "name": "M. Lachaux"
              },
              {
                "authorId": "46183616",
                "name": "Thibaut Lavril"
              },
              {
                "authorId": "2223749565",
                "name": "Jenya Lee"
              },
              {
                "authorId": "2145259939",
                "name": "Diana Liskovich"
              },
              {
                "authorId": "1768032",
                "name": "Yinghai Lu"
              },
              {
                "authorId": "3375249",
                "name": "Yuning Mao"
              },
              {
                "authorId": "1490887583",
                "name": "Xavier Martinet"
              },
              {
                "authorId": "39980906",
                "name": "Todor Mihaylov"
              },
              {
                "authorId": "3047561",
                "name": "Pushkar Mishra"
              },
              {
                "authorId": "2322981055",
                "name": "Igor Molybog"
              },
              {
                "authorId": "40383658",
                "name": "Yixin Nie"
              },
              {
                "authorId": "38579672",
                "name": "Andrew Poulton"
              },
              {
                "authorId": "39906022",
                "name": "J. Reizenstein"
              },
              {
                "authorId": "150282885",
                "name": "Rashi Rungta"
              },
              {
                "authorId": "1859294",
                "name": "Kalyan Saladi"
              },
              {
                "authorId": "14279694",
                "name": "Alan Schelten"
              },
              {
                "authorId": "2214818043",
                "name": "Ruan Silva"
              },
              {
                "authorId": "51324296",
                "name": "Eric Michael Smith"
              },
              {
                "authorId": "2066074360",
                "name": "R. Subramanian"
              },
              {
                "authorId": "2112782199",
                "name": "Xia Tan"
              },
              {
                "authorId": "71292072",
                "name": "Binh Tang"
              },
              {
                "authorId": "2110697298",
                "name": "Ross Taylor"
              },
              {
                "authorId": "2110032535",
                "name": "Adina Williams"
              },
              {
                "authorId": "2223770369",
                "name": "Jian Xiang Kuan"
              },
              {
                "authorId": "2214843767",
                "name": "Puxin Xu"
              },
              {
                "authorId": "14701107",
                "name": "Zhengxu Yan"
              },
              {
                "authorId": "121929334",
                "name": "Iliyan Zarov"
              },
              {
                "authorId": "2108473229",
                "name": "Yuchen Zhang"
              },
              {
                "authorId": "144270981",
                "name": "Angela Fan"
              },
              {
                "authorId": "2165660870",
                "name": "M. Kambadur"
              },
              {
                "authorId": "46617804",
                "name": "Sharan Narang"
              },
              {
                "authorId": "2166043087",
                "name": "Aur'elien Rodriguez"
              },
              {
                "authorId": "1962768",
                "name": "Robert Stojnic"
              },
              {
                "authorId": "2068070",
                "name": "Sergey Edunov"
              },
              {
                "authorId": "2073456043",
                "name": "Thomas Scialom"
              }
            ]
          }
        }
      ]
    },
    "280391721": {
      "citing_paper_info": {
        "title": "PATENTWRITER: A Benchmarking Study for Patent Drafting with LLMs",
        "abstract": "Large language models (LLMs) have emerged as transformative approaches in several important fields. This paper aims for a paradigm shift for patent writing by leveraging LLMs to overcome the tedious patent-filing process. In this work, we present PATENTWRITER, the first unified benchmarking framework for evaluating LLMs in patent abstract generation. Given the first claim of a patent, we evaluate six leading LLMs -- including GPT-4 and LLaMA-3 -- under a consistent setup spanning zero-shot, few-shot, and chain-of-thought prompting strategies to generate the abstract of the patent. Our benchmark PATENTWRITER goes beyond surface-level evaluation: we systematically assess the output quality using a comprehensive suite of metrics -- standard NLP measures (e.g., BLEU, ROUGE, BERTScore), robustness under three types of input perturbations, and applicability in two downstream patent classification and retrieval tasks. We also conduct stylistic analysis to assess length, readability, and tone. Experimental results show that modern LLMs can generate high-fidelity and stylistically appropriate patent abstracts, often surpassing domain-specific baselines. Our code and dataset are open-sourced to support reproducibility and future research.",
        "year": 2025,
        "venue": "",
        "authors": [
          {
            "authorId": "2374049151",
            "name": "Homaira Huda Shomee"
          },
          {
            "authorId": "2329735236",
            "name": "S. Maity"
          },
          {
            "authorId": "3390598",
            "name": "Sourav Medya"
          }
        ]
      },
      "citation_summary": {
        "citation_count": 25,
        "unique_cited_count": 17,
        "influential_count": 1,
        "detailed_records_count": 25
      },
      "cited_papers": [
        "3562169",
        "233583623",
        "252756523",
        "218971783",
        "10182711",
        "229678542",
        "125134679",
        "11080756",
        "202558505",
        "218604766",
        "224292244",
        "270845577",
        "211830249",
        "127986044",
        "265038146",
        "964287",
        "195791872"
      ],
      "citation_details": [
        {
          "citedcorpusid": 964287,
          "isinfluential": false,
          "contexts": [
            "(2) ROUGE-L : ROUGE-L (Lin, 2004) assesses the longest common subsequence (LCS) between the generated and the reference text."
          ],
          "intents": [
            "--"
          ],
          "cited_paper_info": {
            "title": "ROUGE: A Package for Automatic Evaluation of Summaries",
            "abstract": "",
            "year": 2004,
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "1781574",
                "name": "Chin-Yew Lin"
              }
            ]
          }
        },
        {
          "citedcorpusid": 3562169,
          "isinfluential": false,
          "contexts": [
            "Patent claims outline the specific features and characteristics that distinguish the invention from existing technologies (Mehta et al., 2017).",
            "At the heart of the patent process lies the task of patent writing which has been characterized by its meticulous and time-consuming nature (Roberts, 2007; Mehta et al., 2017; Trappey et al., 2020)."
          ],
          "intents": [
            "--",
            "--"
          ],
          "cited_paper_info": {
            "title": "Inventions and Patents: A Practical Tutorial.",
            "abstract": "",
            "year": 2017,
            "venue": "Methods in molecular biology",
            "authors": [
              {
                "authorId": "50167595",
                "name": "Hina Mehta"
              },
              {
                "authorId": "7776960",
                "name": "Lille Tidwell"
              },
              {
                "authorId": "10127406",
                "name": "L. Liotta"
              }
            ]
          }
        },
        {
          "citedcorpusid": 10182711,
          "isinfluential": false,
          "contexts": [
            "The patent Retrieval (PR) task focuses on effectively retrieving relevant patent documents given a specific search query (Shalaby and Zadrozny, 2019)."
          ],
          "intents": [
            "--"
          ],
          "cited_paper_info": {
            "title": "Patent retrieval: a literature review",
            "abstract": "With the ever increasing number of filed patent applications every year, the need for effective and efficient systems for managing such tremendous amounts of data becomes inevitably important. Patent retrieval (PR) is considered the pillar of almost all patent analysis tasks. PR is a subfield of information retrieval (IR) which is concerned with developing techniques and methods that effectively and efficiently retrieve relevant patent documents in response to a given search request. In this paper, we present a comprehensive review on PR methods and approaches. It is clear that recent successes and maturity in IR applications such as Web search cannot be transferred directly to PR without deliberate domain adaptation and customization. Furthermore, state-of-the-art performance in automatic PR is still around average in terms of recall. These observations motivate the need for interactive search tools which provide cognitive assistance to patent professionals with minimal effort. These tools must also be developed in hand with patent professionals considering their practices and expectations. We additionally touch on related tasks to PR such as patent valuation, litigation, licensing, and highlight potential opportunities and open directions for computational scientists in these domains.",
            "year": 2017,
            "venue": "Knowledge and Information Systems",
            "authors": [
              {
                "authorId": "2719971",
                "name": "W. Shalaby"
              },
              {
                "authorId": "2606846",
                "name": "Wlodek Zadrozny"
              }
            ]
          }
        },
        {
          "citedcorpusid": 11080756,
          "isinfluential": false,
          "contexts": [
            "(3) BLEU : It measures the overlap of n-grams between the reference and generated text (Papineni et al., 2002)."
          ],
          "intents": [
            "--"
          ],
          "cited_paper_info": {
            "title": "Bleu: a Method for Automatic Evaluation of Machine Translation",
            "abstract": "Human evaluations of machine translation are extensive but expensive. Human evaluations can take months to finish and involve human labor that can not be reused. We propose a method of automatic machine translation evaluation that is quick, inexpensive, and language-independent, that correlates highly with human evaluation, and that has little marginal cost per run. We present this method as an automated understudy to skilled human judges which substitutes for them when there is need for quick or frequent evaluations.",
            "year": 2002,
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "3323275",
                "name": "K. Papineni"
              },
              {
                "authorId": "46924970",
                "name": "Salim Roukos"
              },
              {
                "authorId": "144582029",
                "name": "T. Ward"
              },
              {
                "authorId": "2587983",
                "name": "Wei-Jing Zhu"
              }
            ]
          }
        },
        {
          "citedcorpusid": 125134679,
          "isinfluential": false,
          "contexts": [
            "(4) Cosine similarity measures the co-sine of the angle between two non-zero vectors in a multi-dimensional space (Gunawan et al., 2018)."
          ],
          "intents": [
            "--"
          ],
          "cited_paper_info": {
            "title": "The Implementation of Cosine Similarity to Calculate Text Relevance between Two Documents",
            "abstract": "",
            "year": 2018,
            "venue": "",
            "authors": [
              {
                "authorId": "66225150",
                "name": "D. Gunawan"
              },
              {
                "authorId": "2233482848",
                "name": "C A Sembiring"
              },
              {
                "authorId": "2074972640",
                "name": "M. A. Budiman"
              }
            ]
          }
        },
        {
          "citedcorpusid": 127986044,
          "isinfluential": true,
          "contexts": [
            "The measures, BERTScore, cosine similarities, ROUGE, and BLEU are shown in Table 2 for the sub-class A61, G06 and H04 and an example (Fig.",
            "Across all three CPC domains, models achieve high BERTScores ( ≥ 0.85) that show strong alignment with human-written abstracts.",
            "We build a comprehensive evaluation measures beyond standard NLP metrics (e.g., BLEU, ROUGE, BERTScore) and assess the practical domain utility of generated texts using two downstream tasks: patent classification and patent retrieval (Secs.",
            "The generated outputs are evaluated using three key dimensions: (1) 4 NLP metrics such as BERTScore, ROUGE, BLEU, and Cosine Similarity to measure surface-level and semantic similarity; (2) 2 domain-specific task performance like classification and retrieval accuracy; and (3) Robustness analysis, which measures the consistency of model outputs under 3 input perturbations.",
            "(1) BERTScore : BERTScore (Zhang et al., 2019) evaluates the semantic similarity between the generated text and reference (original) texts using the contextual embeddings."
          ],
          "intents": [
            "--",
            "--",
            "--",
            "--",
            "--"
          ],
          "cited_paper_info": {
            "title": "BERTScore: Evaluating Text Generation with BERT",
            "abstract": "We propose BERTScore, an automatic evaluation metric for text generation. Analogously to common metrics, BERTScore computes a similarity score for each token in the candidate sentence with each token in the reference sentence. However, instead of exact matches, we compute token similarity using contextual embeddings. We evaluate using the outputs of 363 machine translation and image captioning systems. BERTScore correlates better with human judgments and provides stronger model selection performance than existing metrics. Finally, we use an adversarial paraphrase detection task to show that BERTScore is more robust to challenging examples when compared to existing metrics.",
            "year": 2019,
            "venue": "International Conference on Learning Representations",
            "authors": [
              {
                "authorId": "123437034",
                "name": "Tianyi Zhang"
              },
              {
                "authorId": "145461044",
                "name": "Varsha Kishore"
              },
              {
                "authorId": "24277779",
                "name": "Felix Wu"
              },
              {
                "authorId": "7446832",
                "name": "Kilian Q. Weinberger"
              },
              {
                "authorId": "3167681",
                "name": "Yoav Artzi"
              }
            ]
          }
        },
        {
          "citedcorpusid": 195791872,
          "isinfluential": false,
          "contexts": [
            "With a huge promise, LLMs have also started to gain attention in the patent domain, especially in automating some aspects of the patent drafting process (Krestel et al., 2021; Lee, 2020a; Lee and Hsiang, 2020)."
          ],
          "intents": [
            "--"
          ],
          "cited_paper_info": {
            "title": "Patent Claim Generation by Fine-Tuning OpenAI GPT-2",
            "abstract": "",
            "year": 2019,
            "venue": "World Patent Information",
            "authors": [
              {
                "authorId": "2387987",
                "name": "Jieh-Sheng Lee"
              },
              {
                "authorId": "1798127",
                "name": "J. Hsiang"
              }
            ]
          }
        },
        {
          "citedcorpusid": 202558505,
          "isinfluential": false,
          "contexts": [
            "For instance, in healthcare, LLMs have been used for generating biomedical text (Peng et al., 2023), such as summarizing medical literature (Beltagy et al., 2019), generating clinical notes, and composing drug labels (Goel et al., 2023)."
          ],
          "intents": [
            "--"
          ],
          "cited_paper_info": {
            "title": "SciBERT: A Pretrained Language Model for Scientific Text",
            "abstract": "Obtaining large-scale annotated data for NLP tasks in the scientific domain is challenging and expensive. We release SciBERT, a pretrained language model based on BERT (Devlin et. al., 2018) to address the lack of high-quality, large-scale labeled scientific data. SciBERT leverages unsupervised pretraining on a large multi-domain corpus of scientific publications to improve performance on downstream scientific NLP tasks. We evaluate on a suite of tasks including sequence tagging, sentence classification and dependency parsing, with datasets from a variety of scientific domains. We demonstrate statistically significant improvements over BERT and achieve new state-of-the-art results on several of these tasks. The code and pretrained models are available at https://github.com/allenai/scibert/.",
            "year": 2019,
            "venue": "Conference on Empirical Methods in Natural Language Processing",
            "authors": [
              {
                "authorId": "46181066",
                "name": "Iz Beltagy"
              },
              {
                "authorId": "46258841",
                "name": "Kyle Lo"
              },
              {
                "authorId": "2527954",
                "name": "Arman Cohan"
              }
            ]
          }
        },
        {
          "citedcorpusid": 211830249,
          "isinfluential": false,
          "contexts": [
            "At the heart of the patent process lies the task of patent writing which has been characterized by its meticulous and time-consuming nature (Roberts, 2007; Mehta et al., 2017; Trappey et al., 2020)."
          ],
          "intents": [
            "--"
          ],
          "cited_paper_info": {
            "title": "Intelligent compilation of patent summaries using machine learning and natural language processing techniques",
            "abstract": "",
            "year": 2020,
            "venue": "Advanced Engineering Informatics",
            "authors": [
              {
                "authorId": "1761458",
                "name": "A. Trappey"
              },
              {
                "authorId": "1766308",
                "name": "C. Trappey"
              },
              {
                "authorId": "2719770",
                "name": "Jheng-Long Wu"
              },
              {
                "authorId": "2211354325",
                "name": "Jack W. C. Wang"
              }
            ]
          }
        },
        {
          "citedcorpusid": 218604766,
          "isinfluential": false,
          "contexts": [
            "An early study in this domain is the PatentTransformer(Lee, 2020b), which employs a GPT-2-based architecture trained on patent data to generate patent segments."
          ],
          "intents": [
            "--"
          ],
          "cited_paper_info": {
            "title": "PatentTransformer: A Framework for Personalized Patent Claim Generation",
            "abstract": ". This paper proposes the PatentTransformer framework to generate and measure personalized patent claims. The objective is to help inventors conceive better inventions by learning from relevant inventors. Patent claim generation is a way of “augmented inventing.” for inventors. Such patent claim generation leverages the recent transfer learning in the Deep Learning field, particularly the state-of-the-art Transformer-based models. In terms of system implementation, it is planned to build an \"auto-complete\" function for patent claim drafting. The auto-complete function is analyzed from four different perspectives: extent of generation, generative direction, proximity of generation, and constraint in generation. Technically, the PatentTransformer framework is composed of two Transformer models. One is for text generation and the other is for quality measurement. Specifically, the patent claim generation is based on GPT-2 model and the measurement of personalization is based on BERT model. The training data is inventor-centric and comes from the Inventors Endpoint API provided by the USPTO.",
            "year": 2019,
            "venue": "International Conference on Legal Knowledge and Information Systems",
            "authors": [
              {
                "authorId": "2387987",
                "name": "Jieh-Sheng Lee"
              }
            ]
          }
        },
        {
          "citedcorpusid": 218971783,
          "isinfluential": false,
          "contexts": [
            "Prompting serves as a fundamental and extensively adopted paradigm for directing the behavior of large language models (LLMs) (Brown et al., 2020; Liu et al., 2023)."
          ],
          "intents": [
            "--"
          ],
          "cited_paper_info": {
            "title": "Language Models are Few-Shot Learners",
            "abstract": "Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions - something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally, we find that GPT-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of GPT-3 in general.",
            "year": 2020,
            "venue": "Neural Information Processing Systems",
            "authors": [
              {
                "authorId": "31035595",
                "name": "Tom B. Brown"
              },
              {
                "authorId": "2056658938",
                "name": "Benjamin Mann"
              },
              {
                "authorId": "39849748",
                "name": "Nick Ryder"
              },
              {
                "authorId": "2065894334",
                "name": "Melanie Subbiah"
              },
              {
                "authorId": "152724169",
                "name": "J. Kaplan"
              },
              {
                "authorId": "6515819",
                "name": "Prafulla Dhariwal"
              },
              {
                "authorId": "2072676",
                "name": "Arvind Neelakantan"
              },
              {
                "authorId": "67311962",
                "name": "Pranav Shyam"
              },
              {
                "authorId": "144864359",
                "name": "Girish Sastry"
              },
              {
                "authorId": "119609682",
                "name": "Amanda Askell"
              },
              {
                "authorId": "144517868",
                "name": "Sandhini Agarwal"
              },
              {
                "authorId": "1404060687",
                "name": "Ariel Herbert-Voss"
              },
              {
                "authorId": "2064404342",
                "name": "Gretchen Krueger"
              },
              {
                "authorId": "103143311",
                "name": "T. Henighan"
              },
              {
                "authorId": "48422824",
                "name": "R. Child"
              },
              {
                "authorId": "1992922591",
                "name": "A. Ramesh"
              },
              {
                "authorId": "2052152920",
                "name": "Daniel M. Ziegler"
              },
              {
                "authorId": "49387725",
                "name": "Jeff Wu"
              },
              {
                "authorId": "2059411355",
                "name": "Clemens Winter"
              },
              {
                "authorId": "144239765",
                "name": "Christopher Hesse"
              },
              {
                "authorId": "2108828435",
                "name": "Mark Chen"
              },
              {
                "authorId": "2064673055",
                "name": "Eric Sigler"
              },
              {
                "authorId": "1380985420",
                "name": "Ma-teusz Litwin"
              },
              {
                "authorId": "145565184",
                "name": "Scott Gray"
              },
              {
                "authorId": "1490681878",
                "name": "Benjamin Chess"
              },
              {
                "authorId": "2115193883",
                "name": "Jack Clark"
              },
              {
                "authorId": "133740015",
                "name": "Christopher Berner"
              },
              {
                "authorId": "52238703",
                "name": "Sam McCandlish"
              },
              {
                "authorId": "38909097",
                "name": "Alec Radford"
              },
              {
                "authorId": "1701686",
                "name": "I. Sutskever"
              },
              {
                "authorId": "2698777",
                "name": "Dario Amodei"
              }
            ]
          }
        },
        {
          "citedcorpusid": 224292244,
          "isinfluential": false,
          "contexts": [
            "With a huge promise, LLMs have also started to gain attention in the patent domain, especially in automating some aspects of the patent drafting process (Krestel et al., 2021; Lee, 2020a; Lee and Hsiang, 2020)."
          ],
          "intents": [
            "--"
          ],
          "cited_paper_info": {
            "title": "Controlling Patent Text Generation by Structural Metadata",
            "abstract": "The ultimate goal of my long-term project is \"Augmented Inventing.\" This work is a follow-up effort toward the goal. It leverages the structural metadata in patent documents and the text-to-text mappings between metadata. The structural metadata includes patent title, abstract, independent claim, and dependent claim. By using the structural metadata, it is possible to control what kind of patent text to generate. By using the text-to-text mapping, it is possible to let a generative model generate one type of patent text from another type of patent text. Furthermore, through multiple mappings, it is possible to build a text generation flow, for example, generating from a few words to a patent title, from the title to an abstract, from the abstract to an independent claim, and from the independent claim to multiple dependent claims. The text generation flow can also go backward after training with bi-directional mappings. In addition to those above, the contributions of this work include: (1) released four generative models trained with patent corpus from scratch, (2) released the sample code to demonstrate how to generate patent text bi-directionally, (3) measuring the performances of the models by ROGUE and Universal Sentence Encoder as preliminary evaluations of text generation quality.",
            "year": 2020,
            "venue": "International Conference on Information and Knowledge Management",
            "authors": [
              {
                "authorId": "2387987",
                "name": "Jieh-Sheng Lee"
              }
            ]
          }
        },
        {
          "citedcorpusid": 229678542,
          "isinfluential": false,
          "contexts": [
            "This often requires extensive legal knowledge, technical expertise, and linguistic precision (Risch et al., 2021).",
            "The generation process aims to accurately describe the invention where patent documents require the use of precise and technical language (Risch et al., 2021)."
          ],
          "intents": [
            "--",
            "--"
          ],
          "cited_paper_info": {
            "title": "PatentMatch: A Dataset for Matching Patent Claims & Prior Art",
            "abstract": "Patent examiners need to solve a complex information retrieval task when they assess the novelty and inventive step of claims made in a patent application. Given a claim, they search for prior art, which comprises all relevant publicly available information. This time-consuming task requires a deep understanding of the respective technical domain and the patent-domain-specific language. For these reasons, we address the computer-assisted search for prior art by creating a training dataset for supervised machine learning called PatentMatch. It contains pairs of claims from patent applications and semantically corresponding text passages of different degrees from cited patent documents. Each pair has been labeled by technically-skilled patent examiners from the European Patent Office. Accordingly, the label indicates the degree of semantic correspondence (matching), i.e., whether the text passage is prejudicial to the novelty of the claimed invention or not. Preliminary experiments using a baseline system show that PatentMatch can indeed be used for training a binary text pair classifier on this challenging information retrieval task. The dataset is available online: https://hpi.de/naumann/s/patentmatch.",
            "year": 2020,
            "venue": "arXiv.org",
            "authors": [
              {
                "authorId": "1695993",
                "name": "Julian Risch"
              },
              {
                "authorId": "2090084616",
                "name": "Nicolas Alder"
              },
              {
                "authorId": "2042640458",
                "name": "Christoph Hewel"
              },
              {
                "authorId": "3264110",
                "name": "Ralf Krestel"
              }
            ]
          }
        },
        {
          "citedcorpusid": 233583623,
          "isinfluential": false,
          "contexts": [
            "With a huge promise, LLMs have also started to gain attention in the patent domain, especially in automating some aspects of the patent drafting process (Krestel et al., 2021; Lee, 2020a; Lee and Hsiang, 2020).",
            "Patent classification is an important and time-consuming task in the patent life cycle (Krestel et al., 2021)."
          ],
          "intents": [
            "--",
            "--"
          ],
          "cited_paper_info": {
            "title": "A survey on deep learning for patent analysis",
            "abstract": "",
            "year": 2021,
            "venue": "World Patent Information",
            "authors": [
              {
                "authorId": "3264110",
                "name": "Ralf Krestel"
              },
              {
                "authorId": "2047942384",
                "name": "Renukswamy Chikkamath"
              },
              {
                "authorId": "2042640458",
                "name": "Christoph Hewel"
              },
              {
                "authorId": "1695993",
                "name": "Julian Risch"
              }
            ]
          }
        },
        {
          "citedcorpusid": 252756523,
          "isinfluential": false,
          "contexts": [
            "(Christofidellis et al., 2022) introduce the Patent Generative Transformer (PGT), a transformer-based multitask language model designed to streamline the patent generation process through tasks such as part-of-patent generation."
          ],
          "intents": [
            "--"
          ],
          "cited_paper_info": {
            "title": "PGT: a prompt based generative transformer for the patent domain",
            "abstract": "Patents are a valuable source of knowledge, but drafting them is a time-consuming and expensive task. Methods that assist patent generation can provide a two-fold improvement as they can speed up the generation process and suggest to the inventor ideas and claims. Herein, influenced by recent advances in language modeling via multitask learning and prompt engineering, we present Patent Generative Transformer (PGT), a transformer-based language model trained to facilitate patent drafting. Specifically, the model supports three tasks: part-of-patent generation, text infilling, and patent coherence evaluation. PGT complements inventors and assures the fast and successful transition from their input to a coherent patent disclosure taking advantage of its multitasking nature. We show how the model out-performs a collection of task-specific baselines on relevant metrics. We further test the quality of the generated text via blind testing by subject matter experts. Finally, we explore a zero-shot extension of the model showing how to use PGT for generating domain-specific abstracts.",
            "year": 2022,
            "venue": "",
            "authors": [
              {
                "authorId": "2039675061",
                "name": "Dimitrios Christofidellis"
              },
              {
                "authorId": "2187177717",
                "name": "Antonio Berrios Torres"
              },
              {
                "authorId": "113880332",
                "name": "A. Dave"
              },
              {
                "authorId": "1744102",
                "name": "M. Roveri"
              },
              {
                "authorId": "2187167593",
                "name": "Kristin Schmidt"
              },
              {
                "authorId": "51149179",
                "name": "Sarath Swaminathan"
              },
              {
                "authorId": "144114375",
                "name": "Hans Vandierendonck"
              },
              {
                "authorId": "35312152",
                "name": "D. Zubarev"
              },
              {
                "authorId": "35904689",
                "name": "Matteo Manica"
              }
            ]
          }
        },
        {
          "citedcorpusid": 265038146,
          "isinfluential": false,
          "contexts": [
            "In finance and economics, LLMs have been deployed for generating financial reports and economic fore-casts (Liu et al., 2021; Yang et al., 2023), as well as for supporting financial decision-making tasks such as trading, portfolio management, and risk assessment (Yu et al., 2024)."
          ],
          "intents": [
            "--"
          ],
          "cited_paper_info": {
            "title": "FinBERT: A Pre-trained Financial Language Representation Model for Financial Text Mining",
            "abstract": "There is growing interest in the tasks of financial text mining. Over the past few years, the progress of Natural Language Processing (NLP) based on deep learning advanced rapidly. Significant progress has been made with deep learning showing promising results on financial text mining models. However, as NLP models require large amounts of labeled training data, applying deep learning to financial text mining is often unsuccessful due to the lack of labeled training data in financial fields. To address this issue, we present FinBERT (BERT for Financial Text Mining) that is a domain specific language model pre-trained on large-scale financial corpora. In FinBERT, different from BERT, we construct six pre-training tasks covering more knowledge, simultaneously trained on general corpora and financial domain corpora, which can enable FinBERT model better to capture language knowledge and semantic information. The results show that our FinBERT outperforms all current state-of-the-art models. Extensive experimental results demonstrate the effectiveness and robustness of FinBERT. The source code and pre-trained models of FinBERT are available online.",
            "year": 2020,
            "venue": "International Joint Conference on Artificial Intelligence",
            "authors": [
              {
                "authorId": "46271578",
                "name": "Zhuang Liu"
              },
              {
                "authorId": "2269766783",
                "name": "Degen Huang"
              },
              {
                "authorId": "2112768206",
                "name": "Kaiyu Huang"
              },
              {
                "authorId": "2290143097",
                "name": "Zhuang Li"
              },
              {
                "authorId": "2145804874",
                "name": "Jun Zhao"
              }
            ]
          }
        },
        {
          "citedcorpusid": 270845577,
          "isinfluential": false,
          "contexts": [
            "( Jiang et al., 2025) evaluate various LLMs for patent claim generation and find that generating claims from detailed patent descriptions yields better results than using abstracts."
          ],
          "intents": [
            "--"
          ],
          "cited_paper_info": {
            "title": "Can Large Language Models Generate High-quality Patent Claims?",
            "abstract": "Large language models (LLMs) have shown exceptional performance across various text generation tasks but remain under-explored in the patent domain, which offers highly structured and precise language. This paper constructs a dataset to investigate the performance of current LLMs in patent claim generation. Our results demonstrate that generating claims based on patent descriptions outperforms previous research relying on abstracts. Interestingly, current patent-specific LLMs perform much worse than state-of-the-art general LLMs, highlighting the necessity for future research on in-domain LLMs. We also find that LLMs can produce high-quality first independent claims, but their performances markedly decrease for subsequent dependent claims. Moreover, fine-tuning can enhance the completeness of inventions' features, conceptual clarity, and feature linkage. Among the tested LLMs, GPT-4 demonstrates the best performance in comprehensive human evaluations by patent experts, with better feature coverage, conceptual clarity, and technical coherence. Despite these capabilities, comprehensive revision and modification are still necessary to pass rigorous patent scrutiny and ensure legal robustness.",
            "year": 2024,
            "venue": "North American Chapter of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "2148649304",
                "name": "Lekang Jiang"
              },
              {
                "authorId": "2294313959",
                "name": "Caiqi Zhang"
              },
              {
                "authorId": "2309007437",
                "name": "Pascal A Scherz"
              },
              {
                "authorId": "2290185595",
                "name": "Stephan Goetz"
              }
            ]
          }
        },
        {
          "citedcorpusid": null,
          "isinfluential": false,
          "contexts": [
            "A patent typically contains a large volume of content and requires significant human efforts (Roberts, 2007).",
            "At the heart of the patent process lies the task of patent writing which has been characterized by its meticulous and time-consuming nature (Roberts, 2007; Mehta et al., 2017; Trappey et al., 2020)."
          ],
          "intents": [
            "--",
            "--"
          ],
          "cited_paper_info": {}
        },
        {
          "citedcorpusid": null,
          "isinfluential": false,
          "contexts": [
            "It is typically used by patent examiners, potential licensees, investors, and competitors to quickly grasp the essence of the invention without delving into the detailed description (WIPO, 1994)."
          ],
          "intents": [
            "--"
          ],
          "cited_paper_info": {}
        },
        {
          "citedcorpusid": null,
          "isinfluential": false,
          "contexts": [
            "PatentGPT (Ren and Ma, 2024) introduces cost-efficient large language models trained on 240B IP-related tokens to support tasks like patent drafting and translation."
          ],
          "intents": [
            "--"
          ],
          "cited_paper_info": {}
        },
        {
          "citedcorpusid": null,
          "isinfluential": false,
          "contexts": [
            "In finance and economics, LLMs have been deployed for generating financial reports and economic fore-casts (Liu et al., 2021; Yang et al., 2023), as well as for supporting financial decision-making tasks such as trading, portfolio management, and risk assessment (Yu et al., 2024)."
          ],
          "intents": [
            "--"
          ],
          "cited_paper_info": {}
        },
        {
          "citedcorpusid": null,
          "isinfluential": false,
          "contexts": [
            "We apply both character-level and word-level perturbation using the nlpaug Python library (Ma, 2019)."
          ],
          "intents": [
            "--"
          ],
          "cited_paper_info": {}
        },
        {
          "citedcorpusid": null,
          "isinfluential": false,
          "contexts": [
            "In finance and economics, LLMs have been deployed for generating financial reports and economic fore-casts (Liu et al., 2021; Yang et al., 2023), as well as for supporting financial decision-making tasks such as trading, portfolio management, and risk assessment (Yu et al., 2024).",
            "Large language models (LLMs) are effective AI assistants that can handle complex reasoning tasks that require expert knowledge in various fields (Yang et al., 2023; Peng et al., 2023)."
          ],
          "intents": [
            "--",
            "--"
          ],
          "cited_paper_info": {}
        },
        {
          "citedcorpusid": null,
          "isinfluential": false,
          "contexts": [
            "In social media, LLMs have been used for hate speech detection (Guo et al., 2024) and misinformation mitigation (Chen and Shu, 2024)."
          ],
          "intents": [
            "--"
          ],
          "cited_paper_info": {}
        },
        {
          "citedcorpusid": null,
          "isinfluential": false,
          "contexts": [
            "In social media, LLMs have been used for hate speech detection (Guo et al., 2024) and misinformation mitigation (Chen and Shu, 2024)."
          ],
          "intents": [
            "--"
          ],
          "cited_paper_info": {}
        }
      ]
    },
    "277693490": {
      "citing_paper_info": {
        "title": "An AI-Driven Methodology for Patent Evaluation in the IoT Sector: Assessing Relevance and Future Impact",
        "abstract": ": The rapid expansion of the Internet of Things has led to a surge in patent filings, creating challenges in evaluating their relevance and potential impact. Traditional patent assessment methods, relying on manual review and keyword-based searches, are increasingly inadequate for analyzing the complexity of emerging IoT technologies. In this paper, we propose an AI-driven methodology for patent evaluation that leverages Large Language Models and machine learning techniques to assess patent relevance and estimate future impact. Our framework integrates advanced Natural Language Processing techniques with structured patent metadata to establish a systematic approach to patent analysis. The methodology consists of three key components: (1) feature extraction from patent text using LLM embeddings and conventional NLP methods, (2) relevance classification and clustering to identify emerging technological trends, and (3) an initial formulation of impact estimation based on semantic similarity and citation patterns. While this study focuses primarily on defining the methodology, we include a minimal validation on a sample dataset to illustrate its feasibility and potential. The proposed approach lays the groundwork for a scalable, automated patent evaluation system, with future research directions aimed at refining impact prediction models and expanding empirical validation.",
        "year": 2025,
        "venue": "International Conference on Internet of Things, Big Data and Security",
        "authors": [
          {
            "authorId": "73774776",
            "name": "L. Campanile"
          },
          {
            "authorId": "115958036",
            "name": "R. Zona"
          },
          {
            "authorId": "2354772261",
            "name": "Antonio Perfetti"
          },
          {
            "authorId": "2354807399",
            "name": "Franco Rosatelli"
          }
        ]
      },
      "citation_summary": {
        "citation_count": 8,
        "unique_cited_count": 7,
        "influential_count": 0,
        "detailed_records_count": 8
      },
      "cited_papers": [
        "249642071",
        "17887406",
        "13756489",
        "263648935",
        "49313245",
        "240420063",
        "259360395"
      ],
      "citation_details": [
        {
          "citedcorpusid": 13756489,
          "isinfluential": false,
          "contexts": [
            "Previous studies have employed word embeddings such as Word2Vec (Mikolov et al., 2013) and DL models such as LSTM, but transformer-based models have opened up new possibilities (Vaswani et al., 2017)."
          ],
          "intents": [
            "--"
          ],
          "cited_paper_info": {
            "title": "Attention is All you Need",
            "abstract": "The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.",
            "year": 2017,
            "venue": "Neural Information Processing Systems",
            "authors": [
              {
                "authorId": "40348417",
                "name": "Ashish Vaswani"
              },
              {
                "authorId": "1846258",
                "name": "Noam M. Shazeer"
              },
              {
                "authorId": "3877127",
                "name": "Niki Parmar"
              },
              {
                "authorId": "39328010",
                "name": "Jakob Uszkoreit"
              },
              {
                "authorId": "145024664",
                "name": "Llion Jones"
              },
              {
                "authorId": "19177000",
                "name": "Aidan N. Gomez"
              },
              {
                "authorId": "40527594",
                "name": "Lukasz Kaiser"
              },
              {
                "authorId": "3443442",
                "name": "I. Polosukhin"
              }
            ]
          }
        },
        {
          "citedcorpusid": 17887406,
          "isinfluential": false,
          "contexts": [
            "Previous research has examined the early stages of the use of intelligent, automated meth-ods for patent analysis (Abbas et al., 2014), the introduction of Deep Learning (DL) techniques, which have simplified some patent-related tasks (Krestel et al., 2021), and specific applications, such as…"
          ],
          "intents": [
            "--"
          ],
          "cited_paper_info": {
            "title": "A literature review on the state-of-the-art in patent analysis",
            "abstract": "",
            "year": 2014,
            "venue": "",
            "authors": [
              {
                "authorId": "2348239906",
                "name": "Assad Abbas"
              },
              {
                "authorId": "2144165037",
                "name": "Limin Zhang"
              },
              {
                "authorId": "1740261",
                "name": "S. Khan"
              }
            ]
          }
        },
        {
          "citedcorpusid": 49313245,
          "isinfluential": false,
          "contexts": [
            "Early attempts to apply LLMs to patents were based on relatively small models, such as GPT-2 (Radford, 2018), while more advanced models have not yet been explored in depth in this area."
          ],
          "intents": [
            "--"
          ],
          "cited_paper_info": {
            "title": "Improving Language Understanding by Generative Pre-Training",
            "abstract": "Natural language understanding comprises a wide range of diverse tasks such as textual entailment, question answering, semantic similarity assessment, and document classiﬁcation. Although large unlabeled text corpora are abundant, labeled data for learning these speciﬁc tasks is scarce, making it challenging for discriminatively trained models to perform adequately. We demonstrate that large gains on these tasks can be realized by generative pre-training of a language model on a diverse corpus of unlabeled text, followed by discriminative ﬁne-tuning on each speciﬁc task. In contrast to previous approaches, we make use of task-aware input transformations during ﬁne-tuning to achieve effective transfer while requiring minimal changes to the model architecture. We demonstrate the effectiveness of our approach on a wide range of benchmarks for natural language understanding. Our general task-agnostic model outperforms discriminatively trained models that use architectures speciﬁcally crafted for each task, signiﬁcantly improving upon the state of the art in 9 out of the 12 tasks studied. For instance, we achieve absolute improvements of 8.9% on commonsense reasoning (Stories Cloze Test), 5.7% on question answering (RACE), and 1.5% on textual entailment (MultiNLI).",
            "year": 2018,
            "venue": "",
            "authors": [
              {
                "authorId": "38909097",
                "name": "Alec Radford"
              },
              {
                "authorId": "144958935",
                "name": "Karthik Narasimhan"
              }
            ]
          }
        },
        {
          "citedcorpusid": 240420063,
          "isinfluential": false,
          "contexts": [
            "In addition, recent LLMs have demonstrated extraordinary capabilities across a wide range of tasks in general domains (Min et al., 2023), suggesting their potential use in the management and editing of patent literature, an essential resource for documenting technological evolution."
          ],
          "intents": [
            "--"
          ],
          "cited_paper_info": {
            "title": "Recent Advances in Natural Language Processing via Large Pre-trained Language Models: A Survey",
            "abstract": "Large, pre-trained language models (PLMs) such as BERT and GPT have drastically changed the Natural Language Processing (NLP) field. For numerous NLP tasks, approaches leveraging PLMs have achieved state-of-the-art performance. The key idea is to learn a generic, latent representation of language from a generic task once, then share it across disparate NLP tasks. Language modeling serves as the generic task, one with abundant self-supervised text available for extensive training. This article presents the key fundamental concepts of PLM architectures and a comprehensive view of the shift to PLM-driven NLP techniques. It surveys work applying the pre-training then fine-tuning, prompting, and text generation approaches. In addition, it discusses PLM limitations and suggested directions for future research.",
            "year": 2021,
            "venue": "ACM Computing Surveys",
            "authors": [
              {
                "authorId": "1875233",
                "name": "Bonan Min"
              },
              {
                "authorId": "2136457937",
                "name": "Hayley Ross"
              },
              {
                "authorId": "46185356",
                "name": "Elior Sulem"
              },
              {
                "authorId": "3460489",
                "name": "Amir Pouran Ben Veyseh"
              },
              {
                "authorId": "1811211",
                "name": "Thien Huu Nguyen"
              },
              {
                "authorId": "1724648481",
                "name": "Oscar Sainz"
              },
              {
                "authorId": "1733049",
                "name": "Eneko Agirre"
              },
              {
                "authorId": "2136480655",
                "name": "Ilana Heinz"
              },
              {
                "authorId": "144590225",
                "name": "D. Roth"
              }
            ]
          }
        },
        {
          "citedcorpusid": 249642071,
          "isinfluential": false,
          "contexts": [
            "Data science techniques must address these difficulties in order to effectively extract useful information for design and innovation (Jiang et al., 2022)."
          ],
          "intents": [
            "--"
          ],
          "cited_paper_info": {
            "title": "Patent Data for Engineering Design: A Critical Review and Future Directions",
            "abstract": "\n Patent data have been utilized for engineering design research for long because of its large and expanding size, wide variety, and massive amount of design information contained in patents. Recent advances in artificial intelligence and data science present unprecedented opportunities to develop data-driven design methods and tools and to enable advanced understanding of design science based on the patent database. Herein, we survey the patent-for-design literature categorized by their contributions to design research and practice, including design theories, methods, tools, and strategies, as well as the forms of patent data and the data science methods in respective studies. Based on systematic review and analysis, our review sheds light on promising future research directions for the field.",
            "year": 2021,
            "venue": "Journal of Computing and Information Science in Engineering",
            "authors": [
              {
                "authorId": "2142580538",
                "name": "Shuo Jiang"
              },
              {
                "authorId": "144925091",
                "name": "Serhad Sarica"
              },
              {
                "authorId": "31621699",
                "name": "Binyang Song"
              },
              {
                "authorId": "2143910587",
                "name": "Jie Hu"
              },
              {
                "authorId": "145990580",
                "name": "Jianxi Luo"
              }
            ]
          }
        },
        {
          "citedcorpusid": 259360395,
          "isinfluential": false,
          "contexts": [
            "The recent development in the fields of LLM are reported in (Chang et al., 2024)."
          ],
          "intents": [
            "--"
          ],
          "cited_paper_info": {
            "title": "A Survey on Evaluation of Large Language Models",
            "abstract": "Large language models (LLMs) are gaining increasing popularity in both academia and industry, owing to their unprecedented performance in various applications. As LLMs continue to play a vital role in both research and daily use, their evaluation becomes increasingly critical, not only at the task level, but also at the society level for better understanding of their potential risks. Over the past years, significant efforts have been made to examine LLMs from various perspectives. This paper presents a comprehensive review of these evaluation methods for LLMs, focusing on three key dimensions: what to evaluate, where to evaluate, and how to evaluate. Firstly, we provide an overview from the perspective of evaluation tasks, encompassing general natural language processing tasks, reasoning, medical usage, ethics, education, natural and social sciences, agent applications, and other areas. Secondly, we answer the ‘where’ and ‘how’ questions by diving into the evaluation methods and benchmarks, which serve as crucial components in assessing the performance of LLMs. Then, we summarize the success and failure cases of LLMs in different tasks. Finally, we shed light on several future challenges that lie ahead in LLMs evaluation. Our aim is to offer invaluable insights to researchers in the realm of LLMs evaluation, thereby aiding the development of more proficient LLMs. Our key point is that evaluation should be treated as an essential discipline to better assist the development of LLMs. We consistently maintain the related open-source materials at: https://github.com/MLGroupJLU/LLM-eval-survey",
            "year": 2023,
            "venue": "ACM Transactions on Intelligent Systems and Technology",
            "authors": [
              {
                "authorId": "2140050490",
                "name": "Yu-Chu Chang"
              },
              {
                "authorId": "2206577797",
                "name": "Xu Wang"
              },
              {
                "authorId": "1519290245",
                "name": "Jindong Wang"
              },
              {
                "authorId": "48608007",
                "name": "Yuan Wu"
              },
              {
                "authorId": "2543684",
                "name": "Kaijie Zhu"
              },
              {
                "authorId": "2051536212",
                "name": "Hao Chen"
              },
              {
                "authorId": "2145500840",
                "name": "Linyi Yang"
              },
              {
                "authorId": "3393196",
                "name": "Xiaoyuan Yi"
              },
              {
                "authorId": "35504092",
                "name": "Cunxiang Wang"
              },
              {
                "authorId": "2108024279",
                "name": "Yidong Wang"
              },
              {
                "authorId": "2147205193",
                "name": "Weirong Ye"
              },
              {
                "authorId": "2211964951",
                "name": "Yue Zhang"
              },
              {
                "authorId": "2131636065",
                "name": "Yi Chang"
              },
              {
                "authorId": "2191036692",
                "name": "Philip S. Yu"
              },
              {
                "authorId": "2158406244",
                "name": "Qian Yang"
              },
              {
                "authorId": "1576441343",
                "name": "Xingxu Xie"
              }
            ]
          }
        },
        {
          "citedcorpusid": 263648935,
          "isinfluential": false,
          "contexts": [
            "In particular, in the early stages of technology development, NLP can support idea generation, prediction of industry trends and the matching of problems and solutions (Just, 2024)."
          ],
          "intents": [
            "--"
          ],
          "cited_paper_info": {
            "title": "Natural language processing for innovation search – Reviewing an emerging non-human innovation intermediary",
            "abstract": "",
            "year": 2024,
            "venue": "Technovation",
            "authors": [
              {
                "authorId": "2223034633",
                "name": "Julian Just"
              }
            ]
          }
        },
        {
          "citedcorpusid": null,
          "isinfluential": false,
          "contexts": [
            "Historically, the patent system was established to encourage and regulate technical progress and innovation (Frumkin, 1947)."
          ],
          "intents": [
            "--"
          ],
          "cited_paper_info": {}
        }
      ]
    },
    "278911062": {
      "citing_paper_info": {
        "title": "PEDANTIC: A Dataset for the Automatic Examination of Definiteness in Patent Claims",
        "abstract": "Patent claims define the scope of protection for an invention. If there are ambiguities in a claim, it is rejected by the patent office. In the US, this is referred to as indefiniteness (35 U.S.C {\\S} 112(b)) and is among the most frequent reasons for patent application rejection. The development of automatic methods for patent definiteness examination has the potential to make patent drafting and examination more efficient, but no annotated dataset has been published to date. We introduce PEDANTIC (Patent Definiteness Examination Corpus), a novel dataset of 14k US patent claims from patent applications relating to Natural Language Processing (NLP), annotated with reasons for indefiniteness. We construct PEDANTIC using a fully automatic pipeline that retrieves office action documents from the USPTO and uses Large Language Models (LLMs) to extract the reasons for indefiniteness. A human validation study confirms the pipeline's accuracy in generating high-quality annotations. To gain insight beyond binary classification metrics, we implement an LLM-as-Judge evaluation that compares the free-form reasoning of every model-cited reason with every examiner-cited reason. We show that LLM agents based on Qwen 2.5 32B and 72B struggle to outperform logistic regression baselines on definiteness prediction, even though they often correctly identify the underlying reasons. PEDANTIC provides a valuable resource for patent AI researchers, enabling the development of advanced examination models. We will publicly release the dataset and code.",
        "year": 2025,
        "venue": "arXiv.org",
        "authors": [
          {
            "authorId": "2265381943",
            "name": "Valentin Knappich"
          },
          {
            "authorId": "2265381978",
            "name": "Annemarie Friedrich"
          },
          {
            "authorId": "2325098821",
            "name": "Anna Hatty"
          },
          {
            "authorId": "2066327465",
            "name": "S. Razniewski"
          }
        ]
      },
      "citation_summary": {
        "citation_count": 7,
        "unique_cited_count": 7,
        "influential_count": 1,
        "detailed_records_count": 7
      },
      "cited_papers": [
        "271244753",
        "259224389",
        "264172366",
        "270698898",
        "271213206",
        "259129398",
        "222093274"
      ],
      "citation_details": [
        {
          "citedcorpusid": 222093274,
          "isinfluential": false,
          "contexts": [
            "Kong et al. [20] model patent readability using § 112(a) (lack of disclosure) with linguistic features, finding university-issued patents more readable than corporate-issued ones."
          ],
          "intents": [
            "--"
          ],
          "cited_paper_info": {
            "title": "Linguistic Metrics for Patent Disclosure: Evidence from University Versus Corporate Patents",
            "abstract": "Encouraging inventors to disclose new inventions is an important economic justification for the patent system, yet the technical information contained in patent applications is often inadequate and unclear. This paper proposes a novel approach to measure disclosure in patent applications using algorithms from computation allinguistics. Borrowing methods from the literature on second language acquisition, we analyze core linguistic features of 40,949 U.S. applications in three patent categories related to nanotechnology, batteries, and electricity from 2000 to 2019. Relying on the expectation that universities have more incentives to disclose their inventions than corporations for either incentive reasons or for different source documents that patent attorneys can draw on, we confirm the relevance and usefulness of the linguistic measures by showing that university patents are more readable. Combining the multiple measures using principal component analysis, we find that the gap in disclosure is 0.4 SD, with a wider gap between top applicants. Our results do not change after accounting for the heterogeneity of inventions by controlling for cited-patent fixed effects. We also explore whether one pathway by which corporate patents become less readable is use of multiple examples to mask the “best mode” of inventions. By confirming that computational linguistic measures are useful indicators of readability of patents, we suggest that the disclosure function of patents can be explored empirically in a way that has not previously been feasible.",
            "year": 2020,
            "venue": "Social Science Research Network",
            "authors": [
              {
                "authorId": "118387089",
                "name": "N. Kong"
              },
              {
                "authorId": "1873338",
                "name": "U. Dulleck"
              },
              {
                "authorId": "1978105987",
                "name": "Shupeng Sun"
              },
              {
                "authorId": "2070714",
                "name": "Sowmya Vajjala"
              },
              {
                "authorId": "5743109",
                "name": "A. Jaffe"
              }
            ]
          }
        },
        {
          "citedcorpusid": 259129398,
          "isinfluential": true,
          "contexts": [
            "In addition to established classification metrics, we implement a reference-based LLM-as-Judge [8] evaluation that compares each model-cited rejection reason with every examiner-cited rejection reason.",
            "To that end, we implement an LLM-as-Judge [8] approach that determines whether an examiner-cited reason and a model-cited reason point to the same essential issue in the claim.",
            "To assess the quality of the model’s reasoning for indefi-niteness, we employ a reference-based LLM-as-Judge approach [8]."
          ],
          "intents": [
            "--",
            "--",
            "--"
          ],
          "cited_paper_info": {
            "title": "Judging LLM-as-a-judge with MT-Bench and Chatbot Arena",
            "abstract": "Evaluating large language model (LLM) based chat assistants is challenging due to their broad capabilities and the inadequacy of existing benchmarks in measuring human preferences. To address this, we explore using strong LLMs as judges to evaluate these models on more open-ended questions. We examine the usage and limitations of LLM-as-a-judge, including position, verbosity, and self-enhancement biases, as well as limited reasoning ability, and propose solutions to mitigate some of them. We then verify the agreement between LLM judges and human preferences by introducing two benchmarks: MT-bench, a multi-turn question set; and Chatbot Arena, a crowdsourced battle platform. Our results reveal that strong LLM judges like GPT-4 can match both controlled and crowdsourced human preferences well, achieving over 80% agreement, the same level of agreement between humans. Hence, LLM-as-a-judge is a scalable and explainable way to approximate human preferences, which are otherwise very expensive to obtain. Additionally, we show our benchmark and traditional benchmarks complement each other by evaluating several variants of LLaMA and Vicuna. The MT-bench questions, 3K expert votes, and 30K conversations with human preferences are publicly available at https://github.com/lm-sys/FastChat/tree/main/fastchat/llm_judge.",
            "year": 2023,
            "venue": "Neural Information Processing Systems",
            "authors": [
              {
                "authorId": "2149970173",
                "name": "Lianmin Zheng"
              },
              {
                "authorId": "2537924",
                "name": "Wei-Lin Chiang"
              },
              {
                "authorId": "2209360681",
                "name": "Ying Sheng"
              },
              {
                "authorId": "92721493",
                "name": "Siyuan Zhuang"
              },
              {
                "authorId": "1390573666",
                "name": "Zhanghao Wu"
              },
              {
                "authorId": "2152482391",
                "name": "Yonghao Zhuang"
              },
              {
                "authorId": "143872641",
                "name": "Zi Lin"
              },
              {
                "authorId": "2141335450",
                "name": "Zhuohan Li"
              },
              {
                "authorId": "2117961435",
                "name": "Dacheng Li"
              },
              {
                "authorId": "143977260",
                "name": "E. Xing"
              },
              {
                "authorId": "145140331",
                "name": "Haotong Zhang"
              },
              {
                "authorId": "49988044",
                "name": "Joseph E. Gonzalez"
              },
              {
                "authorId": "2055174324",
                "name": "Ion Stoica"
              }
            ]
          }
        },
        {
          "citedcorpusid": 259224389,
          "isinfluential": false,
          "contexts": [
            "We choose this approach because Tian et al. [29] and Xiong et al. [30] show that asking the LLM for a confidence value is equally or more reliable than logit-based confidence estimation."
          ],
          "intents": [
            "--"
          ],
          "cited_paper_info": {
            "title": "Can LLMs Express Their Uncertainty? An Empirical Evaluation of Confidence Elicitation in LLMs",
            "abstract": "Empowering large language models to accurately express confidence in their answers is essential for trustworthy decision-making. Previous confidence elicitation methods, which primarily rely on white-box access to internal model information or model fine-tuning, have become less suitable for LLMs, especially closed-source commercial APIs. This leads to a growing need to explore the untapped area of black-box approaches for LLM uncertainty estimation. To better break down the problem, we define a systematic framework with three components: prompting strategies for eliciting verbalized confidence, sampling methods for generating multiple responses, and aggregation techniques for computing consistency. We then benchmark these methods on two key tasks-confidence calibration and failure prediction-across five types of datasets (e.g., commonsense and arithmetic reasoning) and five widely-used LLMs including GPT-4 and LLaMA 2 Chat. Our analysis uncovers several key insights: 1) LLMs, when verbalizing their confidence, tend to be overconfident, potentially imitating human patterns of expressing confidence. 2) As model capability scales up, both calibration and failure prediction performance improve. 3) Employing our proposed strategies, such as human-inspired prompts, consistency among multiple responses, and better aggregation strategies can help mitigate this overconfidence from various perspectives. 4) Comparisons with white-box methods indicate that while white-box methods perform better, the gap is narrow, e.g., 0.522 to 0.605 in AUROC. Despite these advancements, none of these techniques consistently outperform others, and all investigated methods struggle in challenging tasks, such as those requiring professional knowledge, indicating significant scope for improvement. We believe this study can serve as a strong baseline and provide insights for eliciting confidence in black-box LLMs.",
            "year": 2023,
            "venue": "International Conference on Learning Representations",
            "authors": [
              {
                "authorId": "38827926",
                "name": "Miao Xiong"
              },
              {
                "authorId": "48430820",
                "name": "Zhiyuan Hu"
              },
              {
                "authorId": "2179626088",
                "name": "Xinyang Lu"
              },
              {
                "authorId": "2157866322",
                "name": "Yifei Li"
              },
              {
                "authorId": "2215497308",
                "name": "Jie Fu"
              },
              {
                "authorId": "2109932032",
                "name": "Junxian He"
              },
              {
                "authorId": "2258715976",
                "name": "Bryan Hooi"
              }
            ]
          }
        },
        {
          "citedcorpusid": 264172366,
          "isinfluential": false,
          "contexts": [
            "Such self-explanations have been shown to perform on par with traditional explainability methods [23], but also have limited faithfulness [24, 25]."
          ],
          "intents": [
            "--"
          ],
          "cited_paper_info": {
            "title": "Can Large Language Models Explain Themselves? A Study of LLM-Generated Self-Explanations",
            "abstract": "Large language models (LLMs) such as ChatGPT have demonstrated superior performance on a variety of natural language processing (NLP) tasks including sentiment analysis, mathematical reasoning and summarization. Furthermore, since these models are instruction-tuned on human conversations to produce\"helpful\"responses, they can and often will produce explanations along with the response, which we call self-explanations. For example, when analyzing the sentiment of a movie review, the model may output not only the positivity of the sentiment, but also an explanation (e.g., by listing the sentiment-laden words such as\"fantastic\"and\"memorable\"in the review). How good are these automatically generated self-explanations? In this paper, we investigate this question on the task of sentiment analysis and for feature attribution explanation, one of the most commonly studied settings in the interpretability literature (for pre-ChatGPT models). Specifically, we study different ways to elicit the self-explanations, evaluate their faithfulness on a set of evaluation metrics, and compare them to traditional explanation methods such as occlusion or LIME saliency maps. Through an extensive set of experiments, we find that ChatGPT's self-explanations perform on par with traditional ones, but are quite different from them according to various agreement metrics, meanwhile being much cheaper to produce (as they are generated along with the prediction). In addition, we identified several interesting characteristics of them, which prompt us to rethink many current model interpretability practices in the era of ChatGPT(-like) LLMs.",
            "year": 2023,
            "venue": "arXiv.org",
            "authors": [
              {
                "authorId": "2259713702",
                "name": "Shiyuan Huang"
              },
              {
                "authorId": "2258958137",
                "name": "Siddarth Mamidanna"
              },
              {
                "authorId": "2258957319",
                "name": "Shreedhar Jangam"
              },
              {
                "authorId": "2259166405",
                "name": "Yilun Zhou"
              },
              {
                "authorId": "37003919",
                "name": "Leilani Gilpin"
              }
            ]
          }
        },
        {
          "citedcorpusid": 270698898,
          "isinfluential": false,
          "contexts": [
            "There is a large body of work concerning explainable AI (XAI) [22] that has developed various techniques to produce explanations for a classifier’s prediction, including feature attribution methods, counter-factual explanations, rule extraction, and example-based reasoning."
          ],
          "intents": [
            "--"
          ],
          "cited_paper_info": {
            "title": "Explainable artificial intelligence: A survey of needs, techniques, applications, and future direction",
            "abstract": "",
            "year": 2024,
            "venue": "Neurocomputing",
            "authors": [
              {
                "authorId": "2273470499",
                "name": "Melkamu Abay Mersha"
              },
              {
                "authorId": "2307946141",
                "name": "K. Lam"
              },
              {
                "authorId": "2308356053",
                "name": "Joseph Wood"
              },
              {
                "authorId": "2307974150",
                "name": "Ali K. AlShami"
              },
              {
                "authorId": "2261083539",
                "name": "Jugal K. Kalita"
              }
            ]
          }
        },
        {
          "citedcorpusid": 271213206,
          "isinfluential": false,
          "contexts": [
            "Datasets for novelty assessment [9, 10, 11, 12] pair patent claims either with short passages [9, 10] or large chunks [11, 12] from prior art.",
            "Novelty has been evaluated using BERT-based models [9, 10, 11, 12, 13, 14], graph neural networks [15, 16], and LLMs [11, 17].",
            "They use citations marked as novelty-destroying by the examiner as positive samples and obtain negative samples either from other citations [9, 10, 11] and/or from related patents [10, 12]."
          ],
          "intents": [
            "--",
            "--",
            "--"
          ],
          "cited_paper_info": {
            "title": "Comparing Complex Concepts with Transformers: Matching Patent Claims Against Natural Language Text",
            "abstract": "A key capability in managing patent applications or a patent portfolio is comparing claims to other text, e.g. a patent specification. Because the language of claims is different from language used elsewhere in the patent application or in non-patent text, this has been challenging for computer based natural language processing. We test two new LLM-based approaches and find that both provide substantially better performance than previously published values. The ability to match dense information from one domain against much more distributed information expressed in a different vocabulary may also be useful beyond the intellectual property space.",
            "year": 2024,
            "venue": "PatentSemTech@SIGIR",
            "authors": [
              {
                "authorId": "2311438235",
                "name": "Matthias Blume"
              },
              {
                "authorId": "2311442203",
                "name": "Ghobad Heidari"
              },
              {
                "authorId": "2311442439",
                "name": "Christoph Hewel"
              }
            ]
          }
        },
        {
          "citedcorpusid": 271244753,
          "isinfluential": false,
          "contexts": [
            "Datasets for novelty assessment [9, 10, 11, 12] pair patent claims either with short passages [9, 10] or large chunks [11, 12] from prior art.",
            "Novelty has been evaluated using BERT-based models [9, 10, 11, 12, 13, 14], graph neural networks [15, 16], and LLMs [11, 17].",
            "They use citations marked as novelty-destroying by the examiner as positive samples and obtain negative samples either from other citations [9, 10, 11] and/or from related patents [10, 12]."
          ],
          "intents": [
            "--",
            "--",
            "--"
          ],
          "cited_paper_info": {
            "title": "ClaimCompare: A Data Pipeline for Evaluation of Novelty Destroying Patent Pairs",
            "abstract": "A fundamental step in the patent application process is the determination of whether there exist prior patents that are novelty destroying. This step is routinely performed by both applicants and examiners, in order to assess the novelty of proposed inventions among the millions of applications filed annually. However, conducting this search is time and labor-intensive, as searchers must navigate complex legal and technical jargon while covering a large amount of legal claims. Automated approaches using information retrieval and machine learning approaches to detect novelty destroying patents present a promising avenue to streamline this process, yet research focusing on this space remains limited. In this paper, we introduce a novel data pipeline, ClaimCompare, designed to generate labeled patent claim datasets suitable for training IR and ML models to address this challenge of novelty destruction assessment. To the best of our knowledge, ClaimCompare is the first pipeline that can generate multiple novelty destroying patent datasets. To illustrate the practical relevance of this pipeline, we utilize it to construct a sample dataset comprising of over 27K patents in the electrochemical domain: 1,045 base patents from USPTO, each associated with 25 related patents labeled according to their novelty destruction towards the base patent. Subsequently, we conduct preliminary experiments showcasing the efficacy of this dataset in fine-tuning transformer models to identify novelty destroying patents, demonstrating 29.2% and 32.7% absolute improvement in MRR and P@1, respectively.",
            "year": 2024,
            "venue": "PatentSemTech@SIGIR",
            "authors": [
              {
                "authorId": "2214024043",
                "name": "Arav Parikh"
              },
              {
                "authorId": "1403034406",
                "name": "Shiri Dori-Hacohen"
              }
            ]
          }
        }
      ]
    },
    "249394904": {
      "citing_paper_info": {
        "title": "A Survey on Sentence Embedding Models Performance for Patent Analysis",
        "abstract": "Patent data is an important source of knowledge for innovation research, while the technological similarity between pairs of patents is a key enabling indicator for patent analysis. Recently researchers have been using patent vector space models based on different NLP embeddings models to calculate the technological similarity between pairs of patents to help better understand innovations, patent landscaping, technology mapping, and patent quality evaluation. More often than not, Text Embedding is a vital precursor to patent analysis tasks. A pertinent question then arises: How should we measure and evaluate the accuracy of these embeddings? To the best of our knowledge, there is no comprehensive survey that builds a clear delineation of embedding models' performance for calculating patent similarity indicators. Therefore, in this study, we provide an overview of the accuracy of these algorithms based on patent classification performance and propose a standard library and dataset for assessing the accuracy of embeddings models based on PatentSBERTa approach. In a detailed discussion, we report the performance of the top 3 algorithms at section, class, and subclass levels. The results based on the first claim of patents show that PatentSBERTa, Bert-for-patents, and TF-IDF Weighted Word Embeddings have the best accuracy for computing sentence embeddings at the subclass level. According to the first results, the performance of the models in different classes varies, which shows researchers in patent analysis can utilize the results of this study to choose the best proper model based on the specific section of patent data they used.",
        "year": 2022,
        "venue": "arXiv.org",
        "authors": [
          {
            "authorId": "2056771181",
            "name": "Hamid Bekamiri"
          },
          {
            "authorId": "47109088",
            "name": "D. Hain"
          },
          {
            "authorId": "3168776",
            "name": "Roman Jurowetzki"
          }
        ]
      },
      "citation_summary": {
        "citation_count": 6,
        "unique_cited_count": 5,
        "influential_count": 0,
        "detailed_records_count": 6
      },
      "cited_papers": [
        "14279047",
        "104292864",
        "14004036",
        "207556454",
        "16322186"
      ],
      "citation_details": [
        {
          "citedcorpusid": 14004036,
          "isinfluential": false,
          "contexts": [
            "Technical jargon, the use of legal language, and attention to intellectual property rights represent a challenge that can significantly affect the accuracy of patent analysis applications (Zhang et al., 2005; QI et al., 2020; Beal & Kafadar, 2008; Tseng et al., 2007).",
            "Technical jargon, the use of legal language, and attention to intellectual property rights here represent a challenge that can significantly affect the accuracy of patent analysis applications (Zhang et al., 2005; QI et al., 2020; Beal & Kafadar, 2008; Tseng et al., 2007)."
          ],
          "intents": [
            "['background']",
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Text mining techniques for patent analysis",
            "abstract": "",
            "year": 2007,
            "venue": "Information Processing & Management",
            "authors": [
              {
                "authorId": "40130996",
                "name": "Yuen-Hsien Tseng"
              },
              {
                "authorId": "2143476751",
                "name": "Chi-Jen Lin"
              },
              {
                "authorId": "3315593",
                "name": "Yu-I Lin"
              }
            ]
          }
        },
        {
          "citedcorpusid": 14279047,
          "isinfluential": false,
          "contexts": [
            "In this approach, we used a simple TF-IDF weighted approach by adding weights to each word based on its frequency within the document using the pre-trained word2vec (Lilleberg et al., 2015)."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Support vector machines and Word2vec for text classification with semantic features",
            "abstract": "",
            "year": 2015,
            "venue": "IEEE International Conference on Cognitive Informatics and Cognitive Computing",
            "authors": [
              {
                "authorId": "1392936593",
                "name": "Joseph Lilleberg"
              },
              {
                "authorId": "2117078124",
                "name": "Yun Zhu"
              },
              {
                "authorId": "2108082363",
                "name": "Yanqing Zhang"
              }
            ]
          }
        },
        {
          "citedcorpusid": 16322186,
          "isinfluential": false,
          "contexts": [
            "Technical jargon, the use of legal language, and attention to intellectual property rights represent a challenge that can significantly affect the accuracy of patent analysis applications (Zhang et al., 2005; QI et al., 2020; Beal & Kafadar, 2008; Tseng et al., 2007)."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "A k-nearest neighbor based algorithm for multi-label classification",
            "abstract": "",
            "year": 2005,
            "venue": "IEEE International Conference on Granular Computing",
            "authors": [
              {
                "authorId": "3039887",
                "name": "Min-Ling Zhang"
              },
              {
                "authorId": "145624000",
                "name": "Zhi-Hua Zhou"
              }
            ]
          }
        },
        {
          "citedcorpusid": 104292864,
          "isinfluential": false,
          "contexts": [
            "For implementing all types of SOA, using text embeddings especially through transformer models have recently gained considerable attention (Bekamiri et al., 2021; Roudsari et al., 2021; Hofstätter et al., 2019)."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Enriching Word Embeddings for Patent Retrieval with Global Context",
            "abstract": "",
            "year": 2019,
            "venue": "European Conference on Information Retrieval",
            "authors": [
              {
                "authorId": "97393346",
                "name": "Sebastian Hofstätter"
              },
              {
                "authorId": "2844293",
                "name": "Navid Rekabsaz"
              },
              {
                "authorId": "144307089",
                "name": "M. Lupu"
              },
              {
                "authorId": "1764160",
                "name": "Carsten Eickhoff"
              },
              {
                "authorId": "1699657",
                "name": "A. Hanbury"
              }
            ]
          }
        },
        {
          "citedcorpusid": 207556454,
          "isinfluential": false,
          "contexts": [
            "It solves the main challenge of both Word2vec and GloVe as the limitation for computing vector representation of words that are not in the model dictionary (Bojanowski et al., 2017)."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Enriching Word Vectors with Subword Information",
            "abstract": "Continuous word representations, trained on large unlabeled corpora are useful for many natural language processing tasks. Popular models that learn such representations ignore the morphology of words, by assigning a distinct vector to each word. This is a limitation, especially for languages with large vocabularies and many rare words. In this paper, we propose a new approach based on the skipgram model, where each word is represented as a bag of character n-grams. A vector representation is associated to each character n-gram; words being represented as the sum of these representations. Our method is fast, allowing to train models on large corpora quickly and allows us to compute word representations for words that did not appear in the training data. We evaluate our word representations on nine different languages, both on word similarity and analogy tasks. By comparing to recently proposed morphological word representations, we show that our vectors achieve state-of-the-art performance on these tasks.",
            "year": 2016,
            "venue": "Transactions of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "2329288",
                "name": "Piotr Bojanowski"
              },
              {
                "authorId": "3024698",
                "name": "Edouard Grave"
              },
              {
                "authorId": "2319608",
                "name": "Armand Joulin"
              },
              {
                "authorId": "2047446108",
                "name": "Tomas Mikolov"
              }
            ]
          }
        },
        {
          "citedcorpusid": null,
          "isinfluential": false,
          "contexts": [
            "● Micro-F1: Micro-F1 calculates metrics globally by counting the total true positives, false negatives, and false positives (Pedregosa et al.2011).",
            "This weight alters ‘macro’ to\ndenote label imbalance; it can also result in an F-score that is not between precision and recall (Pedregosa et al.2011)."
          ],
          "intents": [
            "['background']",
            "['background']"
          ],
          "cited_paper_info": {}
        }
      ]
    },
    "276148061": {
      "citing_paper_info": {
        "title": "Multilabel Classification of Bilingual Patents Using OneVsRestClassifier: A Semiautomated Approach",
        "abstract": "—In response to the increasing complexity and volume of patent applications, this research introduces a semiautomated system to streamline the literature review process for Indonesian patent data. The proposed system employs a synthesis of multilabel classification techniques based on natural language processing (NLP) algorithms. This methodology focuses on developing an iterative and modular system, with each step visualised in detailed flowcharts. The system design incorporates data collection and preprocessing, multilabel classification model development, model optimisation, query and prediction, and results presentation modules. Experimental results demonstrate the promising potential of the multilabel classification model, achieving a micro F1 score of 0.6723 and a macro F1 score of 0.6009. The OneVsRestClassifier model with LinearSVC as the base classifier shows reasonably good performance in handling a bilingual dataset comprising 15,097 patent documents. The optimal model configuration uses TfidfVectorizer with 20,000 features, including bigrams, and an optimal C parameter of 0.1 for LinearSVC. Performance analysis reveals variations across IPC classes, indicating areas for further improvement. The discussion highlights the implications of the proposed system for researchers, patent examiners and industry professionals by facilitating efficient searches within patent databases. This study acknowledges the potential of semiautomated systems to enhance the efficiency of patent analysis while emphasising the need for further research to address identified challenges, such as class imbalance and performance variations across patent categories. This research paves the way for further developments in the field of automated patent classification, aiming to improve efficiency and accuracy in international patent systems while recognising the crucial role of human experts in the patent classification process.",
        "year": 2025,
        "venue": "International Journal of Advanced Computer Science and Applications",
        "authors": [
          {
            "authorId": "2343911480",
            "name": "Slamet Widodo"
          },
          {
            "authorId": "2292603655",
            "name": "Ermatita"
          },
          {
            "authorId": "1706455",
            "name": "Deris Stiawan"
          }
        ]
      },
      "citation_summary": {
        "citation_count": 15,
        "unique_cited_count": 14,
        "influential_count": 3,
        "detailed_records_count": 15
      },
      "cited_papers": [
        "266879580",
        "272454288",
        "261042683",
        "267034389",
        "237592992",
        "268831213",
        "228893483",
        "245343990",
        "182953211",
        "273784551",
        "254616496",
        "213101916",
        "233476134",
        "4961247"
      ],
      "citation_details": [
        {
          "citedcorpusid": 4961247,
          "isinfluential": false,
          "contexts": [
            "The processed data are then used to train and evaluate multilabel classification models, specifically the OneVsRestClassifier algorithm, to assign multiple IPC labels to each patent document [19].",
            "The research methodology is iterative and modular [18], focused on developing a semiautomated system for reviewing Indonesian patent data literature [19]."
          ],
          "intents": [
            "--",
            "--"
          ],
          "cited_paper_info": {
            "title": "A Supervised Requirement-oriented Patent Classification Scheme Based on the Combination of Metadata and Citation Information",
            "abstract": "AbstractPatent classification systems are applied extensively in innovative analysis. Existing patent classification schemes are either technology-dependent or TRIZ-based. The former ones, such as the IPC and UPC, are normally developed by different patent offices in the world mainly for the purpose of patentability examination and patent retrieval, while the latter is for TRIZ users and analysts with no more than 40 categories. These static classifications are too complex and general to meet the in-depth patent classification requirements of a specific technology area or organization. To tackle these drawbacks, in this paper, we propose an automatic requirement-oriented patent classification scheme as a complementary method using supervised machine learning techniques to classify patent dataset into a user-defined taxonomy. The requirement-oriented patent taxonomy can be technology-dependent, application-dependent or a mixture of both tailored to specific business objectives. It is more comprehensible an...",
            "year": 2015,
            "venue": "International Journal of Computational Intelligence Systems",
            "authors": [
              {
                "authorId": "3248841",
                "name": "Fujin Zhu"
              },
              {
                "authorId": "2108196738",
                "name": "Xuefeng Wang"
              },
              {
                "authorId": "2228972",
                "name": "Donghua Zhu"
              },
              {
                "authorId": "49421138",
                "name": "Yuqin Liu"
              }
            ]
          }
        },
        {
          "citedcorpusid": 182953211,
          "isinfluential": true,
          "contexts": [
            "Once collected, these data are then preprocessed, which includes text cleaning, stopword removal and preparation for efficient machine learning classification [1], [15]–[17].",
            "The significance of this research is its significant potential to develop and advance patent classification techniques by substantially improving the accuracy and precision of analysis, as well as accelerating systematic, structured and data-driven decision-making processes [1], [4].",
            "Traditional methods, although widely used, are time consuming, resource intensive and prone to human error and bias, which can lead to inconsistent and unreliable results [1], [2].",
            "…nature, have become unsustainable in the face of rapid technological innovation; the corresponding increase in intellectual property documentation [1] highlights the intrinsic limitations of manual reviews, particularly their vulnerability to human error and the inherent subjectivity in…",
            "Text processing involves tokenisation and stopword removal using a combination of English and Indonesian stopwords [1], [22], [23] [15], [24].",
            "The continuous influx of submissions adds complexity, which demands efficient analysis for intellectual property management and strategic innovation tracking [1]."
          ],
          "intents": [
            "--",
            "--",
            "--",
            "--",
            "--",
            "--"
          ],
          "cited_paper_info": {
            "title": "BIGPATENT: A Large-Scale Dataset for Abstractive and Coherent Summarization",
            "abstract": "Most existing text summarization datasets are compiled from the news domain, where summaries have a flattened discourse structure. In such datasets, summary-worthy content often appears in the beginning of input articles. Moreover, large segments from input articles are present verbatim in their respective summaries. These issues impede the learning and evaluation of systems that can understand an article’s global content structure as well as produce abstractive summaries with high compression ratio. In this work, we present a novel dataset, BIGPATENT, consisting of 1.3 million records of U.S. patent documents along with human written abstractive summaries. Compared to existing summarization datasets, BIGPATENT has the following properties: i) summaries contain a richer discourse structure with more recurring entities, ii) salient content is evenly distributed in the input, and iii) lesser and shorter extractive fragments are present in the summaries. Finally, we train and evaluate baselines and popular learning models on BIGPATENT to shed light on new challenges and motivate future directions for summarization research.",
            "year": 2019,
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "2059341935",
                "name": "Eva Sharma"
              },
              {
                "authorId": "2116521802",
                "name": "Chen Li"
              },
              {
                "authorId": "2153516659",
                "name": "Lu Wang"
              }
            ]
          }
        },
        {
          "citedcorpusid": 213101916,
          "isinfluential": true,
          "contexts": [
            "For instance, [9] combined KNN and fastText but only tested on English patents, neglecting cross-lingual term alignment.",
            "Three critical gaps persist in the literature, which are as follows: 1) Monolingual bias: Most studies [9], [10] have focused on monolingual patent datasets, overlooking the bilingual complexity inherent in countries such as Indonesia.",
            "The latter approach employs fastText based on word embeddings, in which sentence (or document) vectors are obtained by averaging n-gram embeddings, and then vectors are used as features in multinomial logistic regression [9].",
            "2 explains the flowchart of this patent classification system research, which uses machine learning techniques to classify patent documents into IPC codes [9].",
            "Then, the TF–IDF vectoriser is used with the parameters max_features=20000 and ngram_range=(1, 2) for feature extraction [9].",
            "In comparison to earlier approaches, hybrid methods (e.g. KNN+fastText) [9] and fine-tuned transformer-based models (e.g. BERT and XLNet) [3] have been explored by prior work on monolingual patent classification.",
            "In contrast to the hybrid KNN–fastText approach proposed by Yadrintsev and Sochenkov [9], which showed improved classification results on Russian and English texts through a stacking meta-classifier, our work specifically addressed bilingual data (Indonesian–English) and class imbalance in the IPC."
          ],
          "intents": [
            "--",
            "--",
            "--",
            "--",
            "--",
            "--",
            "--"
          ],
          "cited_paper_info": {
            "title": "The Hybrid Method for Accurate Patent Classification",
            "abstract": "",
            "year": 2019,
            "venue": "Lobachevskii Journal of Mathematics",
            "authors": [
              {
                "authorId": "77510891",
                "name": "V. Yadrintsev"
              },
              {
                "authorId": "2469049",
                "name": "I. Sochenkov"
              }
            ]
          }
        },
        {
          "citedcorpusid": 228893483,
          "isinfluential": false,
          "contexts": [
            "This imbalance can lead to biased classifiers that favour the majority class, resulting in poor classification performance for minority classes [29]."
          ],
          "intents": [
            "--"
          ],
          "cited_paper_info": {
            "title": "Identifying potential technological spin-offs using hierarchical information in international patent classification",
            "abstract": "",
            "year": 2020,
            "venue": "",
            "authors": [
              {
                "authorId": "144132628",
                "name": "H. Sasaki"
              },
              {
                "authorId": "2850710",
                "name": "I. Sakata"
              }
            ]
          }
        },
        {
          "citedcorpusid": 233476134,
          "isinfluential": false,
          "contexts": [
            "Traditional methods, although widely used, are time consuming, resource intensive and prone to human error and bias, which can lead to inconsistent and unreliable results [1], [2].",
            "The study in [2] underlined the transformative impact of NLP in the domain of summarisation, simplification and generation of patent texts, indicating an urgent need for research specifically tailored to the nuanced demands of patent documentation.",
            "The intricate technical and legal language in these documents also contributes to the complexity of manual processing [2]."
          ],
          "intents": [
            "--",
            "--",
            "--"
          ],
          "cited_paper_info": {
            "title": "Summarization, Simplification, and Generation: The Case of Patents",
            "abstract": "",
            "year": 2021,
            "venue": "Expert systems with applications",
            "authors": [
              {
                "authorId": "32501582",
                "name": "Silvia Casola"
              },
              {
                "authorId": "2192424",
                "name": "A. Lavelli"
              }
            ]
          }
        },
        {
          "citedcorpusid": 237592992,
          "isinfluential": false,
          "contexts": [
            "The research methodology is iterative and modular [18], focused on developing a semiautomated system for reviewing Indonesian patent data literature [19]."
          ],
          "intents": [
            "--"
          ],
          "cited_paper_info": {
            "title": "Investigating Entropy for Extractive Document Summarization",
            "abstract": "",
            "year": 2021,
            "venue": "Expert systems with applications",
            "authors": [
              {
                "authorId": "152375853",
                "name": "Alka Khurana"
              },
              {
                "authorId": "67173624",
                "name": "Vasudha Bhatnagar"
              }
            ]
          }
        },
        {
          "citedcorpusid": 245343990,
          "isinfluential": true,
          "contexts": [
            "While recent advances in natural language processing (NLP) have automated aspects of patent analysis [3], [4], critical gaps remain.",
            "We use the IPC to train multilabel classification models, which allow for categorisation that represents the diverse nature of patent data [3], [4].",
            "The proposed solution links multilabel classification algorithms to increase efficiency and reduce the resources required for a comprehensive review [3], [4].",
            "Prior reviews confirm that specialised jargon and evolving concepts in the AI or healthcare domains consistently hamper straightforward classification [3], [8].",
            "2) Class imbalance: Prior works [3], [11] have often assumed balanced IPC label distributions, leading to poor performance in rare categories (e.g. Y02A).",
            "The complexity involved in accurately classifying multifaceted documents is further exacerbated in fields such as artificial intelligence, in which the intersection of technology and legal language demands sophisticated computational techniques for precise analysis [3] [4].",
            "In comparison to earlier approaches, hybrid methods (e.g. KNN+fastText) [9] and fine-tuned transformer-based models (e.g. BERT and XLNet) [3] have been explored by prior work on monolingual patent classification.",
            "Similarly, Haghighian Roudsari et al. [3] leveraged BERT, XLNet and other transformer-based models for multilabel patent classification but focused on monolingual English corpora."
          ],
          "intents": [
            "--",
            "--",
            "--",
            "--",
            "--",
            "--",
            "--",
            "--"
          ],
          "cited_paper_info": {
            "title": "PatentNet: multi-label classification of patent documents using deep learning based language understanding",
            "abstract": "Patent classification is an expensive and time-consuming task that has conventionally been performed by domain experts. However, the increase in the number of filed patents and the complexity of the documents make the classification task challenging. The text used in patent documents is not always written in a way to efficiently convey knowledge. Moreover, patent classification is a multi-label classification task with a large number of labels, which makes the problem even more complicated. Hence, automating this expensive and laborious task is essential for assisting domain experts in managing patent documents, facilitating reliable search, retrieval, and further patent analysis tasks. Transfer learning and pre-trained language models have recently achieved state-of-the-art results in many Natural Language Processing tasks. In this work, we focus on investigating the effect of fine-tuning the pre-trained language models, namely, BERT, XLNet, RoBERTa, and ELECTRA, for the essential task of multi-label patent classification. We compare these models with the baseline deep-learning approaches used for patent classification. We use various word embeddings to enhance the performance of the baseline models. The publicly available USPTO-2M patent classification benchmark and M-patent datasets are used for conducting experiments. We conclude that fine-tuning the pre-trained language models on the patent text improves the multi-label patent classification performance. Our findings indicate that XLNet performs the best and achieves a new state-of-the-art classification performance with respect to precision, recall, F1 measure, as well as coverage error, and LRAP.",
            "year": 2021,
            "venue": "Scientometrics",
            "authors": [
              {
                "authorId": "2197861164",
                "name": "Arousha Haghighian Roudsari"
              },
              {
                "authorId": "145653991",
                "name": "Jafar Afshar"
              },
              {
                "authorId": "1728685",
                "name": "Wookey Lee"
              },
              {
                "authorId": "2112280",
                "name": "Suan Lee"
              }
            ]
          }
        },
        {
          "citedcorpusid": 254616496,
          "isinfluential": false,
          "contexts": [
            "The research in [12], [13] provided evidence of the effectiveness of semiautomated approaches in machine learning-based literature reviews, which can be applied in patent data analysis.",
            "The method that determines how often each word appears in one document component, called term frequency (TF), and how rarely it occurs in all document components, called inverse document frequency (IDF), is the inverse of the TF document [12].",
            "3) Local–global integration: Existing frameworks [12], [13] have rarely combined local patent databases (e.g. Indonesian Patent Database) with international repositories (e.g. Google Patents), limiting their ability to capture region-specific innovations."
          ],
          "intents": [
            "--",
            "--",
            "--"
          ],
          "cited_paper_info": {
            "title": "System for Semi-Automated Literature Review Based on Machine Learning",
            "abstract": "This paper presents the design and implementation of a system for semi-automating the literature review process based on machine learning. By using machine learning algorithms, the system determines whether scientific papers belong to the topic that is being explored as part of the review process. The system’s user interface allows the process of creating a literature review to be managed through a series of steps: selecting data sources, building queries and topic searches, displaying the scientific papers found, selecting papers that belong to the set of desired papers, running machine learning algorithms for learning and automated classification, and displaying and exporting the final set of papers. Manual literature reviews are compared with automated reviews, and similarities and differences between the two approaches in terms of duration, accuracy, and ease of use are discussed. This study concludes that the best results in terms of sensitivity and accuracy for the automated literature review process are achieved by using a combined machine learning model, which uses multiple unweighted machine learning models. Cross-testing the models on two alternative datasets revealed an overlap in the machine learning hyperparameters. The stable sensitivity and accuracy in the tests indicate the potential for generalized use in automated literature review.",
            "year": 2022,
            "venue": "Electronics",
            "authors": [
              {
                "authorId": "2196804280",
                "name": "Filip Bacinger"
              },
              {
                "authorId": "1681387",
                "name": "I. Botički"
              },
              {
                "authorId": "27029904",
                "name": "D. Mlinarić"
              }
            ]
          }
        },
        {
          "citedcorpusid": 261042683,
          "isinfluential": false,
          "contexts": [
            "Text processing involves tokenisation and stopword removal using a combination of English and Indonesian stopwords [1], [22], [23] [15], [24].",
            "( Weighting is used with the TF–IDF formula in research conducted with the equation formula from several previous research sources [22]."
          ],
          "intents": [
            "--",
            "--"
          ],
          "cited_paper_info": {
            "title": "Text Similarity Detection Between Documents Using Case Based Reasoning Method with Cosine Similarity Measure (Case Study SIMNG LPPM Universitas Sriwijaya)",
            "abstract": "LPPM Universitas Sriwijaya is an institution that coordinates academic research and community service inside Universitas Sriwijaya. In carrying out the duty, LPPM assesses every proposal’s originality which would be impossible to do manually in the future due to massive data growth. Thus, automatization for the proposal's originality check is needed. The Case Based Reasoning method is used in this research because it allows the system to reuse the information that has been obtained to find documents that are similar to the test document. In this study, the data is represented in the form of the Vector Space Model and uses Cosine Similarity to measure document to document similarity. The data is represented by giving weight for each part of the tested documents. In this study, four formulas from previous research will be used for term weighting then the final result will be compared. The process begins by extracting data, separating parts of the document, figuring the similarity value of the test document to the case base utilizing Cosine Similarity Measure, results filtering with a certain threshold, summarizing the calculation results, and finally preserving the results obtained to be reused in the next calculation. The results of this study indicate that the text-similarity detection between documents has been successfully carried out using the proposed method with the best sensitivity level and the fastest computation time achieved in configuration II.",
            "year": 2022,
            "venue": "Sriwijaya Journal of Informatics and Applications",
            "authors": [
              {
                "authorId": "147665507",
                "name": "Nabila Febriyanti"
              },
              {
                "authorId": "3330964",
                "name": "Dian Palupi Rini"
              },
              {
                "authorId": "1491643764",
                "name": "Osvari Arsalan"
              }
            ]
          }
        },
        {
          "citedcorpusid": 266879580,
          "isinfluential": false,
          "contexts": [
            "Text processing involves tokenisation and stopword removal using a combination of English and Indonesian stopwords [1], [22], [23] [15], [24]."
          ],
          "intents": [
            "--"
          ],
          "cited_paper_info": {
            "title": "COMPARISON OF STEMMING AND SIMILARITY ALGORITHMS IN INDONESIAN TRANSLATED AL-QUR'AN TEXT SEARCH",
            "abstract": "The long history of information retrieval did not begin with Internet. Prior to widespread public daily use of search engines, in the 1960s information retrieval systems were discovered in commercial and intelligence applications. There are two stages in Information Retrieval in doing its main job which is to preprocessing text and to calculate similarity between term (word) and query (keyword) user searched for in a document. Stemming is final stage of pre-processing in an information retrieval system. The way stemming works is to remove affixes from a word, in form of prefixes, suffixes and insertions into form of basic word. Thus, in this paper we did compare search on information retrieval system without using stemming algorithm, using stemming Porter, Nazief & Adriani and Enhanced Confix Stripping with similarity method used is cosine similarity and dice similarity. Based on test results, text search ability on dice similarity is faster in stemming process with Porter Stemmer and ECS algorithms. While in Nazief & Adriani algorithm and without stemming, cosine similarity is faster than dice similarity.",
            "year": 2022,
            "venue": "Jurnal Ilmiah Kursor",
            "authors": [
              {
                "authorId": "71851576",
                "name": "I. Suzanti"
              },
              {
                "authorId": "2278684330",
                "name": "Achmad Jauhari"
              }
            ]
          }
        },
        {
          "citedcorpusid": 267034389,
          "isinfluential": false,
          "contexts": [
            "2) Class imbalance: Prior works [3], [11] have often assumed balanced IPC label distributions, leading to poor performance in rare categories (e.g. Y02A).",
            "[11], [8] emphasised the importance of collaboration in data sharing and of ethical implications in developing NLP tools for scientific research.",
            "For example, [11] reported high accuracy overall but did not address label skew."
          ],
          "intents": [
            "--",
            "--",
            "--"
          ],
          "cited_paper_info": {
            "title": "A framework for language technologies in behavioral research and clinical applications: Ethical challenges, implications, and solutions.",
            "abstract": "Technological advances in the assessment and understanding of speech and language within the domains of automatic speech recognition, natural language processing, and machine learning present a remarkable opportunity for psychologists to learn more about human thought and communication, evaluate a variety of clinical conditions, and predict cognitive and psychological states. These innovations can be leveraged to automate traditionally time-intensive assessment tasks (e.g., educational assessment), provide psychological information and care (e.g., chatbots), and when delivered remotely (e.g., by mobile phone or wearable sensors) promise underserved communities greater access to health care. Indeed, the automatic analysis of speech provides a wealth of information that can be used for patient care in a wide range of settings (e.g., mHealth applications) and for diverse purposes (e.g., behavioral and clinical research, medical tools that are implemented into practice) and patient types (e.g., numerous psychological disorders and in psychiatry and neurology). However, automation of speech analysis is a complex task that requires the integration of several different technologies within a large distributed process with numerous stakeholders. Many organizations have raised awareness about the need for robust systems for ensuring transparency, oversight, and regulation of technologies utilizing artificial intelligence. Since there is limited knowledge about the ethical and legal implications of these applications in psychological science, we provide a balanced view of both the optimism that is widely published on and also the challenges and risks of use, including discrimination and exacerbation of structural inequalities. (PsycInfo Database Record (c) 2024 APA, all rights reserved).",
            "year": 2024,
            "venue": "American Psychologist",
            "authors": [
              {
                "authorId": "1402867836",
                "name": "Catherine Diaz-Asper"
              },
              {
                "authorId": "2156249991",
                "name": "Mathias K Hauglid"
              },
              {
                "authorId": "1557748689",
                "name": "Chelsea Chandler"
              },
              {
                "authorId": "2280108780",
                "name": "Alex S. Cohen"
              },
              {
                "authorId": "2262833856",
                "name": "Peter W. Foltz"
              },
              {
                "authorId": "2880608",
                "name": "B. Elvevåg"
              }
            ]
          }
        },
        {
          "citedcorpusid": 268831213,
          "isinfluential": false,
          "contexts": [
            "The need for comprehensive and well-annotated datasets for training and testing NLP models remains an ongoing hurdle, alongside the development of models that can adeptly navigate the intricacies of patent language and accurately reflect the evolving landscape of technological innovation [6], [10].",
            "First, most systems have focused on monolingual datasets (e.g. English only [5] or Indonesian only [6]), neglecting the bilingual nature of patents in countries such as Indonesia, where filings combine local and international languages.",
            "…has high value and strong relevance to patent examiners, research and development institutions and companies that rely heavily on accurate and efficient patent analysis, with much broader implications for innovation tracking, in-depth competitive analysis and future technology forecasting [6], [7]."
          ],
          "intents": [
            "--",
            "--",
            "--"
          ],
          "cited_paper_info": {
            "title": "Natural Language Processing Patents Landscape Analysis",
            "abstract": "Understanding NLP patents provides valuable insights into innovation trends and competitive dynamics in artificial intelligence. This study uses the Lens patent database to investigate the landscape of NLP patents. The overall patent output in the NLP field on a global scale has exhibited a rapid growth over the past decade, indicating rising research and commercial interests in applying NLP techniques. By analyzing patent assignees, technology categories, and geographic distribution, we identify leading innovators as well as research hotspots in applying NLP. The patent landscape reflects intensifying competition between technology giants and research institutions. This research aims to synthesize key patterns and developments in NLP innovation revealed through patent data analysis, highlighting implications for firms and policymakers. A detailed understanding of NLP patenting activity can inform intellectual property strategy and technology investment decisions in this burgeoning AI domain.",
            "year": 2024,
            "venue": "International Conference on Data Technologies and Applications",
            "authors": [
              {
                "authorId": "1556678958",
                "name": "H. Al-Khalifa"
              },
              {
                "authorId": "2203789370",
                "name": "Taif Omar Al-Omar"
              },
              {
                "authorId": "2221928389",
                "name": "Ghala AlOlyyan"
              }
            ]
          }
        },
        {
          "citedcorpusid": 272454288,
          "isinfluential": false,
          "contexts": [
            "3) Threshold optimisation optimises the threshold to convert the output of the decision function into binary predictions [37]."
          ],
          "intents": [
            "--"
          ],
          "cited_paper_info": {
            "title": "Prediction of Outpatient No-Show Appointments Using Machine Learning Algorithms for Pediatric Patients in Saudi Arabia",
            "abstract": "—Patient no-shows are prevalent in pediatric outpatient visits, leading to underutilized medical resources, increased healthcare costs, reduced clinic efficiency, and decreased access to care. The use of machine learning techniques provides insights to mitigate this problem. This study aimed to develop a predictive model for patient no-shows at the Ministry of National Guard Health-Affairs, Saudi Arabia, and evaluate the results of various machine learning algorithms in predicting these events. Four machine learning algorithms - Gradient Boosting, AdaBoost, Random Forest, and Naive Bayes - were used to create predictive models for patient no-shows. Each model underwent extensive parameter tuning and reliability assessment to ensure robust performance, including sensitivity analysis and cross-validation. Gradient Boosting achieved the highest area under the receiver operating curve (AUC) of 0.902 and Classification Accuracy (CA) of 0.944, while the AdaBoost model achieved an AUC of 0.812 and CA of 0.927. The Naive Bayes and Random Forest models achieved AUCs of 0.677 and 0.889 and CAs of 0.915 and 0.937, respectively. The confusion matrix demonstrated high true-positive rates for no-shows for the Gradient Boosting and Random Forest models, while Naive Bayes had the lowest values. The Gradient Boosting and Random Forest models were most effective in predicting patient no-shows. These models could enhance outpatient clinic efficiency by predicting no-shows. Future research can further refine these models and investigate practical strategies for their implementation",
            "year": 2024,
            "venue": "International Journal of Advanced Computer Science and Applications",
            "authors": [
              {
                "authorId": "145861381",
                "name": "Abdulwahhab Alshammari"
              },
              {
                "authorId": "2294612411",
                "name": "F. Alotaibi"
              },
              {
                "authorId": "2250509422",
                "name": "Sana Alnafrani"
              }
            ]
          }
        },
        {
          "citedcorpusid": 273784551,
          "isinfluential": false,
          "contexts": [
            "To generate a new score, the code-mixed relevance score modifies the TF–IDF score, and weighting and normalisation are applied to obtain the final feature vector EF [36]."
          ],
          "intents": [
            "--"
          ],
          "cited_paper_info": {
            "title": "Ensemble of Weighted Code Mixed Feature Engineering and Machine Learning-Based Multiclass Classification for Enhanced Opinion Mining on Unstructured Data",
            "abstract": "—There is an exponential growth of opinions on online platforms, and the rapid rise in communication technologies generates a significant need to analyze opinions in online social networks (OSN). However, these opinions are unstructured, rendering knowledge extraction from opinions complex and challenging to implement. Although existing opinions mining systems are applied in several applications, limited research is available to handle code-mixed opinions of a non-structured nature where there is a switching of lexicons in languages within a single opinion structure. The challenge lies in interpreting complex opinions in multimedia networks owing to their unstructured nature, volume, and lexical structure. This paper presents a novel ensemble approach using machine learning and natural language processing to interpret code mixed opinions efficiently. Firstly, the opinions are extracted from the input corpus and preprocessed using proposed Extended Feature Vectors (EFV). Subsequently, the opinion mining system is implemented using a novel approach using weighted code mixed opinion mining framework (WCM-OMF) for multiclass classification. The proposed WCM-OMF model achieves an accuracy of 79.11% and 72% for the benchmark datasets, which is a significant improvement over existing Hierarchical LSTM, Random Forest, and SVM models and state-of-the-art-methods. The proposed solution can be implemented in opinion detection of other business sectors beneficial in obtaining actionable insights for efficient decision-making in enterprises and Business Intelligence (BI).",
            "year": 2024,
            "venue": "International Journal of Advanced Computer Science and Applications",
            "authors": [
              {
                "authorId": "2328948134",
                "name": "Ruchi Sharma"
              },
              {
                "authorId": "9204396",
                "name": "P. Shrinath"
              }
            ]
          }
        },
        {
          "citedcorpusid": null,
          "isinfluential": false,
          "contexts": [
            "The research in [12], [13] provided evidence of the effectiveness of semiautomated approaches in machine learning-based literature reviews, which can be applied in patent data analysis.",
            "3) Local–global integration: Existing frameworks [12], [13] have rarely combined local patent databases (e.g. Indonesian Patent Database) with international repositories (e.g. Google Patents), limiting their ability to capture region-specific innovations."
          ],
          "intents": [
            "--",
            "--"
          ],
          "cited_paper_info": {}
        }
      ]
    },
    "248209878": {
      "citing_paper_info": {
        "title": "Research on Product Core Component Acquisition Based on Patent Semantic Network",
        "abstract": "Patent data contain plenty of valuable information. Recently, the lack of innovative ideas has resulted in some enterprises encountering bottlenecks in product research and development (R&D). Some enterprises point out that they do not have enough comprehension of product components. To improve efficiency of product R&D, this paper introduces natural-language processing (NLP) technology, which includes part-of-speech (POS) tagging and subject–action–object (SAO) classification. Our strategy first extracts patent keywords from products, then applies a complex network to obtain core components based on structural holes and centrality of eigenvector algorism. Finally, we use the example of US shower patents to verify the effectiveness and feasibility of the methodology. As a result, this paper examines the acquisition of core components and how they can help enterprises and designers clarify their R&D ideas and design priorities.",
        "year": 2022,
        "venue": "Entropy",
        "authors": [
          {
            "authorId": "97827652",
            "name": "W. Lin"
          },
          {
            "authorId": "46522098",
            "name": "Xiaodong Liu"
          },
          {
            "authorId": "34251941",
            "name": "R. Xiao"
          }
        ]
      },
      "citation_summary": {
        "citation_count": 18,
        "unique_cited_count": 18,
        "influential_count": 1,
        "detailed_records_count": 18
      },
      "cited_papers": [
        "198478902",
        "224900798",
        "55222115",
        "155066609",
        "112136000",
        "55972320",
        "555806",
        "220732566",
        "222418956",
        "49311113",
        "159005556",
        "19233823",
        "9255185",
        "108890446",
        "154921757",
        "38122372",
        "12620679",
        "4568402"
      ],
      "citation_details": [
        {
          "citedcorpusid": 555806,
          "isinfluential": false,
          "contexts": [
            "The working principle of this shower is: when the water has passed through coil 5, the coil will generate electricity, which will be stored in the battery placed inside the showerhead 25; the battery will be used to support the motor 14 to adjust the valve 15, thereby allowing water ﬂow control.",
            "Patent text information mining is mainly divided into three strategies: domain knowledge [14], vector space [15] and patent text semantics [16]."
          ],
          "intents": [
            "['methodology']",
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Supporting Patent Mining by using Ontology-based Semantic Annotations",
            "abstract": "",
            "year": 2007,
            "venue": "International Conference on Wirtschaftsinformatik",
            "authors": [
              {
                "authorId": "2579228",
                "name": "N. Ghoula"
              },
              {
                "authorId": "3348964",
                "name": "Khaled Khelif"
              },
              {
                "authorId": "1727241",
                "name": "R. Dieng"
              }
            ]
          }
        },
        {
          "citedcorpusid": 4568402,
          "isinfluential": false,
          "contexts": [
            "As for An et al. [21], in order to determine the relationship between keywords in patents, prepositions were introduced into the semantic analysis network, thereby overcoming the limitations of keyword network analysis."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Deriving technology intelligence from patents: Preposition-based semantic analysis",
            "abstract": "",
            "year": 2018,
            "venue": "J. Informetrics",
            "authors": [
              {
                "authorId": "84593382",
                "name": "Jaehyeon An"
              },
              {
                "authorId": "2109332425",
                "name": "Kyuwoong Kim"
              },
              {
                "authorId": "49127328",
                "name": "L. Mortara"
              },
              {
                "authorId": "38648012",
                "name": "Sungjoon Lee"
              }
            ]
          }
        },
        {
          "citedcorpusid": 9255185,
          "isinfluential": false,
          "contexts": [
            "Patent information mining research includes three main methods: patent research method based on (i) international patent classiﬁcation (IPC) [6], (ii) citation analysis [7] and (iii) patent text [8]."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Analyzing the time delay between scientific research and technology patents based on the citation distribution model",
            "abstract": "",
            "year": 2017,
            "venue": "Scientometrics",
            "authors": [
              {
                "authorId": "29817116",
                "name": "Guijie Zhang"
              },
              {
                "authorId": "1909662",
                "name": "Yuqiang Feng"
              },
              {
                "authorId": "49568414",
                "name": "Guang Yu"
              },
              {
                "authorId": "3751867",
                "name": "Luning Liu"
              },
              {
                "authorId": "11027746",
                "name": "Yanqiqi Hao"
              }
            ]
          }
        },
        {
          "citedcorpusid": 12620679,
          "isinfluential": false,
          "contexts": [
            "Patent information mining research includes three main methods: patent research method based on (i) international patent classiﬁcation (IPC) [6], (ii) citation analysis [7] and (iii) patent text [8]."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Interactive overlay maps for US patent (USPTO) data based on International Patent Classification (IPC)",
            "abstract": "We report on the development of an interface to the US Patent and Trademark Office (USPTO) that allows for the mapping of patent portfolios as overlays to basemaps constructed from citation relations among all patents contained in this database during the period 1976–2011. Both the interface and the data are in the public domain; the freeware programs VOSViewer and/or Pajek can be used for the visualization. These basemaps and overlays can be generated at both the 3-digit and 4-digit levels of the International Patent Classification (IPC) of the world intellectual property organization (WIPO). The basemaps can provide a stable mental framework for analysts to follow developments over searches for different years, which can be animated. The full flexibility of the advanced search engines of USPTO are available for generating sets of patents and/or patent applications which can thus be visualized and compared. This instrument allows for addressing questions about technological distance, diversity in portfolios, and animating the developments of both technologies and technological capacities of organizations over time.",
            "year": 2012,
            "venue": "Scientometrics",
            "authors": [
              {
                "authorId": "1733565",
                "name": "L. Leydesdorff"
              },
              {
                "authorId": "2453316",
                "name": "D. Kushnir"
              },
              {
                "authorId": "1805026",
                "name": "Ismael Rafols"
              }
            ]
          }
        },
        {
          "citedcorpusid": 19233823,
          "isinfluential": false,
          "contexts": [
            "There has been some research on the application of patent text information mining to product functions [43].",
            "In patent text, the structure of SAO in reference Guo et al. [43] is summarized as shown in Table 4: REVIEW 8 of REVIEW"
          ],
          "intents": [
            "['background']",
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Requirement-oriented core technological components’ identification based on SAO analysis",
            "abstract": "",
            "year": 2017,
            "venue": "Scientometrics",
            "authors": [
              {
                "authorId": "2146344363",
                "name": "Chao Yang"
              },
              {
                "authorId": "2228972",
                "name": "Donghua Zhu"
              },
              {
                "authorId": "2108196738",
                "name": "Xuefeng Wang"
              },
              {
                "authorId": "2153910566",
                "name": "Yi Zhang"
              },
              {
                "authorId": "46266495",
                "name": "Guangquan Zhang"
              },
              {
                "authorId": "144864069",
                "name": "Jie Lu"
              }
            ]
          }
        },
        {
          "citedcorpusid": 38122372,
          "isinfluential": false,
          "contexts": [
            "However, this kind of patent extraction method is relatively rough and based on the macro-level, so it cannot maximize information usage [10]."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Cross-language patent matching via an international patent classification-based concept bridge",
            "abstract": "",
            "year": 2013,
            "venue": "Journal of information science",
            "authors": [
              {
                "authorId": "123331773",
                "name": "Yen-Liang Chen"
              },
              {
                "authorId": "39776056",
                "name": "Yu-Ting Chiu"
              }
            ]
          }
        },
        {
          "citedcorpusid": 49311113,
          "isinfluential": false,
          "contexts": [
            "Use of available, valid patent information could save companies around 60% R&D time and 40% R&D expense [4]."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Patents as Early Indicators of Technology and Investment Trends: Analyzing the Microbiome Space as a Case Study",
            "abstract": "The human microbiome is the collective of microbes living in symbiosis on and within humans. Modulating its composition and function has become an attractive means for the prevention and treatment of a variety of diseases including cancer. Since the initiation of the human microbiome project in 2007, the number of academic publications and active patent families around the microbiome has grown exponentially. Screening patent databases can be useful for the early detection and the tracking of new technology trends. However, it is not sufficient to assess portfolio sizes because emerging players with small but high-quality patent portfolios will be missed within the noise of large but low-quality portfolio owners. Here we used the consolidated database and software tool PatentSight to benchmark patent portfolios, and to analyze key patent owners and innovators in the microbiome space. Our study shows how in-depth patent analyses combining qualitative and quantitative parameters can identify actionable early indicators of technology and investment trends from large patent datasets.",
            "year": 2018,
            "venue": "Frontiers in Bioengineering and Biotechnology",
            "authors": [
              {
                "authorId": "48833038",
                "name": "M. Fankhauser"
              },
              {
                "authorId": "39724692",
                "name": "C. Moser"
              },
              {
                "authorId": "50829909",
                "name": "Theodor Nyfeler"
              }
            ]
          }
        },
        {
          "citedcorpusid": 55222115,
          "isinfluential": false,
          "contexts": [
            "Patent citation analysis starts from the citation network to analyze the relationship between patents and the process of technology evolution [11]."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "The Diffusion of Patented Oil and Gas Technology with Environmental Uses: A Forward Patent Citation Analysis",
            "abstract": "Relevant advances in the mitigation of environmental impact could be obtained by the appropriate diffusion of existing environmental technologies. In this paper, we look at the diffusion of knowledge related to environmental technologies developed within the oil and gas industry. To assess knowledge spillovers from oil and gas inventions as a measure of technology diffusion, we rely on forward patent citations methodology. Results show that there is a strong likelihood that the citing patent will be eventually linked to environmental technologies if the original oil and gas invention has already environmental uses. Moreover, both intra and intersectoral spillovers produce a \"turnabout\" effect, meaning that citing patents show the opposite quality level of the cited patent. Our results support the idea that more sector-specific environmental policies, with an emphasis on diffusion, would significantly improve the use of environmental technologies developed within the oil and gas industry.",
            "year": 2014,
            "venue": "",
            "authors": [
              {
                "authorId": "1398847374",
                "name": "Néstor Duch-Brown"
              },
              {
                "authorId": "1403128863",
                "name": "M. Costa-Campi"
              }
            ]
          }
        },
        {
          "citedcorpusid": 55972320,
          "isinfluential": false,
          "contexts": [
            "In addition, Li et al. [30] raised the problem of avoiding patent infringement; this paper puts forward a problem-solving tool based on modern TRIZ tools, which successfully avoids the problem of the patent equivalent of word infringement."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Calculation of Sentence Semantic Similarity Based on Syntactic Structure",
            "abstract": "Combined with the problem of single direction of the solution of the existing sentence similarity algorithms, an algorithm for sentence semantic similarity based on syntactic structure was proposed. Firstly, analyze the sentence constituent, then through analysis convert sentence similarity into words similarity on the basis of syntactic structure, then convert words similarity into concept similarity through words disambiguation, and, finally, realize the semantic similarity comparison. It also gives the comparison rules in more detail for the modifier words in the sentence which also have certain contributions to the sentence. Under the same test condition, the experiments show that the proposed algorithm is more intuitive understanding of people and has higher accuracy.",
            "year": 2015,
            "venue": "",
            "authors": [
              {
                "authorId": "2108785292",
                "name": "Xiao Li"
              },
              {
                "authorId": "48933915",
                "name": "Qingsheng Li"
              }
            ]
          }
        },
        {
          "citedcorpusid": 108890446,
          "isinfluential": false,
          "contexts": [
            "As critical elements of the product, components (i.e., parts), especially core components, are essential for product function [1]."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Analysis of mechanical systems with adaptable functions for the evaluation of functional coupling and component importance",
            "abstract": "",
            "year": 2015,
            "venue": "",
            "authors": [
              {
                "authorId": "89993965",
                "name": "Y. Yu"
              },
              {
                "authorId": "1912387",
                "name": "Y.-M. Deng"
              },
              {
                "authorId": "2233744505",
                "name": "W. F. Lu"
              },
              {
                "authorId": "1766754",
                "name": "A. Nee"
              }
            ]
          }
        },
        {
          "citedcorpusid": 112136000,
          "isinfluential": false,
          "contexts": [
            "Choi, Lee and You [29] implemented text mining to extract the characteristics of word vectors, and the document is clustered by an outlier detection algorithm to develop a patent recognition network and explore potential technologies."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Technological Capability Analysis of Competitor Using Patent Information: Focused on Mobile Communication Technology development companies",
            "abstract": "** , 한성대학교 지식서비스&컨설팅학과 교수 *** Abstract Patent information analysis has been carried out for technological capability analysis of competitors relating to next generation mobile communication. Various analysis methods, such as applicant analysis, technology classification analysis, indicator analysis and the like have been utilized as a method of analyzing patent information. As a first step for the technological capability analysis of competitors, applicants having high patent activity(PA) were selected, and as a second step therefor, technology classifications showing high technological independence (TI) were selected. Furthermore, portfolios for technology classifications showing high technological independence in the patents of main applicants having high patent activity by matching results the first and second steps together were prepared. Through such a process, portfolios for important technologies which have been concentrically researched by competitors could be analyzed. Accordingly, the present analysis results will help to carry out strategic R&D management, such as the establishment of company R&D plans and patent strategies.",
            "year": 2014,
            "venue": "",
            "authors": [
              {
                "authorId": "2118850477",
                "name": "Seung-Wook Choi"
              },
              {
                "authorId": "2145053742",
                "name": "Chang-Won Lee"
              },
              {
                "authorId": "3387206",
                "name": "Yen-Yoo You"
              }
            ]
          }
        },
        {
          "citedcorpusid": 154921757,
          "isinfluential": false,
          "contexts": [
            "In fact, core components play a vital role in identifying the focus of current product design [3]."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Development of patent roadmap based on technology roadmap by analyzing patterns of patent development",
            "abstract": "",
            "year": 2015,
            "venue": "",
            "authors": [
              {
                "authorId": "2105576530",
                "name": "Yujin Jeong"
              },
              {
                "authorId": "38717655",
                "name": "B. Yoon"
              }
            ]
          }
        },
        {
          "citedcorpusid": 155066609,
          "isinfluential": false,
          "contexts": [
            "It also will be a hot topic in the future [32]."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Novelty-focused patent mapping for technology opportunity analysis",
            "abstract": "",
            "year": 2015,
            "venue": "",
            "authors": [
              {
                "authorId": "47499070",
                "name": "Changyong Lee"
              },
              {
                "authorId": "2224529",
                "name": "Bokyoung Kang"
              },
              {
                "authorId": "40053580",
                "name": "Juneseuk Shin"
              }
            ]
          }
        },
        {
          "citedcorpusid": 159005556,
          "isinfluential": false,
          "contexts": [
            "For products, the core components determine the performance and quality [2]."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Component complementarity and transaction costs: the evolution of product design",
            "abstract": "",
            "year": 2018,
            "venue": "Reviews of Management Sciences",
            "authors": [
              {
                "authorId": "145580181",
                "name": "N. Burton"
              },
              {
                "authorId": "145583468",
                "name": "P. Galvin"
              }
            ]
          }
        },
        {
          "citedcorpusid": 198478902,
          "isinfluential": false,
          "contexts": [
            "Patent information mining research includes three main methods: patent research method based on (i) international patent classiﬁcation (IPC) [6], (ii) citation analysis [7] and (iii) patent text [8]."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "A novelty detection patent mining approach for analyzing technological opportunities",
            "abstract": "",
            "year": 2019,
            "venue": "Advanced Engineering Informatics",
            "authors": [
              {
                "authorId": "100739831",
                "name": "Juite Wang"
              },
              {
                "authorId": "2109358442",
                "name": "Yi-Jing Chen"
              }
            ]
          }
        },
        {
          "citedcorpusid": 220732566,
          "isinfluential": false,
          "contexts": [
            "In addition to SAO, Chen et al. [23] built a patent information extraction framework through named entity recognition and semantic relation extraction, which improved performance of named entity extraction in patent texts."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "A deep learning based method for extracting semantic information from patent documents",
            "abstract": "",
            "year": 2020,
            "venue": "Scientometrics",
            "authors": [
              {
                "authorId": "2146033169",
                "name": "Liang Chen"
              },
              {
                "authorId": "50433328",
                "name": "Shuo Xu"
              },
              {
                "authorId": "48324516",
                "name": "Lijun Zhu"
              },
              {
                "authorId": "2155701047",
                "name": "Jing Zhang"
              },
              {
                "authorId": "2072811437",
                "name": "Xiao-ping Lei"
              },
              {
                "authorId": "31279648",
                "name": "Guancan Yang"
              }
            ]
          }
        },
        {
          "citedcorpusid": 222418956,
          "isinfluential": true,
          "contexts": [
            "Further, β 1 + β 2 + β 3 + β 4 = 1, and β 1 > β 2 > β 3 > β 4 . length between Since β 1 is the most apparent basic interpretation, the weight of β 1 is deﬁned at above 0.5 according to the research of [46]. two sememes in the sememe tree."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Online Screening of X-System Music Playlists Using an Integrative Wellbeing Model Informed by the Theory of Autopoiesis",
            "abstract": "Research suggests that music has a powerful effect on the human mind and body. This article explores the impact of music as an intervention. For this purpose, the X-System technology is used to curate relaxing and enlivening music playlists designed to positively impact wellbeing and emotional state during the COVID-19 pandemic. A wellbeing model grounded in autopoietic theory of self-organisation in living systems is developed to inform the evaluation of the impact of the intervention and ensure the reliability of the data. More specifically, data quality is enhanced by focusing the participants’ awareness on their immediate embodied experience of physical, emotional and relational wellbeing and sense of pleasure/displeasure prior to and after listening to a preferred playlist. The statistical analysis shows significant positive changes in emotional wellbeing, valence and sense of meaning ( $p< 0.001$ ) with a medium effect size. It also reveals a statistically significant change for physical wellbeing ( $p=0.009$ ) with a small effect size. With the relaxing playlists leading to decrease in arousal levels and the enlivening playlists to an increase in activation, it is also concluded that appropriately curated playlists may be able to lead the listener to positive relaxation or activation states or indeed to positive mood change that may have health benefits.",
            "year": 2020,
            "venue": "IEEE Access",
            "authors": [
              {
                "authorId": "3053167",
                "name": "P. Sice"
              },
              {
                "authorId": "1996205263",
                "name": "Garry Elvin"
              },
              {
                "authorId": "40803209",
                "name": "Chirine Riachy"
              },
              {
                "authorId": "2053234571",
                "name": "Yilun Shang"
              },
              {
                "authorId": "71603366",
                "name": "Suzannah Ogwu"
              },
              {
                "authorId": "2073639521",
                "name": "C. Zink"
              }
            ]
          }
        },
        {
          "citedcorpusid": 224900798,
          "isinfluential": false,
          "contexts": [
            "[35] pointed out that components’ healthy and stable operation is the guarantee of stable and safe operation in power system, so they proposed a machine learning method to identify and detect defects in transmission line components."
          ],
          "intents": [
            "--"
          ],
          "cited_paper_info": {
            "title": "Component identification and defect detection in transmission lines based on deep learning",
            "abstract": "Ensuring the stable and safe operation of the power system is an important work of the national power grid companies. The power grid company has established a special power inspection department to troubleshoot transmission line components and replace faulty components in a timely manner. At present, assisted manual inspection by drone inspection has become a trend of power line inspection. Automatically identifying component failures from images of UAV aerial transmission lines is a cutting-edge cross-cutting issue. Based on the above problems, the purpose of this article is to study the component identification and defect detection of transmission lines based on deep learning. This paper expands the dataset by adjusting the size of the convolution kernel of the CNN model and the rotation transformation of the image. The experimental results show that both methods can effectively improve the effectiveness and reliability of component identification and defect detection in transmission line inspection. The recognition and classification experiments were performed using the images collected by the drone. The experimental results show that the effectiveness and reliability of the deep learning method in the identification and defect detection of high-voltage transmission line components are very high. Faster R-CNN performs component identification and defect detection. The detection can reach a recognition speed of nearly 0.17 s per sheet, the recognition rate of the pressure-equalizing ring can reach 96.8%, and the mAP can reach 93.72%.",
            "year": 2021,
            "venue": "Journal of Intelligent & Fuzzy Systems",
            "authors": [
              {
                "authorId": "47410978",
                "name": "Xiangyu Zheng"
              },
              {
                "authorId": "1738472498",
                "name": "Rong Jia"
              },
              {
                "authorId": "2287240700",
                "name": "Aisikaer"
              },
              {
                "authorId": "46350169",
                "name": "Linling Gong"
              },
              {
                "authorId": "1806391290",
                "name": "Guangru Zhang"
              },
              {
                "authorId": "144562617",
                "name": "Jian Dang"
              }
            ]
          }
        }
      ]
    },
    "234010057": {
      "citing_paper_info": {
        "title": "Cross-domain Knowledge Discovery based on Knowledge Graph and Patent Mining",
        "abstract": "This paper studies an approach on cross-domain knowledge discovery to assist the conceptual stage of the design process related to mechanical engineering. Variable methods and tools are proposed to obtain knowledge within a given domain until now. However, methods on cross-domain knowledge analysis is under-developed. In this paper, domain knowledge graph is built automatically by employing natural language process (NLP) and patent mining. They comprise patent documents obtaining and knowledge extraction. Then according to the international patent classification (IPC), the knowledge elements are divided to some different categories. The elements are stored in Databases and then the given domain knowledge graph is constructed after correlation analyses. The cross-domain knowledge surrounding the given domain knowledge is found by mining the correlation among cross-domain knowledge. The cross-domain knowledge can inspire designers about new design of a given domain. And the efficiency of knowledge reusing can be improved by domain knowledge graphs.",
        "year": 2021,
        "venue": "Journal of Physics: Conference Series",
        "authors": [
          {
            "authorId": "1407704833",
            "name": "F. Ye."
          },
          {
            "authorId": "2125274439",
            "name": "Tie Fu"
          },
          {
            "authorId": "2056814351",
            "name": "Lin Gong"
          },
          {
            "authorId": "2111017110",
            "name": "Jun Gao"
          }
        ]
      },
      "citation_summary": {
        "citation_count": 3,
        "unique_cited_count": 3,
        "influential_count": 0,
        "detailed_records_count": 3
      },
      "cited_papers": [
        "1957433",
        "33742869",
        "2762657"
      ],
      "citation_details": [
        {
          "citedcorpusid": 1957433,
          "isinfluential": false,
          "contexts": [
            "Word2Vec and GloVe models project text into a vector.",
            "With the development of NLP, scholars have proposed Word2Vec [7], GloVe [9], ELMo [10] and BERT [5] models to quantitatively describe text semantics."
          ],
          "intents": [
            "--",
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "GloVe: Global Vectors for Word Representation",
            "abstract": "Recent methods for learning vector space representations of words have succeeded in capturing fine-grained semantic and syntactic regularities using vector arithmetic, but the origin of these regularities has remained opaque. We analyze and make explicit the model properties needed for such regularities to emerge in word vectors. The result is a new global logbilinear regression model that combines the advantages of the two major model families in the literature: global matrix factorization and local context window methods. Our model efficiently leverages statistical information by training only on the nonzero elements in a word-word cooccurrence matrix, rather than on the entire sparse matrix or on individual context windows in a large corpus. The model produces a vector space with meaningful substructure, as evidenced by its performance of 75% on a recent word analogy task. It also outperforms related models on similarity tasks and named entity recognition.",
            "year": 2014,
            "venue": "Conference on Empirical Methods in Natural Language Processing",
            "authors": [
              {
                "authorId": "143845796",
                "name": "Jeffrey Pennington"
              },
              {
                "authorId": "2166511",
                "name": "R. Socher"
              },
              {
                "authorId": "144783904",
                "name": "Christopher D. Manning"
              }
            ]
          }
        },
        {
          "citedcorpusid": 2762657,
          "isinfluential": false,
          "contexts": [
            "PMI [8] is employed to measure the related information theoretical association."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Normalized (pointwise) mutual information in collocation extraction",
            "abstract": ". In this paper, we discuss the related information theoretical association measures of mutual information and pointwise mutual information, in the context of collocation extraction. We introduce normalized variants of these measures in order to make them more easily interpretable and at the same time less sensitive to occurrence frequency. We also provide a small empirical study to give more insight into the behaviour of these new measures in a collocation extraction setup.",
            "year": 2009,
            "venue": "",
            "authors": [
              {
                "authorId": "34211567",
                "name": "G. Bouma"
              }
            ]
          }
        },
        {
          "citedcorpusid": 33742869,
          "isinfluential": false,
          "contexts": [
            "[4] proposed to convert claims into conceptual graphs based on syntactic and semantic information."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Extract conceptual graphs from plain texts in patent claims",
            "abstract": "",
            "year": 2012,
            "venue": "Engineering applications of artificial intelligence",
            "authors": [
              {
                "authorId": "3024639",
                "name": "Shih-Yao Yang"
              },
              {
                "authorId": "1700936",
                "name": "V. Soo"
              }
            ]
          }
        }
      ]
    },
    "201666075": {
      "citing_paper_info": {
        "title": "Measuring Patent Claim Generation by Span Relevancy",
        "abstract": "Our goal of patent claim generation is to realize \"augmented inventing\" for inventors by leveraging latest Deep Learning techniques. We envision the possibility of building an \"auto-complete\" function for inventors to conceive better inventions in the era of artificial intelligence. In order to generate patent claims with good quality, a fundamental question is how to measure it. We tackle the problem from a perspective of claim span relevancy. Patent claim language was rarely explored in the NLP field. It is unique in its own way and contains rich explicit and implicit human annotations. In this work, we propose a span-based approach and a generic framework to measure patent claim generation quantitatively. In order to study the effectiveness of patent claim generation, we define a metric to measure whether two consecutive spans in a generated patent claims are relevant. We treat such relevancy measurement as a span-pair classification problem, following the concept of natural language inference. Technically, the span-pair classifier is implemented by fine-tuning a pre-trained language model. The patent claim generation is implemented by fine-tuning the other pre-trained model. Specifically, we fine-tune a pre-trained Google BERT model to measure the patent claim spans generated by a fine-tuned OpenAI GPT-2 model. In this way, we re-use two of the state-of-the-art pre-trained models in the NLP field. Our result shows the effectiveness of the span-pair classifier after fine-tuning the pre-trained model. It further validates the quantitative metric of span relevancy in patent claim generation. Particularly, we found that the span relevancy ratio measured by BERT becomes lower when the diversity in GPT-2 text generation becomes higher.",
        "year": 2019,
        "venue": "arXiv.org",
        "authors": [
          {
            "authorId": "2387987",
            "name": "Jieh-Sheng Lee"
          },
          {
            "authorId": "1798127",
            "name": "J. Hsiang"
          }
        ]
      },
      "citation_summary": {
        "citation_count": 2,
        "unique_cited_count": 2,
        "influential_count": 2,
        "detailed_records_count": 2
      },
      "cited_papers": [
        "146808476",
        "198953378"
      ],
      "citation_details": [
        {
          "citedcorpusid": 146808476,
          "isinfluential": true,
          "contexts": [
            "There are several competing Transformer-based models emerging too, such as RoBERTa [6], MASS [7], XLNet [8] and ERNIE 2.0 [9]."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "MASS: Masked Sequence to Sequence Pre-training for Language Generation",
            "abstract": "Pre-training and fine-tuning, e.g., BERT, have achieved great success in language understanding by transferring knowledge from rich-resource pre-training task to the low/zero-resource downstream tasks. Inspired by the success of BERT, we propose MAsked Sequence to Sequence pre-training (MASS) for the encoder-decoder based language generation tasks. MASS adopts the encoder-decoder framework to reconstruct a sentence fragment given the remaining part of the sentence: its encoder takes a sentence with randomly masked fragment (several consecutive tokens) as input, and its decoder tries to predict this masked fragment. In this way, MASS can jointly train the encoder and decoder to develop the capability of representation extraction and language modeling. By further fine-tuning on a variety of zero/low-resource language generation tasks, including neural machine translation, text summarization and conversational response generation (3 tasks and totally 8 datasets), MASS achieves significant improvements over the baselines without pre-training or with other pre-training methods. Specially, we achieve the state-of-the-art accuracy (37.5 in terms of BLEU score) on the unsupervised English-French translation, even beating the early attention-based supervised model.",
            "year": 2019,
            "venue": "International Conference on Machine Learning",
            "authors": [
              {
                "authorId": "50982078",
                "name": "Kaitao Song"
              },
              {
                "authorId": "48391466",
                "name": "Xu Tan"
              },
              {
                "authorId": "143826491",
                "name": "Tao Qin"
              },
              {
                "authorId": "145313246",
                "name": "Jianfeng Lu"
              },
              {
                "authorId": "2110264337",
                "name": "Tie-Yan Liu"
              }
            ]
          }
        },
        {
          "citedcorpusid": 198953378,
          "isinfluential": true,
          "contexts": [
            "There are several competing Transformer-based models emerging too, such as RoBERTa [6], MASS [7], XLNet [8] and ERNIE 2.0 [9]."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "RoBERTa: A Robustly Optimized BERT Pretraining Approach",
            "abstract": "Language model pretraining has led to significant performance gains but careful comparison between different approaches is challenging. Training is computationally expensive, often done on private datasets of different sizes, and, as we will show, hyperparameter choices have significant impact on the final results. We present a replication study of BERT pretraining (Devlin et al., 2019) that carefully measures the impact of many key hyperparameters and training data size. We find that BERT was significantly undertrained, and can match or exceed the performance of every model published after it. Our best model achieves state-of-the-art results on GLUE, RACE and SQuAD. These results highlight the importance of previously overlooked design choices, and raise questions about the source of recently reported improvements. We release our models and code.",
            "year": 2019,
            "venue": "arXiv.org",
            "authors": [
              {
                "authorId": "11323179",
                "name": "Yinhan Liu"
              },
              {
                "authorId": "40511414",
                "name": "Myle Ott"
              },
              {
                "authorId": "39589154",
                "name": "Naman Goyal"
              },
              {
                "authorId": "3048577",
                "name": "Jingfei Du"
              },
              {
                "authorId": "144863691",
                "name": "Mandar Joshi"
              },
              {
                "authorId": "50536468",
                "name": "Danqi Chen"
              },
              {
                "authorId": "39455775",
                "name": "Omer Levy"
              },
              {
                "authorId": "35084211",
                "name": "M. Lewis"
              },
              {
                "authorId": "1982950",
                "name": "Luke Zettlemoyer"
              },
              {
                "authorId": "1759422",
                "name": "Veselin Stoyanov"
              }
            ]
          }
        }
      ]
    },
    "264492677": {
      "citing_paper_info": {
        "title": "Unveiling Black-Boxes: Explainable Deep Learning Models for Patent Classification",
        "abstract": "Recent technological advancements have led to a large number of patents in a diverse range of domains, making it challenging for human experts to analyze and manage. State-of-the-art methods for multi-label patent classification rely on deep neural networks (DNNs), which are complex and often considered black-boxes due to their opaque decision-making processes. In this paper, we propose a novel deep explainable patent classification framework by introducing layer-wise relevance propagation (LRP) to provide human-understandable explanations for predictions. We train several DNN models, including Bi-LSTM, CNN, and CNN-BiLSTM, and propagate the predictions backward from the output layer up to the input layer of the model to identify the relevance of words for individual predictions. Considering the relevance score, we then generate explanations by visualizing relevant words for the predicted patent class. Experimental results on two datasets comprising two-million patent texts demonstrate high performance in terms of various evaluation measures. The explanations generated for each prediction highlight important relevant words that align with the predicted class, making the prediction more understandable. Explainable systems have the potential to facilitate the adoption of complex AI-enabled methods for patent classification in real-world applications.",
        "year": 2023,
        "venue": "xAI",
        "authors": [
          {
            "authorId": "2257348502",
            "name": "Md Shajalal"
          },
          {
            "authorId": "1766820",
            "name": "Sebastian Denef"
          },
          {
            "authorId": "2261753339",
            "name": "Md. Rezaul Karim"
          },
          {
            "authorId": "2149721151",
            "name": "Alexander Boden"
          },
          {
            "authorId": "2190446797",
            "name": "Gunnar Stevens"
          }
        ]
      },
      "citation_summary": {
        "citation_count": 20,
        "unique_cited_count": 20,
        "influential_count": 4,
        "detailed_records_count": 20
      },
      "cited_papers": [
        "21663443",
        "198953378",
        "235455342",
        "232234881",
        "220732566",
        "238772900",
        "229680129",
        "231786331",
        "247292305",
        "209073823",
        "207556454",
        "245343990",
        "216042147",
        "9327892",
        "2407601",
        "16447573",
        "253451633",
        "235262768",
        "13029170",
        "182953211"
      ],
      "citation_details": [
        {
          "citedcorpusid": 2407601,
          "isinfluential": false,
          "contexts": [
            "With the advent of different word-embedding techniques such as word2vec by Mikolov et al. [21, 22], Glove by Pennington et al. [23], and FastText by Bojanowski et al. [24], the NLP research has been revolutionized with the ability to represent text using high-dimensional semantic vector…"
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Distributed Representations of Sentences and Documents",
            "abstract": "Many machine learning algorithms require the input to be represented as a fixed-length feature vector. When it comes to texts, one of the most common fixed-length features is bag-of-words. Despite their popularity, bag-of-words features have two major weaknesses: they lose the ordering of the words and they also ignore semantics of the words. For example, \"powerful,\" \"strong\" and \"Paris\" are equally distant. In this paper, we propose Paragraph Vector, an unsupervised algorithm that learns fixed-length feature representations from variable-length pieces of texts, such as sentences, paragraphs, and documents. Our algorithm represents each document by a dense vector which is trained to predict words in the document. Its construction gives our algorithm the potential to overcome the weaknesses of bag-of-words models. Empirical results show that Paragraph Vectors outperforms bag-of-words models as well as other techniques for text representations. Finally, we achieve new state-of-the-art results on several text classification and sentiment analysis tasks.",
            "year": 2014,
            "venue": "International Conference on Machine Learning",
            "authors": [
              {
                "authorId": "2827616",
                "name": "Quoc V. Le"
              },
              {
                "authorId": "2047446108",
                "name": "Tomas Mikolov"
              }
            ]
          }
        },
        {
          "citedcorpusid": 9327892,
          "isinfluential": false,
          "contexts": [
            "These XAI models can provide insights into the decision-making process, explaining the reasoning behind specific predictions, the overall model’s priorities in decision making, and thereby enhancing the transparency and trustworthiness of the application [11, 17, 10, 18, 12].",
            "In recent years, there has been a growing interest in developing explainable artificial intelligence (XAI) to unveil the black-box decision-making process of DNN models in diverse fields, including image processing [12], text processing, finance [13, 14], and health applications [15, 16].",
            "…and interpretability of layer-wise relevance propagation (LRP) in other short-text classification tasks [39, 40, 41], we have adopted LRP [12] as the method for explaining the complex neural networks-based patent classification model. preprocessed patent texts semantically by…"
          ],
          "intents": [
            "['methodology']",
            "['background']",
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "On Pixel-Wise Explanations for Non-Linear Classifier Decisions by Layer-Wise Relevance Propagation",
            "abstract": "Understanding and interpreting classification decisions of automated image classification systems is of high value in many applications, as it allows to verify the reasoning of the system and provides additional information to the human expert. Although machine learning methods are solving very successfully a plethora of tasks, they have in most cases the disadvantage of acting as a black box, not providing any information about what made them arrive at a particular decision. This work proposes a general solution to the problem of understanding classification decisions by pixel-wise decomposition of nonlinear classifiers. We introduce a methodology that allows to visualize the contributions of single pixels to predictions for kernel-based classifiers over Bag of Words features and for multilayered neural networks. These pixel contributions can be visualized as heatmaps and are provided to a human expert who can intuitively not only verify the validity of the classification decision, but also focus further analysis on regions of potential interest. We evaluate our method for classifiers trained on PASCAL VOC 2009 images, synthetic image data containing geometric shapes, the MNIST handwritten digits data set and for the pre-trained ImageNet model available as part of the Caffe open source package.",
            "year": 2015,
            "venue": "PLoS ONE",
            "authors": [
              {
                "authorId": "40405884",
                "name": "Sebastian Bach"
              },
              {
                "authorId": "49345823",
                "name": "Alexander Binder"
              },
              {
                "authorId": "144535526",
                "name": "G. Montavon"
              },
              {
                "authorId": "2162050824",
                "name": "Frederick Klauschen"
              },
              {
                "authorId": "145034054",
                "name": "K. Müller"
              },
              {
                "authorId": "1699054",
                "name": "W. Samek"
              }
            ]
          }
        },
        {
          "citedcorpusid": 13029170,
          "isinfluential": false,
          "contexts": [
            "These XAI models can provide insights into the decision-making process, explaining the reasoning behind specific predictions, the overall model’s priorities in decision making, and thereby enhancing the transparency and trustworthiness of the application [11, 17, 10, 18, 12]."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Association for Computational Linguistics \" Why Should I Trust You? \" Explaining the Predictions of Any Classifier",
            "abstract": "",
            "year": null,
            "venue": "",
            "authors": [
              {
                "authorId": "78846919",
                "name": "Marco Tulio Ribeiro"
              },
              {
                "authorId": "34650964",
                "name": "Sameer Singh"
              },
              {
                "authorId": "1730156",
                "name": "Carlos Guestrin"
              }
            ]
          }
        },
        {
          "citedcorpusid": 16447573,
          "isinfluential": false,
          "contexts": [
            "With the advent of different word-embedding techniques such as word2vec by Mikolov et al. [21, 22], Glove by Pennington et al. [23], and FastText by Bojanowski et al. [24], the NLP research has been revolutionized with the ability to represent text using high-dimensional semantic vector…"
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Distributed Representations of Words and Phrases and their Compositionality",
            "abstract": "The recently introduced continuous Skip-gram model is an efficient method for learning high-quality distributed vector representations that capture a large number of precise syntactic and semantic word relationships. In this paper we present several extensions that improve both the quality of the vectors and the training speed. By subsampling of the frequent words we obtain significant speedup and also learn more regular word representations. We also describe a simple alternative to the hierarchical softmax called negative sampling. \n \nAn inherent limitation of word representations is their indifference to word order and their inability to represent idiomatic phrases. For example, the meanings of \"Canada\" and \"Air\" cannot be easily combined to obtain \"Air Canada\". Motivated by this example, we present a simple method for finding phrases in text, and show that learning good vector representations for millions of phrases is possible.",
            "year": 2013,
            "venue": "Neural Information Processing Systems",
            "authors": [
              {
                "authorId": "2047446108",
                "name": "Tomas Mikolov"
              },
              {
                "authorId": "1701686",
                "name": "I. Sutskever"
              },
              {
                "authorId": "2118440152",
                "name": "Kai Chen"
              },
              {
                "authorId": "32131713",
                "name": "G. Corrado"
              },
              {
                "authorId": "49959210",
                "name": "J. Dean"
              }
            ]
          }
        },
        {
          "citedcorpusid": 21663443,
          "isinfluential": false,
          "contexts": [
            "In recent years, the patent classification task has gained significant attention in the field of natural language processing (NLP) research, as evidenced by several notable studies [19, 2, 3]."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "An LSTM Approach to Patent Classification based on Fixed Hierarchy Vectors",
            "abstract": "",
            "year": 2018,
            "venue": "SDM",
            "authors": [
              {
                "authorId": "2113210651",
                "name": "M. Shalaby"
              },
              {
                "authorId": "3264870",
                "name": "J. Stutzki"
              },
              {
                "authorId": "39403212",
                "name": "Matthias Schubert"
              },
              {
                "authorId": "3075189",
                "name": "Stephan Günnemann"
              }
            ]
          }
        },
        {
          "citedcorpusid": 182953211,
          "isinfluential": true,
          "contexts": [
            "BigPatent dataset: BigPatent 2 dataset is prepared by processing 1.3 million patent texts [43]."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "BIGPATENT: A Large-Scale Dataset for Abstractive and Coherent Summarization",
            "abstract": "Most existing text summarization datasets are compiled from the news domain, where summaries have a flattened discourse structure. In such datasets, summary-worthy content often appears in the beginning of input articles. Moreover, large segments from input articles are present verbatim in their respective summaries. These issues impede the learning and evaluation of systems that can understand an article’s global content structure as well as produce abstractive summaries with high compression ratio. In this work, we present a novel dataset, BIGPATENT, consisting of 1.3 million records of U.S. patent documents along with human written abstractive summaries. Compared to existing summarization datasets, BIGPATENT has the following properties: i) summaries contain a richer discourse structure with more recurring entities, ii) salient content is evenly distributed in the input, and iii) lesser and shorter extractive fragments are present in the summaries. Finally, we train and evaluate baselines and popular learning models on BIGPATENT to shed light on new challenges and motivate future directions for summarization research.",
            "year": 2019,
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "2059341935",
                "name": "Eva Sharma"
              },
              {
                "authorId": "2116521802",
                "name": "Chen Li"
              },
              {
                "authorId": "2153516659",
                "name": "Lu Wang"
              }
            ]
          }
        },
        {
          "citedcorpusid": 198953378,
          "isinfluential": false,
          "contexts": [
            "More recently, there has been a growing trend in employing transformer-based pre-trained models, including deep bidirectional transformer (BERT) [28], robust optimized BERT (RoBERTa) [29], distilled BERT (DistilBERT) [30], and XLNet [31], for text representation in NLP tasks."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "RoBERTa: A Robustly Optimized BERT Pretraining Approach",
            "abstract": "Language model pretraining has led to significant performance gains but careful comparison between different approaches is challenging. Training is computationally expensive, often done on private datasets of different sizes, and, as we will show, hyperparameter choices have significant impact on the final results. We present a replication study of BERT pretraining (Devlin et al., 2019) that carefully measures the impact of many key hyperparameters and training data size. We find that BERT was significantly undertrained, and can match or exceed the performance of every model published after it. Our best model achieves state-of-the-art results on GLUE, RACE and SQuAD. These results highlight the importance of previously overlooked design choices, and raise questions about the source of recently reported improvements. We release our models and code.",
            "year": 2019,
            "venue": "arXiv.org",
            "authors": [
              {
                "authorId": "11323179",
                "name": "Yinhan Liu"
              },
              {
                "authorId": "40511414",
                "name": "Myle Ott"
              },
              {
                "authorId": "39589154",
                "name": "Naman Goyal"
              },
              {
                "authorId": "3048577",
                "name": "Jingfei Du"
              },
              {
                "authorId": "144863691",
                "name": "Mandar Joshi"
              },
              {
                "authorId": "50536468",
                "name": "Danqi Chen"
              },
              {
                "authorId": "39455775",
                "name": "Omer Levy"
              },
              {
                "authorId": "35084211",
                "name": "M. Lewis"
              },
              {
                "authorId": "1982950",
                "name": "Luke Zettlemoyer"
              },
              {
                "authorId": "1759422",
                "name": "Veselin Stoyanov"
              }
            ]
          }
        },
        {
          "citedcorpusid": 207556454,
          "isinfluential": false,
          "contexts": [
            "…different word-embedding techniques such as word2vec by Mikolov et al. [21, 22], Glove by Pennington et al. [23], and FastText by Bojanowski et al. [24], the NLP research has been revolutionized with the ability to represent text using high-dimensional semantic vector representations [25, 26, 27]."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Enriching Word Vectors with Subword Information",
            "abstract": "Continuous word representations, trained on large unlabeled corpora are useful for many natural language processing tasks. Popular models that learn such representations ignore the morphology of words, by assigning a distinct vector to each word. This is a limitation, especially for languages with large vocabularies and many rare words. In this paper, we propose a new approach based on the skipgram model, where each word is represented as a bag of character n-grams. A vector representation is associated to each character n-gram; words being represented as the sum of these representations. Our method is fast, allowing to train models on large corpora quickly and allows us to compute word representations for words that did not appear in the training data. We evaluate our word representations on nine different languages, both on word similarity and analogy tasks. By comparing to recently proposed morphological word representations, we show that our vectors achieve state-of-the-art performance on these tasks.",
            "year": 2016,
            "venue": "Transactions of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "2329288",
                "name": "Piotr Bojanowski"
              },
              {
                "authorId": "3024698",
                "name": "Edouard Grave"
              },
              {
                "authorId": "2319608",
                "name": "Armand Joulin"
              },
              {
                "authorId": "2047446108",
                "name": "Tomas Mikolov"
              }
            ]
          }
        },
        {
          "citedcorpusid": 209073823,
          "isinfluential": false,
          "contexts": [
            "…DNN models, including recurrent neural networks (RNN) and their variants such as convolutional neural networks (CNN), long short-term memory networks (LSTM), bidirectional LSTM (Bi-LSTM), and gated recurrent unit (GRU), can learn to classify patents based on their textual content [5, 7, 2, 8, 9].",
            "Generally, these models represent patent text using word embedding and transformer-based pre-trained models [5, 6, 1, 2, 7]."
          ],
          "intents": [
            "['background']",
            "['background']"
          ],
          "cited_paper_info": {
            "title": "A Deep Self-learning Classification Framework for Incomplete Medical Patents with Multi-label",
            "abstract": "",
            "year": 2019,
            "venue": "International Conference on Natural Computation, Fuzzy Systems and Knowledge Discovery",
            "authors": [
              {
                "authorId": "1384647551",
                "name": "Mengzhen Luo"
              },
              {
                "authorId": "2119202927",
                "name": "Xiaoyu Shi"
              },
              {
                "authorId": "1596817752",
                "name": "Qianqian Ji"
              },
              {
                "authorId": "3846181",
                "name": "Mingsheng Shang"
              },
              {
                "authorId": "50045807",
                "name": "Xianbo He"
              },
              {
                "authorId": "2112529368",
                "name": "Weiguo Tao"
              }
            ]
          }
        },
        {
          "citedcorpusid": 216042147,
          "isinfluential": false,
          "contexts": [
            "Additionally, Kang et al. [33] employed a multi-modal embedding approach for searching patent documents."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Prior Art Search Using Multi-modal Embedding of Patent Documents",
            "abstract": "Due to the limitations of the existing prior art search methods, a new patent search paradigm can be innovated by the concepts based on a precise patent document embedding, and a real-time feedback. These concepts can be achieved by the following ideas. The latest language model BERT can be incorporated with the description drawing embedding so that the explorable user interactive model can be adopted to the patent domain for “Building an artificial intelligent patent search system.\" Therefore, these methodologies mainly with the help of deep learning can solve the traditional labor-intensive and time-consuming prior art search.",
            "year": 2020,
            "venue": "International Conference on Big Data and Smart Computing",
            "authors": [
              {
                "authorId": "102453209",
                "name": "Myungchul Kang"
              },
              {
                "authorId": "2112280",
                "name": "Suan Lee"
              },
              {
                "authorId": "1728685",
                "name": "Wookey Lee"
              }
            ]
          }
        },
        {
          "citedcorpusid": 220732566,
          "isinfluential": true,
          "contexts": [
            "…and analyzing patent data, and the methods can be categorized based on different factors such as the techniques utilized, the tasks’ objectives (e.g., multi-class or multi-level classification), and the type of resources used to represent the patent data (i.e., uni-modal vs multi-modal) [20, 7, 9].",
            "…DNN models, including recurrent neural networks (RNN) and their variants such as convolutional neural networks (CNN), long short-term memory networks (LSTM), bidirectional LSTM (Bi-LSTM), and gated recurrent unit (GRU), can learn to classify patents based on their textual content [5, 7, 2, 8, 9].",
            "A DNN-based framework employing Bi-LSTM-CRF and Bi-GRU-HAN models has been introduced to extract semantic information from patents’ texts [7].",
            "Generally, these models represent patent text using word embedding and transformer-based pre-trained models [5, 6, 1, 2, 7]."
          ],
          "intents": [
            "['methodology']",
            "['background']",
            "['methodology']",
            "['background']"
          ],
          "cited_paper_info": {
            "title": "A deep learning based method for extracting semantic information from patent documents",
            "abstract": "",
            "year": 2020,
            "venue": "Scientometrics",
            "authors": [
              {
                "authorId": "2146033169",
                "name": "Liang Chen"
              },
              {
                "authorId": "50433328",
                "name": "Shuo Xu"
              },
              {
                "authorId": "48324516",
                "name": "Lijun Zhu"
              },
              {
                "authorId": "2155701047",
                "name": "Jing Zhang"
              },
              {
                "authorId": "2072811437",
                "name": "Xiao-ping Lei"
              },
              {
                "authorId": "31279648",
                "name": "Guancan Yang"
              }
            ]
          }
        },
        {
          "citedcorpusid": 229680129,
          "isinfluential": false,
          "contexts": [
            "…inspiration from the effectiveness and interpretability of layer-wise relevance propagation (LRP) in other short-text classification tasks [39, 40, 41], we have adopted LRP [12] as the method for explaining the complex neural networks-based patent classification model. preprocessed patent texts…",
            "By summing up all the relevance scores of the neuron in z i in layer i , we can obtain the relevance in layer i , R i = i R i ← j . δ can be either 0 or 1 (we use δ = 1 ) [40, 41]."
          ],
          "intents": [
            "['methodology']",
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "DeepHateExplainer: Explainable Hate Speech Detection in Under-resourced Bengali Language",
            "abstract": "In this paper, we propose an explainable approach for hate speech detection from the under-resourced Bengali language, which we called DeepHateExplainer. In our approach, Bengali texts are first comprehensively preprocessed, before classifying them into political, personal, geopolitical, and religious hates using a neural ensemble method of transformer-based neural architectures (i.e., monolingual Bangla BERT-base, multilingual BERT-cased/uncased, and XLM-RoBERTa). Subsequently, important (most and least) terms are identified using sensitivity analysis and layer-wise relevance propagation (LRP), before providing human-interpretable explanations11To foster reproducible research, we make available the data, source codes, models, and notebooks: https://github.com/rezacsedu/DeepHateExplainer. Finally, we compute comprehensiveness and sufficiency scores to measure the quality of explanations w.r.t faithfulness. Evaluations against machine learning (linear and tree-based models) and neural networks (i.e., CNN, Bi-LSTM, and Conv-LSTM with word embeddings) baselines yield F1-scores of 78%, 91%, 89%, and 84%, for political, personal, geopolitical, and religious hates, respectively, outperforming both ML and DNN baselines22Read an extended version of this paper: https://arxiv.org/abs/2012.14353.",
            "year": 2020,
            "venue": "International Conference on Data Science and Advanced Analytics",
            "authors": [
              {
                "authorId": "9432769",
                "name": "Md. Rezaul Karim"
              },
              {
                "authorId": "2143933030",
                "name": "Sumon Dey"
              },
              {
                "authorId": "117018834",
                "name": "Bharathi Raja Chakravarthi"
              }
            ]
          }
        },
        {
          "citedcorpusid": 231786331,
          "isinfluential": false,
          "contexts": [
            "In recent years, there has been a growing interest in developing explainable artificial intelligence (XAI) to unveil the black-box decision-making process of DNN models in diverse fields, including image processing [12], text processing, finance [13, 14], and health applications [15, 16]."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Unbox the black-box for the medical explainable AI via multi-modal and multi-centre data fusion: A mini-review, two showcases and beyond",
            "abstract": "",
            "year": 2021,
            "venue": "Information Fusion",
            "authors": [
              {
                "authorId": "2149522265",
                "name": "Guang Yang"
              },
              {
                "authorId": "32813168",
                "name": "Qinghao Ye"
              },
              {
                "authorId": "2069552236",
                "name": "Jun Xia"
              }
            ]
          }
        },
        {
          "citedcorpusid": 232234881,
          "isinfluential": false,
          "contexts": [
            "Roudsari et al. [36] compared different word-embedding methods for patent classification performance."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Comparison and Analysis of Embedding Methods for Patent Documents",
            "abstract": "Patent text mining is an important task that requires domain knowledge. The patent text is sometimes not clear and contains many ambiguous and technical words. Traditional text mining approaches are not satisfactory enough for patent text mining. In this paper, we consider various embedding techniques for patent documents and try to find how to represent the patent text for other downstream tasks such as patent classification, patent recommendation, finding similar patents, knowledge mining, etc. We compare several embedding approaches with the patent classification task. The experimental results demonstrate that using contextual word embeddings can perform better than the conventional static word embedding approaches.",
            "year": 2021,
            "venue": "International Conference on Big Data and Smart Computing",
            "authors": [
              {
                "authorId": "9346492",
                "name": "A. H. Roudsari"
              },
              {
                "authorId": "145653991",
                "name": "Jafar Afshar"
              },
              {
                "authorId": "2112280",
                "name": "Suan Lee"
              },
              {
                "authorId": "1728685",
                "name": "Wookey Lee"
              }
            ]
          }
        },
        {
          "citedcorpusid": 235262768,
          "isinfluential": true,
          "contexts": [
            "For example, there are 667 labels in the subclass level [42].",
            "We conducted experiments on a dataset containing 1.5 million patent claims annotated with patent class 1 [42]."
          ],
          "intents": [
            "['background']",
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "PatentSBERTa: A deep NLP based hybrid model for patent distance and classification using augmented SBERT",
            "abstract": "",
            "year": 2021,
            "venue": "Technological forecasting & social change",
            "authors": [
              {
                "authorId": "2056771181",
                "name": "Hamid Bekamiri"
              },
              {
                "authorId": "47109088",
                "name": "D. Hain"
              },
              {
                "authorId": "3168776",
                "name": "Roman Jurowetzki"
              }
            ]
          }
        },
        {
          "citedcorpusid": 235455342,
          "isinfluential": false,
          "contexts": [
            "In recent years, there has been a growing interest in developing explainable artificial intelligence (XAI) to unveil the black-box decision-making process of DNN models in diverse fields, including image processing [12], text processing, finance [13, 14], and health applications [15, 16]."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Deep Learning and Explainable Artificial Intelligence Techniques Applied for Detecting Money Laundering–A Critical Review",
            "abstract": "Money laundering has been a global issue for decades, which is one of the major threat for economy and society. Government, regulatory and financial institutions are combating it together in their respective capacity, however still billions of dollars in fines by authorities make the headlines in the news. High-speed internet services have enabled financial institutions to deliver better customer experience through multi-channel engagements, which has led to exponential growth in transactions and new avenues for laundering the money for fraudsters. Literature shows the usage of statistical methods, data mining and Machine Learning (ML) techniques for money laundering detection, but limited research on Deep Learning (DL) techniques, primarily due to lack of model interpretability and explainability of the decisions made. Several studies are conducted on application of ML for Anti-Money Laundering (AML), and Explainable Artificial Intelligence (XAI) techniques in general, but lacks the study on usage of DL techniques together with XAI. This paper aims to review the current state-of-the-art literature on DL together with XAI for identifying suspicious money laundering transactions and identify future research areas. Key findings of the review are, researchers have preferred variants of Convolutional Neural Networks, and AutoEncoder; graph deep learning together with natural language processing is emerging as an important technology for AML; XAI use is not seen in AML domain; 51% ML methods used in AML are non-interpretable, 58% studies used sample of old real data; key challenges for researchers are access to recent real transaction data and scarcity of labelled training data; and data being highly imbalanced. Future research directions are, application of XAI techniques to bring-out explainability, graph deep learning using natural language processing (NLP), unsupervised and reinforcement learning to handle lack of labelled data; and joint research programs between research community and industry to benefit from domain knowledge and controlled access to data.",
            "year": 2021,
            "venue": "IEEE Access",
            "authors": [
              {
                "authorId": "2112922219",
                "name": "Dattatray Vishnu Kute"
              },
              {
                "authorId": "143620129",
                "name": "B. Pradhan"
              },
              {
                "authorId": "34287537",
                "name": "N. Shukla"
              },
              {
                "authorId": "1441241106",
                "name": "A. Alamri"
              }
            ]
          }
        },
        {
          "citedcorpusid": 238772900,
          "isinfluential": false,
          "contexts": [
            "Similarly, Aroyehun et al. [35] introduced a hierarchical transfer and multi-task learning approach for patent classification, following a similar methodology."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Leveraging label hierarchy using transfer and multi-task learning: A case study on patent classification",
            "abstract": "",
            "year": 2021,
            "venue": "Neurocomputing",
            "authors": [
              {
                "authorId": "46205857",
                "name": "S. Aroyehun"
              },
              {
                "authorId": "2054894372",
                "name": "Jason Angel"
              },
              {
                "authorId": "35122767",
                "name": "Navonil Majumder"
              },
              {
                "authorId": "1747784",
                "name": "Alexander Gelbukh"
              },
              {
                "authorId": "145125161",
                "name": "A. Hussain"
              }
            ]
          }
        },
        {
          "citedcorpusid": 245343990,
          "isinfluential": true,
          "contexts": [
            "…and analyzing patent data, and the methods can be categorized based on different factors such as the techniques utilized, the tasks’ objectives (e.g., multi-class or multi-level classification), and the type of resources used to represent the patent data (i.e., uni-modal vs multi-modal) [20, 7, 9].",
            "…DNN models, including recurrent neural networks (RNN) and their variants such as convolutional neural networks (CNN), long short-term memory networks (LSTM), bidirectional LSTM (Bi-LSTM), and gated recurrent unit (GRU), can learn to classify patents based on their textual content [5, 7, 2, 8, 9].",
            "A multi-level classification framework [9] has been proposed utilizing fine-tuned transformer-based pre-trained models, such as BERT, XLNet, RoBERTa, and ELECTRA[32].",
            "Compared to existing works by Roudsari et al. [9] and Shaobo et al. [2], the performance of our trained models is effective."
          ],
          "intents": [
            "['methodology']",
            "['background']",
            "['methodology']",
            "['result']"
          ],
          "cited_paper_info": {
            "title": "PatentNet: multi-label classification of patent documents using deep learning based language understanding",
            "abstract": "Patent classification is an expensive and time-consuming task that has conventionally been performed by domain experts. However, the increase in the number of filed patents and the complexity of the documents make the classification task challenging. The text used in patent documents is not always written in a way to efficiently convey knowledge. Moreover, patent classification is a multi-label classification task with a large number of labels, which makes the problem even more complicated. Hence, automating this expensive and laborious task is essential for assisting domain experts in managing patent documents, facilitating reliable search, retrieval, and further patent analysis tasks. Transfer learning and pre-trained language models have recently achieved state-of-the-art results in many Natural Language Processing tasks. In this work, we focus on investigating the effect of fine-tuning the pre-trained language models, namely, BERT, XLNet, RoBERTa, and ELECTRA, for the essential task of multi-label patent classification. We compare these models with the baseline deep-learning approaches used for patent classification. We use various word embeddings to enhance the performance of the baseline models. The publicly available USPTO-2M patent classification benchmark and M-patent datasets are used for conducting experiments. We conclude that fine-tuning the pre-trained language models on the patent text improves the multi-label patent classification performance. Our findings indicate that XLNet performs the best and achieves a new state-of-the-art classification performance with respect to precision, recall, F1 measure, as well as coverage error, and LRAP.",
            "year": 2021,
            "venue": "Scientometrics",
            "authors": [
              {
                "authorId": "2197861164",
                "name": "Arousha Haghighian Roudsari"
              },
              {
                "authorId": "145653991",
                "name": "Jafar Afshar"
              },
              {
                "authorId": "1728685",
                "name": "Wookey Lee"
              },
              {
                "authorId": "2112280",
                "name": "Suan Lee"
              }
            ]
          }
        },
        {
          "citedcorpusid": 247292305,
          "isinfluential": false,
          "contexts": [
            "An automated ensemble learning-based framework for single-level patent classification is introduced by Kamateri et al. [38] ."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Automated Single-Label Patent Classification using Ensemble Classifiers",
            "abstract": "Many thousands of patent applications arrive at patent offices around the world every day. One important task when a patent application is submitted is to assign one or more classification codes from the complex and hierarchical patent classification schemes that will enable routing of the patent application to a patent examiner who is knowledgeable about the specific technical field. This task is typically undertaken by patent professionals, however due to the large number of applications and the potential complexity of an invention, they are usually overwhelmed. Therefore, there is a need for this code assignment manual task to be supported or even fully automated by classification systems that will classify patent applications, hopefully with an accuracy close to patent professionals. Like in many other text analysis problems, in the last years, this intellectually demanding task has been studied using word embeddings and deep learning techniques. In this paper these research efforts are shortly reviewed and re-produced with similar deep learning techniques using different feature representations on automatic patent classification in the level of sub-classes. On top of that, an innovative method of ensemble classifiers trained with different parts of the patent document is proposed. To the best of our knowledge, this is the first time that an ensemble method was proposed for the patent classification problem. Our first results are quite promising showing that an ensemble architecture of classifiers significantly outperforms current state-of-the-art techniques using the same classifiers as standalone solutions.",
            "year": 2022,
            "venue": "International Conference on Machine Learning and Computing",
            "authors": [
              {
                "authorId": "2852041",
                "name": "Eleni Kamateri"
              },
              {
                "authorId": "119177158",
                "name": "Vasileios Stamatis"
              },
              {
                "authorId": "1705272",
                "name": "K. Diamantaras"
              },
              {
                "authorId": "1786622",
                "name": "M. Salampasis"
              }
            ]
          }
        },
        {
          "citedcorpusid": 253451633,
          "isinfluential": false,
          "contexts": [
            "In recent years, there has been a growing interest in developing explainable artificial intelligence (XAI) to unveil the black-box decision-making process of DNN models in diverse fields, including image processing [12], text processing, finance [13, 14], and health applications [15, 16]."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Explainable product backorder prediction exploiting CNN: Introducing explainable models in businesses",
            "abstract": "Due to expected positive impacts on business, the application of artificial intelligence has been widely increased. The decision-making procedures of those models are often complex and not easily understandable to the company’s stakeholders, i.e. the people having to follow up on recommendations or try to understand automated decisions of a system. This opaqueness and black-box nature might hinder adoption, as users struggle to make sense and trust the predictions of AI models. Recent research on eXplainable Artificial Intelligence (XAI) focused mainly on explaining the models to AI experts with the purpose of debugging and improving the performance of the models. In this article, we explore how such systems could be made explainable to the stakeholders. For doing so, we propose a new convolutional neural network (CNN)-based explainable predictive model for product backorder prediction in inventory management. Backorders are orders that customers place for products that are currently not in stock. The company now takes the risk to produce or acquire the backordered products while in the meantime, customers can cancel their orders if that takes too long, leaving the company with unsold items in their inventory. Hence, for their strategic inventory management, companies need to make decisions based on assumptions. Our argument is that these tasks can be improved by offering explanations for AI recommendations. Hence, our research investigates how such explanations could be provided, employing Shapley additive explanations to explain the overall models’ priority in decision-making. Besides that, we introduce locally interpretable surrogate models that can explain any individual prediction of a model. The experimental results demonstrate effectiveness in predicting backorders in terms of standard evaluation metrics and outperform known related works with AUC 0.9489. Our approach demonstrates how current limitations of predictive technologies can be addressed in the business domain.",
            "year": 2022,
            "venue": "Electronic Markets",
            "authors": [
              {
                "authorId": "7842366",
                "name": "Md Shajalal"
              },
              {
                "authorId": "2149721151",
                "name": "Alexander Boden"
              },
              {
                "authorId": "2190446797",
                "name": "Gunnar Stevens"
              }
            ]
          }
        }
      ]
    },
    "249381854": {
      "citing_paper_info": {
        "title": "Impact of preprocessing and word embedding on extreme multi-label patent classification tasks",
        "abstract": "",
        "year": 2022,
        "venue": "Applied intelligence (Boston)",
        "authors": [
          {
            "authorId": "2146980790",
            "name": "Guik Jung"
          },
          {
            "authorId": "153286402",
            "name": "Junghoon Shin"
          },
          {
            "authorId": "1693603",
            "name": "Sangjun Lee"
          }
        ]
      },
      "citation_summary": {
        "citation_count": 17,
        "unique_cited_count": 17,
        "influential_count": 3,
        "detailed_records_count": 17
      },
      "cited_papers": [
        "43261268",
        "1595649",
        "209382386",
        "124360271",
        "59634627",
        "209907306",
        "38468402",
        "209949672",
        "58708650",
        "59316881",
        "165163597",
        "13402262",
        "15500867",
        "233243979",
        "204091414",
        "260840",
        "6473756"
      ],
      "citation_details": [
        {
          "citedcorpusid": 260840,
          "isinfluential": false,
          "contexts": [
            "However, to classify patent data into subdivided technology and industrial fields, it is important to evaluate these claims to differentiate patent documents [28]."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Automated Patent Classification",
            "abstract": "",
            "year": 2011,
            "venue": "Current Challenges in Patent Information Retrieval",
            "authors": [
              {
                "authorId": "2217341",
                "name": "K. Benzineb"
              },
              {
                "authorId": "145922197",
                "name": "J. Guyot"
              }
            ]
          }
        },
        {
          "citedcorpusid": 1595649,
          "isinfluential": true,
          "contexts": [
            "The Word2Vec pre-trained model learned approximately 3 million words using GoogleNews as a dataset, whereas the GloVe pre-trained model learned approximately 2.2 million words using the Common Crawl dataset [30, 31].",
            "Yang et al. [31] studied the effects of various word embedding parameters in a CNN-based model for Twitter election classification."
          ],
          "intents": [
            "['methodology']",
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Using word embeddings in Twitter election classification",
            "abstract": "Word embeddings and convolutional neural networks (CNN) have attracted extensive attention in various classification tasks for Twitter, e.g. sentiment classification. However, the effect of the configuration used to generate the word embeddings on the classification performance has not been studied in the existing literature. In this paper, using a Twitter election classification task that aims to detect election-related tweets, we investigate the impact of the background dataset used to train the embedding models, as well as the parameters of the word embedding training process, namely the context window size, the dimensionality and the number of negative samples, on the attained classification performance. By comparing the classification results of word embedding models that have been trained using different background corpora (e.g. Wikipedia articles and Twitter microposts), we show that the background data should align with the Twitter classification dataset both in data type and time period to achieve significantly better performance compared to baselines such as SVM with TF-IDF. Moreover, by evaluating the results of word embedding models trained using various context window sizes and dimensionalities, we find that large context window and dimension sizes are preferable to improve the performance. However, the number of negative samples parameter does not significantly affect the performance of the CNN classifiers. Our experimental results also show that choosing the correct word embedding model for use with CNN leads to statistically significant improvements over various baselines such as random, SVM with TF-IDF and SVM with word embeddings. Finally, for out-of-vocabulary (OOV) words that are not available in the learned word embedding models, we show that a simple OOV strategy to randomly initialise the OOV words without any prior knowledge is sufficient to attain a good classification performance among the current OOV strategies (e.g. a random initialisation using statistics of the pre-trained word embedding models).",
            "year": 2016,
            "venue": "Information Retrieval Journal",
            "authors": [
              {
                "authorId": "2112096414",
                "name": "Xiao Yang"
              },
              {
                "authorId": "145434248",
                "name": "Craig Macdonald"
              },
              {
                "authorId": "1698205",
                "name": "I. Ounis"
              }
            ]
          }
        },
        {
          "citedcorpusid": 6473756,
          "isinfluential": false,
          "contexts": [
            "Several models have been developed that have learned many words using a very large dataset [17–19]."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "A vector space model for automatic indexing",
            "abstract": "",
            "year": 1975,
            "venue": "CACM",
            "authors": [
              {
                "authorId": "1797808",
                "name": "G. Salton"
              },
              {
                "authorId": "2202149907",
                "name": "A. Wong"
              },
              {
                "authorId": "40498308",
                "name": "Chung-Shu Yang"
              }
            ]
          }
        },
        {
          "citedcorpusid": 13402262,
          "isinfluential": false,
          "contexts": [
            "Deep-learning-based word embedding processes learn words using a shallow deep learning network, which expresses the relationship and similarity between words [26, 27]."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "On the effects of using word2vec representations in neural networks for dialogue act recognition",
            "abstract": "",
            "year": 2020,
            "venue": "Computer Speech and Language",
            "authors": [
              {
                "authorId": "1786362",
                "name": "Christophe Cerisara"
              },
              {
                "authorId": "3246597",
                "name": "P. Král"
              },
              {
                "authorId": "2628715",
                "name": "Ladislav Lenc"
              }
            ]
          }
        },
        {
          "citedcorpusid": 15500867,
          "isinfluential": true,
          "contexts": [
            "Word2Vec and Global Vectors (GloVe) for word representation are two common deep learning-based word embedding approaches.",
            "min count = 3 min count = 5 Word2Vec [19] GloVe [20]",
            "GloVe makes it easier to measure the similarity between the embedded words by considering the number of words in the documents and the probability of simultaneous appearances; this method reflects the statistical information of the entire corpus.",
            "Next, we analyzed the word embedding models for five different word representations including pre-trained Word2Vec, GloVe, and skip-gram models, as well as CBoW and GloVe models that were trained using a dataset created from patent information.",
            "The word embedding models we created in this study underwent learning using GloVe, CBOW, and skip-gram.",
            "Both GloVe and Word2Vec conduct learning based on the central and surrounding words.",
            "This shows that the higher accuracy of CBOW compared to pre-trained GloVe was achieved due to the former model’s ability to learn and embed the IPC data.",
            "The Word2Vec and GloVe embedding models are described in detail in Section 3.",
            "The Word2Vec pre-trained model learned approximately 3 million words using GoogleNews as a dataset, whereas the GloVe pre-trained model learned approximately 2.2 million words using the Common Crawl dataset [30, 31].",
            "In GloVe’s learning process, the scalar product of two embedded words is the probability of their simultaneous appearance in the entire corpus .",
            "In the fields excluding the IPC, CBOW is 65.584% , which is less accurate than pre-trained GloVe.",
            "The pre-trained GloVe model contains fewer words than the GloVe and skip-gram models that we trained."
          ],
          "intents": [
            "--",
            "['background']",
            "--",
            "--",
            "--",
            "--",
            "--",
            "--",
            "--",
            "--",
            "--",
            "--"
          ],
          "cited_paper_info": {
            "title": "A C-LSTM Neural Network for Text Classification",
            "abstract": "Neural network models have been demonstrated to be capable of achieving remarkable performance in sentence and document modeling. Convolutional neural network (CNN) and recurrent neural network (RNN) are two mainstream architectures for such modeling tasks, which adopt totally different ways of understanding natural languages. In this work, we combine the strengths of both architectures and propose a novel and unified model called C-LSTM for sentence representation and text classification. C-LSTM utilizes CNN to extract a sequence of higher-level phrase representations, and are fed into a long short-term memory recurrent neural network (LSTM) to obtain the sentence representation. C-LSTM is able to capture both local features of phrases as well as global and temporal sentence semantics. We evaluate the proposed architecture on sentiment classification and question classification tasks. The experimental results show that the C-LSTM outperforms both CNN and LSTM and can achieve excellent performance on these tasks.",
            "year": 2015,
            "venue": "arXiv.org",
            "authors": [
              {
                "authorId": "2384711",
                "name": "Chunting Zhou"
              },
              {
                "authorId": "152873559",
                "name": "Chonglin Sun"
              },
              {
                "authorId": "49293587",
                "name": "Zhiyuan Liu"
              },
              {
                "authorId": "1697834",
                "name": "F. Lau"
              }
            ]
          }
        },
        {
          "citedcorpusid": 38468402,
          "isinfluential": false,
          "contexts": [
            "These patents are regarded as a strategic resource for managing information and knowledge, and they are employed in R&D activities such as the development of new products [1]."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Top management attention to innovation: The role of search selection and intensity",
            "abstract": "We develop and test an attention-based theory of search by top management teams and the influence on firm innovativeness. Using an in-depth field study of 61 publicly traded high-technology firms and their top executives, we find that the location selection and intensity of search independently and jointly influence new product",
            "year": 2009,
            "venue": "",
            "authors": [
              {
                "authorId": "47422657",
                "name": "Qiang Li"
              },
              {
                "authorId": "3141350",
                "name": "Patrick G. Maggitti"
              },
              {
                "authorId": "2800954",
                "name": "Paul E. Tesluk"
              },
              {
                "authorId": "51370984",
                "name": "R. Katila"
              }
            ]
          }
        },
        {
          "citedcorpusid": 43261268,
          "isinfluential": false,
          "contexts": [
            "However, to classify patent data into subdivided technology and industrial fields, it is important to evaluate these claims to differentiate patent documents [28]."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Current Challenges in Patent Information Retrieval",
            "abstract": "",
            "year": 2011,
            "venue": "The Information Retrieval Series",
            "authors": [
              {
                "authorId": "144307089",
                "name": "M. Lupu"
              },
              {
                "authorId": "2059169453",
                "name": "Katja Mayer"
              },
              {
                "authorId": "144959176",
                "name": "J. Tait"
              },
              {
                "authorId": "2494885",
                "name": "A. Trippe"
              }
            ]
          }
        },
        {
          "citedcorpusid": 58708650,
          "isinfluential": false,
          "contexts": [
            "Complexity reduction through the removal of stopwords is very helpful for improving the performance of classification tasks [37]."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "The importance of stop word removal on recall values in text categorization",
            "abstract": "",
            "year": 2003,
            "venue": "Proceedings of ... International Joint Conference on Neural Networks",
            "authors": [
              {
                "authorId": "38819398",
                "name": "Catarina Silva"
              },
              {
                "authorId": "2055013621",
                "name": "B. Ribeiro"
              }
            ]
          }
        },
        {
          "citedcorpusid": 59316881,
          "isinfluential": false,
          "contexts": [
            "Therefore, we previously conducted tests to validate and optimize the effects of preprocessing methods such as stemming (a technique for generalizing words by cutting the ends of words using an algorithm) and word-embedding to map words into n-dimensional vectors [7, 8]."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Evaluating word embedding models: methods and experimental results",
            "abstract": "Abstract Extensive evaluation on a large number of word embedding models for language processing applications is conducted in this work. First, we introduce popular word embedding models and discuss desired properties of word models and evaluation methods (or evaluators). Then, we categorize evaluators into intrinsic and extrinsic two types. Intrinsic evaluators test the quality of a representation independent of specific natural language processing tasks while extrinsic evaluators use word embeddings as input features to a downstream task and measure changes in performance metrics specific to that task. We report experimental results of intrinsic and extrinsic evaluators on six word embedding models. It is shown that different evaluators focus on different aspects of word models, and some are more correlated with natural language processing tasks. Finally, we adopt correlation analysis to study performance consistency of extrinsic and intrinsic evaluators.",
            "year": 2019,
            "venue": "APSIPA Transactions on Signal and Information Processing",
            "authors": [
              {
                "authorId": "144461647",
                "name": "Bin Wang"
              },
              {
                "authorId": "2116560211",
                "name": "Angela Wang"
              },
              {
                "authorId": "51233981",
                "name": "Fenxiao Chen"
              },
              {
                "authorId": "6853732",
                "name": "Yun Cheng Wang"
              },
              {
                "authorId": "9363144",
                "name": "C.-C. Jay Kuo"
              }
            ]
          }
        },
        {
          "citedcorpusid": 59634627,
          "isinfluential": false,
          "contexts": [
            "We used typical stemming algorithms including the Lancaster and Snowball algorithms [38, 39].",
            "We therefore investigate the variation of the classification performance depending on the application of the Lancaster and Snowball stemming algorithms [38, 39]."
          ],
          "intents": [
            "['methodology']",
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Snowball: A language for stemming algorithms",
            "abstract": "",
            "year": 2001,
            "venue": "",
            "authors": [
              {
                "authorId": "2057287718",
                "name": "M. Porter"
              }
            ]
          }
        },
        {
          "citedcorpusid": 124360271,
          "isinfluential": false,
          "contexts": [
            "…on Bi-directional LSTM(Bi-LSTM) to prevent the aforementioned issue as this network is less affected by vanishing gradients than LSTM. Jie et al. [33] proposed a hierarchical feature extraction model (HFEM) for the multi-label classification of patents, which performs classification using…"
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "A Hierarchical Feature Extraction Model for Multi-Label Mechanical Patent Classification",
            "abstract": "Various studies have focused on feature extraction methods for automatic patent classification in recent years. However, most of these approaches are based on the knowledge from experts in related domains. Here we propose a hierarchical feature extraction model (HFEM) for multi-label mechanical patent classification, which is able to capture both local features of phrases as well as global and temporal semantics. First, a n-gram feature extractor based on convolutional neural networks (CNNs) is designed to extract salient local lexical-level features. Next, a long dependency feature extraction model based on the bidirectional long–short-term memory (BiLSTM) neural network model is proposed to capture sequential correlations from higher-level sequence representations. Then the HFEM algorithm and its hierarchical feature extraction architecture are detailed. We establish the training, validation and test datasets, containing 72,532, 18,133, and 2679 mechanical patent documents, respectively, and then check the performance of HFEMs. Finally, we compared the results of the proposed HFEM and three other single neural network models, namely CNN, long–short-term memory (LSTM), and BiLSTM. The experimental results indicate that our proposed HFEM outperforms the other compared models in both precision and recall.",
            "year": 2018,
            "venue": "",
            "authors": [
              {
                "authorId": "145815844",
                "name": "Jie Hu"
              },
              {
                "authorId": "2124883266",
                "name": "Shaobo Li"
              },
              {
                "authorId": "50778791",
                "name": "Jianjun Hu"
              },
              {
                "authorId": "2394550",
                "name": "Guanci Yang"
              }
            ]
          }
        },
        {
          "citedcorpusid": 165163597,
          "isinfluential": true,
          "contexts": [
            "Experiments using our newly created dataset did not show as large of a performance variation as noted in the previously published LAHA study [30].",
            "2 million words using the Common Crawl dataset [30, 31].",
            "For XMTC, multiple successful deep learning models have been developed [29, 30].",
            "3 Label embedding process and network structure of the LAHA classification model’s interaction attention layer [30] 4054 G.",
            "We compared their performances to determine the most suitable model for this classification task [29, 30]."
          ],
          "intents": [
            "['methodology']",
            "['methodology']",
            "['background']",
            "['background']",
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Label-Aware Document Representation via Hybrid Attention for Extreme Multi-Label Text Classification",
            "abstract": "Extreme multi-label text classification (XMTC) aims at tagging a document with most relevant labels from an extremely large-scale label set. It is a challenging problem especially for the tail labels because there are only few training documents to build classifier. This paper is motivated to better explore the semantic relationship between each document and extreme labels by taking advantage of both document content and label correlation. Our objective is to establish an explicit label-aware representation for each document with a hybrid attention deep neural network model(LAHA). LAHA consists of three parts. The first part adopts a multi-label self-attention mechanism to detect the contribution of each word to labels. The second part exploits the label structure and document content to determine the semantic connection between words and labels in a same latent space. An adaptive fusion strategy is designed in the third part to obtain the final label-aware document representation so that the essence of previous two parts can be sufficiently integrated. Extensive experiments have been conducted on six benchmark datasets by comparing with the state-of-the-art methods. The results show the superiority of our proposed LAHA method, especially on the tail labels.",
            "year": 2019,
            "venue": "Neural Processing Letters",
            "authors": [
              {
                "authorId": null,
                "name": "Xin Huang"
              },
              {
                "authorId": "3058753",
                "name": "Boli Chen"
              },
              {
                "authorId": "145942106",
                "name": "Lin Xiao"
              },
              {
                "authorId": "144889532",
                "name": "L. Jing"
              }
            ]
          }
        },
        {
          "citedcorpusid": 204091414,
          "isinfluential": false,
          "contexts": [
            "Therefore, we previously conducted tests to validate and optimize the effects of preprocessing methods such as stemming (a technique for generalizing words by cutting the ends of words using an algorithm) and word-embedding to map words into n-dimensional vectors [7, 8]."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "A Comparative Study of Stemming Algorithms",
            "abstract": "Stemming is a pre-processing step in Text Mining applications as well as a very common requirement of Natural Language processing functions. In fact it is very important in most of the Information Retrieval systems. The main purpose of stemming is to reduce different grammatical forms / word forms of a word like its noun, adjective, verb, adverb etc. to its root form. We can say that the goal of stemming is to reduce inflectional forms and sometimes derivationally related forms of a word to a common base form. In this paper we have discussed different methods of stemming and their comparisons in terms of usage, advantages as well as limitations. The basic difference between stemming and lemmatization is also discussed. Keywordsstemming; text mining; NLP; IR; suffix",
            "year": 2011,
            "venue": "",
            "authors": [
              {
                "authorId": "144342710",
                "name": "A. Jivani"
              }
            ]
          }
        },
        {
          "citedcorpusid": 209382386,
          "isinfluential": false,
          "contexts": [
            "Convolution neural networks (CNNs) and recurrent neural networks (RNNs) [20–22] are two deep learning models that have achieved successful text classification."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Improving BERT-Based Text Classification With Auxiliary Sentence and Domain Knowledge",
            "abstract": "General language model BERT pre-trained on cross-domain text corpus, BookCorpus and Wikipedia, achieves excellent performance on a couple of natural language processing tasks through the way of fine-tuning in the downstream tasks. But it still lacks of task-specific knowledge and domain-related knowledge for further improving the performance of BERT model and more detailed fine-tuning strategy analyses are necessary. To address these problem, a BERT-based text classification model BERT4TC is proposed via constructing auxiliary sentence to turn the classification task into a binary sentence-pair one, aiming to address the limited training data problem and task-awareness problem. The architecture and implementation details of BERT4TC are also presented, as well as a post-training approach for addressing the domain challenge of BERT. Finally, extensive experiments are conducted on seven public widely-studied datasets for analyzing the fine-tuning strategies from the perspectives of learning rate, sequence length and hidden state vector selection. After that, BERT4TC models with different auxiliary sentences and post-training objectives are compared and analyzed in depth. The experiment results show that BERT4TC with suitable auxiliary sentence significantly outperforms both typical feature-based methods and fine-tuning methods, and achieves new state-of-the-art performance on multi-class classification datasets. For binary sentiment classification datasets, our BERT4TC post-trained with suitable domain-related corpus also achieves better results compared with original BERT model.",
            "year": 2019,
            "venue": "IEEE Access",
            "authors": [
              {
                "authorId": "2116001376",
                "name": "Shanshan Yu"
              },
              {
                "authorId": "2743493",
                "name": "Jindian Su"
              },
              {
                "authorId": "2061548356",
                "name": "Da Luo"
              }
            ]
          }
        },
        {
          "citedcorpusid": 209907306,
          "isinfluential": false,
          "contexts": [
            "In general, predictions for classes with a small amount of data exhibit a relatively low accuracy compared to classes with considerable data [36]."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Data imbalance in classification: Experimental evaluation",
            "abstract": "",
            "year": 2020,
            "venue": "Information Sciences",
            "authors": [
              {
                "authorId": "1692763",
                "name": "F. Thabtah"
              },
              {
                "authorId": "2651105",
                "name": "Suhel Hammoud"
              },
              {
                "authorId": "2258772353",
                "name": "Firuz Kamalov"
              },
              {
                "authorId": "153536286",
                "name": "Amanda Gonsalves"
              }
            ]
          }
        },
        {
          "citedcorpusid": 209949672,
          "isinfluential": false,
          "contexts": [
            "[32] conducted an experiment to compare classification performances depending on the combination of pre-trained word embedding models and several deep learning-based models for Turkish text classification tasks."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Improving the accuracy using pre-trained word embeddings on deep neural networks for Turkish text classification",
            "abstract": "",
            "year": 2020,
            "venue": "",
            "authors": [
              {
                "authorId": "1411169666",
                "name": "Murat Aydoğan"
              },
              {
                "authorId": "3173758",
                "name": "A. Karcı"
              }
            ]
          }
        },
        {
          "citedcorpusid": 233243979,
          "isinfluential": false,
          "contexts": [
            "Both evaluation indices are widely used in multi-label classification tasks and are defined as follows [41, 42]: nDCG @ n = DCG @ n iDCG @ n (4) In P @ n , y ∈ 0 , 1 n indicates the ground truth label for each datum and r n ( ˆ y) indicates the index of the label when n items with the highest…"
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Effective Collaborative Representation Learning for Multilabel Text Categorization",
            "abstract": "With the booming of deep learning, massive attention has been paid to developing neural models for multilabel text categorization (MLTC). Most of the works concentrate on disclosing word–label relationship, while less attention is taken in exploiting global clues, particularly with the relationship of document–label. To address this limitation, we propose an effective collaborative representation learning (CRL) model in this article. CRL consists of a factorization component for generating shallow representations of documents and a neural component for deep text-encoding and classification. We have developed strategies for jointly training those two components, including an alternating-least-squares-based approach for factorizing the pointwise mutual information (PMI) matrix of label–document and multitask learning (MTL) strategy for the neural component. According to the experimental results on six data sets, CRL can explicitly take advantage of the relationship of document–label and achieve competitive classification performance in comparison with some state-of-the-art deep methods.",
            "year": 2021,
            "venue": "IEEE Transactions on Neural Networks and Learning Systems",
            "authors": [
              {
                "authorId": "30566977",
                "name": "Hao Wu"
              },
              {
                "authorId": "2054558993",
                "name": "Shaowei Qin"
              },
              {
                "authorId": "6019653",
                "name": "Rencan Nie"
              },
              {
                "authorId": "34767970",
                "name": "Jinde Cao"
              },
              {
                "authorId": "2074757533",
                "name": "Sergey Gorbachev"
              }
            ]
          }
        }
      ]
    },
    "257366455": {
      "citing_paper_info": {
        "title": "Patent Classification Using BERT-for-Patents on USPTO",
        "abstract": "Domain-specific complex nature of patent text, unique drafting styles of patent applicants, and mammoth volume of patent data makes classification a challenging task. To become a helping hand, in the recent time, Google has released pre-trained BERT model trained over 100 million patent documents. However, to the best of our knowledge, there has not been any testament about prediction capabilities and performance of the BERT-for-Patents model over any patent tasks on standard benchmarks. Our work addresses this problem, investigates BERT-for-patents in multi-label patent classification at both CPC and IPC sub-class level. Evidence from experiments enables us to claim that, this work outperformed SOTA by an absolute 2% on micro-F1 with a newly proposed USPTO 2.8M dataset. In order to introduce robustness to the classification process, our collaborative Machine Learning models including NB and SVM uplifted the micro-F1 measures to 70%. This work stands as a corroboration to promote development of patent-specific language models and also claims, robustness in patent analysis tasks can be achieved by not forgetting plain old Machine Learning models. The contributions of this work including code, models, and a novel dataset of the size 2.8M with patent claims are released to the public1, in order to nurture the patent community in developing AI solutions.",
        "year": 2022,
        "venue": "MLNLP",
        "authors": [
          {
            "authorId": "2047942384",
            "name": "Renukswamy Chikkamath"
          },
          {
            "authorId": "2141022879",
            "name": "Vishvapalsinhji Ramsinh Parmar"
          },
          {
            "authorId": "2210802196",
            "name": "Yasser Otiefy"
          },
          {
            "authorId": "36634859",
            "name": "M. Endres"
          }
        ]
      },
      "citation_summary": {
        "citation_count": 7,
        "unique_cited_count": 7,
        "influential_count": 3,
        "detailed_records_count": 7
      },
      "cited_papers": [
        "6628106",
        "251496240",
        "1009397",
        "52273103",
        "174799315",
        "245343990",
        "233583623"
      ],
      "citation_details": [
        {
          "citedcorpusid": 1009397,
          "isinfluential": false,
          "contexts": [
            "We have used binary cross entropy loss [8], and found that converting the problem from multi-class classification using softmax cross entropy to binary classification using binary cross entropy loss will be more suitable for our problem."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Deep Learning for Extreme Multi-label Text Classification",
            "abstract": "",
            "year": 2017,
            "venue": "Annual International ACM SIGIR Conference on Research and Development in Information Retrieval",
            "authors": [
              {
                "authorId": null,
                "name": "Jingzhou Liu"
              },
              {
                "authorId": "1702500",
                "name": "Wei-Cheng Chang"
              },
              {
                "authorId": "9287688",
                "name": "Yuexin Wu"
              },
              {
                "authorId": "35729970",
                "name": "Yiming Yang"
              }
            ]
          }
        },
        {
          "citedcorpusid": 6628106,
          "isinfluential": false,
          "contexts": [
            "For the optimization purpose, the model has used the ADAM optimizer [4]."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Adam: A Method for Stochastic Optimization",
            "abstract": "We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm.",
            "year": 2014,
            "venue": "International Conference on Learning Representations",
            "authors": [
              {
                "authorId": "1726807",
                "name": "Diederik P. Kingma"
              },
              {
                "authorId": "2503659",
                "name": "Jimmy Ba"
              }
            ]
          }
        },
        {
          "citedcorpusid": 52273103,
          "isinfluential": true,
          "contexts": [
            "The results state that Precision, Recall, and F1 scores were approximately 31%, 72%, and 41% respectively [7].",
            "The generated model is called DeepPatent [7].",
            "In 2018, Li et al. [7] extracted the data from USPTO bulk data and produced a new benchmark dataset for patent classification."
          ],
          "intents": [
            "['background']",
            "--",
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "DeepPatent: patent classification with convolutional neural networks and word embedding",
            "abstract": "",
            "year": 2018,
            "venue": "Scientometrics",
            "authors": [
              {
                "authorId": "2124883266",
                "name": "Shaobo Li"
              },
              {
                "authorId": "145815844",
                "name": "Jie Hu"
              },
              {
                "authorId": "5925243",
                "name": "Yuxin Cui"
              },
              {
                "authorId": "50778791",
                "name": "Jianjun Hu"
              }
            ]
          }
        },
        {
          "citedcorpusid": 174799315,
          "isinfluential": true,
          "contexts": [
            "Therefore, training on Abstract SOTA outperforms our model when evaluation on top-k, but our model achieved a way higher performance with threshold of 0.2 with 65.66% micro F1-score.",
            "PatentBERT [6] IPC 632+Claim USPTO 2M 2006∼2014 2015-A 79.",
            "The model has a total of 12 layers, trained on 110 million parameters, followed by fine-tuning and named PatentBERT [6] which further involves two tasks.",
            "80 Top 1 PatentBERT [6] CPC+Claim 3M 2000∼2014 2015-B 84.",
            "83 Top 1 PatentBERT [6] CPC+Claim 2M 2006∼2014 2015-B 84.",
            "91 Top 1 PatentBERT [6] IPC+Title+Abstract USPTO 2M 2006∼2014 2015-A 32.",
            "71 Top 1 PatentBERT [6] IPC 632+Title+Abstract USPTO 2M 2006∼2014 2015-A 80.",
            "For instance, Table 8 shows we have achieved 64.04% of micro-F1, which is not far from the SOTA [6] with microF1 64.91%.",
            "Our results (considered 664 labels at sub-class) outperformed the SOTA (only 632 sub-class labels) by using the new dataset we have generated (USPTO 2.8M), and codes relating to this work including datasets are released to the community.",
            "74 Top 1 PatentBERT [6] CPC+Claim 2M 2006∼2014 2015-A 84.",
            "So to avoid that, it is always recommended to use the stratified splits, iii) although this work (utilizing BERT-for-Patents based on BERT Large, trained on patent data) outperformed the SOTA (utilizing PatentBERT based on BERT Base trained on a wiki like natural English language) over 2M samples data from our new USPTO 2.8M, it is still left unanswered that BERT-for-Patents did not outperform the PatentBERT\non USPTO 2M.",
            "Lee et al. [6] achieved the SOTA results, in this section, we will discuss them in detail.",
            "Our fine-tuned model trained on claims with 664 classes achieved 63.09% micro F1-Score when using top-2 classes as the final prediction which is smaller than the current SOTAwhich achieved 63.74%.",
            "To bring the robustness in classification, we also showed that classic Machine\n4https://github.com/google/patents-public-data/blob/master/models/BERT%20for% 20Patents.md\nLearning models are robust in nature and also outperformed the state-of-the-art (SOTA) by an absolute 3% on USPTO 2.8 M dataset.",
            "[6] achieved the SOTA results, in this section, we will discuss them in detail.",
            "We found that by using threshold 0.3 it achieved 68.79% micro F1-score which outperform SOTA model with about 5% difference.",
            "04% of micro-F1, which is not far from the SOTA [6] with microF1 64.",
            "To this end, we refer only to the recent scientific publications in this direction and also a few SOTA papers in order to compare our models to the standard patent classification benchmarks.",
            "Also, in order to have justifiable comparision, we have considered only 2M samples from USPTO 2.8M and fine-tuned BERT-for-Patents, and this also outperformed the SOTA benchmarks.",
            "This can be a future work to reiterate and reproduce the work from SOTA once again and investigate this problem, iv) although measuring the performances of the models at top@k levels depends on the downstream tasks, it is still unclear which type of model is more preferred in the patent domain from the practitioners such as examiners for analysis, because at the sub-class level, using threshold gives better results in comparison to top@k.",
            "The result proclaims that PatentBERT surpasses DeepPatent by the margin of approximately 11% considering the Precision score at Top@1 [6]."
          ],
          "intents": [
            "--",
            "--",
            "['methodology']",
            "--",
            "--",
            "--",
            "--",
            "--",
            "--",
            "--",
            "--",
            "--",
            "--",
            "--",
            "['background']",
            "--",
            "['background']",
            "--",
            "--",
            "--",
            "['background']"
          ],
          "cited_paper_info": {
            "title": "PatentBERT: Patent Classification with Fine-Tuning a pre-trained BERT Model",
            "abstract": "In this work we focus on fine-tuning a pre-trained BERT model and applying it to patent classification. When applied to large datasets of over two millions patents, our approach outperforms the state of the art by an approach using CNN with word embeddings. In addition, we focus on patent claims without other parts in patent documents. Our contributions include: (1) a new state-of-the-art method based on pre-trained BERT model and fine-tuning for patent classification, (2) a large dataset USPTO-3M at the CPC subclass level with SQL statements that can be used by future researchers, (3) showing that patent claims alone are sufficient for classification task, in contrast to conventional wisdom.",
            "year": 2019,
            "venue": "arXiv.org",
            "authors": [
              {
                "authorId": "2387987",
                "name": "Jieh-Sheng Lee"
              },
              {
                "authorId": "1798127",
                "name": "J. Hsiang"
              }
            ]
          }
        },
        {
          "citedcorpusid": 233583623,
          "isinfluential": false,
          "contexts": [
            "Some of the articles which we recommend to the patent practitioners are as follows; Krestel et al [5] presented a survey to know the wide spectrum of patent analysis in general using Deep Learning, and some of the classification use cases study can be found in [3]."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "A survey on deep learning for patent analysis",
            "abstract": "",
            "year": 2021,
            "venue": "World Patent Information",
            "authors": [
              {
                "authorId": "3264110",
                "name": "Ralf Krestel"
              },
              {
                "authorId": "2047942384",
                "name": "Renukswamy Chikkamath"
              },
              {
                "authorId": "2042640458",
                "name": "Christoph Hewel"
              },
              {
                "authorId": "1695993",
                "name": "Julian Risch"
              }
            ]
          }
        },
        {
          "citedcorpusid": 245343990,
          "isinfluential": true,
          "contexts": [
            "11 Top 1 PatentNet (XLNet) [2] IPC 544+Title+Abstract USPTO 2M 2006∼2014 2015-A 82.",
            "[2] proposed to pre-train various language models, namely, BERT, XLNet, Roberta, and ELECTRA on the standard benchmark such as USPTO 2M and M-Patent datasets.",
            "25 PatentNet (BERT) [2] IPC 544+Title+Abstract USPTO 2M 2006∼2014 2015-A 82.",
            "33 Top 1 PatentNet (RoBERTa) [2] IPC 544+Title+Abstract USPTO 2M 2006∼2014 2015-A 82.",
            "10 Top 1 PatentNet (ELECTRA) [2] IPC 544+Title+Abstract USPTO 2M 2006∼2014 2015-A 82."
          ],
          "intents": [
            "--",
            "['methodology']",
            "--",
            "--",
            "--"
          ],
          "cited_paper_info": {
            "title": "PatentNet: multi-label classification of patent documents using deep learning based language understanding",
            "abstract": "Patent classification is an expensive and time-consuming task that has conventionally been performed by domain experts. However, the increase in the number of filed patents and the complexity of the documents make the classification task challenging. The text used in patent documents is not always written in a way to efficiently convey knowledge. Moreover, patent classification is a multi-label classification task with a large number of labels, which makes the problem even more complicated. Hence, automating this expensive and laborious task is essential for assisting domain experts in managing patent documents, facilitating reliable search, retrieval, and further patent analysis tasks. Transfer learning and pre-trained language models have recently achieved state-of-the-art results in many Natural Language Processing tasks. In this work, we focus on investigating the effect of fine-tuning the pre-trained language models, namely, BERT, XLNet, RoBERTa, and ELECTRA, for the essential task of multi-label patent classification. We compare these models with the baseline deep-learning approaches used for patent classification. We use various word embeddings to enhance the performance of the baseline models. The publicly available USPTO-2M patent classification benchmark and M-patent datasets are used for conducting experiments. We conclude that fine-tuning the pre-trained language models on the patent text improves the multi-label patent classification performance. Our findings indicate that XLNet performs the best and achieves a new state-of-the-art classification performance with respect to precision, recall, F1 measure, as well as coverage error, and LRAP.",
            "year": 2021,
            "venue": "Scientometrics",
            "authors": [
              {
                "authorId": "2197861164",
                "name": "Arousha Haghighian Roudsari"
              },
              {
                "authorId": "145653991",
                "name": "Jafar Afshar"
              },
              {
                "authorId": "1728685",
                "name": "Wookey Lee"
              },
              {
                "authorId": "2112280",
                "name": "Suan Lee"
              }
            ]
          }
        },
        {
          "citedcorpusid": 251496240,
          "isinfluential": false,
          "contexts": [
            "Some of the articles which we recommend to the patent practitioners are as follows; Krestel et al [5] presented a survey to know the wide spectrum of patent analysis in general using Deep Learning, and some of the classification use cases study can be found in [3]."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "A Use Case of Patent Classification Using Deep Learning with Transfer Learning",
            "abstract": "Abstract Purpose Patent classification is one of the areas in Intellectual Property Analytics (IPA), and a growing use case since the number of patent applications has been increasing worldwide. We propose using machine learning algorithms to classify Portuguese patents and evaluate the performance of transfer learning methodologies to solve this task. Design/methodology/approach We applied three different approaches in this paper. First, we used a dataset available by INPI to explore traditional machine learning algorithms and ensemble methods. After preprocessing data by applying TF-IDF, FastText and Doc2Vec, the models were evaluated by cross-validation in 5 folds. In a second approach, we used two different Neural Networks architectures, a Convolutional Neural Network (CNN) and a bi-directional Long Short-Term Memory (BiLSTM). Finally, we used pre-trained BERT, DistilBERT, and ULMFiT models in the third approach. Findings BERTTimbau, a BERT architecture model pre-trained on a large Portuguese corpus, presented the best results for the task, even though with a performance of only 4% superior to a LinearSVC model using TF-IDF feature engineering. Research limitations The dataset was highly imbalanced, as usual in patent applications, so the classes with the lowest samples were expected to present the worst performance. That result happened in some cases, especially in classes with less than 60 training samples. Practical implications Patent classification is challenging because of the hierarchical classification system, the context overlap, and the underrepresentation of the classes. However, the final model presented an acceptable performance given the size of the dataset and the task complexity. This model can support the decision and improve the time by proposing a category in the second level of ICP, which is one of the critical phases of the grant patent process. Originality/value To our knowledge, the proposed models were never implemented for Portuguese patent classification.",
            "year": 2022,
            "venue": "Journal of Data and Information Science",
            "authors": [
              {
                "authorId": "39803194",
                "name": "R. Henriques"
              },
              {
                "authorId": "2181499221",
                "name": "Adria Ferreira"
              },
              {
                "authorId": "1883851",
                "name": "M. Castelli"
              }
            ]
          }
        }
      ]
    },
    "247292305": {
      "citing_paper_info": {
        "title": "Automated Single-Label Patent Classification using Ensemble Classifiers",
        "abstract": "Many thousands of patent applications arrive at patent offices around the world every day. One important task when a patent application is submitted is to assign one or more classification codes from the complex and hierarchical patent classification schemes that will enable routing of the patent application to a patent examiner who is knowledgeable about the specific technical field. This task is typically undertaken by patent professionals, however due to the large number of applications and the potential complexity of an invention, they are usually overwhelmed. Therefore, there is a need for this code assignment manual task to be supported or even fully automated by classification systems that will classify patent applications, hopefully with an accuracy close to patent professionals. Like in many other text analysis problems, in the last years, this intellectually demanding task has been studied using word embeddings and deep learning techniques. In this paper these research efforts are shortly reviewed and re-produced with similar deep learning techniques using different feature representations on automatic patent classification in the level of sub-classes. On top of that, an innovative method of ensemble classifiers trained with different parts of the patent document is proposed. To the best of our knowledge, this is the first time that an ensemble method was proposed for the patent classification problem. Our first results are quite promising showing that an ensemble architecture of classifiers significantly outperforms current state-of-the-art techniques using the same classifiers as standalone solutions.",
        "year": 2022,
        "venue": "International Conference on Machine Learning and Computing",
        "authors": [
          {
            "authorId": "2852041",
            "name": "Eleni Kamateri"
          },
          {
            "authorId": "119177158",
            "name": "Vasileios Stamatis"
          },
          {
            "authorId": "1705272",
            "name": "K. Diamantaras"
          },
          {
            "authorId": "1786622",
            "name": "M. Salampasis"
          }
        ]
      },
      "citation_summary": {
        "citation_count": 5,
        "unique_cited_count": 5,
        "influential_count": 2,
        "detailed_records_count": 5
      },
      "cited_papers": [
        "18463704",
        "237654988",
        "132667166",
        "67737421",
        "220884973"
      ],
      "citation_details": [
        {
          "citedcorpusid": 18463704,
          "isinfluential": false,
          "contexts": [
            "[2] applied stop word removal, stemming, dimensionality reduction and removal of rare terms, and they sent the processed output into a neural network, named HITEC, which copies the tree-like structure of the taxonomy."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "A Hierarchical Online Classifier for Patent Categorization",
            "abstract": "Patent categorization (PC) is a typical application area of text categorization (TC). TC can be applied in different scenarios at the work of patent offices depending on at what stage the categorization is needed. This is a challenging field for TC algorithms, since the applications have to deal simultaneously with large number of categories (in the magnitude of 1000–10000) organized in hierarchy, large number of long documents with huge vocabularies at training, and they are required to work fast and accurate at on-the-fly categorization. In this paper we present a hierarchical online classifier, called HITEC, which meets the above requirements. The novelty of the method lies in the taxonomy dependent architecture of the classifier, the applied weight updating scheme, and in the relaxed category selection method. We evaluate the presented method on two large English patent application databases, the WIPO-alpha and the Espace A/B corpora. We also compare the method to other TC algorithms on these collections, and show that it outperforms them significantly.",
            "year": 2007,
            "venue": "",
            "authors": [
              {
                "authorId": "1754164",
                "name": "D. Tikk"
              },
              {
                "authorId": "47277666",
                "name": "G. Biró"
              },
              {
                "authorId": "2893823",
                "name": "A. Törcsvári"
              }
            ]
          }
        },
        {
          "citedcorpusid": 67737421,
          "isinfluential": false,
          "contexts": [
            "Recently, automated patent classification, which falls under the broad field of text classification [4, 5], has been examined using"
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "A Data-driven Approach to the Automatic Classification of Korean Poetry",
            "abstract": "Automatic classification of text is an increasingly important area of research. It has important applications in virtual assistants and recommender systems. Among the different types of literary works, the poem is one of the most difficult to classify automatically because of the prolific use of metaphors and the short length. In this research, we propose a data-driven approach to automatically classify Korean poems. We use three different methods for finding keywords which can train the classifiers. Our results show that the proposed approach can produce better classification accuracy than using a predefined list of keywords created by a human expert. Index Terms — automatic classification, data-driven, poem, text mining, Korean text, keyword extraction",
            "year": 2017,
            "venue": "",
            "authors": [
              {
                "authorId": "2054962266",
                "name": "J. H. Nam"
              },
              {
                "authorId": "2275441",
                "name": "K. Yow"
              }
            ]
          }
        },
        {
          "citedcorpusid": 132667166,
          "isinfluential": true,
          "contexts": [
            "The method of retrieving the first “X” words for the representation of a patent document is followed in several literature works [4-7].",
            "Recently, automated patent classification has been examined using deep learning (DL) methods such as convolutional neural networks (CNN) [6, 13, 14], Word2Vec and Long-short term memory (LSTM) [4, 5, 10] and other DL methods [7] to predict the most representative classification code(s) for a patent.",
            "Risch and Krestel [12] used the FastText embedding that was trained on a patent dataset together with bi-directional GRUs (another type of RNN) to achieve better performance compared with Word Embeddings trained on Wiki documents.",
            "The bi-directional LSTM achieves better classification performance than almost all similar state-of-the-art methods [15, 14, 7, 3], except for [4] which exploits a language model trained on a domain-specific dataset.",
            "Risch and Krestel [7] used the FastText embedding that was trained on a patent dataset together with bi-directional GRUs (another type of RNN) to achieve better performance compared with Word Embeddings trained on Wiki documents.",
            "Moreover, the ensemble of three identical classifiers trained with different patent text achieves better classification accuracy compared to the results provided by state-of-the-art research efforts [3, 4, 7, 14, 15].",
            "the title, abstract claims and description [7, 10, 12, 14]."
          ],
          "intents": [
            "['methodology']",
            "['methodology']",
            "--",
            "['background']",
            "['methodology']",
            "['result']",
            "--"
          ],
          "cited_paper_info": {
            "title": "Domain-specific word embeddings for patent classification",
            "abstract": "\nPurpose\nPatent offices and other stakeholders in the patent domain need to classify patent applications according to a standardized classification scheme. The purpose of this paper is to examine the novelty of an application it can then be compared to previously granted patents in the same class. Automatic classification would be highly beneficial, because of the large volume of patents and the domain-specific knowledge needed to accomplish this costly manual task. However, a challenge for the automation is patent-specific language use, such as special vocabulary and phrases.\n\n\nDesign/methodology/approach\nTo account for this language use, the authors present domain-specific pre-trained word embeddings for the patent domain. The authors train the model on a very large data set of more than 5m patents and evaluate it at the task of patent classification. To this end, the authors propose a deep learning approach based on gated recurrent units for automatic patent classification built on the trained word embeddings.\n\n\nFindings\nExperiments on a standardized evaluation data set show that the approach increases average precision for patent classification by 17 percent compared to state-of-the-art approaches. In this paper, the authors further investigate the model’s strengths and weaknesses. An extensive error analysis reveals that the learned embeddings indeed mirror patent-specific language use. The imbalanced training data and underrepresented classes are the most difficult remaining challenge.\n\n\nOriginality/value\nThe proposed approach fulfills the need for domain-specific word embeddings for downstream tasks in the patent domain, such as patent classification or patent analysis.\n",
            "year": 2019,
            "venue": "Data Technologies and Applications",
            "authors": [
              {
                "authorId": "1695993",
                "name": "Julian Risch"
              },
              {
                "authorId": "3264110",
                "name": "Ralf Krestel"
              }
            ]
          }
        },
        {
          "citedcorpusid": 220884973,
          "isinfluential": false,
          "contexts": [
            "[3] used a sequence-to-sequence neural network with label embeddings to generate a sequence of labels for all levels where the embeddings of previous level are considered for generating the next labels.",
            "The bi-directional LSTM achieves better classification performance than almost all similar state-of-the-art methods [15, 14, 7, 3], except for [4] which exploits a language model trained on a domain-specific dataset.",
            "Moreover, the ensemble of three identical classifiers trained with different patent text achieves better classification accuracy compared to the results provided by state-of-the-art research efforts [3, 4, 7, 14, 15]."
          ],
          "intents": [
            "['methodology']",
            "['background']",
            "['result']"
          ],
          "cited_paper_info": {
            "title": "Hierarchical Document Classification as a Sequence Generation Task",
            "abstract": "Hierarchical classification schemes are an effective and natural way to organize large document collections. However, complex schemes make the manual classification time-consuming and require domain experts. Current machine learning approaches for hierarchical classification do not exploit all the information contained in the hierarchical schemes. During training, they do not make full use of the inherent parent-child relation of classes. For example, they neglect to tailor document representations, such as embeddings, to each individual hierarchy level. Our model overcomes these problems by addressing hierarchical classification as a sequence generation task. To this end, our neural network transforms a sequence of input words into a sequence of labels, which represents a path through a tree-structured hierarchy scheme. The evaluation uses a patent corpus, which exhibits a complex class hierarchy scheme and high-quality annotations from domain experts and comprises millions of documents. We re-implemented five models from related work and show that our basic model achieves competitive results in comparison with the best approach. A variation of our model that uses the recent Transformer architecture outperforms the other approaches. The error analysis reveals that the encoder of our model has the strongest influence on its classification performance.",
            "year": 2020,
            "venue": "ACM/IEEE Joint Conference on Digital Libraries",
            "authors": [
              {
                "authorId": "1695993",
                "name": "Julian Risch"
              },
              {
                "authorId": "51878929",
                "name": "Samuele Garda"
              },
              {
                "authorId": "3264110",
                "name": "Ralf Krestel"
              }
            ]
          }
        },
        {
          "citedcorpusid": 237654988,
          "isinfluential": true,
          "contexts": [
            "Recently, automated patent classification has been examined using deep learning (DL) methods such as convolutional neural networks (CNN) [6, 13, 14], Word2Vec and Long-short term memory (LSTM) [4, 5, 10] and other DL methods [7] to predict the most representative classification code(s) for a patent.",
            "Moreover, Sofean [11] developed a self-trained Word Embedding trained on million patents and then used a LSTM network to perform patent classification.",
            "For evaluating his method, Sofean kept only the sub-classes appearing in more than 500 documents, resulting to 43 sub-classes, and obtained an accuracy of 67%.",
            "Moreover, Sofean [10] developed a self-trained Word Embedding trained on million patents and then used a LSTM network to perform patent classification.",
            "the title, abstract claims and description [7, 10, 12, 14]."
          ],
          "intents": [
            "['methodology']",
            "--",
            "--",
            "['methodology']",
            "--"
          ],
          "cited_paper_info": {
            "title": "Deep Learning based Pipeline with Multichannel Inputs for Patent Classication",
            "abstract": "",
            "year": 2021,
            "venue": "PatentSemTech@SEMANTiCS",
            "authors": [
              {
                "authorId": "2286136",
                "name": "Mustafa Sofean"
              }
            ]
          }
        }
      ]
    },
    "251496240": {
      "citing_paper_info": {
        "title": "A Use Case of Patent Classification Using Deep Learning with Transfer Learning",
        "abstract": "Abstract Purpose Patent classification is one of the areas in Intellectual Property Analytics (IPA), and a growing use case since the number of patent applications has been increasing worldwide. We propose using machine learning algorithms to classify Portuguese patents and evaluate the performance of transfer learning methodologies to solve this task. Design/methodology/approach We applied three different approaches in this paper. First, we used a dataset available by INPI to explore traditional machine learning algorithms and ensemble methods. After preprocessing data by applying TF-IDF, FastText and Doc2Vec, the models were evaluated by cross-validation in 5 folds. In a second approach, we used two different Neural Networks architectures, a Convolutional Neural Network (CNN) and a bi-directional Long Short-Term Memory (BiLSTM). Finally, we used pre-trained BERT, DistilBERT, and ULMFiT models in the third approach. Findings BERTTimbau, a BERT architecture model pre-trained on a large Portuguese corpus, presented the best results for the task, even though with a performance of only 4% superior to a LinearSVC model using TF-IDF feature engineering. Research limitations The dataset was highly imbalanced, as usual in patent applications, so the classes with the lowest samples were expected to present the worst performance. That result happened in some cases, especially in classes with less than 60 training samples. Practical implications Patent classification is challenging because of the hierarchical classification system, the context overlap, and the underrepresentation of the classes. However, the final model presented an acceptable performance given the size of the dataset and the task complexity. This model can support the decision and improve the time by proposing a category in the second level of ICP, which is one of the critical phases of the grant patent process. Originality/value To our knowledge, the proposed models were never implemented for Portuguese patent classification.",
        "year": 2022,
        "venue": "Journal of Data and Information Science",
        "authors": [
          {
            "authorId": "39803194",
            "name": "R. Henriques"
          },
          {
            "authorId": "2181499221",
            "name": "Adria Ferreira"
          },
          {
            "authorId": "1883851",
            "name": "M. Castelli"
          }
        ]
      },
      "citation_summary": {
        "citation_count": 17,
        "unique_cited_count": 15,
        "influential_count": 4,
        "detailed_records_count": 17
      },
      "cited_papers": [
        "740063",
        "119297355",
        "14012633",
        "35058596",
        "52967399",
        "262553219",
        "67426382",
        "132667166",
        "212516914",
        "207847753",
        "218489934",
        "215043",
        "3626819",
        "158036236",
        "5782890"
      ],
      "citation_details": [
        {
          "citedcorpusid": 215043,
          "isinfluential": false,
          "contexts": [
            "Regardless of not having found any study about patent classification in Portuguese, there are several studies about Portuguese text classification using machine learning algorithms (Gonçalves et al., 2006).",
            "Journal of Data and Information Science Regardless of not having found any study about patent classification in Portuguese, there are several studies about Portuguese text classification using machine learning algorithms (Gonçalves et al., 2006)."
          ],
          "intents": [
            "['background']",
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Analysing Part-of-Speech for Portuguese Text Classification",
            "abstract": "",
            "year": 2006,
            "venue": "Conference on Intelligent Text Processing and Computational Linguistics",
            "authors": [
              {
                "authorId": "144861186",
                "name": "Teresa Gonçalves"
              },
              {
                "authorId": "8401652",
                "name": "C. F. D. Silva"
              },
              {
                "authorId": "1792648",
                "name": "P. Quaresma"
              },
              {
                "authorId": "144845513",
                "name": "R. Vieira"
              }
            ]
          }
        },
        {
          "citedcorpusid": 740063,
          "isinfluential": false,
          "contexts": [
            "The transfer learning approach and pre-trained language models have been achieving the state-of-art in different tasks (Pan & Yang, 2010; Zhuang et al., 2021)."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "A Survey on Transfer Learning",
            "abstract": "A major assumption in many machine learning and data mining algorithms is that the training and future data must be in the same feature space and have the same distribution. However, in many real-world applications, this assumption may not hold. For example, we sometimes have a classification task in one domain of interest, but we only have sufficient training data in another domain of interest, where the latter data may be in a different feature space or follow a different data distribution. In such cases, knowledge transfer, if done successfully, would greatly improve the performance of learning by avoiding much expensive data-labeling efforts. In recent years, transfer learning has emerged as a new learning framework to address this problem. This survey focuses on categorizing and reviewing the current progress on transfer learning for classification, regression, and clustering problems. In this survey, we discuss the relationship between transfer learning and other related machine learning techniques such as domain adaptation, multitask learning and sample selection bias, as well as covariate shift. We also explore some potential future issues in transfer learning research.",
            "year": 2010,
            "venue": "IEEE Transactions on Knowledge and Data Engineering",
            "authors": [
              {
                "authorId": "1746914",
                "name": "Sinno Jialin Pan"
              },
              {
                "authorId": "152290618",
                "name": "Qiang Yang"
              }
            ]
          }
        },
        {
          "citedcorpusid": 3626819,
          "isinfluential": false,
          "contexts": [
            "It is not a trivial task since language by itself is complex, and its production and comprehension traverse many levels of linguistic analysis, like morphological, lexical, syntactical, semantical, and pragmatical (Feldman & Sanger, 2006; Liddy, 2001; Peters et al., 2018)."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Deep Contextualized Word Representations",
            "abstract": "We introduce a new type of deep contextualized word representation that models both (1) complex characteristics of word use (e.g., syntax and semantics), and (2) how these uses vary across linguistic contexts (i.e., to model polysemy). Our word vectors are learned functions of the internal states of a deep bidirectional language model (biLM), which is pre-trained on a large text corpus. We show that these representations can be easily added to existing models and significantly improve the state of the art across six challenging NLP problems, including question answering, textual entailment and sentiment analysis. We also present an analysis showing that exposing the deep internals of the pre-trained network is crucial, allowing downstream models to mix different types of semi-supervision signals.",
            "year": 2018,
            "venue": "North American Chapter of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "39139825",
                "name": "Matthew E. Peters"
              },
              {
                "authorId": "50043859",
                "name": "Mark Neumann"
              },
              {
                "authorId": "2136562",
                "name": "Mohit Iyyer"
              },
              {
                "authorId": "40642935",
                "name": "Matt Gardner"
              },
              {
                "authorId": "143997772",
                "name": "Christopher Clark"
              },
              {
                "authorId": "2544107",
                "name": "Kenton Lee"
              },
              {
                "authorId": "1982950",
                "name": "Luke Zettlemoyer"
              }
            ]
          }
        },
        {
          "citedcorpusid": 5782890,
          "isinfluential": false,
          "contexts": [
            "Intelligent text classification methods provide superior facilities, save time and money while handling the increase of digital texts we are facing (Manning, Raghavan, & Schutze, 2008; Silva & Ribeiro, 2010)."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Inductive Inference for Large Scale Text Classification: Kernel Approaches and Techniques",
            "abstract": "",
            "year": 2009,
            "venue": "Studies in Computational Intelligence",
            "authors": [
              {
                "authorId": "2563511",
                "name": "Catarina Silva"
              },
              {
                "authorId": "144719255",
                "name": "B. Ribeiro"
              }
            ]
          }
        },
        {
          "citedcorpusid": 14012633,
          "isinfluential": true,
          "contexts": [
            " https://www.wipo.int/classifications/ipc/en/ITsupport/Categorization/dataset/index.html Derieux et al. (2010) used different approaches to classify CLEF-IP dataset with English (68%), German (24",
            "With a combined method which builds from 3 to 15 classifiers for each test document, Derieux et al. (2010) trained on less than 1,000 patents to predict 630 classes and had 84.7% accuracy for English documents, 74.1% for documents in German and 78% for documents"
          ],
          "intents": [
            "['methodology']",
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Combining Semantics and Statistics for Patent Classification",
            "abstract": "For the patent classification task of the 2010 CLEF-IP evaluation we have used three different approaches combining semantics and statistics-driven techniques: first approach is based on an indexing-retrieval method using the Lemur system enhanced with a class calculation algorithm; the second approach combined a semantics-driven technique for class model building and the use of an advanced statistical classifier; the third approach combined the two previous methods, attempting to exploit their complementarity for results quality improvement. The results obtained for our system are encouraging: we ranked second in terms of precision on first candidate, which is, from an application point of view, the most pertinent score.",
            "year": 2010,
            "venue": "Conference and Labs of the Evaluation Forum",
            "authors": [
              {
                "authorId": "48733111",
                "name": "F. Derieux"
              },
              {
                "authorId": "2238724",
                "name": "Mihaela Bobeica"
              },
              {
                "authorId": "2330267",
                "name": "Delphine Pois"
              },
              {
                "authorId": "1964787",
                "name": "Jean-Pierre Raysz"
              }
            ]
          }
        },
        {
          "citedcorpusid": 35058596,
          "isinfluential": false,
          "contexts": [
            "Zhang (2014) used SVM to build sub-classifiers for each class of the patents that are combined in a multi-classifier fusion where the final label is selected using an active learning method Wu et al. (2016) proposed a patent classification system based on a self-organising map (SOM), kernel…"
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Interactive patent classification based on multi-classifier fusion and active learning",
            "abstract": "",
            "year": 2014,
            "venue": "Neurocomputing",
            "authors": [
              {
                "authorId": "2141934368",
                "name": "Xiaoyu Zhang"
              }
            ]
          }
        },
        {
          "citedcorpusid": 52967399,
          "isinfluential": false,
          "contexts": [
            "For this property of handle long-term dependencies, BiLSTM has been widely used in text classification (Bispo et al., 2019; Devlin et al., 2019; Hu et al., 2018)."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
            "abstract": "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).",
            "year": 2019,
            "venue": "North American Chapter of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "39172707",
                "name": "Jacob Devlin"
              },
              {
                "authorId": "1744179",
                "name": "Ming-Wei Chang"
              },
              {
                "authorId": "2544107",
                "name": "Kenton Lee"
              },
              {
                "authorId": "3259253",
                "name": "Kristina Toutanova"
              }
            ]
          }
        },
        {
          "citedcorpusid": 67426382,
          "isinfluential": false,
          "contexts": [
            "An ELMo Language Model trained using a corpus created from Portuguese Wikipedia and public documents from Brazil’s Labor Courts were provided by Quinta de Castro et al. (2018)."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",
            "abstract": "",
            "year": 2013,
            "venue": "International Conference on Software Reuse",
            "authors": [
              {
                "authorId": "3029288",
                "name": "F. Amirabdollahian"
              },
              {
                "authorId": "1724361",
                "name": "K. Dautenhahn"
              },
              {
                "authorId": "4474343",
                "name": "C. Dixon"
              },
              {
                "authorId": "1725137",
                "name": "K. Eder"
              },
              {
                "authorId": "143805379",
                "name": "Michael Fisher"
              },
              {
                "authorId": "1749179",
                "name": "K. Koay"
              },
              {
                "authorId": "40616008",
                "name": "E. Magid"
              },
              {
                "authorId": "9460221",
                "name": "Tony Pipe"
              },
              {
                "authorId": "144426526",
                "name": "Maha Salem"
              },
              {
                "authorId": "49163673",
                "name": "J. Saunders"
              },
              {
                "authorId": "35022905",
                "name": "M. Webster"
              }
            ]
          }
        },
        {
          "citedcorpusid": 119297355,
          "isinfluential": true,
          "contexts": [
            "To achieve the goals, we followed typical text classification methodologies (Kowsari et al., 2019).",
            "Tree-based classification algorithms, especially voting classifiers like XGBoost, can be fast and accurate for document classification (Kowsari et al., 2019).",
            "The goal is to choose the best way to represent the words of the text to be able to apply the algorithms (Kowsari et al., 2019)."
          ],
          "intents": [
            "['methodology']",
            "['methodology']",
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Text Classification Algorithms: A Survey",
            "abstract": "In recent years, there has been an exponential growth in the number of complex documentsand texts that require a deeper understanding of machine learning methods to be able to accuratelyclassify texts in many applications. Many machine learning approaches have achieved surpassingresults in natural language processing. The success of these learning algorithms relies on their capacityto understand complex models and non-linear relationships within data. However, finding suitablestructures, architectures, and techniques for text classification is a challenge for researchers. In thispaper, a brief overview of text classification algorithms is discussed. This overview covers differenttext feature extractions, dimensionality reduction methods, existing algorithms and techniques, andevaluations methods. Finally, the limitations of each technique and their application in real-worldproblems are discussed.",
            "year": 2019,
            "venue": "Inf.",
            "authors": [
              {
                "authorId": "2060243",
                "name": "Kamran Kowsari"
              },
              {
                "authorId": "26417934",
                "name": "K. Meimandi"
              },
              {
                "authorId": "26408890",
                "name": "Mojtaba Heidarysafa"
              },
              {
                "authorId": "51266150",
                "name": "Sanjana Mendu"
              },
              {
                "authorId": "1771388",
                "name": "Laura E. Barnes"
              },
              {
                "authorId": "2115530197",
                "name": "Donald E. Brown"
              }
            ]
          }
        },
        {
          "citedcorpusid": 132667166,
          "isinfluential": false,
          "contexts": [
            "Concerned about the specificity of the language used in patent applications, Risch and Krestel (2019) proposed domain-specific word embeddings.",
            "Even knowing that specific domain-language embeddings bring better performance (Risch & Krestel, 2019), as our dataset was not big enough to train a reasonable Language Model, we decided to validate pre-trained Portuguese word embeddings."
          ],
          "intents": [
            "['background']",
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Domain-specific word embeddings for patent classification",
            "abstract": "\nPurpose\nPatent offices and other stakeholders in the patent domain need to classify patent applications according to a standardized classification scheme. The purpose of this paper is to examine the novelty of an application it can then be compared to previously granted patents in the same class. Automatic classification would be highly beneficial, because of the large volume of patents and the domain-specific knowledge needed to accomplish this costly manual task. However, a challenge for the automation is patent-specific language use, such as special vocabulary and phrases.\n\n\nDesign/methodology/approach\nTo account for this language use, the authors present domain-specific pre-trained word embeddings for the patent domain. The authors train the model on a very large data set of more than 5m patents and evaluate it at the task of patent classification. To this end, the authors propose a deep learning approach based on gated recurrent units for automatic patent classification built on the trained word embeddings.\n\n\nFindings\nExperiments on a standardized evaluation data set show that the approach increases average precision for patent classification by 17 percent compared to state-of-the-art approaches. In this paper, the authors further investigate the model’s strengths and weaknesses. An extensive error analysis reveals that the learned embeddings indeed mirror patent-specific language use. The imbalanced training data and underrepresented classes are the most difficult remaining challenge.\n\n\nOriginality/value\nThe proposed approach fulfills the need for domain-specific word embeddings for downstream tasks in the patent domain, such as patent classification or patent analysis.\n",
            "year": 2019,
            "venue": "Data Technologies and Applications",
            "authors": [
              {
                "authorId": "1695993",
                "name": "Julian Risch"
              },
              {
                "authorId": "3264110",
                "name": "Ralf Krestel"
              }
            ]
          }
        },
        {
          "citedcorpusid": 158036236,
          "isinfluential": true,
          "contexts": [
            "Intellectual Property Analytics (IPA) is a growing field that deals with the analysis of intellectual property databases to discover trends, relationships, patterns and leverage researches and innovations based on information that may not be available anywhere else (Aristodemou & Tietze, 2018).",
            "After defining “Intellectual Property Analytics (IPA) as the data science of analysing a large amount of Intellectual Property information to discover relationships, trends and patterns for decision”, Aristodemou and Tietze (2018) reviewed 57 articles.",
            "This first step consumes a big part of the processing time and demands vast  WIPO—World Intellectual Property Organization: the global forum for intellectual property services, policy, information and cooperation.",
            "Intellectual Property (IP) is a category of property related to the “creations of the mind”, it means a vast range of activities from art to scientific works, trademarks, and inventions.",
            "In general, to gain this protection, the inventor will need to file an application describing the invention and submit it to the analysis of utility, novelty, and inventiveness (World Intellectual Property Organization, 2008)."
          ],
          "intents": [
            "--",
            "--",
            "--",
            "--",
            "['background']"
          ],
          "cited_paper_info": {
            "title": "WIPO intellectual property handbook: policy, law and use",
            "abstract": "Albania, Algeria, Andorra, Angola, Antigua and Barbuda, Argentina, Armenia, Australia, Austria, Azerbaijan, Bahamas, Bahrain, Bangladesh, Barbados, Belarus, Belgium, Belize, Benin, Bhutan, Bolivia, Bosnia and Herzegovina, Botswana, Brazil, Brunei Darussalam, Bulgaria, Burkina Faso, Burundi, Cambodia, Cameroon, Canada, Cape Verde, Central African Republic, Chad, Chile, China, Colombia, Congo, Costa Rica, Côte d’Ivoire, Croatia, Cuba, Cyprus, Czech Republic, Democratic People’s Republic of Korea, Democratic Republic of the Congo, Denmark, Djibouti, Dominica, Dominican Republic, Ecuador, Egypt, El Salvador, Equatorial Guinea, Eritrea, Estonia, Ethiopia, Fiji, Finland, France, Gabon, Gambia, Georgia, Germany, Ghana, Greece, Grenada, Guatemala, Guinea, Guinea-Bissau, Guyana, Haiti, Holy See, Honduras, Hungary, Iceland, India, Indonesia, Iran (Islamic Republic of), Iraq, Ireland, Israel, Italy, Jamaica, Japan, Jordan, Kazakhstan, Kenya, Kuwait, Kyrgyzstan, Lao People’s Democratic Republic, Latvia, Lebanon, Lesotho, Liberia, Libyan Arab Jamahiriya, Liechtenstein, Lithuania, Luxembourg, Madagascar, Malawi, Malaysia, Maldives, Mali, Malta, Mauritania, Mauritius, Mexico, Monaco, Mongolia, Morocco, Mozambique, Myanmar, Namibia, Nepal, Netherlands, New Zealand, Nicaragua, Niger, Nigeria, Norway, Oman, Pakistan, Panama, Papua New Guinea, Paraguay, Peru, Philippines, Poland, Portugal, Qatar, Republic of Korea, Republic of Moldova, Romania, Russian Federation, Rwanda, Saint Kitts and Nevis, Saint Lucia, Saint Vincent and the Grenadines, Samoa, San Marino, Sao Tome and Principe, Saudi Arabia, Senegal, Serbia and Montenegro, Seychelles, Sierra Leone, Singapore, Slovakia, Slovenia, Somalia, South Africa, Spain, Sri Lanka, Sudan, Suriname, Swaziland, Sweden, Switzerland, Tajikistan, Thailand, The Former Yugoslav Republic of Macedonia, Togo, Tonga, Trinidad and Tobago, Tunisia, Turkey, Turkmenistan, Uganda, Ukraine, United Arab Emirates, United Kingdom, United Republic of Tanzania, United States of America, Uruguay, Uzbekistan, Venezuela, Viet Nam, Yemen, Zambia, Zimbabwe (180).",
            "year": 2001,
            "venue": "",
            "authors": [
              {
                "authorId": "31646768",
                "name": "K. Idris"
              }
            ]
          }
        },
        {
          "citedcorpusid": 207847753,
          "isinfluential": false,
          "contexts": [
            "The transfer learning approach and pre-trained language models have been achieving the state-of-art in different tasks (Pan & Yang, 2010; Zhuang et al., 2021)."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "A Comprehensive Survey on Transfer Learning",
            "abstract": "Transfer learning aims at improving the performance of target learners on target domains by transferring the knowledge contained in different but related source domains. In this way, the dependence on a large number of target-domain data can be reduced for constructing target learners. Due to the wide application prospects, transfer learning has become a popular and promising area in machine learning. Although there are already some valuable and impressive surveys on transfer learning, these surveys introduce approaches in a relatively isolated way and lack the recent advances in transfer learning. Due to the rapid expansion of the transfer learning area, it is both necessary and challenging to comprehensively review the relevant studies. This survey attempts to connect and systematize the existing transfer learning research studies, as well as to summarize and interpret the mechanisms and the strategies of transfer learning in a comprehensive way, which may help readers have a better understanding of the current research status and ideas. Unlike previous surveys, this survey article reviews more than 40 representative transfer learning approaches, especially homogeneous transfer learning approaches, from the perspectives of data and model. The applications of transfer learning are also briefly introduced. In order to show the performance of different transfer learning models, over 20 representative transfer learning models are used for experiments. The models are performed on three different data sets, that is, Amazon Reviews, Reuters-21578, and Office-31, and the experimental results demonstrate the importance of selecting appropriate transfer learning models for different applications in practice.",
            "year": 2019,
            "venue": "Proceedings of the IEEE",
            "authors": [
              {
                "authorId": "1799525",
                "name": "Fuzhen Zhuang"
              },
              {
                "authorId": "51052604",
                "name": "Zhiyuan Qi"
              },
              {
                "authorId": "108237152",
                "name": "Keyu Duan"
              },
              {
                "authorId": "120791559",
                "name": "Dongbo Xi"
              },
              {
                "authorId": "1864765978",
                "name": "Yongchun Zhu"
              },
              {
                "authorId": "1968806",
                "name": "Hengshu Zhu"
              },
              {
                "authorId": "144467554",
                "name": "Hui Xiong"
              },
              {
                "authorId": "144131273",
                "name": "Qing He"
              }
            ]
          }
        },
        {
          "citedcorpusid": 212516914,
          "isinfluential": false,
          "contexts": [
            "For this property of handle long-term dependencies, BiLSTM has been widely used in text classification (Bispo et al., 2019; Devlin et al., 2019; Hu et al., 2018)."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Long Short-Term Memory Model for Classification of English-PtBR Cross-Lingual Hate Speech",
            "abstract": "Automatic and accurate recognition of hate speech is a difficult job. In addition to the inherent ambiguity of the natural language, deep understanding of the linguistic structure is imperative. Usually, discriminatory discourse does not make use of typical expressions and often abuse of sarcasm. Good knowledge of world and assessment of context are thus highly demanded. Several approaches have been proposed for automating hate speech recognition task. Many of them consider a combination of strategies in order to achieve better results: character-based or word-based N-grams, lexical features such as the presence or absence of negative words, classes or expressions indicative of insult, punctuation marks, repetition of letters, the presence of emoji, etc. The solitary use of linguistic features such as POS tagging have shown itself inefficient. The recent usage of neural networks to create a distributed representation of the sentences within a hate speech corpus is a promising path. Unfortunately, providing such a corpus is hard. Except for the English language, hate speech corpora are rarely found. This work proposes a cross-lingual approach to automatically recognize hate speech in Portuguese language, leveraging the knowledge of English corpora. A deep Long Short-Term Memory (LSTM) model has been trained and many different experimentation scenarios were set to deal with embeddings, TFIDF, N-grams, GloVe vocabulary and so on. At the end, a Gradient Boosting Decision Tree (GBDT) was used to improve classification results. We achieved accuracy of up to 70% in the better scenarios. Two important contributions of this work are: (i) An effective approach to deal with the lack of hate speech corpora in the desired language and (ii) a hate speech database in Portuguese to contribute to research community.",
            "year": 2019,
            "venue": "Journal of Computer Science",
            "authors": [
              {
                "authorId": "151228742",
                "name": "T. Bispo"
              },
              {
                "authorId": "144180186",
                "name": "Hendrik T. Macedo"
              },
              {
                "authorId": "2150267241",
                "name": "Flávio A. O. Santos"
              },
              {
                "authorId": "90196119",
                "name": "R. L. D. Silva"
              },
              {
                "authorId": "145254176",
                "name": "L. Matos"
              },
              {
                "authorId": "144431964",
                "name": "Bruno O. Prado"
              },
              {
                "authorId": "152135898",
                "name": "G. J. F. D. Silva"
              },
              {
                "authorId": "1817522",
                "name": "Adolfo P. Guimarães"
              }
            ]
          }
        },
        {
          "citedcorpusid": 218489934,
          "isinfluential": true,
          "contexts": [
            "With much more data and classes, Abdelgawad et al. (2020) had an accuracy of 55.02% and outperformed their baseline with 65.43% accuracy using a CNN model.",
            "With this optimised neural network, they achieved 55.02% accuracy on the public Wipo-Alpha dataset  (Abdelgawad et al., 2020)."
          ],
          "intents": [
            "['methodology']",
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Optimizing Neural Networks for Patent Classification",
            "abstract": "",
            "year": 2019,
            "venue": "ECML/PKDD",
            "authors": [
              {
                "authorId": "1434548968",
                "name": "Louay Abdelgawad"
              },
              {
                "authorId": "1792360",
                "name": "Peter Klügl"
              },
              {
                "authorId": "8722708",
                "name": "Erdan Genc"
              },
              {
                "authorId": "2154062",
                "name": "S. Falkner"
              },
              {
                "authorId": "144661829",
                "name": "F. Hutter"
              }
            ]
          }
        },
        {
          "citedcorpusid": 262553219,
          "isinfluential": false,
          "contexts": [
            "Intelligent text classification methods provide superior facilities, save time and money while handling the increase of digital texts we are facing (Manning, Raghavan, & Schutze, 2008; Silva & Ribeiro, 2010)."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "An Introduction to Information Retrieval",
            "abstract": "",
            "year": 2013,
            "venue": "",
            "authors": [
              {
                "authorId": "144161686",
                "name": "S. Ceri"
              },
              {
                "authorId": "1710630",
                "name": "A. Bozzon"
              },
              {
                "authorId": "40350773",
                "name": "Marco Brambilla"
              },
              {
                "authorId": "2539248",
                "name": "Emanuele Della Valle"
              },
              {
                "authorId": "1704595",
                "name": "P. Fraternali"
              },
              {
                "authorId": "1794305",
                "name": "S. Quarteroni"
              }
            ]
          }
        },
        {
          "citedcorpusid": null,
          "isinfluential": false,
          "contexts": [
            "It is not a trivial task since language by itself is complex, and its production and comprehension traverse many levels of linguistic analysis, like morphological, lexical, syntactical, semantical, and pragmatical (Feldman & Sanger, 2006; Liddy, 2001; Peters et al., 2018)."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {}
        },
        {
          "citedcorpusid": null,
          "isinfluential": false,
          "contexts": [
            "Each section is subdivided into classes, subclasses, groups, and subgroups, counting in the last level approximately 72,000 sub-groups (Espacenet, 2021)."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {}
        }
      ]
    },
    "236301503": {
      "citing_paper_info": {
        "title": "Patent2Vec: Multi-view representation learning on patent-graphs for patent classification",
        "abstract": "",
        "year": 2021,
        "venue": "World wide web (Bussum)",
        "authors": [
          {
            "authorId": "1999183010",
            "name": "Lintao Fang"
          },
          {
            "authorId": "2108006217",
            "name": "Le Zhang"
          },
          {
            "authorId": "46476917",
            "name": "Han Wu"
          },
          {
            "authorId": "50383766",
            "name": "Tong Xu"
          },
          {
            "authorId": "144790855",
            "name": "Ding Zhou"
          },
          {
            "authorId": "2227868312",
            "name": "Enhong Chen"
          }
        ]
      },
      "citation_summary": {
        "citation_count": 19,
        "unique_cited_count": 19,
        "influential_count": 2,
        "detailed_records_count": 19
      },
      "cited_papers": [
        "12289712",
        "5959482",
        "3144218",
        "166228294",
        "216043471",
        "4533146",
        "196200412",
        "132667166",
        "17341970",
        "3051291",
        "6419793",
        "8399404",
        "215882005",
        "3292002",
        "387208",
        "22139415",
        "7732372",
        "69194991",
        "7697669"
      ],
      "citation_details": [
        {
          "citedcorpusid": 387208,
          "isinfluential": false,
          "contexts": [
            "The first is the representative methods in multi-label text classification field, such as FastXML [30], FastText [15] PfastreXML [14], and recently state-of-the-art methods including DeepPatent [22] and PatentBert [21].",
            "The details of these baselines are illustrated as follows: – FastXML [30] : FastXML is a tree-based classifiers, are inspired by the ideas of decision tree and build decision trees based on a instances by recursively splitting internal nodes."
          ],
          "intents": [
            "['methodology']",
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "FastXML: a fast, accurate and stable tree-classifier for extreme multi-label learning",
            "abstract": "",
            "year": 2014,
            "venue": "Knowledge Discovery and Data Mining",
            "authors": [
              {
                "authorId": "40252215",
                "name": "Yashoteja Prabhu"
              },
              {
                "authorId": "145859952",
                "name": "M. Varma"
              }
            ]
          }
        },
        {
          "citedcorpusid": 3051291,
          "isinfluential": true,
          "contexts": [
            "Inspired by the skip-gram model [20], some random-walk based models are proposed to deploy truncated random walks on graph for presentation learning, such as Deepwalk [29], Node2vec [9] and metapath2vec [5].",
            "For example, Deep-walk [29] applies the depth first search strategy to create node sequence, and Node2vec [9] extends Deepwalk by taking advantage of the breadth first search strategy, while they can only be used for the homogeneous graph.",
            "The second is the network representation learning methods, i.e. Deepwalk [29] , Node2vec [9], LINE [36] and GCN [17].",
            "– Deepwalk [29] : Deepwalk takes the truncated random walks for each node to generate the training corpus of node sequences, and then learns the node embedding via maximizing the likelihood of context node prediction from the center nodes."
          ],
          "intents": [
            "['methodology']",
            "['background']",
            "['methodology']",
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "DeepWalk: online learning of social representations",
            "abstract": "We present DeepWalk, a novel approach for learning latent representations of vertices in a network. These latent representations encode social relations in a continuous vector space, which is easily exploited by statistical models. DeepWalk generalizes recent advancements in language modeling and unsupervised feature learning (or deep learning) from sequences of words to graphs. DeepWalk uses local information obtained from truncated random walks to learn latent representations by treating walks as the equivalent of sentences. We demonstrate DeepWalk's latent representations on several multi-label network classification tasks for social networks such as BlogCatalog, Flickr, and YouTube. Our results show that DeepWalk outperforms challenging baselines which are allowed a global view of the network, especially in the presence of missing information. DeepWalk's representations can provide F1 scores up to 10% higher than competing methods when labeled data is sparse. In some experiments, DeepWalk's representations are able to outperform all baseline methods while using 60% less training data. DeepWalk is also scalable. It is an online learning algorithm which builds useful incremental results, and is trivially parallelizable. These qualities make it suitable for a broad class of real world applications such as network classification, and anomaly detection.",
            "year": 2014,
            "venue": "Knowledge Discovery and Data Mining",
            "authors": [
              {
                "authorId": "2271808",
                "name": "Bryan Perozzi"
              },
              {
                "authorId": "1388360943",
                "name": "Rami Al-Rfou"
              },
              {
                "authorId": "1721948",
                "name": "S. Skiena"
              }
            ]
          }
        },
        {
          "citedcorpusid": 3144218,
          "isinfluential": true,
          "contexts": [
            "The second is the network representation learning methods, i.e. Deepwalk [29] , Node2vec [9], LINE [36] and GCN [17].",
            "– GCN [17] : It obtains node embeddings by aggregating node features on graph.",
            "Here we use semantic features learned from word2vec as the input feature of each node, and GCN is conducted on citation graph.",
            "For example, GCN [17] performs graph convolution by aggregating neighborhood information to learn embedding.",
            "[22] present an attention-based GCN model over textual graph to solve patent classification problem.",
            "Deepwalk [29] , Node2vec [9], LINE [36] and GCN [17]."
          ],
          "intents": [
            "--",
            "--",
            "--",
            "['methodology']",
            "--",
            "--"
          ],
          "cited_paper_info": {
            "title": "Semi-Supervised Classification with Graph Convolutional Networks",
            "abstract": "We present a scalable approach for semi-supervised learning on graph-structured data that is based on an efficient variant of convolutional neural networks which operate directly on graphs. We motivate the choice of our convolutional architecture via a localized first-order approximation of spectral graph convolutions. Our model scales linearly in the number of graph edges and learns hidden layer representations that encode both local graph structure and features of nodes. In a number of experiments on citation networks and on a knowledge graph dataset we demonstrate that our approach outperforms related methods by a significant margin.",
            "year": 2016,
            "venue": "International Conference on Learning Representations",
            "authors": [
              {
                "authorId": "41016725",
                "name": "Thomas Kipf"
              },
              {
                "authorId": "1678311",
                "name": "M. Welling"
              }
            ]
          }
        },
        {
          "citedcorpusid": 3292002,
          "isinfluential": false,
          "contexts": [
            "GAT [37] applies the attention mechanism for aggregating representation of neighbors to parallel update node representation."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Graph Attention Networks",
            "abstract": "We present graph attention networks (GATs), novel neural network architectures that operate on graph-structured data, leveraging masked self-attentional layers to address the shortcomings of prior methods based on graph convolutions or their approximations. By stacking layers in which nodes are able to attend over their neighborhoods' features, we enable (implicitly) specifying different weights to different nodes in a neighborhood, without requiring any kind of costly matrix operation (such as inversion) or depending on knowing the graph structure upfront. In this way, we address several key challenges of spectral-based graph neural networks simultaneously, and make our model readily applicable to inductive as well as transductive problems. Our GAT models have achieved or matched state-of-the-art results across four established transductive and inductive graph benchmarks: the Cora, Citeseer and Pubmed citation network datasets, as well as a protein-protein interaction dataset (wherein test graphs remain unseen during training).",
            "year": 2017,
            "venue": "International Conference on Learning Representations",
            "authors": [
              {
                "authorId": "3444569",
                "name": "Petar Velickovic"
              },
              {
                "authorId": "7153363",
                "name": "Guillem Cucurull"
              },
              {
                "authorId": "8742492",
                "name": "Arantxa Casanova"
              },
              {
                "authorId": "144290131",
                "name": "Adriana Romero"
              },
              {
                "authorId": "144269589",
                "name": "P. Lio’"
              },
              {
                "authorId": "1751762",
                "name": "Yoshua Bengio"
              }
            ]
          }
        },
        {
          "citedcorpusid": 4533146,
          "isinfluential": false,
          "contexts": [
            "After that, following the practice in [23], we design the covariance matrix as follows: where Finally, we concatenate the original view representation with enrich knowledge to form the new enhanced representations for each view. where symbol ⊕ represents the concatenation operation and P ∗ i is a…"
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Towards Faster Training of Global Covariance Pooling Networks by Iterative Matrix Square Root Normalization",
            "abstract": "Global covariance pooling in convolutional neural networks has achieved impressive improvement over the classical first-order pooling. Recent works have shown matrix square root normalization plays a central role in achieving state-of-the-art performance. However, existing methods depend heavily on eigendecomposition (EIG) or singular value decomposition (SVD), suffering from inefficient training due to limited support of EIG and SVD on GPU. Towards addressing this problem, we propose an iterative matrix square root normalization method for fast end-to-end training of global covariance pooling networks. At the core of our method is a meta-layer designed with loop-embedded directed graph structure. The meta-layer consists of three consecutive nonlinear structured layers, which perform pre-normalization, coupled matrix iteration and post-compensation, respectively. Our method is much faster than EIG or SVD based ones, since it involves only matrix multiplications, suitable for parallel implementation on GPU. Moreover, the proposed network with ResNet architecture can converge in much less epochs, further accelerating network training. On large-scale ImageNet, we achieve competitive performance superior to existing counterparts. By finetuning our models pre-trained on ImageNet, we establish state-of-the-art results on three challenging fine-grained benchmarks. The source code and network models will be available at http://www.peihuali.org/iSQRT-COV.",
            "year": 2017,
            "venue": "2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition",
            "authors": [
              {
                "authorId": "40426020",
                "name": "P. Li"
              },
              {
                "authorId": "144418234",
                "name": "Jiangtao Xie"
              },
              {
                "authorId": "49110790",
                "name": "Qilong Wang"
              },
              {
                "authorId": "30122644",
                "name": "Zilin Gao"
              }
            ]
          }
        },
        {
          "citedcorpusid": 5959482,
          "isinfluential": false,
          "contexts": [
            "Finally, we utilize the skip-gram model proposed in Word2vec [26] to learn representation of each node."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Efficient Estimation of Word Representations in Vector Space",
            "abstract": "We propose two novel model architectures for computing continuous vector\nrepresentations of words from very large data sets. The quality of these\nrepresentations is measured in a word similarity task, and the results are\ncompared to the previously best performing techniques based on different types\nof neural networks. We observe large improvements in accuracy at much lower\ncomputational cost, i.e. it takes less than a day to learn high quality word\nvectors from a 1.6 billion words data set. Furthermore, we show that these\nvectors provide state-of-the-art performance on our test set for measuring\nsyntactic and semantic word similarities.",
            "year": 2013,
            "venue": "International Conference on Learning Representations",
            "authors": [
              {
                "authorId": "2047446108",
                "name": "Tomas Mikolov"
              },
              {
                "authorId": "2118440152",
                "name": "Kai Chen"
              },
              {
                "authorId": "32131713",
                "name": "G. Corrado"
              },
              {
                "authorId": "49959210",
                "name": "J. Dean"
              }
            ]
          }
        },
        {
          "citedcorpusid": 6419793,
          "isinfluential": false,
          "contexts": [
            "For example, canonical correlation analysis based method DeepCCA [38] and CM-GANs [28] that use GANs to model cross-view joint distribution."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Stochastic optimization for deep CCA via nonlinear orthogonal iterations",
            "abstract": "Deep CCA is a recently proposed deep neural network extension to the traditional canonical correlation analysis (CCA), and has been successful for multi-view representation learning in several domains. However, stochastic optimization of the deep CCA objective is not straightforward, because it does not decouple over training examples. Previous optimizers for deep CCA are either batch-based algorithms or stochastic optimization using large minibatches, which can have high memory consumption. In this paper, we tackle the problem of stochastic optimization for deep CCA with small minibatches, based on an iterative solution to the CCA objective, and show that we can achieve as good performance as previous optimizers and thus alleviate the memory requirement.",
            "year": 2015,
            "venue": "Allerton Conference on Communication, Control, and Computing",
            "authors": [
              {
                "authorId": "1702290",
                "name": "Weiran Wang"
              },
              {
                "authorId": "144365054",
                "name": "R. Arora"
              },
              {
                "authorId": "2924113",
                "name": "Karen Livescu"
              },
              {
                "authorId": "1706280",
                "name": "N. Srebro"
              }
            ]
          }
        },
        {
          "citedcorpusid": 7697669,
          "isinfluential": false,
          "contexts": [
            "The first is the representative methods in multi-label text classification field, such as FastXML [30], FastText [15] PfastreXML [14], and recently state-of-the-art methods including DeepPatent [22] and PatentBert [21].",
            "– PFastreXML [14] : PFastreXML is an extension of FastXML, it prioritizes prediction of tail labels and handles missing labels by proposing the propensity scored loss."
          ],
          "intents": [
            "['methodology']",
            "--"
          ],
          "cited_paper_info": {
            "title": "Extreme Multi-label Loss Functions for Recommendation, Tagging, Ranking & Other Missing Label Applications",
            "abstract": "",
            "year": 2016,
            "venue": "Knowledge Discovery and Data Mining",
            "authors": [
              {
                "authorId": "2059143344",
                "name": "Himanshu Jain"
              },
              {
                "authorId": "40252215",
                "name": "Yashoteja Prabhu"
              },
              {
                "authorId": "145859952",
                "name": "M. Varma"
              }
            ]
          }
        },
        {
          "citedcorpusid": 7732372,
          "isinfluential": false,
          "contexts": [
            "Inspired by multi-modal representation learning [18], we propose a view alignment module to address this issue by projecting the single-view representation into a shared representation space and we then further assume that single view embedding of a patent is near to it’s fused representation while…"
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Unifying Visual-Semantic Embeddings with Multimodal Neural Language Models",
            "abstract": "Inspired by recent advances in multimodal learning and machine translation, we introduce an encoder-decoder pipeline that learns (a): a multimodal joint embedding space with images and text and (b): a novel language model for decoding distributed representations from our space. Our pipeline effectively unifies joint image-text embedding models with multimodal neural language models. We introduce the structure-content neural language model that disentangles the structure of a sentence to its content, conditioned on representations produced by the encoder. The encoder allows one to rank images and sentences while the decoder can generate novel descriptions from scratch. Using LSTM to encode sentences, we match the state-of-the-art performance on Flickr8K and Flickr30K without using object detections. We also set new best results when using the 19-layer Oxford convolutional network. Furthermore we show that with linear encoders, the learned embedding space captures multimodal regularities in terms of vector space arithmetic e.g. *image of a blue car* - \"blue\" + \"red\" is near images of red cars. Sample captions generated for 800 images are made available for comparison.",
            "year": 2014,
            "venue": "arXiv.org",
            "authors": [
              {
                "authorId": "3450996",
                "name": "Ryan Kiros"
              },
              {
                "authorId": "145124475",
                "name": "R. Salakhutdinov"
              },
              {
                "authorId": "1804104",
                "name": "R. Zemel"
              }
            ]
          }
        },
        {
          "citedcorpusid": 8399404,
          "isinfluential": false,
          "contexts": [
            "The second is the network representation learning methods, i.e. Deepwalk [29] , Node2vec [9], LINE [36] and GCN [17].",
            "– LINE [36] : It learns the node embedding by preserving the first-order proximity or second-order proximity of the network structure separately."
          ],
          "intents": [
            "['methodology']",
            "['background']"
          ],
          "cited_paper_info": {
            "title": "LINE: Large-scale Information Network Embedding",
            "abstract": "This paper studies the problem of embedding very large information networks into low-dimensional vector spaces, which is useful in many tasks such as visualization, node classification, and link prediction. Most existing graph embedding methods do not scale for real world information networks which usually contain millions of nodes. In this paper, we propose a novel network embedding method called the ``LINE,'' which is suitable for arbitrary types of information networks: undirected, directed, and/or weighted. The method optimizes a carefully designed objective function that preserves both the local and global network structures. An edge-sampling algorithm is proposed that addresses the limitation of the classical stochastic gradient descent and improves both the effectiveness and the efficiency of the inference. Empirical experiments prove the effectiveness of the LINE on a variety of real-world information networks, including language networks, social networks, and citation networks. The algorithm is very efficient, which is able to learn the embedding of a network with millions of vertices and billions of edges in a few hours on a typical single machine. The source code of the LINE is available online\\footnote{\\url{https://github.com/tangjianpku/LINE}}.",
            "year": 2015,
            "venue": "The Web Conference",
            "authors": [
              {
                "authorId": "145357803",
                "name": "Jian Tang"
              },
              {
                "authorId": "35955224",
                "name": "Meng Qu"
              },
              {
                "authorId": "2108927052",
                "name": "Mingzhe Wang"
              },
              {
                "authorId": "47474380",
                "name": "Ming Zhang"
              },
              {
                "authorId": "2112592519",
                "name": "Jun Yan"
              },
              {
                "authorId": "1743469",
                "name": "Qiaozhu Mei"
              }
            ]
          }
        },
        {
          "citedcorpusid": 12289712,
          "isinfluential": false,
          "contexts": [
            "Representative methods include graphical model-based fusion and neural network based fusion, consisting of multi-view autoencoder [48], multi-view convolutional neural network [7] and multi-view recurrent neural network [16]."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Convolutional Two-Stream Network Fusion for Video Action Recognition",
            "abstract": "Recent applications of Convolutional Neural Networks (ConvNets) for human action recognition in videos have proposed different solutions for incorporating the appearance and motion information. We study a number of ways of fusing ConvNet towers both spatially and temporally in order to best take advantage of this spatio-temporal information. We make the following findings: (i) that rather than fusing at the softmax layer, a spatial and temporal network can be fused at a convolution layer without loss of performance, but with a substantial saving in parameters, (ii) that it is better to fuse such networks spatially at the last convolutional layer than earlier, and that additionally fusing at the class prediction layer can boost accuracy, finally (iii) that pooling of abstract convolutional features over spatiotemporal neighbourhoods further boosts performance. Based on these studies we propose a new ConvNet architecture for spatiotemporal fusion of video snippets, and evaluate its performance on standard benchmarks where this architecture achieves state-of-the-art results.",
            "year": 2016,
            "venue": "Computer Vision and Pattern Recognition",
            "authors": [
              {
                "authorId": "2366173928",
                "name": "Christoph Feichtenhofer"
              },
              {
                "authorId": "1718587",
                "name": "A. Pinz"
              },
              {
                "authorId": "1688869",
                "name": "Andrew Zisserman"
              }
            ]
          }
        },
        {
          "citedcorpusid": 17341970,
          "isinfluential": false,
          "contexts": [
            "The traditional techniques are based on matrix factorization, like LLE [33] and GraRep [1]."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "GraRep: Learning Graph Representations with Global Structural Information",
            "abstract": "",
            "year": 2015,
            "venue": "International Conference on Information and Knowledge Management",
            "authors": [
              {
                "authorId": "2105817",
                "name": "Shaosheng Cao"
              },
              {
                "authorId": "143844110",
                "name": "Wei Lu"
              },
              {
                "authorId": "3101288",
                "name": "Qiongkai Xu"
              }
            ]
          }
        },
        {
          "citedcorpusid": 22139415,
          "isinfluential": false,
          "contexts": [
            "On the other hand, specially designed patent classifiers [8] involve traditional non-deep learning meth-ods like association rule [11], and support vector machine [25, 44], and deep learning ones like CNN [12, 22, 25], GRU [31], Bert [32], graph neural networks [35] and so on."
          ],
          "intents": [
            "--"
          ],
          "cited_paper_info": {
            "title": "Automated Patent Classification Using Word Embedding",
            "abstract": "",
            "year": 2017,
            "venue": "International Conference on Machine Learning and Applications",
            "authors": [
              {
                "authorId": "35272389",
                "name": "Mattyws F. Grawe"
              },
              {
                "authorId": "144097599",
                "name": "C. A. Martins"
              },
              {
                "authorId": "2078352187",
                "name": "Andreia Gentil Bonfante"
              }
            ]
          }
        },
        {
          "citedcorpusid": 69194991,
          "isinfluential": false,
          "contexts": [
            "The first one is multi-view representation fusion, which aims to integrate multi-view data into a single representation to comprehensively represent data [24]."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "A Survey of Multi-View Representation Learning",
            "abstract": "Recently, multi-view representation learning has become a rapidly growing direction in machine learning and data mining areas. This paper introduces two categories for multi-view representation learning: multi-view representation alignment and multi-view representation fusion. Consequently, we first review the representative methods and theories of multi-view representation learning based on the perspective of alignment, such as correlation-based alignment. Representative examples are canonical correlation analysis (CCA) and its several extensions. Then, from the perspective of representation fusion, we investigate the advancement of multi-view representation learning that ranges from generative methods including multi-modal topic learning, multi-view sparse coding, and multi-view latent space Markov networks, to neural network-based methods including multi-modal autoencoders, multi-view convolutional neural networks, and multi-modal recurrent neural networks. Further, we also investigate several important applications of multi-view representation learning. Overall, this survey aims to provide an insightful overview of theoretical foundation and state-of-the-art developments in the field of multi-view representation learning and to help researchers find the most appropriate tools for particular applications.",
            "year": 2016,
            "venue": "IEEE Transactions on Knowledge and Data Engineering",
            "authors": [
              {
                "authorId": "2201596761",
                "name": "Yingming Li"
              },
              {
                "authorId": "2150427580",
                "name": "Ming Yang"
              },
              {
                "authorId": "2118748124",
                "name": "Zhongfei Zhang"
              }
            ]
          }
        },
        {
          "citedcorpusid": 132667166,
          "isinfluential": false,
          "contexts": [
            "On the other hand, specially designed patent classifiers [8] involve traditional non-deep learning meth-ods like association rule [11], and support vector machine [25, 44], and deep learning ones like CNN [12, 22, 25], GRU [31], Bert [32], graph neural networks [35] and so on."
          ],
          "intents": [
            "--"
          ],
          "cited_paper_info": {
            "title": "Domain-specific word embeddings for patent classification",
            "abstract": "\nPurpose\nPatent offices and other stakeholders in the patent domain need to classify patent applications according to a standardized classification scheme. The purpose of this paper is to examine the novelty of an application it can then be compared to previously granted patents in the same class. Automatic classification would be highly beneficial, because of the large volume of patents and the domain-specific knowledge needed to accomplish this costly manual task. However, a challenge for the automation is patent-specific language use, such as special vocabulary and phrases.\n\n\nDesign/methodology/approach\nTo account for this language use, the authors present domain-specific pre-trained word embeddings for the patent domain. The authors train the model on a very large data set of more than 5m patents and evaluate it at the task of patent classification. To this end, the authors propose a deep learning approach based on gated recurrent units for automatic patent classification built on the trained word embeddings.\n\n\nFindings\nExperiments on a standardized evaluation data set show that the approach increases average precision for patent classification by 17 percent compared to state-of-the-art approaches. In this paper, the authors further investigate the model’s strengths and weaknesses. An extensive error analysis reveals that the learned embeddings indeed mirror patent-specific language use. The imbalanced training data and underrepresented classes are the most difficult remaining challenge.\n\n\nOriginality/value\nThe proposed approach fulfills the need for domain-specific word embeddings for downstream tasks in the patent domain, such as patent classification or patent analysis.\n",
            "year": 2019,
            "venue": "Data Technologies and Applications",
            "authors": [
              {
                "authorId": "1695993",
                "name": "Julian Risch"
              },
              {
                "authorId": "3264110",
                "name": "Ralf Krestel"
              }
            ]
          }
        },
        {
          "citedcorpusid": 166228294,
          "isinfluential": false,
          "contexts": [
            "Representation learning aims at learning low-dimensional vector of the data and then applies it to downstream tasks [2, 39, 41, 43]."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "MCNE: An End-to-End Framework for Learning Multiple Conditional Network Representations of Social Network",
            "abstract": "Recently, the Network Representation Learning (NRL) techniques, which represent graph structure via low-dimension vectors to support social-oriented application, have attracted wide attention. Though large efforts have been made, they may fail to describe the multiple aspects of similarity between social users, as only a single vector for one unique aspect has been represented for each node. To that end, in this paper, we propose a novel end-to-end framework named MCNE to learn multiple conditional network representations, so that various preferences for multiple behaviors could be fully captured. Specifically, we first design a binary mask layer to divide the single vector as conditional embeddings for multiple behaviors. Then, we introduce the attention network to model interaction relationship among multiple preferences, and further utilize the adapted message sending and receiving operation of graph neural network, so that multi-aspect preference information from high-order neighbors will be captured. Finally, we utilize Bayesian Personalized Ranking loss function to learn the preference similarity on each behavior, and jointly learn multiple conditional node embeddings via multi-task learning framework. Extensive experiments on public datasets validate that our MCNE framework could significantly outperform several state-of-the-art baselines, and further support the visualization and transfer learning tasks with excellent interpretability and robustness.",
            "year": 2019,
            "venue": "Knowledge Discovery and Data Mining",
            "authors": [
              {
                "authorId": "2144219662",
                "name": "Hao Wang"
              },
              {
                "authorId": "50383766",
                "name": "Tong Xu"
              },
              {
                "authorId": "50384136",
                "name": "Qi Liu"
              },
              {
                "authorId": "1862782",
                "name": "Defu Lian"
              },
              {
                "authorId": "2227868312",
                "name": "Enhong Chen"
              },
              {
                "authorId": "30124005",
                "name": "Dongfang Du"
              },
              {
                "authorId": "2112252643",
                "name": "Han Wu"
              },
              {
                "authorId": "2087139793",
                "name": "Wen Su"
              }
            ]
          }
        },
        {
          "citedcorpusid": 196200412,
          "isinfluential": false,
          "contexts": [
            "Representation learning aims at learning low-dimensional vector of the data and then applies it to downstream tasks [2, 39, 41, 43]."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Adversarial Substructured Representation Learning for Mobile User Profiling",
            "abstract": "Mobile user profiles are a summary of characteristics of user-specific mobile activities. Mobile user profiling is to extract a user's interest and behavioral patterns from mobile behavioral data. While some efforts have been made for mobile user profiling, existing methods can be improved via representation learning with awareness of substructures in users' behavioral graphs. Specifically, in this paper, we study the problem of mobile users profiling with POI check-in data. To this end, we first construct a graph, where a vertex is a POI category and an edge is the transition frequency of a user between two POI categories, to represent each user. We then formulate mobile user profiling as a task of representation learning from user behavioral graphs. We later develop a deep adversarial substructured learning framework for the task. This framework has two mutually-enhanced components. The first component is to preserve the structure of the entire graph, which is formulated as an encoding-decoding paradigm. In particular, the structure of the entire graph is preserved by minimizing reconstruction loss between an original graph and a reconstructed graph. The second component is to preserve the structure of subgraphs, which is formulated as a substructure detector based adversarial training paradigm. In particular, this paradigm includes a substructure detector and an adversarial trainer. Instead of using non-differentiable substructure detection algorithms, we pre-train a differentiable convolutional neural network as the detector to approximate these detection algorithms. The adversarial trainer is to match the detected substructure of the reconstructed graph to the detected substructure of the original graph. Also, we provide an effective solution for the optimization problems. Moreover, we exploit the learned representations of users for the next activity type prediction. Finally, we present extensive experimental results to demonstrate the improved performances of the proposed method.",
            "year": 2019,
            "venue": "Knowledge Discovery and Data Mining",
            "authors": [
              {
                "authorId": "35629977",
                "name": "Pengyang Wang"
              },
              {
                "authorId": "2274395",
                "name": "Yanjie Fu"
              },
              {
                "authorId": "144467554",
                "name": "Hui Xiong"
              },
              {
                "authorId": "2108673315",
                "name": "Xiaolin Li"
              }
            ]
          }
        },
        {
          "citedcorpusid": 215882005,
          "isinfluential": false,
          "contexts": [
            "TFE [47] employs random walks on both original and transpose networks to learn representations for competitive analysis."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Large-Scale Talent Flow Embedding for Company Competitive Analysis",
            "abstract": "Recent years have witnessed the growing interests in investigating the competition among companies. Existing studies for company competitive analysis generally rely on subjective survey data and inferential analysis. Instead, in this paper, we aim to develop a new paradigm for studying the competition among companies through the analysis of talent flows. The rationale behind this is that the competition among companies usually leads to talent movement. Along this line, we first build a Talent Flow Network based on the large-scale job transition records of talents, and formulate the concept of “competitiveness” for companies with consideration of their bi-directional talent flows in the network. Then, we propose a Talent Flow Embedding (TFE) model to learn the bi-directional talent attractions of each company, which can be leveraged for measuring the pairwise competitive relationships between companies. Specifically, we employ the random-walk based model in original and transpose networks respectively to learn representations of companies by preserving their competitiveness. Furthermore, we design a multi-task strategy to refine the learning results from a fine-grained perspective, which can jointly embed multiple talent flow networks by assuming the features of company keep stable but take different roles in networks of different job positions. Finally, extensive experiments on a large-scale real-world dataset clearly validate the effectiveness of our TFE model in terms of company competitive analysis and reveal some interesting rules of competition based on the derived insights on talent flows.",
            "year": 2020,
            "venue": "The Web Conference",
            "authors": [
              {
                "authorId": "2108006217",
                "name": "Le Zhang"
              },
              {
                "authorId": "41157498",
                "name": "Tong Xu"
              },
              {
                "authorId": "1968806",
                "name": "Hengshu Zhu"
              },
              {
                "authorId": "1986859645",
                "name": "Chuan Qin"
              },
              {
                "authorId": "2112722963",
                "name": "Qingxin Meng"
              },
              {
                "authorId": "2093122576",
                "name": "Hui Xiong"
              },
              {
                "authorId": "2227868312",
                "name": "Enhong Chen"
              }
            ]
          }
        },
        {
          "citedcorpusid": 216043471,
          "isinfluential": false,
          "contexts": [
            "And the others focus on adapting various techniques to design special classifiers for better classification results, including SVM [44], CNN [22] and Bert [32].",
            "For example, DeepPatent [22] built a deep convolutional neural network model combined with the word embedding to classify patent documents and PatentBert [32] exploited the powerful pre-training language model Bert [3] and then fine-tuning it to handle multi-label patent classification problem.",
            "On the other hand, specially designed patent classifiers [8] involve traditional non-deep learning meth-ods like association rule [11], and support vector machine [25, 44], and deep learning ones like CNN [12, 22, 25], GRU [31], Bert [32], graph neural networks [35] and so on."
          ],
          "intents": [
            "['methodology']",
            "['methodology']",
            "--"
          ],
          "cited_paper_info": {
            "title": "Multi-label Patent Classification using Attention-Aware Deep Learning Model",
            "abstract": "Patent classification is challenging and essential for any further patent analysis task. We tackle the classification task on lower level patent classification (subgroup level) by using AttentionXML. Recently, pretraining methods for Natural Language Processing (NLP), such as DistilBERT pre-trained model, have achieved state-of-the-art results on some NLP tasks such as text classification. In this work we focus on investigating the effect of applying DistilBERT pre-trained model and fine-tuning it for the important task of multi-label patent classification. Moreover, the large USPTO-3M dataset (3,050,625 patents) based on CPC subclass and subgroup level is used for the purpose of comparing previous deep learning related studies.",
            "year": 2020,
            "venue": "International Conference on Big Data and Smart Computing",
            "authors": [
              {
                "authorId": "9346492",
                "name": "A. H. Roudsari"
              },
              {
                "authorId": "145653991",
                "name": "Jafar Afshar"
              },
              {
                "authorId": "1646750026",
                "name": "Charles Cheolgi Lee"
              },
              {
                "authorId": "1728685",
                "name": "Wookey Lee"
              }
            ]
          }
        }
      ]
    },
    "232422905": {
      "citing_paper_info": {
        "title": "A Multi-task Approach to Neural Multi-label Hierarchical Patent Classification Using Transformers",
        "abstract": "",
        "year": 2021,
        "venue": "European Conference on Information Retrieval",
        "authors": [
          {
            "authorId": "2252806",
            "name": "S. Pujari"
          },
          {
            "authorId": "33985877",
            "name": "Annemarie Friedrich"
          },
          {
            "authorId": "2013656",
            "name": "Jannik Strotgen"
          }
        ]
      },
      "citation_summary": {
        "citation_count": 14,
        "unique_cited_count": 14,
        "influential_count": 3,
        "detailed_records_count": 14
      },
      "cited_papers": [
        "16447573",
        "202558505",
        "53277820",
        "8394887",
        "40100965",
        "31804187",
        "211532403",
        "9672033",
        "277550631",
        "52284222",
        "5959482",
        "207758666",
        "16136048",
        "67864300"
      ],
      "citation_details": [
        {
          "citedcorpusid": 5959482,
          "isinfluential": false,
          "contexts": [
            "We keep hyperparameter settings as proposed, representing each document using a 100-dimensional Word2Vec [25] model trained on train and dev, using 256 and 512 as the hidden sizes in the BiLSTM and for each fully connected layer, respectively."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Efficient Estimation of Word Representations in Vector Space",
            "abstract": "We propose two novel model architectures for computing continuous vector\nrepresentations of words from very large data sets. The quality of these\nrepresentations is measured in a word similarity task, and the results are\ncompared to the previously best performing techniques based on different types\nof neural networks. We observe large improvements in accuracy at much lower\ncomputational cost, i.e. it takes less than a day to learn high quality word\nvectors from a 1.6 billion words data set. Furthermore, we show that these\nvectors provide state-of-the-art performance on our test set for measuring\nsyntactic and semantic word similarities.",
            "year": 2013,
            "venue": "International Conference on Learning Representations",
            "authors": [
              {
                "authorId": "2047446108",
                "name": "Tomas Mikolov"
              },
              {
                "authorId": "2118440152",
                "name": "Kai Chen"
              },
              {
                "authorId": "32131713",
                "name": "G. Corrado"
              },
              {
                "authorId": "49959210",
                "name": "J. Dean"
              }
            ]
          }
        },
        {
          "citedcorpusid": 8394887,
          "isinfluential": false,
          "contexts": [
            "We here address the task of patent classification, which while constituting a hierarchical multi-label text classification problem, is often addressed using flat classifiers [11], though with several notable exceptions [4,5,36]."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Experiment with a hierarchical text categorization method on the WIPO-alpha patent collection",
            "abstract": "",
            "year": 2003,
            "venue": "Fourth International Symposium on Uncertainty Modeling and Analysis, 2003. ISUMA 2003.",
            "authors": [
              {
                "authorId": "1754164",
                "name": "D. Tikk"
              },
              {
                "authorId": "2286820537",
                "name": "Gy ¨ orgy Bir ´ o"
              }
            ]
          }
        },
        {
          "citedcorpusid": 9672033,
          "isinfluential": false,
          "contexts": [
            "Since the seminal works using Convolutional Neural Networks (CNNs) for sentence classification [16,18], neural modeling has become the predominant approach."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Convolutional Neural Networks for Sentence Classification",
            "abstract": "We report on a series of experiments with convolutional neural networks (CNN) trained on top of pre-trained word vectors for sentence-level classification tasks. We show that a simple CNN with little hyperparameter tuning and static vectors achieves excellent results on multiple benchmarks. Learning task-specific vectors through fine-tuning offers further gains in performance. We additionally propose a simple modification to the architecture to allow for the use of both task-specific and static vectors. The CNN models discussed herein improve upon the state of the art on 4 out of 7 tasks, which include sentiment analysis and question classification.",
            "year": 2014,
            "venue": "Conference on Empirical Methods in Natural Language Processing",
            "authors": [
              {
                "authorId": "38367242",
                "name": "Yoon Kim"
              }
            ]
          }
        },
        {
          "citedcorpusid": 16136048,
          "isinfluential": false,
          "contexts": [
            "Second, patent language often intentionally conceals the type of invention by avoiding terminology commonly used in technical reports [28]."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Automatic translation of scholarly terms into patent terms",
            "abstract": "",
            "year": 2009,
            "venue": "Current Challenges in Patent Information Retrieval",
            "authors": [
              {
                "authorId": "34996235",
                "name": "Hidetsugu Nanba"
              },
              {
                "authorId": "34512483",
                "name": "Hideaki Kamaya"
              },
              {
                "authorId": "1775372",
                "name": "T. Takezawa"
              },
              {
                "authorId": "144859189",
                "name": "M. Okumura"
              },
              {
                "authorId": "2658816",
                "name": "Akihiro Shinmori"
              },
              {
                "authorId": "2418662",
                "name": "H. Tanigawa"
              }
            ]
          }
        },
        {
          "citedcorpusid": 16447573,
          "isinfluential": false,
          "contexts": [
            "optimized by [1], proposes a convolutional neural network based on non-contextual word2vec [26] embeddings predicting IPC codes on subclass level."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Distributed Representations of Words and Phrases and their Compositionality",
            "abstract": "The recently introduced continuous Skip-gram model is an efficient method for learning high-quality distributed vector representations that capture a large number of precise syntactic and semantic word relationships. In this paper we present several extensions that improve both the quality of the vectors and the training speed. By subsampling of the frequent words we obtain significant speedup and also learn more regular word representations. We also describe a simple alternative to the hierarchical softmax called negative sampling. \n \nAn inherent limitation of word representations is their indifference to word order and their inability to represent idiomatic phrases. For example, the meanings of \"Canada\" and \"Air\" cannot be easily combined to obtain \"Air Canada\". Motivated by this example, we present a simple method for finding phrases in text, and show that learning good vector representations for millions of phrases is possible.",
            "year": 2013,
            "venue": "Neural Information Processing Systems",
            "authors": [
              {
                "authorId": "2047446108",
                "name": "Tomas Mikolov"
              },
              {
                "authorId": "1701686",
                "name": "I. Sutskever"
              },
              {
                "authorId": "2118440152",
                "name": "Kai Chen"
              },
              {
                "authorId": "32131713",
                "name": "G. Corrado"
              },
              {
                "authorId": "49959210",
                "name": "J. Dean"
              }
            ]
          }
        },
        {
          "citedcorpusid": 31804187,
          "isinfluential": false,
          "contexts": [
            "Despite having been studied in the data mining, ML, and IR communities for many years [2], text classiﬁcation remains a very active research ﬁeld addressing a variety of domains [15,23,37]."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Mining Text Data",
            "abstract": "",
            "year": 2012,
            "venue": "Springer US",
            "authors": [
              {
                "authorId": "1682418",
                "name": "C. Aggarwal"
              },
              {
                "authorId": "1736467",
                "name": "ChengXiang Zhai"
              }
            ]
          }
        },
        {
          "citedcorpusid": 40100965,
          "isinfluential": false,
          "contexts": [
            "In a recent shared task on patent classification [27], an approach training separate SVM classifiers per node using simple n-gram and POS-tag based features [5] performed comparably to a flat neural approach [12] based on the ULMFiT contextual language model [13]."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Universal Language Model Fine-tuning for Text Classification",
            "abstract": "Inductive transfer learning has greatly impacted computer vision, but existing approaches in NLP still require task-specific modifications and training from scratch. We propose Universal Language Model Fine-tuning (ULMFiT), an effective transfer learning method that can be applied to any task in NLP, and introduce techniques that are key for fine-tuning a language model. Our method significantly outperforms the state-of-the-art on six text classification tasks, reducing the error by 18-24% on the majority of datasets. Furthermore, with only 100 labeled examples, it matches the performance of training from scratch on 100 times more data. We open-source our pretrained models and code.",
            "year": 2018,
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "2093348519",
                "name": "Jeremy Howard"
              },
              {
                "authorId": "2884561",
                "name": "Sebastian Ruder"
              }
            ]
          }
        },
        {
          "citedcorpusid": 52284222,
          "isinfluential": false,
          "contexts": [
            "Outside the context of patent classification, [40] uses a very similar approach to [35]; [20] and [38] address neural hierarchical text classification by training level-wise classifiers and chaining predictions top-down."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Graph Convolutional Networks for Text Classification",
            "abstract": "Text classification is an important and classical problem in natural language processing. There have been a number of studies that applied convolutional neural networks (convolution on regular grid, e.g., sequence) to classification. However, only a limited number of studies have explored the more flexible graph convolutional neural networks (convolution on non-grid, e.g., arbitrary graph) for the task. In this work, we propose to use graph convolutional networks for text classification. We build a single text graph for a corpus based on word co-occurrence and document word relations, then learn a Text Graph Convolutional Network (Text GCN) for the corpus. Our Text GCN is initialized with one-hot representation for word and document, it then jointly learns the embeddings for both words and documents, as supervised by the known class labels for documents. Our experimental results on multiple benchmark datasets demonstrate that a vanilla Text GCN without any external word embeddings or knowledge outperforms state-of-the-art methods for text classification. On the other hand, Text GCN also learns predictive word and document embeddings. In addition, experimental results show that the improvement of Text GCN over state-of-the-art comparison methods become more prominent as we lower the percentage of training data, suggesting the robustness of Text GCN to less training data in text classification.",
            "year": 2018,
            "venue": "AAAI Conference on Artificial Intelligence",
            "authors": [
              {
                "authorId": "100680875",
                "name": "Liang Yao"
              },
              {
                "authorId": "145449667",
                "name": "Chengsheng Mao"
              },
              {
                "authorId": "1683396",
                "name": "Yuan Luo"
              }
            ]
          }
        },
        {
          "citedcorpusid": 53277820,
          "isinfluential": true,
          "contexts": [
            "We here address the task of patent classification, which while constituting a hierarchical multi-label text classification problem, is often addressed using flat classifiers [11], though with several notable exceptions [4,5,36].",
            "Such algorithms train one “local” classifier per node of the taxonomy predicting whether an instance belongs to the respective category or not, and have been shown to be highly effective for hierarchical patent classification in previous work using symbolic features such as n-grams and part-of-speech tags [4,5].",
            "While some architectures or algorithms directly reflect these taxonomies [4,5], others apply flat or global approaches either predicting only leaf-level labels or simply treating all labels independently [14,21,22].",
            "In a recent shared task on patent classification [27], an approach training separate SVM classifiers per node using simple n-gram and POS-tag based features [5] performed comparably to a flat neural approach [12] based on the ULMFiT contextual language model [13].",
            "First, instead of predicting labels at a single hierarchical level [1,5,21,22], we model predictions across the label taxonomy."
          ],
          "intents": [
            "['background']",
            "['background']",
            "['background']",
            "['methodology']",
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Classifying Patent Applications with Ensemble Methods",
            "abstract": "We present methods for the automatic classification of patent applications using an annotated dataset provided by the organizers of the ALTA 2018 shared task - Classifying Patent Applications. The goal of the task is to use computational methods to categorize patent applications according to a coarse-grained taxonomy of eight classes based on the International Patent Classification (IPC). We tested a variety of approaches for this task and the best results, 0.778 micro-averaged F1-Score, were achieved by SVM ensembles using a combination of words and characters as features. Our team, BMZ, was ranked first among 14 teams in the competition.",
            "year": 2018,
            "venue": "Australasian Language Technology Association Workshop",
            "authors": [
              {
                "authorId": "2043513",
                "name": "Fernando Benites"
              },
              {
                "authorId": "2854981",
                "name": "S. Malmasi"
              },
              {
                "authorId": "145130358",
                "name": "Marcos Zampieri"
              }
            ]
          }
        },
        {
          "citedcorpusid": 67864300,
          "isinfluential": false,
          "contexts": [
            "In a recent shared task on patent classification [27], an approach training separate SVM classifiers per node using simple n-gram and POS-tag based features [5] performed comparably to a flat neural approach [12] based on the ULMFiT contextual language model [13].",
            "Similarly, approaches based on contextual word embeddings and transformers have shown promising performance [12,21,22]."
          ],
          "intents": [
            "['methodology']",
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Universal Language Model Fine-tuning for Patent Classification",
            "abstract": "",
            "year": 2018,
            "venue": "Australasian Language Technology Association Workshop",
            "authors": [
              {
                "authorId": "73767302",
                "name": "Jason Hepburn"
              }
            ]
          }
        },
        {
          "citedcorpusid": 202558505,
          "isinfluential": true,
          "contexts": [
            "We address this large-scale classification task using a novel combination of a pretrained language model [3,8] and a local hierarchical learning algorithm.",
            "SciBERT has been trained on a corpus of scientific publications and is hence closer to the patent domain than the standard BERT model [8].",
            "underlying SciBERT neural language model [3] for creating document representations.",
            "First, flat-CNN corresponds to DeepPatent [22], which uses a CNN with kernels of sizes {3, 4, 5} and 512 filters on top of SciBERT.",
            "The ensemble of classifiers is trained in a multi-task setup and makes use of a single\nunderlying SciBERT neural language model [3] for creating document representations.",
            "First, neural models generally perform better than the non-neural TwistBytes system, with SciBERT-based models outperforming HARNN.",
            "We use the HuggingFace Transformers library [39] for integrating SciBERT [3].",
            "For each word-piece token, we compute an embedding by summing up the corresponding weights of the last four SciBERT layers.",
            "Second, flat-CLS is based on PatentBERT [21], using SciBERT’s 786-dimensional CLS embedding directly as document embedding."
          ],
          "intents": [
            "['methodology']",
            "--",
            "['background']",
            "--",
            "--",
            "--",
            "['methodology']",
            "--",
            "--"
          ],
          "cited_paper_info": {
            "title": "SciBERT: A Pretrained Language Model for Scientific Text",
            "abstract": "Obtaining large-scale annotated data for NLP tasks in the scientific domain is challenging and expensive. We release SciBERT, a pretrained language model based on BERT (Devlin et. al., 2018) to address the lack of high-quality, large-scale labeled scientific data. SciBERT leverages unsupervised pretraining on a large multi-domain corpus of scientific publications to improve performance on downstream scientific NLP tasks. We evaluate on a suite of tasks including sequence tagging, sentence classification and dependency parsing, with datasets from a variety of scientific domains. We demonstrate statistically significant improvements over BERT and achieve new state-of-the-art results on several of these tasks. The code and pretrained models are available at https://github.com/allenai/scibert/.",
            "year": 2019,
            "venue": "Conference on Empirical Methods in Natural Language Processing",
            "authors": [
              {
                "authorId": "46181066",
                "name": "Iz Beltagy"
              },
              {
                "authorId": "46258841",
                "name": "Kyle Lo"
              },
              {
                "authorId": "2527954",
                "name": "Arman Cohan"
              }
            ]
          }
        },
        {
          "citedcorpusid": 207758666,
          "isinfluential": true,
          "contexts": [
            "In addition, in line with previous work [14,38], we evaluate the predictions as a ranking task, which does not require defining a threshold.",
            "In this work, we compare to the state-of-the-art HARNN system [14] (see Sec.",
            "While some architectures or algorithms directly reflect these taxonomies [4,5], others apply flat or global approaches either predicting only leaf-level labels or simply treating all labels independently [14,21,22].",
            "Similar to prior work on neural patent classification [14,21,22], we use the patent’s title and abstract as input to our model.",
            "Prior work [14] has focused on evaluating per-instance (micro) scores.",
            "HARNN-orig [14] uses a prediction threshold of 0.",
            "In order to compare to a recent state-of-the-art neural model for hierarchical patent classification, we run the Hierarchical Attention-based Recurrent Neural Network [14] on our datasets."
          ],
          "intents": [
            "--",
            "['methodology']",
            "['background']",
            "['result']",
            "['methodology']",
            "['methodology']",
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Hierarchical Multi-label Text Classification: An Attention-based Recurrent Network Approach",
            "abstract": "Hierarchical multi-label text classification (HMTC) is a fundamental but challenging task of numerous applications (e.g., patent annotation), where documents are assigned to multiple categories stored in a hierarchical structure. Categories at different levels of a document tend to have dependencies. However, the majority of prior studies for the HMTC task employ classifiers to either deal with all categories simultaneously or decompose the original problem into a set of flat multi-label classification subproblems, ignoring the associations between texts and the hierarchical structure and the dependencies among different levels of the hierarchical structure. To that end, in this paper, we propose a novel framework called Hierarchical Attention-based Recurrent Neural Network (HARNN) for classifying documents into the most relevant categories level by level via integrating texts and the hierarchical category structure. Specifically, we first apply a documentation representing layer for obtaining the representation of texts and the hierarchical structure. Then, we develop an hierarchical attention-based recurrent layer to model the dependencies among different levels of the hierarchical structure in a top-down fashion. Here, a hierarchical attention strategy is proposed to capture the associations between texts and the hierarchical structure. Finally, we design a hybrid method which is capable of predicting the categories of each level while classifying all categories in the entire hierarchical structure precisely. Extensive experimental results on two real-world datasets demonstrate the effectiveness and explanatory power of HARNN.",
            "year": 2019,
            "venue": "International Conference on Information and Knowledge Management",
            "authors": [
              {
                "authorId": "145909469",
                "name": "Wei Huang"
              },
              {
                "authorId": "2227868312",
                "name": "Enhong Chen"
              },
              {
                "authorId": "50384136",
                "name": "Qi Liu"
              },
              {
                "authorId": "2142754662",
                "name": "Yuying Chen"
              },
              {
                "authorId": "145863758",
                "name": "Zai Huang"
              },
              {
                "authorId": "2152797757",
                "name": "Yang Liu"
              },
              {
                "authorId": "47122432",
                "name": "Zhou Zhao"
              },
              {
                "authorId": "2109982523",
                "name": "Dandan Zhang"
              },
              {
                "authorId": "2144180493",
                "name": "Shijin Wang"
              }
            ]
          }
        },
        {
          "citedcorpusid": 211532403,
          "isinfluential": false,
          "contexts": [
            "Recently, transformer-based neural language models such as BERT [8] have been shown to be highly effective for a variety of natural language processing tasks [33], following a “pre-train and fine-tune” approach."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "A Primer in BERTology: What We Know About How BERT Works",
            "abstract": "Abstract Transformer-based models have pushed state of the art in many areas of NLP, but our understanding of what is behind their success is still limited. This paper is the first survey of over 150 studies of the popular BERT model. We review the current state of knowledge about how BERT works, what kind of information it learns and how it is represented, common modifications to its training objectives and architecture, the overparameterization issue, and approaches to compression. We then outline directions for future research.",
            "year": 2020,
            "venue": "Transactions of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "145046059",
                "name": "Anna Rogers"
              },
              {
                "authorId": "152176221",
                "name": "Olga Kovaleva"
              },
              {
                "authorId": "1681193",
                "name": "Anna Rumshisky"
              }
            ]
          }
        },
        {
          "citedcorpusid": 277550631,
          "isinfluential": false,
          "contexts": [
            "We use the HuggingFace Transformers library [39] for integrating SciBERT [3]."
          ],
          "intents": [
            "--"
          ],
          "cited_paper_info": {
            "title": "HuggingFace's Transformers: State-of-the-art Natural Language Processing",
            "abstract": "Recent progress in natural language processing has been driven by advances in both model architecture and model pretraining. Transformer architectures have facilitated building higher-capacity models and pretraining has made it possible to effectively utilize this capacity for a wide variety of tasks. \\textit{Transformers} is an open-source library with the goal of opening up these advances to the wider machine learning community. The library consists of carefully engineered state-of-the art Transformer architectures under a unified API. Backing this library is a curated collection of pretrained models made by and available for the community. \\textit{Transformers} is designed to be extensible by researchers, simple for practitioners, and fast and robust in industrial deployments. The library is available at \\url{this https URL}.",
            "year": 2019,
            "venue": "arXiv.org",
            "authors": [
              {
                "authorId": "2257007291",
                "name": "Thomas Wolf"
              },
              {
                "authorId": "1380459402",
                "name": "Lysandre Debut"
              },
              {
                "authorId": "51918868",
                "name": "Victor Sanh"
              },
              {
                "authorId": "40811585",
                "name": "Julien Chaumond"
              },
              {
                "authorId": "40899333",
                "name": "Clement Delangue"
              },
              {
                "authorId": "1382164294",
                "name": "Anthony Moi"
              },
              {
                "authorId": "1382164165",
                "name": "Pierric Cistac"
              },
              {
                "authorId": "1382164170",
                "name": "Tim Rault"
              },
              {
                "authorId": "2185329",
                "name": "Rémi Louf"
              },
              {
                "authorId": "97662964",
                "name": "Morgan Funtowicz"
              },
              {
                "authorId": "48776237",
                "name": "Joe Davison"
              },
              {
                "authorId": "88728159",
                "name": "Sam Shleifer"
              },
              {
                "authorId": "138609838",
                "name": "Patrick von Platen"
              },
              {
                "authorId": "2257128341",
                "name": "Clara Ma"
              },
              {
                "authorId": "2268491803",
                "name": "Yacine Jernite"
              },
              {
                "authorId": "3008389",
                "name": "J. Plu"
              },
              {
                "authorId": "2257127518",
                "name": "Canwen Xu"
              },
              {
                "authorId": "1379806208",
                "name": "Teven Le Scao"
              },
              {
                "authorId": "103682620",
                "name": "Sylvain Gugger"
              },
              {
                "authorId": "2125818054",
                "name": "Mariama Drame"
              },
              {
                "authorId": "2113836945",
                "name": "Quentin Lhoest"
              },
              {
                "authorId": "2260132137",
                "name": "Alexander M. Rush"
              }
            ]
          }
        }
      ]
    },
    "174799315": {
      "citing_paper_info": {
        "title": "PatentBERT: Patent Classification with Fine-Tuning a pre-trained BERT Model",
        "abstract": "In this work we focus on fine-tuning a pre-trained BERT model and applying it to patent classification. When applied to large datasets of over two millions patents, our approach outperforms the state of the art by an approach using CNN with word embeddings. In addition, we focus on patent claims without other parts in patent documents. Our contributions include: (1) a new state-of-the-art method based on pre-trained BERT model and fine-tuning for patent classification, (2) a large dataset USPTO-3M at the CPC subclass level with SQL statements that can be used by future researchers, (3) showing that patent claims alone are sufficient for classification task, in contrast to conventional wisdom.",
        "year": 2019,
        "venue": "arXiv.org",
        "authors": [
          {
            "authorId": "2387987",
            "name": "Jieh-Sheng Lee"
          },
          {
            "authorId": "1798127",
            "name": "J. Hsiang"
          }
        ]
      },
      "citation_summary": {
        "citation_count": 4,
        "unique_cited_count": 4,
        "influential_count": 1,
        "detailed_records_count": 4
      },
      "cited_papers": [
        "43859778",
        "52967399",
        "3626819",
        "220953870"
      ],
      "citation_details": [
        {
          "citedcorpusid": 3626819,
          "isinfluential": false,
          "contexts": [
            "Such pre-training models include ELMo (Peters et al., 2018), ULMFiT (Universal Language Model with Fine-tuning) (Howard and Ruder, 2018), OpenAI Transformer (Radford et al., 2018) and BERT (Devlin et al., 2018).",
            "Such pre-training models include ELMo (Peters et al., 2018), ULMFiT (Universal Language Model with Fine-tuning) (Howard and Ruder, 2018), OpenAI Transformer (Radford et al."
          ],
          "intents": [
            "['methodology']",
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Deep Contextualized Word Representations",
            "abstract": "We introduce a new type of deep contextualized word representation that models both (1) complex characteristics of word use (e.g., syntax and semantics), and (2) how these uses vary across linguistic contexts (i.e., to model polysemy). Our word vectors are learned functions of the internal states of a deep bidirectional language model (biLM), which is pre-trained on a large text corpus. We show that these representations can be easily added to existing models and significantly improve the state of the art across six challenging NLP problems, including question answering, textual entailment and sentiment analysis. We also present an analysis showing that exposing the deep internals of the pre-trained network is crucial, allowing downstream models to mix different types of semi-supervision signals.",
            "year": 2018,
            "venue": "North American Chapter of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "39139825",
                "name": "Matthew E. Peters"
              },
              {
                "authorId": "50043859",
                "name": "Mark Neumann"
              },
              {
                "authorId": "2136562",
                "name": "Mohit Iyyer"
              },
              {
                "authorId": "40642935",
                "name": "Matt Gardner"
              },
              {
                "authorId": "143997772",
                "name": "Christopher Clark"
              },
              {
                "authorId": "2544107",
                "name": "Kenton Lee"
              },
              {
                "authorId": "1982950",
                "name": "Luke Zettlemoyer"
              }
            ]
          }
        },
        {
          "citedcorpusid": 43859778,
          "isinfluential": true,
          "contexts": [
            "o traditional text classifiers. Their dataset has 699,000 patents (70/30 as training/testing data split). Their best result 3 achieved 71.02% in micro-F1 score at the IPC subclass level. Lim and Kwon [14] showed 87.2% precision when using titles, abstracts, claims, technical fields and backgrounds of patents. However, no respective recall or F1 value was disclosed. A fair comparison is therefore not f"
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "IPC Multi-label Classification Based on the Field Functionality of Patent Documents",
            "abstract": "",
            "year": 2016,
            "venue": "International Conference on Advanced Data Mining and Applications",
            "authors": [
              {
                "authorId": "2174527",
                "name": "Sora Lim"
              },
              {
                "authorId": "2115745371",
                "name": "Y. Kwon"
              }
            ]
          }
        },
        {
          "citedcorpusid": 52967399,
          "isinfluential": false,
          "contexts": [
            "Hepburn (2018) used SVM and ULMFiT to achieve the best results in the student category."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
            "abstract": "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).",
            "year": 2019,
            "venue": "North American Chapter of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "39172707",
                "name": "Jacob Devlin"
              },
              {
                "authorId": "1744179",
                "name": "Ming-Wei Chang"
              },
              {
                "authorId": "2544107",
                "name": "Kenton Lee"
              },
              {
                "authorId": "3259253",
                "name": "Kristina Toutanova"
              }
            ]
          }
        },
        {
          "citedcorpusid": 220953870,
          "isinfluential": false,
          "contexts": [
            "Back to the CLEF-IP competition itself, Verberne and D’hondt [16] reached their best F1-value 70."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Quantifying the Challenges in Parsing Patent Claims",
            "abstract": "the 1st International Workshop on Advances in Patent Information Retrieval (AsPIRe-2010), 28 maart 2010",
            "year": 2010,
            "venue": "Patent Information Retrieval",
            "authors": [
              {
                "authorId": "1702730",
                "name": "S. Verberne"
              },
              {
                "authorId": "1447164726",
                "name": "E.K.L. D'hondt"
              },
              {
                "authorId": "52109694",
                "name": "N. Oostdijk"
              },
              {
                "authorId": "1713642",
                "name": "C. Koster"
              }
            ]
          }
        }
      ]
    },
    "52273103": {
      "citing_paper_info": {
        "title": "DeepPatent: patent classification with convolutional neural networks and word embedding",
        "abstract": "",
        "year": 2018,
        "venue": "Scientometrics",
        "authors": [
          {
            "authorId": "2124883266",
            "name": "Shaobo Li"
          },
          {
            "authorId": "145815844",
            "name": "Jie Hu"
          },
          {
            "authorId": "5925243",
            "name": "Yuxin Cui"
          },
          {
            "authorId": "50778791",
            "name": "Jianjun Hu"
          }
        ]
      },
      "citation_summary": {
        "citation_count": 7,
        "unique_cited_count": 7,
        "influential_count": 2,
        "detailed_records_count": 7
      },
      "cited_papers": [
        "156004541",
        "29338123",
        "313952",
        "5959482",
        "23750999",
        "12711052",
        "7449220"
      ],
      "citation_details": [
        {
          "citedcorpusid": 313952,
          "isinfluential": true,
          "contexts": [
            "2003), support vector machine (SVM) (Wu et al. 2010; Fall et al. 2003; D’Hondt et al. 2013), Naı̈ve Bayes (NB) (Fall et al.",
            "models have been proposed for patent classification such as the k-Nearest Neighbor (k-NN) (Fall et al. 2003), support vector machine (SVM) (Wu et al.",
            "2013), Naı̈ve Bayes (NB) (Fall et al. 2003; D’Hondt et al. 2013), k-means clustering (Kim et al.",
            "…the past few years, a number of different algorithms and models have been proposed for patent classification such as the k-Nearest Neighbor (k-NN) (Fall et al. 2003), support vector machine (SVM) (Wu et al. 2010; Fall et al. 2003; D’Hondt et al. 2013), Naı̈ve Bayes (NB) (Fall et al. 2003; D’Hondt…",
            "Many algorithms have been used to classify patent documents, among which ANN, SVMs, and kNN are the most commonly used algorithms in the field of automated patent classification (Benzineb and Guyot 2011).",
            "…k-Nearest Neighbor (k-NN) (Fall et al. 2003), support vector machine (SVM) (Wu et al. 2010; Fall et al. 2003; D’Hondt et al. 2013), Naı̈ve Bayes (NB) (Fall et al. 2003; D’Hondt et al. 2013), k-means clustering (Kim et al. 2008), and artificial neural network (ANN) (Trappey et al. 2006; Guyot et…",
            "Chen and Chang (2012) presented a three-phase categorization method which contains SVM, K-means, and kNN algorithms.",
            "The top 50 recall scores of DeepPatent are also much higher than of those of SVMLight (97.35% compared to 87.61% and 89.56%).",
            "From the first 3 rows of Table 3, it can be found that DeepPatent apparently achieved the best performance with Top 4 F1 score of 55.09% compared to 47.42% and 48.56% of SVMLight.",
            "They found that their HGA-SVM was able to increase the prediction accuracy by 1.7% of the SVM patent classification system.",
            "Zhang et al. (2015) proposed a method for sentiment classification based on word vector and SVM.",
            "They use word vector to cluster similar features in the selected domain (Chinese language) and put the features into an SVM algorithm, which achieved superior performance in sentiment classification.",
            "…have been proposed for patent classification such as the k-Nearest Neighbor (k-NN) (Fall et al. 2003), support vector machine (SVM) (Wu et al. 2010; Fall et al. 2003; D’Hondt et al. 2013), Naı̈ve Bayes (NB) (Fall et al. 2003; D’Hondt et al. 2013), k-means clustering (Kim et al. 2008), and…",
            "It is found that our DeepPatent algorithm significantly outperforms most of the other methods using only the title and abstract information of the patents, achieving a Top1 precision of 83.98%, better than 69.95% and 71.85% of SVMLight (which use the abstract information) and 74.43% by LCS (which uses abstract and description).",
            "For example, the SVM-based model using combined handcrafted similarity and semantic features achieved the best performance (82.1% top1 precision) over the EPO corpus when the full content is used.",
            "Out of the 11 methods compared in Table 3, DeepPatent, SVMLights, and LCS all use the abstract with or without titles, which makes their performance to be more comparable.",
            "The performance of DeepPatent with 83.98% top1 precision is better than the state-of-the-art result of 82.1% top1 precision achieved by the SVM with full content information of the patent as sample input and complicated human-designed features.",
            "DeepPatent also performs better than the SVM-based algorithm reported in (Derieux et al. 2010) that uses two types of human-designed features including similarity and semantic and statistics along with full content information as input.",
            "Our DeepPatent algorithm with automatic feature extraction has achieved a precision of 81.11% for top 1 label predictions, which is better than all traditional machine learning algorithms except for one SVM method with precision score 82.1% that was derived with a large amount of feature engineering effort and using the full content as input.",
            "A hybrid genetic algorithm support vector machines (SVMs) was then applied on this data set, which reached an accuracy of 82%.",
            "Wu et al. (2010) applied various types SVM kernel functions to various source datasets created by using the title, abstract, claim, and the description part of the English patent documents.",
            "In the past few years, a number of different algorithms and models have been proposed for patent classification such as the k-Nearest Neighbor (k-NN) (Fall et al. 2003), support vector machine (SVM) (Wu et al. 2010; Fall et al. 2003; D’Hondt et al. 2013), Naı̈ve Bayes (NB) (Fall et al. 2003; D’Hondt et al. 2013), k-means clustering (Kim et al. 2008), and artificial neural network (ANN) (Trappey et al. 2006; Guyot et al. 2010), but all with limited success."
          ],
          "intents": [
            "--",
            "['methodology']",
            "--",
            "['methodology']",
            "--",
            "['background']",
            "--",
            "--",
            "--",
            "--",
            "--",
            "--",
            "['methodology']",
            "--",
            "--",
            "--",
            "--",
            "--",
            "--",
            "--",
            "--",
            "--"
          ],
          "cited_paper_info": {
            "title": "Automated categorization in the international patent classification",
            "abstract": "",
            "year": 2003,
            "venue": "SIGF",
            "authors": [
              {
                "authorId": "8635679",
                "name": "C. Fall"
              },
              {
                "authorId": "2893823",
                "name": "A. Törcsvári"
              },
              {
                "authorId": "2217341",
                "name": "K. Benzineb"
              },
              {
                "authorId": "2570524",
                "name": "G. Karetka"
              }
            ]
          }
        },
        {
          "citedcorpusid": 5959482,
          "isinfluential": true,
          "contexts": [
            "The effectiveness of pre-trained word embedding\nIn this study, the continuous skip-gram (Mikolov et al. 2013b) model is used to pre-train our dataset, which can capture a large number of precise syntactic and semantic word relationships and learn high-quality distributed vector representations for…",
            "To address this issue, negative-sampling (Mikolov et al. 2013a) can be used to reduce the cost of computation.",
            "One of the preeminent VSMs text encoding approach is the emerging word vector (Mikolov et al. 2013b), which has been shown to be able to capture meaningful syntactic and semantic regularities and identify text contents and subsets of the content.",
            "In this paper, we use the skip-gram model proposed by Mikolov et al. (2013a) as our distributed word representation approach."
          ],
          "intents": [
            "['methodology']",
            "['methodology']",
            "['background']",
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Efficient Estimation of Word Representations in Vector Space",
            "abstract": "We propose two novel model architectures for computing continuous vector\nrepresentations of words from very large data sets. The quality of these\nrepresentations is measured in a word similarity task, and the results are\ncompared to the previously best performing techniques based on different types\nof neural networks. We observe large improvements in accuracy at much lower\ncomputational cost, i.e. it takes less than a day to learn high quality word\nvectors from a 1.6 billion words data set. Furthermore, we show that these\nvectors provide state-of-the-art performance on our test set for measuring\nsyntactic and semantic word similarities.",
            "year": 2013,
            "venue": "International Conference on Learning Representations",
            "authors": [
              {
                "authorId": "2047446108",
                "name": "Tomas Mikolov"
              },
              {
                "authorId": "2118440152",
                "name": "Kai Chen"
              },
              {
                "authorId": "32131713",
                "name": "G. Corrado"
              },
              {
                "authorId": "49959210",
                "name": "J. Dean"
              }
            ]
          }
        },
        {
          "citedcorpusid": 7449220,
          "isinfluential": false,
          "contexts": [
            "D’hondt and Verberne (2010) showed that the title and abstract sections are more informative than the full-text representation of the patent document for patent retrieval."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "CLEF-IP 2010: Prior Art Retrieval Using the Different Sections in Patent Documents",
            "abstract": "In this paper we describe our participation in the 2010 CLEF-IP Prior Art Retrieval task where we examined the impact of information in dierent sections of patent documents, namely the title, abstract, claims, description and IPC-R sections, on the retrieval and re-ranking of patent documents. Using a standard bag-of-words approach in Lemur we found that the IPC-R sections are the most informative for patent retrieval. We then performed a re-ranking of the retrieved documents using a Logistic Regression Model, trained on the retrieved documents in the training set. We found indications that the information contained in the text sections of the patent document can contribute to a better ranking of the retrieved documents. The ocial results have shown that among the nine groups that participated in the Prior Art Retrieval task we achieved the eigth rank in terms of both Mean Average Precision (MAP) and Recall.",
            "year": 2010,
            "venue": "Conference and Labs of the Evaluation Forum",
            "authors": [
              {
                "authorId": "1447164726",
                "name": "E.K.L. D'hondt"
              },
              {
                "authorId": "1702730",
                "name": "S. Verberne"
              }
            ]
          }
        },
        {
          "citedcorpusid": 12711052,
          "isinfluential": false,
          "contexts": [
            "For each experiment, we use the followings evaluation metrics as used in CLEP-IP competition (Piroi et al. 2011) to evaluate various methods.",
            "Our study has two stages: at the first stage, we extract title and abstract sections from two patent corpuses USPTO-2 M and CLEF-IP 2011 (Piroi et al. 2011), and then use the skipgram model to transform words in the extracted text into encoding real-value vectors."
          ],
          "intents": [
            "['methodology']",
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "CLEF-IP 2011: Retrieval in the Intellectual Property Domain",
            "abstract": "The patent system is designed to encourage disclosure of new technologies and novel ideas by granting exclusive rights on the use of inventions to their inventors, for a limited period of time. Before a patent can be granted, patent o ces around the world perform thorough searches to ensure that no previous similar disclosures were made. In the intellectual property terminology, such kind of searches are called prior art searches. In some industries, the number of granted patents a company owns has a high impact on the market value of the company. This underlines the importance of well-performed prior art searches. Together with the Trec Chem track [5], also organized by our institution, the Clef Ip e ort comes to complete the work that is being done in the series of Ntcir workshops (see for example [4]). The rst Clef Ip track ran within Clef 2009. The purpose of the track was twofold: to encourage and facilitate research in the area of patent retrieval by providing a large clean data set for experimentation; to create a large test collection of patents in the three main European languages for the evaluation of cross lingual information access. The Clef Ip data set includes documents published by the European Patent O ce (Epo) which contain a mixture of English, German and French content. The track focused on the task of prior art search. In 2010 and 2011, the Clef Ip track was organized as a benchmarking activity (lab) in the Clef conference. In these years, the main goal of the Clef Ip e ort remained the same to foster research in the patent retrieval area, and provide a large clean data set. To this end, the number of tasks in the track was increased and the data set was enlarged. Recognizing the importance of patent classi cations in the daily activity of an intellectual property professional, in 2010 the Clef Ip benchmarking activity included a patent classi cation task. The participants were asked to classify",
            "year": 2011,
            "venue": "Conference and Labs of the Evaluation Forum",
            "authors": [
              {
                "authorId": "3309646",
                "name": "Florina Piroi"
              },
              {
                "authorId": "144307089",
                "name": "M. Lupu"
              },
              {
                "authorId": "1699657",
                "name": "A. Hanbury"
              },
              {
                "authorId": "2978384",
                "name": "V. Zenz"
              }
            ]
          }
        },
        {
          "citedcorpusid": 23750999,
          "isinfluential": false,
          "contexts": [
            "Word vector models are developed based on the distributional hypothesis (Sahlgren 2008), which states that words that appear in the same contexts share similar semantic meaning."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "The Distributional Hypothesis",
            "abstract": "Distributional approaches to meaning acquisition utilize distributional properties of linguistic entities as the building blocks of semantics. In doing so, they rely fundamentally on a set of assumptions about the nature of language and meaning referred to as the distributional hypothesis. This hypothesis is often stated in terms like “words which are similar in meaning occur in similar contexts” (Rubenstein & Goodenough, 1965); “words with similar meanings will occur with similar neighbors if enough text material is available” (Schutze & Pedersen, 1995); “a representation that captures much of how words are used in natural context will capture much of what we mean by meaning” (Landauer & Dumais, 1997); and “words that occur in the same contexts tend to have similar meanings” (Pantel, 2005), just to quote a few representative examples. The general idea behind the distributional hypothesis seems clear enough: there is a correlation between distributional similarity and meaning similarity, which allows us to utilize the former in order to estimate the latter. However, one can pose two very basic questions concerning the distributional hypothesis. The first is what kind of distributional properties we should look for, and what — if any — the differences are between different kinds of distributional properties. Looking at algorithms for distributional meaning acquisition we can discern two distinct approaches. The first is to build distributional profiles for words based on which other words surround them, as exemplified by Schutze (1992) and the Hyperspace Analogue to Language (HAL) model (Lund, Burgess, & Atchley, 1995). The second is to build distributional profiles based on in which text regions words occur, as exemplified by the Latent Semantic Analysis (LSA) model (Landauer & Dumais, 1997). These approaches are often treated as functionally equivalent when it comes to representing meaning similarities, despite the fact that they are based on different types of distributional raw materials. The second question is in what sense it is meaning that is conveyed by distributional patterns. Proponents of distributional methods often seem comfortable to ascribe meaning to distributional representations without explaining",
            "year": 2008,
            "venue": "",
            "authors": [
              {
                "authorId": "1689109",
                "name": "Magnus Sahlgren"
              }
            ]
          }
        },
        {
          "citedcorpusid": 29338123,
          "isinfluential": false,
          "contexts": [
            "Moreover, patent information usually stays current as most patent applications are published 18 months after the first filing, irrespective of their country of origin (Wagner and Wakeman 2016)."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Contracting in Medical Equipment Maintenance Services: An Empirical Investigation",
            "abstract": "Equipment manufacturers offer different types of maintenance service plans (MSPs) that delineate payment structures between equipment operators and maintenance service providers. These MSPs allocate risks differently and thus induce different kinds of incentives. A fundamental question, therefore, is how such structures impact service performance and the service chain value. We answer empirically this question. Our study is based on a unique panel data covering the sales and service records of over 700 diagnostic medical body scanners. By exploiting the presence of a standard warranty period, we overcome the key challenge of isolating the incentive effects of MSPs on service performance from the confounding effects of adverse selection. We found that moving an operator from a basic pay-per-service plan to a fixed-fee full-protection plan leads to both a reduction in reliability and an increase in service costs. We further show that the increase in cost is driven by both the operator and the service provider. Our results point to the presence of losses in service chain value in the maintenance of medical equipment, and provide the first evidence that a basic pay-per-service plan, where the risk of equipment failure is borne by the operator, can actually improve performance and costs.",
            "year": 2014,
            "venue": "Management Sciences",
            "authors": [
              {
                "authorId": "2251087100",
                "name": "Tian Heong Chan"
              },
              {
                "authorId": "3200502",
                "name": "F. Véricourt"
              },
              {
                "authorId": "2463453",
                "name": "Omar Besbes"
              }
            ]
          }
        },
        {
          "citedcorpusid": 156004541,
          "isinfluential": false,
          "contexts": [
            "2013), technology transfer (Lemley and Feldman 2016), technology innovation (Lee et al.",
            "…patent databases to enhance their research and development (R&D) activities such as new product development (Li et al. 2013), technology transfer (Lemley and Feldman 2016), technology innovation (Lee et al. 2012), technology forecasting (Altuntas et al. 2015) and mergers and technology…"
          ],
          "intents": [
            "['background']",
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Patent Licensing, Technology Transfer, & Innovation",
            "abstract": "Traditional justifications for patents are based on direct or indirect contribution to product creation. Non-practicing entities (NPEs) might provide such innovation, either directly, through working the patent or transfer of technology to others who do, or indirectly, when others copy. Available evidence suggests, however, that ex post licensing demands from NPEs do not normally involve these activities. Some have argued that patents are valuable without technology transfer because the ability to exclude may drive commercialization that would not otherwise occur. We demonstrate that even if commercialization theories sometimes justify patent protection, they cannot justify most NPE lawsuits or licensing demands.",
            "year": 2016,
            "venue": "",
            "authors": [
              {
                "authorId": "1751962",
                "name": "Mark A. Lemley"
              },
              {
                "authorId": "48725386",
                "name": "Robin C. Feldman"
              }
            ]
          }
        }
      ]
    },
    "132667166": {
      "citing_paper_info": {
        "title": "Domain-specific word embeddings for patent classification",
        "abstract": "\nPurpose\nPatent offices and other stakeholders in the patent domain need to classify patent applications according to a standardized classification scheme. The purpose of this paper is to examine the novelty of an application it can then be compared to previously granted patents in the same class. Automatic classification would be highly beneficial, because of the large volume of patents and the domain-specific knowledge needed to accomplish this costly manual task. However, a challenge for the automation is patent-specific language use, such as special vocabulary and phrases.\n\n\nDesign/methodology/approach\nTo account for this language use, the authors present domain-specific pre-trained word embeddings for the patent domain. The authors train the model on a very large data set of more than 5m patents and evaluate it at the task of patent classification. To this end, the authors propose a deep learning approach based on gated recurrent units for automatic patent classification built on the trained word embeddings.\n\n\nFindings\nExperiments on a standardized evaluation data set show that the approach increases average precision for patent classification by 17 percent compared to state-of-the-art approaches. In this paper, the authors further investigate the model’s strengths and weaknesses. An extensive error analysis reveals that the learned embeddings indeed mirror patent-specific language use. The imbalanced training data and underrepresented classes are the most difficult remaining challenge.\n\n\nOriginality/value\nThe proposed approach fulfills the need for domain-specific word embeddings for downstream tasks in the patent domain, such as patent classification or patent analysis.\n",
        "year": 2019,
        "venue": "Data Technologies and Applications",
        "authors": [
          {
            "authorId": "1695993",
            "name": "Julian Risch"
          },
          {
            "authorId": "3264110",
            "name": "Ralf Krestel"
          }
        ]
      },
      "citation_summary": {
        "citation_count": 8,
        "unique_cited_count": 8,
        "influential_count": 1,
        "detailed_records_count": 8
      },
      "cited_papers": [
        "12577135",
        "4456086",
        "5959482",
        "12711052",
        "1957433",
        "2125960",
        "1407274",
        "7449220"
      ],
      "citation_details": [
        {
          "citedcorpusid": 1407274,
          "isinfluential": true,
          "contexts": [
            "Dhondt et al. report a microf1 0.751 and a micro-precision of 0.800 on a subset of 532,264 English abstracts from the so called CLEF-IP 2010 corpus [13].",
            "With regard to the usefulness of metadata, such as applicants, inventors, and address, they conclude that it does not improve classification [8].",
            "Abstract and description achieve best precision and recall at the retrieval task and significantly outperform title and claims [8], [16].",
            "[7] x Verberne and Dhondt [8] x Li et al.",
            "D’hondt et al. find that bigrams are important phrasal features to capture multi-word terms, which are frequent in patents [15]."
          ],
          "intents": [
            "--",
            "['background']",
            "['background']",
            "['background']",
            "--"
          ],
          "cited_paper_info": {
            "title": "Patent Classification Experiments with the Linguistic Classification System LCS",
            "abstract": "We report the results of a series of classification experiments with the Linguistic Classification System LCS in the context of CLEF-IP 2011. We participated in the main classification task: classifying documents on the subclass level. We investigated (1) the use of different sections (abstract, description, metadata) from the patent documents; (2) adding dependency triples to the bag-of-words representation; (3) adding the WIPO corpus to the EPO training data; (4) the use of patent citations in the test data for reranking the classes; and (5) the threshold on the class scores for class selection. We found that adding full descriptions to abstracts gives a clear improvement; the first 400 words of the description also improves classification but to a lesser degree. Adding metadata (applicants, inventors en address) did not improve classification. Adding dependency triples to words gives a much higher recall at the cost of a lower precision but this effect is largely due to the class selection threshold. We did not find an effect from adding the WIPO corpus, nor from reranking with patent citations. In future work, we plan to investigate whether there are other methods for reranking with patent citations that does give an improvement, because we feel that the citations may still give valuable information. Our most important finding however is the importance of the threshold on the class selection. For the current work, we only compared two values for the threshold and the results are much better for 1.0 than for 0.5. The 0.5 threshold gives higher recall in all runs, which was the original motivation for submitting runs with a lower threshold. However, because the much lower precision, the F-scores are lower. We think that there is still some improvement to be gained from proper tuning of the class selection threshold, and the use of a flexible threshold (also taking into account the different text representations). This is part of our future work.",
            "year": 2010,
            "venue": "Conference and Labs of the Evaluation Forum",
            "authors": [
              {
                "authorId": "1702730",
                "name": "S. Verberne"
              },
              {
                "authorId": "25486420",
                "name": "M. Vogel"
              },
              {
                "authorId": "1447164726",
                "name": "E.K.L. D'hondt"
              }
            ]
          }
        },
        {
          "citedcorpusid": 1957433,
          "isinfluential": false,
          "contexts": [
            "A similar approach, termed global vectors (GloVe), trains word embeddings on global word-word co-occurrence counts rather than on context windows of limited size [20]."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "GloVe: Global Vectors for Word Representation",
            "abstract": "Recent methods for learning vector space representations of words have succeeded in capturing fine-grained semantic and syntactic regularities using vector arithmetic, but the origin of these regularities has remained opaque. We analyze and make explicit the model properties needed for such regularities to emerge in word vectors. The result is a new global logbilinear regression model that combines the advantages of the two major model families in the literature: global matrix factorization and local context window methods. Our model efficiently leverages statistical information by training only on the nonzero elements in a word-word cooccurrence matrix, rather than on the entire sparse matrix or on individual context windows in a large corpus. The model produces a vector space with meaningful substructure, as evidenced by its performance of 75% on a recent word analogy task. It also outperforms related models on similarity tasks and named entity recognition.",
            "year": 2014,
            "venue": "Conference on Empirical Methods in Natural Language Processing",
            "authors": [
              {
                "authorId": "143845796",
                "name": "Jeffrey Pennington"
              },
              {
                "authorId": "2166511",
                "name": "R. Socher"
              },
              {
                "authorId": "144783904",
                "name": "Christopher D. Manning"
              }
            ]
          }
        },
        {
          "citedcorpusid": 2125960,
          "isinfluential": false,
          "contexts": [
            "compare automatic document classification for the two classification schemes [26]."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Automated Patent Categorization and Guided Patent Search using IPC as Inspired by MeSH and PubMed",
            "abstract": "Document search on PubMed, the pre-eminent database for biomedical literature, relies on the annotation of its documents with relevant terms from the Medical Subject Headings ontology (MeSH) for improving recall through query expansion. Patent documents are another important information source, though they are considerably less accessible. One option to expand patent search beyond pure keywords is the inclusion of classification information: Since every patent is assigned at least one class code, it should be possible for these assignments to be automatically used in a similar way as the MeSH annotations in PubMed. In order to develop a system for this task, it is necessary to have a good understanding of the properties of both classification systems. This report describes our comparative analysis of MeSH and the main patent classification system, the International Patent Classification (IPC). We investigate the hierarchical structures as well as the properties of the terms/classes respectively, and we compare the assignment of IPC codes to patents with the annotation of PubMed documents with MeSH terms. Our analysis shows a strong structural similarity of the hierarchies, but significant differences of terms and annotations. The low number of IPC class assignments and the lack of occurrences of class labels in patent texts imply that current patent search is severely limited. To overcome these limits, we evaluate a method for the automated assignment of additional classes to patent documents, and we propose a system for guided patent search based on the use of class co-occurrence information and external resources.",
            "year": 2013,
            "venue": "Journal of Biomedical Semantics",
            "authors": [
              {
                "authorId": "5936705",
                "name": "Daniela A. Eisinger"
              },
              {
                "authorId": "46855299",
                "name": "G. Tsatsaronis"
              },
              {
                "authorId": "2168590",
                "name": "Markus Bundschus"
              },
              {
                "authorId": "1407743556",
                "name": "U. Wieneke"
              },
              {
                "authorId": "79909855",
                "name": "M. Schroeder"
              }
            ]
          }
        },
        {
          "citedcorpusid": 4456086,
          "isinfluential": false,
          "contexts": [
            "that applicant and address improves classification [17]."
          ],
          "intents": [
            "--"
          ],
          "cited_paper_info": {
            "title": "LCI-INSA Linguistic Experiment for CLEF-IP Classification Track",
            "abstract": "We present the experiment the LCI group has performed to prepare our submission to CLEF-IP Classification Track. In this preliminary experiment we used a part of the available target documents as test set and the rest as train set. We describe the systems AGFL used for extracting these triples and the LCS used for classification by the Winnow algorithm. We show that the use of linguistic triples in place of bags of words improves the accuracy, as well as using the names and addresses of the applicants. we found that using the complete descriptions as bags of words does not really perform better than using only abstracts and titles. Some simple mathematics show that the official measures are redundant and that R@N should be used to evaluate a ranking, P@1 to evaluate routing and that the usual precision, recall and F1 should be used on the results of a real classification, that is a selection of the classes performed internally by the classifier.",
            "year": 2010,
            "venue": "Conference and Labs of the Evaluation Forum",
            "authors": [
              {
                "authorId": "145639626",
                "name": "Jean Beney"
              }
            ]
          }
        },
        {
          "citedcorpusid": 5959482,
          "isinfluential": false,
          "contexts": [
            "propose an efficient way to train word embeddings [19].",
            "of tokens, relations of these representations in a vector space can mirror semantic relations of words [19]."
          ],
          "intents": [
            "['background']",
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Efficient Estimation of Word Representations in Vector Space",
            "abstract": "We propose two novel model architectures for computing continuous vector\nrepresentations of words from very large data sets. The quality of these\nrepresentations is measured in a word similarity task, and the results are\ncompared to the previously best performing techniques based on different types\nof neural networks. We observe large improvements in accuracy at much lower\ncomputational cost, i.e. it takes less than a day to learn high quality word\nvectors from a 1.6 billion words data set. Furthermore, we show that these\nvectors provide state-of-the-art performance on our test set for measuring\nsyntactic and semantic word similarities.",
            "year": 2013,
            "venue": "International Conference on Learning Representations",
            "authors": [
              {
                "authorId": "2047446108",
                "name": "Tomas Mikolov"
              },
              {
                "authorId": "2118440152",
                "name": "Kai Chen"
              },
              {
                "authorId": "32131713",
                "name": "G. Corrado"
              },
              {
                "authorId": "49959210",
                "name": "J. Dean"
              }
            ]
          }
        },
        {
          "citedcorpusid": 7449220,
          "isinfluential": false,
          "contexts": [
            "Abstract and description achieve best precision and recall at the retrieval task and significantly outperform title and claims [8], [16].",
            "2010 patent retrieval and re-ranking tasks [16]."
          ],
          "intents": [
            "['background']",
            "--"
          ],
          "cited_paper_info": {
            "title": "CLEF-IP 2010: Prior Art Retrieval Using the Different Sections in Patent Documents",
            "abstract": "In this paper we describe our participation in the 2010 CLEF-IP Prior Art Retrieval task where we examined the impact of information in dierent sections of patent documents, namely the title, abstract, claims, description and IPC-R sections, on the retrieval and re-ranking of patent documents. Using a standard bag-of-words approach in Lemur we found that the IPC-R sections are the most informative for patent retrieval. We then performed a re-ranking of the retrieved documents using a Logistic Regression Model, trained on the retrieved documents in the training set. We found indications that the information contained in the text sections of the patent document can contribute to a better ranking of the retrieved documents. The ocial results have shown that among the nine groups that participated in the Prior Art Retrieval task we achieved the eigth rank in terms of both Mean Average Precision (MAP) and Recall.",
            "year": 2010,
            "venue": "Conference and Labs of the Evaluation Forum",
            "authors": [
              {
                "authorId": "1447164726",
                "name": "E.K.L. D'hondt"
              },
              {
                "authorId": "1702730",
                "name": "S. Verberne"
              }
            ]
          }
        },
        {
          "citedcorpusid": 12577135,
          "isinfluential": false,
          "contexts": [
            "An ensemble of different classifiers slightly improves micro-F1 score on a refined version of the WIPO-alpha dataset according to Mathiassen and Ortiz-Arroyo [11]."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Automatic Categorization of Patent Applications Using Classifier Combinations",
            "abstract": "",
            "year": 2006,
            "venue": "Ideal",
            "authors": [
              {
                "authorId": "2257894",
                "name": "Henrik Mathiassen"
              },
              {
                "authorId": "34800059",
                "name": "D. O. Arroyo"
              }
            ]
          }
        },
        {
          "citedcorpusid": 12711052,
          "isinfluential": false,
          "contexts": [
            "As its predecessor, the CLEF-IP track of 2011 [14] provided datasets and tasks for a large number of publications concerning retrieval in the intellectual property domain."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "CLEF-IP 2011: Retrieval in the Intellectual Property Domain",
            "abstract": "The patent system is designed to encourage disclosure of new technologies and novel ideas by granting exclusive rights on the use of inventions to their inventors, for a limited period of time. Before a patent can be granted, patent o ces around the world perform thorough searches to ensure that no previous similar disclosures were made. In the intellectual property terminology, such kind of searches are called prior art searches. In some industries, the number of granted patents a company owns has a high impact on the market value of the company. This underlines the importance of well-performed prior art searches. Together with the Trec Chem track [5], also organized by our institution, the Clef Ip e ort comes to complete the work that is being done in the series of Ntcir workshops (see for example [4]). The rst Clef Ip track ran within Clef 2009. The purpose of the track was twofold: to encourage and facilitate research in the area of patent retrieval by providing a large clean data set for experimentation; to create a large test collection of patents in the three main European languages for the evaluation of cross lingual information access. The Clef Ip data set includes documents published by the European Patent O ce (Epo) which contain a mixture of English, German and French content. The track focused on the task of prior art search. In 2010 and 2011, the Clef Ip track was organized as a benchmarking activity (lab) in the Clef conference. In these years, the main goal of the Clef Ip e ort remained the same to foster research in the patent retrieval area, and provide a large clean data set. To this end, the number of tasks in the track was increased and the data set was enlarged. Recognizing the importance of patent classi cations in the daily activity of an intellectual property professional, in 2010 the Clef Ip benchmarking activity included a patent classi cation task. The participants were asked to classify",
            "year": 2011,
            "venue": "Conference and Labs of the Evaluation Forum",
            "authors": [
              {
                "authorId": "3309646",
                "name": "Florina Piroi"
              },
              {
                "authorId": "144307089",
                "name": "M. Lupu"
              },
              {
                "authorId": "1699657",
                "name": "A. Hanbury"
              },
              {
                "authorId": "2978384",
                "name": "V. Zenz"
              }
            ]
          }
        }
      ]
    },
    "4565108": {
      "citing_paper_info": {
        "title": "Patent Keyword Extraction Algorithm Based on Distributed Representation for Patent Classification",
        "abstract": "Many text mining tasks such as text retrieval, text summarization, and text comparisons depend on the extraction of representative keywords from the main text. Most existing keyword extraction algorithms are based on discrete bag-of-words type of word representation of the text. In this paper, we propose a patent keyword extraction algorithm (PKEA) based on the distributed Skip-gram model for patent classification. We also develop a set of quantitative performance measures for keyword extraction evaluation based on information gain and cross-validation, based on Support Vector Machine (SVM) classification, which are valuable when human-annotated keywords are not available. We used a standard benchmark dataset and a homemade patent dataset to evaluate the performance of PKEA. Our patent dataset includes 2500 patents from five distinct technological fields related to autonomous cars (GPS systems, lidar systems, object recognition systems, radar systems, and vehicle control systems). We compared our method with Frequency, Term Frequency-Inverse Document Frequency (TF-IDF), TextRank and Rapid Automatic Keyword Extraction (RAKE). The experimental results show that our proposed algorithm provides a promising way to extract keywords from patent texts for patent classification.",
        "year": 2018,
        "venue": "Entropy",
        "authors": [
          {
            "authorId": "145815844",
            "name": "Jie Hu"
          },
          {
            "authorId": "2124883266",
            "name": "Shaobo Li"
          },
          {
            "authorId": "2110254652",
            "name": "Yong Yao"
          },
          {
            "authorId": "2112586000",
            "name": "Liya Yu"
          },
          {
            "authorId": "2394550",
            "name": "Guanci Yang"
          },
          {
            "authorId": "50778791",
            "name": "Jianjun Hu"
          }
        ]
      },
      "citation_summary": {
        "citation_count": 11,
        "unique_cited_count": 11,
        "influential_count": 1,
        "detailed_records_count": 11
      },
      "cited_papers": [
        "14103933",
        "8486034",
        "6725897",
        "8157622",
        "18841612",
        "14187502",
        "5134972",
        "14004036",
        "114396490",
        "154996364",
        "5959482"
      ],
      "citation_details": [
        {
          "citedcorpusid": 5134972,
          "isinfluential": false,
          "contexts": [
            "[34] found that new technological opportunities can be identified by building a patent keyword evolution map; Wu et al."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Modeling and analyzing technology innovation in the energy sector: Patent-based HMM approach",
            "abstract": "",
            "year": 2012,
            "venue": "Computers & industrial engineering",
            "authors": [
              {
                "authorId": "38648012",
                "name": "Sungjoon Lee"
              },
              {
                "authorId": "40602760",
                "name": "Hyoung-joo Lee"
              },
              {
                "authorId": "38717655",
                "name": "B. Yoon"
              }
            ]
          }
        },
        {
          "citedcorpusid": 5959482,
          "isinfluential": false,
          "contexts": [
            "The architecture of Skip-gram model [20].",
            "[20,21] as our distributed word representation approach."
          ],
          "intents": [
            "--",
            "--"
          ],
          "cited_paper_info": {
            "title": "Efficient Estimation of Word Representations in Vector Space",
            "abstract": "We propose two novel model architectures for computing continuous vector\nrepresentations of words from very large data sets. The quality of these\nrepresentations is measured in a word similarity task, and the results are\ncompared to the previously best performing techniques based on different types\nof neural networks. We observe large improvements in accuracy at much lower\ncomputational cost, i.e. it takes less than a day to learn high quality word\nvectors from a 1.6 billion words data set. Furthermore, we show that these\nvectors provide state-of-the-art performance on our test set for measuring\nsyntactic and semantic word similarities.",
            "year": 2013,
            "venue": "International Conference on Learning Representations",
            "authors": [
              {
                "authorId": "2047446108",
                "name": "Tomas Mikolov"
              },
              {
                "authorId": "2118440152",
                "name": "Kai Chen"
              },
              {
                "authorId": "32131713",
                "name": "G. Corrado"
              },
              {
                "authorId": "49959210",
                "name": "J. Dean"
              }
            ]
          }
        },
        {
          "citedcorpusid": 6725897,
          "isinfluential": false,
          "contexts": [
            "[31] presented three statistical methods to improve the performance of keyword extraction which were based on TF-IDF ."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Keyword Extraction Using Word Co-occurrence",
            "abstract": "",
            "year": 2010,
            "venue": "2010 Workshops on Database and Expert Systems Applications",
            "authors": [
              {
                "authorId": "1712490",
                "name": "Christian Wartena"
              },
              {
                "authorId": "2711559",
                "name": "R. Brussee"
              },
              {
                "authorId": "2803279",
                "name": "Wout Slakhorst"
              }
            ]
          }
        },
        {
          "citedcorpusid": 8157622,
          "isinfluential": true,
          "contexts": [
            "(2-Tailed) Mean Std.\nDeviation Std. Error Mean\n95% Confidence Interval of the Difference\nLower Upper\nPair 1 PKEA-Frequency 0.03015 0.01052 0.00333 0.02263 0.03768 9.065 9 0.000 Pair 2 PKEA-TFIDF 0.02271 0.00878 0.00278 0.01642 0.02899 8.175 9 0.000 Pair 3 PKEA-RAKE 0.03482 0.01177 0.00372 0.02640 0.04325 9.354 9 0.000 Pair 4 PKEA-TextRank 0.02937 0.02150 0.00680 0.01399 0.04475 4.319 9 0.002\nPaper [17] provides SemEval-2010 dataset and evaluation methods.",
            "For example, the best performances achieved on SemEval-2010 [17] and Hulth2003 [18] are only 27.2% and 38.7% in precision aspect.",
            "For example, the best performances achieved on SemEval-2010 [17] and Hulth2003 [18] are only 27.",
            "Based on the usage of extracted keywords, keyword extraction algorithms can be roughly divided into two categories: one type are algorithms for extracting semantic keywords to summarize corresponding text [17,19] and the other type are for extracting discriminative keywords to classify texts into categories [16].",
            "Paper [17] provides SemEval-2010 dataset and evaluation methods.",
            "SemEval-2010 [17] dataset.",
            "Besides, the experimental results on the SemEval-2010 dataset al.so demonstrate that our PKEA has generalization capability to extract key phrases from the other types of texts.",
            "The SemEval-2010 dataset is a benchmark dataset in key phrases extraction filed which consist of 144 training and 100 test papers belonging to four 1998 ACM classification: C2."
          ],
          "intents": [
            "--",
            "--",
            "['background']",
            "['methodology']",
            "['methodology']",
            "--",
            "--",
            "--"
          ],
          "cited_paper_info": {
            "title": "SemEval-2010 Task 5 : Automatic Keyphrase Extraction from Scientific Articles",
            "abstract": "",
            "year": 2010,
            "venue": "International Workshop on Semantic Evaluation",
            "authors": [
              {
                "authorId": "1736741380",
                "name": "Su Nam Kim"
              },
              {
                "authorId": "2044056",
                "name": "Olena Medelyan"
              },
              {
                "authorId": "37596605",
                "name": "Min-Yen Kan"
              },
              {
                "authorId": "145465286",
                "name": "Timothy Baldwin"
              }
            ]
          }
        },
        {
          "citedcorpusid": 8486034,
          "isinfluential": false,
          "contexts": [
            "Patent analysis techniques for determining patent quality for Research And Development (R&D) tasks [6,7] and technological road mapping [8] have also been developed."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "A patent quality analysis for innovative technology and product development",
            "abstract": "",
            "year": 2012,
            "venue": "Advanced Engineering Informatics",
            "authors": [
              {
                "authorId": "1761458",
                "name": "A. Trappey"
              },
              {
                "authorId": "1766308",
                "name": "C. Trappey"
              },
              {
                "authorId": "3358489",
                "name": "Chun-Yi Wu"
              },
              {
                "authorId": "2143376116",
                "name": "Chi-Wei Lin"
              }
            ]
          }
        },
        {
          "citedcorpusid": 14004036,
          "isinfluential": false,
          "contexts": [
            "[35] studied the technology subject clustering in patent analysis and summarized the procedure of keyword selection: weight calculation, similarity calculation and clustering algorithm selection, multi-step clustering, clustering cluster labels, further grouping the clustering results."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Text mining techniques for patent analysis",
            "abstract": "",
            "year": 2007,
            "venue": "Information Processing & Management",
            "authors": [
              {
                "authorId": "40130996",
                "name": "Yuen-Hsien Tseng"
              },
              {
                "authorId": "2143476751",
                "name": "Chi-Jen Lin"
              },
              {
                "authorId": "3315593",
                "name": "Yu-I Lin"
              }
            ]
          }
        },
        {
          "citedcorpusid": 14103933,
          "isinfluential": false,
          "contexts": [
            "The k-bisecting clustering algorithm shows the capability to extract strongly relevant keywords from Wikipedia articles [32]."
          ],
          "intents": [
            "--"
          ],
          "cited_paper_info": {
            "title": "Topic Detection by Clustering Keywords",
            "abstract": "",
            "year": 2008,
            "venue": "2008 19th International Workshop on Database and Expert Systems Applications",
            "authors": [
              {
                "authorId": "1712490",
                "name": "Christian Wartena"
              },
              {
                "authorId": "2711559",
                "name": "R. Brussee"
              }
            ]
          }
        },
        {
          "citedcorpusid": 14187502,
          "isinfluential": false,
          "contexts": [
            "Meanwhile, a few studies concentrated on different keyword extraction methods [10], while others tried to identify the most appropriate section for keyword extraction.",
            "Meanwhile, keyword extraction algorithms have received a lot of attention as a quick way to acquire meaningful information from unstructured text, which can help to achieve more effective patent mining [3,9,10]."
          ],
          "intents": [
            "['background']",
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Extracting the significant-rare keywords for patent analysis",
            "abstract": "",
            "year": 2009,
            "venue": "Expert systems with applications",
            "authors": [
              {
                "authorId": "48515232",
                "name": "Yan-Ru Li"
              },
              {
                "authorId": "34139587",
                "name": "Leuo-hong Wang"
              },
              {
                "authorId": "5255173",
                "name": "Chao-Fu Hong"
              }
            ]
          }
        },
        {
          "citedcorpusid": 18841612,
          "isinfluential": false,
          "contexts": [
            "Many commonly used classification algorithms have been tried, such as decision trees [23], Naive Bayes classifiers [24], Support Vector Machines (SVM) [19], maximum entropy models [25], hidden Markov models [26], conditional random field models [14], and so on.",
            "Supervised Machine learning approaches (Decision Tree [23], Naïve Bayes [24], SVM [19], Maximum Entropy [25], HMM [26], CRF [14]) High readability, great flexibility to include a wide variety of arbitrary, non-independent features of the input."
          ],
          "intents": [
            "['methodology']",
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Research of Information Extraction Algorithm based on Hidden Markov Model",
            "abstract": "",
            "year": 2010,
            "venue": "International Conference on Information Science and Engineering",
            "authors": [
              {
                "authorId": "3184694",
                "name": "Cailan Zhou"
              },
              {
                "authorId": "2109025362",
                "name": "Shasha Li"
              }
            ]
          }
        },
        {
          "citedcorpusid": 114396490,
          "isinfluential": false,
          "contexts": [
            "Meanwhile, keyword extraction algorithms have received a lot of attention as a quick way to acquire meaningful information from unstructured text, which can help to achieve more effective patent mining [3,9,10]."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "The evolution of patent mining: Applying bibliometrics analysis and keyword network analysis",
            "abstract": "",
            "year": 2016,
            "venue": "",
            "authors": [
              {
                "authorId": "2550175",
                "name": "Farshad Madani"
              },
              {
                "authorId": "34961132",
                "name": "C. Weber"
              }
            ]
          }
        },
        {
          "citedcorpusid": 154996364,
          "isinfluential": false,
          "contexts": [
            "[36] used keywords to cluster technologic topics, treated the co-occurrence keywords between different clusters as technical transition words."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Identifying Technology Trends for R&D Planning Using TRIZ and Text Mining",
            "abstract": "The current pace of technological development has forced many companies to invest significant capital and resources in research and development (R&D) activities. A systematic and efficient method of identifying technology trends and their evolutionary potentials can help companies guide their R&D planning and wisely allocate their R&D resources. This study proposes a framework combining the evolutionary trends developed by the Theory of Inventive Problem Solving, or Teoriya Reshniya Izobretatelskikh Zadatch (TRIZ) in Russian, with the visualization technique of text mining to systematically identify technology trends from patent documents. As technological information in patent documents is stored almost entirely in text format, the text mining method allows R&D personnel to efficiently identify technology trends and effectively conduct R&D planning. Utilizing text mining method on patents of magnetic random access memory (MRAM) systems and the underlying principles of TRIZ evolutionary trends, this study shows that MRAM includes 10 important technology trends. These trends have almost reached the evolutionary limit phase defined by TRIZ, which means that MRAM is fast becoming a mature technology. Therefore, for businesses that intend to acquire MRAM technology they do not possess, a wise R&D plan may be licensing the technology, buying the technology from others, or participating in a joint venture rather than using in-house R&D.",
            "year": 2010,
            "venue": "",
            "authors": [
              {
                "authorId": "1769185",
                "name": "Ming-Yeu Wang"
              },
              {
                "authorId": "2463647",
                "name": "D. Chang"
              },
              {
                "authorId": "98401403",
                "name": "C. Kao"
              }
            ]
          }
        }
      ]
    },
    "216371276": {
      "citing_paper_info": {
        "title": "Construction and evaluation of gold standards for patent classification—A case study on quantum computing",
        "abstract": "",
        "year": 2020,
        "venue": "World Patent Information",
        "authors": [
          {
            "authorId": "2113975020",
            "name": "Steve Harris"
          },
          {
            "authorId": "2494885",
            "name": "A. Trippe"
          },
          {
            "authorId": "2081699346",
            "name": "David M. Challis"
          },
          {
            "authorId": "1659157939",
            "name": "Nigel Swycher"
          }
        ]
      },
      "citation_summary": {
        "citation_count": 2,
        "unique_cited_count": 2,
        "influential_count": 0,
        "detailed_records_count": 2
      },
      "cited_papers": [
        "3641284",
        "7553535"
      ],
      "citation_details": [
        {
          "citedcorpusid": 3641284,
          "isinfluential": false,
          "contexts": [
            "In order to illustrate possible causes, a UMAP dimensional reduction [13] was performed on the entire gold standard data, using a pre-existing deep learning embedding 10 of CPC class codes."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "UMAP: Uniform Manifold Approximation and Projection for Dimension Reduction",
            "abstract": "UMAP (Uniform Manifold Approximation and Projection) is a novel manifold learning technique for dimension reduction. UMAP is constructed from a theoretical framework based in Riemannian geometry and algebraic topology. The result is a practical scalable algorithm that applies to real world data. The UMAP algorithm is competitive with t-SNE for visualization quality, and arguably preserves more of the global structure with superior run time performance. Furthermore, UMAP has no computational restrictions on embedding dimension, making it viable as a general purpose dimension reduction technique for machine learning.",
            "year": 2018,
            "venue": "arXiv.org",
            "authors": [
              {
                "authorId": "31785573",
                "name": "Leland McInnes"
              },
              {
                "authorId": "2062756303",
                "name": "John Healy"
              }
            ]
          }
        },
        {
          "citedcorpusid": 7553535,
          "isinfluential": false,
          "contexts": [
            "Demšar [10] identifies the most frequently used information retrieval metrics used in the analysis of supervised learning classifier performance in the literature as 𝑝𝑟𝑒𝑐𝑖𝑠𝑖𝑜𝑛 , 𝑟𝑒𝑐𝑎𝑙𝑙 , 𝐹 1 , and 𝑎𝑐𝑐𝑢𝑟𝑎𝑐𝑦 , with AUC also being used, though less often.",
            "One important aspect of classifier performance that is often under reported is the variance of results over repeat runs with different datasets [10]."
          ],
          "intents": [
            "--",
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Statistical Comparisons of Classifiers over Multiple Data Sets",
            "abstract": "While methods for comparing two learning algorithms on a single data set have been scrutinized for quite some time already, the issue of statistical tests for comparisons of more algorithms on multiple data sets, which is even more essential to typical machine learning studies, has been all but ignored. This article reviews the current practice and then theoretically and empirically examines several suitable tests. Based on that, we recommend a set of simple, yet safe and robust non-parametric tests for statistical comparisons of classifiers: the Wilcoxon signed ranks test for comparison of two classifiers and the Friedman test with the corresponding post-hoc tests for comparison of more classifiers over multiple data sets. Results of the latter can also be neatly presented with the newly introduced CD (critical difference) diagrams.",
            "year": 2006,
            "venue": "Journal of machine learning research",
            "authors": [
              {
                "authorId": "2701105",
                "name": "J. Demšar"
              }
            ]
          }
        }
      ]
    },
    "124360271": {
      "citing_paper_info": {
        "title": "A Hierarchical Feature Extraction Model for Multi-Label Mechanical Patent Classification",
        "abstract": "Various studies have focused on feature extraction methods for automatic patent classification in recent years. However, most of these approaches are based on the knowledge from experts in related domains. Here we propose a hierarchical feature extraction model (HFEM) for multi-label mechanical patent classification, which is able to capture both local features of phrases as well as global and temporal semantics. First, a n-gram feature extractor based on convolutional neural networks (CNNs) is designed to extract salient local lexical-level features. Next, a long dependency feature extraction model based on the bidirectional long–short-term memory (BiLSTM) neural network model is proposed to capture sequential correlations from higher-level sequence representations. Then the HFEM algorithm and its hierarchical feature extraction architecture are detailed. We establish the training, validation and test datasets, containing 72,532, 18,133, and 2679 mechanical patent documents, respectively, and then check the performance of HFEMs. Finally, we compared the results of the proposed HFEM and three other single neural network models, namely CNN, long–short-term memory (LSTM), and BiLSTM. The experimental results indicate that our proposed HFEM outperforms the other compared models in both precision and recall.",
        "year": 2018,
        "venue": "",
        "authors": [
          {
            "authorId": "145815844",
            "name": "Jie Hu"
          },
          {
            "authorId": "2124883266",
            "name": "Shaobo Li"
          },
          {
            "authorId": "50778791",
            "name": "Jianjun Hu"
          },
          {
            "authorId": "2394550",
            "name": "Guanci Yang"
          }
        ]
      },
      "citation_summary": {
        "citation_count": 11,
        "unique_cited_count": 9,
        "influential_count": 1,
        "detailed_records_count": 11
      },
      "cited_papers": [
        "18451567",
        "29763249",
        "6628106",
        "16447573",
        "14012633",
        "46320871",
        "26341880",
        "1642392",
        "37424485"
      ],
      "citation_details": [
        {
          "citedcorpusid": 1642392,
          "isinfluential": false,
          "contexts": [
            "[17] proposed an approach for feature extraction for dependency parsing based on a BiLSTM encoder.",
            "Meanwhile, convolutional neural networks (CNN) can capture salient local lexical-level features and bidirectional long–short-term memory (BiLSTM) can learn long-term dependencies from sequences of higher-level representations in the patent text [16,17]."
          ],
          "intents": [
            "['methodology']",
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Simple and Accurate Dependency Parsing Using Bidirectional LSTM Feature Representations",
            "abstract": "We present a simple and effective scheme for dependency parsing which is based on bidirectional-LSTMs (BiLSTMs). Each sentence token is associated with a BiLSTM vector representing the token in its sentential context, and feature vectors are constructed by concatenating a few BiLSTM vectors. The BiLSTM is trained jointly with the parser objective, resulting in very effective feature extractors for parsing. We demonstrate the effectiveness of the approach by applying it to a greedy transition-based parser as well as to a globally optimized graph-based parser. The resulting parsers have very simple architectures, and match or surpass the state-of-the-art accuracies on English and Chinese.",
            "year": 2016,
            "venue": "Transactions of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "2022679",
                "name": "E. Kiperwasser"
              },
              {
                "authorId": "2089067",
                "name": "Yoav Goldberg"
              }
            ]
          }
        },
        {
          "citedcorpusid": 6628106,
          "isinfluential": false,
          "contexts": [
            "We adopt Adaptive Moment Estimation (ADAM) [32] to minimize the objective function to solve the optimization problem."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Adam: A Method for Stochastic Optimization",
            "abstract": "We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm.",
            "year": 2014,
            "venue": "International Conference on Learning Representations",
            "authors": [
              {
                "authorId": "1726807",
                "name": "Diederik P. Kingma"
              },
              {
                "authorId": "2503659",
                "name": "Jimmy Ba"
              }
            ]
          }
        },
        {
          "citedcorpusid": 14012633,
          "isinfluential": false,
          "contexts": [
            "To address these issues, some scholars used syntactic-and semantic-based approaches to alleviate these problems [9,18,21].",
            "The bag-of-words (BOW) model [8,18] is a typical, statistically-based text representation approach, which is almost always used in patent analysis studies [1,18]."
          ],
          "intents": [
            "--",
            "--"
          ],
          "cited_paper_info": {
            "title": "Combining Semantics and Statistics for Patent Classification",
            "abstract": "For the patent classification task of the 2010 CLEF-IP evaluation we have used three different approaches combining semantics and statistics-driven techniques: first approach is based on an indexing-retrieval method using the Lemur system enhanced with a class calculation algorithm; the second approach combined a semantics-driven technique for class model building and the use of an advanced statistical classifier; the third approach combined the two previous methods, attempting to exploit their complementarity for results quality improvement. The results obtained for our system are encouraging: we ranked second in terms of precision on first candidate, which is, from an application point of view, the most pertinent score.",
            "year": 2010,
            "venue": "Conference and Labs of the Evaluation Forum",
            "authors": [
              {
                "authorId": "48733111",
                "name": "F. Derieux"
              },
              {
                "authorId": "2238724",
                "name": "Mihaela Bobeica"
              },
              {
                "authorId": "2330267",
                "name": "Delphine Pois"
              },
              {
                "authorId": "1964787",
                "name": "Jean-Pierre Raysz"
              }
            ]
          }
        },
        {
          "citedcorpusid": 16447573,
          "isinfluential": false,
          "contexts": [
            "Previous studies showed that distributed representation has great potential to represent texts from both semantic and syntactic perspectives without any external domain knowledge [14,15]."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Distributed Representations of Words and Phrases and their Compositionality",
            "abstract": "The recently introduced continuous Skip-gram model is an efficient method for learning high-quality distributed vector representations that capture a large number of precise syntactic and semantic word relationships. In this paper we present several extensions that improve both the quality of the vectors and the training speed. By subsampling of the frequent words we obtain significant speedup and also learn more regular word representations. We also describe a simple alternative to the hierarchical softmax called negative sampling. \n \nAn inherent limitation of word representations is their indifference to word order and their inability to represent idiomatic phrases. For example, the meanings of \"Canada\" and \"Air\" cannot be easily combined to obtain \"Air Canada\". Motivated by this example, we present a simple method for finding phrases in text, and show that learning good vector representations for millions of phrases is possible.",
            "year": 2013,
            "venue": "Neural Information Processing Systems",
            "authors": [
              {
                "authorId": "2047446108",
                "name": "Tomas Mikolov"
              },
              {
                "authorId": "1701686",
                "name": "I. Sutskever"
              },
              {
                "authorId": "2118440152",
                "name": "Kai Chen"
              },
              {
                "authorId": "32131713",
                "name": "G. Corrado"
              },
              {
                "authorId": "49959210",
                "name": "J. Dean"
              }
            ]
          }
        },
        {
          "citedcorpusid": 18451567,
          "isinfluential": true,
          "contexts": [
            "Moreover, some researchers tried to identify which parts in a patent document can provide more representative information for classification tasks [5,13].",
            "Stutzk [5] treated PAC as a multi-label hierarchical classification problem and employed k Nearest Neighbors (k-NN) and a one-versus-rest SVM to classify patent data with additional geospatial data.",
            "TF-IDF is often use in patent classification as a text feature extractor [4,5]."
          ],
          "intents": [
            "['background']",
            "['methodology']",
            "--"
          ],
          "cited_paper_info": {
            "title": "Geodata supported classification of patent applications",
            "abstract": "",
            "year": 2016,
            "venue": "GeoRich@SIGMOD",
            "authors": [
              {
                "authorId": "3264870",
                "name": "J. Stutzki"
              },
              {
                "authorId": "39403212",
                "name": "Matthias Schubert"
              }
            ]
          }
        },
        {
          "citedcorpusid": 26341880,
          "isinfluential": false,
          "contexts": [
            "A CNN is a class of deep, multilayer, feed-forward and back-propagation artificial neural networks [26]."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "A survey of randomized algorithms for training neural networks",
            "abstract": "",
            "year": 2016,
            "venue": "Information Sciences",
            "authors": [
              {
                "authorId": "2108005362",
                "name": "Le Zhang"
              },
              {
                "authorId": "1688355",
                "name": "P. Suganthan"
              }
            ]
          }
        },
        {
          "citedcorpusid": 29763249,
          "isinfluential": false,
          "contexts": [
            "Signiﬁcant efforts have been made in many previous studies [3–7].",
            "Some of them focused on the patent text representation [8,9], trying to ﬁnd the best solution to represent the patent text, while some of them were dedicated to designing the most effective classiﬁcation algorithms [3,4,7]."
          ],
          "intents": [
            "--",
            "--"
          ],
          "cited_paper_info": {
            "title": "A patent quality analysis and classification system using self-organizing maps with support vector machine",
            "abstract": "",
            "year": 2016,
            "venue": "Applied Soft Computing",
            "authors": [
              {
                "authorId": "2719770",
                "name": "Jheng-Long Wu"
              },
              {
                "authorId": "1717197",
                "name": "P. Chang"
              },
              {
                "authorId": "2707223",
                "name": "Cheng-Chin Tsao"
              },
              {
                "authorId": "1726734",
                "name": "C. Fan"
              }
            ]
          }
        },
        {
          "citedcorpusid": 37424485,
          "isinfluential": false,
          "contexts": [
            "To address these issues, some scholars used syntactic- and semantic-based approaches to alleviate these problems [9,18,21]."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "SAO-Based Semantic Mining of Patents for Semi-Automatic Construction of a Customer Job Map",
            "abstract": "The Outcome-Driven Innovation (ODI) method based on the ‘Jobs-to-be-done’ concept is very useful in the identification of unmet customer needs and has been adopted widely in the industry. The Job Map, a tool of the ODI method, is used to understand customers by defining their behavioral process. Complications must be overcome before the Job Map can be applied to the specific problem in question, such as a time-consuming process, dealing with a large amount of data, and experts’ biased work. To solve these problems, this study develops a patent mining-based method based on the subject-action-object (SAO) structure to support the creation of a Job Map by semi-automatizing data collection and analysis. This effort at better utilizing computers in customer analysis for product design will contribute to expanding computerized methods for solving design and engineering problems in practice.",
            "year": 2017,
            "venue": "",
            "authors": [
              {
                "authorId": "51473422",
                "name": "J. Lim"
              },
              {
                "authorId": "7236231",
                "name": "Sungchul Choi"
              },
              {
                "authorId": "123259487",
                "name": "Chiehyeon Lim"
              },
              {
                "authorId": "24357608",
                "name": "Kwangsoo Kim"
              }
            ]
          }
        },
        {
          "citedcorpusid": 46320871,
          "isinfluential": false,
          "contexts": [
            "[25] proposed a two-layered feed-forward neural network and employed the Levenberg–Marquardt algorithm to train the network for 1948 patent documents from United States Patent Classification (USPC) 360/324."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "A framework for automatic TRIZ level of invention estimation of patents using natural language processing, knowledge-transfer and patent citation metrics",
            "abstract": "",
            "year": 2012,
            "venue": "Comput. Aided Des.",
            "authors": [
              {
                "authorId": "1700892",
                "name": "Z. Li"
              },
              {
                "authorId": "145799151",
                "name": "D. Tate"
              },
              {
                "authorId": "2060352562",
                "name": "Christopher Lane"
              },
              {
                "authorId": "2064888431",
                "name": "Christopher Adams"
              }
            ]
          }
        },
        {
          "citedcorpusid": null,
          "isinfluential": false,
          "contexts": [
            "TF-IDF is often use in patent classiﬁcation as a text feature extractor [4,5].",
            "Some of them focused on the patent text representation [8,9], trying to ﬁnd the best solution to represent the patent text, while some of them were dedicated to designing the most effective classiﬁcation algorithms [3,4,7]."
          ],
          "intents": [
            "--",
            "--"
          ],
          "cited_paper_info": {}
        },
        {
          "citedcorpusid": null,
          "isinfluential": false,
          "contexts": [
            "For example, in Web 1T ﬁve-gram [20], Google Inc. (Mountain View, CA, USA) provides the dataset with its length ranging from unigrams to ﬁve-grams."
          ],
          "intents": [
            "--"
          ],
          "cited_paper_info": {}
        }
      ]
    },
    "245343990": {
      "citing_paper_info": {
        "title": "PatentNet: multi-label classification of patent documents using deep learning based language understanding",
        "abstract": "Patent classification is an expensive and time-consuming task that has conventionally been performed by domain experts. However, the increase in the number of filed patents and the complexity of the documents make the classification task challenging. The text used in patent documents is not always written in a way to efficiently convey knowledge. Moreover, patent classification is a multi-label classification task with a large number of labels, which makes the problem even more complicated. Hence, automating this expensive and laborious task is essential for assisting domain experts in managing patent documents, facilitating reliable search, retrieval, and further patent analysis tasks. Transfer learning and pre-trained language models have recently achieved state-of-the-art results in many Natural Language Processing tasks. In this work, we focus on investigating the effect of fine-tuning the pre-trained language models, namely, BERT, XLNet, RoBERTa, and ELECTRA, for the essential task of multi-label patent classification. We compare these models with the baseline deep-learning approaches used for patent classification. We use various word embeddings to enhance the performance of the baseline models. The publicly available USPTO-2M patent classification benchmark and M-patent datasets are used for conducting experiments. We conclude that fine-tuning the pre-trained language models on the patent text improves the multi-label patent classification performance. Our findings indicate that XLNet performs the best and achieves a new state-of-the-art classification performance with respect to precision, recall, F1 measure, as well as coverage error, and LRAP.",
        "year": 2021,
        "venue": "Scientometrics",
        "authors": [
          {
            "authorId": "2197861164",
            "name": "Arousha Haghighian Roudsari"
          },
          {
            "authorId": "145653991",
            "name": "Jafar Afshar"
          },
          {
            "authorId": "1728685",
            "name": "Wookey Lee"
          },
          {
            "authorId": "2112280",
            "name": "Suan Lee"
          }
        ]
      },
      "citation_summary": {
        "citation_count": 29,
        "unique_cited_count": 26,
        "influential_count": 5,
        "detailed_records_count": 29
      },
      "cited_papers": [
        "6628106",
        "4565108",
        "221593833",
        "22998",
        "131988231",
        "205163",
        "203639485",
        "5937747",
        "7449220",
        "218489934",
        "12711052",
        "43261268",
        "21663443",
        "67179900",
        "5959482",
        "7365231",
        "198953378",
        "51578296",
        "218971783",
        "313952",
        "235386502",
        "174799315",
        "2871882",
        "29763249",
        "3397190",
        "13756489"
      ],
      "citation_details": [
        {
          "citedcorpusid": 22998,
          "isinfluential": true,
          "contexts": [
            "Multi-label evaluation measures can be divided into two main groups, namely, label-based metrics and example-based metrics (Tsoumakas et al., 2009).",
            "Coverage error indicates, on average, how many labels in the ranked list of the estimated probabilities are required to account for all the true positive labels and is computed as follows: where LRAP is related to the average precision score.",
            "This demonstrates the effectiveness of fine-tuning these models for multi-label patent classification and their ability to capture the patent documents features better than the XLNet outperformed all the other models and achieved the highest value of micro-F1 of 0.736 and 0.850 of LRAP.",
            "Moreover, we also report two ranking-based measures, Coverage Error (Wu & Zhou, 2017) and Label Ranking Average Precision (LRAP) (Tsoumakas et al., 2009)."
          ],
          "intents": [
            "['background']",
            "--",
            "--",
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Mining Multi-label Data",
            "abstract": "",
            "year": 2010,
            "venue": "Data Mining and Knowledge Discovery Handbook",
            "authors": [
              {
                "authorId": "2502501",
                "name": "Grigorios Tsoumakas"
              },
              {
                "authorId": "1749884",
                "name": "I. Katakis"
              },
              {
                "authorId": "1697941",
                "name": "I. Vlahavas"
              }
            ]
          }
        },
        {
          "citedcorpusid": 205163,
          "isinfluential": false,
          "contexts": [
            "The micro precision, recall, and F 1 measure (Gibaja & Ventura, 2014) are calculated as follows."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Multi-Label Learning. A Review of the State of the Art and Ongoing Research",
            "abstract": "Multi-label learning is quite a recent supervised learning paradigm. Owing to its capabilities to improve performance in problems where a pattern may have more than one associated class, it has attracted the attention of researchers, producing an increasing number of publications. The paper presents an up-to-date overview about multi-label learning with the aim of sorting and describing the main approaches developed till now. The formal deﬁnition of the paradigm, the analysis of its impact on the literature, its main applications, works developed and ongoing research are presented.",
            "year": 2014,
            "venue": "",
            "authors": [
              {
                "authorId": "8200631",
                "name": "E. G. Galindo"
              },
              {
                "authorId": "145239852",
                "name": "Sebastián Ventura"
              }
            ]
          }
        },
        {
          "citedcorpusid": 313952,
          "isinfluential": false,
          "contexts": [
            "…features from it (D’hondt et al., 2013; Shalaby et al., 2018; Hu et al., 2018a; Hu et al., 2018b; Li et al., 2018) while others focused on designing more effective classification algorithms (Fall et al., 2003; Al Shamsi & Aung, 2016; D’hondt et al., 2017; Wu et al., 2010, 2016; Song et al., 2019).",
            "…methods (such as Bag-of-Words, TF-IDF, and N-grams) and machine learning classification models (such as SVM, K-Nearest Neighbor, and Naive Bayes) (Fall et al., 2003; Wu et al., 2010; D’hondt et al., 2013), all with limited success that is achieved through tedious hand-crafted feature extractions…",
            "Previous attempts for automating patent classification have been based on traditional text or document representation methods (such as Bag-of-Words, TF-IDF, and N-grams) and machine learning classification models (such as SVM, K-Nearest Neighbor, and Naive Bayes) (Fall et al., 2003; Wu et al., 2010; D’hondt et al., 2013), all with limited success that is achieved through tedious hand-crafted feature extractions for representing the patent document."
          ],
          "intents": [
            "['background']",
            "['methodology']",
            "--"
          ],
          "cited_paper_info": {
            "title": "Automated categorization in the international patent classification",
            "abstract": "",
            "year": 2003,
            "venue": "SIGF",
            "authors": [
              {
                "authorId": "8635679",
                "name": "C. Fall"
              },
              {
                "authorId": "2893823",
                "name": "A. Törcsvári"
              },
              {
                "authorId": "2217341",
                "name": "K. Benzineb"
              },
              {
                "authorId": "2570524",
                "name": "G. Karetka"
              }
            ]
          }
        },
        {
          "citedcorpusid": 2871882,
          "isinfluential": true,
          "contexts": [
            "The deliberate usage of vague and new terms in patent documents are intended to protect and keep as broad as possible rights for the contained intellectual property (D’hondt et al., 2017; Shalaby et al., 2018; Gomez & Moens, 2014).",
            "Moreover, the distribution of the patent documents across the IPC categories is highly imbalanced (Gomez & Moens, 2014; Lupu et al., 2017).",
            "These standard taxonomies consist of complex hierarchical structures that cover all technology areas and help maintain inter-operability among various patent offices worldwide (Gomez & Moens, 2014).",
            "Gomez & Moens (2014) did a comprehensive survey of several previous works that tackled the automated patent classification problem in the IPC hierarchy.",
            "…patent documents is critical and will help experts manage patent documents, facilitate reliable patent search and retrieval, reduce the risk of missing a relevant patent in preventing patent infringement, and further patent analysis tasks (Yun & Geum, 2020; Souza et al., 2020; Gomez & Moens, 2014)."
          ],
          "intents": [
            "['background']",
            "['background']",
            "['background']",
            "['background']",
            "['background']"
          ],
          "cited_paper_info": {
            "title": "A Survey of Automated Hierarchical Classification of Patents",
            "abstract": "",
            "year": 2014,
            "venue": "Professional Search in the Modern World",
            "authors": [
              {
                "authorId": "144004546",
                "name": "J. Gómez"
              },
              {
                "authorId": "145446752",
                "name": "Marie-Francine Moens"
              }
            ]
          }
        },
        {
          "citedcorpusid": 3397190,
          "isinfluential": false,
          "contexts": [
            "Meanwhile, RNNs are well known for performing sequential processing of the text by modeling units of the sequence and are able to capture the sequential nature that lies in a language (Young et al., 2018)."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Recent Trends in Deep Learning Based Natural Language Processing",
            "abstract": "Deep learning methods employ multiple processing layers to learn hierarchical representations of data and have produced state-of-the-art results in many domains. Recently, a variety of model designs and methods have blossomed in the context of natural language processing (NLP). In this paper, we review significant deep learning related models and methods that have been employed for numerous NLP tasks and provide a walk-through of their evolution. We also summarize, compare and contrast the various models and put forward a detailed understanding of the past, present and future of deep learning in NLP.",
            "year": 2017,
            "venue": "IEEE Computational Intelligence Magazine",
            "authors": [
              {
                "authorId": "2061649994",
                "name": "Tom Young"
              },
              {
                "authorId": "8223433",
                "name": "Devamanyu Hazarika"
              },
              {
                "authorId": "1746416",
                "name": "Soujanya Poria"
              },
              {
                "authorId": "49943757",
                "name": "E. Cambria"
              }
            ]
          }
        },
        {
          "citedcorpusid": 4565108,
          "isinfluential": true,
          "contexts": [
            "Therefore, to compare our experiments with the DeepPatent model proposed by Li .",
            "Their hybrid model combines CNN and BiLSTM model for hierarchical feature extraction.",
            "Implementing DeepPatent again on the dataset and using our trained Skip-gram word vectors, we obtained a precision, recall, and F1 of 77.10%, 52.00%, and 58.95% at the top 1 label prediction, respectively.",
            "…the patent text and how to extract semantic features from it (D’hondt et al., 2013; Shalaby et al., 2018; Hu et al., 2018a; Hu et al., 2018b; Li et al., 2018) while others focused on designing more effective classification algorithms (Fall et al., 2003; Al Shamsi & Aung, 2016; D’hondt et…",
            "Roudsari et al. (2021) used a CNN model and investigated the effect of utilizing static versus contextual word embeddings on multi-label patent classification but only considered 89 labels at the IPC subclass level.",
            "Some previous research (Li et al., 2018; Hu et al., 2018a) utilized the evaluation measures from the CLEP-IP competition (Piroi et al., 2011).",
            "CNN model is hierarchical, and different kernel sizes can extract various important n-gram features or semantic clues from the input.",
            "The Transformer avoids the recurrence and convolutions entirely and allows for much more parallelized computations than RNNs and CNNs.",
            "Nevertheless, these embedding vectors, combined with other neural networks such as RNN and CNNs, have successfully achieved good results on various NLP tasks such as text classification (Minaee et al., 2020).",
            "DeepPatent is a CNN model.",
            "…studies have implemented deep learning models such as convolutional neural network (CNN) (Li et al., 2018; Abdelgawad et al., 2019), recurrent neural network (RNN) based models (Grawe et al., 2017; Risch & Krestel, 2019), and their combination Hu et al. (2018a) for automated patent classification.",
            "The CNN architecture is as follows: a convolutional layer with three kernel sizes (3, 4, and 5), a max-pool-ing layer applied to the output of each convolutional layer, and a fully connected layer with units equal to the number of labels and sigmoid activation function.",
            "Similar to Hu et al. (2018a) and their M-CLEF dataset, the title, abstract, description, and claim part of English patents that belong to the F category of IPC taxonomy are extracted.",
            "Moreover, the micro-F1 of 0.523 obtained from the original DeepPatent (Skip-gram) increased to 0.572 using the XLNet model.",
            "Li et al. (2018) evaluated the effect of using different patent sections and the number of words on the classification performance on the USPTO-2M dataset.",
            "Li et al. (2018) proposed a deep learning algorithm, DeepPatent, using the combination of word embedding (word2vec) and the famous convolutional neural networks (CNN) for the patent classification task.",
            "CNN can extract important n-gram features from the input sentence and, therefore, produce an information latent semantic representation of the input sequences for the downstream task.",
            "DeepPatent : We used a CNN architecture similar to DeepPatent (Li et al., 2018) and trained 200-dimensional word embeddings based on the skip-gram model.",
            "Hu et al. (2018a) used word2vec word embedding and proposed a hybrid model for multi-label patent classification but only considering mechanical patents.",
            "DeepPatent with the fastText embeddings achieved 79.77%, 65.52%, and 69.79% of precision, recall, and F1 at the top 1 label prediction, respectively.",
            "Among the base-line models, except when using the CBOW word embeddings, DeepPatent obtained the best performance, followed by the CNN-BiLSTM model.",
            "It should be noted that there is a slight difference between the dataset used in this paper and the data described in (Li et al., 2018).",
            "Furthermore, some attempts have been made to find which part of the patent text can be more representative and provide better classification results (Gomez, 2019; Hu et al., 2018a; Wu et al., 2010; D’hondt & Verberne, 2010).",
            "However, once again, all the pre-trained language models defeated DeepPatent in terms of all metrics.",
            "We compared these models with other deep learning-based models proposed for patent classification, namely, CNN, LSTM, BiLSTM, and CNN-BiLSTM.",
            "Similar to Li et al. (2018), we train DeepPatent for 50 epochs on the USPTO-2M dataset.",
            "The USPTO-2M is a large-scale patent classification benchmark made publicly available by Li et al. (2018) 2 .",
            "For the experiments on the M-patent dataset, the number of epochs is set to 40 same as Hu et al. (2018a).",
            "Some focused on the best way to represent the patent text and how to extract semantic features from it (D’hondt et al., 2013; Shalaby et al., 2018; Hu et al., 2018a; Hu et al., 2018b; Li et al., 2018) while others focused on designing more effective classification algorithms (Fall et al., 2003; Al…",
            "With the usage of our trained fastText embeddings instead of word2vec embeddings, we outper-formed the original DeepPatent model, resulting in better performance than the original model.",
            "CNN models are faster to train than other models and can achieve somewhat acceptable performances.",
            "DeepPatent outperformed other algorithms from the CLEF-IP (Piroi et al., 2011) competition and contributed the new USPTO-2M dataset, which is much larger than the previous benchmarks.",
            "Furthermore, we also conducted experiments on the USPTO-2M dataset contributed by Li et al. (2018) and used their DeepPatent as the baseline model.",
            "Similar to Hu et al. (2018a), we further employ other deep learning baseline models used for classification, namely, LSTM, BiLSTM, CNN and CNN-BiLSTM.",
            "…the best way to represent the patent text and how to extract semantic features from it (D’hondt et al., 2013; Shalaby et al., 2018; Hu et al., 2018a; Hu et al., 2018b; Li et al., 2018) while others focused on designing more effective classification algorithms (Fall et al., 2003; Al Shamsi & Aung,…",
            "Recently, some studies have implemented deep learning models such as convolutional neural network (CNN) (Li et al., 2018; Abdelgawad et al., 2019), recurrent neural network (RNN) based models (Grawe et al., 2017; Risch & Krestel, 2019), and their combination Hu et al. (2018a) for automated patent…",
            "Recently, some studies have implemented deep learning models such as convolutional neural network (CNN) (Li et al., 2018; Abdelgawad et al., 2019), recurrent neural network (RNN) based models (Grawe et al., 2017; Risch & Krestel, 2019), and their combination Hu et al. (2018a) for automated patent classification.",
            "CNN-BiLSTM -"
          ],
          "intents": [
            "--",
            "--",
            "--",
            "['background']",
            "--",
            "['methodology']",
            "--",
            "--",
            "--",
            "--",
            "--",
            "--",
            "['result']",
            "--",
            "['background']",
            "['methodology']",
            "--",
            "['methodology']",
            "['methodology']",
            "--",
            "--",
            "['methodology']",
            "['background']",
            "--",
            "--",
            "['methodology']",
            "['background']",
            "['methodology']",
            "['background']",
            "--",
            "--",
            "--",
            "['methodology']",
            "['methodology']",
            "['background']",
            "['background']",
            "--",
            "--"
          ],
          "cited_paper_info": {
            "title": "Patent Keyword Extraction Algorithm Based on Distributed Representation for Patent Classification",
            "abstract": "Many text mining tasks such as text retrieval, text summarization, and text comparisons depend on the extraction of representative keywords from the main text. Most existing keyword extraction algorithms are based on discrete bag-of-words type of word representation of the text. In this paper, we propose a patent keyword extraction algorithm (PKEA) based on the distributed Skip-gram model for patent classification. We also develop a set of quantitative performance measures for keyword extraction evaluation based on information gain and cross-validation, based on Support Vector Machine (SVM) classification, which are valuable when human-annotated keywords are not available. We used a standard benchmark dataset and a homemade patent dataset to evaluate the performance of PKEA. Our patent dataset includes 2500 patents from five distinct technological fields related to autonomous cars (GPS systems, lidar systems, object recognition systems, radar systems, and vehicle control systems). We compared our method with Frequency, Term Frequency-Inverse Document Frequency (TF-IDF), TextRank and Rapid Automatic Keyword Extraction (RAKE). The experimental results show that our proposed algorithm provides a promising way to extract keywords from patent texts for patent classification.",
            "year": 2018,
            "venue": "Entropy",
            "authors": [
              {
                "authorId": "145815844",
                "name": "Jie Hu"
              },
              {
                "authorId": "2124883266",
                "name": "Shaobo Li"
              },
              {
                "authorId": "2110254652",
                "name": "Yong Yao"
              },
              {
                "authorId": "2112586000",
                "name": "Liya Yu"
              },
              {
                "authorId": "2394550",
                "name": "Guanci Yang"
              },
              {
                "authorId": "50778791",
                "name": "Jianjun Hu"
              }
            ]
          }
        },
        {
          "citedcorpusid": 5937747,
          "isinfluential": false,
          "contexts": [
            "…features from it (D’hondt et al., 2013; Shalaby et al., 2018; Hu et al., 2018a; Hu et al., 2018b; Li et al., 2018) while others focused on designing more effective classification algorithms (Fall et al., 2003; Al Shamsi & Aung, 2016; D’hondt et al., 2017; Wu et al., 2010, 2016; Song et al., 2019)."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Automatic patent classification by a three-phase model with document frequency matrix and boosted tree",
            "abstract": "",
            "year": 2016,
            "venue": "International Conference on Electronic Devices, Systems and Applications",
            "authors": [
              {
                "authorId": "38396228",
                "name": "F. Shamsi"
              },
              {
                "authorId": "1797305",
                "name": "Z. Aung"
              }
            ]
          }
        },
        {
          "citedcorpusid": 5959482,
          "isinfluential": false,
          "contexts": [
            "Besides, the word embeddings (e.g., word2vec (Mikolov et al., 2013)), which are the basis of the proposed deep-learning models, may face the out-of-vocabulary problem and cannot account for polysemy.",
            "To overcome the shortcomings of term frequency features, Mikolov et al. (2013) proposed the word2vec model for learning distributed representations of words in low dimensional spaces."
          ],
          "intents": [
            "['background']",
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Efficient Estimation of Word Representations in Vector Space",
            "abstract": "We propose two novel model architectures for computing continuous vector\nrepresentations of words from very large data sets. The quality of these\nrepresentations is measured in a word similarity task, and the results are\ncompared to the previously best performing techniques based on different types\nof neural networks. We observe large improvements in accuracy at much lower\ncomputational cost, i.e. it takes less than a day to learn high quality word\nvectors from a 1.6 billion words data set. Furthermore, we show that these\nvectors provide state-of-the-art performance on our test set for measuring\nsyntactic and semantic word similarities.",
            "year": 2013,
            "venue": "International Conference on Learning Representations",
            "authors": [
              {
                "authorId": "2047446108",
                "name": "Tomas Mikolov"
              },
              {
                "authorId": "2118440152",
                "name": "Kai Chen"
              },
              {
                "authorId": "32131713",
                "name": "G. Corrado"
              },
              {
                "authorId": "49959210",
                "name": "J. Dean"
              }
            ]
          }
        },
        {
          "citedcorpusid": 6628106,
          "isinfluential": false,
          "contexts": [
            "For all the models, Adam (Kingma & Ba, 2014) optimizer and a binary cross-entropy loss function were used."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Adam: A Method for Stochastic Optimization",
            "abstract": "We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm.",
            "year": 2014,
            "venue": "International Conference on Learning Representations",
            "authors": [
              {
                "authorId": "1726807",
                "name": "Diederik P. Kingma"
              },
              {
                "authorId": "2503659",
                "name": "Jimmy Ba"
              }
            ]
          }
        },
        {
          "citedcorpusid": 7365231,
          "isinfluential": false,
          "contexts": [
            "However, for the experiment on the M-patent dataset we set the number of epochs to 15 with early stopping (Caruana et al., 2001) of patience level 3 to prevent over-fitting."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Overfitting in Neural Nets: Backpropagation, Conjugate Gradient, and Early Stopping",
            "abstract": "The conventional wisdom is that backprop nets with excess hidden units generalize poorly. We show that nets with excess capacity generalize well when trained with backprop and early stopping. Experiments suggest two reasons for this: 1) Overfitting can vary significantly in different regions of the model. Excess capacity allows better fit to regions of high non-linearity, and backprop often avoids overfitting the regions of low non-linearity. 2) Regardless of size, nets learn task subcomponents in similar sequence. Big nets pass through stages similar to those learned by smaller nets. Early stopping can stop training the large net when it generalizes comparably to a smaller net. We also show that conjugate gradient can yield worse generalization because it overfits regions of low non-linearity when learning to fit regions of high non-linearity.",
            "year": 2000,
            "venue": "Neural Information Processing Systems",
            "authors": [
              {
                "authorId": "145727186",
                "name": "R. Caruana"
              },
              {
                "authorId": "145840115",
                "name": "S. Lawrence"
              },
              {
                "authorId": "145157784",
                "name": "C. Lee Giles"
              }
            ]
          }
        },
        {
          "citedcorpusid": 7449220,
          "isinfluential": false,
          "contexts": [
            "Furthermore, some attempts have been made to find which part of the patent text can be more representative and provide better classification results (Gomez, 2019; Hu et al., 2018a; Wu et al., 2010; D’hondt & Verberne, 2010)."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "CLEF-IP 2010: Prior Art Retrieval Using the Different Sections in Patent Documents",
            "abstract": "In this paper we describe our participation in the 2010 CLEF-IP Prior Art Retrieval task where we examined the impact of information in dierent sections of patent documents, namely the title, abstract, claims, description and IPC-R sections, on the retrieval and re-ranking of patent documents. Using a standard bag-of-words approach in Lemur we found that the IPC-R sections are the most informative for patent retrieval. We then performed a re-ranking of the retrieved documents using a Logistic Regression Model, trained on the retrieved documents in the training set. We found indications that the information contained in the text sections of the patent document can contribute to a better ranking of the retrieved documents. The ocial results have shown that among the nine groups that participated in the Prior Art Retrieval task we achieved the eigth rank in terms of both Mean Average Precision (MAP) and Recall.",
            "year": 2010,
            "venue": "Conference and Labs of the Evaluation Forum",
            "authors": [
              {
                "authorId": "1447164726",
                "name": "E.K.L. D'hondt"
              },
              {
                "authorId": "1702730",
                "name": "S. Verberne"
              }
            ]
          }
        },
        {
          "citedcorpusid": 12711052,
          "isinfluential": true,
          "contexts": [
            "M-patent : Moreover, we used a smaller dataset, which is a subset of the CLEF-IP 2011 (Piroi et al., 2011) dataset.",
            "The CLEF-IP dataset includes 1.35 million patents gathered from the European Patent Office (EPO) 4 and World Intellectual Property Organization (WIPO) 5 .",
            "Some previous research (Li et al., 2018; Hu et al., 2018a) utilized the evaluation measures from the CLEP-IP competition (Piroi et al., 2011).",
            ", 2018a) utilized the evaluation measures from the CLEP-IP competition (Piroi et al., 2011).",
            "M-patent: Moreover, we used a smaller dataset, which is a subset of the CLEF-IP 2011 (Piroi et al., 2011) dataset.",
            "DeepPatent outperformed other algorithms from the CLEF-IP (Piroi et al., 2011) competition and contributed the new USPTO-2M dataset, which is much larger than the previous benchmarks."
          ],
          "intents": [
            "['methodology']",
            "--",
            "['methodology']",
            "['methodology']",
            "['methodology']",
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "CLEF-IP 2011: Retrieval in the Intellectual Property Domain",
            "abstract": "The patent system is designed to encourage disclosure of new technologies and novel ideas by granting exclusive rights on the use of inventions to their inventors, for a limited period of time. Before a patent can be granted, patent o ces around the world perform thorough searches to ensure that no previous similar disclosures were made. In the intellectual property terminology, such kind of searches are called prior art searches. In some industries, the number of granted patents a company owns has a high impact on the market value of the company. This underlines the importance of well-performed prior art searches. Together with the Trec Chem track [5], also organized by our institution, the Clef Ip e ort comes to complete the work that is being done in the series of Ntcir workshops (see for example [4]). The rst Clef Ip track ran within Clef 2009. The purpose of the track was twofold: to encourage and facilitate research in the area of patent retrieval by providing a large clean data set for experimentation; to create a large test collection of patents in the three main European languages for the evaluation of cross lingual information access. The Clef Ip data set includes documents published by the European Patent O ce (Epo) which contain a mixture of English, German and French content. The track focused on the task of prior art search. In 2010 and 2011, the Clef Ip track was organized as a benchmarking activity (lab) in the Clef conference. In these years, the main goal of the Clef Ip e ort remained the same to foster research in the patent retrieval area, and provide a large clean data set. To this end, the number of tasks in the track was increased and the data set was enlarged. Recognizing the importance of patent classi cations in the daily activity of an intellectual property professional, in 2010 the Clef Ip benchmarking activity included a patent classi cation task. The participants were asked to classify",
            "year": 2011,
            "venue": "Conference and Labs of the Evaluation Forum",
            "authors": [
              {
                "authorId": "3309646",
                "name": "Florina Piroi"
              },
              {
                "authorId": "144307089",
                "name": "M. Lupu"
              },
              {
                "authorId": "1699657",
                "name": "A. Hanbury"
              },
              {
                "authorId": "2978384",
                "name": "V. Zenz"
              }
            ]
          }
        },
        {
          "citedcorpusid": 13756489,
          "isinfluential": false,
          "contexts": [
            "Recently, Transformer architecture proposed by Vaswani et al. (2017), which is based only on the attention mechanism, has made it possible to train large models with big data on GPUs efficiently.",
            "� ∑ The Transformer architecture proposed by Vaswani et al. (2017), as a sequence transduction model, is the foundation of NLP’s recent essential advancements."
          ],
          "intents": [
            "['background']",
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Attention is All you Need",
            "abstract": "The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.",
            "year": 2017,
            "venue": "Neural Information Processing Systems",
            "authors": [
              {
                "authorId": "40348417",
                "name": "Ashish Vaswani"
              },
              {
                "authorId": "1846258",
                "name": "Noam M. Shazeer"
              },
              {
                "authorId": "3877127",
                "name": "Niki Parmar"
              },
              {
                "authorId": "39328010",
                "name": "Jakob Uszkoreit"
              },
              {
                "authorId": "145024664",
                "name": "Llion Jones"
              },
              {
                "authorId": "19177000",
                "name": "Aidan N. Gomez"
              },
              {
                "authorId": "40527594",
                "name": "Lukasz Kaiser"
              },
              {
                "authorId": "3443442",
                "name": "I. Polosukhin"
              }
            ]
          }
        },
        {
          "citedcorpusid": 21663443,
          "isinfluential": true,
          "contexts": [
            "The deliberate usage of vague and new terms in patent documents are intended to protect and keep as broad as possible rights for the contained intellectual property (D’hondt et al., 2017; Shalaby et al., 2018; Gomez & Moens, 2014).",
            "However, they neglect the semantics of the word, and similar documents will not be considered similar if the terms used in them do not overlap enough (Shalaby et al., 2018).",
            "Shalaby et al. (2018) proposed a novel classification approach that represents patent documents and their structure as a Fixed Hierarchy Vectors (FHV) model and then used a single-layered LSTM architecture for multi-label classification.",
            "However, utilizing traditional text processing methods has not successfully processed patent text and effectively extracted features from it for patent search, retrieval, and classification (Shalaby et al., 2018).",
            "Some focused on the best way to represent the patent text and how to extract semantic features from it (D’hondt et al., 2013; Shalaby et al., 2018; Hu et al., 2018a; Hu et al., 2018b; Li et al., 2018) while others focused on designing more effective classification algorithms (Fall et al., 2003; Al…"
          ],
          "intents": [
            "['background']",
            "['background']",
            "['methodology']",
            "['methodology']",
            "['background']"
          ],
          "cited_paper_info": {
            "title": "An LSTM Approach to Patent Classification based on Fixed Hierarchy Vectors",
            "abstract": "",
            "year": 2018,
            "venue": "SDM",
            "authors": [
              {
                "authorId": "2113210651",
                "name": "M. Shalaby"
              },
              {
                "authorId": "3264870",
                "name": "J. Stutzki"
              },
              {
                "authorId": "39403212",
                "name": "Matthias Schubert"
              },
              {
                "authorId": "3075189",
                "name": "Stephan Günnemann"
              }
            ]
          }
        },
        {
          "citedcorpusid": 29763249,
          "isinfluential": false,
          "contexts": [
            "…features from it (D’hondt et al., 2013; Shalaby et al., 2018; Hu et al., 2018a; Hu et al., 2018b; Li et al., 2018) while others focused on designing more effective classification algorithms (Fall et al., 2003; Al Shamsi & Aung, 2016; D’hondt et al., 2017; Wu et al., 2010, 2016; Song et al., 2019)."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "A patent quality analysis and classification system using self-organizing maps with support vector machine",
            "abstract": "",
            "year": 2016,
            "venue": "Applied Soft Computing",
            "authors": [
              {
                "authorId": "2719770",
                "name": "Jheng-Long Wu"
              },
              {
                "authorId": "1717197",
                "name": "P. Chang"
              },
              {
                "authorId": "2707223",
                "name": "Cheng-Chin Tsao"
              },
              {
                "authorId": "1726734",
                "name": "C. Fan"
              }
            ]
          }
        },
        {
          "citedcorpusid": 43261268,
          "isinfluential": false,
          "contexts": [
            "Moreover, the distribution of the patent documents across the IPC categories is highly imbalanced (Gomez & Moens, 2014; Lupu et al., 2017)."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Current Challenges in Patent Information Retrieval",
            "abstract": "",
            "year": 2011,
            "venue": "The Information Retrieval Series",
            "authors": [
              {
                "authorId": "144307089",
                "name": "M. Lupu"
              },
              {
                "authorId": "2059169453",
                "name": "Katja Mayer"
              },
              {
                "authorId": "144959176",
                "name": "J. Tait"
              },
              {
                "authorId": "2494885",
                "name": "A. Trippe"
              }
            ]
          }
        },
        {
          "citedcorpusid": 51578296,
          "isinfluential": false,
          "contexts": [
            "New categories may appear in the classification structures, or the existing categories that have experienced little use are combined with other categories (Lupu & Hanbury, 2013)."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Patent Retrieval",
            "abstract": "Intellectual property and the patent system in particular have been extremely present in research and discussion, even in the public media, in the last few years. Without going into any controversial issues regarding the patent system, we approach a very real and growing problem: searching for innovation. The target collection for this task does not consist of patent documents only, but it is in these documents that the main difference is found compared to web or news information retrieval. In addition, the issue of patent search implies a particular user model and search process model. This review is concerned with how research and technology in the field of Information Retrieval assists or even changes the processes of patent search. It is a survey of work done on patent data in relation to Information Retrieval in the last 20–25 years. It explains the sources of difficulty and the existing document processing and retrieval methods of the domain, and provides a motivation for further research in the area.",
            "year": 2013,
            "venue": "Foundations and Trends in Information Retrieval",
            "authors": [
              {
                "authorId": "144307089",
                "name": "M. Lupu"
              },
              {
                "authorId": "1699657",
                "name": "A. Hanbury"
              }
            ]
          }
        },
        {
          "citedcorpusid": 67179900,
          "isinfluential": false,
          "contexts": [
            "…features from it (D’hondt et al., 2013; Shalaby et al., 2018; Hu et al., 2018a; Hu et al., 2018b; Li et al., 2018) while others focused on designing more effective classification algorithms (Fall et al., 2003; Al Shamsi & Aung, 2016; D’hondt et al., 2017; Wu et al., 2010, 2016; Song et al., 2019)."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "An effective High Recall Retrieval method",
            "abstract": "",
            "year": 2017,
            "venue": "Data & Knowledge Engineering",
            "authors": [
              {
                "authorId": "143671534",
                "name": "Justin JongSu Song"
              },
              {
                "authorId": "1728685",
                "name": "Wookey Lee"
              },
              {
                "authorId": "145653991",
                "name": "Jafar Afshar"
              }
            ]
          }
        },
        {
          "citedcorpusid": 131988231,
          "isinfluential": false,
          "contexts": [
            "A similar approach called, GloVe (Pennington et al., 2014), provides word embedding based on the global word-word co-occurrence counts rather than the fixed-sized windows of context."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
            "abstract": "",
            "year": 2014,
            "venue": "Conference on Empirical Methods in Natural Language Processing",
            "authors": [
              {
                "authorId": "1719404",
                "name": "Alessandro Moschitti"
              },
              {
                "authorId": "144865353",
                "name": "B. Pang"
              },
              {
                "authorId": "1735272",
                "name": "Walter Daelemans"
              }
            ]
          }
        },
        {
          "citedcorpusid": 174799315,
          "isinfluential": false,
          "contexts": [
            "Lee and Hsiang (2019) adopted the fine-tuning approach and used the pre-trained BERT model for the patent classification task."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "PatentBERT: Patent Classification with Fine-Tuning a pre-trained BERT Model",
            "abstract": "In this work we focus on fine-tuning a pre-trained BERT model and applying it to patent classification. When applied to large datasets of over two millions patents, our approach outperforms the state of the art by an approach using CNN with word embeddings. In addition, we focus on patent claims without other parts in patent documents. Our contributions include: (1) a new state-of-the-art method based on pre-trained BERT model and fine-tuning for patent classification, (2) a large dataset USPTO-3M at the CPC subclass level with SQL statements that can be used by future researchers, (3) showing that patent claims alone are sufficient for classification task, in contrast to conventional wisdom.",
            "year": 2019,
            "venue": "arXiv.org",
            "authors": [
              {
                "authorId": "2387987",
                "name": "Jieh-Sheng Lee"
              },
              {
                "authorId": "1798127",
                "name": "J. Hsiang"
              }
            ]
          }
        },
        {
          "citedcorpusid": 198953378,
          "isinfluential": false,
          "contexts": [
            "XLNet is based upon Transformer-XL (Dai et al., 2019) architecture, not the vanilla Transformer, resulting in achieving better performance compared to the BERT model.",
            "This has led to the appearance of Transformer-based pre-trained language models such as BERT (Devlin et al., 2018), XLNet (Yang et al., 2019), RoBERTa (Liu et al., 2019), ELECTRA (Clark et al., 2020), OpenAI GPT-3 Brown et al. (2020), among many others.",
            "Moreover, Liu et al. (2019) proposed RoBERTa that altered the key hyperparameters in BERT and conducted pre-training longer on much larger batches, learning rates, and data."
          ],
          "intents": [
            "['methodology']",
            "['methodology']",
            "['background']"
          ],
          "cited_paper_info": {
            "title": "RoBERTa: A Robustly Optimized BERT Pretraining Approach",
            "abstract": "Language model pretraining has led to significant performance gains but careful comparison between different approaches is challenging. Training is computationally expensive, often done on private datasets of different sizes, and, as we will show, hyperparameter choices have significant impact on the final results. We present a replication study of BERT pretraining (Devlin et al., 2019) that carefully measures the impact of many key hyperparameters and training data size. We find that BERT was significantly undertrained, and can match or exceed the performance of every model published after it. Our best model achieves state-of-the-art results on GLUE, RACE and SQuAD. These results highlight the importance of previously overlooked design choices, and raise questions about the source of recently reported improvements. We release our models and code.",
            "year": 2019,
            "venue": "arXiv.org",
            "authors": [
              {
                "authorId": "11323179",
                "name": "Yinhan Liu"
              },
              {
                "authorId": "40511414",
                "name": "Myle Ott"
              },
              {
                "authorId": "39589154",
                "name": "Naman Goyal"
              },
              {
                "authorId": "3048577",
                "name": "Jingfei Du"
              },
              {
                "authorId": "144863691",
                "name": "Mandar Joshi"
              },
              {
                "authorId": "50536468",
                "name": "Danqi Chen"
              },
              {
                "authorId": "39455775",
                "name": "Omer Levy"
              },
              {
                "authorId": "35084211",
                "name": "M. Lewis"
              },
              {
                "authorId": "1982950",
                "name": "Luke Zettlemoyer"
              },
              {
                "authorId": "1759422",
                "name": "Veselin Stoyanov"
              }
            ]
          }
        },
        {
          "citedcorpusid": 203639485,
          "isinfluential": false,
          "contexts": [
            "Furthermore, some attempts have been made to find which part of the patent text can be more representative and provide better classification results (Gomez, 2019; Hu et al., 2018a; Wu et al., 2010; D’hondt & Verberne, 2010)."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Analysis of the effect of data properties in automated patent classification",
            "abstract": "",
            "year": 2019,
            "venue": "Scientometrics",
            "authors": [
              {
                "authorId": "144004546",
                "name": "J. Gómez"
              }
            ]
          }
        },
        {
          "citedcorpusid": 218489934,
          "isinfluential": false,
          "contexts": [
            "Recently, some studies have implemented deep learning models such as convolutional neural network (CNN) (Li et al., 2018; Abdelgawad et al., 2019), recurrent neural network (RNN) based models (Grawe et al., 2017; Risch & Krestel, 2019), and their combination Hu et al. (2018a) for automated patent…",
            "Moreover, Abdelgawad et al. (2019) applied some recent deep learning models and investigated the effect of different word embeddings and neural network optimization on the patent classification task."
          ],
          "intents": [
            "['background']",
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Optimizing Neural Networks for Patent Classification",
            "abstract": "",
            "year": 2019,
            "venue": "ECML/PKDD",
            "authors": [
              {
                "authorId": "1434548968",
                "name": "Louay Abdelgawad"
              },
              {
                "authorId": "1792360",
                "name": "Peter Klügl"
              },
              {
                "authorId": "8722708",
                "name": "Erdan Genc"
              },
              {
                "authorId": "2154062",
                "name": "S. Falkner"
              },
              {
                "authorId": "144661829",
                "name": "F. Hutter"
              }
            ]
          }
        },
        {
          "citedcorpusid": 218971783,
          "isinfluential": false,
          "contexts": [
            "The emergence of new language representation models, such as OpenAI GPT-3 and many more to come, are making the fine-tuning approach more efficient and easier to implement.",
            "This has led to the appearance of Transformer-based pre-trained language models such as BERT (Devlin et al., 2018), XLNet (Yang et al., 2019), RoBERTa (Liu et al., 2019), ELECTRA (Clark et al., 2020), OpenAI GPT-3 Brown et al. (2020), among many others."
          ],
          "intents": [
            "--",
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Language Models are Few-Shot Learners",
            "abstract": "Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions - something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally, we find that GPT-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of GPT-3 in general.",
            "year": 2020,
            "venue": "Neural Information Processing Systems",
            "authors": [
              {
                "authorId": "31035595",
                "name": "Tom B. Brown"
              },
              {
                "authorId": "2056658938",
                "name": "Benjamin Mann"
              },
              {
                "authorId": "39849748",
                "name": "Nick Ryder"
              },
              {
                "authorId": "2065894334",
                "name": "Melanie Subbiah"
              },
              {
                "authorId": "152724169",
                "name": "J. Kaplan"
              },
              {
                "authorId": "6515819",
                "name": "Prafulla Dhariwal"
              },
              {
                "authorId": "2072676",
                "name": "Arvind Neelakantan"
              },
              {
                "authorId": "67311962",
                "name": "Pranav Shyam"
              },
              {
                "authorId": "144864359",
                "name": "Girish Sastry"
              },
              {
                "authorId": "119609682",
                "name": "Amanda Askell"
              },
              {
                "authorId": "144517868",
                "name": "Sandhini Agarwal"
              },
              {
                "authorId": "1404060687",
                "name": "Ariel Herbert-Voss"
              },
              {
                "authorId": "2064404342",
                "name": "Gretchen Krueger"
              },
              {
                "authorId": "103143311",
                "name": "T. Henighan"
              },
              {
                "authorId": "48422824",
                "name": "R. Child"
              },
              {
                "authorId": "1992922591",
                "name": "A. Ramesh"
              },
              {
                "authorId": "2052152920",
                "name": "Daniel M. Ziegler"
              },
              {
                "authorId": "49387725",
                "name": "Jeff Wu"
              },
              {
                "authorId": "2059411355",
                "name": "Clemens Winter"
              },
              {
                "authorId": "144239765",
                "name": "Christopher Hesse"
              },
              {
                "authorId": "2108828435",
                "name": "Mark Chen"
              },
              {
                "authorId": "2064673055",
                "name": "Eric Sigler"
              },
              {
                "authorId": "1380985420",
                "name": "Ma-teusz Litwin"
              },
              {
                "authorId": "145565184",
                "name": "Scott Gray"
              },
              {
                "authorId": "1490681878",
                "name": "Benjamin Chess"
              },
              {
                "authorId": "2115193883",
                "name": "Jack Clark"
              },
              {
                "authorId": "133740015",
                "name": "Christopher Berner"
              },
              {
                "authorId": "52238703",
                "name": "Sam McCandlish"
              },
              {
                "authorId": "38909097",
                "name": "Alec Radford"
              },
              {
                "authorId": "1701686",
                "name": "I. Sutskever"
              },
              {
                "authorId": "2698777",
                "name": "Dario Amodei"
              }
            ]
          }
        },
        {
          "citedcorpusid": 221593833,
          "isinfluential": false,
          "contexts": [
            "…patent documents is critical and will help experts manage patent documents, facilitate reliable patent search and retrieval, reduce the risk of missing a relevant patent in preventing patent infringement, and further patent analysis tasks (Yun & Geum, 2020; Souza et al., 2020; Gomez & Moens, 2014).",
            "This increase in the number of patent documents makes patent analysis and management more complicated and time-consuming for patent experts and examiners, posing significant challenges for many patent information users (Yun & Geum, 2020; Chen et al., 2020b)."
          ],
          "intents": [
            "['background']",
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Automated classification of patents: A topic modeling approach",
            "abstract": "",
            "year": 2020,
            "venue": "Computers & industrial engineering",
            "authors": [
              {
                "authorId": "52413784",
                "name": "Junghwan Yun"
              },
              {
                "authorId": "32862153",
                "name": "Y. Geum"
              }
            ]
          }
        },
        {
          "citedcorpusid": 235386502,
          "isinfluential": false,
          "contexts": [
            "Nevertheless, these embedding vectors, combined with other neural networks such as RNN and CNNs, have successfully achieved good results on various NLP tasks such as text classification (Minaee et al., 2020).",
            "These models convert the input text to a feature vector representation considering the context by extracting higher-level features from word vectors (Minaee et al., 2020)."
          ],
          "intents": [
            "--",
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Deep Learning--based Text Classification",
            "abstract": "Deep learning--based models have surpassed classical machine learning--based approaches in various text classification tasks, including sentiment analysis, news categorization, question answering, and natural language inference. In this article, we provide a comprehensive review of more than 150 deep learning--based models for text classification developed in recent years, and we discuss their technical contributions, similarities, and strengths. We also provide a summary of more than 40 popular datasets widely used for text classification. Finally, we provide a quantitative analysis of the performance of different deep learning models on popular benchmarks, and we discuss future research directions.",
            "year": 2020,
            "venue": "ACM Computing Surveys",
            "authors": [
              {
                "authorId": "2164604",
                "name": "Shervin Minaee"
              },
              {
                "authorId": "49943757",
                "name": "E. Cambria"
              },
              {
                "authorId": "48441311",
                "name": "Jianfeng Gao"
              }
            ]
          }
        },
        {
          "citedcorpusid": null,
          "isinfluential": false,
          "contexts": [
            "The publicly available Simple Transformers (Rajapakse, 2019) library built on top of the famous Hugging Face transformers library (Wolf et al., 2019) was used for conducting the experiments."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {}
        },
        {
          "citedcorpusid": null,
          "isinfluential": false,
          "contexts": [
            "On the other hand, in the micro-averaging strategy, the counters of misses and hits are aggregated first, and then the desired metric is calculated only once (Charte et al., 2016)."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {}
        },
        {
          "citedcorpusid": null,
          "isinfluential": false,
          "contexts": [
            "On the other hand, ELECTRA (Clark et al., 2020) pre-training language model uses replaced token detection ( RTD ), a new sample-efficient pre-training task, that learns a bidirectional model from all input positions.",
            "(Clark et al., 2020) We propose using pre-trained language models for representing the patent text and fine-tuning them for the downstream task of the multi-label patent classification problem.",
            "This has led to the appearance of Transformer-based pre-trained language models such as BERT (Devlin et al., 2018), XLNet (Yang et al., 2019), RoBERTa (Liu et al., 2019), ELECTRA (Clark et al., 2020), OpenAI GPT-3 Brown et al. (2020), among many others."
          ],
          "intents": [
            "['methodology']",
            "['methodology']",
            "['methodology']"
          ],
          "cited_paper_info": {}
        }
      ]
    },
    "203639485": {
      "citing_paper_info": {
        "title": "Analysis of the effect of data properties in automated patent classification",
        "abstract": "",
        "year": 2019,
        "venue": "Scientometrics",
        "authors": [
          {
            "authorId": "144004546",
            "name": "J. Gómez"
          }
        ]
      },
      "citation_summary": {
        "citation_count": 11,
        "unique_cited_count": 11,
        "influential_count": 3,
        "detailed_records_count": 11
      },
      "cited_papers": [
        "695217",
        "64850768",
        "1407274",
        "16447573",
        "35058596",
        "15533714",
        "14587278",
        "62403967",
        "2871882",
        "3116168",
        "10988962"
      ],
      "citation_details": [
        {
          "citedcorpusid": 695217,
          "isinfluential": true,
          "contexts": [
            "2014); testing methods for extreme machine learning (Wang et al. 2014) or dimensionality reduction (Shalaby et al.",
            "These works had different goals than the particularities of patent classification, such as testing the efficiency and/or scalability of their particular methods in general text classification (Gomez and Moens 2014), hierarchical classification (Wang et al. 2014), or node classification in graphs (Dallachiesa et al.",
            "2014); extract relevant content (Härtinger and Clarke 2015; Rodriguez-Esteban and Bundschus 2016; Wang et al. 2014); and investigate technology characteristics (Arts et al."
          ],
          "intents": [
            "['methodology']",
            "['background']",
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Automated Categorisation of Patent Claims that Reference Human Genome Sequences",
            "abstract": "",
            "year": 2014,
            "venue": "Australasian Document Computing Symposium",
            "authors": [
              {
                "authorId": "2221234045",
                "name": "Donglu Wang"
              },
              {
                "authorId": "2420278",
                "name": "Gabriela Ferraro"
              },
              {
                "authorId": "1712592",
                "name": "H. Suominen"
              },
              {
                "authorId": "79846493",
                "name": "O. Jefferson"
              }
            ]
          }
        },
        {
          "citedcorpusid": 1407274,
          "isinfluential": true,
          "contexts": [
            "Some works have already tried using phrases (D’hondt et al. 2013, 2014; Verberne et al. 2010; Verberne and D’hondt 2011), but the performance obtained is similar or even worse than using word features.",
            "2010; Verberne and D’hondt 2011), using the abstract section and words and triplets features.",
            "2010; Verberne and D’hondt 2011), but the performance obtained is similar or even worse than using word features.",
            "Some of the best models were based on the Winnow classifier (Guyot et al. 2010; Verberne et al. 2010; Verberne and D’hondt 2011), using the abstract section and words and triplets features."
          ],
          "intents": [
            "--",
            "['methodology']",
            "['result']",
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Patent Classification Experiments with the Linguistic Classification System LCS",
            "abstract": "We report the results of a series of classification experiments with the Linguistic Classification System LCS in the context of CLEF-IP 2011. We participated in the main classification task: classifying documents on the subclass level. We investigated (1) the use of different sections (abstract, description, metadata) from the patent documents; (2) adding dependency triples to the bag-of-words representation; (3) adding the WIPO corpus to the EPO training data; (4) the use of patent citations in the test data for reranking the classes; and (5) the threshold on the class scores for class selection. We found that adding full descriptions to abstracts gives a clear improvement; the first 400 words of the description also improves classification but to a lesser degree. Adding metadata (applicants, inventors en address) did not improve classification. Adding dependency triples to words gives a much higher recall at the cost of a lower precision but this effect is largely due to the class selection threshold. We did not find an effect from adding the WIPO corpus, nor from reranking with patent citations. In future work, we plan to investigate whether there are other methods for reranking with patent citations that does give an improvement, because we feel that the citations may still give valuable information. Our most important finding however is the importance of the threshold on the class selection. For the current work, we only compared two values for the threshold and the results are much better for 1.0 than for 0.5. The 0.5 threshold gives higher recall in all runs, which was the original motivation for submitting runs with a lower threshold. However, because the much lower precision, the F-scores are lower. We think that there is still some improvement to be gained from proper tuning of the class selection threshold, and the use of a flexible threshold (also taking into account the different text representations). This is part of our future work.",
            "year": 2010,
            "venue": "Conference and Labs of the Evaluation Forum",
            "authors": [
              {
                "authorId": "1702730",
                "name": "S. Verberne"
              },
              {
                "authorId": "25486420",
                "name": "M. Vogel"
              },
              {
                "authorId": "1447164726",
                "name": "E.K.L. D'hondt"
              }
            ]
          }
        },
        {
          "citedcorpusid": 2871882,
          "isinfluential": false,
          "contexts": [
            "These works had different goals than the particularities of patent classification, such as testing the efficiency and/or scalability of their particular methods in general text classification (Gomez and Moens 2014), hierarchical classification (Wang et al.",
            "2017; Fall and Benzineb 2002; Gomez and Moens 2014).",
            "The content of a patent is generally organized in the following sections (Fall and Benzineb 2002; Benzineb and Guyot 2011; Lupu and Hanbury 2013; Gomez and Moens 2014):\n• Title: indicates a descriptive name of the patent.",
            "The automated patent classification task has been tackled along the last decade as a text classification problem using several methods and approaches (Benzineb and Guyot 2011; Gomez and Moens 2014).",
            "…classification of patents, starting with some surveys about the task (Fall and Benzineb 2002; Krier and Zaccà 2002; Benzineb and Guyot 2011; Gomez and Moens 2014) where some issues are highlighted, such as model accuracy, scalability, use of the hierarchy of categories, patent sections to…"
          ],
          "intents": [
            "['background']",
            "['background']",
            "['background']",
            "['methodology']",
            "['background']"
          ],
          "cited_paper_info": {
            "title": "A Survey of Automated Hierarchical Classification of Patents",
            "abstract": "",
            "year": 2014,
            "venue": "Professional Search in the Modern World",
            "authors": [
              {
                "authorId": "144004546",
                "name": "J. Gómez"
              },
              {
                "authorId": "145446752",
                "name": "Marie-Francine Moens"
              }
            ]
          }
        },
        {
          "citedcorpusid": 3116168,
          "isinfluential": false,
          "contexts": [
            "2009) for NB, Liblinear (Fan et al. 2008) for SVM and LR, and for KNN a proprietary implementation that takes advantage of the sparseness of the vector representation.",
            "2009) for NB, Liblinear (Fan et al."
          ],
          "intents": [
            "['methodology']",
            "--"
          ],
          "cited_paper_info": {
            "title": "LIBLINEAR: A Library for Large Linear Classification",
            "abstract": "LIBLINEAR is an open source library for large-scale linear classification. It supports logistic regression and linear support vector machines. We provide easy-to-use command-line tools and library calls for users and developers. Comprehensive documents are available for both beginners and advanced users. Experiments demonstrate that LIBLINEAR is very efficient on large sparse data sets.",
            "year": 2008,
            "venue": "Journal of machine learning research",
            "authors": [
              {
                "authorId": "1849128",
                "name": "Rong-En Fan"
              },
              {
                "authorId": "2782886",
                "name": "Kai-Wei Chang"
              },
              {
                "authorId": "1793529",
                "name": "Cho-Jui Hsieh"
              },
              {
                "authorId": "2144799660",
                "name": "Xiang-Rui Wang"
              },
              {
                "authorId": "1711460",
                "name": "Chih-Jen Lin"
              }
            ]
          }
        },
        {
          "citedcorpusid": 10988962,
          "isinfluential": false,
          "contexts": [
            "Some of the best models were based on the Winnow classifier (Guyot et al.",
            "2006) and Winnow (D’hondt et al. 2017; Koster et al. 2003) using different features such as phrases and deep learning word representations.",
            "2006) and Winnow (D’hondt et al."
          ],
          "intents": [
            "--",
            "['methodology']",
            "--"
          ],
          "cited_paper_info": {
            "title": "Multi-classification of Patent Applications with Winnow",
            "abstract": "",
            "year": 2003,
            "venue": "Ershov Memorial Conference",
            "authors": [
              {
                "authorId": "1713642",
                "name": "C. Koster"
              },
              {
                "authorId": "2286294",
                "name": "M. Seutter"
              },
              {
                "authorId": "145639626",
                "name": "Jean Beney"
              }
            ]
          }
        },
        {
          "citedcorpusid": 14587278,
          "isinfluential": false,
          "contexts": [
            "We decided to use word features because in several works it have been pointed out that these features outperform other more complex representations in several classification/prediction tasks (Cinar et al. 2015; Basile et al. 2017), including patent classification (D’hondt et al."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Inferring User Interests on Social Media from Text and Images",
            "abstract": "",
            "year": 2015,
            "venue": "2015 IEEE International Conference on Data Mining Workshop (ICDMW)",
            "authors": [
              {
                "authorId": "2562951",
                "name": "Yagmur Gizem Cinar"
              },
              {
                "authorId": "3348963",
                "name": "Susana Zoghbi"
              },
              {
                "authorId": "145446752",
                "name": "Marie-Francine Moens"
              }
            ]
          }
        },
        {
          "citedcorpusid": 15533714,
          "isinfluential": false,
          "contexts": [
            "For these issues, an interesting research direction would be to apply methods that manipulate word features to increase the cohesion between documents from the same category, while at the same time increasing the separability between categories (Gomez and Moens 2010).",
            "2015) that find the features that are highly associated with specific categories, maximizing the intra document similarity and minimizing the inter category similarity (Gomez and Moens 2010)."
          ],
          "intents": [
            "['background']",
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Using Biased Discriminant Analysis for Email Filtering",
            "abstract": "",
            "year": 2010,
            "venue": "International Conference on Knowledge-Based Intelligent Information & Engineering Systems",
            "authors": [
              {
                "authorId": "144004546",
                "name": "J. Gómez"
              },
              {
                "authorId": "145446752",
                "name": "Marie-Francine Moens"
              }
            ]
          }
        },
        {
          "citedcorpusid": 16447573,
          "isinfluential": false,
          "contexts": [
            "The w2v features where computed using the Gensim module from Python5 by training a skip-gram model (Mikolov et al. 2013) over the training part of each dataset (see next subsection), extracting word embeddings of 300 dimensions."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Distributed Representations of Words and Phrases and their Compositionality",
            "abstract": "The recently introduced continuous Skip-gram model is an efficient method for learning high-quality distributed vector representations that capture a large number of precise syntactic and semantic word relationships. In this paper we present several extensions that improve both the quality of the vectors and the training speed. By subsampling of the frequent words we obtain significant speedup and also learn more regular word representations. We also describe a simple alternative to the hierarchical softmax called negative sampling. \n \nAn inherent limitation of word representations is their indifference to word order and their inability to represent idiomatic phrases. For example, the meanings of \"Canada\" and \"Air\" cannot be easily combined to obtain \"Air Canada\". Motivated by this example, we present a simple method for finding phrases in text, and show that learning good vector representations for millions of phrases is possible.",
            "year": 2013,
            "venue": "Neural Information Processing Systems",
            "authors": [
              {
                "authorId": "2047446108",
                "name": "Tomas Mikolov"
              },
              {
                "authorId": "1701686",
                "name": "I. Sutskever"
              },
              {
                "authorId": "2118440152",
                "name": "Kai Chen"
              },
              {
                "authorId": "32131713",
                "name": "G. Corrado"
              },
              {
                "authorId": "49959210",
                "name": "J. Dean"
              }
            ]
          }
        },
        {
          "citedcorpusid": 35058596,
          "isinfluential": false,
          "contexts": [
            "2005; Zhang 2014).",
            "There are other works that have used the WIPO datasets (specially the WIPO-alpha) for experimentation, many of them focusing on kernel classifier methods that take the hierarchical structure into account (Bi and Kwok 2014; Cai and Hofmann 2004; Chen and Chang 2012; Rousu et al. 2006; Tsochantaridis et al. 2005; Zhang 2014)."
          ],
          "intents": [
            "--",
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Interactive patent classification based on multi-classifier fusion and active learning",
            "abstract": "",
            "year": 2014,
            "venue": "Neurocomputing",
            "authors": [
              {
                "authorId": "2141934368",
                "name": "Xiaoyu Zhang"
              }
            ]
          }
        },
        {
          "citedcorpusid": 62403967,
          "isinfluential": false,
          "contexts": [
            "CLEF-IP tracks included a task on patent classification Piroi (2010, 2011)."
          ],
          "intents": [
            "--"
          ],
          "cited_paper_info": {
            "title": "CLEF-IP 2010: Classification Task Evaluation Summary",
            "abstract": "This report presents the detailed evaluation measures done in the frame of the CLEF-IP 2010 benchmarking activity, Classification Task.",
            "year": 2010,
            "venue": "",
            "authors": [
              {
                "authorId": "3309646",
                "name": "Florina Piroi"
              }
            ]
          }
        },
        {
          "citedcorpusid": 64850768,
          "isinfluential": true,
          "contexts": [
            "Some of the best models were based on the Winnow classifier (Guyot et al.",
            "2006) and Winnow (D’hondt et al.",
            "2006) and Winnow (D’hondt et al. 2017; Koster et al. 2003) using different features such as phrases and deep learning word representations.",
            "2017), including patent classification (D’hondt et al. 2017).",
            "Nevertheless, despite the research done it is still an open problem with several unsolved issues regarding the general low accuracy obtained (Benzineb and Guyot 2011; D’hondt et al. 2017; Fall and Benzineb 2002; Gomez and Moens 2014)."
          ],
          "intents": [
            "--",
            "--",
            "['methodology']",
            "['background']",
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Patent Classification on Subgroup Level Using Balanced Winnow",
            "abstract": "",
            "year": 2017,
            "venue": "",
            "authors": [
              {
                "authorId": "1447164726",
                "name": "E.K.L. D'hondt"
              },
              {
                "authorId": "1702730",
                "name": "S. Verberne"
              },
              {
                "authorId": "52109694",
                "name": "N. Oostdijk"
              },
              {
                "authorId": "1728633",
                "name": "L. Boves"
              }
            ]
          }
        }
      ]
    },
    "208822039": {
      "citing_paper_info": {
        "title": "A Study on Trend Analysis of Applicants Based on Patent Classification Systems",
        "abstract": "In recent times, with the development of science and technology, new technologies have been rapidly emerging, and innovators are making efforts to acquire intellectual property rights to preserve their competitive advantage as well as to enhance innovative competitiveness. As a result, the number of patents being acquired increases exponentially every year, and the social and economic ripple effects of developed technologies are also increasing. Now, innovators are focusing on evaluating existing technologies to develop more valuable ones. However, existing patent analysis studies mainly focus on discovering core technologies amongst the technologies derived from patents or analyzing trend changes for specific techniques; the analysis of innovators who develop such core technologies is insufficient. In this paper, we propose a model for analyzing the technical inventions of applicants based on patent classification systems such as international patent classification (IPC) and cooperative patent classification (CPC). Through the proposed model, the common invention patterns of applicants are extracted and used to analyze their technical inventions. The proposed model shows that patent classification systems can be used to extract the trends in applicants’ technological inventions and to track changes in their innovative patterns.",
        "year": 2019,
        "venue": "Inf.",
        "authors": [
          {
            "authorId": "1484246160",
            "name": "Soohyeon Chae"
          },
          {
            "authorId": "39733655",
            "name": "Jangwon Gim"
          }
        ]
      },
      "citation_summary": {
        "citation_count": 4,
        "unique_cited_count": 3,
        "influential_count": 0,
        "detailed_records_count": 4
      },
      "cited_papers": [
        "116477453",
        "115288648",
        "64652083"
      ],
      "citation_details": [
        {
          "citedcorpusid": 64652083,
          "isinfluential": false,
          "contexts": [
            "Reference [12,13] propose IPC automatic classification system has focused on applying various existing machine learning methods to the patent documents rather than considering the characteristics of the data or the structure of the patent documents."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "IPC Multi-label Classification Applying the Characteristics of Patent Documents",
            "abstract": "",
            "year": 2016,
            "venue": "Computer Science and its Applications / Computer Science and Ubiquitous Computing",
            "authors": [
              {
                "authorId": "2174527",
                "name": "Sora Lim"
              },
              {
                "authorId": "2115745371",
                "name": "Y. Kwon"
              }
            ]
          }
        },
        {
          "citedcorpusid": 115288648,
          "isinfluential": false,
          "contexts": [
            "Reference [20] provide a means of accelerating searches for relevant documents based on the CPC classiﬁcation system."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Searching with combination sets in CPC: An efficient way to retrieve relevant documents",
            "abstract": "",
            "year": 2018,
            "venue": "World Patent Information",
            "authors": [
              {
                "authorId": "143827678",
                "name": "P. Masson"
              }
            ]
          }
        },
        {
          "citedcorpusid": 116477453,
          "isinfluential": false,
          "contexts": [
            "As the number of patent applications gradually increased, the international patent classification (IPC) was developed in 1968 due to the necessity of classifying patents by various domains [6]."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Analysis of the patent documentation coverage of the CPC in comparison with the IPC with a focus on Asian documentation",
            "abstract": "",
            "year": 2018,
            "venue": "World Patent Information",
            "authors": [
              {
                "authorId": "101878620",
                "name": "Bart Degroote"
              },
              {
                "authorId": "2062597739",
                "name": "Pierre Held"
              }
            ]
          }
        },
        {
          "citedcorpusid": null,
          "isinfluential": false,
          "contexts": [
            "In the era of the Fourth Industrial Revolution, new technologies are rapidly being invented, and inventors are making significant efforts to claim rights to sources and core technologies [1,2]."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {}
        }
      ]
    },
    "67864300": {
      "citing_paper_info": {
        "title": "Universal Language Model Fine-tuning for Patent Classification",
        "abstract": "",
        "year": 2018,
        "venue": "Australasian Language Technology Association Workshop",
        "authors": [
          {
            "authorId": "73767302",
            "name": "Jason Hepburn"
          }
        ]
      },
      "citation_summary": {
        "citation_count": 3,
        "unique_cited_count": 3,
        "influential_count": 1,
        "detailed_records_count": 3
      },
      "cited_papers": [
        "212756",
        "680295",
        "40100965"
      ],
      "citation_details": [
        {
          "citedcorpusid": 212756,
          "isinfluential": false,
          "contexts": [
            "This model uses the state of the art language model AWD LSTM trained on Wikitext-103 (Merity et al., 2017)"
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Regularizing and Optimizing LSTM Language Models",
            "abstract": "Recurrent neural networks (RNNs), such as long short-term memory networks (LSTMs), serve as a fundamental building block for many sequence learning tasks, including machine translation, language modeling, and question answering. In this paper, we consider the specific problem of word-level language modeling and investigate strategies for regularizing and optimizing LSTM-based models. We propose the weight-dropped LSTM which uses DropConnect on hidden-to-hidden weights as a form of recurrent regularization. Further, we introduce NT-ASGD, a variant of the averaged stochastic gradient method, wherein the averaging trigger is determined using a non-monotonic condition as opposed to being tuned by the user. Using these and other regularization strategies, we achieve state-of-the-art word level perplexities on two data sets: 57.3 on Penn Treebank and 65.8 on WikiText-2. In exploring the effectiveness of a neural cache in conjunction with our proposed model, we achieve an even lower state-of-the-art perplexity of 52.8 on Penn Treebank and 52.0 on WikiText-2.",
            "year": 2017,
            "venue": "International Conference on Learning Representations",
            "authors": [
              {
                "authorId": "3375440",
                "name": "Stephen Merity"
              },
              {
                "authorId": "2844898",
                "name": "N. Keskar"
              },
              {
                "authorId": "2166511",
                "name": "R. Socher"
              }
            ]
          }
        },
        {
          "citedcorpusid": 680295,
          "isinfluential": false,
          "contexts": [
            "Seneviratne et al. (2015) build on Falls work with a focus on improving the efﬁciency of clas-siﬁcation."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "A Signature Approach to Patent Classification",
            "abstract": "",
            "year": 2015,
            "venue": "Asia Information Retrieval Symposium",
            "authors": [
              {
                "authorId": "2086855",
                "name": "Dilesha Seneviratne"
              },
              {
                "authorId": "1707376",
                "name": "S. Geva"
              },
              {
                "authorId": "1692855",
                "name": "G. Zuccon"
              },
              {
                "authorId": "2420278",
                "name": "Gabriela Ferraro"
              },
              {
                "authorId": "1412499172",
                "name": "Timothy Chappell"
              },
              {
                "authorId": "32055777",
                "name": "M. Meireles"
              }
            ]
          }
        },
        {
          "citedcorpusid": 40100965,
          "isinfluential": true,
          "contexts": [
            "Universal Language Model Fine-tuning (ULM-FiT) is a transfer learning technique introduced by Howard and Ruder (2018).",
            "Section 4.3 describes ULMFiT from Howard and Ruder (2018) and how it is adapted to this task.",
            "As this step is not domain speciﬁc here we have used the pretrained model 3 from Howard and Ruder (2018)."
          ],
          "intents": [
            "['methodology']",
            "['background']",
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Universal Language Model Fine-tuning for Text Classification",
            "abstract": "Inductive transfer learning has greatly impacted computer vision, but existing approaches in NLP still require task-specific modifications and training from scratch. We propose Universal Language Model Fine-tuning (ULMFiT), an effective transfer learning method that can be applied to any task in NLP, and introduce techniques that are key for fine-tuning a language model. Our method significantly outperforms the state-of-the-art on six text classification tasks, reducing the error by 18-24% on the majority of datasets. Furthermore, with only 100 labeled examples, it matches the performance of training from scratch on 100 times more data. We open-source our pretrained models and code.",
            "year": 2018,
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "2093348519",
                "name": "Jeremy Howard"
              },
              {
                "authorId": "2884561",
                "name": "Sebastian Ruder"
              }
            ]
          }
        }
      ]
    },
    "115115260": {
      "citing_paper_info": {
        "title": "ICT: A new taxonomy based on the international patent classification",
        "abstract": "This work proposes a definition of Information and Communication Technologies (ICT) based on the technology classes of the International Patent Classification (IPC) in which patents are classified. This new taxonomy, called the “J tag”, aligns with the definitions of the ICT sector (2007) and of ICT products (2008) put forward by the OECD, and stems from the in-depth knowledge of Japan Patent Office experts, as well of experts from the Intellectual Property (IP) Offices participating in the OECD-led IP Task Force. Expert judgment of patent class content, relevance for ICT-related products, completeness and accuracy are the principles guiding the inclusion of IPC classes in the “J tag” taxonomy. ICT technologies are subdivided into 13 areas defined with respect to the specific technical features and functions they are supposed to accomplish (e.g. mobile communication), and details provided about the ways in which technologies relate to ICT products.",
        "year": 2017,
        "venue": "",
        "authors": [
          {
            "authorId": "88541763",
            "name": "Takashi Inaba"
          },
          {
            "authorId": "50190643",
            "name": "Mariagrazia Squicciarini"
          }
        ]
      },
      "citation_summary": {
        "citation_count": 1,
        "unique_cited_count": 1,
        "influential_count": 0,
        "detailed_records_count": 1
      },
      "cited_papers": [
        "60713919"
      ],
      "citation_details": [
        {
          "citedcorpusid": 60713919,
          "isinfluential": false,
          "contexts": [
            "Video equipment, television, image processing, acoustic equipment, and audio signal processing-related technologies (see e.g. Rosenfeld at al., 2014; Bovik, 2010; Spanias at al., 2006; ATIS, 2001)."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Handbook of Image and Video Processing",
            "abstract": "",
            "year": 2000,
            "venue": "",
            "authors": [
              {
                "authorId": "144442185",
                "name": "J. Gibson"
              },
              {
                "authorId": "1747569",
                "name": "A. Bovik"
              }
            ]
          }
        }
      ]
    },
    "22139415": {
      "citing_paper_info": {
        "title": "Automated Patent Classification Using Word Embedding",
        "abstract": "",
        "year": 2017,
        "venue": "International Conference on Machine Learning and Applications",
        "authors": [
          {
            "authorId": "35272389",
            "name": "Mattyws F. Grawe"
          },
          {
            "authorId": "144097599",
            "name": "C. A. Martins"
          },
          {
            "authorId": "2078352187",
            "name": "Andreia Gentil Bonfante"
          }
        ]
      },
      "citation_summary": {
        "citation_count": 3,
        "unique_cited_count": 3,
        "influential_count": 0,
        "detailed_records_count": 3
      },
      "cited_papers": [
        "1915014",
        "16447573",
        "4456086"
      ],
      "citation_details": [
        {
          "citedcorpusid": 1915014,
          "isinfluential": false,
          "contexts": [
            "The temporal evolution of the path integral over all error signals flowing back in time exponentially depends on the magnitude of the weights, implying that the backpropagated error either vanishes or explode [7], [8]."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Long Short-Term Memory",
            "abstract": "",
            "year": 1997,
            "venue": "Neural Computation",
            "authors": [
              {
                "authorId": "3308557",
                "name": "Sepp Hochreiter"
              },
              {
                "authorId": "145341374",
                "name": "J. Schmidhuber"
              }
            ]
          }
        },
        {
          "citedcorpusid": 4456086,
          "isinfluential": false,
          "contexts": [
            "[3] it achieves 68% of F1 at subclass level, using the Winnow algorithm to perform the classification task."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "LCI-INSA Linguistic Experiment for CLEF-IP Classification Track",
            "abstract": "We present the experiment the LCI group has performed to prepare our submission to CLEF-IP Classification Track. In this preliminary experiment we used a part of the available target documents as test set and the rest as train set. We describe the systems AGFL used for extracting these triples and the LCS used for classification by the Winnow algorithm. We show that the use of linguistic triples in place of bags of words improves the accuracy, as well as using the names and addresses of the applicants. we found that using the complete descriptions as bags of words does not really perform better than using only abstracts and titles. Some simple mathematics show that the official measures are redundant and that R@N should be used to evaluate a ranking, P@1 to evaluate routing and that the usual precision, recall and F1 should be used on the results of a real classification, that is a selection of the classes performed internally by the classifier.",
            "year": 2010,
            "venue": "Conference and Labs of the Evaluation Forum",
            "authors": [
              {
                "authorId": "145639626",
                "name": "Jean Beney"
              }
            ]
          }
        },
        {
          "citedcorpusid": 16447573,
          "isinfluential": false,
          "contexts": [
            "The stopword removal do not affect the overall result for the Word2Vec model [9], but removing it affects decreasing the time to train the Word2Vec."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Distributed Representations of Words and Phrases and their Compositionality",
            "abstract": "The recently introduced continuous Skip-gram model is an efficient method for learning high-quality distributed vector representations that capture a large number of precise syntactic and semantic word relationships. In this paper we present several extensions that improve both the quality of the vectors and the training speed. By subsampling of the frequent words we obtain significant speedup and also learn more regular word representations. We also describe a simple alternative to the hierarchical softmax called negative sampling. \n \nAn inherent limitation of word representations is their indifference to word order and their inability to represent idiomatic phrases. For example, the meanings of \"Canada\" and \"Air\" cannot be easily combined to obtain \"Air Canada\". Motivated by this example, we present a simple method for finding phrases in text, and show that learning good vector representations for millions of phrases is possible.",
            "year": 2013,
            "venue": "Neural Information Processing Systems",
            "authors": [
              {
                "authorId": "2047446108",
                "name": "Tomas Mikolov"
              },
              {
                "authorId": "1701686",
                "name": "I. Sutskever"
              },
              {
                "authorId": "2118440152",
                "name": "Kai Chen"
              },
              {
                "authorId": "32131713",
                "name": "G. Corrado"
              },
              {
                "authorId": "49959210",
                "name": "J. Dean"
              }
            ]
          }
        }
      ]
    },
    "70121299": {
      "citing_paper_info": {
        "title": "Fast and Accurate Patent Classification in Search Engines",
        "abstract": "This article presents a new approach to large scale patent classification. The need to classify documents often takes place in professional information retrieval systems. In this paper we describe our approach, based on linguistically-supported k-nearest neighbors. We experimentally evaluate it on the Russian and English datasets and compare modern classification technique fastText. We show that KNN is a viable alternative to traditional text classifiers, achieving comparable accuracy while using less additional hardware resources.",
        "year": 2018,
        "venue": "Journal of Physics: Conference Series",
        "authors": [
          {
            "authorId": "77510891",
            "name": "V. Yadrintsev"
          },
          {
            "authorId": "35291933",
            "name": "Amir Bakarov"
          },
          {
            "authorId": "1956107",
            "name": "R. Suvorov"
          },
          {
            "authorId": "2469049",
            "name": "I. Sochenkov"
          }
        ]
      },
      "citation_summary": {
        "citation_count": 2,
        "unique_cited_count": 2,
        "influential_count": 0,
        "detailed_records_count": 2
      },
      "cited_papers": [
        "195348644",
        "262553219"
      ],
      "citation_details": [
        {
          "citedcorpusid": 195348644,
          "isinfluential": false,
          "contexts": [
            "Some works also tried to exploit this feature, proposing more accurate document representations for patent classification [13], or involving active learning methodologies [14]."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Computational Linguistics",
            "abstract": "",
            "year": 2015,
            "venue": "Communications in Computer and Information Science",
            "authors": [
              {
                "authorId": "2479037",
                "name": "Frank Rudzicz"
              },
              {
                "authorId": "2319870069",
                "name": "Graeme Hirst"
              }
            ]
          }
        },
        {
          "citedcorpusid": 262553219,
          "isinfluential": false,
          "contexts": [
            "The use of the search engine for similarity search assumes that the first step is indexing [4]."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "An Introduction to Information Retrieval",
            "abstract": "",
            "year": 2013,
            "venue": "",
            "authors": [
              {
                "authorId": "144161686",
                "name": "S. Ceri"
              },
              {
                "authorId": "1710630",
                "name": "A. Bozzon"
              },
              {
                "authorId": "40350773",
                "name": "Marco Brambilla"
              },
              {
                "authorId": "2539248",
                "name": "Emanuele Della Valle"
              },
              {
                "authorId": "1704595",
                "name": "P. Fraternali"
              },
              {
                "authorId": "1794305",
                "name": "S. Quarteroni"
              }
            ]
          }
        }
      ]
    },
    "266999133": {
      "citing_paper_info": {
        "title": "Semantic Similarity Matching for Patent Documents Using Ensemble BERT-related Model and Novel Text Processing Method",
        "abstract": "In the realm of patent document analysis, assessing semantic similarity between phrases presents a significant challenge, notably amplifying the inherent complexities of Cooperative Patent Classification (CPC) research. Firstly, this study addresses these challenges, recognizing early CPC work while acknowledging past struggles with language barriers and document intricacy. Secondly, it underscores the persisting difficulties of CPC research. To overcome these challenges and bolster the CPC system, This paper presents two key innovations. Firstly, it introduces an ensemble approach that incorporates four BERT-related models, enhancing semantic similarity accuracy through weighted averaging. Secondly, a novel text preprocessing method tailored for patent documents is introduced, featuring a distinctive input structure with token scoring that aids in capturing semantic relationships during CPC context training, utilizing BCELoss. Our experimental findings conclusively establish the effectiveness of both our Ensemble Model and novel text processing strategies when deployed on the U.S. Patent Phrase to Phrase Matching dataset.",
        "year": 2024,
        "venue": "arXiv.org",
        "authors": [
          {
            "authorId": "2269702502",
            "name": "Liqiang Yu"
          },
          {
            "authorId": "2263769868",
            "name": "Bo Liu"
          },
          {
            "authorId": "2268028100",
            "name": "Qunwei Lin"
          },
          {
            "authorId": "2268031101",
            "name": "Xinyu Zhao"
          },
          {
            "authorId": "2264122056",
            "name": "Change Che"
          }
        ]
      },
      "citation_summary": {
        "citation_count": 12,
        "unique_cited_count": 10,
        "influential_count": 0,
        "detailed_records_count": 12
      },
      "cited_papers": [
        "8419226",
        "52273103",
        "244346093",
        "245697948",
        "21663443",
        "222322720",
        "174799315",
        "13927959",
        "261685991",
        "17189853"
      ],
      "citation_details": [
        {
          "citedcorpusid": 8419226,
          "isinfluential": false,
          "contexts": [
            "While early publications by Lent et al. [1], Larkey [2], and Gey et al. [3] laid the foundation for CPC, they also exposed limitations related to language barriers, precision, and adapting to the complexity of patent documents."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Entry Vocabulary - a Technology to Enhance Digital Search",
            "abstract": "",
            "year": 2001,
            "venue": "Human Language Technology - The Baltic Perspectiv",
            "authors": [
              {
                "authorId": "1772996",
                "name": "F. Gey"
              },
              {
                "authorId": "1706786",
                "name": "M. Buckland"
              },
              {
                "authorId": "2346862",
                "name": "Aitao Chen"
              },
              {
                "authorId": "1803434",
                "name": "R. Larson"
              }
            ]
          }
        },
        {
          "citedcorpusid": 13927959,
          "isinfluential": false,
          "contexts": [
            "Lent et al. [1] explored text data trends, relevant to CPC’s patent document organization.",
            "While early publications by Lent et al. [1], Larkey [2], and Gey et al. [3] laid the foundation for CPC, they also exposed limitations related to language barriers, precision, and adapting to the complexity of patent documents."
          ],
          "intents": [
            "['background']",
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Discovering Trends in Text Databases",
            "abstract": "We address the problem of discovering trends in text databases. Trends can be used, for example, to discover that a company is shifting interests from one domain to another. We are given a database V of documents. Each document consists of one or more text fields and a timestamp. The unit of text is a word and a phrase is a list of words. (We defer the discussion of more complex structures till the “Methodology” secl-inn \\ Ao.aw.;,tc.rl ..r;th r...rh nhrano ;a s h;rtmw nf the YAVU., ~uu”~Icu”n,L& ““lull \\.uIUIA yuLCll”U I” Lo ,YYUY”~ y “I Yll” frequency of occurrence of the phrase, obtained by partitioning the documents based upon their timestamps. The frequency of occurrence in a particular time period is the number of documents that contain the phrase. (Other measures of frequency are possible, e.g. counting each occurrence of the phrase in a document.) A trend is a specific subsequence of the history of a phrase that satisfies the users’ query over the histories. For example, the user may specify a “spike” query to finds those phrases whose frequency of occurrence increased and then decreased.",
            "year": 1997,
            "venue": "Knowledge Discovery and Data Mining",
            "authors": [
              {
                "authorId": "2080339690",
                "name": "Brian Lent"
              },
              {
                "authorId": "144947410",
                "name": "R. Agrawal"
              },
              {
                "authorId": "34641476",
                "name": "R. Srikant"
              }
            ]
          }
        },
        {
          "citedcorpusid": 17189853,
          "isinfluential": false,
          "contexts": [
            "Al-Shboul and Myaeng [5] employ Wikipedia for effective query expansion but face limitations with specialized technical terms.",
            "Notably, Al-Shboul and Myaeng’s work [5] introduced ”Wikipedia-based query phrase expansion” to enhance CPC’s search precision and recall."
          ],
          "intents": [
            "['background']",
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Wikipedia-based query phrase expansion in patent class search",
            "abstract": "",
            "year": 2014,
            "venue": "Information retrieval (Boston)",
            "authors": [
              {
                "authorId": "1403736012",
                "name": "B. Al-Shboul"
              },
              {
                "authorId": "1754166",
                "name": "Sung-Hyon Myaeng"
              }
            ]
          }
        },
        {
          "citedcorpusid": 21663443,
          "isinfluential": false,
          "contexts": [
            "Shalaby et al. [7] introduced an innovative method using Long Short-Term Memory (LSTM) networks enhanced the accuracy of patent classification, offering greater adaptability to changing patent taxonomies and more efficient patent organization and retrieval.",
            "Shal-aby et al. [7] introduced LSTM, boosting patent classification accuracy and adaptability to changing taxonomies."
          ],
          "intents": [
            "['methodology']",
            "['background']"
          ],
          "cited_paper_info": {
            "title": "An LSTM Approach to Patent Classification based on Fixed Hierarchy Vectors",
            "abstract": "",
            "year": 2018,
            "venue": "SDM",
            "authors": [
              {
                "authorId": "2113210651",
                "name": "M. Shalaby"
              },
              {
                "authorId": "3264870",
                "name": "J. Stutzki"
              },
              {
                "authorId": "39403212",
                "name": "Matthias Schubert"
              },
              {
                "authorId": "3075189",
                "name": "Stephan Günnemann"
              }
            ]
          }
        },
        {
          "citedcorpusid": 52273103,
          "isinfluential": false,
          "contexts": [
            "Li et al. [8], in their deep learning approach, demonstrated improved classification accuracy but required extensive computational resources.",
            "Li et al.’s [8] ”DeepPatent” with convolutional neural networks and word embeddings contributes to evolving and refining CPC’s capabilities."
          ],
          "intents": [
            "['background']",
            "['background']"
          ],
          "cited_paper_info": {
            "title": "DeepPatent: patent classification with convolutional neural networks and word embedding",
            "abstract": "",
            "year": 2018,
            "venue": "Scientometrics",
            "authors": [
              {
                "authorId": "2124883266",
                "name": "Shaobo Li"
              },
              {
                "authorId": "145815844",
                "name": "Jie Hu"
              },
              {
                "authorId": "5925243",
                "name": "Yuxin Cui"
              },
              {
                "authorId": "50778791",
                "name": "Jianjun Hu"
              }
            ]
          }
        },
        {
          "citedcorpusid": 174799315,
          "isinfluential": false,
          "contexts": [
            "Lee and Hsiang fine-tuned a BERT model for patent classification in their pioneering work ”PatentBERT” [9].",
            "Moreover, the application of BERT-related techniques, as evidenced in the works by Lee and Hsiang [9], and Bekamiri et al. [10], significantly advanced CPC research by enhancing classification accuracy and efficiency."
          ],
          "intents": [
            "['methodology']",
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "PatentBERT: Patent Classification with Fine-Tuning a pre-trained BERT Model",
            "abstract": "In this work we focus on fine-tuning a pre-trained BERT model and applying it to patent classification. When applied to large datasets of over two millions patents, our approach outperforms the state of the art by an approach using CNN with word embeddings. In addition, we focus on patent claims without other parts in patent documents. Our contributions include: (1) a new state-of-the-art method based on pre-trained BERT model and fine-tuning for patent classification, (2) a large dataset USPTO-3M at the CPC subclass level with SQL statements that can be used by future researchers, (3) showing that patent claims alone are sufficient for classification task, in contrast to conventional wisdom.",
            "year": 2019,
            "venue": "arXiv.org",
            "authors": [
              {
                "authorId": "2387987",
                "name": "Jieh-Sheng Lee"
              },
              {
                "authorId": "1798127",
                "name": "J. Hsiang"
              }
            ]
          }
        },
        {
          "citedcorpusid": 222322720,
          "isinfluential": false,
          "contexts": [
            "…sequence length of 400, a learning rate of 110 − 5 , attention weight perturbation epsilon of 110 − 2 , an Adversarial Weight Perturbation (AWP) [20] learning rate of 110 − 4 , a maximum gradient norm of 1000, epsilon of 110 − 5 , and the utilization of Text Processing Strategy V3 with BCELoss…"
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Adversarial Weight Perturbation Helps Robust Generalization",
            "abstract": "The study on improving the robustness of deep neural networks against adversarial examples grows rapidly in recent years. Among them, adversarial training is the most promising one, which flattens the input loss landscape (loss change with respect to input) via training on adversarially perturbed examples. However, how the widely used weight loss landscape (loss change with respect to weight) performs in adversarial training is rarely explored. In this paper, we investigate the weight loss landscape from a new perspective, and identify a clear correlation between the flatness of weight loss landscape and robust generalization gap. Several well-recognized adversarial training improvements, such as early stopping, designing new objective functions, or leveraging unlabeled data, all implicitly flatten the weight loss landscape. Based on these observations, we propose a simple yet effective Adversarial Weight Perturbation (AWP) to explicitly regularize the flatness of weight loss landscape, forming a double-perturbation mechanism in the adversarial training framework that adversarially perturbs both inputs and weights. Extensive experiments demonstrate that AWP indeed brings flatter weight loss landscape and can be easily incorporated into various existing adversarial training methods to further boost their adversarial robustness.",
            "year": 2020,
            "venue": "Neural Information Processing Systems",
            "authors": [
              {
                "authorId": "1492154834",
                "name": "Dongxian Wu"
              },
              {
                "authorId": "3085483",
                "name": "Shutao Xia"
              },
              {
                "authorId": "1919541",
                "name": "Yisen Wang"
              }
            ]
          }
        },
        {
          "citedcorpusid": 244346093,
          "isinfluential": false,
          "contexts": [
            "…to the traditional methods mentioned earlier, the ensemble method leverages the strengths of multiple BERT-related models, including DeBERTaV3 [15] related models Microsoft’s DeBERTa-v3-large, Morit-zLaurer’s DeBERTa-v3-large-mnli-fever-anli-ling-wanli, An-ferico’s BERT for patents [16], and…"
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "DeBERTaV3: Improving DeBERTa using ELECTRA-Style Pre-Training with Gradient-Disentangled Embedding Sharing",
            "abstract": "This paper presents a new pre-trained language model, DeBERTaV3, which improves the original DeBERTa model by replacing mask language modeling (MLM) with replaced token detection (RTD), a more sample-efficient pre-training task. Our analysis shows that vanilla embedding sharing in ELECTRA hurts training efficiency and model performance. This is because the training losses of the discriminator and the generator pull token embeddings in different directions, creating the\"tug-of-war\"dynamics. We thus propose a new gradient-disentangled embedding sharing method that avoids the tug-of-war dynamics, improving both training efficiency and the quality of the pre-trained model. We have pre-trained DeBERTaV3 using the same settings as DeBERTa to demonstrate its exceptional performance on a wide range of downstream natural language understanding (NLU) tasks. Taking the GLUE benchmark with eight tasks as an example, the DeBERTaV3 Large model achieves a 91.37% average score, which is 1.37% over DeBERTa and 1.91% over ELECTRA, setting a new state-of-the-art (SOTA) among the models with a similar structure. Furthermore, we have pre-trained a multi-lingual model mDeBERTa and observed a larger improvement over strong baselines compared to English models. For example, the mDeBERTa Base achieves a 79.8% zero-shot cross-lingual accuracy on XNLI and a 3.6% improvement over XLM-R Base, creating a new SOTA on this benchmark. We have made our pre-trained models and inference code publicly available at https://github.com/microsoft/DeBERTa.",
            "year": 2021,
            "venue": "International Conference on Learning Representations",
            "authors": [
              {
                "authorId": "50462546",
                "name": "Pengcheng He"
              },
              {
                "authorId": "48441311",
                "name": "Jianfeng Gao"
              },
              {
                "authorId": "2109136147",
                "name": "Weizhu Chen"
              }
            ]
          }
        },
        {
          "citedcorpusid": 245697948,
          "isinfluential": false,
          "contexts": [
            "Hoshino et al. [13] explore IPC prediction using neural networks and CPC’s IPC classification.",
            "Hoshino et al. [13] investigate IPC prediction using neural networks and CPC’s IPC classification for patent document content."
          ],
          "intents": [
            "['methodology']",
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "IPC prediction of patent documents using neural network with attention for hierarchical structure",
            "abstract": "International patent classifications (IPCs) are assigned to patent documents; however, since the procedure for assigning classifications is manually done by the patent examiner, it takes a lot of time and effort to select some IPCs from about 70,000 IPCs. Hence, some research has been conducted on patent classification with machine learning. However, patent documents are very voluminous, and learning with all the claims (the part describing the content of the patent) as input would run out of the necessary memory, even if the batch size is set to a very small size. Therefore, most of the existing methods learn by excluding some information, such as using only the first claim as input. In this study, we propose a model that considers the contents of all claims by extracting important information for input. In addition, we focus on the hierarchical structure of the IPC, and propose a new decoder architecture to consider it. Finally, we conducted an experiment using actual patent data to verify the accuracy of the prediction. The results showed a significant improvement in accuracy compared to existing methods, and the actual applicability of the method was also discussed.",
            "year": 2022,
            "venue": "PLoS ONE",
            "authors": [
              {
                "authorId": "1762790",
                "name": "Y. Hoshino"
              },
              {
                "authorId": "2039655709",
                "name": "Yoshimasa Utsumi"
              },
              {
                "authorId": "2053376345",
                "name": "Yoshirō Matsuda"
              },
              {
                "authorId": "10768752",
                "name": "Yoshitoshi Tanaka"
              },
              {
                "authorId": "145778306",
                "name": "K. Nakata"
              }
            ]
          }
        },
        {
          "citedcorpusid": 261685991,
          "isinfluential": false,
          "contexts": [
            "We rigorously evaluated our model’s performance and generalization with a 4-fold Cross-Validation [19] approach, maintaining label balance using MultiLabelStratifiedKFold."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Cross-Validation Methods.",
            "abstract": "This paper gives a review of cross-validation methods. The original applications in multiple linear regression are considered first. It is shown how predictive accuracy depends on sample size and the number of predictor variables. Both two-sample and single-sample cross-validation indices are investigated. The application of cross-validation methods to the analysis of moment structures is then justified. An equivalence of a single-sample cross-validation index and the Akaike information criterion is pointed out. It is seen that the optimal number of parameters suggested by both single-sample and two-sample cross-validation indices will depend on sample size. Copyright 2000 Academic Press.",
            "year": 2000,
            "venue": "Journal of Mathematical Psychology",
            "authors": [
              {
                "authorId": "2053216705",
                "name": "Michael W. Browne"
              }
            ]
          }
        },
        {
          "citedcorpusid": null,
          "isinfluential": false,
          "contexts": [
            "Moreover, the application of BERT-related techniques, as evidenced in the works by Lee and Hsiang [9], and Bekamiri et al. [10], significantly advanced CPC research by enhancing classification accuracy and efficiency."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {}
        },
        {
          "citedcorpusid": null,
          "isinfluential": false,
          "contexts": [
            "…earlier, the ensemble method leverages the strengths of multiple BERT-related models, including DeBERTaV3 [15] related models Microsoft’s DeBERTa-v3-large, Morit-zLaurer’s DeBERTa-v3-large-mnli-fever-anli-ling-wanli, An-ferico’s BERT for patents [16], and Google’s ELECTRA-large-discriminator [17]."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {}
        }
      ]
    },
    "522219": {
      "citing_paper_info": {
        "title": "Supervised Approaches to Assign Cooperative Patent Classification (CPC) Codes to Patents",
        "abstract": "",
        "year": 2017,
        "venue": "International Conference on Mining Intelligence and Knowledge Exploration",
        "authors": [
          {
            "authorId": "144828812",
            "name": "Tung Tran"
          },
          {
            "authorId": "1711213",
            "name": "Ramakanth Kavuluru"
          }
        ]
      },
      "citation_summary": {
        "citation_count": 3,
        "unique_cited_count": 3,
        "influential_count": 1,
        "detailed_records_count": 3
      },
      "cited_papers": [
        "313952",
        "154444149",
        "205438988"
      ],
      "citation_details": [
        {
          "citedcorpusid": 313952,
          "isinfluential": false,
          "contexts": [
            "[4] explored the task of patent classification for IPC using various supervised algorithms such as support vector machines (SVM), näıve Bayes, and k-nearest neighbors (k-NN).",
            "This choice is motivated by the fact that labels at the subclass level are fairly static moving forward while group and subgroup labels are more likely to undergo revision with each update [4] of PCS system."
          ],
          "intents": [
            "['methodology']",
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Automated categorization in the international patent classification",
            "abstract": "",
            "year": 2003,
            "venue": "SIGF",
            "authors": [
              {
                "authorId": "8635679",
                "name": "C. Fall"
              },
              {
                "authorId": "2893823",
                "name": "A. Törcsvári"
              },
              {
                "authorId": "2217341",
                "name": "K. Benzineb"
              },
              {
                "authorId": "2570524",
                "name": "G. Karetka"
              }
            ]
          }
        },
        {
          "citedcorpusid": 154444149,
          "isinfluential": false,
          "contexts": [
            "[20], the claims of a patent can actually be considered a collection of separate inventions; together they can be used to determine the true measure of a patent."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Measuring national technological performance with patent claims data",
            "abstract": "",
            "year": 1994,
            "venue": "",
            "authors": [
              {
                "authorId": "30791863",
                "name": "X. Tong"
              },
              {
                "authorId": "2246440920",
                "name": "J.Davidson Frame"
              }
            ]
          }
        },
        {
          "citedcorpusid": 205438988,
          "isinfluential": true,
          "contexts": [
            "The USPC has been the official PCS used and maintained by the USPTO since the first patent was issued.",
            "Liu and Shih [11] proposed a hybrid system for USPC classification using patent network analysis in addition to traditional content-based features.",
            "Technology class assignments are available for each of the three PCSs: the U.S. Patent Classification (USPC) system, the Cooperative Patent Classification (CPC) system, and the International Patent Classification (IPC) system.",
            "Automatic patent classification in the literature has primarily focused on IPC [2–4] or USPC [10, 11], and we observe k-NN to be a popular approach for many proposed systems [8, 11, 14]."
          ],
          "intents": [
            "--",
            "['background']",
            "--",
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Hybrid-patent classification based on patent-network analysis",
            "abstract": "Effective patent management is essential for organizations to maintain their competitive advantage. The classification of patents is a critical part of patent management and industrial analysis. This study proposes a hybrid-patent-classification approach that combines a novel patent-network-based classification method with three conventional classification methods to analyze query patents and predict their classes. The novel patent network contains various types of nodes that represent different features extracted from patent documents. The nodes are connected based on the relationship metrics derived from the patent metadata. The proposed classification method predicts a query patent's class by analyzing all reachable nodes in the patent network and calculating their relevance to the query patent. It then classifies the query patent with a modified k-nearest neighbor classifier. To further improve the approach, we combine it with content-based, citation-based, and metadata-based classification methods to develop a hybrid-classification approach. We evaluate the performance of the hybrid approach on a test dataset of patent documents obtained from the U.S. Patent and Trademark Office, and compare its performance with that of the three conventional methods. The results demonstrate that the proposed patent-network-based approach yields more accurate class predictions than the patent network-based approach.",
            "year": 2011,
            "venue": "J. Assoc. Inf. Sci. Technol.",
            "authors": [
              {
                "authorId": "1741839",
                "name": "Duen-Ren Liu"
              },
              {
                "authorId": "2407700",
                "name": "Meng-Jung Shih"
              }
            ]
          }
        }
      ]
    },
    "32869670": {
      "citing_paper_info": {
        "title": "Product functional information based automatic patent classification: Method and experimental studies",
        "abstract": "",
        "year": 2017,
        "venue": "Information Systems",
        "authors": [
          {
            "authorId": "48625097",
            "name": "Wenqiang Li"
          },
          {
            "authorId": "2152884327",
            "name": "Yan Li"
          },
          {
            "authorId": "2118446046",
            "name": "Jian Chen"
          },
          {
            "authorId": "10721049",
            "name": "Chao-yi Hou"
          }
        ]
      },
      "citation_summary": {
        "citation_count": 11,
        "unique_cited_count": 3,
        "influential_count": 0,
        "detailed_records_count": 11
      },
      "cited_papers": [
        "37779608",
        "34991896",
        "32800624"
      ],
      "citation_details": [
        {
          "citedcorpusid": 32800624,
          "isinfluential": false,
          "contexts": [
            "Many methods can be used for the training and classification of data [33]."
          ],
          "intents": [
            "--"
          ],
          "cited_paper_info": {
            "title": "Naive (Bayes) at Forty: The Independence Assumption in Information Retrieval",
            "abstract": "",
            "year": 1998,
            "venue": "European Conference on Machine Learning",
            "authors": [
              {
                "authorId": "35153517",
                "name": "D. Lewis"
              }
            ]
          }
        },
        {
          "citedcorpusid": 34991896,
          "isinfluential": false,
          "contexts": [
            "The FBSE (function - behavior - structure constraints) design model [20] defines the design process as a mapping among the function domain, behavior domain, structure domain and constraint domain."
          ],
          "intents": [
            "--"
          ],
          "cited_paper_info": {
            "title": "Function–behavior–structure paths and their role in analogy-based design",
            "abstract": "",
            "year": 1996,
            "venue": "Artificial intelligence for engineering design, analysis and manufacturing",
            "authors": [
              {
                "authorId": "3160466",
                "name": "L. Qian"
              },
              {
                "authorId": "145922843",
                "name": "J. Gero"
              }
            ]
          }
        },
        {
          "citedcorpusid": 37779608,
          "isinfluential": false,
          "contexts": [
            "Because the patent text contains a lot of information, to select which information as the basis of the patent mining and classification has been recently studied by some researchers [25, 26, 27]."
          ],
          "intents": [
            "--"
          ],
          "cited_paper_info": {
            "title": "Grouping of TRIZ Inventive Principles to facilitate automatic patent classification",
            "abstract": "",
            "year": 2008,
            "venue": "Expert systems with applications",
            "authors": [
              {
                "authorId": "2115065574",
                "name": "Cong He"
              },
              {
                "authorId": "2935248",
                "name": "H. Loh"
              }
            ]
          }
        },
        {
          "citedcorpusid": null,
          "isinfluential": false,
          "contexts": [
            "Hodes et al [2] proposed a patent analysis method by applying the ontology technology."
          ],
          "intents": [
            "--"
          ],
          "cited_paper_info": {}
        },
        {
          "citedcorpusid": null,
          "isinfluential": false,
          "contexts": [
            "Liu et al [7] proposed a product maturity prediction model by mining the technology contents from patent texts.",
            "Based on TRIZ theory, Liu et al [10] studied the scientific effects in patents."
          ],
          "intents": [
            "--",
            "--"
          ],
          "cited_paper_info": {}
        },
        {
          "citedcorpusid": null,
          "isinfluential": false,
          "contexts": [
            "Xiao et al [5] developed a Patent Map method that can analyze the patent information and summarize the distribution and trends of technologies based on the rearrangement of different priorities of the patent collection."
          ],
          "intents": [
            "--"
          ],
          "cited_paper_info": {}
        },
        {
          "citedcorpusid": null,
          "isinfluential": false,
          "contexts": [
            "Reference [1] Guo Jieshen and Xiao Guohua.",
            "According to the investigation from the World Intellectual Property Organization, 90%-95% of inventions in the world are reported by patents and 80% of them are not recorded by other texts [1]."
          ],
          "intents": [
            "--",
            "--"
          ],
          "cited_paper_info": {}
        },
        {
          "citedcorpusid": null,
          "isinfluential": false,
          "contexts": [
            "Cascini et al [8-9] developed a Subject-Action-Object ternary structure model to describe the technical features of patent text and it can provide the corresponding patent knowledge support for the TRIZ’s implement process.",
            "[9] Cascini G, Fantechi A, Spinicci E."
          ],
          "intents": [
            "--",
            "--"
          ],
          "cited_paper_info": {}
        },
        {
          "citedcorpusid": null,
          "isinfluential": false,
          "contexts": [
            "Boyko et al [13] analyzed the patent texts and extract structure information from patents by using the ontology and natural language processing techniques."
          ],
          "intents": [
            "--"
          ],
          "cited_paper_info": {}
        },
        {
          "citedcorpusid": null,
          "isinfluential": false,
          "contexts": [
            "Figure 2 Product design process and design elements Moreover, because of the different description methods on the features of products, the existing function body models are normally established in regard to different areas and design goals [21-22]."
          ],
          "intents": [
            "--"
          ],
          "cited_paper_info": {}
        },
        {
          "citedcorpusid": null,
          "isinfluential": false,
          "contexts": [
            "Xue et al [14] put forward a patent automatic categorization method to support the innovation design process, but the method has no formalized and standard classification criteria."
          ],
          "intents": [
            "--"
          ],
          "cited_paper_info": {}
        }
      ]
    },
    "5937747": {
      "citing_paper_info": {
        "title": "Automatic patent classification by a three-phase model with document frequency matrix and boosted tree",
        "abstract": "",
        "year": 2016,
        "venue": "International Conference on Electronic Devices, Systems and Applications",
        "authors": [
          {
            "authorId": "38396228",
            "name": "F. Shamsi"
          },
          {
            "authorId": "1797305",
            "name": "Z. Aung"
          }
        ]
      },
      "citation_summary": {
        "citation_count": 4,
        "unique_cited_count": 4,
        "influential_count": 0,
        "detailed_records_count": 4
      },
      "cited_papers": [
        "2600314",
        "44339554",
        "8486034",
        "52262525"
      ],
      "citation_details": [
        {
          "citedcorpusid": 2600314,
          "isinfluential": false,
          "contexts": [
            "The main functionality is to achieve a maximum correlation of new learners with the negative gradient of the loss function based on earlier iterations of the learning scheme [3]."
          ],
          "intents": [
            "--"
          ],
          "cited_paper_info": {
            "title": "Building diverse and optimized ensembles of gradient boosted trees for high-dimensional data",
            "abstract": "",
            "year": 2014,
            "venue": "2014 IEEE 3rd International Conference on Cloud Computing and Intelligence Systems",
            "authors": [
              {
                "authorId": "2804727",
                "name": "T. Abdunabi"
              },
              {
                "authorId": "144721342",
                "name": "O. Basir"
              }
            ]
          }
        },
        {
          "citedcorpusid": 8486034,
          "isinfluential": false,
          "contexts": [
            "Another recent scenario is whereby manufacturers are charged of violating intellectual property by their competitors which prevents new products from entering the global market [1]."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "A patent quality analysis for innovative technology and product development",
            "abstract": "",
            "year": 2012,
            "venue": "Advanced Engineering Informatics",
            "authors": [
              {
                "authorId": "1761458",
                "name": "A. Trappey"
              },
              {
                "authorId": "1766308",
                "name": "C. Trappey"
              },
              {
                "authorId": "3358489",
                "name": "Chun-Yi Wu"
              },
              {
                "authorId": "2143376116",
                "name": "Chi-Wei Lin"
              }
            ]
          }
        },
        {
          "citedcorpusid": 44339554,
          "isinfluential": false,
          "contexts": [
            "Nevertheless, a common disadvantage of applying this type of basic text classiﬁers is that they generally view patents as text without using the patents’ hierarchical structural properties to enhance classiﬁ-cation accuracy [9]."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Automated categorization of German-language patent documents",
            "abstract": "",
            "year": 2004,
            "venue": "Expert systems with applications",
            "authors": [
              {
                "authorId": "8635679",
                "name": "C. Fall"
              },
              {
                "authorId": "2893823",
                "name": "A. Törcsvári"
              },
              {
                "authorId": "50612388",
                "name": "P. Fiévet"
              },
              {
                "authorId": "2570524",
                "name": "G. Karetka"
              }
            ]
          }
        },
        {
          "citedcorpusid": 52262525,
          "isinfluential": false,
          "contexts": [
            "The levels of classification represent an index containing classes, subclasses, groups, and subgroups [7]."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Biomedical Engineering in International Patent Classification",
            "abstract": "",
            "year": 2013,
            "venue": "BioMed",
            "authors": [
              {
                "authorId": "14078630",
                "name": "N. V. Alisova"
              }
            ]
          }
        }
      ]
    },
    "210875699": {
      "citing_paper_info": {
        "title": "Patent Automatic Classification Based on Symmetric Hierarchical Convolution Neural Network",
        "abstract": "With the rapid growth of patent applications, it has become an urgent problem to automatically classify the accepted patent application documents accurately and quickly. Most previous patent automatic classification studies are based on feature engineering and traditional machine learning methods like SVM, and some even rely on the knowledge of domain experts, hence they suffer from low accuracy problem and have poor generalization ability. In this paper, we propose a patent automatic classification method via the symmetric hierarchical convolution neural network (CNN) named PAC-HCNN. We use the title and abstract of the patent as the input data, and then apply the word embedding technique to segment and vectorize the input data. Then we design a symmetric hierarchical CNN framework to classify the patents based on the word embeddings, which is much more efficient than traditional RNN models dealing with texts, meanwhile keeping the history and future information of the input sequence. We also add gated linear units (GLUs) and residual connection to help realize the deep CNN. Additionally, we equip our model with a self attention mechanism to address the long-term dependency problem. Experiments are performed on large-scale datasets for Chinese short text patent classification. Experimental results prove our proposed model’s effectiveness, and it performs better than other state-of-the-art models significantly and consistently on both fine-grained and coarse-grained classification.",
        "year": 2020,
        "venue": "Symmetry",
        "authors": [
          {
            "authorId": "2115718616",
            "name": "Huiming Zhu"
          },
          {
            "authorId": "1978348101",
            "name": "Chunhui He"
          },
          {
            "authorId": "1390839515",
            "name": "Yang Fang"
          },
          {
            "authorId": "49687214",
            "name": "Bin Ge"
          },
          {
            "authorId": "2141226022",
            "name": "Meng Xing"
          },
          {
            "authorId": "153215984",
            "name": "W. Xiao"
          }
        ]
      },
      "citation_summary": {
        "citation_count": 10,
        "unique_cited_count": 9,
        "influential_count": 0,
        "detailed_records_count": 10
      },
      "cited_papers": [
        "29763249",
        "214134213",
        "3648736",
        "5591459",
        "34404946",
        "5909155",
        "9672033",
        "10659969",
        "17028914"
      ],
      "citation_details": [
        {
          "citedcorpusid": 3648736,
          "isinfluential": false,
          "contexts": [
            "Through stacking CNN layers symmetrically, hierarchical representations on the input are created, that is, lower layers will process close words in sequences while higher layers process distant words [9], and using symmetric structure enables the proposed model to preserve both the history and future information."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Convolutional Sequence to Sequence Learning",
            "abstract": "The prevalent approach to sequence to sequence learning maps an input sequence to a variable length output sequence via recurrent neural networks. We introduce an architecture based entirely on convolutional neural networks. Compared to recurrent models, computations over all elements can be fully parallelized during training and optimization is easier since the number of non-linearities is fixed and independent of the input length. Our use of gated linear units eases gradient propagation and we equip each decoder layer with a separate attention module. We outperform the accuracy of the deep LSTM setup of Wu et al. (2016) on both WMT'14 English-German and WMT'14 English-French translation at an order of magnitude faster speed, both on GPU and CPU.",
            "year": 2017,
            "venue": "International Conference on Machine Learning",
            "authors": [
              {
                "authorId": "2401865",
                "name": "Jonas Gehring"
              },
              {
                "authorId": "2325985",
                "name": "Michael Auli"
              },
              {
                "authorId": "2529182",
                "name": "David Grangier"
              },
              {
                "authorId": "13759615",
                "name": "Denis Yarats"
              },
              {
                "authorId": "2921469",
                "name": "Yann Dauphin"
              }
            ]
          }
        },
        {
          "citedcorpusid": 5591459,
          "isinfluential": false,
          "contexts": [
            "Considering that short text often encounters data sparsity and fuzziness problem in representation due to the lack of context, a short text classification method was proposed combining semantic clustering and CNN [20]."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Sequential Short-Text Classification with Recurrent and Convolutional Neural Networks",
            "abstract": "Recent approaches based on artificial neural networks (ANNs) have shown promising results for short-text classification. However, many short texts occur in sequences (e.g., sentences in a document or utterances in a dialog), and most existing ANN-based systems do not leverage the preceding short texts when classifying a subsequent one. In this work, we present a model based on recurrent neural networks and convolutional neural networks that incorporates the preceding short texts. Our model achieves state-of-the-art results on three different datasets for dialog act prediction.",
            "year": 2016,
            "venue": "North American Chapter of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "2108574824",
                "name": "Ji Young Lee"
              },
              {
                "authorId": "2462276",
                "name": "Franck Dernoncourt"
              }
            ]
          }
        },
        {
          "citedcorpusid": 5909155,
          "isinfluential": false,
          "contexts": [
            "com/fxsjy/ jieba) to segment the text data, and then apply the gensim [8] framework to project the words into low-dimensional vectors.",
            "In the experiments, the open source gensim [8] word vector training tool is used to pre-train all the experimental datasets, obtaining the corresponding embeddings."
          ],
          "intents": [
            "['methodology']",
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Text comparison using word vector representations and dimensionality reduction",
            "abstract": "This paper describes a technique to compare large text sources using word vector representations (word2vec) and dimensionality reduction (t-SNE) and how it can be implemented using Python. The technique provides a bird's-eye view of text sources, e.g. text summaries and their source material, and enables users to explore text sources like a geographical map. Word vector representations capture many linguistic properties such as gender, tense, plurality and even semantic concepts like \"capital city of\". Using dimensionality reduction, a 2D map can be computed where semantically similar words are close to each other. The technique uses the word2vec model from the gensim Python library and t-SNE from scikit-learn.",
            "year": 2016,
            "venue": "arXiv.org",
            "authors": [
              {
                "authorId": "34698604",
                "name": "Hendrik Heuer"
              }
            ]
          }
        },
        {
          "citedcorpusid": 9672033,
          "isinfluential": false,
          "contexts": [
            "CNN model was used to classify short text sentences [19]."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Convolutional Neural Networks for Sentence Classification",
            "abstract": "We report on a series of experiments with convolutional neural networks (CNN) trained on top of pre-trained word vectors for sentence-level classification tasks. We show that a simple CNN with little hyperparameter tuning and static vectors achieves excellent results on multiple benchmarks. Learning task-specific vectors through fine-tuning offers further gains in performance. We additionally propose a simple modification to the architecture to allow for the use of both task-specific and static vectors. The CNN models discussed herein improve upon the state of the art on 4 out of 7 tasks, which include sentiment analysis and question classification.",
            "year": 2014,
            "venue": "Conference on Empirical Methods in Natural Language Processing",
            "authors": [
              {
                "authorId": "38367242",
                "name": "Yoon Kim"
              }
            ]
          }
        },
        {
          "citedcorpusid": 10659969,
          "isinfluential": false,
          "contexts": [
            "Except for the PAC-HCNN, Fasttext and GRU, the other models are implemented using the corresponding algorithms in the Scikit-learn [26] toolkit."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Scikit-learn: Machine Learning in Python",
            "abstract": "Scikit-learn is a Python module integrating a wide range of state-of-the-art machine learning algorithms for medium-scale supervised and unsupervised problems. This package focuses on bringing machine learning to non-specialists using a general-purpose high-level language. Emphasis is put on ease of use, performance, documentation, and API consistency. It has minimal dependencies and is distributed under the simplified BSD license, encouraging its use in both academic and commercial settings. Source code, binaries, and documentation can be downloaded from http://scikit-learn.sourceforge.net.",
            "year": 2011,
            "venue": "Journal of machine learning research",
            "authors": [
              {
                "authorId": "2570016",
                "name": "Fabian Pedregosa"
              },
              {
                "authorId": "3025780",
                "name": "G. Varoquaux"
              },
              {
                "authorId": "1797840",
                "name": "Alexandre Gramfort"
              },
              {
                "authorId": "52200573",
                "name": "V. Michel"
              },
              {
                "authorId": "8493461",
                "name": "B. Thirion"
              },
              {
                "authorId": "2958756",
                "name": "O. Grisel"
              },
              {
                "authorId": "27257992",
                "name": "Mathieu Blondel"
              },
              {
                "authorId": "1881041",
                "name": "Gilles Louppe"
              },
              {
                "authorId": "2780213",
                "name": "P. Prettenhofer"
              },
              {
                "authorId": "2067827437",
                "name": "Ron Weiss"
              },
              {
                "authorId": "39571582",
                "name": "Ron J. Weiss"
              },
              {
                "authorId": "2081469",
                "name": "J. Vanderplas"
              },
              {
                "authorId": "144720379",
                "name": "Alexandre Passos"
              },
              {
                "authorId": "3084321",
                "name": "D. Cournapeau"
              },
              {
                "authorId": "2423884",
                "name": "M. Brucher"
              },
              {
                "authorId": "35243423",
                "name": "M. Perrot"
              },
              {
                "authorId": "1710398",
                "name": "E. Duchesnay"
              }
            ]
          }
        },
        {
          "citedcorpusid": 17028914,
          "isinfluential": false,
          "contexts": [
            "and more balanced sub-problems, and min-max modular support vector machine is then used to handle the subproblems [17]."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Large-scale patent classification with min-max modular support vector machines",
            "abstract": "",
            "year": 2008,
            "venue": "IEEE World Congress on Computational Intelligence",
            "authors": [
              {
                "authorId": "2962167",
                "name": "Xiao-Lei Chu"
              },
              {
                "authorId": "46658056",
                "name": "Chao Ma"
              },
              {
                "authorId": "2153124120",
                "name": "Jing Li"
              },
              {
                "authorId": "1715839",
                "name": "Bao-Liang Lu"
              },
              {
                "authorId": "1802277",
                "name": "M. Utiyama"
              },
              {
                "authorId": "1714134",
                "name": "H. Isahara"
              }
            ]
          }
        },
        {
          "citedcorpusid": 29763249,
          "isinfluential": false,
          "contexts": [
            ", the authors of [4] developed an automatic patent quality analysis and classification system SOM-KPCA-SVM, using data mining methods to identify and classify the quality of new patents in a timely manner."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "A patent quality analysis and classification system using self-organizing maps with support vector machine",
            "abstract": "",
            "year": 2016,
            "venue": "Applied Soft Computing",
            "authors": [
              {
                "authorId": "2719770",
                "name": "Jheng-Long Wu"
              },
              {
                "authorId": "1717197",
                "name": "P. Chang"
              },
              {
                "authorId": "2707223",
                "name": "Cheng-Chin Tsao"
              },
              {
                "authorId": "1726734",
                "name": "C. Fan"
              }
            ]
          }
        },
        {
          "citedcorpusid": 34404946,
          "isinfluential": false,
          "contexts": [
            "Additionally, in our CNN based framework, we have fixed non-linearities and kernel numbers, while for RNN model, n operations are needed to process the first words, and a single set of operations are needed for the last word [24]."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Deep keyphrase generation with a convolutional sequence to sequence model",
            "abstract": "",
            "year": 2017,
            "venue": "International Conference on Systems and Informatics",
            "authors": [
              {
                "authorId": "2144287885",
                "name": "Yong Zhang"
              },
              {
                "authorId": "1390839515",
                "name": "Yang Fang"
              },
              {
                "authorId": "153215984",
                "name": "W. Xiao"
              }
            ]
          }
        },
        {
          "citedcorpusid": 214134213,
          "isinfluential": false,
          "contexts": [
            "However, those CNN based models like Fasttext can not effectively comprehend the text content, comparing with recurrent neural network (RNN) that processes a text word by word [22]."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Kollector: Detecting Fraudulent Activities on Mobile Devices Using Deep Learning",
            "abstract": "With the rapid growth in smartphone usage, preventing leakage of personal information and privacy has become a challenging task. One major consequence of such leakage is impersonation. This type of illegal usage is nearly impossible to prevent as existing preventive mechanisms (e.g., passcode and fingerprinting), are not capable of continuously monitoring usage and determining whether the user is authorized. Once unauthorized users can defeat the initial protection mechanisms, they would have full access to the devices including using stored passwords to access high-value websites. We present Kollector, a new framework to detect impersonation based on a multi-view bagging deep learning approach to capture sequential tapping information on the smart-phone's keyboard. We construct a sequential-tapping biometrics model to continuously authenticate the user while typing. We empirically evaluated our system using real-world phone usage sessions from 26 users over eight weeks. We then compared our model against commonly used shallow machine techniques and find that our system performs better than other approaches and can achieve an 8.42 percent equal error rate, a 94.24 percent accuracy and a 94.41 percent H-mean using only the accelerometer and only five keyboard taps. We also experiment with using only three keyboard taps and find that the system still yields high accuracy while giving additional opportunities to make more decisions that can result in more accurate final decisions.",
            "year": 2020,
            "venue": "IEEE Transactions on Mobile Computing",
            "authors": [
              {
                "authorId": "49755259",
                "name": "Lichao Sun"
              },
              {
                "authorId": "1678185",
                "name": "Bokai Cao"
              },
              {
                "authorId": "2110218064",
                "name": "Ji Wang"
              },
              {
                "authorId": "95208700",
                "name": "W. Srisa-an"
              },
              {
                "authorId": "144019071",
                "name": "Philip S. Yu"
              },
              {
                "authorId": "2967326",
                "name": "A. Leow"
              },
              {
                "authorId": "2642533",
                "name": "Stephen Checkoway"
              }
            ]
          }
        },
        {
          "citedcorpusid": null,
          "isinfluential": false,
          "contexts": [
            "Because of the complex and time-consuming process of text categorization, Facebook’s open-source sentence categorization method and word feature learning model were proposed, Fasttext, on the field of Chinese text categorization and verified its effectiveness [21]."
          ],
          "intents": [
            "--"
          ],
          "cited_paper_info": {}
        }
      ]
    },
    "35058596": {
      "citing_paper_info": {
        "title": "Interactive patent classification based on multi-classifier fusion and active learning",
        "abstract": "",
        "year": 2014,
        "venue": "Neurocomputing",
        "authors": [
          {
            "authorId": "2141934368",
            "name": "Xiaoyu Zhang"
          }
        ]
      },
      "citation_summary": {
        "citation_count": 6,
        "unique_cited_count": 6,
        "influential_count": 0,
        "detailed_records_count": 6
      },
      "cited_papers": [
        "154037572",
        "215966761",
        "7489485",
        "16025939",
        "64641472",
        "1479461"
      ],
      "citation_details": [
        {
          "citedcorpusid": 1479461,
          "isinfluential": false,
          "contexts": [
            "The key to active learning is selective sampling, which is traditionally conducted in batch mode with the patents selected and labeled in a batch [15,16]."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Incorporating Diversity in Active Learning with Support Vector Machines",
            "abstract": "In many real world applications, active selection of training examples can significantly reduce the number of labelled training examples to learn a classification function. Different strategies in the field of support vector machines have been proposed that iteratively select a single new example from a set of unlabelled examples, query the corresponding class label and then perform retraining of the current classifier. However, to reduce computational time for training, it might be necessary to select batches of new training examples instead of single examples. Strategies for single examples can be extended straightforwardly to select batches by choosing the h > 1 examples that get the highest values for the individual selection criterion. We present a new approach that is especially designed to construct batches and incorporates a diversity measure. It has low computational requirements making it feasible for large scale problems with several thousands of examples. Experimental results indicate that this approach provides a faster method to attain a level of generalization accuracy in terms of the number of labelled examples.",
            "year": 2003,
            "venue": "International Conference on Machine Learning",
            "authors": [
              {
                "authorId": "2047100972",
                "name": "K. Brinker"
              }
            ]
          }
        },
        {
          "citedcorpusid": 7489485,
          "isinfluential": false,
          "contexts": [
            "To address this problem, a novel dynamic batch selective sampling mode is proposed [19,20], which selects the patents one after another, using the information derived from the previously labeled patent to guide further selection."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Weighted Co-SVM for Image Retrieval with MVB Strategy",
            "abstract": "",
            "year": 2007,
            "venue": "2007 IEEE International Conference on Image Processing",
            "authors": [
              {
                "authorId": "120069940",
                "name": "Xiaoyu Zhang"
              },
              {
                "authorId": "143949499",
                "name": "Jian Cheng"
              },
              {
                "authorId": "1694235",
                "name": "Hanqing Lu"
              },
              {
                "authorId": "38450168",
                "name": "Songde Ma"
              }
            ]
          }
        },
        {
          "citedcorpusid": 16025939,
          "isinfluential": false,
          "contexts": [
            "Unfortunately, batch selective sampling is not effective enough for its neglect of patents' correlation [17,18]."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "The Value of Unlabeled Data for Classification Problems",
            "abstract": "Recently, there has been increasing interest in using unlabeled data for classi(cid:12)cation. However, whether these unlabeled data are truly useful is still under debate. In order to have a better understanding of relevant issues, it is worthwhile to precisely formulate the problem and carefully analyze the value of unlabeled data under certain learning models. In this paper, we approach this problem from the statistical point of view, where we assume that a correct model of the underlying distribution is given. We demonstrate that Fisher information matrices can be used to judge the asymptotic value of unlabeled data. We apply this methodology to both \\passive partially supervised learning\" and \\active learning\", and draw conclusions from this analysis. Experiments will be provided to support our claims.",
            "year": 2000,
            "venue": "International Conference on Machine Learning",
            "authors": [
              {
                "authorId": "2117881943",
                "name": "Tong Zhang"
              }
            ]
          }
        },
        {
          "citedcorpusid": 64641472,
          "isinfluential": false,
          "contexts": [
            "Machine learning is the computational process of extracting patterns in data and making predictions based on experience learned from these patterns [5,6]."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Data Mining Practical Machine Learning Tools and Techniques",
            "abstract": "",
            "year": 2014,
            "venue": "",
            "authors": [
              {
                "authorId": "71619391",
                "name": "อนิรุธ สืบสิงห์"
              }
            ]
          }
        },
        {
          "citedcorpusid": 154037572,
          "isinfluential": false,
          "contexts": [
            "The analysis of patents is a widely used method to discover inventive activity and output over different fields, regions, and time, and reveal trends in science and technology [2,3]."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "A Penny for Your Quotes : Patent Citations and the Value of Innovations",
            "abstract": "The use of patents in economic research has been seriously hindered by the fact that patents vary enormously in their importance or value, and hence, simple patent counts cannot be informative about innovative output. The purpose of this article is to put forward patent counts weighted by citations as indicators of the value of innovations, thereby overcoming the limitations of simple counts. The empirical analysis of a particular innovation (Computed Tomography scanners) indeed shows a close association between citation-based patent indices and independent measures of the social value of innovations in that field. Moreover, the weighting scheme appears to be nonlinear (increasing) in the number of citations, implying that the informational content of citations rises at the margin. As in previous studies, simple patent counts are found to be highly correlated with contemporaneous RD however, here the association is within a field over time rather than cross-sectional.",
            "year": 1990,
            "venue": "",
            "authors": [
              {
                "authorId": "6000281",
                "name": "M. Trajtenberg"
              }
            ]
          }
        },
        {
          "citedcorpusid": 215966761,
          "isinfluential": false,
          "contexts": [
            "Support vector machine is adopted for classifier construction [12,13]."
          ],
          "intents": [
            "--"
          ],
          "cited_paper_info": {
            "title": "A Tutorial on Support Vector Machines for Pattern Recognition",
            "abstract": "",
            "year": 1998,
            "venue": "Data mining and knowledge discovery",
            "authors": [
              {
                "authorId": "2676309",
                "name": "C. Burges"
              }
            ]
          }
        }
      ]
    },
    "218592387": {
      "citing_paper_info": {
        "title": "Text Representations for Patent Classification",
        "abstract": "",
        "year": 2013,
        "venue": "International Conference on Computational Logic",
        "authors": [
          {
            "authorId": "1447164726",
            "name": "E.K.L. D'hondt"
          },
          {
            "authorId": "1702730",
            "name": "S. Verberne"
          },
          {
            "authorId": "1713642",
            "name": "C. Koster"
          },
          {
            "authorId": "1728633",
            "name": "L. Boves"
          }
        ]
      },
      "citation_summary": {
        "citation_count": 19,
        "unique_cited_count": 19,
        "influential_count": 2,
        "detailed_records_count": 19
      },
      "cited_papers": [
        "14012633",
        "1891311",
        "14972283",
        "14854253",
        "185413",
        "10988962",
        "14534907",
        "13546567",
        "5156714",
        "497031",
        "16147669",
        "14972195",
        "16810209",
        "18493939",
        "14311264",
        "62218074",
        "5965390",
        "16644750",
        "3542573"
      ],
      "citation_details": [
        {
          "citedcorpusid": 185413,
          "isinfluential": false,
          "contexts": [
            "Larkey(1999) was the ﬁrst to present a fully automated patent classiﬁcation system, but she did not report her overall accuracy results."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "A patent search and classification system",
            "abstract": "",
            "year": 1999,
            "venue": "Digital library",
            "authors": [
              {
                "authorId": "1742457",
                "name": "L. Larkey"
              }
            ]
          }
        },
        {
          "citedcorpusid": 497031,
          "isinfluential": false,
          "contexts": [
            "Currently, three classiﬁer algorithms are available: Naive Bayes, Balanced Winnow (Dagan, Karov, and Roth 1997), and SVM-light (Joachims 1999)."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Mistake-Driven Learning in Text Categorization",
            "abstract": "Learning problems in the text processing domain often map the text to a space whose dimensions are the measured features of the text, e.g., its words. Three characteristic properties of this domain are (a) very high dimensionality, (b) both the learned concepts and the instances reside very sparsely in the feature space, and (c) a high variation in the number of active features in an instance. In this work we study three mistake-driven learning algorithms for a typical task of this nature -- text categorization. We argue that these algorithms -- which categorize documents by learning a linear separator in the feature space -- have a few properties that make them ideal for this domain. We then show that a quantum leap in performance is achieved when we further modify the algorithms to better address some of the specific characteristics of the domain. In particular, we demonstrate (1) how variation in document length can be tolerated by either normalizing feature weights or by using negative weights, (2) the positive effect of applying a threshold range in training, (3) alternatives in considering feature frequency, and (4) the benefits of discarding features while training. Overall, we present an algorithm, a variation of Littlestone's Winnow, which performs significantly better than any other algorithm tested on this task using a similar feature set.",
            "year": 1997,
            "venue": "Conference on Empirical Methods in Natural Language Processing",
            "authors": [
              {
                "authorId": "7465342",
                "name": "Ido Dagan"
              },
              {
                "authorId": "47137139",
                "name": "Yael Karov"
              },
              {
                "authorId": "144590225",
                "name": "D. Roth"
              }
            ]
          }
        },
        {
          "citedcorpusid": 1891311,
          "isinfluential": false,
          "contexts": [
            "17 In spite of the technical difﬁculties (Parapatics and Dittenbach 2009) and loss of linguistic accuracy for patent texts reported in Mille and Wanner (2008), most patent processing systems that use linguistic phrases use the Stanford parser because its dependency scheme has a number of properties…"
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Making Text Resources Accessible to the Reader: the Case of Patent Claims",
            "abstract": "",
            "year": 2008,
            "venue": "International Conference on Language Resources and Evaluation",
            "authors": [
              {
                "authorId": "2738095",
                "name": "Simon Mille"
              },
              {
                "authorId": "1758440",
                "name": "Leo Wanner"
              }
            ]
          }
        },
        {
          "citedcorpusid": 3542573,
          "isinfluential": false,
          "contexts": [
            "…loss of linguistic accuracy for patent texts reported in Mille and Wanner (2008), most patent processing systems that use linguistic phrases use the Stanford parser because its dependency scheme has a number of properties that are valuable for Text Mining purposes (de Marneffe and Manning 2008)."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "The Stanford Typed Dependencies Representation",
            "abstract": "This paper examines the Stanford typed dependencies representation, which was designed to provide a straightforward description of grammatical relations for any user who could benefit from automatic text understanding. For such purposes, we argue that dependency schemes must follow a simple design and provide semantically contentful information, as well as offer an automatic procedure to extract the relations. We consider the underlying design principles of the Stanford scheme from this perspective, and compare it to the GR and PARC representations. Finally, we address the question of the suitability of the Stanford scheme for parser evaluation.",
            "year": 2008,
            "venue": "CF+CDPE@COLING",
            "authors": [
              {
                "authorId": "2241127",
                "name": "M. Marneffe"
              },
              {
                "authorId": "144783904",
                "name": "Christopher D. Manning"
              }
            ]
          }
        },
        {
          "citedcorpusid": 5156714,
          "isinfluential": false,
          "contexts": [
            "Therefore, it is not surprising that most papers concluded that classiﬁers using only syntactic phrases perform worse than the baseline, except when the BOW baseline is low for that particular classiﬁcation task (Mitra et al. 1997; F¨urnkranz 1999).",
            "Therefore, it is not surprising that most papers concluded that classifiers using only syntactic phrases perform worse than the baseline, except when the BOW baseline is low for that particular classification task (Mitra et al. 1997; Fürnkranz 1999)."
          ],
          "intents": [
            "['background']",
            "['background']"
          ],
          "cited_paper_info": {
            "title": "An Analysis of Statistical and Syntactic Phrases",
            "abstract": "As the amount of textual information available through the World Wide Web grows, there is a growing need for high-precision IR systems that enable a user to find useful information from the masses of available textual data. Phrases have traditionally been regarded as precision-enhancing devices and have proved useful as content-identifiers in representing documents. In this study, we compare the usefulness of phrases recognized using linguistic methods and those recognized by statistical techniques. We focus in particular on high-precision retrieval. We discover that once a good basic ranking scheme is being used, the use of phrases does not have a major effect on precision at high ranks. Phrases are more useful at lower ranks where the connection between documents and relevance is more tenuous. Also, we find that the syntactic and statistical methods for recognizing phrases yield comparable performance.",
            "year": 1997,
            "venue": "RIAO Conference",
            "authors": [
              {
                "authorId": "1798723",
                "name": "Mandar Mitra"
              },
              {
                "authorId": "144009691",
                "name": "C. Buckley"
              },
              {
                "authorId": "145163573",
                "name": "A. Singhal"
              },
              {
                "authorId": "1748501",
                "name": "Claire Cardie"
              }
            ]
          }
        },
        {
          "citedcorpusid": 5965390,
          "isinfluential": false,
          "contexts": [
            "The bigrams in this category are similar to those investigated by Caropreso, Matwin, and Sebastiani (2001) and Tan, Wang, and Lee (2002).",
            "For an excellent overview of the work on using phrases done up to 2002, see Bekkerman and Allan (2003), and Tan, Wang, and Lee (2002).",
            "Tan, Wang, and Lee (2002) proposed selecting highly representative and meaningful bigrams based on the Mutual Information scores of the words in a bigram compared with the unigram class model."
          ],
          "intents": [
            "['methodology']",
            "['methodology']",
            "['background']"
          ],
          "cited_paper_info": {
            "title": "The use of bigrams to enhance text categorization",
            "abstract": "",
            "year": 2002,
            "venue": "Information Processing & Management",
            "authors": [
              {
                "authorId": "17164362",
                "name": "Chade-Meng Tan"
              },
              {
                "authorId": "47904091",
                "name": "Yuan-fang Wang"
              },
              {
                "authorId": "3501844",
                "name": "Chansik Lee"
              }
            ]
          }
        },
        {
          "citedcorpusid": 10988962,
          "isinfluential": true,
          "contexts": [
            "(cid:1) For each document the LCS returns a ranked list of all possible labels and the attendant conﬁdence scores.",
            "In such cases, the LCS has a back-off mechanism that automatically (re)selects terms that were initially discarded during GTS.",
            "(cid:1) The classiﬁcation quality was determined by calculating the Precision, Recall, and F1 measures per document/class combination (see, e.g., Koster, Seutter, and Beney 2003), on the document level (micro-averaged scores).",
            "We will leave term (feature) selection to the preprocessing module of the Linguistic Classiﬁcation System (LCS) which we used for all experiments (see Section 3.3).",
            "We therefore only used the Balanced Winnow algorithm for our classiﬁcation experiments, which were run with the following LCS conﬁguration, based on tuning experiments on the same data by Koster et al. (2011): (cid:1) Global term selection (GTS): Document frequency minimum is 2, term frequency minimum is 3.",
            "We ran the experiments with the LCS using the settings reported in Section 3.3.",
            "The classiﬁcation experiments were carried out within the framework of the LCS (Koster, Seutter, and Beney 2003).",
            "We conﬁgured the LCS to return a minimum of one label (with the highest score, even if it is lower than the threshold) and a maximum of four labels for each document.",
            "The same data set was later used by Koster and Seutter (2003), who experimented with a combined representation of words and phrases consisting of head-modiﬁer pairs.",
            "During local term selection, the LCS ﬁnds the most representative terms for each class by selecting the terms whose distributions in the sets of positive and negative training examples for that class are maximally different from the general term distribution.",
            "We used the LCS option to automatically select the most representative terms for every class, with a hard maximum of 10,000 terms per class.",
            "The LCS has been developed for the purpose of comparing different text representations."
          ],
          "intents": [
            "--",
            "--",
            "['methodology']",
            "--",
            "--",
            "--",
            "['methodology']",
            "--",
            "['methodology']",
            "--",
            "--",
            "--"
          ],
          "cited_paper_info": {
            "title": "Multi-classification of Patent Applications with Winnow",
            "abstract": "",
            "year": 2003,
            "venue": "Ershov Memorial Conference",
            "authors": [
              {
                "authorId": "1713642",
                "name": "C. Koster"
              },
              {
                "authorId": "2286294",
                "name": "M. Seutter"
              },
              {
                "authorId": "145639626",
                "name": "Jean Beney"
              }
            ]
          }
        },
        {
          "citedcorpusid": 13546567,
          "isinfluential": false,
          "contexts": [
            "In a follow-up study, Özgür and Güngör (2012) found that for the three different data sets, different types of linguistic phrases have most impact.",
            "Özgür and Güngör (2010, 2012) achieve small but significant improvements when combining unigrams with a subset of the dependency types from the Stanford parser on three different data sets, including the Reuters-21578 set."
          ],
          "intents": [
            "['background']",
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Optimization of dependency and pruning usage in text classification",
            "abstract": "",
            "year": 2012,
            "venue": "Pattern Analysis and Applications",
            "authors": [
              {
                "authorId": "1887825",
                "name": "Levent Özgür"
              },
              {
                "authorId": "1765713",
                "name": "Tunga Güngör"
              }
            ]
          }
        },
        {
          "citedcorpusid": 14012633,
          "isinfluential": false,
          "contexts": [
            "In the same competition, Derieux et al. (2010) came second (in terms of P@1)."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Combining Semantics and Statistics for Patent Classification",
            "abstract": "For the patent classification task of the 2010 CLEF-IP evaluation we have used three different approaches combining semantics and statistics-driven techniques: first approach is based on an indexing-retrieval method using the Lemur system enhanced with a class calculation algorithm; the second approach combined a semantics-driven technique for class model building and the use of an advanced statistical classifier; the third approach combined the two previous methods, attempting to exploit their complementarity for results quality improvement. The results obtained for our system are encouraging: we ranked second in terms of precision on first candidate, which is, from an application point of view, the most pertinent score.",
            "year": 2010,
            "venue": "Conference and Labs of the Evaluation Forum",
            "authors": [
              {
                "authorId": "48733111",
                "name": "F. Derieux"
              },
              {
                "authorId": "2238724",
                "name": "Mihaela Bobeica"
              },
              {
                "authorId": "2330267",
                "name": "Delphine Pois"
              },
              {
                "authorId": "1964787",
                "name": "Jean-Pierre Raysz"
              }
            ]
          }
        },
        {
          "citedcorpusid": 14311264,
          "isinfluential": false,
          "contexts": [
            "(cid:1) Local term selection (LTS): Simple Chi Square (Galavotti, Sebastiani, and Simi 2000)."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Experiments on the Use of Feature Selection and Negative Evidence in Automated Text Categorization",
            "abstract": "",
            "year": 2000,
            "venue": "European Conference on Research and Advanced Technology for Digital Libraries",
            "authors": [
              {
                "authorId": "2745994",
                "name": "Luigi Galavotti"
              },
              {
                "authorId": "145077269",
                "name": "F. Sebastiani"
              },
              {
                "authorId": "35184435",
                "name": "M. Simi"
              }
            ]
          }
        },
        {
          "citedcorpusid": 14534907,
          "isinfluential": false,
          "contexts": [
            "Both Mladenic and Grobelnik (1998) and F¨urnkranz (1998) showed that classiﬁers trained on combinations of unigrams and n -grams composed of at most three words performed better than classiﬁers that only use unigrams; no improvement was obtained when using larger n -grams."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Word sequences as features in text-learning",
            "abstract": "Dunja Mladeni c & Marko Grobelnik J.Stefan Institute Jamova 39, 1000 Ljubljana, Slovenia Dunja.Mladenic@ijs.si, Marko.Grobelnik@ijs.si Abstract This paper proposes an e cient algorithm for the generation of new features that enrich the known bagof-words document representation. New features are generated based on word sequences of di erent length. Learning is performed using Naive Bayesian classi er on feature-vectors, where only highly scored features are used. The performance of enriched document representation is evaluated on the problem of automatic document categorization using Yahoo text hierarchy. Our experiments show that using word sequences of length up to 3 instead of using only single words improves the performance, while longer sequences in average have no in uence to the performance.",
            "year": 1998,
            "venue": "",
            "authors": [
              {
                "authorId": "1764321",
                "name": "D. Mladenić"
              },
              {
                "authorId": "1775954",
                "name": "M. Grobelnik"
              }
            ]
          }
        },
        {
          "citedcorpusid": 14854253,
          "isinfluential": false,
          "contexts": [
            "Both Mladenic and Grobelnik (1998) and Fürnkranz (1998) showed that classifiers trained on combinations of unigrams and n-grams composed of at most three words performed better than classifiers that only use unigrams; no improvement was obtained when using larger n-grams."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "A Study Using $n$-gram Features for Text Categorization",
            "abstract": "In this paper, we study the effect of using -grams (sequences of words of length ) for text categorization. We use an efficient algorithm for generating such -gram features in two benchmark domains, the 20 newsgroups data set and 21,578 REUTERS newswire articles. Our results with the rule learning algorithm RIPPER indicate that, after the removal of stop words, word sequences of length 2 or 3 are most useful. Using longer sequences reduces classification performance.",
            "year": 1998,
            "venue": "",
            "authors": [
              {
                "authorId": "1747752",
                "name": "Johannes Fürnkranz"
              }
            ]
          }
        },
        {
          "citedcorpusid": 14972195,
          "isinfluential": false,
          "contexts": [
            "17 In spite of the technical difﬁculties (Parapatics and Dittenbach 2009) and loss of linguistic accuracy for patent texts reported in Mille and Wanner (2008), most patent processing systems that use linguistic phrases use the Stanford parser because its dependency scheme has a number of properties…",
            "This was done automatically, using the following regular expressions (based on Parapatics and Dittenbach 2009): We then used a perl script to divide the running text into sentences, by splitting on end-of-sentence punctuation such as question marks and full stops."
          ],
          "intents": [
            "['background']",
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Patent claim decomposition for improved information extraction",
            "abstract": "",
            "year": 2009,
            "venue": "Current Challenges in Patent Information Retrieval",
            "authors": [
              {
                "authorId": "2821974",
                "name": "Peter Parapatics"
              },
              {
                "authorId": "1709905",
                "name": "Michael Dittenbach"
              }
            ]
          }
        },
        {
          "citedcorpusid": 14972283,
          "isinfluential": true,
          "contexts": [
            "Therefore, it is not surprising that the best scoring multi-class, multi-label 3 classiﬁcation results for the well-known Reuters-21578 data set have been obtained using a BOW representation (Bekkerman and Allan 2003).",
            "With the advent of increasing computational power and bigger data sets, however, the topic has been revisited in the last two decades (Bekkerman and Allan 2003).",
            "For an excellent overview of the work on using phrases done up to 2002, see Bekkerman and Allan (2003), and Tan, Wang, and Lee (2002).",
            "All experimental results, however, show that using only phrases as index terms leads to a decrease in classiﬁcation accuracy compared with the BOW baseline (Bekkerman and Allan 2003).",
            "The difference with the results on the Reuters-21578 data set (discussed in Section 2.1.1), however, may not completely be due to genre differences: Bekkerman and Allan (2003) remark that the unigram baseline for the Reuters-21578 task is difﬁcult to improve upon, because in that data set a few…",
            "Inline with Bekkerman and Allan (2003) we can conclude that with the large quantities of text available today, the role of phrases as features in text classiﬁcation must be reconsidered.",
            "Bekkerman and Allan (2003) failed to improve over their unigram baseline when using similar selection criteria based on the distributional clustering of unigram models."
          ],
          "intents": [
            "['background']",
            "['background']",
            "['methodology']",
            "['result']",
            "['result']",
            "['background']",
            "--"
          ],
          "cited_paper_info": {
            "title": "Using Bigrams in Text Categorization",
            "abstract": "In the past decade a sufficient effort has been expended on attempting to come up with a document representation which is richer than the simple Bag-Of-Words (BOW). One of the widely explored approaches to enrich the BOW representation is in using n-grams (usually bigrams) of words in addition to (or in place of) single words (unigrams). After more than ten years of unsuccessful attempts to improve the text categorization results by applying bigrams, many researchers agree that there might be a certain limitation in usability of bigrams for text categorization. We analyze the related works and discuss possible reasons for this limitation. In addition, we demonstrate our own attempt to incorporate bigrams in a document representation based on distributional clusters of unigrams, and report (statistically insignificant) improvement to our baseline results on the 20 Newsgroups (20NG) dataset. Nevertheless, the reported result is (to our knowledge) the best categorization result ever achieved on this highly popular dataset.",
            "year": 2003,
            "venue": "",
            "authors": [
              {
                "authorId": "1988453",
                "name": "Ron Bekkerman"
              },
              {
                "authorId": "144890574",
                "name": "James Allan"
              }
            ]
          }
        },
        {
          "citedcorpusid": 16147669,
          "isinfluential": false,
          "contexts": [
            "Braga, Monard, and Matsubara (2009) used a Multinomial Naive Bayes classiﬁer to investigate classiﬁcation performance with unigrams and bigrams by comparing multiview classiﬁcation (the results of two independent classiﬁers trained with unigram and bigram features are merged) with monoview…"
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Combining Unigrams and Bigrams in Semi-Supervised Text Classiﬁcation",
            "abstract": ". Unlabeled documents vastly outnumber labeled documents in text classiﬁcation. For this reason, semi-supervised learning is well suited to the task. Representing text as a combination of unigrams and bigrams has not shown consistent improvements compared to using uni-grams in supervised text classiﬁcation. Therefore, a natural question is whether this ﬁnding extends to semi-supervised learning, which provides a diﬀerent way of combining multiple representations of data. In this paper, we investigate this question experimentally running two semi-supervised algorithms, Co-Training and Self-Training , on several text datasets. Our results do not indicate improvements by combining unigrams and bigrams in semi-supervised text classiﬁcation. In addition, they suggest that this fact may stem from a strong “correlation” between unigrams and bigrams.",
            "year": 2009,
            "venue": "",
            "authors": [
              {
                "authorId": "3073662",
                "name": "Í. Braga"
              },
              {
                "authorId": "2285701157",
                "name": "Edson Maria Carolina Monard"
              },
              {
                "authorId": "2285678102",
                "name": "Takashi Matsubara"
              }
            ]
          }
        },
        {
          "citedcorpusid": 16644750,
          "isinfluential": false,
          "contexts": [
            "Lewis (1992) was the ﬁrst to investigate the use of phrases as index terms for text classiﬁcation.",
            "Lewis (1992) and Apt´e, Damerau, and Weiss (1994) were the ﬁrst to investigate the impact of syntactic phrases 6 as features for text classiﬁcation."
          ],
          "intents": [
            "--",
            "['background']"
          ],
          "cited_paper_info": {
            "title": "An evaluation of phrasal and clustered representations on a text categorization task",
            "abstract": "",
            "year": 1992,
            "venue": "Annual International ACM SIGIR Conference on Research and Development in Information Retrieval",
            "authors": [
              {
                "authorId": "35153517",
                "name": "D. Lewis"
              }
            ]
          }
        },
        {
          "citedcorpusid": 16810209,
          "isinfluential": false,
          "contexts": [
            "In this informal benchmark Koster, Seutter, and Beney (2001) achieved the best results, using the Bal-anced Winnow algorithm with a word-only text representation."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Classifying Patent Applications with Winnow",
            "abstract": "A new and distinct variety of peach, Prunus persica, tree with the following unique combination of desirable features: 1. Heavy and regular production of large, white flesh, clingstone fruit. 2. Fruit with a mild, sweet, subacid flavor with excellent eating quality. 3. Fruit with firm white flesh, good handling and shipping qualities. 4. An attractive fruit with a white ground color nearly overspread with a red blush. 5. Large tree size with vigorous, upright growth. 6. Relatively uniform ripening of fruit throughout the tree.",
            "year": 2001,
            "venue": "",
            "authors": [
              {
                "authorId": "1713642",
                "name": "C. Koster"
              },
              {
                "authorId": "2286294",
                "name": "M. Seutter"
              },
              {
                "authorId": "145639626",
                "name": "Jean Beney"
              }
            ]
          }
        },
        {
          "citedcorpusid": 18493939,
          "isinfluential": false,
          "contexts": [
            "Dumais et al. (1998) and Scott and Matwin (1999) did not observe a signiﬁcant improvement in classiﬁcation on the Reuters-21578 collection when noun phrases obtained with a shallow parser were used instead of unigrams."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Feature Engineering for Text Classification",
            "abstract": "Most research in text classification to date has used a “bag of words” representation in which each feature corresponds to a single word. This paper examines some alternative ways to represent text based on syntactic and semantic relationships between words (phrases, synonyms and hypernyms). We describe the new representations and try to justify our hypothesis that they could improve the performance of a rule-based learner. The representations are evaluated using the RIPPER learning algorithm on the Reuters-21578 and DigiTrad test corpora. On their own the new representations are not found to produce significant performance improvements. We also try combining classifiers based on different representations using a majority voting technique, and this improves performance on both test collections. In our opinion, more sophisticated Natural Language Processing techniques need to be developed before better text representations can be produced for classification.",
            "year": 1999,
            "venue": "International Conference on Machine Learning",
            "authors": [
              {
                "authorId": "2093248943",
                "name": "Sam Scott"
              },
              {
                "authorId": "1749003",
                "name": "S. Matwin"
              }
            ]
          }
        },
        {
          "citedcorpusid": 62218074,
          "isinfluential": false,
          "contexts": [
            "P@1). They also used a mixed document representation of both single words and longer phrases, which had been extracted from the corpus by counting word co-occurrences. Verberne, Vogel, and D’hondt (2010) and Beney (2010) experimented with a combined representation of words and syntactic phrases derived from an English and French syntactic parser, respectively.",
            "Currently, three classifier algorithms are available: Naive Bayes, Balanced Winnow (Dagan, Karov, and Roth 1997), and SVM-light (Joachims 1999). Verberne, Vogel, and D’hondt (2010) found that Balanced Winnow and SVM-light yield comparable classification accuracy scores for patent texts on a similar data set, but that Balanced Winnow is much faster than SVM-light for classification problems with a large number of classes."
          ],
          "intents": [
            "['methodology']",
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Automation of patent classification",
            "abstract": "",
            "year": 2002,
            "venue": "",
            "authors": [
              {
                "authorId": "2110645560",
                "name": "H. Smith"
              }
            ]
          }
        }
      ]
    },
    "38122372": {
      "citing_paper_info": {
        "title": "Cross-language patent matching via an international patent classification-based concept bridge",
        "abstract": "",
        "year": 2013,
        "venue": "Journal of information science",
        "authors": [
          {
            "authorId": "123331773",
            "name": "Yen-Liang Chen"
          },
          {
            "authorId": "39776056",
            "name": "Yu-Ting Chiu"
          }
        ]
      },
      "citation_summary": {
        "citation_count": 22,
        "unique_cited_count": 20,
        "influential_count": 1,
        "detailed_records_count": 22
      },
      "cited_papers": [
        "14753351",
        "262553219",
        "8802092",
        "32805873",
        "62723298",
        "16740761",
        "18337412",
        "18151324",
        "16210722",
        "267853955",
        "10862030",
        "7612539",
        "59881598",
        "54116525",
        "3252915",
        "16803425",
        "16279815",
        "1229321",
        "42343081",
        "14004036"
      ],
      "citation_details": [
        {
          "citedcorpusid": 1229321,
          "isinfluential": false,
          "contexts": [
            "According to Joorabchi and Mahdi, utilizing keyphrases is an effective way to reveal subjects in scientific and research documents, and to retrieve information from databases [41].",
            "Compound nouns are named as key-phrases (or keyphrases) in other research [39–41]."
          ],
          "intents": [
            "['background']",
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Automatic keyphrase annotation of scientific documents using Wikipedia and genetic algorithms",
            "abstract": "",
            "year": 2013,
            "venue": "Journal of information science",
            "authors": [
              {
                "authorId": "2183390",
                "name": "Arash Joorabchi"
              },
              {
                "authorId": "1784658",
                "name": "A. Mahdi"
              }
            ]
          }
        },
        {
          "citedcorpusid": 3252915,
          "isinfluential": false,
          "contexts": [
            "In this work, the well-known and convincing methods of latent semantic analysis (LSA) and latent semantic indexing (LSI) [48, 49] are adopted to group similar keywords together and a dummy index is used to express these keywords."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Latent semantic analysis",
            "abstract": "A new method for automatic indexing and retrieval is described. The approach is to take advantage of implicit higher-order structure in the association of terms with documents (\"semantic structure\") in order to improve the detection of relevant documents on the basis of terms found in queries. The particular technique used is singular-value decomposition, in which a large term by document matrix is decomposed into a set of ca 100 orthogonal factors from which the original matrix can be approximated by linear combination. Documents are represented by ca 100 item vectors of factor weights. Queries are represented as pseudo-document vectors formed from weighted combinations of terms, and documents with supra-threshold cosine values are returned. Initial tests find this completely automatic method for retrieval to be promising.",
            "year": 2008,
            "venue": "Scholarpedia",
            "authors": [
              {
                "authorId": "1836606",
                "name": "T. Landauer"
              },
              {
                "authorId": "1728602",
                "name": "S. Dumais"
              }
            ]
          }
        },
        {
          "citedcorpusid": 7612539,
          "isinfluential": false,
          "contexts": [
            "For example, word sense disambiguation is a crucial problem in mapping similar concepts and measuring similar contexts, and is even harder to process across different languages [17, 18]."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Translation disambiguation for cross-language information retrieval using context-based translation probability",
            "abstract": "",
            "year": 2009,
            "venue": "Journal of information science",
            "authors": [
              {
                "authorId": "2930143",
                "name": "Kazuaki Kishida"
              },
              {
                "authorId": "3280058",
                "name": "Emi Ishita"
              }
            ]
          }
        },
        {
          "citedcorpusid": 8802092,
          "isinfluential": false,
          "contexts": [
            "Cetintas and Si designed a method to generate and post-process a query automatically for prior art patent search [36]."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Effective query generation and postprocessing strategies for prior art patent search",
            "abstract": "Rapid increase in global competition demands increased protection of intellectual property rights and underlines the importance of patents as major intellectual property documents. Prior art patent search is the task of identifying related patents for a given patent file, and is an essential step in judging the validity of a patent application. This article proposes an automated query generation and postprocessing method for prior art patent search. The proposed approach first constructs structured queries by combining terms extracted from different fields of a query patent and then reranks the retrieved patents by utilizing the International Patent Classification (IPC) code similarities between the query patent and the retrieved patents along with the retrieval score. An extensive set of empirical results carried out on a large-scale, real-world dataset shows that utilizing 20 or 30 query terms extracted from all fields of an original query patent according to their log(tf)idf values helps form a representative search query out of the query patent and is found to be more effective than is using any number of query terms from any single field. It is shown that combining terms extracted from different fields of the query patent by giving higher importance to terms extracted from the abstract, claims, and description fields than to terms extracted from the title field is more effective than treating all extracted terms equally while forming the search query. Finally, utilizing the similarities between the IPC codes of the query patent and retrieved patents is shown to be beneficial to improve the effectiveness of the prior art search. © 2012 Wiley Periodicals, Inc.",
            "year": 2012,
            "venue": "J. Assoc. Inf. Sci. Technol.",
            "authors": [
              {
                "authorId": "1778765",
                "name": "Suleyman Cetintas"
              },
              {
                "authorId": "145388187",
                "name": "Luo Si"
              }
            ]
          }
        },
        {
          "citedcorpusid": 10862030,
          "isinfluential": false,
          "contexts": [
            "[42], we use n-gram phrases to replace single words or terms to prevent implication misunderstanding within a document.",
            "introduced an approach to discover compound nouns, using grammatical patterns and a T-GSP (Text Generalized Sequential Patterns) algorithm to extract frequent text sequences that satisfy a given set of grammatical rules [42]."
          ],
          "intents": [
            "['methodology']",
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Rough Sets and Intelligent Systems Paradigms",
            "abstract": "",
            "year": 2014,
            "venue": "Lecture Notes in Computer Science",
            "authors": [
              {
                "authorId": "2236204",
                "name": "Marzena Kryszkiewicz"
              },
              {
                "authorId": "1694128",
                "name": "C. Cornelis"
              },
              {
                "authorId": "2285585318",
                "name": "Davide Ciucci"
              },
              {
                "authorId": "2285566664",
                "name": "Jesús Medina-Moreno"
              },
              {
                "authorId": "1748072",
                "name": "H. Motoda"
              },
              {
                "authorId": "2237781074",
                "name": "Zbigniew W. Ras"
              }
            ]
          }
        },
        {
          "citedcorpusid": 14004036,
          "isinfluential": false,
          "contexts": [
            "It should also be noted that the irregular lengths of the documents and the variety of terminologies within them make analysis even harder [2]."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Text mining techniques for patent analysis",
            "abstract": "",
            "year": 2007,
            "venue": "Information Processing & Management",
            "authors": [
              {
                "authorId": "40130996",
                "name": "Yuen-Hsien Tseng"
              },
              {
                "authorId": "2143476751",
                "name": "Chi-Jen Lin"
              },
              {
                "authorId": "3315593",
                "name": "Yu-I Lin"
              }
            ]
          }
        },
        {
          "citedcorpusid": 14753351,
          "isinfluential": false,
          "contexts": [
            "Problems that may occur during the translation process include lack of fidelity, information loss, translation ambiguity and others [6, 15, 16].",
            "According to He et al. [16] and Go¨ker and Davies [15], there are numerous ways that poor or erroneous translations can occur in translation-based cross-language information retrieval."
          ],
          "intents": [
            "['background']",
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Making MIRACLEs: Interactive translingual search for Cebuano and Hindi",
            "abstract": "",
            "year": 2003,
            "venue": "TALIP",
            "authors": [
              {
                "authorId": "40249854",
                "name": "Daqing He"
              },
              {
                "authorId": "1737250",
                "name": "Douglas W. Oard"
              },
              {
                "authorId": "2110215030",
                "name": "Jianqiang Wang"
              },
              {
                "authorId": "2116813766",
                "name": "Jun Luo"
              },
              {
                "authorId": "1398175407",
                "name": "Dina Demner-Fushman"
              },
              {
                "authorId": "143758717",
                "name": "Kareem Darwish"
              },
              {
                "authorId": "1680292",
                "name": "P. Resnik"
              },
              {
                "authorId": "2803071",
                "name": "S. Khudanpur"
              },
              {
                "authorId": "2006827",
                "name": "Michael Nossal"
              },
              {
                "authorId": "2649427",
                "name": "M. Subotin"
              },
              {
                "authorId": "3201827",
                "name": "Anton Leuski"
              }
            ]
          }
        },
        {
          "citedcorpusid": 16210722,
          "isinfluential": false,
          "contexts": [
            "For example, word sense disambiguation is a crucial problem in mapping similar concepts and measuring similar contexts, and is even harder to process across different languages [17, 18]."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Word sense disambiguation based on positional weighted context",
            "abstract": "",
            "year": 2013,
            "venue": "Journal of information science",
            "authors": [
              {
                "authorId": "2110442990",
                "name": "Shilin Huang"
              },
              {
                "authorId": "1687974",
                "name": "Xiaolin Zheng"
              },
              {
                "authorId": "2115251607",
                "name": "Haixiao Kang"
              },
              {
                "authorId": "1852242",
                "name": "Deren Chen"
              }
            ]
          }
        },
        {
          "citedcorpusid": 16279815,
          "isinfluential": false,
          "contexts": [
            "Karanikolas and Skourlas proposed a method to extract the most frequent and discriminate terms from documents within a class to form key-phrases, and then use these key-phrases to classify a new document [39, 40]."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Text Classification: Forming Candidate Key-Phrases from Existing Shorter Ones",
            "abstract": "The hard problem of the Text Classification usually has vario us aspects and potential solutions. In this paper, two main research direc tions for narrative docu- ments' classification are considered. The first one is based o n data mining and rule induction techniques, while the second combines the traditional Text Retrieval tech- niques (use of the vector space model, index terms, and similarity measures), Natural Language Processing and Instance based Learning techniques. Key-phrases can be used as attributes for mining rules or as a basis for measurin g the similarity of new (unclassified) documents with existing (classified) ones. H ence, we eventually focus on the problem of extracting key-phrases from text's collec tion in order to use them as attributes for text classification. A new algorithm for th e discovery of key-phrases is described. Candidate key-phrases are built using freque nt smaller ones and special emphasis is given to the reduction of the complexity of the algorithm.",
            "year": 2006,
            "venue": "",
            "authors": [
              {
                "authorId": "1909103",
                "name": "N. Karanikolas"
              },
              {
                "authorId": "1733758",
                "name": "C. Skourlas"
              }
            ]
          }
        },
        {
          "citedcorpusid": 16740761,
          "isinfluential": false,
          "contexts": [
            "developed a PatMedia search engine to retrieve patent images on the basis of contents [37]."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "PatMedia: Augmenting Patent Search with Content-Based Image Retrieval",
            "abstract": "",
            "year": 2012,
            "venue": "Information Retrieval Facility Conference",
            "authors": [
              {
                "authorId": "3019137",
                "name": "S. Vrochidis"
              },
              {
                "authorId": "2559834",
                "name": "A. Moumtzidou"
              },
              {
                "authorId": "47708293",
                "name": "G. Ypma"
              },
              {
                "authorId": "1715604",
                "name": "Y. Kompatsiaris"
              }
            ]
          }
        },
        {
          "citedcorpusid": 16803425,
          "isinfluential": false,
          "contexts": [
            "A company should continuously analyse the knowledge within patent documents to create new product designs and to maintain competitive advantages [1]."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "A Fuzzy Ontological Knowledge Document Clustering Methodology",
            "abstract": "",
            "year": 2009,
            "venue": "IEEE Transactions on Systems, Man, and Cybernetics, Part B (Cybernetics)",
            "authors": [
              {
                "authorId": "1761458",
                "name": "A. Trappey"
              },
              {
                "authorId": "1766308",
                "name": "C. Trappey"
              },
              {
                "authorId": "1807690",
                "name": "Fu-Chiang Hsu"
              },
              {
                "authorId": "1761776",
                "name": "David W. Hsiao"
              }
            ]
          }
        },
        {
          "citedcorpusid": 18151324,
          "isinfluential": true,
          "contexts": [
            "Problems that may occur during the translation process include lack of fidelity, information loss, translation ambiguity and others [6, 15, 16].",
            "The translation-based methods can be further categorized into two groups, text-translation-based methods [6–9] and index-set-mapping meth-ods [6, 10–12].",
            "Kishida [6] reviewed the research on the use of text-translation-based and index-set-mapping approaches to solve the CLDM problem, and stated that, in the future, it would be useful to discover ways to find resources for resource-poor languages.",
            "Traditional cross-language document retrieval and document matching methods are translation-based methods [6].",
            "According to Kishida [6], there are three main approaches to CLDM translation: machine translation [30], translation by a bilingual machine-readable dictionary [7, 8] and parallel or comparable corpus-based methods [10, 31]."
          ],
          "intents": [
            "['background']",
            "['methodology']",
            "['background']",
            "['methodology']",
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Technical issues of cross-language information retrieval: a review",
            "abstract": "",
            "year": 2005,
            "venue": "Information Processing & Management",
            "authors": [
              {
                "authorId": "2930143",
                "name": "Kazuaki Kishida"
              }
            ]
          }
        },
        {
          "citedcorpusid": 18337412,
          "isinfluential": false,
          "contexts": [
            "Karanikolas and Skourlas proposed a method to extract the most frequent and discriminate terms from documents within a class to form key-phrases, and then use these key-phrases to classify a new document [39, 40].",
            "Compound nouns are named as key-phrases (or keyphrases) in other research [39–41]."
          ],
          "intents": [
            "['methodology']",
            "['background']"
          ],
          "cited_paper_info": {
            "title": "KEY-PHRASE EXTRACTION FOR CLASSIFICATION",
            "abstract": "In this paper we consider the problem of extracting key-phrases from a bilingual texts collection and using them for text classification. A key-phrase could be defined as a sequence of words of a given size in a given partial order that occur within a sentence. We describe an algorithm for the discovery of key-phrases. Then, a framework of handling multilingual texts / documents is described which combines the use of the traditional vector space model with a new similarity measure which is based on the key-phrases. This framework is used for finding the most similar documents of a training set with any new document and selecting the classes of the similar documents as the most plausible ones for the new document. Some experimental results are also presented.",
            "year": 2004,
            "venue": "",
            "authors": [
              {
                "authorId": "1909103",
                "name": "N. Karanikolas"
              },
              {
                "authorId": "1733758",
                "name": "C. Skourlas"
              }
            ]
          }
        },
        {
          "citedcorpusid": 32805873,
          "isinfluential": false,
          "contexts": [
            "Kishida applied double-pass algorithm to cluster multi-lingual documents – English, French, German and Italian news articles – for document translation [29]."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Double-pass clustering technique for multilingual document collections",
            "abstract": "",
            "year": 2011,
            "venue": "Journal of information science",
            "authors": [
              {
                "authorId": "2930143",
                "name": "Kazuaki Kishida"
              }
            ]
          }
        },
        {
          "citedcorpusid": 42343081,
          "isinfluential": false,
          "contexts": [
            "Lu et al. [21] used a semi-automatic term translation method to construct a Chinese–English MeSH (Medical Subject Headings) to use in a Cross-Language Medical Information Retrieval system for medical information."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Using Web resources to construct multilingual medical thesaurus for cross-language medical information retrieval",
            "abstract": "",
            "year": 2008,
            "venue": "Decision Support Systems",
            "authors": [
              {
                "authorId": "1685008",
                "name": "Wen-Hsiang Lu"
              },
              {
                "authorId": "145630726",
                "name": "R. Lin"
              },
              {
                "authorId": "2956687",
                "name": "Yi-Che Chan"
              },
              {
                "authorId": "2110722386",
                "name": "Kuan-Hsi Chen"
              }
            ]
          }
        },
        {
          "citedcorpusid": 54116525,
          "isinfluential": false,
          "contexts": [
            "Multi-lingual thesauri are needed for the construction of language-independent representations, which are expensive to build; additionally, how to automatically map query keywords and document terms to a language-independent representation is still an open question [15, 33, 34]."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Cross Language Retrieval - English/Russian/French",
            "abstract": "",
            "year": 2002,
            "venue": "",
            "authors": [
              {
                "authorId": "2225432",
                "name": "Marjorie M. K. Hlava"
              }
            ]
          }
        },
        {
          "citedcorpusid": 59881598,
          "isinfluential": false,
          "contexts": [
            "According to Kishida [6], there are three main approaches to CLDM translation: machine translation [30], translation by a bilingual machine-readable dictionary [7, 8] and parallel or comparable corpus-based methods [10, 31]."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "On The Effective Use of Large Parallel Corpora in Cross-Language Text Retrieval",
            "abstract": "",
            "year": 1998,
            "venue": "",
            "authors": [
              {
                "authorId": "2110853037",
                "name": "Mark W. Davis"
              }
            ]
          }
        },
        {
          "citedcorpusid": 62723298,
          "isinfluential": false,
          "contexts": [
            "For example, the different order of words in different languages is always a concern in any natural language processing [13, 14]."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Technical translation as information transfer across language boundaries",
            "abstract": "",
            "year": 1980,
            "venue": "",
            "authors": [
              {
                "authorId": "2830074",
                "name": "P. Ganeshsundaram"
              }
            ]
          }
        },
        {
          "citedcorpusid": 262553219,
          "isinfluential": false,
          "contexts": [
            "According to Manning et al. [47], using all the terms within a document set to represent every document could lower the accuracy rate."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "An Introduction to Information Retrieval",
            "abstract": "",
            "year": 2013,
            "venue": "",
            "authors": [
              {
                "authorId": "144161686",
                "name": "S. Ceri"
              },
              {
                "authorId": "1710630",
                "name": "A. Bozzon"
              },
              {
                "authorId": "40350773",
                "name": "Marco Brambilla"
              },
              {
                "authorId": "2539248",
                "name": "Emanuele Della Valle"
              },
              {
                "authorId": "1704595",
                "name": "P. Fraternali"
              },
              {
                "authorId": "1794305",
                "name": "S. Quarteroni"
              }
            ]
          }
        },
        {
          "citedcorpusid": 267853955,
          "isinfluential": false,
          "contexts": [
            "For example, the different order of words in different languages is always a concern in any natural language processing [13, 14]."
          ],
          "intents": [
            "--"
          ],
          "cited_paper_info": {
            "title": "Dynamic context generation for natural language understanding: a multifaceted knowledge approach",
            "abstract": "We describe a comprehensive framework for text understanding, based on the representation of context. It is designed to serve as a representation of semantics for the full range of interpretive and inferential needs of general natural language processing. Its most distinctive feature is its uniform representation of the various simple and independent linguistic sources that play a role in determining meaning: lexical associations, syntactic restrictions, case-role expectations, and most importantly, contextual effects. Compositional syntactic structure from a shallow parsing is represented in a neural net-based associative memory, where it then interacts through a Bayesian network with semantic associations and the context or \"gist\" of the passage carried forward from preceding sentences. Experiments with more than 2000 sentences in different languages are included.",
            "year": 2003,
            "venue": "IEEE Trans. Syst. Man Cybern. Part A",
            "authors": [
              {
                "authorId": "2266128988",
                "name": "Samuel W. K. Chan"
              },
              {
                "authorId": "2265664821",
                "name": "J. Franklin"
              }
            ]
          }
        },
        {
          "citedcorpusid": null,
          "isinfluential": false,
          "contexts": [
            "The hierarchy contains a total of eight sections, more than 120 classes, more than 600 sub-classes, and approximately 70,000 groups [19]."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {}
        },
        {
          "citedcorpusid": null,
          "isinfluential": false,
          "contexts": [
            "First, part-of-speech tagging [43] is performed on the USPTO patents and CKIP tagging [31, 44] is performed on the TIPO patents."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {}
        }
      ]
    },
    "216654726": {
      "citing_paper_info": {
        "title": "Using skipgrams and PoS-based feature selection for patent classification",
        "abstract": "Until recently, phrases were deemed suboptimal features for text classification because of their sparseness (Lewis 1992). In recent work (Koster et al. 2011, D’hondt et al. Forthcoming), however, it was found that for classifying English patent documents, combining phrasal and unigram representations leads to significantly better classification results, because phrases are better suited to catch the Multi-Word Terms (MWT) abundant in the terminology-rich technical patent texts. In this article, we consider the task of patent classification of English abstracts at the class level (about 120 classes) of the International Patent Classification (IPC). We compare (a) the impact of two types of phrases to capture meaningful information (bigrams and skipgrams); and (b) the impact of performing additional filtering of the classification features, based on their Part of Speech (PoS). For this purpose we performed a series of classification experiments using different phrasal text representations and feature selection to determine which representation is most beneficial to English patent classification. We further investigated which type of information (as captured by the PoS-filtered skipgrams) has most impact during classification. The results show that combining unigrams and PoS-filtered skipgrams leads to a significant improvement in classification scores over the unigram baseline. Additional experiments show that the most important phrasal features are bigrams and additional useful phrases can be captured by allowing at most 2 skips in the skipgram approach. Deeper analysis revealed that the noun-noun combinations and – to a lesser extent – the adjectival-noun combinations are the most informative phrasal features for patent classification.",
        "year": 2012,
        "venue": "The Clinician",
        "authors": [
          {
            "authorId": "1447164726",
            "name": "E.K.L. D'hondt"
          },
          {
            "authorId": "1702730",
            "name": "S. Verberne"
          },
          {
            "authorId": "2287296121",
            "name": "Niklas Weber"
          },
          {
            "authorId": "2287579598",
            "name": "Kees Koster"
          },
          {
            "authorId": "1728633",
            "name": "L. Boves"
          }
        ]
      },
      "citation_summary": {
        "citation_count": 8,
        "unique_cited_count": 8,
        "influential_count": 0,
        "detailed_records_count": 8
      },
      "cited_papers": [
        "18397139",
        "114388067",
        "14311264",
        "14718266",
        "254375866",
        "16147669",
        "220444944",
        "10988962"
      ],
      "citation_details": [
        {
          "citedcorpusid": 10988962,
          "isinfluential": false,
          "contexts": [
            "Classification was done using the Linguistic Classification System (LCS, cf. (Koster et al. 2003)).",
            "(Koster et al. 2003))."
          ],
          "intents": [
            "['methodology']",
            "--"
          ],
          "cited_paper_info": {
            "title": "Multi-classification of Patent Applications with Winnow",
            "abstract": "",
            "year": 2003,
            "venue": "Ershov Memorial Conference",
            "authors": [
              {
                "authorId": "1713642",
                "name": "C. Koster"
              },
              {
                "authorId": "2286294",
                "name": "M. Seutter"
              },
              {
                "authorId": "145639626",
                "name": "Jean Beney"
              }
            ]
          }
        },
        {
          "citedcorpusid": 14311264,
          "isinfluential": false,
          "contexts": [
            "• Local Term Selection: Simple Chi Square (Galavotti et al. 2000), selecting the 10,000 most representative term per class."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Experiments on the Use of Feature Selection and Negative Evidence in Automated Text Categorization",
            "abstract": "",
            "year": 2000,
            "venue": "European Conference on Research and Advanced Technology for Digital Libraries",
            "authors": [
              {
                "authorId": "2745994",
                "name": "Luigi Galavotti"
              },
              {
                "authorId": "145077269",
                "name": "F. Sebastiani"
              },
              {
                "authorId": "35184435",
                "name": "M. Simi"
              }
            ]
          }
        },
        {
          "citedcorpusid": 14718266,
          "isinfluential": false,
          "contexts": [
            "2012), machine translation (Lin and Och 2004) and plagiarism detection (Hartrumpf et al. 2010).",
            "In addition, skipgrams have been used in a number of NLP application such as irony detection (Reyes et al. 2012), machine translation (Lin and Och 2004) and plagiarism detection (Hartrumpf et al. 2010)."
          ],
          "intents": [
            "['background']",
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Semantic Duplicate Identification with Parsing and Machine Learning",
            "abstract": "",
            "year": 2010,
            "venue": "International Conference on Text, Speech and Dialogue",
            "authors": [
              {
                "authorId": "3021200",
                "name": "Sven Hartrumpf"
              },
              {
                "authorId": "2040468",
                "name": "Tim vor der Brück"
              },
              {
                "authorId": "2190127",
                "name": "Christian Eichhorn"
              }
            ]
          }
        },
        {
          "citedcorpusid": 16147669,
          "isinfluential": false,
          "contexts": [
            "Braga et al. (2009) used a Multinomial Naive Bayes classifier to investigate classification performance with uni- and bigrams by comparing multi-view classification, (the results of two independent classifiers trained with unigram and bigram features are merged) with mono-view classification…"
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Combining Unigrams and Bigrams in Semi-Supervised Text Classiﬁcation",
            "abstract": ". Unlabeled documents vastly outnumber labeled documents in text classiﬁcation. For this reason, semi-supervised learning is well suited to the task. Representing text as a combination of unigrams and bigrams has not shown consistent improvements compared to using uni-grams in supervised text classiﬁcation. Therefore, a natural question is whether this ﬁnding extends to semi-supervised learning, which provides a diﬀerent way of combining multiple representations of data. In this paper, we investigate this question experimentally running two semi-supervised algorithms, Co-Training and Self-Training , on several text datasets. Our results do not indicate improvements by combining unigrams and bigrams in semi-supervised text classiﬁcation. In addition, they suggest that this fact may stem from a strong “correlation” between unigrams and bigrams.",
            "year": 2009,
            "venue": "",
            "authors": [
              {
                "authorId": "3073662",
                "name": "Í. Braga"
              },
              {
                "authorId": "2285701157",
                "name": "Edson Maria Carolina Monard"
              },
              {
                "authorId": "2285678102",
                "name": "Takashi Matsubara"
              }
            ]
          }
        },
        {
          "citedcorpusid": 18397139,
          "isinfluential": false,
          "contexts": [
            "Ptaszynski et al. (2011) looked at the usability of skipgrams with more skips and compared these to a regular n-gram approach in language modelling.",
            "The combinatorial explosion (Ptaszynski et al. 2011) of features raises the problem of selecting only those features that are truly representative for a class in text classification."
          ],
          "intents": [
            "['result']",
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Language Combinatorics: A Sentence Pattern Extraction Architecture Based on Combinatorial Explosion",
            "abstract": "A “sentence pattern” in modern Natural Language Processing is often considered as a subsequent string of words (n-grams). However, in many branches of linguistics, like Pragmatics or Corpus Linguistics, it has been noticed that simple n-gram patterns are not sufficient to reveal the whole sophistication of grammar patterns. We present a language independent architecture for extracting from sentences more sophisticated patterns than n-grams. In this architecture a “sentence pattern” is considered as n-element ordered combination of sentence elements. Experiments showed that the method extracts significantly more frequent patterns than the usual n-gram approach.",
            "year": 2011,
            "venue": "",
            "authors": [
              {
                "authorId": "1731034",
                "name": "M. Ptaszynski"
              },
              {
                "authorId": "51953877",
                "name": "Rafal Rzepka"
              },
              {
                "authorId": "144710424",
                "name": "K. Araki"
              },
              {
                "authorId": "2868792",
                "name": "Yoshio Momouchi"
              }
            ]
          }
        },
        {
          "citedcorpusid": 114388067,
          "isinfluential": false,
          "contexts": [
            "…will describe the invention in generic terms: this results in (complex) noun phrases that consist of a generic noun with a function indicator, for example, ‘fastening device’ to indicate any kind of screw, nail, rope, etc, or ‘means establishing fluid communication’ to mean ‘valve’ (Lawson 1997)."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "2.1.3 The Terms and Arts of Patentese: Wolves in Sheep’s Clothing",
            "abstract": "",
            "year": 1997,
            "venue": "",
            "authors": [
              {
                "authorId": "145587041",
                "name": "V. Lawson"
              }
            ]
          }
        },
        {
          "citedcorpusid": 220444944,
          "isinfluential": false,
          "contexts": [
            "For more information, see (Oostdijk et al. 2010)."
          ],
          "intents": [
            "--"
          ],
          "cited_paper_info": {
            "title": "Constructing a Broad-coverage Lexicon for Text Mining in the Patent Domain",
            "abstract": "",
            "year": 2010,
            "venue": "International Conference on Language Resources and Evaluation",
            "authors": [
              {
                "authorId": "52109694",
                "name": "N. Oostdijk"
              },
              {
                "authorId": "1702730",
                "name": "S. Verberne"
              },
              {
                "authorId": "1713642",
                "name": "C. Koster"
              }
            ]
          }
        },
        {
          "citedcorpusid": 254375866,
          "isinfluential": false,
          "contexts": [
            "In addition, skipgrams have been used in a number of NLP application such as irony detection (Reyes et al. 2012), machine translation (Lin and Och 2004) and plagiarism detection (Hartrumpf et al.",
            "In addition, skipgrams have been used in a number of NLP application such as irony detection (Reyes et al. 2012), machine translation (Lin and Och 2004) and plagiarism detection (Hartrumpf et al. 2010)."
          ],
          "intents": [
            "['methodology']",
            "['methodology']"
          ],
          "cited_paper_info": {}
        }
      ]
    },
    "15296714": {
      "citing_paper_info": {
        "title": "Multilayer source selection as a tool for supporting patent search and classification",
        "abstract": "",
        "year": 2015,
        "venue": "Information Retrieval Journal",
        "authors": [
          {
            "authorId": "7407022",
            "name": "Anastasia Giahanou"
          },
          {
            "authorId": "1786622",
            "name": "M. Salampasis"
          },
          {
            "authorId": "1718676",
            "name": "G. Paltoglou"
          }
        ]
      },
      "citation_summary": {
        "citation_count": 40,
        "unique_cited_count": 40,
        "influential_count": 6,
        "detailed_records_count": 40
      },
      "cited_papers": [
        "1486807",
        "13183982",
        "3363535",
        "13916954",
        "218592387",
        "62217932",
        "33433994",
        "20487815",
        "18463704",
        "8802092",
        "4456086",
        "7185758",
        "62191434",
        "2068066",
        "12711052",
        "17755188",
        "13514893",
        "10269743",
        "33391284",
        "18345205",
        "120323383",
        "14123572",
        "7293536",
        "14215145",
        "109342149",
        "1307261",
        "18596979",
        "2119510",
        "1385016",
        "185413",
        "16468576",
        "2724227",
        "207226010",
        "62570235",
        "62174397",
        "169268",
        "14148682",
        "51578296",
        "111108875",
        "14443484"
      ],
      "citation_details": [
        {
          "citedcorpusid": 169268,
          "isinfluential": false,
          "contexts": [
            "Another collection selection study involving topically organized patents is reported in the literature (Larkey et al. 2000), however this study was conducted many years ago with a different (USPTO) patent dataset."
          ],
          "intents": [
            "--"
          ],
          "cited_paper_info": {
            "title": "Collection selection and results merging with topically organized U.S. patents and TREC data",
            "abstract": "",
            "year": 2000,
            "venue": "International Conference on Information and Knowledge Management",
            "authors": [
              {
                "authorId": "1742457",
                "name": "L. Larkey"
              },
              {
                "authorId": "1735186",
                "name": "Margaret E. Connell"
              },
              {
                "authorId": "144987107",
                "name": "Jamie Callan"
              }
            ]
          }
        },
        {
          "citedcorpusid": 185413,
          "isinfluential": false,
          "contexts": [
            "However, in the patent domain where similar inventions contain to a large extent very different terminology (Larkey 1999) the idea of building hyper-documents centered around a specific technical concept such as IPCs is well suited."
          ],
          "intents": [
            "--"
          ],
          "cited_paper_info": {
            "title": "A patent search and classification system",
            "abstract": "",
            "year": 1999,
            "venue": "Digital library",
            "authors": [
              {
                "authorId": "1742457",
                "name": "L. Larkey"
              }
            ]
          }
        },
        {
          "citedcorpusid": 1307261,
          "isinfluential": true,
          "contexts": [
            "This measure has been widely adopted in the DIR research community (Callan et al. 1995; Nottelmann and Fuhr 2003; Si and Callan 2005) and is very important for evaluating collection selection algorithms at recalloriented tasks as it provides an indication if an algorithm is able to rank sub-collections containing a large number of relevant documents high in the ranking.",
            "The collection retrieval inference network (CORI) algorithm (Callan et al. 1995) is one",
            "The CORI results merging algorithm (Callan et al. 1995) is based on a heuristic weighted scores merging algorithm.",
            "This measure has been widely adopted in the DIR research community (Callan et al. 1995; Nottelmann and Fuhr 2003; Si and Callan 2005) and is very important for evaluating collection selection algorithms at recalloriented tasks as it provides an indication if an algorithm is able to rank…",
            "The collection retrieval inference network (CORI) algorithm (Callan et al. 1995) is one of the most widely used source selection algorithms.",
            "There are a number of source selection approaches including CORI (Callan et al. 1995),",
            "There are a number of source selection approaches including CORI (Callan et al. 1995), gGlOSS (French et al. 1999), and others (Si et al. 2002), that characterize different collections using collection statistics like term frequencies."
          ],
          "intents": [
            "--",
            "--",
            "--",
            "--",
            "--",
            "--",
            "--"
          ],
          "cited_paper_info": {
            "title": "Searching distributed collections with inference networks",
            "abstract": "",
            "year": 1995,
            "venue": "Annual International ACM SIGIR Conference on Research and Development in Information Retrieval",
            "authors": [
              {
                "authorId": "144987107",
                "name": "Jamie Callan"
              },
              {
                "authorId": "2110326971",
                "name": "Zhihong Lu"
              },
              {
                "authorId": "144456145",
                "name": "W. Bruce Croft"
              }
            ]
          }
        },
        {
          "citedcorpusid": 1385016,
          "isinfluential": false,
          "contexts": [
            "…are created (Callan and Connell 2001), source selection, in which a subset of the available information collections is chosen to process the query (Paltoglou et al. 2011) and results merging, in which the separate results are combined into a single merged result list which is returned to the user…",
            "The DIR process can be perceived as three separate but interleaved sub-processes: Source representation, in which surrogates of the available remote collections are created (Callan and Connell 2001), source selection, in which a subset of the available information collections is chosen to process the query (Paltoglou et al. 2011) and results merging, in which the separate results are combined into a single merged result list which is returned to the user (Si and Callan 2003a; Paltoglou"
          ],
          "intents": [
            "--",
            "--"
          ],
          "cited_paper_info": {
            "title": "Modeling information sources as integrals for effective and efficient source selection",
            "abstract": "",
            "year": 2011,
            "venue": "Information Processing & Management",
            "authors": [
              {
                "authorId": "1718676",
                "name": "G. Paltoglou"
              },
              {
                "authorId": "1786622",
                "name": "M. Salampasis"
              },
              {
                "authorId": "66550501",
                "name": "M. Satratzemi"
              }
            ]
          }
        },
        {
          "citedcorpusid": 1486807,
          "isinfluential": false,
          "contexts": [
            "There are a number of source selection approaches including CORI (Callan et al. 1995), gGlOSS (French et al. 1999), and others (Si et al. 2002), that characterize different collections using collection statistics like term frequencies."
          ],
          "intents": [
            "--"
          ],
          "cited_paper_info": {
            "title": "Comparing the performance of database selection algorithms",
            "abstract": "We compare the performance of two database selection algorithms reported in the literature. Their performance is compared using a common testbed designed specifically for database selection techniques. The testbed is a decomposition of the TREC/TIPSTER data into 236 subcollections. We present results of a recent investigation of the performance of the CORI algorithm and compare the performance with earlier work that examined the performance of gGlOSS. The databases from our testbed were ranked using both the gGlOSS and CORI techniques and compared to the RBR baseline, a baseline derived from TREC relevance judgements. We examined the degree to which CORI and gGlOSS approximate this baseline. Our results confirm our earlier observation that the gGlOSS Ideal(l) ranks do not estimate relevance-based ranks well. We also find that CORI is a uniformly better estimator of relevance-based ranks than gGlOSS for the test environment used in this study. Part of the advantage of the CORI algorithm can be explained by a strong correlation between gGlOSS and a size-based baseline (SBR). We also find that CORI produces consistently accurate rankings on testbeds ranging from 100--921 sites. However for a given level of recall, search effort appears to scale linearly with the number of databases.",
            "year": 1999,
            "venue": "Annual International ACM SIGIR Conference on Research and Development in Information Retrieval",
            "authors": [
              {
                "authorId": "1713791",
                "name": "J. French"
              },
              {
                "authorId": "33968184",
                "name": "Allison L. Powell"
              },
              {
                "authorId": "144987107",
                "name": "Jamie Callan"
              },
              {
                "authorId": "1679482",
                "name": "C. Viles"
              },
              {
                "authorId": "2691595",
                "name": "T. Emmitt"
              },
              {
                "authorId": "2777992",
                "name": "K. Prey"
              },
              {
                "authorId": "46688656",
                "name": "Y. Mou"
              }
            ]
          }
        },
        {
          "citedcorpusid": 2068066,
          "isinfluential": false,
          "contexts": [
            "On the basis of this structure, a number of researchers presented methods which function by aggregating evidence for relevance from different sources (Sigurbjörnsson et al. 2004; Kong and Lalmas 2007).",
            "On the basis of this structure, a number of researchers presented methods which function by aggregating evidence for relevance from different sources (Sigurbjörnsson et al. 2004; Kong and Lalmas 2007)."
          ],
          "intents": [
            "--",
            "--"
          ],
          "cited_paper_info": {
            "title": "Multiple sources of evidence for XML retrieval",
            "abstract": "Document-centric XML collections contain text-rich documents, marked up with XML tags. The tags add lightweight semantics to the text. Querying such collections calls for a hybrid query language: the text-rich nature of the documents suggest a content-oriented (IR) approach, while the mark-up allows users to add structural constraints to their IR queries. We will show how evidence for relevancy from different sources helps to answer such hybrid queries. We evaluate our methods using the INEX 2003 test set, and show that structural hints in hybrid queries help to improve retrieval effectiveness.",
            "year": 2004,
            "venue": "Annual International ACM SIGIR Conference on Research and Development in Information Retrieval",
            "authors": [
              {
                "authorId": "1780952",
                "name": "Börkur Sigurbjörnsson"
              },
              {
                "authorId": "1753628",
                "name": "J. Kamps"
              },
              {
                "authorId": "1696030",
                "name": "M. de Rijke"
              }
            ]
          }
        },
        {
          "citedcorpusid": 2119510,
          "isinfluential": true,
          "contexts": [
            "Both BordaFuse (Aslam and Montague 2001) and ReciRank utilize the rankings of the documents to calculate the score for each collection.",
            "On the other side both the fusion-based methods Reciprocal Rank and BordaFuse and ReDDE consistently produced worse results.",
            "In the experiments we used both the standard CORI and ReDDE algorithms and the fusion-based source selection methods (Paltoglou et al. 2009) using Reciprocal Rank and BordaFuse (Aslam and Montague 2001) for comparison.",
            "Reciprocal Rank and BordaFuse (Aslam and Montague 2001) for comparison.",
            "Both BordaFuse (Aslam and Montague 2001) and ReciRank utilize the rankings of the"
          ],
          "intents": [
            "--",
            "--",
            "--",
            "--",
            "--"
          ],
          "cited_paper_info": {
            "title": "Models for metasearch",
            "abstract": "",
            "year": 2001,
            "venue": "Annual International ACM SIGIR Conference on Research and Development in Information Retrieval",
            "authors": [
              {
                "authorId": "1737902",
                "name": "J. Aslam"
              },
              {
                "authorId": "31664187",
                "name": "Mark H. Montague"
              }
            ]
          }
        },
        {
          "citedcorpusid": 2724227,
          "isinfluential": false,
          "contexts": [
            "…skepticism, search technologies are being used increasingly in the workplace as a result of the explosion of content becoming electronically available, and those who deal with patents in their professional life are becoming more knowledgeable about new search technologies and tools (Atkinson 2008).",
            "However, despite the overall skepticism, search technologies are being used increasingly in the workplace as a result of the explosion of content becoming electronically available, and those who deal with patents in their professional life are becoming more knowledgeable about new search technologies and tools (Atkinson 2008)."
          ],
          "intents": [
            "--",
            "--"
          ],
          "cited_paper_info": {
            "title": "Toward a more rational patent search paradigm",
            "abstract": "",
            "year": 2008,
            "venue": "Patent Information Retrieval",
            "authors": [
              {
                "authorId": "32193002",
                "name": "Kristine H. Atkinson"
              }
            ]
          }
        },
        {
          "citedcorpusid": 3363535,
          "isinfluential": true,
          "contexts": [
            "…there is no clear distinction between the terms DIR and federated search (Shokouhi and Si 2011), the latter represents a more realistic DIR scenario that allows the simultaneous search of multiple searchable, remote and physically distributed, resources. ezDL is probably the most…",
            "ReDDE (Si and Callan 2003b) focuses on exactly that purpose.",
            "Semi-supervised learning (Si and Callan 2003a), makes use of a centralized index, which in our method was comprised of the whole set of documents from the dataset.",
            "…selection, in which a subset of the available information collections is chosen to process the query (Paltoglou et al. 2011) and results merging, in which the separate results are combined into a single merged result list which is returned to the user (Si and Callan 2003a; Paltoglou et al. 2008).",
            "Distributed Information Retrieval (DIR), also known as federated search (Si and Callan 2003a), offers users the capability of simultaneously searching multiple online remote information sources through a single point of search."
          ],
          "intents": [
            "--",
            "--",
            "--",
            "--",
            "--"
          ],
          "cited_paper_info": {
            "title": "Relevant document distribution estimation method for resource selection",
            "abstract": "",
            "year": 2003,
            "venue": "Annual International ACM SIGIR Conference on Research and Development in Information Retrieval",
            "authors": [
              {
                "authorId": "145388187",
                "name": "Luo Si"
              },
              {
                "authorId": "144987107",
                "name": "Jamie Callan"
              }
            ]
          }
        },
        {
          "citedcorpusid": 4456086,
          "isinfluential": false,
          "contexts": [
            "A number of approaches were proposed to address the patent classification using the CLEF-IP data sets (Guyot et al. 2010; Derieux et al. 2010; Verberne and D’hondt 2011; Beney 2010)."
          ],
          "intents": [
            "--"
          ],
          "cited_paper_info": {
            "title": "LCI-INSA Linguistic Experiment for CLEF-IP Classification Track",
            "abstract": "We present the experiment the LCI group has performed to prepare our submission to CLEF-IP Classification Track. In this preliminary experiment we used a part of the available target documents as test set and the rest as train set. We describe the systems AGFL used for extracting these triples and the LCS used for classification by the Winnow algorithm. We show that the use of linguistic triples in place of bags of words improves the accuracy, as well as using the names and addresses of the applicants. we found that using the complete descriptions as bags of words does not really perform better than using only abstracts and titles. Some simple mathematics show that the official measures are redundant and that R@N should be used to evaluate a ranking, P@1 to evaluate routing and that the usual precision, recall and F1 should be used on the results of a real classification, that is a selection of the classes performed internally by the classifier.",
            "year": 2010,
            "venue": "Conference and Labs of the Evaluation Forum",
            "authors": [
              {
                "authorId": "145639626",
                "name": "Jean Beney"
              }
            ]
          }
        },
        {
          "citedcorpusid": 7185758,
          "isinfluential": false,
          "contexts": [
            "…the professionals, there is a feeling that intelligent search tools ‘‘that no longer just do what you say but also what you mean’’ (Wolter 2012) can and must help and this feeling is reflected in various studies (e.g. Bonino et al. 2010) and evaluation campaigns (e.g. Roda et al. 2009; Lupu 2011)."
          ],
          "intents": [
            "--"
          ],
          "cited_paper_info": {
            "title": "CLEF-IP 2009: Retrieval Experiments in the Intellectual Property Domain",
            "abstract": "",
            "year": 2009,
            "venue": "Conference and Labs of the Evaluation Forum",
            "authors": [
              {
                "authorId": "35179184",
                "name": "G. Roda"
              },
              {
                "authorId": "144959176",
                "name": "J. Tait"
              },
              {
                "authorId": "3309646",
                "name": "Florina Piroi"
              },
              {
                "authorId": "2978384",
                "name": "V. Zenz"
              }
            ]
          }
        },
        {
          "citedcorpusid": 7293536,
          "isinfluential": false,
          "contexts": [
            "Cai and Hofmann (2004) proposed a multiclass SVM for hierarchical categorization.",
            "…of the patent domain (Larkey 1998; Kohonen et al. 2000; Fall et al. 2003; D’hondt et al. 2013), incorporating the hierarchy into the classification algorithm (Chakrabarti et al. 1998; Cai and Hofmann 2004; Tikk et al. 2007) or using linguistic analysis (Gey et al. 2001; D’hondt et al. 2013).",
            "2013), incorporating the hierarchy into the classification algorithm (Chakrabarti et al. 1998; Cai and Hofmann 2004; Tikk et al. 2007) or using linguistic analysis (Gey et al."
          ],
          "intents": [
            "--",
            "--",
            "--"
          ],
          "cited_paper_info": {
            "title": "Hierarchical document categorization with support vector machines",
            "abstract": "Automatically categorizing documents into pre-defined topic hierarchies or taxonomies is a crucial step in knowledge and content management. Standard machine learning techniques like Support Vector Machines and related large margin methods have been successfully applied for this task, albeit the fact that they ignore the inter-class relationships. In this paper, we propose a novel hierarchical classification method that generalizes Support Vector Machine learning and that is based on discriminant functions that are structured in a way that mirrors the class hierarchy. Our method can work with arbitrary, not necessarily singly connected taxonomies and can deal with task-specific loss functions. All parameters are learned jointly by optimizing a common objective function corresponding to a regularized upper bound on the empirical loss. We present experimental results on the WIPO-alpha patent collection to show the competitiveness of our approach.",
            "year": 2004,
            "venue": "International Conference on Information and Knowledge Management",
            "authors": [
              {
                "authorId": "3050583",
                "name": "Lijuan Cai"
              },
              {
                "authorId": "143936663",
                "name": "Thomas Hofmann"
              }
            ]
          }
        },
        {
          "citedcorpusid": 8802092,
          "isinfluential": false,
          "contexts": [
            "A significant number of methods have been proposed with the aim to improve the priorart search by utilizing the IPC codes assigned to the patent documents and the patent applications/topics (Itoh 2005; Konishi 2005; Harris et al. 2011; Cetintas and Si 2012).",
            "Other researchers added more sophisticated features that also require the IPC codes of the patent application to be known before the prior-art search (Cetintas and Si 2012)."
          ],
          "intents": [
            "--",
            "--"
          ],
          "cited_paper_info": {
            "title": "Effective query generation and postprocessing strategies for prior art patent search",
            "abstract": "Rapid increase in global competition demands increased protection of intellectual property rights and underlines the importance of patents as major intellectual property documents. Prior art patent search is the task of identifying related patents for a given patent file, and is an essential step in judging the validity of a patent application. This article proposes an automated query generation and postprocessing method for prior art patent search. The proposed approach first constructs structured queries by combining terms extracted from different fields of a query patent and then reranks the retrieved patents by utilizing the International Patent Classification (IPC) code similarities between the query patent and the retrieved patents along with the retrieval score. An extensive set of empirical results carried out on a large-scale, real-world dataset shows that utilizing 20 or 30 query terms extracted from all fields of an original query patent according to their log(tf)idf values helps form a representative search query out of the query patent and is found to be more effective than is using any number of query terms from any single field. It is shown that combining terms extracted from different fields of the query patent by giving higher importance to terms extracted from the abstract, claims, and description fields than to terms extracted from the title field is more effective than treating all extracted terms equally while forming the search query. Finally, utilizing the similarities between the IPC codes of the query patent and retrieved patents is shown to be beneficial to improve the effectiveness of the prior art search. © 2012 Wiley Periodicals, Inc.",
            "year": 2012,
            "venue": "J. Assoc. Inf. Sci. Technol.",
            "authors": [
              {
                "authorId": "1778765",
                "name": "Suleyman Cetintas"
              },
              {
                "authorId": "145388187",
                "name": "Luo Si"
              }
            ]
          }
        },
        {
          "citedcorpusid": 10269743,
          "isinfluential": false,
          "contexts": [
            "Another widespread application of DIR methods is the digital library search systems (Buckland and Plaunt 1997; Larson 2003)."
          ],
          "intents": [
            "--"
          ],
          "cited_paper_info": {
            "title": "Selecting Libraries, Selecting Documents, Selecting Data",
            "abstract": "We use “selecting” as a general term for selection processes, including filtering, retrieval, routing, and searching. The search for recorded knowledge in a digital library environment is examined in terms of selection at three levels: 1. Selecting which library (repository) to look in; 2. Selecting which document(s) within a library to look at; and 3. Selecting fragments of data (text, numeric data, images) from within a document. These tasks with their differing problems have, historically, been treated as separate and different. Examination and comparison of these three processes reveal similarities and differences between the three levels. The three selecting processes are fundamentally the same in theory. The differences in practice are seen as arising from differing deficiencies in internal structure or lack of metadata. Identification of these deficiencies provides a basis for an agenda of research and development.",
            "year": 1997,
            "venue": "",
            "authors": [
              {
                "authorId": "1706786",
                "name": "M. Buckland"
              },
              {
                "authorId": "2302203",
                "name": "C. Plaunt"
              }
            ]
          }
        },
        {
          "citedcorpusid": 12711052,
          "isinfluential": true,
          "contexts": [
            "Due to its main characteristic CORI has been repeatedly reported in the literature (Powell and French 2003) not performing consistently well in environments containing a mix of ‘‘small’’ and ‘‘very large’’ document collections.",
            "In recent years, CLEF-IP has organized patent classification evaluation tasks and has provided to the researchers very large patent data to train their systems on more realistic data sets (Piroi et al. 2010, 2011, 2012).",
            "The topic collection contains 3973 query topics in English, German or French.",
            "In order to compare the performance of the collection selection methods we use the measure Rk proposed by French and Powell (2000).",
            "The patent documents have XML format and contain content in English, German or French (Piroi et al. 2011).",
            "There are a number of source selection approaches including CORI (Callan et al. 1995), gGlOSS (French et al. 1999), and others (Si et al. 2002), that characterize different collections using collection statistics like term frequencies."
          ],
          "intents": [
            "--",
            "--",
            "--",
            "--",
            "--",
            "--"
          ],
          "cited_paper_info": {
            "title": "CLEF-IP 2011: Retrieval in the Intellectual Property Domain",
            "abstract": "The patent system is designed to encourage disclosure of new technologies and novel ideas by granting exclusive rights on the use of inventions to their inventors, for a limited period of time. Before a patent can be granted, patent o ces around the world perform thorough searches to ensure that no previous similar disclosures were made. In the intellectual property terminology, such kind of searches are called prior art searches. In some industries, the number of granted patents a company owns has a high impact on the market value of the company. This underlines the importance of well-performed prior art searches. Together with the Trec Chem track [5], also organized by our institution, the Clef Ip e ort comes to complete the work that is being done in the series of Ntcir workshops (see for example [4]). The rst Clef Ip track ran within Clef 2009. The purpose of the track was twofold: to encourage and facilitate research in the area of patent retrieval by providing a large clean data set for experimentation; to create a large test collection of patents in the three main European languages for the evaluation of cross lingual information access. The Clef Ip data set includes documents published by the European Patent O ce (Epo) which contain a mixture of English, German and French content. The track focused on the task of prior art search. In 2010 and 2011, the Clef Ip track was organized as a benchmarking activity (lab) in the Clef conference. In these years, the main goal of the Clef Ip e ort remained the same to foster research in the patent retrieval area, and provide a large clean data set. To this end, the number of tasks in the track was increased and the data set was enlarged. Recognizing the importance of patent classi cations in the daily activity of an intellectual property professional, in 2010 the Clef Ip benchmarking activity included a patent classi cation task. The participants were asked to classify",
            "year": 2011,
            "venue": "Conference and Labs of the Evaluation Forum",
            "authors": [
              {
                "authorId": "3309646",
                "name": "Florina Piroi"
              },
              {
                "authorId": "144307089",
                "name": "M. Lupu"
              },
              {
                "authorId": "1699657",
                "name": "A. Hanbury"
              },
              {
                "authorId": "2978384",
                "name": "V. Zenz"
              }
            ]
          }
        },
        {
          "citedcorpusid": 13183982,
          "isinfluential": false,
          "contexts": [
            "Another widespread application of DIR methods is the digital library search systems (Buckland and Plaunt 1997; Larson 2003)."
          ],
          "intents": [
            "--"
          ],
          "cited_paper_info": {
            "title": "Research and Advanced Technology for Digital Libraries",
            "abstract": "",
            "year": 2001,
            "venue": "Lecture Notes in Computer Science",
            "authors": [
              {
                "authorId": "3000550",
                "name": "P. Constantopoulos"
              },
              {
                "authorId": "1398927405",
                "name": "Ingeborg Sølvberg"
              }
            ]
          }
        },
        {
          "citedcorpusid": 13514893,
          "isinfluential": false,
          "contexts": [
            "However, the effectiveness of their approach is examined on a much smaller collection as they used the English WIPO-alpha collection containing 75,250 patent documents.",
            "Additionally, it is very interesting that some DIR approaches managed to perform better than the centralized approach which is again consistent from our previous studies (Salampasis et al. 2012; Giachanou et al. 2013).",
            "As previously mentioned, we select CORI as the underlying source selection method because previous studies showed that it performs better than other collection selection methods (BordaFuse, Reciprocal Rank) when applied at the patent domain (Salampasis et al. 2012; Giachanou et al. 2013)."
          ],
          "intents": [
            "--",
            "--",
            "--"
          ],
          "cited_paper_info": {
            "title": "Multilayer Collection Selection and Search of Topically Organized Patents",
            "abstract": "We present a patent search system that explores three issues: (a) topical organization of patents based on their IPC, (b) collection selection of topically organised patent collections and (c) integration of collection selection tools to patent search systems. Patent documents produced worldwide have manually-assigned classification codes which in our work are used to cluster, distribute and index patents through hundreds or thousands of sub-collections. We propose a new collection selection method suitable for search systems having documents organized using hierarchical classification schemes such as IPC/CPC. The new method uses multiple evidence utilising, for each collection, the ranking of ancestors collections in higher level of the classification hierarchy. We tested our method on CLEF-IP 2011 and compared its performance to state-of-the-art collection selection algorithms. We also integrated this method as a component suggesting patent collections in the iPerFedPat patent search system.",
            "year": 2013,
            "venue": "",
            "authors": [
              {
                "authorId": "1786622",
                "name": "M. Salampasis"
              },
              {
                "authorId": "2280401616",
                "name": "Anastasia Giachanou"
              },
              {
                "authorId": "1718676",
                "name": "G. Paltoglou"
              }
            ]
          }
        },
        {
          "citedcorpusid": 13916954,
          "isinfluential": false,
          "contexts": [
            "Our work can be considered also relevant to a recent DIR work which aims to reduce the uncertainty in resource selection (Markov et al. 2013).",
            "Further to the previous explanations it should be said that generally resource selection is plagued by uncertainty (Markov et al. 2013) as it is usually based on limited information compared to centralized retrieval."
          ],
          "intents": [
            "--",
            "--"
          ],
          "cited_paper_info": {
            "title": "Reducing the Uncertainty in Resource Selection",
            "abstract": "",
            "year": 2013,
            "venue": "European Conference on Information Retrieval",
            "authors": [
              {
                "authorId": "145258872",
                "name": "Ilya Markov"
              },
              {
                "authorId": "1716332",
                "name": "L. Azzopardi"
              },
              {
                "authorId": "145876066",
                "name": "F. Crestani"
              }
            ]
          }
        },
        {
          "citedcorpusid": 14123572,
          "isinfluential": false,
          "contexts": [
            "(Larkey 1998; Kohonen et al. 2000; Fall et al. 2003; D’hondt et al. 2013), incorporating the hierarchy into the classification algorithm (Chakrabarti et al.",
            "…different techniques such as modifying and extending a conventional text classification algorithm in the context of the patent domain (Larkey 1998; Kohonen et al. 2000; Fall et al. 2003; D’hondt et al. 2013), incorporating the hierarchy into the classification algorithm (Chakrabarti et al. 1998;…"
          ],
          "intents": [
            "--",
            "--"
          ],
          "cited_paper_info": {
            "title": "Self organization of a massive document collection",
            "abstract": "This article describes the implementation of a system that is able to organize vast document collections according to textual similarities. It is based on the self-organizing map (SOM) algorithm. As the feature vectors for the documents statistical representations of their vocabularies are used. The main goal in our work has been to scale up the SOM algorithm to be able to deal with large amounts of high-dimensional data. In a practical experiment we mapped 6,840,568 patent abstracts onto a 1,002,240-node SOM. As the feature vectors we used 500-dimensional vectors of stochastic figures obtained as random projections of weighted word histograms.",
            "year": 2000,
            "venue": "IEEE Trans. Neural Networks Learn. Syst.",
            "authors": [
              {
                "authorId": "1688681",
                "name": "T. Kohonen"
              },
              {
                "authorId": "1711144",
                "name": "Samuel Kaski"
              },
              {
                "authorId": "2395884",
                "name": "K. Lagus"
              },
              {
                "authorId": "1839145",
                "name": "J. Salojärvi"
              },
              {
                "authorId": "2701836",
                "name": "Jukka Honkela"
              },
              {
                "authorId": "2607558",
                "name": "V. Paatero"
              },
              {
                "authorId": "2079443079",
                "name": "Antti Saarela"
              }
            ]
          }
        },
        {
          "citedcorpusid": 14148682,
          "isinfluential": false,
          "contexts": [
            "patent office and which is reported elsewhere in the literature (Giachanou et al. 2014).",
            "Finally we would like also to mention that a web-based tool implementing classification search using the multilayer method was evaluated in a user study conducted in a national\npatent office and which is reported elsewhere in the literature (Giachanou et al. 2014)."
          ],
          "intents": [
            "--",
            "--"
          ],
          "cited_paper_info": {
            "title": "A User-Centered Evaluation of a Web Based Patent Classification Tool",
            "abstract": "This paper presents a user-centered study of a web based system that can automatically suggest classification codes with the aim to assist patent examiners on the task of patent classification. The aim of the study is twofold. Firstly, we aim to obtain a better understanding of the search tactics patent examiners apply when they do classification search. Secondly, we examine the effect of searching at different levels of the classification scheme on classification search performance. For this user study, two conditions were tested. Both systems are web based. However, the two systems differ in their ability to allow patent examiners selecting the level from which the results will be returned. The results show that systems that allow searching at the level of subgroup are more effective for classification search.",
            "year": 2014,
            "venue": "MindTheGap@iConference",
            "authors": [
              {
                "authorId": "7407022",
                "name": "Anastasia Giahanou"
              },
              {
                "authorId": "1786622",
                "name": "M. Salampasis"
              },
              {
                "authorId": "66550501",
                "name": "M. Satratzemi"
              },
              {
                "authorId": "2171906",
                "name": "N. Samaras"
              }
            ]
          }
        },
        {
          "citedcorpusid": 14215145,
          "isinfluential": false,
          "contexts": [
            "In order to compare the performance of the collection selection methods we use the measure Rk proposed by French and Powell (2000)."
          ],
          "intents": [
            "--"
          ],
          "cited_paper_info": {
            "title": "Metrics for evaluating database selection techniques",
            "abstract": "",
            "year": 1999,
            "venue": "Proceedings. Tenth International Workshop on Database and Expert Systems Applications. DEXA 99",
            "authors": [
              {
                "authorId": "1713791",
                "name": "J. French"
              },
              {
                "authorId": "33968184",
                "name": "Allison L. Powell"
              }
            ]
          }
        },
        {
          "citedcorpusid": 14443484,
          "isinfluential": true,
          "contexts": [
            "…as three separate but interleaved sub-processes: Source representation, in which surrogates of the available remote collections are created (Callan and Connell 2001), source selection, in which a subset of the available information collections is chosen to process the query (Paltoglou et…",
            "The DIR process can be perceived as three separate but interleaved sub-processes: Source representation, in which surrogates of the available remote collections are created (Callan and Connell 2001), source selection, in which a subset of the available information collections is chosen to process the query (Paltoglou et al.",
            "Alternatively, statistics can be approximated by sampling uncooperative providers with a set of queries (Callan and Connell 2001)."
          ],
          "intents": [
            "--",
            "--",
            "--"
          ],
          "cited_paper_info": {
            "title": "Query-based sampling of text databases",
            "abstract": "",
            "year": 2001,
            "venue": "ACM Trans. Inf. Syst.",
            "authors": [
              {
                "authorId": "144987107",
                "name": "Jamie Callan"
              },
              {
                "authorId": "1735186",
                "name": "Margaret E. Connell"
              }
            ]
          }
        },
        {
          "citedcorpusid": 16468576,
          "isinfluential": false,
          "contexts": [
            "The superiority of CORI in the patent domain as collection selection method compared to BordaFuse and Reciprocal Rank is something consistent with our previous study (Salampasis et al. 2012).",
            "In the experiments described in this paper we use the CORI collection selection algorithm as it has been shown to be more effective than other collection selection algorithms, such as BordaFuse, ReciRank, ReDEE that were tested before (Salampasis et al. 2012).",
            "Additionally, it is very interesting that some DIR approaches managed to perform better than the centralized approach which is again consistent from our previous studies (Salampasis et al. 2012; Giachanou et al. 2013).",
            "because previous studies showed that it performs better than other collection selection methods (BordaFuse, Reciprocal Rank) when applied at the patent domain (Salampasis et al. 2012; Giachanou et al. 2013).",
            "As previously mentioned, we select CORI as the underlying source selection method because previous studies showed that it performs better than other collection selection methods (BordaFuse, Reciprocal Rank) when applied at the patent domain (Salampasis et al. 2012; Giachanou et al. 2013).",
            "…; Score Ci 12 ; . . .; Score Ci 1n ð2Þ\nIn the experiments described in this paper we use the CORI collection selection algorithm as it has been shown to be more effective than other collection selection algorithms, such as BordaFuse, ReciRank, ReDEE that were tested before (Salampasis et al. 2012).",
            "In this paper, we extend our previous work of applying DIR methods to topically organized patents (Salampasis et al. 2012)."
          ],
          "intents": [
            "--",
            "--",
            "--",
            "--",
            "--",
            "--",
            "--"
          ],
          "cited_paper_info": {
            "title": "Report on the CLEF-IP 2012 Experiments: Search of Topically Organized Patents",
            "abstract": "This technical report presents the work which has been carried out using Distributed Information Retrieval methods for federated search of patent documents for the passage retrieval starting from claims (patentability or novelty search) task. Patent documents produced worldwide have manuallyassigned classification codes which in our work are used to cluster, distribute and index patents through hundreds or thousands of sub-collections. We tested different combinations of source selection (CORI, BordaFuse, Reciprocal Rank) and results merging algorithms (SSL, CORI). We also tested different combinations of the number of collections requested and documents retrieved from each collection. One of the aims of the experiments was to test older DIR methods that characterize different collections using collection statistics like term frequencies and how they perform in patent search. Also to experiment with newer DIR methods which focus on explicitly estimating the number of relevant documents in each collection and usually attain improvements in precision over previous approaches, but their recall is usually lower. However, the most important aim was to examine how DIR methods will perform if patents are topically organized using their IPC and if DIR methods can approximate the performance of a centralized index approach. We submitted 8 runs. According to PRES @100 our best DIR approach ranked 7 across 31 submitted results, however our best DIR (not submitted) run outperforms all submitted runs.",
            "year": 2012,
            "venue": "Conference and Labs of the Evaluation Forum",
            "authors": [
              {
                "authorId": "1786622",
                "name": "M. Salampasis"
              },
              {
                "authorId": "1718676",
                "name": "G. Paltoglou"
              },
              {
                "authorId": "7407022",
                "name": "Anastasia Giahanou"
              }
            ]
          }
        },
        {
          "citedcorpusid": 17755188,
          "isinfluential": false,
          "contexts": [
            "In our study, we use the Inquery algorithm (Allan et al. 2000) as it is implemented in the Lemur toolkit."
          ],
          "intents": [
            "--"
          ],
          "cited_paper_info": {
            "title": "INQUERY and TREC-7",
            "abstract": "ipated in only four of the tracks that were part of the TREC-7 workshop. We worked on ad-hoc retrieval, ltering, VLC, and the SDR track. This report covers the work done on each track successively. We start with a discussion of IR tools that were broadly applied in our work. Although UMass used a wide range of tools, from Unix shell scripts, to PC spreadsheets, three major tools were applied across almost all tracks: the Inquery search engine, the InRoute ltering engine, and a a query expansion technique known as LCA. This section provides a brief overview of each of those so that the discussion does not have to repeated for each track. 1.1 Inquery All tracks other than the ltering track used Inqueryy6] as the search engine, sometimes for training, and always for generating the nal ranked lists for the test. We used Inquery V3.2, an in-house development version of the Inquery system made available by the CIIR (V3.1). The diierences between the two are not consequential for this study. The current belief function used by Inquery to calculate the belief in term t within document d is: w t;d = 0:4 + 0:6 tf t;d tf t;d + 0:5 + 1:5 length(d) avg len log N+0:5 nt log N + 1 where n t is the number of documents containing term t, N is the number of documents in the collection, \\avg len\" is the average length (in words) of documents in the collection, length(d) is the length (in words) of document d, and tf t;d is the number of times term t occurs in document d.",
            "year": 1998,
            "venue": "Text Retrieval Conference",
            "authors": [
              {
                "authorId": "144890574",
                "name": "James Allan"
              },
              {
                "authorId": "144987107",
                "name": "Jamie Callan"
              },
              {
                "authorId": "144721996",
                "name": "M. Sanderson"
              },
              {
                "authorId": "2362587",
                "name": "Jinxi Xu"
              },
              {
                "authorId": "144301186",
                "name": "Steven Wegmann"
              }
            ]
          }
        },
        {
          "citedcorpusid": 18345205,
          "isinfluential": false,
          "contexts": [
            "…in a very detailed and purposeful assignment process, something which is very different by the creation of sub-collections using automated clustering algorithms or the naive division method by chronological or source order, a division method which has been extensively used in past DIR research.",
            "…a number of different techniques such as modifying and extending a conventional text classification algorithm in the context of the patent domain (Larkey 1998; Kohonen et al. 2000; Fall et al. 2003; D’hondt et al. 2013), incorporating the hierarchy into the classification algorithm (Chakrabarti…"
          ],
          "intents": [
            "--",
            "--"
          ],
          "cited_paper_info": {
            "title": "Some Issues in the Automatic Classification of US Patents",
            "abstract": "Abstract : The classification of US patents poses some special problems due to the enormous size of the corpus, the size and complex hierarchical structure of the classification system, and the size and structure of patent documents. The representation of the complex structure of documents has not been a standard area of research in text categorization, but we have found it to be an important factor in our previous work on classifying patient medical records (Larkey and Croft, 1996) and in our current work on US patents. Our classification approach is to combine the results of k-nearest-neighbor classifiers with those of Bayesian classifiers. The k-nearest-neighbor classifier allows us to represent the document structure using the query operators in the Inquery information retrieval system. The Bayesian classifiers can use the hierarchical relations among patent subclasses to select closely related negative examples to train more discriminating classifiers.",
            "year": 1997,
            "venue": "",
            "authors": [
              {
                "authorId": "1742457",
                "name": "L. Larkey"
              }
            ]
          }
        },
        {
          "citedcorpusid": 18463704,
          "isinfluential": true,
          "contexts": [
            "Tikk et al. (2007) presented a hierarchical online classifier called HITEC algorithm which applies a neural network with the aim to utilize the hierarchical structure of patent taxonomy.",
            "Note that a subgroup may have more refined subgroups (i.e. defining 6th, 7th level etc. at the IPC hierarchy).",
            "…of the patent domain (Larkey 1998; Kohonen et al. 2000; Fall et al. 2003; D’hondt et al. 2013), incorporating the hierarchy into the classification algorithm (Chakrabarti et al. 1998; Cai and Hofmann 2004; Tikk et al. 2007) or using linguistic analysis (Gey et al. 2001; D’hondt et al. 2013).",
            "Also, patents are published electronically using a strict technical form and structure (Adams 2010)."
          ],
          "intents": [
            "--",
            "--",
            "--",
            "--"
          ],
          "cited_paper_info": {
            "title": "A Hierarchical Online Classifier for Patent Categorization",
            "abstract": "Patent categorization (PC) is a typical application area of text categorization (TC). TC can be applied in different scenarios at the work of patent offices depending on at what stage the categorization is needed. This is a challenging field for TC algorithms, since the applications have to deal simultaneously with large number of categories (in the magnitude of 1000–10000) organized in hierarchy, large number of long documents with huge vocabularies at training, and they are required to work fast and accurate at on-the-fly categorization. In this paper we present a hierarchical online classifier, called HITEC, which meets the above requirements. The novelty of the method lies in the taxonomy dependent architecture of the classifier, the applied weight updating scheme, and in the relaxed category selection method. We evaluate the presented method on two large English patent application databases, the WIPO-alpha and the Espace A/B corpora. We also compare the method to other TC algorithms on these collections, and show that it outperforms them significantly.",
            "year": 2007,
            "venue": "",
            "authors": [
              {
                "authorId": "1754164",
                "name": "D. Tikk"
              },
              {
                "authorId": "47277666",
                "name": "G. Biró"
              },
              {
                "authorId": "2893823",
                "name": "A. Törcsvári"
              }
            ]
          }
        },
        {
          "citedcorpusid": 18596979,
          "isinfluential": false,
          "contexts": [
            "…the professionals, there is a feeling that intelligent search tools ‘‘that no longer just do what you say but also what you mean’’ (Wolter 2012) can and must help and this feeling is reflected in various studies (e.g. Bonino et al. 2010) and evaluation campaigns (e.g. Roda et al. 2009; Lupu 2011)."
          ],
          "intents": [
            "--"
          ],
          "cited_paper_info": {
            "title": "The status of retrieval evaluation in the patent domain",
            "abstract": "",
            "year": 2011,
            "venue": "Patent Information Retrieval",
            "authors": [
              {
                "authorId": "144307089",
                "name": "M. Lupu"
              }
            ]
          }
        },
        {
          "citedcorpusid": 20487815,
          "isinfluential": false,
          "contexts": [
            "Generally PRES was preferred in patent retrieval evaluation tasks (Piroi and Zenz 2011; Piroi et al. 2012)."
          ],
          "intents": [
            "--"
          ],
          "cited_paper_info": {
            "title": "Evaluating Information Retrieval in the Intellectual Property Domain: The Clef-Ip Campaign",
            "abstract": "",
            "year": 2011,
            "venue": "Current Challenges in Patent Information Retrieval",
            "authors": [
              {
                "authorId": "3309646",
                "name": "Florina Piroi"
              },
              {
                "authorId": "2978384",
                "name": "V. Zenz"
              }
            ]
          }
        },
        {
          "citedcorpusid": 33391284,
          "isinfluential": false,
          "contexts": [
            "In recent years, CLEF-IP has organized patent classification evaluation tasks and has provided to the researchers very large patent data to train their systems on more realistic data sets (Piroi et al. 2010, 2011, 2012).",
            "Generally PRES was preferred in patent retrieval evaluation tasks (Piroi and Zenz 2011; Piroi et al. 2012)."
          ],
          "intents": [
            "--",
            "--"
          ],
          "cited_paper_info": {
            "title": "CLEF 2012 Evaluation Labs and Workshop, Online Working Notes, Rome, Italy, September 17-20, 2012",
            "abstract": "",
            "year": 2014,
            "venue": "Conference and Labs of the Evaluation Forum",
            "authors": [
              {
                "authorId": "3309646",
                "name": "Florina Piroi"
              },
              {
                "authorId": "144307089",
                "name": "M. Lupu"
              },
              {
                "authorId": "1699657",
                "name": "A. Hanbury"
              },
              {
                "authorId": "1824934",
                "name": "A. Sexton"
              },
              {
                "authorId": "1745226",
                "name": "Walid Magdy"
              },
              {
                "authorId": "1713472",
                "name": "I. V. Filippov"
              }
            ]
          }
        },
        {
          "citedcorpusid": 33433994,
          "isinfluential": false,
          "contexts": [
            "Although there is no clear distinction between the terms DIR and federated search (Shokouhi and Si 2011), the latter represents a more realistic DIR scenario that allows the simultaneous search of multiple searchable, remote and physically distributed, resources. ezDL is probably the most…"
          ],
          "intents": [
            "--"
          ],
          "cited_paper_info": {
            "title": "Federated Search",
            "abstract": "",
            "year": 2009,
            "venue": "Encyclopedia of Database Systems",
            "authors": [
              {
                "authorId": "2285421643",
                "name": "lokaler Katalog"
              }
            ]
          }
        },
        {
          "citedcorpusid": 51578296,
          "isinfluential": false,
          "contexts": [
            "For example, Lupu and Hanbury (2013) in a recent review of patent retrieval present a typical prior art search use case."
          ],
          "intents": [
            "--"
          ],
          "cited_paper_info": {
            "title": "Patent Retrieval",
            "abstract": "Intellectual property and the patent system in particular have been extremely present in research and discussion, even in the public media, in the last few years. Without going into any controversial issues regarding the patent system, we approach a very real and growing problem: searching for innovation. The target collection for this task does not consist of patent documents only, but it is in these documents that the main difference is found compared to web or news information retrieval. In addition, the issue of patent search implies a particular user model and search process model. This review is concerned with how research and technology in the field of Information Retrieval assists or even changes the processes of patent search. It is a survey of work done on patent data in relation to Information Retrieval in the last 20–25 years. It explains the sources of difficulty and the existing document processing and retrieval methods of the domain, and provides a motivation for further research in the area.",
            "year": 2013,
            "venue": "Foundations and Trends in Information Retrieval",
            "authors": [
              {
                "authorId": "144307089",
                "name": "M. Lupu"
              },
              {
                "authorId": "1699657",
                "name": "A. Hanbury"
              }
            ]
          }
        },
        {
          "citedcorpusid": 62174397,
          "isinfluential": false,
          "contexts": [
            "As a result, even among the professionals, there is a feeling that intelligent search tools ‘‘that no longer just do what you say but also what you mean’’ (Wolter 2012) can and must help and this feeling is reflected in various studies (e.",
            "…a result, even among the professionals, there is a feeling that intelligent search tools ‘‘that no longer just do what you say but also what you mean’’ (Wolter 2012) can and must help and this feeling is reflected in various studies (e.g. Bonino et al. 2010) and evaluation campaigns (e.g. Roda et…"
          ],
          "intents": [
            "--",
            "--"
          ],
          "cited_paper_info": {
            "title": "It takes all kinds to make a world – Some thoughts on the use of classification in patent searching",
            "abstract": "",
            "year": 2012,
            "venue": "",
            "authors": [
              {
                "authorId": "122245974",
                "name": "B. Wolter"
              }
            ]
          }
        },
        {
          "citedcorpusid": 62191434,
          "isinfluential": false,
          "contexts": [
            "In prior art search probably the most important filter is based on the International Patent Classification (IPC or CPC)(1) classification (Vijvers 1990; Adams 2000).",
            "In prior art search probably the most important filter is based on the International Patent Classification (IPC or CPC)1 classification (Vijvers 1990; Adams 2000)."
          ],
          "intents": [
            "--",
            "--"
          ],
          "cited_paper_info": {
            "title": "Using the International Patent Classification in an online environment",
            "abstract": "",
            "year": 2000,
            "venue": "",
            "authors": [
              {
                "authorId": "144478848",
                "name": "S. Adams"
              }
            ]
          }
        },
        {
          "citedcorpusid": 62217932,
          "isinfluential": false,
          "contexts": [
            "Search technologies have been used for professional search (e.g. patent, medical, scientific literature search) for more than 40 years now as an important method for information access (Adams 2010).",
            "Also, patents are published electronically using a strict technical form and structure (Adams 2010)."
          ],
          "intents": [
            "--",
            "--"
          ],
          "cited_paper_info": {
            "title": "The text, the full text and nothing but the text: Part 1 – Standards for creating textual information in patent documents and general search implications ☆",
            "abstract": "",
            "year": 2010,
            "venue": "",
            "authors": [
              {
                "authorId": "144478848",
                "name": "S. Adams"
              }
            ]
          }
        },
        {
          "citedcorpusid": 62570235,
          "isinfluential": false,
          "contexts": [
            "…the professionals, there is a feeling that intelligent search tools ‘‘that no longer just do what you say but also what you mean’’ (Wolter 2012) can and must help and this feeling is reflected in various studies (e.g. Bonino et al. 2010) and evaluation campaigns (e.g. Roda et al. 2009; Lupu 2011)."
          ],
          "intents": [
            "--"
          ],
          "cited_paper_info": {
            "title": "Review of the state-of-the-art in patent information and forthcoming evolutions in intelligent patent informatics",
            "abstract": "",
            "year": 2010,
            "venue": "",
            "authors": [
              {
                "authorId": "1806604",
                "name": "Dario Bonino"
              },
              {
                "authorId": "2008278",
                "name": "A. Ciaramella"
              },
              {
                "authorId": "1712546",
                "name": "Fulvio Corno"
              }
            ]
          }
        },
        {
          "citedcorpusid": 109342149,
          "isinfluential": false,
          "contexts": [
            "Of course there are good reasons for this, apart from the psychological inertia produced by every successful scientific method exercised for a long period of time (Loh et al. 2006)."
          ],
          "intents": [
            "--"
          ],
          "cited_paper_info": {
            "title": "Automatic classification of patent documents for TRIZ users",
            "abstract": "",
            "year": 2006,
            "venue": "",
            "authors": [
              {
                "authorId": "2935248",
                "name": "H. Loh"
              },
              {
                "authorId": "2115065574",
                "name": "Cong He"
              },
              {
                "authorId": "3332817",
                "name": "Lixiang Shen"
              }
            ]
          }
        },
        {
          "citedcorpusid": 111108875,
          "isinfluential": false,
          "contexts": [
            "In prior art search probably the most important filter is based on the International Patent Classification (IPC or CPC)(1) classification (Vijvers 1990; Adams 2000).",
            "In prior art search probably the most important filter is based on the International Patent Classification (IPC or CPC)1 classification (Vijvers 1990; Adams 2000)."
          ],
          "intents": [
            "--",
            "--"
          ],
          "cited_paper_info": {
            "title": "The international patent classification as a search tool",
            "abstract": "",
            "year": 1990,
            "venue": "",
            "authors": [
              {
                "authorId": "98359798",
                "name": "W. Vijvers"
              }
            ]
          }
        },
        {
          "citedcorpusid": 120323383,
          "isinfluential": false,
          "contexts": [
            "An interesting observation is that the influence factor parameter, in broad terms, can be seen as the k parameter in the k-Nearest Neighbors algorithm (k-NN). k-NN introduced by Fix and Hodges (1951) is one of the simplest and most popular classification algorithms."
          ],
          "intents": [
            "--"
          ],
          "cited_paper_info": {
            "title": "Discriminatory Analysis - Nonparametric Discrimination: Consistency Properties",
            "abstract": "Abstract : The discrimination problem (two population case) may be defined as follows: e random variable Z, of observed value z, is distributed over some space (say, p-dimensional) either according to distribution F, or according to distribution G. The problem is to decide, on the basis of z, which of the two distributions Z has.",
            "year": 1989,
            "venue": "",
            "authors": [
              {
                "authorId": "47358212",
                "name": "E. Fix"
              },
              {
                "authorId": "2541903",
                "name": "J. Hodges"
              }
            ]
          }
        },
        {
          "citedcorpusid": 207226010,
          "isinfluential": false,
          "contexts": [
            "One of the first attempts to incorporate the hierarchy into the categorization algorithm was made by Chakrabarti et al. (1998) who performed some small-scale tests based on a Bayesian hierarchical patent classification system which could classify the patents into 12 subclasses organized in three…",
            "…of the patent domain (Larkey 1998; Kohonen et al. 2000; Fall et al. 2003; D’hondt et al. 2013), incorporating the hierarchy into the classification algorithm (Chakrabarti et al. 1998; Cai and Hofmann 2004; Tikk et al. 2007) or using linguistic analysis (Gey et al. 2001; D’hondt et al. 2013).",
            "2013), incorporating the hierarchy into the classification algorithm (Chakrabarti et al. 1998; Cai and Hofmann 2004; Tikk et al. 2007) or using linguistic analysis (Gey et al."
          ],
          "intents": [
            "--",
            "--",
            "--"
          ],
          "cited_paper_info": {
            "title": "Enhanced hypertext categorization using hyperlinks",
            "abstract": "",
            "year": 1998,
            "venue": "ACM SIGMOD Conference",
            "authors": [
              {
                "authorId": "40941894",
                "name": "Soumen Chakrabarti"
              },
              {
                "authorId": "1786444",
                "name": "B. Dom"
              },
              {
                "authorId": "1688317",
                "name": "P. Indyk"
              }
            ]
          }
        },
        {
          "citedcorpusid": 218592387,
          "isinfluential": false,
          "contexts": [
            "…in a very detailed and purposeful assignment process, something which is very different by the creation of sub-collections using automated clustering algorithms or the naive division method by chronological or source order, a division method which has been extensively used in past DIR research."
          ],
          "intents": [
            "--"
          ],
          "cited_paper_info": {
            "title": "Text Representations for Patent Classification",
            "abstract": "",
            "year": 2013,
            "venue": "International Conference on Computational Logic",
            "authors": [
              {
                "authorId": "1447164726",
                "name": "E.K.L. D'hondt"
              },
              {
                "authorId": "1702730",
                "name": "S. Verberne"
              },
              {
                "authorId": "1713642",
                "name": "C. Koster"
              },
              {
                "authorId": "1728633",
                "name": "L. Boves"
              }
            ]
          }
        }
      ]
    },
    "250408264": {
      "citing_paper_info": {
        "title": "The Harvard USPTO Patent Dataset: A Large-Scale, Well-Structured, and Multi-Purpose Corpus of Patent Applications",
        "abstract": "Innovation is a major driver of economic and social development, and information about many kinds of innovation is embedded in semi-structured data from patents and patent applications. Although the impact and novelty of innovations expressed in patent data are difficult to measure through traditional means, ML offers a promising set of techniques for evaluating novelty, summarizing contributions, and embedding semantics. In this paper, we introduce the Harvard USPTO Patent Dataset (HUPD), a large-scale, well-structured, and multi-purpose corpus of English-language patent applications filed to the United States Patent and Trademark Office (USPTO) between 2004 and 2018. With more than 4.5 million patent documents, HUPD is two to three times larger than comparable corpora. Unlike previously proposed patent datasets in NLP, HUPD contains the inventor-submitted versions of patent applications--not the final versions of granted patents--thereby allowing us to study patentability at the time of filing using NLP methods for the first time. It is also novel in its inclusion of rich structured metadata alongside the text of patent filings: By providing each application's metadata along with all of its text fields, the dataset enables researchers to perform new sets of NLP tasks that leverage variation in structured covariates. As a case study on the types of research HUPD makes possible, we introduce a new task to the NLP community--namely, binary classification of patent decisions. We additionally show the structured metadata provided in the dataset enables us to conduct explicit studies of concept shifts for this task. Finally, we demonstrate how HUPD can be used for three additional tasks: multi-class classification of patent subject areas, language modeling, and summarization.",
        "year": 2022,
        "venue": "Neural Information Processing Systems",
        "authors": [
          {
            "authorId": "51903517",
            "name": "Mirac Suzgun"
          },
          {
            "authorId": "2268317740",
            "name": "Luke Melas-Kyriazi"
          },
          {
            "authorId": "152970139",
            "name": "Suproteem K. Sarkar"
          },
          {
            "authorId": "1794750",
            "name": "S. Kominers"
          },
          {
            "authorId": "1692491",
            "name": "Stuart M. Shieber"
          }
        ]
      },
      "citation_summary": {
        "citation_count": 27,
        "unique_cited_count": 21,
        "influential_count": 8,
        "detailed_records_count": 27
      },
      "cited_papers": [
        "277550631",
        "233583623",
        "195792769",
        "230435736",
        "22139415",
        "182953211",
        "110408470",
        "229955249",
        "112011152",
        "52273103",
        "233296858",
        "1238927",
        "198953378",
        "522219",
        "235166857",
        "204838007",
        "869093",
        "203626972",
        "211574327",
        "280198324",
        "203639485"
      ],
      "citation_details": [
        {
          "citedcorpusid": 522219,
          "isinfluential": false,
          "contexts": [
            "Previous studies attempted to predict the IPC or CPC codes of patents at the class and subclass levels using various statistical methods, including classical statistical learning tools [24, 5, 8, 50, 16] and neural architectures [18, 28, 59]."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Supervised Approaches to Assign Cooperative Patent Classification (CPC) Codes to Patents",
            "abstract": "",
            "year": 2017,
            "venue": "International Conference on Mining Intelligence and Knowledge Exploration",
            "authors": [
              {
                "authorId": "144828812",
                "name": "Tung Tran"
              },
              {
                "authorId": "1711213",
                "name": "Ramakanth Kavuluru"
              }
            ]
          }
        },
        {
          "citedcorpusid": 869093,
          "isinfluential": false,
          "contexts": [
            "These results are consistent with the ﬁndings of Bell et al. [30]."
          ],
          "intents": [
            "['result']"
          ],
          "cited_paper_info": {
            "title": "Who Becomes an Inventor in America? The Importance of Exposure to Innovation",
            "abstract": "We characterize the factors that determine who becomes an inventor in America by using de-identified data on 1.2 million inventors from patent records linked to tax records. We establish three sets of results. First, children from high-income (top 1%) families are ten times as likely to become inventors as those from below-median income families. There are similarly large gaps by race and gender. Differences in innate ability, as measured by test scores in early childhood, explain relatively little of these gaps. Second, exposure to innovation during childhood has significant causal effects on children's propensities to become inventors. Growing up in a neighborhood or family with a high innovation rate in a specific technology class leads to a higher probability of patenting in exactly the same technology class. These exposure effects are gender-specific: girls are more likely to become inventors in a particular technology class if they grow up in an area with more female inventors in that technology class. Third, the financial returns to inventions are extremely skewed and highly correlated with their scientific impact, as measured by citations. Consistent with the importance of exposure effects and contrary to standard models of career selection, women and disadvantaged youth are as under-represented among high-impact inventors as they are among inventors as a whole. We develop a simple model of inventors' careers that matches these empirical results. The model implies that increasing exposure to innovation in childhood may have larger impacts on innovation than increasing the financial incentives to innovate, for instance by reducing tax rates. In particular, there are many \"lost Einsteins\" - individuals who would have had highly impactful inventions had they been exposed to innovation.",
            "year": 2017,
            "venue": "Quarterly Journal of Economics",
            "authors": [
              {
                "authorId": "123272174",
                "name": "Alex Bell"
              },
              {
                "authorId": "50976894",
                "name": "Raj Chetty"
              },
              {
                "authorId": "73822534",
                "name": "Xavier Jaravel"
              },
              {
                "authorId": "94735281",
                "name": "Neviana Petkova"
              },
              {
                "authorId": "49475665",
                "name": "J. Van Reenen"
              }
            ]
          }
        },
        {
          "citedcorpusid": 1238927,
          "isinfluential": true,
          "contexts": [
            "WikiBio [10] is also a large scientiﬁc corpus that is worth mentioning: It contains almost 0.73 million unique biographies of famous people extracted from English Wikipedia, where each biography contains the ﬁrst paragraph of the article and the infobox (fact table)."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Neural Text Generation from Structured Data with Application to the Biography Domain",
            "abstract": "This paper introduces a neural model for concept-to-text generation that scales to large, rich domains. We experiment with a new dataset of biographies from Wikipedia that is an order of magnitude larger than existing resources with over 700k samples. The dataset is also vastly more diverse with a 400k vocabulary, compared to a few hundred words for Weathergov or Robocup. Our model builds upon recent work on conditional neural language model for text generation. To deal with the large vocabulary, we extend these models to mix a fixed vocabulary with copy actions that transfer sample-specific words from the input database to the generated output sentence. Our neural model significantly out-performs a classical Kneser-Ney language model adapted to this task by nearly 15 BLEU.",
            "year": 2016,
            "venue": "Conference on Empirical Methods in Natural Language Processing",
            "authors": [
              {
                "authorId": "2875254",
                "name": "R. Lebret"
              },
              {
                "authorId": "2529182",
                "name": "David Grangier"
              },
              {
                "authorId": "2325985",
                "name": "Michael Auli"
              }
            ]
          }
        },
        {
          "citedcorpusid": 22139415,
          "isinfluential": false,
          "contexts": [
            "[18] Mattyws F Grawe, Claudia A Martins, and Andreia G Bonfante.",
            "Previous studies attempted to predict the IPC or CPC codes of patents at the class and subclass levels using various statistical methods, including classical statistical learning tools [24, 5, 8, 50, 16] and neural architectures [18, 28, 59]."
          ],
          "intents": [
            "['background']",
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Automated Patent Classification Using Word Embedding",
            "abstract": "",
            "year": 2017,
            "venue": "International Conference on Machine Learning and Applications",
            "authors": [
              {
                "authorId": "35272389",
                "name": "Mattyws F. Grawe"
              },
              {
                "authorId": "144097599",
                "name": "C. A. Martins"
              },
              {
                "authorId": "2078352187",
                "name": "Andreia Gentil Bonfante"
              }
            ]
          }
        },
        {
          "citedcorpusid": 52273103,
          "isinfluential": true,
          "contexts": [
            "In recent years, there have been some efforts to address this limitation, but all the patent corpora in NLP so far, including CLEF-IP 2011 [38], USPTO-2M [28], and BIGPATENT [46], are still limited in their scopes and features, as shown in Table 1.",
            "10 As shown in Table 1, WIPO-alpha, CLEF-IP, and USPTO-2M have been the main gymnasia for model training for the IPC/CPC classiﬁcation tasks, but these corpora are still limited in their scopes.",
            "Previous studies attempted to predict the IPC or CPC codes of patents at the class and subclass levels using various statistical methods, including classical statistical learning tools [24, 5, 8, 50, 16] and neural architectures [18, 28, 59].",
            "[28]) that the abstract alone contains useful information about the appropriate principal technology area the patent application might belong to.",
            "In recent years, there have been efforts to produce NLP datasets of patent text, including CLEF-IP 2011 [2], USPTO-2M [3], and B IG P ATENT [4]."
          ],
          "intents": [
            "['background']",
            "--",
            "['methodology']",
            "['background']",
            "--"
          ],
          "cited_paper_info": {
            "title": "DeepPatent: patent classification with convolutional neural networks and word embedding",
            "abstract": "",
            "year": 2018,
            "venue": "Scientometrics",
            "authors": [
              {
                "authorId": "2124883266",
                "name": "Shaobo Li"
              },
              {
                "authorId": "145815844",
                "name": "Jie Hu"
              },
              {
                "authorId": "5925243",
                "name": "Yuxin Cui"
              },
              {
                "authorId": "50778791",
                "name": "Jianjun Hu"
              }
            ]
          }
        },
        {
          "citedcorpusid": 110408470,
          "isinfluential": false,
          "contexts": [
            "[27] Mark A Lemley and Bhaven N Sampat.",
            "(6)We refer our readers to the study by Lemley and Sampat [27] for a discussion on data controversies around calculating patent grant rates."
          ],
          "intents": [
            "['background']",
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Is the Patent Office a Rubber Stamp?",
            "abstract": "A growing chorus of voices is sounding a common refrain - the U.S. Patent and Trademark Office (PTO) is issuing far too many bad patents. These criticisms are complicated by the rather surprising fact that we don't actually know what percentage of patent applications actually issue as patents. In this paper, we use a novel dataset of all published patent applications filed in January 2001 to estimate the grant rate. These data also allow us to examine the uses of continuation applications, and to assess dynamics of applicant-examiner interaction over the patent prosecution process. We find that the PTO rejects a surprisingly high percentage of patents. While more than two-thirds of all applications result in at least one patent, a significant number of applications are rejected and then finally abandoned by the applicant. We also find that the likelihood of obtaining a patent varies significantly by industry, but in surprising ways. Finally, despite a variety of reforms that might be thought to reduce the use and abuse of continuation applications, we find a high use of continuation applications of various types.",
            "year": 2008,
            "venue": "",
            "authors": [
              {
                "authorId": "1751962",
                "name": "Mark A. Lemley"
              },
              {
                "authorId": "2076533",
                "name": "B. Sampat"
              }
            ]
          }
        },
        {
          "citedcorpusid": 112011152,
          "isinfluential": false,
          "contexts": [
            "Filing metadata—including acceptance decisions, ﬁling dates, titles, and classiﬁcation information—were separately obtained from the USPTO Patent Examination Research Datasets [26, 27] in February 2021."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "The USPTO Patent Examination Research Dataset: A Window on the Process of Patent Examination",
            "abstract": "A surprisingly small amount of empirical research has been focused on the process of obtaining a patent grant from the United States Patent and Trademark Office (PTO). The purpose of this document is to describe the Patent Examination Dataset (PatEX), make a large amount of information from the Public Patent Application Information Retrieval system (Public PAIR) more readily available to researchers. PatEX includes records on over 9 million US patent applications, with information complete as of January 24, 2015 for all applications included in Public PAIR with filing dates prior to January 1, 2015. Variables in PatEX cover most of the relevant information related to US patent examination, including characteristics of inventions, applications, applicants, attorneys, and examiners, and status codes for all actions taken, by both the applicant and examiner, throughout the examination process. A significant section of this documentation describes the selectivity issues that arise from the omission of “nonpublic” applications. We find that the selection issues were much more pronounced for applications received prior to the implementation of the American Inventors Protection Act (AIPA) in late 2000. We also find that the extent of any selection bias will be at least partially determined by the sub-population of interest in any given research project.",
            "year": 2015,
            "venue": "",
            "authors": [
              {
                "authorId": "49052817",
                "name": "Stuart J. H. Graham"
              },
              {
                "authorId": "144571706",
                "name": "Alan C. Marco"
              },
              {
                "authorId": "2110527173",
                "name": "Richard D. Miller"
              }
            ]
          }
        },
        {
          "citedcorpusid": 182953211,
          "isinfluential": true,
          "contexts": [
            "Sharma et al. [4] initiated such explorations, introducing the ﬁrst summarization dataset on patents, called B IG P ATENT , and trained summarization tools on their dataset to generate the abstract section of a patent given its description section.",
            "It is natural to compare HUPD to one of the most widely-used existing patent datasets in NLP, B IG P ATENT [4].",
            "We perform this conditional generation task with the same motivation that inspired Sharma et al. [4]; our setup is similar to theirs apart from the size and scope of our dataset.",
            "In recent years, there have been efforts to produce NLP datasets of patent text, including CLEF-IP 2011 [2], USPTO-2M [3], and B IG P ATENT [4].",
            "One difference in our current setup is that our dataset allows us to explore using either the claims or description section for the source text, whereas in Sharma et al. [4] only the description section is available."
          ],
          "intents": [
            "['methodology']",
            "['background']",
            "['methodology']",
            "['background']",
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "BIGPATENT: A Large-Scale Dataset for Abstractive and Coherent Summarization",
            "abstract": "Most existing text summarization datasets are compiled from the news domain, where summaries have a flattened discourse structure. In such datasets, summary-worthy content often appears in the beginning of input articles. Moreover, large segments from input articles are present verbatim in their respective summaries. These issues impede the learning and evaluation of systems that can understand an article’s global content structure as well as produce abstractive summaries with high compression ratio. In this work, we present a novel dataset, BIGPATENT, consisting of 1.3 million records of U.S. patent documents along with human written abstractive summaries. Compared to existing summarization datasets, BIGPATENT has the following properties: i) summaries contain a richer discourse structure with more recurring entities, ii) salient content is evenly distributed in the input, and iii) lesser and shorter extractive fragments are present in the summaries. Finally, we train and evaluate baselines and popular learning models on BIGPATENT to shed light on new challenges and motivate future directions for summarization research.",
            "year": 2019,
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "2059341935",
                "name": "Eva Sharma"
              },
              {
                "authorId": "2116521802",
                "name": "Chen Li"
              },
              {
                "authorId": "2153516659",
                "name": "Lu Wang"
              }
            ]
          }
        },
        {
          "citedcorpusid": 195792769,
          "isinfluential": false,
          "contexts": [
            "Recently, Transformers [19] have been considered for this task: Lee and Hsiang [20], for instance, ﬁne-tuned a pre-trained BERT [21] to predict IPC/CPC codes of patents."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Patent classification by fine-tuning BERT language model",
            "abstract": "",
            "year": 2020,
            "venue": "World Patent Information",
            "authors": [
              {
                "authorId": "2108319166",
                "name": "Jieh-Sheng Lee"
              },
              {
                "authorId": "1798127",
                "name": "J. Hsiang"
              }
            ]
          }
        },
        {
          "citedcorpusid": 198953378,
          "isinfluential": true,
          "contexts": [
            "Our baselines consisted of various subsets of Bernoulli and Multinomial naive Bayes classifiers, logistic regression (Logistic), CNN, DistilBERT [44], BERT [9], DistilROBERTa [44], RoBERTa [29], and T5-Small [40] for different tasks.",
            "Initially, we trained individual domain-speciﬁc classiﬁers, ranging from NB classiﬁers to RoBERTa, to predict the acceptability of patent applications in the most common IPC subclasses (see Figure 2).",
            "Our baselines for this task consisted of various subsets of Bernoulli and Multinomial naive Bayes classifiers, logistic regression (Logistic), CNN, DistilBERT [44], BERT [9], DistilROBERTa [44], RoBERTa [29], and T5-Small [40] for different tasks.",
            "[58] conducted similar experiments using their BIGBIRD model [29] and showed improvements over BERT models.",
            "We performed masked language modeling with DistilRoBERTa [36] (82M parameters), initializing with a model pretrained on OpenWebText [37].",
            "Our baselines for this task consisted of various subsets of Bernoulli and Multinomial naive Bayes classiﬁers, logistic regression (Logistic), CNN, DistilBERT [36], BERT [21], DistilROBERTa [36], RoBERTa [23], and T5-Small [54] for different tasks.",
            "Using our custom DistilRoBERTa model, we computed embedding vectors for the abstracts of each patent application and then reduced the dimensionality of these vectors using UMAP.",
            "[29] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov."
          ],
          "intents": [
            "['methodology']",
            "--",
            "['methodology']",
            "['methodology']",
            "--",
            "--",
            "--",
            "['background']"
          ],
          "cited_paper_info": {
            "title": "RoBERTa: A Robustly Optimized BERT Pretraining Approach",
            "abstract": "Language model pretraining has led to significant performance gains but careful comparison between different approaches is challenging. Training is computationally expensive, often done on private datasets of different sizes, and, as we will show, hyperparameter choices have significant impact on the final results. We present a replication study of BERT pretraining (Devlin et al., 2019) that carefully measures the impact of many key hyperparameters and training data size. We find that BERT was significantly undertrained, and can match or exceed the performance of every model published after it. Our best model achieves state-of-the-art results on GLUE, RACE and SQuAD. These results highlight the importance of previously overlooked design choices, and raise questions about the source of recently reported improvements. We release our models and code.",
            "year": 2019,
            "venue": "arXiv.org",
            "authors": [
              {
                "authorId": "11323179",
                "name": "Yinhan Liu"
              },
              {
                "authorId": "40511414",
                "name": "Myle Ott"
              },
              {
                "authorId": "39589154",
                "name": "Naman Goyal"
              },
              {
                "authorId": "3048577",
                "name": "Jingfei Du"
              },
              {
                "authorId": "144863691",
                "name": "Mandar Joshi"
              },
              {
                "authorId": "50536468",
                "name": "Danqi Chen"
              },
              {
                "authorId": "39455775",
                "name": "Omer Levy"
              },
              {
                "authorId": "35084211",
                "name": "M. Lewis"
              },
              {
                "authorId": "1982950",
                "name": "Luke Zettlemoyer"
              },
              {
                "authorId": "1759422",
                "name": "Veselin Stoyanov"
              }
            ]
          }
        },
        {
          "citedcorpusid": 203626972,
          "isinfluential": true,
          "contexts": [
            "Our baselines consisted of various subsets of Bernoulli and Multinomial naive Bayes classifiers, logistic regression (Logistic), CNN, DistilBERT [44], BERT [9], DistilROBERTa [44], RoBERTa [29], and T5-Small [40] for different tasks.",
            "Our baselines for this task consisted of various subsets of Bernoulli and Multinomial naive Bayes classifiers, logistic regression (Logistic), CNN, DistilBERT [44], BERT [9], DistilROBERTa [44], RoBERTa [29], and T5-Small [40] for different tasks.",
            "We performed masked language modeling with DistilRoBERTa [36] (82M parameters), initializing with a model pretrained on OpenWebText [37].",
            "[44] Victor Sanh, Lysandre Debut, Julien Chaumond, and Thomas Wolf.",
            "We performed masked language modeling with DistilRoBERTa [44] (82M parameters), initializing with",
            "Our baselines for this task consisted of various subsets of Bernoulli and Multinomial naive Bayes classiﬁers, logistic regression (Logistic), CNN, DistilBERT [36], BERT [21], DistilROBERTa [36], RoBERTa [23], and T5-Small [54] for different tasks.",
            "Using our custom DistilRoBERTa model, we computed embedding vectors for the abstracts of each patent application and then reduced the dimensionality of these vectors using UMAP."
          ],
          "intents": [
            "['methodology']",
            "['methodology']",
            "--",
            "['background']",
            "['methodology']",
            "--",
            "--"
          ],
          "cited_paper_info": {
            "title": "DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter",
            "abstract": "As Transfer Learning from large-scale pre-trained models becomes more prevalent in Natural Language Processing (NLP), operating these large models in on-the-edge and/or under constrained computational training or inference budgets remains challenging. In this work, we propose a method to pre-train a smaller general-purpose language representation model, called DistilBERT, which can then be fine-tuned with good performances on a wide range of tasks like its larger counterparts. While most prior work investigated the use of distillation for building task-specific models, we leverage knowledge distillation during the pre-training phase and show that it is possible to reduce the size of a BERT model by 40%, while retaining 97% of its language understanding capabilities and being 60% faster. To leverage the inductive biases learned by larger models during pre-training, we introduce a triple loss combining language modeling, distillation and cosine-distance losses. Our smaller, faster and lighter model is cheaper to pre-train and we demonstrate its capabilities for on-device computations in a proof-of-concept experiment and a comparative on-device study.",
            "year": 2019,
            "venue": "arXiv.org",
            "authors": [
              {
                "authorId": "51918868",
                "name": "Victor Sanh"
              },
              {
                "authorId": "1380459402",
                "name": "Lysandre Debut"
              },
              {
                "authorId": "40811585",
                "name": "Julien Chaumond"
              },
              {
                "authorId": "50335211",
                "name": "Thomas Wolf"
              }
            ]
          }
        },
        {
          "citedcorpusid": 203639485,
          "isinfluential": false,
          "contexts": [
            "Previous studies attempted to predict the IPC or CPC codes of patents at the class and subclass levels using various statistical methods, including classical statistical learning tools [24, 5, 8, 50, 16] and neural architectures [18, 28, 59]."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Analysis of the effect of data properties in automated patent classification",
            "abstract": "",
            "year": 2019,
            "venue": "Scientometrics",
            "authors": [
              {
                "authorId": "144004546",
                "name": "J. Gómez"
              }
            ]
          }
        },
        {
          "citedcorpusid": 204838007,
          "isinfluential": true,
          "contexts": [
            "Our baselines consisted of various subsets of Bernoulli and Multinomial naive Bayes classifiers, logistic regression (Logistic), CNN, DistilBERT [44], BERT [9], DistilROBERTa [44], RoBERTa [29], and T5-Small [40] for different tasks.",
            "Table 10: Examples of claims summaries produced by our T5-Small model.",
            "Our baselines for this task consisted of various subsets of Bernoulli and Multinomial naive Bayes classifiers, logistic regression (Logistic), CNN, DistilBERT [44], BERT [9], DistilROBERTa [44], RoBERTa [29], and T5-Small [40] for different tasks.",
            "[40] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu.",
            "Our baselines for this task consisted of various subsets of Bernoulli and Multinomial naive Bayes classiﬁers, logistic regression (Logistic), CNN, DistilBERT [36], BERT [21], DistilROBERTa [36], RoBERTa [23], and T5-Small [54] for different tasks.",
            "We used the T5-Small “Text-to-Text Transformer” architecture of Raffel et al. [54] with 60 million parameters."
          ],
          "intents": [
            "['methodology']",
            "--",
            "['methodology']",
            "['background']",
            "--",
            "--"
          ],
          "cited_paper_info": {
            "title": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer",
            "abstract": "Transfer learning, where a model is first pre-trained on a data-rich task before being fine-tuned on a downstream task, has emerged as a powerful technique in natural language processing (NLP). The effectiveness of transfer learning has given rise to a diversity of approaches, methodology, and practice. In this paper, we explore the landscape of transfer learning techniques for NLP by introducing a unified framework that converts every language problem into a text-to-text format. Our systematic study compares pre-training objectives, architectures, unlabeled datasets, transfer approaches, and other factors on dozens of language understanding tasks. By combining the insights from our exploration with scale and our new \"Colossal Clean Crawled Corpus\", we achieve state-of-the-art results on many benchmarks covering summarization, question answering, text classification, and more. To facilitate future work on transfer learning for NLP, we release our dataset, pre-trained models, and code.",
            "year": 2019,
            "venue": "Journal of machine learning research",
            "authors": [
              {
                "authorId": "2402716",
                "name": "Colin Raffel"
              },
              {
                "authorId": "1846258",
                "name": "Noam M. Shazeer"
              },
              {
                "authorId": "145625142",
                "name": "Adam Roberts"
              },
              {
                "authorId": "3844009",
                "name": "Katherine Lee"
              },
              {
                "authorId": "46617804",
                "name": "Sharan Narang"
              },
              {
                "authorId": "1380243217",
                "name": "Michael Matena"
              },
              {
                "authorId": "2389316",
                "name": "Yanqi Zhou"
              },
              {
                "authorId": "2157338362",
                "name": "Wei Li"
              },
              {
                "authorId": "35025299",
                "name": "Peter J. Liu"
              }
            ]
          }
        },
        {
          "citedcorpusid": 211574327,
          "isinfluential": true,
          "contexts": [
            "Another concurrent work by Saier and Färber [42] introduced the unarXive dataset, a collection of over one million academic papers with links to almost 2.7 million unique publications; however, this dataset is not as comprehensive and well-structured as S2ORC or our dataset."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "unarXive: a large scholarly data set with publications’ full-text, annotated in-text citations, and links to metadata",
            "abstract": "In recent years, scholarly data sets have been used for various purposes, such as paper recommendation, citation recommendation, citation context analysis, and citation context-based document summarization. The evaluation of approaches to such tasks and their applicability in real-world scenarios heavily depend on the used data set. However, existing scholarly data sets are limited in several regards. In this paper, we propose a new data set based on all publications from all scientific disciplines available on arXiv.org. Apart from providing the papers’ plain text, in-text citations were annotated via global identifiers. Furthermore, citing and cited publications were linked to the Microsoft Academic Graph, providing access to rich metadata. Our data set consists of over one million documents and 29.2 million citation contexts. The data set, which is made freely available for research purposes, not only can enhance the future evaluation of research paper-based and citation context-based approaches, but also serve as a basis for new ways to analyze in-text citations, as we show prototypically in this article.",
            "year": 2020,
            "venue": "Scientometrics",
            "authors": [
              {
                "authorId": "104447782",
                "name": "T. Saier"
              },
              {
                "authorId": "151112226",
                "name": "Michael Färber"
              }
            ]
          }
        },
        {
          "citedcorpusid": 229955249,
          "isinfluential": false,
          "contexts": [
            "This initiative aims to enhance the quality of applications without putting any further financial burden on the shoulders of patent applicants who might have limited resources and means, as well as educating inventors about intellectual property protection and the patent filing process [41]."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Introducing the Pro Se Assistance Program",
            "abstract": "Independent inventors and small business owners have a long tradition of creating innovative products and opening up new sectors of the American ma",
            "year": 2016,
            "venue": "",
            "authors": [
              {
                "authorId": "67303477",
                "name": "Inventorseye Newsletter"
              }
            ]
          }
        },
        {
          "citedcorpusid": 230435736,
          "isinfluential": false,
          "contexts": [
            "Given the size of HUPD (350GB of raw text), it is also possible to compare it to the extremely large-scale NLP corpora currently used for language model pretraining, such as Colossal Clean Crawled Corpus (C4) [47], the Pile [48], and OpenWebText [37]."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "The Pile: An 800GB Dataset of Diverse Text for Language Modeling",
            "abstract": "Recent work has demonstrated that increased training dataset diversity improves general cross-domain knowledge and downstream generalization capability for large-scale language models. With this in mind, we present the Pile : an 825 GiB English text corpus tar-geted at training large-scale language models. The Pile is constructed from 22 diverse high-quality subsets—both existing and newly constructed—many of which derive from academic or professional sources. Our evaluation of the untuned performance of GPT-2 and GPT-3 on the Pile shows that these models struggle on many of its components, such as academic writing. Conversely, models trained on the Pile improve signiﬁcantly over both Raw CC and CC-100 on all components of the Pile, while improving performance on downstream evaluations. Through an in-depth exploratory analysis, we document potentially concerning aspects of the data for prospective users. We make publicly available the code used in its construction. 1",
            "year": 2020,
            "venue": "arXiv.org",
            "authors": [
              {
                "authorId": "2027599537",
                "name": "Leo Gao"
              },
              {
                "authorId": "103476203",
                "name": "Stella Biderman"
              },
              {
                "authorId": "2044098905",
                "name": "Sid Black"
              },
              {
                "authorId": "2044198157",
                "name": "Laurence Golding"
              },
              {
                "authorId": "47000911",
                "name": "Travis Hoppe"
              },
              {
                "authorId": "2064610125",
                "name": "Charles Foster"
              },
              {
                "authorId": "80842917",
                "name": "Jason Phang"
              },
              {
                "authorId": "46350295",
                "name": "Horace He"
              },
              {
                "authorId": "2044198037",
                "name": "Anish Thite"
              },
              {
                "authorId": "2044198503",
                "name": "Noa Nabeshima"
              },
              {
                "authorId": "2037326180",
                "name": "Shawn Presser"
              },
              {
                "authorId": "2044198134",
                "name": "Connor Leahy"
              }
            ]
          }
        },
        {
          "citedcorpusid": 233296858,
          "isinfluential": false,
          "contexts": [
            "Furthermore, Dodge et al. [49] found that this patent text was not clean: A signiﬁcant percentage was machine-translated from non-English languages and/or extracted from images with OCR.",
            "In fact, a recent analysis of the C4 dataset [49] found that “patents.google.com” is the single most-frequent source of text in the corpus, as measured by number of tokens."
          ],
          "intents": [
            "['background']",
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Documenting the English Colossal Clean Crawled Corpus",
            "abstract": "As language models are trained on ever more text, researchers are turning to some of the largest corpora available. Unlike most other types of datasets in NLP, large unlabeled text corpora are often presented with minimal documentation, and best practices for documenting them have not been established. In this work we provide the ﬁrst documentation for the Colossal Clean Crawled Corpus (C4; Raf-fel et al., 2020), a dataset created by applying a set of ﬁlters to a single snapshot of Common Crawl. We begin with a high-level summary of the data, including distributions of where the text came from and when it was written. We then give more detailed analysis on salient parts of this data, including the most frequent sources of text (e.g. patents.google.com , which contains a signiﬁcant percentage of machine translated and/or OCR’d text), the effect that the ﬁlters had on the data (they disproportionately remove text in AAE), and evidence that some other benchmark NLP dataset examples are contained in the text. We release a web interface to an interactive, indexed copy of this dataset, encouraging the community to continuously explore and report additional ﬁndings.",
            "year": 2021,
            "venue": "arXiv.org",
            "authors": [
              {
                "authorId": "34176020",
                "name": "Jesse Dodge"
              },
              {
                "authorId": "2729164",
                "name": "Maarten Sap"
              },
              {
                "authorId": "3451494",
                "name": "Ana Marasović"
              },
              {
                "authorId": "2301202406",
                "name": "William Agnew"
              },
              {
                "authorId": "1387994137",
                "name": "Gabriel Ilharco"
              },
              {
                "authorId": "3458736",
                "name": "Dirk Groeneveld"
              },
              {
                "authorId": "40642935",
                "name": "Matt Gardner"
              }
            ]
          }
        },
        {
          "citedcorpusid": 233583623,
          "isinfluential": false,
          "contexts": [
            "[23] Ralf Krestel, Renukswamy Chikkamath, Christoph Hewel, and Julian Risch."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "A survey on deep learning for patent analysis",
            "abstract": "",
            "year": 2021,
            "venue": "World Patent Information",
            "authors": [
              {
                "authorId": "3264110",
                "name": "Ralf Krestel"
              },
              {
                "authorId": "2047942384",
                "name": "Renukswamy Chikkamath"
              },
              {
                "authorId": "2042640458",
                "name": "Christoph Hewel"
              },
              {
                "authorId": "1695993",
                "name": "Julian Risch"
              }
            ]
          }
        },
        {
          "citedcorpusid": 235166857,
          "isinfluential": false,
          "contexts": [
            "The WikiBio dataset has been historically used for table-to-text generation [36], but it has been also adopted for question-answering [45].",
            "[45] Lei Sha, Patrick Hohenecker, and Thomas Lukasiewicz."
          ],
          "intents": [
            "['methodology']",
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Controlling Text Edition by Changing Answers of Specific Questions",
            "abstract": "In this paper, we introduce the new task of controllable text edition, in which we take as input a long text, a question, and a target answer, and the output is a minimally modified text, so that it fits the target answer. This task is very important in many situations, such as changing some conditions, consequences, or properties in a legal document, or changing some key information of an event in a news text. This is very challenging, as it is hard to obtain a parallel corpus for training, and we need to first find all text positions that should be changed and then decide how to change them. We constructed the new dataset WikiBioCTE for this task based on the existing dataset WikiBio (originally created for table-to-text generation). We use WikiBioCTE for training, and manually labeled a test set for testing. We also propose novel evaluation metrics and a novel method for solving the new task. Experimental results on the test set show that our proposed method is a good fit for this novel NLP task.",
            "year": 2021,
            "venue": "Findings",
            "authors": [
              {
                "authorId": "39058310",
                "name": "Lei Sha"
              },
              {
                "authorId": "15991062",
                "name": "Patrick Hohenecker"
              },
              {
                "authorId": "1690572",
                "name": "Thomas Lukasiewicz"
              }
            ]
          }
        },
        {
          "citedcorpusid": 277550631,
          "isinfluential": true,
          "contexts": [
            "(21)We used scikit-learn [37] to train and evaluate our NB classifiers and HuggingFace’s Transformers codebase [57] to implement and finetune our Transformers.",
            "We used scikit-learn [37] to train and evaluate our NB classifiers and HuggingFace’s Transformers codebase [57] to implement and finetune our Transformers.",
            "[57] Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz, et al."
          ],
          "intents": [
            "--",
            "--",
            "--"
          ],
          "cited_paper_info": {
            "title": "HuggingFace's Transformers: State-of-the-art Natural Language Processing",
            "abstract": "Recent progress in natural language processing has been driven by advances in both model architecture and model pretraining. Transformer architectures have facilitated building higher-capacity models and pretraining has made it possible to effectively utilize this capacity for a wide variety of tasks. \\textit{Transformers} is an open-source library with the goal of opening up these advances to the wider machine learning community. The library consists of carefully engineered state-of-the art Transformer architectures under a unified API. Backing this library is a curated collection of pretrained models made by and available for the community. \\textit{Transformers} is designed to be extensible by researchers, simple for practitioners, and fast and robust in industrial deployments. The library is available at \\url{this https URL}.",
            "year": 2019,
            "venue": "arXiv.org",
            "authors": [
              {
                "authorId": "2257007291",
                "name": "Thomas Wolf"
              },
              {
                "authorId": "1380459402",
                "name": "Lysandre Debut"
              },
              {
                "authorId": "51918868",
                "name": "Victor Sanh"
              },
              {
                "authorId": "40811585",
                "name": "Julien Chaumond"
              },
              {
                "authorId": "40899333",
                "name": "Clement Delangue"
              },
              {
                "authorId": "1382164294",
                "name": "Anthony Moi"
              },
              {
                "authorId": "1382164165",
                "name": "Pierric Cistac"
              },
              {
                "authorId": "1382164170",
                "name": "Tim Rault"
              },
              {
                "authorId": "2185329",
                "name": "Rémi Louf"
              },
              {
                "authorId": "97662964",
                "name": "Morgan Funtowicz"
              },
              {
                "authorId": "48776237",
                "name": "Joe Davison"
              },
              {
                "authorId": "88728159",
                "name": "Sam Shleifer"
              },
              {
                "authorId": "138609838",
                "name": "Patrick von Platen"
              },
              {
                "authorId": "2257128341",
                "name": "Clara Ma"
              },
              {
                "authorId": "2268491803",
                "name": "Yacine Jernite"
              },
              {
                "authorId": "3008389",
                "name": "J. Plu"
              },
              {
                "authorId": "2257127518",
                "name": "Canwen Xu"
              },
              {
                "authorId": "1379806208",
                "name": "Teven Le Scao"
              },
              {
                "authorId": "103682620",
                "name": "Sylvain Gugger"
              },
              {
                "authorId": "2125818054",
                "name": "Mariama Drame"
              },
              {
                "authorId": "2113836945",
                "name": "Quentin Lhoest"
              },
              {
                "authorId": "2260132137",
                "name": "Alexander M. Rush"
              }
            ]
          }
        },
        {
          "citedcorpusid": 280198324,
          "isinfluential": false,
          "contexts": [
            "Prior to 2001, patent applications to the USPTO were not published: They were instead kept in secrecy until their patent issue dates (p. 174; Menell et al. [6])."
          ],
          "intents": [
            "--"
          ],
          "cited_paper_info": {
            "title": "Intellectual Property in the New Technological Age: 2020 Volume I – Perspectives, Trade Secrets, and Patents",
            "abstract": "",
            "year": 2020,
            "venue": "",
            "authors": [
              {
                "authorId": "2928101",
                "name": "Peter S. Menell"
              },
              {
                "authorId": "1751962",
                "name": "Mark A. Lemley"
              },
              {
                "authorId": "67201559",
                "name": "R. Merges"
              },
              {
                "authorId": "97553551",
                "name": "Shyamkrishna Balganesh"
              }
            ]
          }
        },
        {
          "citedcorpusid": null,
          "isinfluential": false,
          "contexts": [
            "The collection contains bibliographic information on more than 90 million patent documents from 17 countries, in addition to the full textual content of millions of US patent documents, provided by IFI CLAIMS Patent Services [56].",
            "The collection contains bibliographic information on more than 90 million patent documents from 17 countries, in addition to the full textual content of millions of US patent documents, provided by IFI CLAIMS Patent Services [50]."
          ],
          "intents": [
            "['background']",
            "--"
          ],
          "cited_paper_info": {}
        },
        {
          "citedcorpusid": null,
          "isinfluential": false,
          "contexts": [
            "The collection contains bibliographic information on more than 90 million patent documents from 17 countries, in addition to the full textual content of millions of US patent documents, provided by IFI CLAIMS Patent Services [50]."
          ],
          "intents": [
            "--"
          ],
          "cited_paper_info": {}
        },
        {
          "citedcorpusid": null,
          "isinfluential": false,
          "contexts": [
            "In the ﬁscal year 2020 alone, the USPTO received more than 650,000 patent ﬁlings, including requests for continued examinations [1]."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {}
        },
        {
          "citedcorpusid": null,
          "isinfluential": false,
          "contexts": [
            "[49] A Toole, N Pairolero, A Giczy, J Forman, C Pulliam, M Such, K Chaki, D Orange, A Thomas Homescu, J Frumkin, et al.",
            "[49], we define a “patent application” as a non-provisional application for an invention and a “patent” as a granted patent."
          ],
          "intents": [
            "--",
            "--"
          ],
          "cited_paper_info": {}
        },
        {
          "citedcorpusid": null,
          "isinfluential": false,
          "contexts": [
            "The Social Security Administration data contains names obtained from Social Security card applications for births that occurred in the United States between 1880 and 2020 (inclusive) [39]."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {}
        },
        {
          "citedcorpusid": null,
          "isinfluential": false,
          "contexts": [
            "4 Following the terminology used by Toole et al. [5], we deﬁne a “patent application” as a non-provisional application for an invention and a “patent” as a granted patent."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {}
        }
      ]
    },
    "237580469": {
      "citing_paper_info": {
        "title": "Identifying the Technology Convergence Using Patent Text Information: A Graph Convolutional Networks (GCN)-Based Approach",
        "abstract": "The potential for new values and products created by technology convergence to disruptively transform existing industries and markets is high. In this regard, it has been crucial for companies to understand and identify potential convergence patterns as early as possible to make timely strategic plans. This study proposes a new semantic method by showing how a graph convolutional network model can be used to monitor technology convergence. In particular, the model is trained to generate patents and technology keyword vectors from which new indicators are derived. We validate these new indicators and show that the proposed method outperforms existing studies using information regarding cross-citations and co-occurrence of international patent classification classes. Furthermore, we presented the usefulness of the proposed method to monitor technology convergence using a case study of the convergence between artificial intelligence (AI) and distributed ledger technology (DLT). The results show that convergence between AI and DLT is driven mainly by employing AI for DLT, and the role of each keyword (sub-domain) in the convergence process is also presented.",
        "year": 2022,
        "venue": "Social Science Research Network",
        "authors": [
          {
            "authorId": "2141025858",
            "name": "Chengzheng Zhu"
          },
          {
            "authorId": "2635231",
            "name": "K. Motohashi"
          }
        ]
      },
      "citation_summary": {
        "citation_count": 3,
        "unique_cited_count": 3,
        "influential_count": 0,
        "detailed_records_count": 3
      },
      "cited_papers": [
        "154980469",
        "27204330",
        "21864683"
      ],
      "citation_details": [
        {
          "citedcorpusid": 21864683,
          "isinfluential": false,
          "contexts": [
            "…(Curran and Leker, 2011; Karvonen and Kässi, 2013; Preschitschek et al., 2013; Ko et al., 2014; Kim et al., 2014; Passing and Moehrle, 2015; Zhou et al., 2019; Eilers et al., 2019) and forecasting future convergence trends (Kim and Lee, 2017; Kim et al., 2019; Lee et al., 2020; Kim and Sohn, 2020).",
            "Regarding the citation-based method (Geum et al., 2012; Kim et al., 2014; Kim and Lee, 2017; Zhou et al., 2019), patent citation information can be used as a proxy for knowledge flows, revealing convergence mechanisms in different technological domains (Ko et al., 2014); an increasing beyond-domain…",
            "Kim and Lee (2017) created a dependency-structure matrix in a very similar way; however, instead of identifying historical trends, they conducted a time-series analysis using neural networks to predict future convergence."
          ],
          "intents": [
            "['background']",
            "['methodology']",
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Forecasting and identifying multi-technology convergence based on patent data: the case of IT and BT industries in 2020",
            "abstract": "",
            "year": 2017,
            "venue": "Scientometrics",
            "authors": [
              {
                "authorId": "2116321079",
                "name": "Jeeeun Kim"
              },
              {
                "authorId": "38648012",
                "name": "Sungjoon Lee"
              }
            ]
          }
        },
        {
          "citedcorpusid": 27204330,
          "isinfluential": false,
          "contexts": [
            "In the era of the third industrial revolution (digitalization) and the fourth industrial revolution (smartization or intelligentization), which has entailed a high-dynamic business environment, open innovation and technology convergence are promising in providing new technology, market, and job opportunities (Hang Sik Park, 2017).",
            "…revolution (digitalization) and the fourth industrial revolution (smartization or intelligentization), which has entailed a high-dynamic business environment, open innovation and technology convergence are promising in providing new technology, market, and job opportunities (Hang Sik Park, 2017).",
            ", 2019) as follows: (1) emerging technologies might not be counted in the current classification scheme; (2) classification codes are assigned to a patent until it is granted; and (3) the issue of time lag, i."
          ],
          "intents": [
            "--",
            "['background']",
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Technology convergence, open innovation, and dynamic economy",
            "abstract": "The world economy has been confronting low economic growth for several years. Many experts agree that concepts such as openness, convergence, and creation of new market demand through new emerging technologies (e.g. Internet of Things, big data, and Artificial Intelligence) may solve the current economic crisis throughout the world. When these concepts are linked to a network, the law of increasing returns will come true. As the issue of the 4th industrial revolution mentioned in the 2016 World Economic Forum is similar, the enlargement of open innovation and convergence will lead to a new dynamic economy and sustainable development.",
            "year": 2017,
            "venue": "",
            "authors": [
              {
                "authorId": "121416712",
                "name": "Hang-Sik Park"
              }
            ]
          }
        },
        {
          "citedcorpusid": 154980469,
          "isinfluential": false,
          "contexts": [
            ", 2019) and (2) methods built on the basis of Word2Vec or Doc2Vec, which generate technological field representations by simple aggregation of word and patent vectors, are insufficient to capture the semantic relationships between patents and technological fields.",
            ", 2019) as follows: (1) emerging technologies might not be counted in the current classification scheme; (2) classification codes are assigned to a patent until it is granted; and (3) the issue of time lag, i.",
            "The proposed method solved the abovementioned problems through (1) vectorizing the patents and technology keywords using the entire patent abstracts, and (2) training the vectors on the basis of bibliometric graph structures, which are edges connecting patents and technology keywords."
          ],
          "intents": [
            "['methodology']",
            "['background']",
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Patent indicators for monitoring convergence - examples from NFF and ICT",
            "abstract": "",
            "year": 2011,
            "venue": "",
            "authors": [
              {
                "authorId": "46970618",
                "name": "C. Curran"
              },
              {
                "authorId": "2615126",
                "name": "J. Leker"
              }
            ]
          }
        }
      ]
    },
    "257365382": {
      "citing_paper_info": {
        "title": "Multi label classification of Artificial Intelligence related patents using Modified D2SBERT and Sentence Attention mechanism",
        "abstract": "Patent classification is an essential task in patent information management and patent knowledge mining. It is very important to classify patents related to artificial intelligence, which is the biggest topic these days. However, artificial intelligence-related patents are very difficult to classify because it is a mixture of complex technologies and legal terms. Moreover, due to the unsatisfactory performance of current algorithms, it is still mostly done manually, wasting a lot of time and money. Therefore, we present a method for classifying artificial intelligence-related patents published by the USPTO using natural language processing technique and deep learning methodology. We use deformed BERT and sentence attention overcome the limitations of BERT. Our experiment result is highest performance compared to other deep learning methods.",
        "year": 2023,
        "venue": "arXiv.org",
        "authors": [
          {
            "authorId": "2087142583",
            "name": "Yongmin Yoo"
          },
          {
            "authorId": "1644077770",
            "name": "Tak-Sung Heo"
          },
          {
            "authorId": "2150295260",
            "name": "D. Lim"
          },
          {
            "authorId": "2186115288",
            "name": "Deaho Seo"
          }
        ]
      },
      "citation_summary": {
        "citation_count": 11,
        "unique_cited_count": 10,
        "influential_count": 1,
        "detailed_records_count": 11
      },
      "cited_papers": [
        "218592387",
        "155400052",
        "12941012",
        "231718841",
        "235262768",
        "247996537",
        "158586458",
        "154775533",
        "237600744",
        "18295873"
      ],
      "citation_details": [
        {
          "citedcorpusid": 12941012,
          "isinfluential": false,
          "contexts": [
            "In addition to classification of easily accessible texts such as news and books [14,15], more specialized and complex legal documents [16,17,18,19] and studies on the classification of patient medical records [20,21,22], etc."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "A novel text mining approach based on TF-IDF and Support Vector Machine for news classification",
            "abstract": "",
            "year": 2016,
            "venue": "IEEE International Conference on Engineering and Technology",
            "authors": [
              {
                "authorId": "30633445",
                "name": "S. M. Dadgar"
              },
              {
                "authorId": "31405663",
                "name": "Mohammad Shirzad Araghi"
              },
              {
                "authorId": "31166923",
                "name": "M. M. Farahani"
              }
            ]
          }
        },
        {
          "citedcorpusid": 18295873,
          "isinfluential": false,
          "contexts": [
            "It is also very useful for technology roadmaps [2,3,4] technology forecasting [5,6,7], technology trend analysis [8,9,10], and furthermore, market marketing driven by technological change [11,12]."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Emerging Technology Forecasting Using New Patent Information Analysis",
            "abstract": "Emerging technology drives technological development and innovation in diverse fields of technology. Emerging technology forecasting can predict the possible areas of emerging technology. However, it is difficult to forecast the emerging technology because most technology forecasting tasks depend on the subjective experience of experts. Patent analysis is an objective method to recognize the trends in technological development. Many patent analysis methods have been researched; these methods apply text mining techniques to analyze the text data of patent documents such as the title and abstract. This approach has some limitations, namely the computing cost and information loss associated with the preprocessing step of text mining. Therefore, we propose a new patent information analysis to overcome these problems. Using the International Patent Classification codes from the patent documents of a target technology, we construct an emerging technology forecasting model. This research combines statistical inference and neural networks to construct our model for new patent information analysis. We perform a case study to verify how our research can be practically applied, using nanotechnology as the target technology. Therefore, we contribute this research to R&D planning.",
            "year": 2012,
            "venue": "",
            "authors": [
              {
                "authorId": "40590513",
                "name": "Sunghae Jun"
              },
              {
                "authorId": "2108154922",
                "name": "Seung-Joo Lee"
              }
            ]
          }
        },
        {
          "citedcorpusid": 154775533,
          "isinfluential": false,
          "contexts": [
            "It is also very useful for technology roadmaps [2,3,4] technology forecasting [5,6,7], technology trend analysis [8,9,10], and furthermore, market marketing driven by technological change [11,12]."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Strategic planning for technology development with patent analysis",
            "abstract": "Patents contain much technological information. This paper demonstrates that patent analysis facilitates a unique scheme for technology development. Two cases are employed to illustrate the effectiveness of a patent database. The first case provides strategic planning for developing Light Emitting Diode (LED) material technology. The second case, involving Thin Film Transistor (TFT), displays that the development trend in related patents corresponds to the technology roadmap in the industrial sector. To reduce the uncertainties of solely employing patent data, this study also incorporates academic journal findings and industrial information. Results presented herein demonstrate that patents can function not only as a map for tracking the technology trajectory, but also a guidepost for technology planning and forecasting.",
            "year": 1997,
            "venue": "",
            "authors": [
              {
                "authorId": "48641448",
                "name": "Shang-Jyh Liu"
              },
              {
                "authorId": "2048653418",
                "name": "Joenson Shyu"
              }
            ]
          }
        },
        {
          "citedcorpusid": 155400052,
          "isinfluential": false,
          "contexts": [
            "It is also very useful for technology roadmaps [2,3,4] technology forecasting [5,6,7], technology trend analysis [8,9,10], and furthermore, market marketing driven by technological change [11,12]."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "A Technology Valuation Model Using Quantitative Patent Analysis: A Case Study of Technology Transfer in Big Data Marketing",
            "abstract": "",
            "year": 2015,
            "venue": "",
            "authors": [
              {
                "authorId": "40590513",
                "name": "Sunghae Jun"
              },
              {
                "authorId": "1730690",
                "name": "Sangsung Park"
              },
              {
                "authorId": "1679368",
                "name": "Dong-Sik Jang"
              }
            ]
          }
        },
        {
          "citedcorpusid": 158586458,
          "isinfluential": false,
          "contexts": [
            "It is also very useful for technology roadmaps [2,3,4] technology forecasting [5,6,7], technology trend analysis [8,9,10], and furthermore, market marketing driven by technological change [11,12]."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Forecasting emerging technologies: A supervised learning approach through patent analysis",
            "abstract": "",
            "year": 2017,
            "venue": "",
            "authors": [
              {
                "authorId": "8749100",
                "name": "Moses Ntanda Kyebambe"
              },
              {
                "authorId": "50012920",
                "name": "Ge Cheng"
              },
              {
                "authorId": "2108862615",
                "name": "Y. Huang"
              },
              {
                "authorId": "2140520517",
                "name": "Chunhui He"
              },
              {
                "authorId": "2229011211",
                "name": "Zhenyu Zhang"
              }
            ]
          }
        },
        {
          "citedcorpusid": 218592387,
          "isinfluential": false,
          "contexts": [
            "In addition to classification of easily accessible texts such as news and books [14,15], more specialized and complex legal documents [16,17,18,19] and studies on the classification of patient medical records [20,21,22], etc."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Text Representations for Patent Classification",
            "abstract": "",
            "year": 2013,
            "venue": "International Conference on Computational Logic",
            "authors": [
              {
                "authorId": "1447164726",
                "name": "E.K.L. D'hondt"
              },
              {
                "authorId": "1702730",
                "name": "S. Verberne"
              },
              {
                "authorId": "1713642",
                "name": "C. Koster"
              },
              {
                "authorId": "1728633",
                "name": "L. Boves"
              }
            ]
          }
        },
        {
          "citedcorpusid": 231718841,
          "isinfluential": false,
          "contexts": [
            "Embedding the patent title and abstract was used for CNN learning, and an f1 score of 43% was recorded in the problem of classifying 606 ICP codes [22].",
            "In addition to classification of easily accessible texts such as news and books [14,15], more specialized and complex legal documents [16,17,18,19] and studies on the classification of patient medical records [20,21,22], etc."
          ],
          "intents": [
            "['methodology']",
            "['background']"
          ],
          "cited_paper_info": {
            "title": "An explainable CNN approach for medical codes prediction from clinical text",
            "abstract": "Clinical notes are unstructured text documents generated by clinicians during patient encounters, generally are annotated with International Classification of Diseases (ICD) codes, which give formatted information about the diagnosis and treatment. ICD code has shown its potentials in many fields, but manual coding is labor-intensive and error-prone, lead to researches of automatic coding. Two specific challenges of this task are (1) given an annotated clinical notes, the reasons behind specific diagnoses and treatments are implicit; (2) explainability is important for practical automatic coding method, the method should not only explain its prediction output but also have explainable internal mechanics. This study aims to develop an explainable CNN approach to address these two challenges. Our key idea is that for the automatic ICD coding task, the presence of informative snippets in the clinical text that correlated with each code plays an important role in the prediction of codes, and an informative snippet can be considered as a local and low-level feature. We infer that there exists a correspondence between a convolution filter and a local and low-level feature. Base on the inference, we come up with the Shallow and Wide Attention convolutional Mechanism (SWAM) to improve the CNN-based models’ ability to learn local and low-level features for each label. We evaluate our approach on MIMIC-III, an open-access dataset of ICU medical records. Our approach substantially outperforms previous results on top-50 medical code prediction on MIMIC-III dataset, the precision of the worst-performing 10% labels in previous works is increased from 0% to 53% on average. We attribute this improvement to SWAM, by which the wide architecture with attention mechanism gives the model ability to more extensively learn the unique features of different codes, and we prove it by an ablation experiment. Besides, we perform manual analysis of the performance imbalance between different codes, and preliminary conclude the characteristics that determine the difficulty of learning specific codes. Our main contributions can be summarized into the following three: (1) We present local and low-level features, a.k.a. informative snippets play an important role in the automatic ICD coding task, and the informative snippets extracted from the clinical text provide explanations for each code. (2) We propose that there exists a correspondence between a convolution filter and a local and low-level feature. A combination of wide and shallow convolutional layer and attention layer can help the CNN-based models better learn local and low-level features. (3) We improved the precision of the worst-performing 10% labels from 0 to 53% on average.",
            "year": 2021,
            "venue": "BMC Medical Informatics and Decision Making",
            "authors": [
              {
                "authorId": "32532865",
                "name": "Shuyuan Hu"
              },
              {
                "authorId": "49061788",
                "name": "Fei Teng"
              },
              {
                "authorId": "2111131848",
                "name": "Lufei Huang"
              },
              {
                "authorId": "1707802857",
                "name": "Jun Yan"
              },
              {
                "authorId": "2135733804",
                "name": "Haibo Zhang"
              }
            ]
          }
        },
        {
          "citedcorpusid": 235262768,
          "isinfluential": true,
          "contexts": [
            "H Bekamiri et al. proposed a method of classifying CPC codes using SBERT (Sentence BERT) and KNN to solve the problem of slow prediction speed of the existing BERT model.",
            "48% for 663 labels with classification using KNN [24].",
            "We trained SBERT using CPC and Claim and achieved an F1 score of 66.48% for 663 labels with classification using KNN [24]."
          ],
          "intents": [
            "--",
            "['methodology']",
            "--"
          ],
          "cited_paper_info": {
            "title": "PatentSBERTa: A deep NLP based hybrid model for patent distance and classification using augmented SBERT",
            "abstract": "",
            "year": 2021,
            "venue": "Technological forecasting & social change",
            "authors": [
              {
                "authorId": "2056771181",
                "name": "Hamid Bekamiri"
              },
              {
                "authorId": "47109088",
                "name": "D. Hain"
              },
              {
                "authorId": "3168776",
                "name": "Roman Jurowetzki"
              }
            ]
          }
        },
        {
          "citedcorpusid": 237600744,
          "isinfluential": false,
          "contexts": [
            "In addition to classification of easily accessible texts such as news and books [14,15], more specialized and complex legal documents [16,17,18,19] and studies on the classification of patient medical records [20,21,22], etc."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "BerConvoNet: A deep learning framework for fake news classification",
            "abstract": "",
            "year": 2021,
            "venue": "Applied Soft Computing",
            "authors": [
              {
                "authorId": "2127778141",
                "name": "Monika Choudhary"
              },
              {
                "authorId": "2854051",
                "name": "S. Chouhan"
              },
              {
                "authorId": "2371703",
                "name": "Emmanuel S. PIlli"
              },
              {
                "authorId": "145353981",
                "name": "Santosh Kumar Vipparthi"
              }
            ]
          }
        },
        {
          "citedcorpusid": 247996537,
          "isinfluential": false,
          "contexts": [
            "The text classification problem we will experiment with is a representative downstream task of natural language processing, and research is being actively conducted in various fields [13]."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "DAGAM: Data Augmentation with Generation And Modification",
            "abstract": "Text classification is a representative downstream task of natural language processing, and has exhibited excellent performance since the advent of pre-trained language models based on Transformer architecture. However, in pre-trained language models, under-fitting often occurs due to the size of the model being very large compared to the amount of available training data. Along with significant importance of data collection in modern machine learning paradigm, studies have been actively conducted for natural language data augmentation. In light of this, we introduce three data augmentation schemes that help reduce underfitting problems of large-scale language models. Primarily we use a generation model for data augmentation, which is defined as Data Augmentation with Generation (DAG). Next, we augment data using text modification techniques such as corruption and word order change (Data Augmentation with Modification, DAM). Finally, we propose Data Augmentation with Generation And Modification (DAGAM), which combines DAG and DAM techniques for a boosted performance. We conduct data augmentation for six benchmark datasets of text classification task, and verify the usefulness of DAG, DAM, and DAGAM through BERT-based fine-tuning and evaluation, deriving better results compared to the performance with original datasets.",
            "year": 2022,
            "venue": "arXiv.org",
            "authors": [
              {
                "authorId": "51895085",
                "name": "Byeong-Cheol Jo"
              },
              {
                "authorId": "1644077770",
                "name": "Tak-Sung Heo"
              },
              {
                "authorId": "2110274738",
                "name": "Yeongjoon Park"
              },
              {
                "authorId": "2087142583",
                "name": "Yongmin Yoo"
              },
              {
                "authorId": "2121567090",
                "name": "Won-Ik Cho"
              },
              {
                "authorId": "2109327392",
                "name": "Kyungsun Kim"
              }
            ]
          }
        },
        {
          "citedcorpusid": null,
          "isinfluential": false,
          "contexts": [
            "Department of Commerce that issues patents to inventors and companies for trademark registration and inventions for identifying products and intellectual property rights, called the USPTO, published artificial intelligencerelated patents from 2016 to 2020 on the USPTO site [35]."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {}
        }
      ]
    },
    "1407274": {
      "citing_paper_info": {
        "title": "Patent Classification Experiments with the Linguistic Classification System LCS",
        "abstract": "We report the results of a series of classification experiments with the Linguistic Classification System LCS in the context of CLEF-IP 2011. We participated in the main classification task: classifying documents on the subclass level. We investigated (1) the use of different sections (abstract, description, metadata) from the patent documents; (2) adding dependency triples to the bag-of-words representation; (3) adding the WIPO corpus to the EPO training data; (4) the use of patent citations in the test data for reranking the classes; and (5) the threshold on the class scores for class selection. We found that adding full descriptions to abstracts gives a clear improvement; the first 400 words of the description also improves classification but to a lesser degree. Adding metadata (applicants, inventors en address) did not improve classification. Adding dependency triples to words gives a much higher recall at the cost of a lower precision but this effect is largely due to the class selection threshold. We did not find an effect from adding the WIPO corpus, nor from reranking with patent citations. In future work, we plan to investigate whether there are other methods for reranking with patent citations that does give an improvement, because we feel that the citations may still give valuable information. Our most important finding however is the importance of the threshold on the class selection. For the current work, we only compared two values for the threshold and the results are much better for 1.0 than for 0.5. The 0.5 threshold gives higher recall in all runs, which was the original motivation for submitting runs with a lower threshold. However, because the much lower precision, the F-scores are lower. We think that there is still some improvement to be gained from proper tuning of the class selection threshold, and the use of a flexible threshold (also taking into account the different text representations). This is part of our future work.",
        "year": 2010,
        "venue": "Conference and Labs of the Evaluation Forum",
        "authors": [
          {
            "authorId": "1702730",
            "name": "S. Verberne"
          },
          {
            "authorId": "25486420",
            "name": "M. Vogel"
          },
          {
            "authorId": "1447164726",
            "name": "E.K.L. D'hondt"
          }
        ]
      },
      "citation_summary": {
        "citation_count": 1,
        "unique_cited_count": 1,
        "influential_count": 1,
        "detailed_records_count": 1
      },
      "cited_papers": [
        "10988962"
      ],
      "citation_details": [
        {
          "citedcorpusid": 10988962,
          "isinfluential": true,
          "contexts": [
            "The main findings of this paper are: (1) adding dependency triples to words has a positive effect on classification accuracy and (2) selecting classes by using a threshold on the classification scores instead of returning a fixed number of classes per document improves classification scores while at the same time it lowers the number of classes needs to be judged manually by the professionals at the patent office.",
            "words and triples), (2) the target data vs.",
            "We plan to look into (1) the distribution of IPC classes in the test data compared to the target data, (2) the subset of IPC classes that are covered by the target data but not by the test data and (3) the impact of triples compared to words in the class profiles of these classes.",
            "The user can regulate the selection of classes with three parameters: (1) a threshold that puts a lower bound on the classification score for a class to be selected, (2) the maximum number of classes selected per document (‘maxranks’) and (3) the minimum number of classes selected per document (‘minranks’).",
            "R = |relevant classes ∩ selected classes| |relevant classes| (2)"
          ],
          "intents": [
            "['background']",
            "['background']",
            "['methodology']",
            "['background']",
            "--"
          ],
          "cited_paper_info": {
            "title": "Multi-classification of Patent Applications with Winnow",
            "abstract": "",
            "year": 2003,
            "venue": "Ershov Memorial Conference",
            "authors": [
              {
                "authorId": "1713642",
                "name": "C. Koster"
              },
              {
                "authorId": "2286294",
                "name": "M. Seutter"
              },
              {
                "authorId": "145639626",
                "name": "Jean Beney"
              }
            ]
          }
        }
      ]
    },
    "15660405": {
      "citing_paper_info": {
        "title": "Patent Classification Using Ontology-Based Patent Network Analysis",
        "abstract": "Patent management is increasingly important for organizations to sustain their competitive advantage. The classification of patents is essential for patent management and industrial analysis. In this study, we propose a novel patent network-based classification method to analyze query patents and predict their classes. The proposed patent network, which contains various types of nodes that represent different features extracted from patent documents, is constructed based on the relationship metrics derived from patent metadata. The novel approach analyzes reachable nodes in the patent ontology network to calculate their relevance to query patents, after which it uses the modified k-nearest neighbor classifier to classify query patents. We evaluate the performance of the proposed approach on a test dataset of patent documents obtained from the United States Patent and Trademark Office (USPTO), and compare it with the performance of the three conventional methods. The results demonstrate that the proposed patent network-based approach outperforms the conventional approaches.",
        "year": 2010,
        "venue": "Pacific Asia Conference on Information Systems",
        "authors": [
          {
            "authorId": "2407700",
            "name": "Meng-Jung Shih"
          },
          {
            "authorId": "1741839",
            "name": "Duen-Ren Liu"
          }
        ]
      },
      "citation_summary": {
        "citation_count": 7,
        "unique_cited_count": 6,
        "influential_count": 2,
        "detailed_records_count": 7
      },
      "cited_papers": [
        "37779608",
        "1133021",
        "207163413",
        "19306379",
        "7725217",
        "16041292"
      ],
      "citation_details": [
        {
          "citedcorpusid": 1133021,
          "isinfluential": false,
          "contexts": [
            "The algorithm used for patent network analysis is a modification of the ontologybased network analysis algorithm developed by O'Hara et al. (2002) for identifying an individual's communities of practice."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Identifying Communities of Practice",
            "abstract": "",
            "year": 2002,
            "venue": "Information Systems",
            "authors": [
              {
                "authorId": "1394341279",
                "name": "K. O’Hara"
              },
              {
                "authorId": "145842687",
                "name": "Harith Alani"
              },
              {
                "authorId": "1705314",
                "name": "N. Shadbolt"
              }
            ]
          }
        },
        {
          "citedcorpusid": 7725217,
          "isinfluential": true,
          "contexts": [
            "For the text, the weights of terms are calculated by the tfidf approach (Salton and Buckley, 1988); the weight of each inventor is calculated as , where #inv is the total number of inventors of the patent; and the weight of each IPC code is calculated as , where #ipc is the number of IPC code assigned to the patent.",
            "For the text, the weights of terms are calculated by the tfidf approach (Salton and Buckley, 1988); the weight of each inventor is calculated as , where #inv is the total number of inventors of the patent; and the weight of each IPC code is calculated as , where #ipc is the number of IPC code…",
            "However, precision and recall are usually in conflict with each other, so the F-measure is used to balance the two results.",
            "We used standard classification performance metrics, namely, the accuracy rate, precision rate, recall rate, and F-measure (Salton & Buckley, 1988; Van Rijsbergen, 1979), to evaluate the performance of the classifiers.",
            "For instances of class i: Finally, to obtain a single performance measure, we used a simple F-measure to balance the precision and recall scores, as shown in Eq.",
            "6: (5) (6) Precision, recall and F-measure were used to assess the classification performance.",
            "The most popular term weighting function is term frequency / inverse document frequency (tfidf), developed by Salton and Buckley (Salton & Buckley, 1988)."
          ],
          "intents": [
            "--",
            "['methodology']",
            "--",
            "['methodology']",
            "--",
            "--",
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Term-Weighting Approaches in Automatic Text Retrieval",
            "abstract": "",
            "year": 1988,
            "venue": "Information Processing & Management",
            "authors": [
              {
                "authorId": "1797808",
                "name": "G. Salton"
              },
              {
                "authorId": "144009691",
                "name": "C. Buckley"
              }
            ]
          }
        },
        {
          "citedcorpusid": 16041292,
          "isinfluential": false,
          "contexts": [
            "The similarity of two patent documents is defined as the cosine value of their term vectors (Yang, 1994)."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Expert network: effective and efficient learning from human decisions in text categorization and retrieval",
            "abstract": "",
            "year": 1994,
            "venue": "Annual International ACM SIGIR Conference on Research and Development in Information Retrieval",
            "authors": [
              {
                "authorId": "35729970",
                "name": "Yiming Yang"
              }
            ]
          }
        },
        {
          "citedcorpusid": 19306379,
          "isinfluential": true,
          "contexts": [
            "In recent years, a considerable number of such schemes have been proposed (e.g., Kim & Choi, 2007; Kohonen, et al., 2000; Lai & Wu, 2005; Larkey, 1999; Richter & MacFarlane, 2005; Cong & Tong, 2008; Cong & Loh, 2010; Trappey, et al., 2006).",
            "The co-citation approach (Lai & Wu, 2005) classifies a query patent according to the majority vote of the classes of its cited patents.",
            "Some approaches utilize citation relationships to improve the performance of patent classification (Lai & Wu, 2005; Li, et al., 2007); while others employ patent metadata, such as the inventor's name, and thereby achieve improvements in the classification performance (Richter & MacFarlane, 2005).",
            "Approaches that utilize citations have been proposed (Lai & Wu, 2005; Li et al., 2007)."
          ],
          "intents": [
            "['background']",
            "['background']",
            "['methodology']",
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Using the patent co-citation approach to establish a new patent classification system",
            "abstract": "",
            "year": 2005,
            "venue": "Information Processing & Management",
            "authors": [
              {
                "authorId": "2680739",
                "name": "K. Lai"
              },
              {
                "authorId": "144253665",
                "name": "S. Wu"
              }
            ]
          }
        },
        {
          "citedcorpusid": 37779608,
          "isinfluential": false,
          "contexts": [
            "In recent years, a considerable number of such schemes have been proposed (e.g., Kim & Choi, 2007; Kohonen, et al., 2000; Lai & Wu, 2005; Larkey, 1999; Richter & MacFarlane, 2005; Cong & Tong, 2008; Cong & Loh, 2010; Trappey, et al., 2006).",
            "…full text of a patent document as the basis for classification, some approaches classify patent documents by considering normative sections, such as the abstract, background, and results (Kim & Choi, 2007; Fall, 2003, 2004; Larkey, 1999; Cong & Tong, 2008; Loh, et al., 2006; Trappey, et al., 2006).",
            "Most existing studies consider information content to classify patent documents, and several classification algorithms have been developed based on different content features (e.g., Larkey, 1999; Fall et al., 2003 and 2004; Trappey et al., 2006; Loh et al., 2006; Kim & Choi 2007; Cong & Tong, 2008)."
          ],
          "intents": [
            "['background']",
            "['background']",
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Grouping of TRIZ Inventive Principles to facilitate automatic patent classification",
            "abstract": "",
            "year": 2008,
            "venue": "Expert systems with applications",
            "authors": [
              {
                "authorId": "2115065574",
                "name": "Cong He"
              },
              {
                "authorId": "2935248",
                "name": "H. Loh"
              }
            ]
          }
        },
        {
          "citedcorpusid": 207163413,
          "isinfluential": false,
          "contexts": [
            "Approaches that utilize citations have been proposed (Lai & Wu, 2005; Li et al., 2007)."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Automatic patent classification using citation network information: an experimental study in nanotechnology",
            "abstract": "",
            "year": 2007,
            "venue": "ACM/IEEE Joint Conference on Digital Libraries",
            "authors": [
              {
                "authorId": "2266968460",
                "name": "Xin Li"
              },
              {
                "authorId": "47666658",
                "name": "Hsinchun Chen"
              },
              {
                "authorId": "2109107987",
                "name": "Zhu Zhang"
              },
              {
                "authorId": "2148947633",
                "name": "Jiexun Li"
              }
            ]
          }
        },
        {
          "citedcorpusid": null,
          "isinfluential": false,
          "contexts": [
            "We used standard classification performance metrics, namely, the accuracy rate, precision rate, recall rate, and F-measure (Salton & Buckley, 1988; Van Rijsbergen, 1979), to evaluate the performance of the classifiers."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {}
        }
      ]
    },
    "7597828": {
      "citing_paper_info": {
        "title": "myClass: A Mature Tool for Patent Classification",
        "abstract": "In this task 2,000 patents in three languages (English, French and German) were to be classified among approximately 600 categories. We used a classifier based on neural networks of the Winnow type. This classifier is already used for similar tasks in professional applications. We tested three different approaches to improve the classification accuracy: the first one aimed at solving the issue of poorly-documented categories, the second one was meant to enrich the overall training corpus and the third one was based on the processing of the corpus' collocations. Although we ranked first in this competition, none of the three approaches mentioned above provided for a clear improvement in classification accuracy; our results were essentially due to the implementation of the classification algorithm itself.",
        "year": 2010,
        "venue": "Conference and Labs of the Evaluation Forum",
        "authors": [
          {
            "authorId": "145922197",
            "name": "J. Guyot"
          },
          {
            "authorId": "2217341",
            "name": "K. Benzineb"
          },
          {
            "authorId": "1792960",
            "name": "G. Falquet"
          }
        ]
      },
      "citation_summary": {
        "citation_count": 1,
        "unique_cited_count": 1,
        "influential_count": 0,
        "detailed_records_count": 1
      },
      "cited_papers": [
        "43261268"
      ],
      "citation_details": [
        {
          "citedcorpusid": 43261268,
          "isinfluential": false,
          "contexts": [
            "A general introduction to patent classification can be found in [9]."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Current Challenges in Patent Information Retrieval",
            "abstract": "",
            "year": 2011,
            "venue": "The Information Retrieval Series",
            "authors": [
              {
                "authorId": "144307089",
                "name": "M. Lupu"
              },
              {
                "authorId": "2059169453",
                "name": "Katja Mayer"
              },
              {
                "authorId": "144959176",
                "name": "J. Tait"
              },
              {
                "authorId": "2494885",
                "name": "A. Trippe"
              }
            ]
          }
        }
      ]
    },
    "34189742": {
      "citing_paper_info": {
        "title": "Pattern-oriented associative rule-based patent classification",
        "abstract": "",
        "year": 2010,
        "venue": "Expert systems with applications",
        "authors": [
          {
            "authorId": "2115065574",
            "name": "Cong He"
          },
          {
            "authorId": "2935248",
            "name": "H. Loh"
          }
        ]
      },
      "citation_summary": {
        "citation_count": 3,
        "unique_cited_count": 3,
        "influential_count": 2,
        "detailed_records_count": 3
      },
      "cited_papers": [
        "25145605",
        "195710767",
        "5262555"
      ],
      "citation_details": [
        {
          "citedcorpusid": 5262555,
          "isinfluential": true,
          "contexts": [
            "Compared with three currently popular classification algorithms (SVM, C4.5 and NB), the new approach shows some improvement.",
            "As shown in Table 13, the rule-based approach and SVM outperforms NB and C4.5 for all of the 7 Classes.",
            "In Loh et al. (2006) and He and Loh (2007), we have reported our experiments on TRIZ Principle-based patent classification which were performed by three currently popular classification algorithms: Support Vector Machine (SVM) (Yang & Liu, 1999), Naïve Bayes (NB) (Rish, 2001; Yang & Liu, 1999) and Decision Tree (C4.5) (Quinlan, 1993).",
            "As we know, both NB and C4.5 are based on probabilistic models and SVM works by making a separation hyperplane in the vector space of the features of the documents.",
            "We take the F(2) value in the last column in Tables 6– 12 (i.e. k = 6) for each class and compare it with the F(2) value achieved by the three classifier we have used in our previous experiments (i.e. NB, C4.5 and SVM).",
            "…we have reported our experiments on TRIZ Principle-based patent classification which were performed by three currently popular classification algorithms: Support Vector Machine (SVM) (Yang & Liu, 1999), Naïve Bayes (NB) (Rish, 2001; Yang & Liu, 1999) and Decision Tree (C4.5) (Quinlan, 1993)."
          ],
          "intents": [
            "--",
            "--",
            "--",
            "--",
            "--",
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "C4.5: Programs for Machine Learning",
            "abstract": "From the Publisher: \nClassifier systems play a major role in machine learning and knowledge-based systems, and Ross Quinlan's work on ID3 and C4.5 is widely acknowledged to have made some of the most significant contributions to their development. This book is a complete guide to the C4.5 system as implemented in C for the UNIX environment. It contains a comprehensive guide to the system's use , the source code (about 8,800 lines), and implementation notes. The source code and sample datasets are also available on a 3.5-inch floppy diskette for a Sun workstation. \n \nC4.5 starts with large sets of cases belonging to known classes. The cases, described by any mixture of nominal and numeric properties, are scrutinized for patterns that allow the classes to be reliably discriminated. These patterns are then expressed as models, in the form of decision trees or sets of if-then rules, that can be used to classify new cases, with emphasis on making the models understandable as well as accurate. The system has been applied successfully to tasks involving tens of thousands of cases described by hundreds of properties. The book starts from simple core learning methods and shows how they can be elaborated and extended to deal with typical problems such as missing data and over hitting. Advantages and disadvantages of the C4.5 approach are discussed and illustrated with several case studies. \n \nThis book and software should be of interest to developers of classification-based intelligent systems and to students in machine learning and expert systems courses.",
            "year": 1992,
            "venue": "",
            "authors": [
              {
                "authorId": "145341779",
                "name": "J. R. Quinlan"
              }
            ]
          }
        },
        {
          "citedcorpusid": 25145605,
          "isinfluential": false,
          "contexts": [
            "In previous reports, it works well in many text classification tasks (Antonie & Zaiane, 2002; Chen et al., 2004; Chidanand et al., 1994; Han, Manavogulu, Giles, & Zha, 2003).",
            "In previous reports, it works well in many text classification tasks (Antonie & Zaiane, 2002; Chen et al., 2004; Chidanand et al., 1994; Han, Manavogulu, Giles, & Zha, 2003). However, the newly proposed classification task, TRIZ Principlebased patent classification, encounters some special issues, which differs from many other text classification applications. In many other text classification applications, nouns and noun phases are supposed to be the most crucial text information. In some classification tasks, even only nouns were extracted and used. We can also see from the example rules presented by Chidanand et al. (1994) as shown in the last section that the distinguishing features to identify the class ‘‘wheat” are mostly nouns."
          ],
          "intents": [
            "['background']",
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Text categorization based on frequent patterns with term frequency",
            "abstract": "",
            "year": 2004,
            "venue": "Proceedings of 2004 International Conference on Machine Learning and Cybernetics (IEEE Cat. No.04EX826)",
            "authors": [
              {
                "authorId": "2143799361",
                "name": "Xiaoyang Chen"
              },
              {
                "authorId": "2154865966",
                "name": "Yi Chen"
              },
              {
                "authorId": "2152511329",
                "name": "Lei Wang"
              },
              {
                "authorId": "121617210",
                "name": "Yun-Fa Hu"
              }
            ]
          }
        },
        {
          "citedcorpusid": 195710767,
          "isinfluential": true,
          "contexts": [
            "As discussed in He and Loh (2007), evaluation only by precision or recall is not recommended.",
            "1 For the details of the dataset and the label for the class, please refer to He & Loh (2007).",
            "Therefore, from our current dataset (He & Loh, 2007), we take the seven largest classes (Classes 01050615, 073031, 082939, 091011, 14, 262832 and 3536371) which address 20 of the 40 Principles.",
            "Considering the requirements for TRIZ users, F(2)-value was proposed to evaluate our performance which combines both precision and recall while assigning more weights to the precision component (He & Loh, 2007).",
            "In Loh et al. (2006) and He and Loh (2007), we have reported our experiments on TRIZ Principle-based patent classification which were performed by three currently popular classification algorithms: Support Vector Machine (SVM) (Yang & Liu, 1999), Naïve Bayes (NB) (Rish, 2001; Yang & Liu, 1999) and…",
            "In addition, although they, especially SVM, work well in our experiments (He & Loh, 2007), their work processes are like a black box which is not ‘‘readable” by human beings.",
            "(2006) and He and Loh (2007), we have reported our experiments on TRIZ Principle-based patent classification which were performed by three currently popular classification algorithms: Support Vector Machine (SVM) (Yang & Liu, 1999), Naïve Bayes (NB) (Rish, 2001; Yang & Liu, 1999) and Decision Tree (C4."
          ],
          "intents": [
            "['background']",
            "['background']",
            "--",
            "['methodology']",
            "['methodology']",
            "['background']",
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Grouping of TRIZ Inventive Principles to facilitate automatic patent classification",
            "abstract": "",
            "year": 2008,
            "venue": "",
            "authors": [
              {
                "authorId": "2065297432",
                "name": "He Cong"
              },
              {
                "authorId": "144885633",
                "name": "Loh Han Tong"
              }
            ]
          }
        }
      ]
    },
    "12853243": {
      "citing_paper_info": {
        "title": "Test Collections for Patent Retrieval and Patent Classification in the Fifth NTCIR Workshop",
        "abstract": "",
        "year": 2006,
        "venue": "International Conference on Language Resources and Evaluation",
        "authors": [
          {
            "authorId": "143800037",
            "name": "Atsushi Fujii"
          },
          {
            "authorId": "2144702",
            "name": "Makoto Iwayama"
          },
          {
            "authorId": "1678892",
            "name": "N. Kando"
          }
        ]
      },
      "citation_summary": {
        "citation_count": 2,
        "unique_cited_count": 2,
        "influential_count": 0,
        "detailed_records_count": 2
      },
      "cited_papers": [
        "267804819",
        "18465199"
      ],
      "citation_details": [
        {
          "citedcorpusid": 18465199,
          "isinfluential": false,
          "contexts": [
            "We evaluated patent classiﬁcation systems through a multi-dimensional classiﬁcation structure called “F-term” (Schellner, 2002), which is used in the JPO.",
            "We evaluated patent classification systems through a multi-dimensional classification structure called “Fterm” (Schellner, 2002), which is used in the JPO."
          ],
          "intents": [
            "['methodology']",
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Japanese File Index classification and F-terms",
            "abstract": "",
            "year": 2002,
            "venue": "",
            "authors": [
              {
                "authorId": "52575228",
                "name": "Irene Schellner"
              }
            ]
          }
        },
        {
          "citedcorpusid": 267804819,
          "isinfluential": false,
          "contexts": [
            "Details of evaluation results for participating systems, which are described in the NTCIR-5 Proceedings (Fujii et al., 2005; Iwayama et al., 2005), are beyond the scope of this paper.",
            "The process of selecting these topics is described in the NTCIR-5 Proceedings (Fujii et al., 2005)."
          ],
          "intents": [
            "['background']",
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Overview of Patent Retrieval Task at NTCIR-5",
            "abstract": "In the Fifth NTCIR Workshop, we organized the Patent Retrieval Task and performed three subtasks; Document Retrieval, Passage Retrieval, and Classification. This paper describes the Document Retrieval Subtask and Passage Retrieval Subtask, both of which were intended for patent-to-patent invalidity search task. We show the evaluation results of the groups participating in those subtasks.",
            "year": 2005,
            "venue": "NTCIR Conference on Evaluation of Information Access Technologies",
            "authors": [
              {
                "authorId": "2285172850",
                "name": "Atsushi Fujii"
              },
              {
                "authorId": "2144702",
                "name": "Makoto Iwayama"
              },
              {
                "authorId": "2285320115",
                "name": "Noriko Kando"
              }
            ]
          }
        }
      ]
    },
    "1872798": {
      "citing_paper_info": {
        "title": "Computer-Assisted Categorization of Patent Documents in the International Patent Classification",
        "abstract": "The World Intellectual Property Organization is currently developing a system for assisting users in categorizing patent documents in the International Patent Classification (IPC). The system should support the classification of documents in several languages and aims to assist users in locating relevant IPC symbols by providing them with a convenient web-based service. The approach taken for developing such a system relies on powerful machine learning algorithms that are trained on manually classified documents to recognize IPC topics. We detail in-house results of applying a custom-built state-of-the-art computer-assisted categorizer to English, French, Russian, and Germanlanguage patent documents. We find that reliable computer-assisted categorization at IPC subclass level is an achievable goal for the statistical methods employed here. A categorization system suggesting three IPC symbols for each document can predict the main IPC class correctly for around 90% of documents, and the main IPC subclass for about 85% of documents. The accuracy of the system at main group level is enhanced if the user first validates the correct IPC class.",
        "year": 2003,
        "venue": "",
        "authors": [
          {
            "authorId": "8635679",
            "name": "C. Fall"
          },
          {
            "authorId": "2217341",
            "name": "K. Benzineb"
          },
          {
            "authorId": "145922197",
            "name": "J. Guyot"
          },
          {
            "authorId": "2893823",
            "name": "A. Törcsvári"
          }
        ]
      },
      "citation_summary": {
        "citation_count": 3,
        "unique_cited_count": 3,
        "influential_count": 1,
        "detailed_records_count": 3
      },
      "cited_papers": [
        "16810209",
        "17081177",
        "185413"
      ],
      "citation_details": [
        {
          "citedcorpusid": 185413,
          "isinfluential": false,
          "contexts": [
            "Larkey [7] has created a tool for attributing US patent classifications.",
            "The inclusion of phrases during document indexing is reported to have increased the system’s precision for patent searching but not for categorization [7]."
          ],
          "intents": [
            "['methodology']",
            "['background']"
          ],
          "cited_paper_info": {
            "title": "A patent search and classification system",
            "abstract": "",
            "year": 1999,
            "venue": "Digital library",
            "authors": [
              {
                "authorId": "1742457",
                "name": "L. Larkey"
              }
            ]
          }
        },
        {
          "citedcorpusid": 16810209,
          "isinfluential": true,
          "contexts": [
            "The participant with the best results has published his findings separately [9].",
            "Their result is obtained with an English-language training set containing 68,416 different documents [9].",
            "To learn word distributions, we make use of state-of-the-art neural network techniques, and apply in particular a variant of the winnow algorithm [9]."
          ],
          "intents": [
            "['background']",
            "['background']",
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Classifying Patent Applications with Winnow",
            "abstract": "A new and distinct variety of peach, Prunus persica, tree with the following unique combination of desirable features: 1. Heavy and regular production of large, white flesh, clingstone fruit. 2. Fruit with a mild, sweet, subacid flavor with excellent eating quality. 3. Fruit with firm white flesh, good handling and shipping qualities. 4. An attractive fruit with a white ground color nearly overspread with a red blush. 5. Large tree size with vigorous, upright growth. 6. Relatively uniform ripening of fruit throughout the tree.",
            "year": 2001,
            "venue": "",
            "authors": [
              {
                "authorId": "1713642",
                "name": "C. Koster"
              },
              {
                "authorId": "2286294",
                "name": "M. Seutter"
              },
              {
                "authorId": "145639626",
                "name": "Jean Beney"
              }
            ]
          }
        },
        {
          "citedcorpusid": 17081177,
          "isinfluential": false,
          "contexts": [
            "Patent documents also include acronyms and much new terminology [12]."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "What Shall We Evaluate ?-- Preliminary Discussion for the NTCIR Patent IR Challenge ( PIC ) Based on the Brainstorming with the Specialized Intermediaries in Patent Searching and Patent Attorneys",
            "abstract": "",
            "year": 2001,
            "venue": "",
            "authors": [
              {
                "authorId": "1678892",
                "name": "N. Kando"
              }
            ]
          }
        }
      ]
    }
  }
}