Name (extracted)	Citing Article	Citied Article	Features
USPTO-2M	https://doi.org/10.1007/s11192-021-04179-4	https://doi.org/10.3390/e20020104	The USPTO-2M dataset is primarily used for training and evaluating patent classification models, particularly the DeepPatent model. It focuses on large-scale benchmarking and improving classification accuracy by examining the impact of different patent sections and word counts. The dataset provides a significantly larger benchmark compared to previous datasets, enabling robust training and evaluation of NLP models for patent classification tasks.
WIPO-alpha	https://doi.org/10.2478/jdis-2022-0015, https://www.semanticscholar.org/paper/e2e3e36c00c801cf51a47c4724bcf0430e4c0100, https://doi.org/10.1108/DTA-01-2019-0002 (+1)	https://doi.org/10.1007/11875581_124	The WIPO-alpha dataset is used in research to evaluate and optimize classification models for patent documents. Studies employ neural networks, SVMs, and ensemble methods to improve accuracy and micro-F1 scores in patent categorization, addressing the hierarchical complexity of the dataset. This enables researchers to enhance the performance of classifiers in handling the nuanced structure of patent data.
CLEF-IP	https://doi.org/10.1109/BigComp51126.2021.00037, https://doi.org/10.1145/3383583.3398538, https://doi.org/10.1007/s11192-021-04179-4 (+1)	https://www.semanticscholar.org/paper/23ed0f8595a04d76efc2f71b3c72c30bc56c4fe4	The CLEF-IP dataset is used for patent classification and retrieval research, focusing on short texts like titles and abstracts. It employs methodologies such as CNN models with word2vec embeddings and evaluates retrieval systems in the intellectual property domain. The dataset includes 1.35 million patents from the European Patent Office and World Intellectual Property Organization, supporting multilingual classification in English and German using semantic and statistical approaches.
IPC	https://doi.org/10.1186/2041-1480-4-S1-S3	https://doi.org/10.1136/jamia.2001.0080317	The IPC dataset is used to compare international patent classification systems, focusing on their hierarchical structures and effectiveness in patent searching. Research employs this dataset to evaluate and enhance the accuracy and efficiency of patent classification, aiding in more precise patent retrieval and analysis.
ALTA 2018 shared task on Classifying Patent Applications	https://www.semanticscholar.org/paper/e2e3e36c00c801cf51a47c4724bcf0430e4c0100	https://www.semanticscholar.org/paper/f35824ac534cde7b84aa073424ec30edcb3ff776	The ALTA 2018 shared task dataset on Classifying Patent Applications is used to train and evaluate systems for automatically categorizing Australian patent applications according to the IPC taxonomy. Research focuses on improving the accuracy of classifying patent applications into top sections, employing methodologies that assess and enhance automatic classification performance.
M-patent dataset	https://doi.org/10.1007/s11192-021-04179-4	https://doi.org/10.3390/e20020104	The M-patent dataset is used for patent classification experiments, specifically to train and evaluate models over 40 epochs. Neural network models are trained using this dataset, with early stopping applied to prevent over-fitting. This methodology, following Hu et al. (2018a), enables researchers to assess model performance in classifying patents accurately.
CLEF-IP 2011	https://doi.org/10.1007/s11192-021-04179-4	https://www.semanticscholar.org/paper/23ed0f8595a04d76efc2f71b3c72c30bc56c4fe4	The CLEF-IP 2011 dataset is used as a subset for training and evaluating models in patent classification, specifically focusing on intellectual property retrieval tasks. It enables researchers to develop and assess algorithms that improve the accuracy and efficiency of retrieving relevant patents, enhancing the overall process of intellectual property management.
CLEF-IP 2010	https://doi.org/10.1108/DTA-01-2019-0002, https://doi.org/10.1109/ICDIM.2018.8846972	https://www.semanticscholar.org/paper/38fa94345fa000d66d92cf2e8217fbf014010db1	The CLEF-IP 2010 dataset is used to evaluate the performance of patent classification models, specifically focusing on micro-F1 and micro-precision metrics. It involves a subset of 532,264 English abstracts, enabling researchers to assess and compare the effectiveness of different classification approaches in this domain.
million patents	https://doi.org/10.1145/3529836.3529849	https://doi.org/10.1016/J.WPI.2021.102060	The 'million patents' dataset is used to train a Word Embedding model for patent classification, focusing on capturing semantic features of patent texts. This involves a deep learning pipeline that leverages the large volume of patent data to enhance the model's ability to understand and classify patent documents accurately.
annotated dataset provided by the organizers of the ALTA 2018 shared task	https://www.semanticscholar.org/paper/e2e3e36c00c801cf51a47c4724bcf0430e4c0100	https://www.semanticscholar.org/paper/f35824ac534cde7b84aa073424ec30edcb3ff776	The annotated dataset provided by the organizers of the ALTA 2018 shared task is used for classifying patent applications, with a focus on enhancing classification accuracy. Researchers employ supervised learning methods to improve the precision of patent classification, leveraging the dataset's annotated labels to train and evaluate their models. This dataset enables the development and testing of algorithms aimed at automating the patent classification process.
M-CLEF	https://doi.org/10.1007/s11192-021-04179-4	https://doi.org/10.3390/e20020104	The M-CLEF dataset is used to extract and analyze the title, abstract, description, and claim sections of English patents within the F category of the IPC taxonomy. This extraction supports patent classification research, enabling the development and evaluation of NLP models tailored for categorizing patents accurately.
699,000 patents	https://www.semanticscholar.org/paper/dca404a59d66f196f55789e28033eaec85da6a91	https://doi.org/10.1007/978-3-319-49586-6_48	The '699,000 patents' dataset is used to train and evaluate patent classifiers, specifically for IPC subclass level classification. Researchers employ a 70/30 training/testing data split to develop and assess the performance of these classifiers. This dataset enables the refinement of natural language processing techniques for accurate patent classification, enhancing the efficiency and precision of patent categorization systems.
Hulth2003	https://doi.org/10.3390/e20020104	https://www.semanticscholar.org/paper/03589e1917debe6df148cac8963fd008e4140237	The Hulth2003 dataset is used to evaluate keyphrase extraction models, specifically focusing on precision metrics in the context of patent classification. This dataset enables researchers to assess the performance of their models by providing a benchmark for precision, facilitating advancements in automated keyphrase extraction for patents.
CLEF-IP evaluation track	https://doi.org/10.1186/2041-1480-4-S1-S3	https://www.semanticscholar.org/paper/70f5f7fc239e651d1ef0f3331b5ea7051f7f5cf3	The CLEF-IP evaluation track dataset is used to assess the performance of patent classification models, focusing on intellectual property data. It is specifically designed to improve the accuracy of categorizing patents. The dataset enables researchers to evaluate and refine their models, ensuring they can effectively handle and classify complex patent information.
NTCIR-5 Patent Retrieval Task	https://doi.org/10.1186/2041-1480-4-S1-S3	https://www.semanticscholar.org/paper/70f5f7fc239e651d1ef0f3331b5ea7051f7f5cf3	The NTCIR-5 Patent Retrieval Task dataset is used to evaluate patent retrieval and classification systems. It focuses on categorizing patents into predefined classes, employing a structured dataset to assess the performance of these systems. This enables researchers to test and improve algorithms for patent classification, ensuring accurate and efficient categorization.
Espacenet	https://doi.org/10.1186/2041-1480-4-S1-S3	https://doi.org/10.1016/j.aei.2013.02.002	The Espacenet dataset, comprising nearly 80 million patent documents as of late 2012, is primarily used to illustrate the vast scale of patent data available for research. It serves as a reference to emphasize the complexity and richness of patent databases, which can be leveraged for various research purposes, though specific methodologies or research questions are not detailed in the provided descriptions.
CLEF-IP track of 2011	https://doi.org/10.1108/DTA-01-2019-0002	https://www.semanticscholar.org/paper/23ed0f8595a04d76efc2f71b3c72c30bc56c4fe4	The CLEF-IP track of 2011 dataset is used for retrieval tasks in the intellectual property domain, specifically supporting research in patent classification and information retrieval methodologies. It provides a structured resource for developing and evaluating algorithms that enhance the accuracy and efficiency of patent classification and information retrieval systems.
DeepPatent2	https://doi.org/10.48550/arXiv.2404.19360	https://doi.org/10.48550/arXiv.2301.07584	The DeepPatent2 dataset is used for patent classification experiments, specifically focusing on joint representation learning for text and 3D point cloud data. This approach integrates textual information with geometric features to enhance classification accuracy. The dataset enables researchers to explore advanced methodologies that combine multi-modal data, addressing the complex task of patent classification in a more holistic manner.
Product 14.12	https://doi.org/10.1186/2041-1480-4-S1-S3		The Product 14.12 dataset is used for patent annotation analysis, leveraging XML files published by the European Patent Office (EPO). It provides subscription-based access to these files, enabling researchers to analyze patent annotations. This dataset supports research focused on understanding and improving the annotation processes in patent documents, facilitating detailed examination and categorization of patent content.
patent dataset	https://www.semanticscholar.org/paper/d4e4a545160d8a6eba5338463168f51e8cbe411e	https://doi.org/10.18653/v1/D19-1371	The patent dataset is used to train a vocabulary using the Sentencepiece algorithm, specifically tailored for the domain of patents. This enhances language modeling for scientific text, improving the representation and processing of technical language in natural language processing tasks.
