Name (extracted)	Citing Article	Citied Article	Features
USPTO-2M	https://www.semanticscholar.org/paper/949f92adedc4c90d4f5f368f6ebb8c6aebcfe2d2, https://doi.org/10.1007/s11192-021-04179-4	https://doi.org/10.3390/e20020104	The USPTO-2M dataset is primarily used for patent classification in natural language processing (NLP) research. It is employed to train and evaluate various deep learning models, including BERT, XLNet, RoBERTa, and DeepPatent, focusing on multi-label classification and improving language understanding in the patent domain. The dataset facilitates large-scale benchmarking and enhances subclass-level accuracy through supervised learning. Research using this dataset often evaluates the impact of different patent sections and word counts on classification performance, providing a robust benchmark for model performance in the patent classification task.
CLEF-IP	https://doi.org/10.1109/TEM.2022.3152216, https://doi.org/10.1007/s11192-021-04179-4, https://doi.org/10.1109/BigComp51126.2021.00037 (+2)	https://www.semanticscholar.org/paper/23ed0f8595a04d76efc2f71b3c72c30bc56c4fe4	The CLEF-IP dataset is primarily used for patent retrieval and classification tasks, involving large-scale patent documents from the European Patent Office and World Intellectual Property Organization. It supports research in semantic similarity, classification, and retrieval systems, often employing CNN models, word2vec embeddings, and various semantic and statistical approaches. The dataset facilitates the evaluation and improvement of patent classification models, particularly in handling short texts like titles and abstracts, and supports multi-language classification in English and German.
USPTO	https://www.semanticscholar.org/paper/949f92adedc4c90d4f5f368f6ebb8c6aebcfe2d2	https://doi.org/10.1016/j.techfore.2020.120146	The USPTO dataset is utilized for training and evaluating machine learning models in patent classification. It provides a large corpus of labeled patent documents, enabling supervised learning approaches. Researchers use this dataset to assess model performance metrics such as accuracy, kappa, and MAE, often employing techniques like ANN, MLP, and BiLSTM-ATT-CRF. The dataset includes maintenance periods and indices, which are crucial for model training and evaluation.
PatentNet	https://doi.org/10.1145/3578741.3578746	https://doi.org/10.1007/s11192-021-04179-4	PatentNet is used for multi-label classification of patent documents, specifically focusing on IPC classification using deep learning models. The dataset includes titles and abstracts from USPTO patents (2006-2014) and supports training and evaluation of models like RoBERTa for language understanding and classification tasks. It enables researchers to develop and test algorithms for accurate IPC classification, enhancing the organization and retrieval of patent information.
WIPO-alpha	https://doi.org/10.1108/DTA-01-2019-0002, https://doi.org/10.1109/ICDIM.2018.8846972, https://doi.org/10.2478/jdis-2022-0015	https://doi.org/10.1007/11875581_124	The WIPO-alpha dataset is used to evaluate the performance of classifiers in patent categorization, specifically focusing on improving micro-F1 scores through ensemble methods and optimizing neural networks for higher accuracy. This dataset enables researchers to benchmark and refine machine learning models for more effective patent classification.
CLEF-IP 2011	https://doi.org/10.1007/s11192-021-04179-4, https://doi.org/10.1108/DTA-01-2019-0002	https://www.semanticscholar.org/paper/23ed0f8595a04d76efc2f71b3c72c30bc56c4fe4	The CLEF-IP 2011 dataset is used for patent classification and information retrieval in the intellectual property domain. It supports research by providing subsets for training and evaluation, enabling the development and testing of methodologies focused on intellectual property retrieval tasks. This dataset facilitates the improvement of classification accuracy and retrieval effectiveness in patent-related research.
USPTO 2.8M	https://doi.org/10.1145/3578741.3578746	https://www.semanticscholar.org/paper/dca404a59d66f196f55789e28033eaec85da6a91	The USPTO 2.8M dataset is primarily used for training and evaluating patent classification models, focusing on sub-class labels. It contains 2.8 million patents and is utilized to outperform state-of-the-art benchmarks. Researchers employ BERT-based models, such as BERT-for-Patents and fine-tuned BERT, often using subsets of 2M samples for comparisons. This dataset enables robust evaluation and advancement in patent classification through large-scale data.
Patentsview	https://www.semanticscholar.org/paper/949f92adedc4c90d4f5f368f6ebb8c6aebcfe2d2	https://doi.org/10.1016/j.techfore.2020.120146	The Patentsview dataset is used to extract patent data, particularly abstracts and claims, for training and evaluating deep learning models. It supports research in classifying patents and analyzing investor reactions and citations. Predefined CNN and Bi-LSTM models are employed to process the textual data, enabling detailed analysis and classification tasks.
USPTO-3M	https://www.semanticscholar.org/paper/949f92adedc4c90d4f5f368f6ebb8c6aebcfe2d2	https://doi.org/10.18653/v1/N19-1423	The USPTO-3M dataset is used to enhance patent classification research by providing a larger, more comprehensive set of training data at the subclass level. This dataset supports the development and evaluation of natural language processing models, enabling researchers to improve the accuracy and scope of patent classification systems.
CLEF-IP 2010	https://doi.org/10.1108/DTA-01-2019-0002, https://doi.org/10.1109/ICDIM.2018.8846972	https://www.semanticscholar.org/paper/8e7a318e03a08049196ca13f3c785f7a455f31c7	The CLEF-IP 2010 dataset is used for prior art retrieval tasks, specifically for re-ranking patents by leveraging various sections within patent documents to enhance retrieval accuracy. It is also utilized to evaluate the performance of patent classification models, focusing on metrics such as micro-f1 and precision, often using a subset of the data. This dataset enables researchers to refine and assess methods for improving the efficiency and accuracy of patent-related information retrieval and classification.
USPTO 2M 2006∼2014	https://doi.org/10.1145/3578741.3578746	https://www.semanticscholar.org/paper/dca404a59d66f196f55789e28033eaec85da6a91	The USPTO 2M 2006∼2014 dataset is used for training and evaluating PatentBERT on patent classification tasks, specifically focusing on the International Patent Classification (IPC) system. It includes 2 million patents from 2006 to 2014, with emphasis on IPC codes, titles, and abstracts. This dataset enables researchers to develop and test models for accurate patent classification.
M-patent dataset	https://doi.org/10.1007/s11192-021-04179-4	https://www.semanticscholar.org/paper/072d756c8b17a78018298e67ff29e6d3a4fe5770	The M-patent dataset is used for patent classification experiments, specifically to train and evaluate neural network models. Researchers apply early stopping to prevent over-fitting and train models over 40 epochs, following the methodology outlined by Hu et al. (2018a). This dataset enables robust model evaluation and helps in refining neural network architectures for patent classification tasks.
Indonesian Patent Database	https://doi.org/10.14569/ijacsa.2025.01601106	https://doi.org/10.3390/electronics11244124	The Indonesian Patent Database is used to integrate region-specific innovations with global patent data, enhancing the capture of local technological advancements. This integration allows researchers to better understand and analyze the unique contributions of Indonesian innovations within a broader global context, providing a more comprehensive view of technological development.
Google Patents	https://doi.org/10.14569/ijacsa.2025.01601106	https://doi.org/10.3390/electronics11244124	The Google Patents dataset is used to enhance the comprehensive coverage of global and regional patent information by combining it with local patent databases. This integration supports research that requires a broad and detailed view of patent data, enabling more thorough analysis and insights into technological trends and innovations.
USPTO bulk data	https://doi.org/10.1145/3578741.3578746	https://doi.org/10.1007/s11192-018-2905-5	The USPTO bulk data is used to create a benchmark dataset for patent classification, specifically employing convolutional neural networks and word embedding techniques. This dataset enables researchers to develop and evaluate models for classifying patents, focusing on improving the accuracy and efficiency of automated classification systems.
IPC 632+Claim USPTO 2M 2006∼2014	https://doi.org/10.1145/3578741.3578746	https://www.semanticscholar.org/paper/dca404a59d66f196f55789e28033eaec85da6a91	The IPC 632+Claim USPTO 2M 2006∼2014 dataset is used for training and evaluating PatentBERT, specifically for patent classification tasks. It leverages a large dataset of USPTO patents from 2006 to 2014, enabling researchers to develop and test models that accurately classify patents into International Patent Classification (IPC) categories. This dataset's extensive coverage and detailed claims data enhance the robustness and reliability of NLP models in patent classification.
USPTO data	https://www.semanticscholar.org/paper/949f92adedc4c90d4f5f368f6ebb8c6aebcfe2d2	https://doi.org/10.1109/ICDIM.2018.8846972	The USPTO data is used to evaluate the precision of early works in patent classification, specifically focusing on the performance of domain-specific word embeddings. This dataset enables researchers to assess how well these embeddings capture the nuances of patent language, contributing to the development of more accurate classification models.
USPTO 2.8 M	https://doi.org/10.1145/3578741.3578746	https://www.semanticscholar.org/paper/dca404a59d66f196f55789e28033eaec85da6a91	The USPTO 2.8 M dataset is used to evaluate the performance of machine learning models in patent classification, focusing on comparing classic models to state-of-the-art methods. This dataset enables researchers to assess and contrast the effectiveness of different algorithms in classifying patents, providing insights into model accuracy and efficiency.
USPTO 2M	https://doi.org/10.1145/3578741.3578746	https://doi.org/10.1007/s11192-021-04179-4	The USPTO 2M dataset is used for pre-training language models on patent data, specifically to enhance multi-label classification performance. Deep learning techniques are employed to improve the model's ability to classify patents accurately. This dataset enables researchers to leverage large-scale patent data for developing more effective and precise classification systems.
M-Patent	https://doi.org/10.1145/3578741.3578746	https://doi.org/10.1007/s11192-021-04179-4	The M-Patent dataset is used for pre-training language models on patent data, specifically to enhance language understanding and classification accuracy in patent documents. This approach leverages the dataset's rich textual content to improve the performance of natural language processing models in the context of patent analysis.
United States Patent Classification (USPC) 360/324	https://doi.org/10.3390/SU10010219	https://doi.org/10.1016/j.cad.2011.12.006	The United States Patent Classification (USPC) 360/324 dataset is used to train a two-layered feed-forward neural network for patent classification, specifically within the domain of US patents. This approach focuses on enhancing the accuracy of classifying patents into their respective categories, leveraging the structured nature of the USPC system. The dataset's detailed classification codes enable researchers to develop and validate models that can efficiently categorize new patents.
WIPO	https://doi.org/10.1109/TEM.2022.3152216	https://doi.org/10.1007/0-387-23550-7_13	The WIPO dataset is used to train and evaluate basic classifiers such as Naive Bayes (NB), K-Nearest Neighbors (KNN), and Support Vector Machines (SVM) for patent classification. It focuses on hierarchical text categorization, enabling researchers to address the specific challenge of organizing patents into a structured hierarchy. This dataset facilitates the development and comparison of classification algorithms in the context of patent data.
DG Concordance	https://doi.org/10.2139/ssrn.2161982	https://www.semanticscholar.org/paper/ba68b230ba1541fb3e5571bc9fb53e698ab5b7de	The 'DG Concordance' dataset is mentioned in the citation context but lacks detailed descriptions of its usage, methodology, research questions, or specific characteristics. Therefore, there is insufficient evidence to provide a comprehensive description of how it is actually used in research.
M-CLEF	https://doi.org/10.1007/s11192-021-04179-4	https://doi.org/10.3390/e20020104	The M-CLEF dataset is used to extract and analyze the title, abstract, description, and claim sections of English patents within the F category of the IPC taxonomy. It supports patent classification research by providing structured data that facilitates the development and evaluation of natural language processing models for categorizing patents.
million patents	https://doi.org/10.1145/3529836.3529849	https://doi.org/10.1016/J.WPI.2021.102060	The 'million patents' dataset is used to train Word Embeddings for patent classification, specifically focusing on enhancing classification accuracy through self-trained embeddings. This approach leverages the large volume of patent data to improve the effectiveness of natural language processing models in categorizing patents.
699,000 patents	https://www.semanticscholar.org/paper/dca404a59d66f196f55789e28033eaec85da6a91	https://doi.org/10.1007/978-3-319-49586-6_48	The '699,000 patents' dataset is used to train and evaluate patent classifiers, particularly for IPC subclass classification. Researchers employ a 70/30 training/testing split to investigate the performance of multi-label classification models. This dataset enables the assessment of classifier accuracy and effectiveness in categorizing patents into multiple subclasses, supporting advancements in patent classification methodologies.
BigPatent 2	https://doi.org/10.1007/978-3-031-44067-0_24	https://doi.org/10.18653/v1/P19-1212	The BigPatent 2 dataset is used to prepare a large-scale dataset for abstractive and coherent summarization of patent texts. It focuses on processing 1.3 million patent documents, enabling researchers to develop and evaluate summarization models that can generate concise and coherent summaries of complex patent content.
dataset containing 1.5 million patent claims	https://doi.org/10.1007/978-3-031-44067-0_24	https://doi.org/10.1016/j.techfore.2024.123536	The dataset containing 1.5 million patent claims is used to conduct experiments on patent classification, focusing on the performance evaluation of deep NLP models on annotated patent claims. This dataset enables researchers to test and refine NLP methodologies, specifically addressing the accuracy and efficiency of classifying patent claims.
CLEF-IP 2010 corpus	https://doi.org/10.1108/DTA-01-2019-0002	https://www.semanticscholar.org/paper/38fa94345fa000d66d92cf2e8217fbf014010db1	The CLEF-IP 2010 corpus is used to evaluate the performance of linguistic classification systems on patent abstracts. Researchers focus on metrics such as micro-precision and micro-F1 scores to assess the accuracy and effectiveness of these systems. This dataset enables the comparison and improvement of natural language processing techniques specifically tailored for patent documents.
