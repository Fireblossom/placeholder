Analysis_Source	Name (extracted)	Citing Article	Citied Article	Features	Name_Variants	Homepage_URL
cited_context | citing_context	000 Patents	https://www.semanticscholar.org/paper/dca404a59d66f196f55789e28033eaec85da6a91	https://doi.org/10.1007/978-3-319-49586-6_48	The '699,000 patents' dataset is used to train and evaluate patent classifiers, particularly for IPC subclass classification. Researchers employ a 70/30 training/testing split to investigate the performance of multi-label classification models. This dataset enables the assessment of classifier accuracy and effectiveness in categorizing patents into multiple subclasses, supporting advancements in patent classification methodologies.; The '699,000 patents' dataset is used to train and evaluate patent classifiers, specifically for IPC subclass level classification. Researchers employ a 70/30 training/testing data split to develop and assess the performance of these classifiers. This dataset enables the refinement of natural language processing techniques for accurate patent classification, enhancing the efficiency and precision of patent categorization systems.	000 patents	
cited_context | citing_context	699	https://www.semanticscholar.org/paper/dca404a59d66f196f55789e28033eaec85da6a91	https://doi.org/10.1007/978-3-319-49586-6_48	The '699,000 patents' dataset is used to train and evaluate patent classifiers, particularly for IPC subclass classification. Researchers employ a 70/30 training/testing split to investigate the performance of multi-label classification models. This dataset enables the assessment of classifier accuracy and effectiveness in categorizing patents into multiple subclasses, supporting advancements in patent classification methodologies.; The '699,000 patents' dataset is used to train and evaluate patent classifiers, specifically for IPC subclass level classification. Researchers employ a 70/30 training/testing data split to develop and assess the performance of these classifiers. This dataset enables the refinement of natural language processing techniques for accurate patent classification, enhancing the efficiency and precision of patent categorization systems.	699	
cited_context	ALTA 2018 shared task on Classifying Patent Applications	https://www.semanticscholar.org/paper/e2e3e36c00c801cf51a47c4724bcf0430e4c0100	https://www.semanticscholar.org/paper/f35824ac534cde7b84aa073424ec30edcb3ff776	The ALTA 2018 shared task dataset on Classifying Patent Applications is used to train and evaluate systems for automatically categorizing Australian patent applications according to the IPC taxonomy. Research focuses on improving the accuracy of classifying patent applications into top sections, employing methodologies that assess and enhance automatic classification performance.		
cited_context	annotated dataset provided by the organizers of the ALTA 2018 shared task	https://www.semanticscholar.org/paper/e2e3e36c00c801cf51a47c4724bcf0430e4c0100	https://www.semanticscholar.org/paper/f35824ac534cde7b84aa073424ec30edcb3ff776	The annotated dataset provided by the organizers of the ALTA 2018 shared task is used for classifying patent applications, with a focus on enhancing classification accuracy. Researchers employ supervised learning methods to improve the precision of patent classification, leveraging the dataset's annotated labels to train and evaluate their models. This dataset enables the development and testing of algorithms aimed at automating the patent classification process.		
citing_context	BigPatent 2	https://doi.org/10.1007/978-3-031-44067-0_24	https://doi.org/10.18653/v1/P19-1212	The BigPatent 2 dataset is used to prepare a large-scale dataset for abstractive and coherent summarization of patent texts. It focuses on processing 1.3 million patent documents, enabling researchers to develop and evaluate summarization models that can generate concise and coherent summaries of complex patent content.		
cited_context | citing_context	Clef-Ip	https://doi.org/10.1109/TEM.2022.3152216, https://doi.org/10.1007/s11192-021-04179-4, https://doi.org/10.1109/BigComp51126.2021.00037 (+2), https://doi.org/10.1109/BigComp51126.2021.00037, https://doi.org/10.1145/3383583.3398538, https://doi.org/10.1007/s11192-021-04179-4 (+1)	https://www.semanticscholar.org/paper/23ed0f8595a04d76efc2f71b3c72c30bc56c4fe4	The CLEF-IP dataset is primarily used for patent retrieval and classification tasks, involving large-scale patent documents from the European Patent Office and World Intellectual Property Organization. It supports research in semantic similarity, classification, and retrieval systems, often employing CNN models, word2vec embeddings, and various semantic and statistical approaches. The dataset facilitates the evaluation and improvement of patent classification models, particularly in handling short texts like titles and abstracts, and supports multi-language classification in English and German.; The CLEF-IP dataset is used for patent classification and retrieval research, focusing on short texts like titles and abstracts. It employs methodologies such as CNN models with word2vec embeddings and evaluates retrieval systems in the intellectual property domain. The dataset includes 1.35 million patents from the European Patent Office and World Intellectual Property Organization, supporting multilingual classification in English and German using semantic and statistical approaches.	CLEF-IP	
cited_context | citing_context	Clef-Ip 2010	https://doi.org/10.1108/DTA-01-2019-0002, https://doi.org/10.1109/ICDIM.2018.8846972	https://www.semanticscholar.org/paper/8e7a318e03a08049196ca13f3c785f7a455f31c7, https://www.semanticscholar.org/paper/38fa94345fa000d66d92cf2e8217fbf014010db1	The CLEF-IP 2010 dataset is used for prior art retrieval tasks, specifically for re-ranking patents by leveraging various sections within patent documents to enhance retrieval accuracy. It is also utilized to evaluate the performance of patent classification models, focusing on metrics such as micro-f1 and precision, often using a subset of the data. This dataset enables researchers to refine and assess methods for improving the efficiency and accuracy of patent-related information retrieval and classification.; The CLEF-IP 2010 dataset is used to evaluate the performance of patent classification models, specifically focusing on micro-F1 and micro-precision metrics. It involves a subset of 532,264 English abstracts, enabling researchers to assess and compare the effectiveness of different classification approaches in this domain.	CLEF-IP 2010	
citing_context	CLEF-IP 2010 corpus	https://doi.org/10.1108/DTA-01-2019-0002	https://www.semanticscholar.org/paper/38fa94345fa000d66d92cf2e8217fbf014010db1	The CLEF-IP 2010 corpus is used to evaluate the performance of linguistic classification systems on patent abstracts. Researchers focus on metrics such as micro-precision and micro-F1 scores to assess the accuracy and effectiveness of these systems. This dataset enables the comparison and improvement of natural language processing techniques specifically tailored for patent documents.		
cited_context | citing_context	Clef-Ip 2011	https://doi.org/10.1007/s11192-021-04179-4, https://doi.org/10.1108/DTA-01-2019-0002, https://doi.org/10.1007/s11192-021-04179-4	https://www.semanticscholar.org/paper/23ed0f8595a04d76efc2f71b3c72c30bc56c4fe4	The CLEF-IP 2011 dataset is used for patent classification and information retrieval in the intellectual property domain. It supports research by providing subsets for training and evaluation, enabling the development and testing of methodologies focused on intellectual property retrieval tasks. This dataset facilitates the improvement of classification accuracy and retrieval effectiveness in patent-related research.; The CLEF-IP 2011 dataset is used as a subset for training and evaluating models in patent classification, specifically focusing on intellectual property retrieval tasks. It enables researchers to develop and assess algorithms that improve the accuracy and efficiency of retrieving relevant patents, enhancing the overall process of intellectual property management.	CLEF-IP 2011	
cited_context	CLEF-IP evaluation track	https://doi.org/10.1186/2041-1480-4-S1-S3	https://www.semanticscholar.org/paper/70f5f7fc239e651d1ef0f3331b5ea7051f7f5cf3	The CLEF-IP evaluation track dataset is used to assess the performance of patent classification models, focusing on intellectual property data. It is specifically designed to improve the accuracy of categorizing patents. The dataset enables researchers to evaluate and refine their models, ensuring they can effectively handle and classify complex patent information.		
cited_context	CLEF-IP track of 2011	https://doi.org/10.1108/DTA-01-2019-0002	https://www.semanticscholar.org/paper/23ed0f8595a04d76efc2f71b3c72c30bc56c4fe4	The CLEF-IP track of 2011 dataset is used for retrieval tasks in the intellectual property domain, specifically supporting research in patent classification and information retrieval methodologies. It provides a structured resource for developing and evaluating algorithms that enhance the accuracy and efficiency of patent classification and information retrieval systems.		
citing_context	dataset containing 1.5 million patent claims	https://doi.org/10.1007/978-3-031-44067-0_24	https://doi.org/10.1016/j.techfore.2024.123536	The dataset containing 1.5 million patent claims is used to conduct experiments on patent classification, focusing on the performance evaluation of deep NLP models on annotated patent claims. This dataset enables researchers to test and refine NLP methodologies, specifically addressing the accuracy and efficiency of classifying patent claims.		
cited_context	DeepPatent2	https://doi.org/10.48550/arXiv.2404.19360	https://doi.org/10.48550/arXiv.2301.07584	The DeepPatent2 dataset is used for patent classification experiments, specifically focusing on joint representation learning for text and 3D point cloud data. This approach integrates textual information with geometric features to enhance classification accuracy. The dataset enables researchers to explore advanced methodologies that combine multi-modal data, addressing the complex task of patent classification in a more holistic manner.		
citing_context	DG Concordance	https://doi.org/10.2139/ssrn.2161982	https://www.semanticscholar.org/paper/ba68b230ba1541fb3e5571bc9fb53e698ab5b7de	The 'DG Concordance' dataset is mentioned in the citation context but lacks detailed descriptions of its usage, methodology, research questions, or specific characteristics. Therefore, there is insufficient evidence to provide a comprehensive description of how it is actually used in research.		
cited_context	Espacenet	https://doi.org/10.1186/2041-1480-4-S1-S3	https://doi.org/10.1016/j.aei.2013.02.002	The Espacenet dataset, comprising nearly 80 million patent documents as of late 2012, is primarily used to illustrate the vast scale of patent data available for research. It serves as a reference to emphasize the complexity and richness of patent databases, which can be leveraged for various research purposes, though specific methodologies or research questions are not detailed in the provided descriptions.		
citing_context	Google Patents	https://doi.org/10.14569/ijacsa.2025.01601106	https://doi.org/10.3390/electronics11244124	The Google Patents dataset is used to enhance the comprehensive coverage of global and regional patent information by combining it with local patent databases. This integration supports research that requires a broad and detailed view of patent data, enabling more thorough analysis and insights into technological trends and innovations.		
cited_context	Hulth2003	https://doi.org/10.3390/e20020104	https://www.semanticscholar.org/paper/03589e1917debe6df148cac8963fd008e4140237	The Hulth2003 dataset is used to evaluate keyphrase extraction models, specifically focusing on precision metrics in the context of patent classification. This dataset enables researchers to assess the performance of their models by providing a benchmark for precision, facilitating advancements in automated keyphrase extraction for patents.		
citing_context	Indonesian Patent Database	https://doi.org/10.14569/ijacsa.2025.01601106	https://doi.org/10.3390/electronics11244124	The Indonesian Patent Database is used to integrate region-specific innovations with global patent data, enhancing the capture of local technological advancements. This integration allows researchers to better understand and analyze the unique contributions of Indonesian innovations within a broader global context, providing a more comprehensive view of technological development.		
cited_context	IPC	https://doi.org/10.1186/2041-1480-4-S1-S3	https://doi.org/10.1136/jamia.2001.0080317	The IPC dataset is used to compare international patent classification systems, focusing on their hierarchical structures and effectiveness in patent searching. Research employs this dataset to evaluate and enhance the accuracy and efficiency of patent classification, aiding in more precise patent retrieval and analysis.		
citing_context	IPC 632+Claim USPTO 2M 2006∼2014	https://doi.org/10.1145/3578741.3578746	https://www.semanticscholar.org/paper/dca404a59d66f196f55789e28033eaec85da6a91	The IPC 632+Claim USPTO 2M 2006∼2014 dataset is used for training and evaluating PatentBERT, specifically for patent classification tasks. It leverages a large dataset of USPTO patents from 2006 to 2014, enabling researchers to develop and test models that accurately classify patents into International Patent Classification (IPC) categories. This dataset's extensive coverage and detailed claims data enhance the robustness and reliability of NLP models in patent classification.		
cited_context | citing_context	M-Clef	https://doi.org/10.1007/s11192-021-04179-4	https://doi.org/10.3390/e20020104	The M-CLEF dataset is used to extract and analyze the title, abstract, description, and claim sections of English patents within the F category of the IPC taxonomy. It supports patent classification research by providing structured data that facilitates the development and evaluation of natural language processing models for categorizing patents.; The M-CLEF dataset is used to extract and analyze the title, abstract, description, and claim sections of English patents within the F category of the IPC taxonomy. This extraction supports patent classification research, enabling the development and evaluation of NLP models tailored for categorizing patents accurately.	M-CLEF	
citing_context	M-Patent	https://doi.org/10.1145/3578741.3578746	https://doi.org/10.1007/s11192-021-04179-4	The M-Patent dataset is used for pre-training language models on patent data, specifically to enhance language understanding and classification accuracy in patent documents. This approach leverages the dataset's rich textual content to improve the performance of natural language processing models in the context of patent analysis.		
cited_context | citing_context	M-Patent Dataset	https://doi.org/10.1007/s11192-021-04179-4	https://www.semanticscholar.org/paper/072d756c8b17a78018298e67ff29e6d3a4fe5770, https://doi.org/10.3390/e20020104	The M-patent dataset is used for patent classification experiments, specifically to train and evaluate neural network models. Researchers apply early stopping to prevent over-fitting and train models over 40 epochs, following the methodology outlined by Hu et al. (2018a). This dataset enables robust model evaluation and helps in refining neural network architectures for patent classification tasks.; The M-patent dataset is used for patent classification experiments, specifically to train and evaluate models over 40 epochs. Neural network models are trained using this dataset, with early stopping applied to prevent over-fitting. This methodology, following Hu et al. (2018a), enables researchers to assess model performance in classifying patents accurately.	M-patent dataset	
cited_context | citing_context	Million Patents	https://doi.org/10.1145/3529836.3529849	https://doi.org/10.1016/J.WPI.2021.102060	The 'million patents' dataset is used to train Word Embeddings for patent classification, specifically focusing on enhancing classification accuracy through self-trained embeddings. This approach leverages the large volume of patent data to improve the effectiveness of natural language processing models in categorizing patents.; The 'million patents' dataset is used to train a Word Embedding model for patent classification, focusing on capturing semantic features of patent texts. This involves a deep learning pipeline that leverages the large volume of patent data to enhance the model's ability to understand and classify patent documents accurately.	million patents	
cited_context	NTCIR-5 Patent Retrieval Task	https://doi.org/10.1186/2041-1480-4-S1-S3	https://www.semanticscholar.org/paper/70f5f7fc239e651d1ef0f3331b5ea7051f7f5cf3	The NTCIR-5 Patent Retrieval Task dataset is used to evaluate patent retrieval and classification systems. It focuses on categorizing patents into predefined classes, employing a structured dataset to assess the performance of these systems. This enables researchers to test and improve algorithms for patent classification, ensuring accurate and efficient categorization.		
cited_context	patent dataset	https://www.semanticscholar.org/paper/d4e4a545160d8a6eba5338463168f51e8cbe411e	https://doi.org/10.18653/v1/D19-1371	The patent dataset is used to train a vocabulary using the Sentencepiece algorithm, specifically tailored for the domain of patents. This enhances language modeling for scientific text, improving the representation and processing of technical language in natural language processing tasks.		
citing_context	PatentNet	https://doi.org/10.1145/3578741.3578746	https://doi.org/10.1007/s11192-021-04179-4	PatentNet is used for multi-label classification of patent documents, specifically focusing on IPC classification using deep learning models. The dataset includes titles and abstracts from USPTO patents (2006-2014) and supports training and evaluation of models like RoBERTa for language understanding and classification tasks. It enables researchers to develop and test algorithms for accurate IPC classification, enhancing the organization and retrieval of patent information.		
citing_context	Patentsview	https://www.semanticscholar.org/paper/949f92adedc4c90d4f5f368f6ebb8c6aebcfe2d2	https://doi.org/10.1016/j.techfore.2020.120146	The Patentsview dataset is used to extract patent data, particularly abstracts and claims, for training and evaluating deep learning models. It supports research in classifying patents and analyzing investor reactions and citations. Predefined CNN and Bi-LSTM models are employed to process the textual data, enabling detailed analysis and classification tasks.		
cited_context	Product 14.12	https://doi.org/10.1186/2041-1480-4-S1-S3		The Product 14.12 dataset is used for patent annotation analysis, leveraging XML files published by the European Patent Office (EPO). It provides subscription-based access to these files, enabling researchers to analyze patent annotations. This dataset supports research focused on understanding and improving the annotation processes in patent documents, facilitating detailed examination and categorization of patent content.		
citing_context	United States Patent Classification (USPC) 360/324	https://doi.org/10.3390/SU10010219	https://doi.org/10.1016/j.cad.2011.12.006	The United States Patent Classification (USPC) 360/324 dataset is used to train a two-layered feed-forward neural network for patent classification, specifically within the domain of US patents. This approach focuses on enhancing the accuracy of classifying patents into their respective categories, leveraging the structured nature of the USPC system. The dataset's detailed classification codes enable researchers to develop and validate models that can efficiently categorize new patents.		
citing_context	USPTO	https://www.semanticscholar.org/paper/949f92adedc4c90d4f5f368f6ebb8c6aebcfe2d2	https://doi.org/10.1016/j.techfore.2020.120146	The USPTO dataset is utilized for training and evaluating machine learning models in patent classification. It provides a large corpus of labeled patent documents, enabling supervised learning approaches. Researchers use this dataset to assess model performance metrics such as accuracy, kappa, and MAE, often employing techniques like ANN, MLP, and BiLSTM-ATT-CRF. The dataset includes maintenance periods and indices, which are crucial for model training and evaluation.		
citing_context	USPTO 2.8 M	https://doi.org/10.1145/3578741.3578746	https://www.semanticscholar.org/paper/dca404a59d66f196f55789e28033eaec85da6a91	The USPTO 2.8 M dataset is used to evaluate the performance of machine learning models in patent classification, focusing on comparing classic models to state-of-the-art methods. This dataset enables researchers to assess and contrast the effectiveness of different algorithms in classifying patents, providing insights into model accuracy and efficiency.		
citing_context	USPTO 2.8M	https://doi.org/10.1145/3578741.3578746	https://www.semanticscholar.org/paper/dca404a59d66f196f55789e28033eaec85da6a91	The USPTO 2.8M dataset is primarily used for training and evaluating patent classification models, focusing on sub-class labels. It contains 2.8 million patents and is utilized to outperform state-of-the-art benchmarks. Researchers employ BERT-based models, such as BERT-for-Patents and fine-tuned BERT, often using subsets of 2M samples for comparisons. This dataset enables robust evaluation and advancement in patent classification through large-scale data.		
citing_context	USPTO 2M	https://doi.org/10.1145/3578741.3578746	https://doi.org/10.1007/s11192-021-04179-4	The USPTO 2M dataset is used for pre-training language models on patent data, specifically to enhance multi-label classification performance. Deep learning techniques are employed to improve the model's ability to classify patents accurately. This dataset enables researchers to leverage large-scale patent data for developing more effective and precise classification systems.		
citing_context	USPTO 2M 2006∼2014	https://doi.org/10.1145/3578741.3578746	https://www.semanticscholar.org/paper/dca404a59d66f196f55789e28033eaec85da6a91	The USPTO 2M 2006∼2014 dataset is used for training and evaluating PatentBERT on patent classification tasks, specifically focusing on the International Patent Classification (IPC) system. It includes 2 million patents from 2006 to 2014, with emphasis on IPC codes, titles, and abstracts. This dataset enables researchers to develop and test models for accurate patent classification.		
citing_context	USPTO bulk data	https://doi.org/10.1145/3578741.3578746	https://doi.org/10.1007/s11192-018-2905-5	The USPTO bulk data is used to create a benchmark dataset for patent classification, specifically employing convolutional neural networks and word embedding techniques. This dataset enables researchers to develop and evaluate models for classifying patents, focusing on improving the accuracy and efficiency of automated classification systems.		
citing_context	USPTO data	https://www.semanticscholar.org/paper/949f92adedc4c90d4f5f368f6ebb8c6aebcfe2d2	https://doi.org/10.1109/ICDIM.2018.8846972	The USPTO data is used to evaluate the precision of early works in patent classification, specifically focusing on the performance of domain-specific word embeddings. This dataset enables researchers to assess how well these embeddings capture the nuances of patent language, contributing to the development of more accurate classification models.		
cited_context | citing_context	Uspto-2M	https://www.semanticscholar.org/paper/949f92adedc4c90d4f5f368f6ebb8c6aebcfe2d2, https://doi.org/10.1007/s11192-021-04179-4, https://doi.org/10.1007/s11192-021-04179-4	https://doi.org/10.3390/e20020104	The USPTO-2M dataset is primarily used for patent classification in natural language processing (NLP) research. It is employed to train and evaluate various deep learning models, including BERT, XLNet, RoBERTa, and DeepPatent, focusing on multi-label classification and improving language understanding in the patent domain. The dataset facilitates large-scale benchmarking and enhances subclass-level accuracy through supervised learning. Research using this dataset often evaluates the impact of different patent sections and word counts on classification performance, providing a robust benchmark for model performance in the patent classification task.; The USPTO-2M dataset is primarily used for training and evaluating patent classification models, particularly the DeepPatent model. It focuses on large-scale benchmarking and improving classification accuracy by examining the impact of different patent sections and word counts. The dataset provides a significantly larger benchmark compared to previous datasets, enabling robust training and evaluation of NLP models for patent classification tasks.	USPTO-2M	
citing_context	USPTO-3M	https://www.semanticscholar.org/paper/949f92adedc4c90d4f5f368f6ebb8c6aebcfe2d2	https://doi.org/10.18653/v1/N19-1423	The USPTO-3M dataset is used to enhance patent classification research by providing a larger, more comprehensive set of training data at the subclass level. This dataset supports the development and evaluation of natural language processing models, enabling researchers to improve the accuracy and scope of patent classification systems.		
citing_context	WIPO	https://doi.org/10.1109/TEM.2022.3152216	https://doi.org/10.1007/0-387-23550-7_13	The WIPO dataset is used to train and evaluate basic classifiers such as Naive Bayes (NB), K-Nearest Neighbors (KNN), and Support Vector Machines (SVM) for patent classification. It focuses on hierarchical text categorization, enabling researchers to address the specific challenge of organizing patents into a structured hierarchy. This dataset facilitates the development and comparison of classification algorithms in the context of patent data.		
cited_context | citing_context	Wipo-Alpha	https://doi.org/10.1108/DTA-01-2019-0002, https://doi.org/10.1109/ICDIM.2018.8846972, https://doi.org/10.2478/jdis-2022-0015, https://doi.org/10.2478/jdis-2022-0015, https://www.semanticscholar.org/paper/e2e3e36c00c801cf51a47c4724bcf0430e4c0100, https://doi.org/10.1108/DTA-01-2019-0002 (+1)	https://doi.org/10.1007/11875581_124	The WIPO-alpha dataset is used to evaluate the performance of classifiers in patent categorization, specifically focusing on improving micro-F1 scores through ensemble methods and optimizing neural networks for higher accuracy. This dataset enables researchers to benchmark and refine machine learning models for more effective patent classification.; The WIPO-alpha dataset is used in research to evaluate and optimize classification models for patent documents. Studies employ neural networks, SVMs, and ensemble methods to improve accuracy and micro-F1 scores in patent categorization, addressing the hierarchical complexity of the dataset. This enables researchers to enhance the performance of classifiers in handling the nuanced structure of patent data.	WIPO-alpha	
