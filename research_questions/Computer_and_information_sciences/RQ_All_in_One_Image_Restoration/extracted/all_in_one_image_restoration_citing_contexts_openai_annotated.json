{
  "results": [
    {
      "context_text": "For image denoising, we use the combination of BSD400 and WED as training set, and that of BSD68 and Urban100 as testing sets like [55].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "BSD400",
        "WED",
        "BSD68",
        "Urban100"
      ],
      "dataset_descriptions": {
        "BSD400": "Used for training image denoising models, providing a diverse set of natural images with human-segmented annotations.",
        "WED": "Used for training image denoising models, complementing BSD400 with additional images and annotations.",
        "BSD68": "Used for testing image denoising models, offering a smaller set of high-quality images for evaluation.",
        "Urban100": "Used for testing image denoising models, focusing on urban scenes to assess performance on specific image types."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions specific datasets used for training and testing in the context of image denoising. These datasets are clearly named and used for the purpose of training and evaluation.",
      "processing_time": 43.47737526893616,
      "citing_paper_id": "250551851",
      "cited_paper_id": 64193
    },
    {
      "context_text": ", BSD400, BSD68 [31], WED [30], and Urban100 for denoising; Rain100L [49] for deraining; and RESIDE [22] for dehazing.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "BSD400",
        "BSD68",
        "WED",
        "Urban100",
        "Rain100L",
        "RESIDE"
      ],
      "dataset_descriptions": {
        "BSD400": "Used for denoising, providing a benchmark for evaluating image denoising algorithms with natural images.",
        "BSD68": "Used for denoising, offering a smaller subset of images for quick evaluation of denoising methods.",
        "WED": "Used for denoising, containing images with various noise levels to test denoising performance.",
        "Urban100": "Used for denoising, featuring high-resolution urban scenes to assess denoising quality in complex environments.",
        "Rain100L": "Used for deraining, providing synthetic rain images to evaluate the effectiveness of rain removal algorithms.",
        "RESIDE": "Used for dehazing, containing hazy images to test and improve dehazing techniques."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions several datasets used for specific image restoration tasks such as denoising, deraining, and dehazing. These datasets are clearly named and specific.",
      "processing_time": 54.08214497566223,
      "citing_paper_id": "250551851",
      "cited_paper_id": 64193
    },
    {
      "context_text": ", BSD400, BSD68 [31], WED [30], and Urban100 for denoising; Rain100L [49] for deraining; and RESIDE [22] for dehazing.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "BSD400",
        "BSD68",
        "WED",
        "Urban100",
        "Rain100L",
        "RESIDE"
      ],
      "dataset_descriptions": {
        "BSD400": "Used for denoising, providing a benchmark for evaluating image denoising algorithms with natural images.",
        "BSD68": "Used for denoising, offering a smaller subset of images for quick evaluation of denoising methods.",
        "WED": "Used for denoising, containing images with various noise levels to test denoising performance.",
        "Urban100": "Used for denoising, featuring high-resolution urban scenes to assess denoising quality in complex environments.",
        "Rain100L": "Used for deraining, providing synthetic rain images to evaluate the effectiveness of rain removal algorithms.",
        "RESIDE": "Used for dehazing, containing hazy images to test and improve dehazing techniques."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions several datasets used for specific image restoration tasks such as denoising, deraining, and dehazing. These datasets are clearly named and specific.",
      "processing_time": 54.08214497566223,
      "citing_paper_id": "250551851",
      "cited_paper_id": 73439498
    },
    {
      "context_text": "To demonstrate the effectiveness of our network structure, we conduct an ablation study on the BSD68 by removing one of the DCN layer and the SFT layer.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "BSD68"
      ],
      "dataset_descriptions": {
        "BSD68": "Used to conduct an ablation study on the network structure, specifically evaluating the impact of removing the DCN and SFT layers on performance."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions the BSD68 dataset, which is a well-known dataset in image processing and computer vision. It is used for an ablation study to evaluate the effectiveness of the network structure.",
      "processing_time": 28.847737550735474,
      "citing_paper_id": "250551851",
      "cited_paper_id": 64193
    },
    {
      "context_text": "Denoising: Table 1 reports the results on BSD68 and Urban100 comparing with five denoising methods under the one-by-one setting.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "BSD68",
        "Urban100"
      ],
      "dataset_descriptions": {
        "BSD68": "Used to evaluate denoising methods, focusing on performance metrics under the one-by-one setting.",
        "Urban100": "Used to evaluate denoising methods, focusing on performance metrics under the one-by-one setting."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions two specific datasets, BSD68 and Urban100, which are used for denoising experiments. These datasets are relevant to the topic of image restoration.",
      "processing_time": 33.81640362739563,
      "citing_paper_id": "250551851",
      "cited_paper_id": 64193
    },
    {
      "context_text": "To be specific, BSD400 consists of 400 clean natural images and BSD68 includes 68 natural images.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "BSD400",
        "BSD68"
      ],
      "dataset_descriptions": {
        "BSD400": "Used to provide a large set of clean natural images for training and evaluation in image restoration tasks, focusing on the quality and diversity of natural scenes.",
        "BSD68": "Used to provide a smaller set of natural images for evaluation, specifically designed to test the performance of image restoration algorithms on a diverse range of natural scenes."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions two specific datasets, BSD400 and BSD68, which are used in the field of image processing and computer vision. These datasets are clearly identified and are relevant to the topic of all-in-one image restoration.",
      "processing_time": 38.95549750328064,
      "citing_paper_id": "250551851",
      "cited_paper_id": 64193
    },
    {
      "context_text": "Datasets: In our experiments, we use the following six datasets for evaluations, i.e., BSD400, BSD68 [31], WED [30], and Urban100 for denoising; Rain100L [49] for deraining; and RESIDE [22] for dehazing.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "BSD400",
        "BSD68",
        "WED",
        "Urban100",
        "Rain100L",
        "RESIDE"
      ],
      "dataset_descriptions": {
        "BSD400": "Used for denoising evaluations, focusing on natural image restoration and quality assessment.",
        "BSD68": "Used for denoising evaluations, focusing on natural image restoration and quality assessment.",
        "WED": "Used for denoising evaluations, focusing on natural image restoration and quality assessment.",
        "Urban100": "Used for denoising evaluations, focusing on urban scene image restoration and quality assessment.",
        "Rain100L": "Used for deraining evaluations, focusing on removing rain streaks from images and assessing visual quality.",
        "RESIDE": "Used for dehazing evaluations, focusing on improving visibility in hazy images and assessing restoration quality."
      },
      "confidence_score": 1.0,
      "reasoning": "The context explicitly lists several datasets used for evaluating image restoration techniques, including denoising, deraining, and dehazing.",
      "processing_time": 51.94595980644226,
      "citing_paper_id": "250551851",
      "cited_paper_id": 64193
    },
    {
      "context_text": "Datasets: In our experiments, we use the following six datasets for evaluations, i.e., BSD400, BSD68 [31], WED [30], and Urban100 for denoising; Rain100L [49] for deraining; and RESIDE [22] for dehazing.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "BSD400",
        "BSD68",
        "WED",
        "Urban100",
        "Rain100L",
        "RESIDE"
      ],
      "dataset_descriptions": {
        "BSD400": "Used for denoising evaluations, focusing on natural image restoration and quality assessment.",
        "BSD68": "Used for denoising evaluations, focusing on natural image restoration and quality assessment.",
        "WED": "Used for denoising evaluations, focusing on natural image restoration and quality assessment.",
        "Urban100": "Used for denoising evaluations, focusing on urban scene image restoration and quality assessment.",
        "Rain100L": "Used for deraining evaluations, focusing on removing rain streaks from images and assessing visual quality.",
        "RESIDE": "Used for dehazing evaluations, focusing on improving visibility in hazy images and assessing restoration quality."
      },
      "confidence_score": 1.0,
      "reasoning": "The context explicitly lists several datasets used for evaluating image restoration techniques, including denoising, deraining, and dehazing.",
      "processing_time": 51.94595980644226,
      "citing_paper_id": "250551851",
      "cited_paper_id": 73439498
    },
    {
      "context_text": "To this end, we synthesize a degraded version of BSD68 which is with spatially variant noises.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "BSD68"
      ],
      "dataset_descriptions": {
        "BSD68": "Synthesized version of BSD68 with spatially variant noises, used to evaluate image restoration techniques under varying noise conditions."
      },
      "confidence_score": 0.85,
      "reasoning": "The context mentions 'BSD68', which is a known dataset in image processing. However, it is described as a synthesized version with spatially variant noises, indicating a modified form of the original dataset.",
      "processing_time": 28.21607732772827,
      "citing_paper_id": "250551851",
      "cited_paper_id": 64193
    },
    {
      "context_text": "Other image restoration tasks have also faced the similar challenge, such as debluring [2,12,29,32–34,36], deraining [10,17,24,42,46,49, 50,52], and dehazing [1,15,20,25,28,35,37,38].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various image restoration tasks and their associated papers. No verifiable resources are named.",
      "processing_time": 21.634925365447998,
      "citing_paper_id": "250551851",
      "cited_paper_id": 4563057
    },
    {
      "context_text": "Other image restoration tasks have also faced the similar challenge, such as debluring [2,12,29,32–34,36], deraining [10,17,24,42,46,49, 50,52], and dehazing [1,15,20,25,28,35,37,38].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various image restoration tasks and their associated papers. No verifiable resources are named.",
      "processing_time": 21.634925365447998,
      "citing_paper_id": "250551851",
      "cited_paper_id": 49333383
    },
    {
      "context_text": "Other image restoration tasks have also faced the similar challenge, such as debluring [2,12,29,32–34,36], deraining [10,17,24,42,46,49, 50,52], and dehazing [1,15,20,25,28,35,37,38].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various image restoration tasks and their associated papers. No verifiable resources are named.",
      "processing_time": 21.634925365447998,
      "citing_paper_id": "250551851",
      "cited_paper_id": 73439498
    },
    {
      "context_text": "Other image restoration tasks have also faced the similar challenge, such as debluring [2,12,29,32–34,36], deraining [10,17,24,42,46,49, 50,52], and dehazing [1,15,20,25,28,35,37,38].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various image restoration tasks and their associated papers. No verifiable resources are named.",
      "processing_time": 21.634925365447998,
      "citing_paper_id": "250551851",
      "cited_paper_id": 119188226
    },
    {
      "context_text": "Although promising results have been achieved in a specific area, such as denosing [5, 27, 40, 53–55], deblurring [12, 32–34], deraining [10, 11, 17, 46, 50, 52] and dehazing [1, 7, 9, 15, 19, 35, 37, 38], image restoration has encountered the following obstacles in practice.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general areas of research within image restoration. No verifiable resources are identified.",
      "processing_time": 20.697274923324585,
      "citing_paper_id": "250551851",
      "cited_paper_id": 4563057
    },
    {
      "context_text": "Although promising results have been achieved in a specific area, such as denosing [5, 27, 40, 53–55], deblurring [12, 32–34], deraining [10, 11, 17, 46, 50, 52] and dehazing [1, 7, 9, 15, 19, 35, 37, 38], image restoration has encountered the following obstacles in practice.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general areas of research within image restoration. No verifiable resources are identified.",
      "processing_time": 20.697274923324585,
      "citing_paper_id": "250551851",
      "cited_paper_id": 49333383
    },
    {
      "context_text": "Although promising results have been achieved in a specific area, such as denosing [5, 27, 40, 53–55], deblurring [12, 32–34], deraining [10, 11, 17, 46, 50, 52] and dehazing [1, 7, 9, 15, 19, 35, 37, 38], image restoration has encountered the following obstacles in practice.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general areas of research within image restoration. No verifiable resources are identified.",
      "processing_time": 20.697274923324585,
      "citing_paper_id": "250551851",
      "cited_paper_id": 119188226
    },
    {
      "context_text": "where τ is a temperature hyper-parameter per [14, 48] and K denotes the number of negative samples.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only hyper-parameters and methods. The cited papers' titles do not provide additional context to identify datasets.",
      "processing_time": 22.963459253311157,
      "citing_paper_id": "250551851",
      "cited_paper_id": 4591284
    },
    {
      "context_text": "where τ is a temperature hyper-parameter per [14, 48] and K denotes the number of negative samples.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only hyper-parameters and methods. The cited papers' titles do not provide additional context to identify datasets.",
      "processing_time": 22.963459253311157,
      "citing_paper_id": "250551851",
      "cited_paper_id": 207930212
    },
    {
      "context_text": "To optimize AirNet, we employ the ADAM optimizer [18] with the default {βi}(2)i=1 and set the maximal iteration number as 1,500.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only the ADAM optimizer, which is a method.",
      "processing_time": 16.48947834968567,
      "citing_paper_id": "250551851",
      "cited_paper_id": 6628106
    },
    {
      "context_text": "In recent, some works [13, 26, 39, 51] show certain generalizability to different degradations.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to other works. There are no clear identifiers for datasets, models, or methods.",
      "processing_time": 32.79988765716553,
      "citing_paper_id": "250551851",
      "cited_paper_id": 47007607
    },
    {
      "context_text": "For example, AirNet is 1.4 and 0.0074 higher than the best method under the OBO setting in PSNR and SSIM, respectively.",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only performance metrics (PSNR and SSIM) and a comparison with other methods. No verifiable resources are identified.",
      "processing_time": 23.576210975646973,
      "citing_paper_id": "250551851",
      "cited_paper_id": 62732555
    },
    {
      "context_text": "For example, AirNet is 1.4 and 0.0074 higher than the best method under the OBO setting in PSNR and SSIM, respectively.",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only performance metrics (PSNR and SSIM) and a comparison with other methods. No verifiable resources are identified.",
      "processing_time": 23.576210975646973,
      "citing_paper_id": "250551851",
      "cited_paper_id": 207761262
    },
    {
      "context_text": "Metrics DIDMDN [52] UMRL [50] SIRR [46] MSPFN [17] LPNet [11] AirNet\nPSNR 23.79 32.39 32.37 33.50 33.61 34.90 SSIM 0.7731 0.9210 0.9258 0.9480 0.9583 0.9660\nFigure 4.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only metrics and model names. The cited papers do not provide additional context to identify datasets.",
      "processing_time": 17.296959161758423,
      "citing_paper_id": "250551851",
      "cited_paper_id": 62732555
    },
    {
      "context_text": "Metrics DIDMDN [52] UMRL [50] SIRR [46] MSPFN [17] LPNet [11] AirNet\nPSNR 23.79 32.39 32.37 33.50 33.61 34.90 SSIM 0.7731 0.9210 0.9258 0.9480 0.9583 0.9660\nFigure 4.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only metrics and model names. The cited papers do not provide additional context to identify datasets.",
      "processing_time": 17.296959161758423,
      "citing_paper_id": "250551851",
      "cited_paper_id": 119188226
    },
    {
      "context_text": "Metrics DIDMDN [52] UMRL [50] SIRR [46] MSPFN [17] LPNet [11] AirNet\nPSNR 23.79 32.39 32.37 33.50 33.61 34.90 SSIM 0.7731 0.9210 0.9258 0.9480 0.9583 0.9660\nFigure 4.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only metrics and model names. The cited papers do not provide additional context to identify datasets.",
      "processing_time": 17.296959161758423,
      "citing_paper_id": "250551851",
      "cited_paper_id": 207761262
    },
    {
      "context_text": ", Peak Signal-to-Noise Ratio (PSNR) [16] and Structure Similarity (SSIM) [45].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context mentions PSNR and SSIM, which are metrics, not datasets. No specific datasets are mentioned.",
      "processing_time": 19.379305839538574,
      "citing_paper_id": "250551851",
      "cited_paper_id": 62732555
    },
    {
      "context_text": ", Peak Signal-to-Noise Ratio (PSNR) [16] and Structure Similarity (SSIM) [45].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context mentions PSNR and SSIM, which are metrics, not datasets. No specific datasets are mentioned.",
      "processing_time": 19.379305839538574,
      "citing_paper_id": "250551851",
      "cited_paper_id": 207761262
    },
    {
      "context_text": "Evaluation metrics: Following [7, 11, 40], two popular metrics are used for quantitative comparisons, i.e., Peak Signal-to-Noise Ratio (PSNR) [16] and Structure Similarity (SSIM) [45].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context mentions PSNR and SSIM as evaluation metrics but does not refer to any specific datasets. These metrics are excluded as per the instructions.",
      "processing_time": 20.66130018234253,
      "citing_paper_id": "250551851",
      "cited_paper_id": 62732555
    },
    {
      "context_text": "Evaluation metrics: Following [7, 11, 40], two popular metrics are used for quantitative comparisons, i.e., Peak Signal-to-Noise Ratio (PSNR) [16] and Structure Similarity (SSIM) [45].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context mentions PSNR and SSIM as evaluation metrics but does not refer to any specific datasets. These metrics are excluded as per the instructions.",
      "processing_time": 20.66130018234253,
      "citing_paper_id": "250551851",
      "cited_paper_id": 203042751
    },
    {
      "context_text": "Evaluation metrics: Following [7, 11, 40], two popular metrics are used for quantitative comparisons, i.e., Peak Signal-to-Noise Ratio (PSNR) [16] and Structure Similarity (SSIM) [45].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context mentions PSNR and SSIM as evaluation metrics but does not refer to any specific datasets. These metrics are excluded as per the instructions.",
      "processing_time": 20.66130018234253,
      "citing_paper_id": "250551851",
      "cited_paper_id": 207761262
    },
    {
      "context_text": "More specif-\nically, AirNet is 0.03 higher than FDGAN in PSNR.",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a comparison of PSNR values between AirNet and FDGAN.",
      "processing_time": 16.66347312927246,
      "citing_paper_id": "250551851",
      "cited_paper_id": 62732555
    },
    {
      "context_text": "Dehazing As shown in Table 3 and Figure 5, AirNet is slightly better than the best baseline in PSNR.",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a comparison of AirNet with baselines using PSNR.",
      "processing_time": 21.06060242652893,
      "citing_paper_id": "250551851",
      "cited_paper_id": 62732555
    },
    {
      "context_text": "For image deraining, we conduct experiments on Rain100L which consists of 200 rainy-clean training pairs and 100 testing image pairs.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Rain100L"
      ],
      "dataset_descriptions": {
        "Rain100L": "Used for image deraining experiments, consisting of 200 rainy-clean training pairs and 100 testing image pairs to evaluate deraining performance."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions a specific dataset, Rain100L, used for image deraining experiments. The dataset is clearly identified and its usage is described.",
      "processing_time": 28.959079027175903,
      "citing_paper_id": "250551851",
      "cited_paper_id": 73439498
    },
    {
      "context_text": "Quantitative results of image deraining on the Rain100L dataset.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Rain100L"
      ],
      "dataset_descriptions": {
        "Rain100L": "Used to evaluate image deraining techniques, focusing on quantitative performance metrics in the context of single-image rain removal."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions the Rain100L dataset, which is a specific dataset used for evaluating image deraining techniques.",
      "processing_time": 25.632705450057983,
      "citing_paper_id": "250551851",
      "cited_paper_id": 73439498
    },
    {
      "context_text": "Comparisons of the SOTA derain methods on the Rain100L database.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Rain100L"
      ],
      "dataset_descriptions": {
        "Rain100L": "Used to evaluate state-of-the-art derain methods, focusing on performance metrics and visual quality improvements in rain removal from images."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions the 'Rain100L database' which is a specific dataset used for evaluating derain methods. The dataset is clearly identified and relevant to the research topic of image restoration.",
      "processing_time": 28.669726371765137,
      "citing_paper_id": "250551851",
      "cited_paper_id": 73439498
    },
    {
      "context_text": "To be specific, the denoising baselines contain CBM3D [5], DnCNN [53], IRCNN [54], FFDNet [55] and BRDNet [40]. the deraining baselines are DIDMDN [52], UMRL [50], SIRR [46], MSPFN [17], and LPNet [11].",
      "catation_intent": "none",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span mentions several methods and models but does not refer to any specific datasets. The context is focused on describing the baselines used for denoising and deraining, which are methods rather than datasets.",
      "processing_time": 24.500290393829346,
      "citing_paper_id": "250551851",
      "cited_paper_id": 119188226
    },
    {
      "context_text": "To be specific, the denoising baselines contain CBM3D [5], DnCNN [53], IRCNN [54], FFDNet [55] and BRDNet [40]. the deraining baselines are DIDMDN [52], UMRL [50], SIRR [46], MSPFN [17], and LPNet [11].",
      "catation_intent": "none",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span mentions several methods and models but does not refer to any specific datasets. The context is focused on describing the baselines used for denoising and deraining, which are methods rather than datasets.",
      "processing_time": 24.500290393829346,
      "citing_paper_id": "250551851",
      "cited_paper_id": 203042751
    },
    {
      "context_text": "Metrics DIDMDN [52] UMRL [50] SIRR [46] MSPFN [17] LPNet [11] AirNet",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only model names and metrics. There are no clear identifiers for datasets.",
      "processing_time": 19.354090929031372,
      "citing_paper_id": "250551851",
      "cited_paper_id": 119188226
    },
    {
      "context_text": "the deraining baselines are DIDMDN [52], UMRL [50], SIRR [46], MSPFN [17], and LPNet [11].",
      "catation_intent": "none",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span mentions several methods/models but does not refer to any specific datasets. No dataset names are present in the context.",
      "processing_time": 17.267072200775146,
      "citing_paper_id": "250551851",
      "cited_paper_id": 119188226
    },
    {
      "context_text": "To this end, we choose five IRSD methods (i.e., BRDNet [40], LPNet [11], FDGAN [7] and MPRNet [51]) and one IRMD method (i.e., DL [8]) as baselines.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span mentions several methods/models but does not refer to any specific datasets. The context is focused on comparing different image restoration methods.",
      "processing_time": 19.349358558654785,
      "citing_paper_id": "250551851",
      "cited_paper_id": 203042751
    },
    {
      "context_text": "Evaluation metrics: Following [7, 11, 40], two popular metrics are used for quantitative comparisons, i.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only evaluation metrics. No dataset names are present in the text.",
      "processing_time": 20.64717674255371,
      "citing_paper_id": "250551851",
      "cited_paper_id": 203042751
    },
    {
      "context_text": "To be specific, the denoising baselines contain CBM3D [5], DnCNN [53], IRCNN [54], FFDNet [55] and BRDNet [40].",
      "catation_intent": "none",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span mentions several methods/models (CBM3D, DnCNN, IRCNN, FFDNet, BRDNet) but does not reference any specific datasets. The context is focused on denoising baselines, which are methods, not datasets.",
      "processing_time": 24.712950229644775,
      "citing_paper_id": "250551851",
      "cited_paper_id": 203042751
    },
    {
      "context_text": "Method CBM3D [5] DnCNN [53] IRCNN [54] FFDNet [55] BRDNet [40] DL [8] Ours",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods and models. There are no clear identifiers for datasets within the provided context.",
      "processing_time": 18.180343627929688,
      "citing_paper_id": "250551851",
      "cited_paper_id": 203042751
    },
    {
      "context_text": ", BRDNet [40], LPNet [11], FDGAN [7] and MPRNet [51]) and one IRMD method (i.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods and models. There are no verifiable resources that meet the criteria.",
      "processing_time": 18.15872883796692,
      "citing_paper_id": "250551851",
      "cited_paper_id": 203042751
    },
    {
      "context_text": "Contrastive learning [4, 14, 41] is the state-of-the-art unsupervised representation learning method, which aims at maximizing the similarity between positive pairs while minimizing that of negative pairs, where the positive and negative pairs are obtained through data augmentations.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a method (contrastive learning) and its purpose. No verifiable resources are identified.",
      "processing_time": 18.593758821487427,
      "citing_paper_id": "250551851",
      "cited_paper_id": 207930212
    },
    {
      "context_text": "On the one hand, the methods [3, 8, 23] have to specify the corruption type and ratio, whereas our method does not.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods. There are no verifiable resources or datasets mentioned.",
      "processing_time": 19.3140766620636,
      "citing_paper_id": "250551851",
      "cited_paper_id": 227239228
    },
    {
      "context_text": "Notably, the referred all-in-one solution is different from existing so-called unified image restoration methods [3, 8, 23] in given aspects.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only references to other papers. There are no clear identifiers for datasets, benchmarks, or other verifiable resources.",
      "processing_time": 23.236645936965942,
      "citing_paper_id": "250551851",
      "cited_paper_id": 227239228
    },
    {
      "context_text": "Image Restoration for Multiple Degradations: Recently, there are some works [3, 23] shift their attention to IRMD by adopting a multi-input and -output network structure.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a shift in attention towards a multi-input and -output network structure for image restoration.",
      "processing_time": 17.826592922210693,
      "citing_paper_id": "250551851",
      "cited_paper_id": 227239228
    },
    {
      "context_text": "[3] proposed a transformer-based image restoration method which handles multiple-degradations by using an architecture of multiheads and multi-tails.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for image restoration.",
      "processing_time": 18.97087812423706,
      "citing_paper_id": "250551851",
      "cited_paper_id": 227239228
    },
    {
      "context_text": "Denoising task evaluation is performed on the BSD68 [41] and Urban100 [25] datasets.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "BSD68",
        "Urban100"
      ],
      "dataset_descriptions": {
        "BSD68": "Used for evaluating denoising algorithms, focusing on natural image restoration. The dataset consists of 68 images with ground truth noise levels.",
        "Urban100": "Used for evaluating denoising algorithms, focusing on urban scene restoration. The dataset consists of 100 high-resolution images of urban scenes."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions two specific datasets, BSD68 and Urban100, which are used for denoising task evaluation. These are well-known datasets in the field of image restoration.",
      "processing_time": 38.05803394317627,
      "citing_paper_id": "268553835",
      "cited_paper_id": 64193
    },
    {
      "context_text": "The images are obtained from LOL-v1 [64], SOTS [32], Rain100L [32], GoPro [44], and BSD68 [41], respectively.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "LOL-v1",
        "SOTS",
        "Rain100L",
        "GoPro",
        "BSD68"
      ],
      "dataset_descriptions": {
        "LOL-v1": "Used to evaluate low-light image enhancement techniques, focusing on improving visibility and color accuracy in dark conditions.",
        "SOTS": "Applied to assess dehazing methods, specifically targeting synthetic and real-world hazy images to improve clarity and detail.",
        "Rain100L": "Employed to test rain removal algorithms, concentrating on synthetic rainy images to enhance visual quality and reduce rain streaks.",
        "GoPro": "Utilized to evaluate video deblurring techniques, focusing on reducing motion blur in high-speed camera footage.",
        "BSD68": "Used to assess denoising algorithms, specifically targeting grayscale images to reduce noise while preserving edges and details."
      },
      "confidence_score": 1.0,
      "reasoning": "The citation mentions specific datasets used for image restoration experiments, which are directly relevant to the research topic.",
      "processing_time": 49.93006443977356,
      "citing_paper_id": "268553835",
      "cited_paper_id": 64193
    },
    {
      "context_text": "Table 4: Denoising results in the single-task setting on Urban100 [25] and BSD68 [41].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Urban100",
        "BSD68"
      ],
      "dataset_descriptions": {
        "Urban100": "Used to evaluate denoising performance in the single-task setting, focusing on high-resolution urban images.",
        "BSD68": "Used to assess denoising effectiveness in the single-task setting, comprising a diverse set of natural images."
      },
      "confidence_score": 1.0,
      "reasoning": "The citation mentions two specific datasets, Urban100 and BSD68, which are used for denoising experiments in the single-task setting.",
      "processing_time": 33.41320013999939,
      "citing_paper_id": "268553835",
      "cited_paper_id": 64193
    },
    {
      "context_text": "Table 6: Image denoising results of directly applying the pre-trained model under the five-degradation setting to the Urban100 [25], Kodak24 [53] and BSD68 [41] datasets.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Urban100",
        "Kodak24",
        "BSD68"
      ],
      "dataset_descriptions": {
        "Urban100": "Used to evaluate image denoising performance, focusing on urban scenes with complex textures and structures.",
        "Kodak24": "Used to assess image denoising quality, emphasizing natural images with diverse content and high resolution.",
        "BSD68": "Used to test image denoising effectiveness, particularly on a variety of image types and degradation levels."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions three specific datasets used for image denoising experiments. These datasets are commonly used in image processing and computer vision for evaluating restoration algorithms.",
      "processing_time": 38.473384618759155,
      "citing_paper_id": "268553835",
      "cited_paper_id": 64193
    },
    {
      "context_text": "Table 6: Image denoising results of directly applying the pre-trained model under the five-degradation setting to the Urban100 [25], Kodak24 [53] and BSD68 [41] datasets.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Urban100",
        "Kodak24",
        "BSD68"
      ],
      "dataset_descriptions": {
        "Urban100": "Used to evaluate image denoising performance, focusing on urban scenes with complex textures and structures.",
        "Kodak24": "Used to assess image denoising quality, emphasizing natural images with diverse content and high resolution.",
        "BSD68": "Used to test image denoising effectiveness, particularly on a variety of image types and degradation levels."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions three specific datasets used for image denoising experiments. These datasets are commonly used in image processing and computer vision for evaluating restoration algorithms.",
      "processing_time": 38.473384618759155,
      "citing_paper_id": "268553835",
      "cited_paper_id": null
    },
    {
      "context_text": "…deep learning, a great number of convolutional neural network (CNN) based methods have been proposed and attained superior performance over traditional methods on various image restoration tasks, such as image denoising [77,79], dehazing [47,51], deraining [26,50], and motion de-blurring [13, 16].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only general tasks and methods. No dataset names are provided.",
      "processing_time": 18.929038524627686,
      "citing_paper_id": "268553835",
      "cited_paper_id": 996788
    },
    {
      "context_text": "…deep learning, a great number of convolutional neural network (CNN) based methods have been proposed and attained superior performance over traditional methods on various image restoration tasks, such as image denoising [77,79], dehazing [47,51], deraining [26,50], and motion de-blurring [13, 16].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only general tasks and methods. No dataset names are provided.",
      "processing_time": 18.929038524627686,
      "citing_paper_id": "268553835",
      "cited_paper_id": 214623217
    },
    {
      "context_text": "Deep-learning methods learn image priors either implicitly from data [18,37,45,49,52,78,80], or explicitly by incorporating task-specific knowledge into the network architectures [9, 60, 62, 70, 72, 73].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general references to deep-learning methods and image restoration techniques.",
      "processing_time": 20.634135484695435,
      "citing_paper_id": "268553835",
      "cited_paper_id": 1900475
    },
    {
      "context_text": "Deep-learning methods learn image priors either implicitly from data [18,37,45,49,52,78,80], or explicitly by incorporating task-specific knowledge into the network architectures [9, 60, 62, 70, 72, 73].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general references to deep-learning methods and image restoration techniques.",
      "processing_time": 20.634135484695435,
      "citing_paper_id": "268553835",
      "cited_paper_id": 202733344
    },
    {
      "context_text": "To this end, deep neural network-based image restoration approaches [18,45,49,52,59,71,78, 80] have emerged as preferable choices over the conventional handcrafted algorithms [19,24,28,29,37,42,57].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to deep neural network-based image restoration approaches and conventional handcrafted algorithms.",
      "processing_time": 18.579360961914062,
      "citing_paper_id": "268553835",
      "cited_paper_id": 1900475
    },
    {
      "context_text": "To this end, deep neural network-based image restoration approaches [18,45,49,52,59,71,78, 80] have emerged as preferable choices over the conventional handcrafted algorithms [19,24,28,29,37,42,57].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to deep neural network-based image restoration approaches and conventional handcrafted algorithms.",
      "processing_time": 18.579360961914062,
      "citing_paper_id": "268553835",
      "cited_paper_id": 202733344
    },
    {
      "context_text": "To this end, deep neural network-based image restoration approaches [18,45,49,52,59,71,78, 80] have emerged as preferable choices over the conventional handcrafted algorithms [19,24,28,29,37,42,57].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to deep neural network-based image restoration approaches and conventional handcrafted algorithms.",
      "processing_time": 18.579360961914062,
      "citing_paper_id": "268553835",
      "cited_paper_id": 206769988
    },
    {
      "context_text": "Starting from these clean images of BSD400 [2] and WED [40], we generate their corresponding noisy versions by adding Gaussian noise with varying levels ( σ ∈ { 15 , 25 , 50 } ).",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "BSD400",
        "WED"
      ],
      "dataset_descriptions": {
        "BSD400": "Used to generate noisy images for image restoration experiments by adding Gaussian noise with varying levels (σ ∈ {15, 25, 50}).",
        "WED": "Used to generate noisy images for image restoration experiments by adding Gaussian noise with varying levels (σ ∈ {15, 25, 50})."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions BSD400 and WED as specific datasets used to generate noisy images for image restoration experiments.",
      "processing_time": 37.195643186569214,
      "citing_paper_id": "268553835",
      "cited_paper_id": 4840263
    },
    {
      "context_text": "For single-task image denoising, we combine images of BSD400 [2] and WED [40] datasets for model training; the BSD400 encompasses 400 training images, while the WED dataset consists of 4,744 images.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "BSD400",
        "WED"
      ],
      "dataset_descriptions": {
        "BSD400": "Used for training a single-task image denoising model, providing 400 training images to enhance model performance.",
        "WED": "Used for training a single-task image denoising model, contributing 4,744 images to improve model robustness."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions two specific datasets, BSD400 and WED, which are used for training a model for single-task image denoising. Both datasets have clear identifiers and are relevant to the research topic.",
      "processing_time": 38.0340781211853,
      "citing_paper_id": "268553835",
      "cited_paper_id": 4840263
    },
    {
      "context_text": "MDTA first normalizes the input X ∈ R H × W × C using a layer normalization operator [3], and then generates the query ( Q ∈ R H × W × C ), key ( K ∈ R H × W × C ), and value ( V ∈ R H × W × C ) projections using combinations of 1 × 1 convolution and 3 × 3 depth-wise convolution layers.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (Layer Normalization).",
      "processing_time": 16.87908387184143,
      "citing_paper_id": "268553835",
      "cited_paper_id": 8236317
    },
    {
      "context_text": "…degradation types and levels which hinders their broader application, or require training separate copies of the same network on different degradation types, which is computationally expensive and tedious procedure, and maybe infeasible solution for deployment on resource-constraint edge-devices.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only general issues with existing methods in image restoration.",
      "processing_time": 30.761719703674316,
      "citing_paper_id": "268553835",
      "cited_paper_id": 8671030
    },
    {
      "context_text": "Particularly, in the restored images, our method preserves better structural fidelity and fine textures.",
      "catation_intent": "reusable resource",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only the performance of a method. No dataset names are present in the text.",
      "processing_time": 20.615688800811768,
      "citing_paper_id": "268553835",
      "cited_paper_id": 8671030
    },
    {
      "context_text": "Particularly, in the restored images, our method preserves better structural fidelity and fine textures.",
      "catation_intent": "reusable resource",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only the performance of a method. No dataset names are present in the text.",
      "processing_time": 20.615688800811768,
      "citing_paper_id": "268553835",
      "cited_paper_id": 250551851
    },
    {
      "context_text": "…architecture of AdaIR employs a 4-level encoder-decoder structure, with varying numbers of Transformer blocks (TB) at each level, specifically [4, 6, 6, 8] from Table 1: Comparisons under the three-degradation all-in-one setting: a unified model is trained on a combined set of images obtained…",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific, verifiable datasets. It only describes the architecture and training setup of a model.",
      "processing_time": 17.798795223236084,
      "citing_paper_id": "268553835",
      "cited_paper_id": 14092238
    },
    {
      "context_text": "…of AdaIR employs a 4-level encoder-decoder structure, with varying numbers of Transformer blocks (TB) at each level, specifically [4, 6, 6, 8] from Table 1: Comparisons under the three-degradation all-in-one setting: a unified model is trained on a combined set of images obtained from…",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific, verifiable datasets. It only describes the architecture of a model and a training setting.",
      "processing_time": 17.19440221786499,
      "citing_paper_id": "268553835",
      "cited_paper_id": 14092238
    },
    {
      "context_text": "Since it is a highly ill-posed problem, many conventional methods have been proposed that utilize hand-crafted features and assumptions to reduce the solution space [4, 24].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only conventional methods and their approaches to solving an ill-posed problem.",
      "processing_time": 16.375813245773315,
      "citing_paper_id": "268553835",
      "cited_paper_id": 18774783
    },
    {
      "context_text": "The architecture of AdaIR employs a 4-level encoder-decoder structure, with varying numbers of Transformer blocks (TB) at each level, specifically [4, 6, 6, 8] from Table 1: Comparisons under the three-degradation all-in-one setting: a unified model is trained on a combined set of images obtained…",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a model architecture and degradation settings. There are no clear identifiers for datasets.",
      "processing_time": 17.510643243789673,
      "citing_paper_id": "268553835",
      "cited_paper_id": 18774783
    },
    {
      "context_text": "Formally, given a degraded image I ∈ R H × W × 3 , AdaIR first extracts shallow features Y 0 ∈ R H × W × C using a 3 × 3 convolution layer; where H × W denotes the spatial size and C represents the number of channels.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only a method for image restoration. No dataset names are present in the text.",
      "processing_time": 16.845601081848145,
      "citing_paper_id": "268553835",
      "cited_paper_id": 49333383
    },
    {
      "context_text": "As seen, different tasks pay different attention to different frequency subbands.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only a general statement about tasks and frequency subbands.",
      "processing_time": 15.917367458343506,
      "citing_paper_id": "268553835",
      "cited_paper_id": 49333383
    },
    {
      "context_text": "H-L Unit: This unit computes the spatial attention map from high-frequency mined features that are then used to complement features of the low-frequency branch.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a method or module for computing spatial attention maps.",
      "processing_time": 31.02398681640625,
      "citing_paper_id": "268553835",
      "cited_paper_id": 49867180
    },
    {
      "context_text": "We compare AdaIR against various general image restoration methods (BRDNet [56], LPNet [22], FDGAN [20], and MPRNet [73]), as well as specialized all-in-one approaches (DL [21], AirNet [33], and PromptIR [46]).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions various methods and models but does not specify any datasets. The cited papers' titles do not indicate the presence of specific datasets.",
      "processing_time": 18.88434362411499,
      "citing_paper_id": "268553835",
      "cited_paper_id": 198185751
    },
    {
      "context_text": "We compare AdaIR against various general image restoration methods (BRDNet [56], LPNet [22], FDGAN [20], and MPRNet [73]), as well as specialized all-in-one approaches (DL [21], AirNet [33], and PromptIR [46]).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions various methods and models but does not specify any datasets. The cited papers' titles do not indicate the presence of specific datasets.",
      "processing_time": 18.88434362411499,
      "citing_paper_id": "268553835",
      "cited_paper_id": 203042751
    },
    {
      "context_text": "We compare AdaIR against various general image restoration methods (BRDNet [56], LPNet [22], FDGAN [20], and MPRNet [73]), as well as specialized all-in-one approaches (DL [21], AirNet [33], and PromptIR [46]).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions various methods and models but does not specify any datasets. The cited papers' titles do not indicate the presence of specific datasets.",
      "processing_time": 18.88434362411499,
      "citing_paper_id": "268553835",
      "cited_paper_id": 210838848
    },
    {
      "context_text": "We compare AdaIR against various general image restoration methods (BRDNet [56], LPNet [22], FDGAN [20], and MPRNet [73]), as well as specialized all-in-one approaches (DL [21], AirNet [33], and PromptIR [46]).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions various methods and models but does not specify any datasets. The cited papers' titles do not indicate the presence of specific datasets.",
      "processing_time": 18.88434362411499,
      "citing_paper_id": "268553835",
      "cited_paper_id": 250551851
    },
    {
      "context_text": "For example, there are larger discrepancies in low frequency between degraded and target image pairs of the low-light image enhancement and dehazing datasets.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.3,
      "reasoning": "The context mentions 'low-light image enhancement and dehazing datasets' but does not provide specific names. The cited papers do not clarify specific datasets either.",
      "processing_time": 23.167009353637695,
      "citing_paper_id": "268553835",
      "cited_paper_id": 216562731
    },
    {
      "context_text": "For example, there are larger discrepancies in low frequency between degraded and target image pairs of the low-light image enhancement and dehazing datasets.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.3,
      "reasoning": "The context mentions 'low-light image enhancement and dehazing datasets' but does not provide specific names. The cited papers do not clarify specific datasets either.",
      "processing_time": 23.167009353637695,
      "citing_paper_id": "268553835",
      "cited_paper_id": 247939726
    },
    {
      "context_text": "…of AdaIR employs a 4-level encoder-decoder structure, with varying numbers of Transformer blocks (TB) at each level, specifically [4, 6, 6, 8] from Table 1: Comparisons under the three-degradation all-in-one setting: a unified model is trained on a combined set of images obtained from all…",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific, verifiable datasets. It only describes the architecture of a model and a training setting.",
      "processing_time": 17.171101331710815,
      "citing_paper_id": "268553835",
      "cited_paper_id": 227239228
    },
    {
      "context_text": "Early unified models [8, 34] employ distinct encoder and decoder heads to attend to different restoration tasks.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models or methods. There are no clear identifiers for datasets in the provided context.",
      "processing_time": 19.72603678703308,
      "citing_paper_id": "268553835",
      "cited_paper_id": 227239228
    },
    {
      "context_text": "Right , the sub-graph illustrates the mean values of Fourier spectra on the square of length shown on the x-axis, across five tasks.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a sub-graph illustrating mean values of Fourier spectra across tasks. No clear, verifiable dataset names are provided.",
      "processing_time": 19.724576711654663,
      "citing_paper_id": "268553835",
      "cited_paper_id": 231639270
    },
    {
      "context_text": "Right , the sub-graph illustrates the mean values of Fourier spectra on the square of length shown on the x-axis, across five tasks.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a sub-graph illustrating mean values of Fourier spectra across tasks. No clear, verifiable dataset names are provided.",
      "processing_time": 19.724576711654663,
      "citing_paper_id": "268553835",
      "cited_paper_id": 247939726
    },
    {
      "context_text": "Next, we adapt the multi-dconv head transposed cross attention (Fig.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only a method or model. There are no verifiable resources or datasets mentioned.",
      "processing_time": 19.722090244293213,
      "citing_paper_id": "268553835",
      "cited_paper_id": 232404237
    },
    {
      "context_text": "AirNet [33] PromptIR [46] Ours cess in the natural language processing domain [5,30,54].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only models or methods. There are no clear identifiers for datasets within the given context.",
      "processing_time": 20.166414260864258,
      "citing_paper_id": "268553835",
      "cited_paper_id": 233296808
    },
    {
      "context_text": "On the decoder side, the latent features Y l are processed with interleaved Adaptive Frequency Learning Block (AFLB) and TBs to progressively reconstruct high-resolution clean output.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only model components and methodologies. No verifiable resources are identified.",
      "processing_time": 19.240094900131226,
      "citing_paper_id": "268553835",
      "cited_paper_id": 236976210
    },
    {
      "context_text": "On the decoder side, the latent features Y l are processed with interleaved Adaptive Frequency Learning Block (AFLB) and TBs to progressively reconstruct high-resolution clean output.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only model components and methodologies. No verifiable resources are identified.",
      "processing_time": 19.240094900131226,
      "citing_paper_id": "268553835",
      "cited_paper_id": 244346144
    },
    {
      "context_text": "Next, we describe the two key components of AFLB: (1) F requency Mi ning M odule (FMiM) and F requency Mo dulation M odule (FMoM).",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only modules of a method. The cited papers do not provide additional context about datasets.",
      "processing_time": 20.158878803253174,
      "citing_paper_id": "268553835",
      "cited_paper_id": 237627482
    },
    {
      "context_text": "Next, we describe the two key components of AFLB: (1) F requency Mi ning M odule (FMiM) and F requency Mo dulation M odule (FMoM).",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only modules of a method. The cited papers do not provide additional context about datasets.",
      "processing_time": 20.158878803253174,
      "citing_paper_id": "268553835",
      "cited_paper_id": 250551851
    },
    {
      "context_text": "52 dB PSNR. Finally, Table 7(e) shows that the overall AdaIR brings 3 .",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only performance metrics and results.",
      "processing_time": 17.484694719314575,
      "citing_paper_id": "268553835",
      "cited_paper_id": 244346144
    },
    {
      "context_text": "In tables, the best and second-best image fidelity scores (PSNR and SSIM [63]) are highlighted in red and blue, respectively.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only metrics (PSNR and SSIM).",
      "processing_time": 15.19308876991272,
      "citing_paper_id": "268553835",
      "cited_paper_id": 244346144
    },
    {
      "context_text": "1), we specifically design the Adaptive Frequency Learning Block (AFLB) that extracts low-and high-frequency components from the input features and then modulate them to accentuate the corresponding informative subbands for each degradation.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only a method (AFLB) for image restoration.",
      "processing_time": 15.878814220428467,
      "citing_paper_id": "268553835",
      "cited_paper_id": 244346144
    },
    {
      "context_text": "22 dB PSNR (second column).",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a performance metric (PSNR).",
      "processing_time": 15.194188117980957,
      "citing_paper_id": "268553835",
      "cited_paper_id": 244346144
    },
    {
      "context_text": "63 dB PSNR gain over the recent best method Promp-tIR [46], and 1 .",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a comparison with a method called PromptIR.",
      "processing_time": 15.85896611213684,
      "citing_paper_id": "268553835",
      "cited_paper_id": 244346144
    },
    {
      "context_text": "The results are PSNR scores.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only a metric (PSNR) which is excluded.",
      "processing_time": 18.479143857955933,
      "citing_paper_id": "268553835",
      "cited_paper_id": 244346144
    },
    {
      "context_text": "58 dB PSNR over the baseline model, using only a fixed mask to decompose the spectra of input images.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a performance metric (PSNR) and a method (fixed mask).",
      "processing_time": 17.439183712005615,
      "citing_paper_id": "268553835",
      "cited_paper_id": 244346144
    },
    {
      "context_text": "We introduce the Adaptive Frequency Learning Block (AFLB), which is a plugin block specifically designed for easy integration into existing image restoration architectures.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (AFLB) for image restoration.",
      "processing_time": 15.863959074020386,
      "citing_paper_id": "268553835",
      "cited_paper_id": 244346144
    },
    {
      "context_text": "Since we want to adaptively extract different frequency parts from the input features X , we design a lightweight Mask Generation Block (MGB) to generate a 2D mask that serves as a frequency boundary to separate the spectra of input image I .",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a method for image restoration. No verifiable resources are identified.",
      "processing_time": 17.110886096954346,
      "citing_paper_id": "268553835",
      "cited_paper_id": 244346144
    },
    {
      "context_text": "On Urban100 [25] for the noise level σ = 25 , AdaIR produces a significant performance gain of 0.39 dB PSNR over IDR [76].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Urban100"
      ],
      "dataset_descriptions": {
        "Urban100": "Used to evaluate image restoration performance at a noise level of σ = 25, comparing AdaIR against IDR in terms of PSNR improvement."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions 'Urban100' as a dataset used for evaluating image restoration performance at a specific noise level.",
      "processing_time": 40.915995836257935,
      "citing_paper_id": "268553835",
      "cited_paper_id": 244346144
    },
    {
      "context_text": "In contrast, the frequency differences are generally evenly distributed for image denoising. serves as a vital component in numerous downstream applications across diverse domains, including image/video content creation, surveillance, medical imaging, and remote sensing.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The provided context does not mention any specific datasets, only general applications of image restoration. No dataset names are present in the citation span.",
      "processing_time": 17.719969987869263,
      "citing_paper_id": "268553835",
      "cited_paper_id": 244346144
    },
    {
      "context_text": "In contrast, the frequency differences are generally evenly distributed for image denoising. serves as a vital component in numerous downstream applications across diverse domains, including image/video content creation, surveillance, medical imaging, and remote sensing.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The provided context does not mention any specific datasets, only general applications of image restoration. No dataset names are present in the citation span.",
      "processing_time": 17.719969987869263,
      "citing_paper_id": "268553835",
      "cited_paper_id": 245837508
    },
    {
      "context_text": "In contrast, the frequency differences are generally evenly distributed for image denoising. serves as a vital component in numerous downstream applications across diverse domains, including image/video content creation, surveillance, medical imaging, and remote sensing.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The provided context does not mention any specific datasets, only general applications of image restoration. No dataset names are present in the citation span.",
      "processing_time": 17.719969987869263,
      "citing_paper_id": "268553835",
      "cited_paper_id": 248426764
    },
    {
      "context_text": "In contrast, the frequency differences are generally evenly distributed for image denoising. serves as a vital component in numerous downstream applications across diverse domains, including image/video content creation, surveillance, medical imaging, and remote sensing.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The provided context does not mention any specific datasets, only general applications of image restoration. No dataset names are present in the citation span.",
      "processing_time": 17.719969987869263,
      "citing_paper_id": "268553835",
      "cited_paper_id": 270736284
    },
    {
      "context_text": "This strategy provides PSNR of 30 .",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only a performance metric (PSNR).",
      "processing_time": 18.041961669921875,
      "citing_paper_id": "268553835",
      "cited_paper_id": 244346144
    },
    {
      "context_text": "37 dB PSNR; see Table 7(c).",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a performance metric (PSNR).",
      "processing_time": 15.377471923828125,
      "citing_paper_id": "268553835",
      "cited_paper_id": 244346144
    },
    {
      "context_text": "Table 2 reports dehazing results; compared to the previous all-in-one approaches PromptIR [46] and AirNet [33], our method obtains PSNR gains of 0 .",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only comparisons with other methods. No verifiable resources are identified.",
      "processing_time": 19.21768617630005,
      "citing_paper_id": "268553835",
      "cited_paper_id": 244346144
    },
    {
      "context_text": "The results of the two branches are added together, on which the sigmoid function is applied to produce the final attention descriptor A L − H ∈ R 1 × 1 × C , which is used to modulate the mined high-frequency features X h .",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only a method for producing an attention descriptor for image restoration.",
      "processing_time": 16.482810735702515,
      "citing_paper_id": "268553835",
      "cited_paper_id": 244346144
    },
    {
      "context_text": "Such solutions, though perform well on some datasets, may not generalize well to complicated real-world images [81].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific dataset names, only a general reference to 'some datasets'. No multi-word proper nouns, acronyms, or hyphenated names with digits are present.",
      "processing_time": 23.85426616668701,
      "citing_paper_id": "268553835",
      "cited_paper_id": 246285588
    },
    {
      "context_text": "Since different types of degradations affect image content at different frequency bands (as shown in Fig.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only a general statement about image degradation. No clear, verifiable resource names are present.",
      "processing_time": 18.80876851081848,
      "citing_paper_id": "268553835",
      "cited_paper_id": 248426764
    },
    {
      "context_text": "A similar performance trend can be observed in image quality scores provided in Table 4 for denoising.",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific dataset names, only a general reference to image quality scores for denoising.",
      "processing_time": 20.514920949935913,
      "citing_paper_id": "268553835",
      "cited_paper_id": 250551851
    },
    {
      "context_text": "Similarly, on the deraining task, our AdaIR advances the state-of-the-art [46] by 1 .",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a performance improvement on a deraining task.",
      "processing_time": 16.117537021636963,
      "citing_paper_id": "268553835",
      "cited_paper_id": 250551851
    },
    {
      "context_text": "The top super-row methods denote the general image restoration approaches, and the rest are specialized all-in-one approaches.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general and specialized approaches to image restoration.",
      "processing_time": 18.801135778427124,
      "citing_paper_id": "268553835",
      "cited_paper_id": 250551851
    },
    {
      "context_text": "For instance, on the one hand, noisy and rainy images are contaminated with high-frequency content, while on the other hand, low-light and hazy images are dominated by low-frequency degraded content, thus indicating the need to treat each restoration task on its own merits.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general types of image degradation. No clear, verifiable resources are identified.",
      "processing_time": 18.433504343032837,
      "citing_paper_id": "268553835",
      "cited_paper_id": 250551851
    },
    {
      "context_text": "Recently, several prompt-learning-based schemes have been proposed [1, 14, 39, 46].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to prompt-learning-based schemes. No dataset names are present in the context.",
      "processing_time": 22.075374841690063,
      "citing_paper_id": "268553835",
      "cited_paper_id": 259243623
    },
    {
      "context_text": "Recently, several prompt-learning-based schemes have been proposed [1, 14, 39, 46].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to prompt-learning-based schemes. No dataset names are present in the context.",
      "processing_time": 22.075374841690063,
      "citing_paper_id": "268553835",
      "cited_paper_id": 265659287
    },
    {
      "context_text": "Recently, several prompt-learning-based schemes have been proposed [1, 14, 39, 46].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to prompt-learning-based schemes. No dataset names are present in the context.",
      "processing_time": 22.075374841690063,
      "citing_paper_id": "268553835",
      "cited_paper_id": 267320695
    },
    {
      "context_text": "…knowledge collection based on underlying physics characteristics of degradation types, and the second stage is responsible for ingredients-oriented knowledge integration that progressively restores the image; or developing prompt-learning strategies [39,46] inspired from their suc-AdaIR",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method or strategy for image restoration.",
      "processing_time": 15.13010859489441,
      "citing_paper_id": "268553835",
      "cited_paper_id": 259243623
    },
    {
      "context_text": "Recently, an increasing number of attempts have been made [33, 39, 46, 76] to address multiple degradations with a single model.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to papers addressing multiple degradations with a single model.",
      "processing_time": 16.75191617012024,
      "citing_paper_id": "268553835",
      "cited_paper_id": 259243623
    },
    {
      "context_text": "When averaged across various restoration tasks and settings, our AdaIR obtains 0 .",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a performance metric across various restoration tasks.",
      "processing_time": 15.593212604522705,
      "citing_paper_id": "268553835",
      "cited_paper_id": 267199899
    },
    {
      "context_text": "The encoder takes high-resolution features Y 0 as input, and progressively transforms them into a lower-resolution latent representation Y l ∈ R H 8 × W 8 × 8 C .",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only describes the transformation process within an encoder. No verifiable resources are identified.",
      "processing_time": 17.05953311920166,
      "citing_paper_id": "268553835",
      "cited_paper_id": 271500033
    },
    {
      "context_text": "The spectra are all resized to 320 × 320 for comparisons.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a resizing operation for spectra. No verifiable resources are identified.",
      "processing_time": 16.744176149368286,
      "citing_paper_id": "268553835",
      "cited_paper_id": 273644217
    },
    {
      "context_text": "Testing is performed on CBSD68 [49], Urban100 [50], and Kodak24 [51] datasets. ii) Image dehazing: we use the OTS dataset of RESIDE-β [52] with 72,135 pairs for training and 500 images from SOTS-Outdoor [52] dataset for testing. iii) Image deraining: we use the Rain100L [53] dataset with 200 pairs…",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "CBSD68",
        "Urban100",
        "Kodak24",
        "OTS dataset of RESIDE-β",
        "SOTS-Outdoor",
        "Rain100L"
      ],
      "dataset_descriptions": {
        "CBSD68": "Used for testing image denoising, focusing on benchmarking performance against other methods.",
        "Urban100": "Used for testing image denoising, focusing on benchmarking performance against other methods.",
        "Kodak24": "Used for testing image denoising, focusing on benchmarking performance against other methods.",
        "OTS dataset of RESIDE-β": "Used for training image dehazing, providing a large set of paired images for model development.",
        "SOTS-Outdoor": "Used for testing image dehazing, focusing on evaluating model performance on real-world outdoor scenes.",
        "Rain100L": "Used for training and testing image deraining, providing paired images to assess model effectiveness in removing rain streaks."
      },
      "confidence_score": 1.0,
      "reasoning": "The citation mentions specific datasets used for testing and training in image restoration tasks, including denoising, dehazing, and deraining.",
      "processing_time": 54.5172905921936,
      "citing_paper_id": "271974248",
      "cited_paper_id": 64193
    },
    {
      "context_text": "This targeted approach has yielded significant success across a spectrum of restoration tasks, e.g. , denoising [1]–[3], dehazing [4]–[6], deraining [7]–[9], deblurring [10]– [12], and low-light enhancement [13]–[15].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various image restoration tasks. The cited papers' titles do not provide additional dataset information.",
      "processing_time": 19.14996075630188,
      "citing_paper_id": "271974248",
      "cited_paper_id": 996788
    },
    {
      "context_text": "This targeted approach has yielded significant success across a spectrum of restoration tasks, e.g. , denoising [1]–[3], dehazing [4]–[6], deraining [7]–[9], deblurring [10]– [12], and low-light enhancement [13]–[15].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various image restoration tasks. The cited papers' titles do not provide additional dataset information.",
      "processing_time": 19.14996075630188,
      "citing_paper_id": "271974248",
      "cited_paper_id": 52008443
    },
    {
      "context_text": "This targeted approach has yielded significant success across a spectrum of restoration tasks, e.g. , denoising [1]–[3], dehazing [4]–[6], deraining [7]–[9], deblurring [10]– [12], and low-light enhancement [13]–[15].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various image restoration tasks. The cited papers' titles do not provide additional dataset information.",
      "processing_time": 19.14996075630188,
      "citing_paper_id": "271974248",
      "cited_paper_id": 248085101
    },
    {
      "context_text": "Compared to task-specific [1], [2], [4]–[15], [36]–[38] and general [16]–[21], [39], [40] image restoration, All-in-One restoration is more advantageous in terms of model storage efficiency and practical applications.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to other papers. No dataset names are present in the text.",
      "processing_time": 17.96931505203247,
      "citing_paper_id": "271974248",
      "cited_paper_id": 996788
    },
    {
      "context_text": "Compared to task-specific [1], [2], [4]–[15], [36]–[38] and general [16]–[21], [39], [40] image restoration, All-in-One restoration is more advantageous in terms of model storage efficiency and practical applications.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to other papers. No dataset names are present in the text.",
      "processing_time": 17.96931505203247,
      "citing_paper_id": "271974248",
      "cited_paper_id": 231802205
    },
    {
      "context_text": "Compared to task-specific [1], [2], [4]–[15], [36]–[38] and general [16]–[21], [39], [40] image restoration, All-in-One restoration is more advantageous in terms of model storage efficiency and practical applications.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to other papers. No dataset names are present in the text.",
      "processing_time": 17.96931505203247,
      "citing_paper_id": "271974248",
      "cited_paper_id": 248426764
    },
    {
      "context_text": "Compared to task-specific [1], [2], [4]–[15], [36]–[38] and general [16]–[21], [39], [40] image restoration, All-in-One restoration is more advantageous in terms of model storage efficiency and practical applications.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to other papers. No dataset names are present in the text.",
      "processing_time": 17.96931505203247,
      "citing_paper_id": "271974248",
      "cited_paper_id": 256594710
    },
    {
      "context_text": "Compared to task-specific [1], [2], [4]–[15], [36]–[38] and general [16]–[21], [39], [40] image restoration, All-in-One restoration is more advantageous in terms of model storage efficiency and practical applications.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to other papers. No dataset names are present in the text.",
      "processing_time": 17.96931505203247,
      "citing_paper_id": "271974248",
      "cited_paper_id": 267938238
    },
    {
      "context_text": "From level-1 to level-4, the numbers of TB and ETB are [4, 6, 6, 8], attention heads in MDTA and PGCA are both [1, 2, 4, 8], and the channel numbers are [48, 96, 192, 384].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The provided context does not mention any specific datasets, only architectural details of a neural network model. No verifiable resources are identified.",
      "processing_time": 18.76751470565796,
      "citing_paper_id": "271974248",
      "cited_paper_id": 996788
    },
    {
      "context_text": "From level-1 to level-4, the numbers of TB and ETB are [4, 6, 6, 8], attention heads in MDTA and PGCA are both [1, 2, 4, 8], and the channel numbers are [48, 96, 192, 384].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The provided context does not mention any specific datasets, only architectural details of a neural network model. No verifiable resources are identified.",
      "processing_time": 18.76751470565796,
      "citing_paper_id": "271974248",
      "cited_paper_id": 214623217
    },
    {
      "context_text": "For denoising, we employ DnCNN [1], FFDNet [2], and ADFNet [3].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions models (DnCNN, FFDNet, ADFNet) but does not reference any specific datasets. The context is about using these models for denoising, which is a task in image restoration.",
      "processing_time": 24.27614116668701,
      "citing_paper_id": "271974248",
      "cited_paper_id": 996788
    },
    {
      "context_text": "…metrics, including Peak Signal-to-Noise Ratio (PSNR), Structural Similarity Index (SSIM), and Learned Perceptual Image Patch Similarity (LPIPS) [54], as well as non-reference metrics such as the Underwater Colour Image Quality Evaluation Metric (UCIQE) [55], Underwater Image Quality Measure…",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions various metrics but does not refer to any specific datasets. The context is focused on evaluation metrics rather than datasets.",
      "processing_time": 18.743605136871338,
      "citing_paper_id": "271974248",
      "cited_paper_id": 4766599
    },
    {
      "context_text": "Additionally, we report the performance of All-in-One methods on perceptual metrics, such as LPIPS [54] and MUSIQ [57].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions perceptual metrics LPIPS and MUSIQ but does not refer to them as datasets. They are likely metrics or models, not datasets.",
      "processing_time": 21.66078519821167,
      "citing_paper_id": "271974248",
      "cited_paper_id": 4766599
    },
    {
      "context_text": "A of the LPIPS [54] and MUSIQ [57] perceptual metrics for the All-in-One methods under “ N+H+R+B+L ” setting.",
      "catation_intent": "findings",
      "resource_type": "metric",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions perceptual metrics LPIPS and MUSIQ but does not refer to them as datasets. They are likely metrics or models, not datasets.",
      "processing_time": 22.25175905227661,
      "citing_paper_id": "271974248",
      "cited_paper_id": 4766599
    },
    {
      "context_text": "…the Rain100L [53] dataset with 200 pairs of images for training and 100 pairs for testing. iv) Image deblurring: we train the model on the GoPro [11] dataset, which contains 2,103 pairs for training and 1,111 pairs for testing. v) Low-light enhancement: we use the LOL [13] dataset, which…",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Rain100L",
        "GoPro",
        "LOL"
      ],
      "dataset_descriptions": {
        "Rain100L": "Used for training and testing rain removal models, with 200 image pairs for training and 100 for testing, focusing on image restoration under rainy conditions.",
        "GoPro": "Used for training image deblurring models, containing 2,103 image pairs for training and 1,111 for testing, focusing on dynamic scene deblurring.",
        "LOL": "Used for low-light image enhancement, though the exact number of image pairs is not specified, focusing on improving image quality in low-light conditions."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions three specific datasets used for training and testing in different image restoration tasks: Rain100L, GoPro, and LOL. These datasets are clearly identified and their usage is described.",
      "processing_time": 45.88535165786743,
      "citing_paper_id": "271974248",
      "cited_paper_id": 8671030
    },
    {
      "context_text": "The L ce can be described as where S ( · , · ) denotes cosine similarity and Y ij is the label of the current image I i ; Φ I ( · ) and Φ T ( · ) denote CLIP image encoder and CLIP text encoder, respectively.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only model components and loss functions. The cited papers do not provide additional context to identify datasets.",
      "processing_time": 21.370386600494385,
      "citing_paper_id": "271974248",
      "cited_paper_id": 14124313
    },
    {
      "context_text": "The L ce can be described as where S ( · , · ) denotes cosine similarity and Y ij is the label of the current image I i ; Φ I ( · ) and Φ T ( · ) denote CLIP image encoder and CLIP text encoder, respectively.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only model components and loss functions. The cited papers do not provide additional context to identify datasets.",
      "processing_time": 21.370386600494385,
      "citing_paper_id": "271974248",
      "cited_paper_id": 266933199
    },
    {
      "context_text": "…RESIDE-β [52] with 72,135 pairs for training and 500 images from SOTS-Outdoor [52] dataset for testing. iii) Image deraining: we use the Rain100L [53] dataset with 200 pairs of images for training and 100 pairs for testing. iv) Image deblurring: we train the model on the GoPro [11] dataset,…",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "RESIDE-β",
        "SOTS-Outdoor",
        "Rain100L",
        "GoPro"
      ],
      "dataset_descriptions": {
        "RESIDE-β": "Used for training in image dehazing, providing 72,135 image pairs to improve model performance in removing haze from images.",
        "SOTS-Outdoor": "Used for testing in image dehazing, containing 500 outdoor images to evaluate the model's ability to remove haze.",
        "Rain100L": "Used for training in image deraining, consisting of 200 image pairs to enhance the model's capability to remove rain streaks.",
        "GoPro": "Used for training in image deblurring, providing a dataset to improve the model's performance in reducing motion blur."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions specific datasets used for training and testing in various image restoration tasks, which align with the research topic of all-in-one image restoration.",
      "processing_time": 49.2416889667511,
      "citing_paper_id": "271974248",
      "cited_paper_id": 15443600
    },
    {
      "context_text": "Since the RTTS dataset lacks reference images, we additionally employed several non-reference metrics for quantitative comparison, such as FADE [58], BRISQUE [59], and NIMA [60].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "RTTS"
      ],
      "dataset_descriptions": {
        "RTTS": "Used for evaluating image restoration methods, specifically addressing the challenge of lacking reference images by employing non-reference metrics for quantitative comparison."
      },
      "confidence_score": 0.9,
      "reasoning": "The context mentions the RTTS dataset, which is used for image restoration evaluation. However, it lacks reference images, so non-reference metrics are employed for quantitative comparison.",
      "processing_time": 27.53364324569702,
      "citing_paper_id": "271974248",
      "cited_paper_id": 16388844
    },
    {
      "context_text": "…Image Quality Evaluation Metric (UCIQE) [55], Underwater Image Quality Measure (UIQM) [56], Multi-scale Image Quality Trans-former (MUSIQ) [57], Fog Aware Density Evaluator (FADE) [58], Blind/Referenceless Image Spatial Quality Evaluator (BRISQUE) [59], and Neural Image Assessment (NIMA) [60].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span mentions several metrics and methods, but does not refer to any specific datasets. Metrics and methods are excluded according to the instructions.",
      "processing_time": 21.63099980354309,
      "citing_paper_id": "271974248",
      "cited_paper_id": 16388844
    },
    {
      "context_text": "…Image Patch Similarity (LPIPS) [54], as well as non-reference metrics such as the Underwater Colour Image Quality Evaluation Metric (UCIQE) [55], Underwater Image Quality Measure (UIQM) [56], Multi-scale Image Quality Trans-former (MUSIQ) [57], Fog Aware Density Evaluator (FADE) [58],…",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions several metrics and methods, but does not refer to any specific datasets. The context is focused on evaluation metrics for image quality, particularly in underwater and foggy conditions.",
      "processing_time": 23.434395790100098,
      "citing_paper_id": "271974248",
      "cited_paper_id": 16568282
    },
    {
      "context_text": "For dehazing, we utilize DehazeNet [4], AODNet [65], FDGAN [5], and DehazeFormer [6].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions several methods/models but does not refer to any specific datasets. The context is focused on dehazing techniques and models used for that purpose.",
      "processing_time": 22.679919242858887,
      "citing_paper_id": "271974248",
      "cited_paper_id": 30151664
    },
    {
      "context_text": "…for training and 100 pairs for testing. iv) Image deblurring: we train the model on the GoPro [11] dataset, which contains 2,103 pairs for training and 1,111 pairs for testing. v) Low-light enhancement: we use the LOL [13] dataset, which contains 485 pairs for training and 15 pairs for testing.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "GoPro",
        "LOL"
      ],
      "dataset_descriptions": {
        "GoPro": "Used for training and testing an image deblurring model, containing 2,103 pairs for training and 1,111 pairs for testing.",
        "LOL": "Used for training and testing a low-light enhancement model, containing 485 pairs for training and 15 pairs for testing."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions two specific datasets, GoPro and LOL, used for training and testing models for image deblurring and low-light enhancement, respectively.",
      "processing_time": 36.75386142730713,
      "citing_paper_id": "271974248",
      "cited_paper_id": 52008443
    },
    {
      "context_text": "Lastly, for low-light image enhancement, we incorporate Retinex-Net [13], URetinex [14], and Retinexformer [15].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions models/methods (Retinex-Net, URetinex, Retinexformer) rather than datasets. No specific datasets are named or described.",
      "processing_time": 23.61306071281433,
      "citing_paper_id": "271974248",
      "cited_paper_id": 52008443
    },
    {
      "context_text": "…methods: LPNet [61], ADFNet [3], DehazeFormer [6], and DRSformer [9]; five general methods: MPRNet [16], Restormer [19], NAFNet [18], FSNet [20], and MambaIR [40], and seven All-in-One methods: DL [62], AirNet [23], IDR [25], NDR [28], PromptIR [27], Gridformer [32] and InstructIR [33].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various methods and models. No verifiable resources are identified.",
      "processing_time": 20.79335618019104,
      "citing_paper_id": "271974248",
      "cited_paper_id": 195787503
    },
    {
      "context_text": "…methods: LPNet [61], ADFNet [3], DehazeFormer [6], and DRSformer [9]; five general methods: MPRNet [16], Restormer [19], NAFNet [18], FSNet [20], and MambaIR [40], and seven All-in-One methods: DL [62], AirNet [23], IDR [25], NDR [28], PromptIR [27], Gridformer [32] and InstructIR [33].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various methods and models. No verifiable resources are identified.",
      "processing_time": 20.79335618019104,
      "citing_paper_id": "271974248",
      "cited_paper_id": 258959359
    },
    {
      "context_text": "…methods: LPNet [61], ADFNet [3], DehazeFormer [6], and DRSformer [9]; five general methods: MPRNet [16], Restormer [19], NAFNet [18], FSNet [20], and MambaIR [40], and seven All-in-One methods: DL [62], AirNet [23], IDR [25], NDR [28], PromptIR [27], Gridformer [32] and InstructIR [33].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various methods and models. No verifiable resources are identified.",
      "processing_time": 20.79335618019104,
      "citing_paper_id": "271974248",
      "cited_paper_id": 264305756
    },
    {
      "context_text": "…methods: LPNet [61], ADFNet [3], DehazeFormer [6], and DRSformer [9]; five general methods: MPRNet [16], Restormer [19], NAFNet [18], FSNet [20], and MambaIR [40], and seven All-in-One methods: DL [62], AirNet [23], IDR [25], NDR [28], PromptIR [27], Gridformer [32] and InstructIR [33].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various methods and models. No verifiable resources are identified.",
      "processing_time": 20.79335618019104,
      "citing_paper_id": "271974248",
      "cited_paper_id": 267320695
    },
    {
      "context_text": "…methods: LPNet [61], ADFNet [3], DehazeFormer [6], and DRSformer [9]; five general methods: MPRNet [16], Restormer [19], NAFNet [18], FSNet [20], and MambaIR [40], and seven All-in-One methods: DL [62], AirNet [23], IDR [25], NDR [28], PromptIR [27], Gridformer [32] and InstructIR [33].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various methods and models. No verifiable resources are identified.",
      "processing_time": 20.79335618019104,
      "citing_paper_id": "271974248",
      "cited_paper_id": 267938238
    },
    {
      "context_text": "3) Baselines: In the All-in-One (“ N+H+R”) setting, we select four task-specific methods: LPNet [61], ADFNet [3], DehazeFormer [6], and DRSformer [9]; five general methods: MPRNet [16], Restormer [19], NAFNet [18], FSNet [20], and MambaIR [40], and seven All-in-One methods: DL [62], AirNet [23],…",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only models and methods. There are no verifiable resources that meet the criteria for inclusion.",
      "processing_time": 22.2277889251709,
      "citing_paper_id": "271974248",
      "cited_paper_id": 198185751
    },
    {
      "context_text": "For deraining, we apply UMR [7], MSPFN [8], LPNet [61], and DRSformer [9].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context mentions methods/models (UMR, MSPFN, LPNet, DRSformer) but does not reference any specific datasets. No dataset names are present in the context or titles.",
      "processing_time": 24.466150283813477,
      "citing_paper_id": "271974248",
      "cited_paper_id": 198185751
    },
    {
      "context_text": "For deraining, we apply UMR [7], MSPFN [8], LPNet [61], and DRSformer [9].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context mentions methods/models (UMR, MSPFN, LPNet, DRSformer) but does not reference any specific datasets. No dataset names are present in the context or titles.",
      "processing_time": 24.466150283813477,
      "citing_paper_id": "271974248",
      "cited_paper_id": 214623217
    },
    {
      "context_text": "In recent years, prompt learning as an emerging learning paradigm has seen significant advancements in the field of natural language processing [41].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general advancement in prompt learning within natural language processing.",
      "processing_time": 20.42345142364502,
      "citing_paper_id": "271974248",
      "cited_paper_id": 218971783
    },
    {
      "context_text": "…(“ N+H+R”) setting, we select four task-specific methods: LPNet [61], ADFNet [3], DehazeFormer [6], and DRSformer [9]; five general methods: MPRNet [16], Restormer [19], NAFNet [18], FSNet [20], and MambaIR [40], and seven All-in-One methods: DL [62], AirNet [23], IDR [25], NDR [28], PromptIR…",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only models and methods. There are no clear identifiers for datasets within the text.",
      "processing_time": 22.231541395187378,
      "citing_paper_id": "271974248",
      "cited_paper_id": 231802205
    },
    {
      "context_text": "XIV, advanced All-in-One models like PromptIR [27] underperform AirNet [23], while general models like Restormer [19] surpass MPRNet [16].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only models and their performance comparisons. No verifiable resources are identified.",
      "processing_time": 21.613402366638184,
      "citing_paper_id": "271974248",
      "cited_paper_id": 231802205
    },
    {
      "context_text": "XIV, advanced All-in-One models like PromptIR [27] underperform AirNet [23], while general models like Restormer [19] surpass MPRNet [16].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only models and their performance comparisons. No verifiable resources are identified.",
      "processing_time": 21.613402366638184,
      "citing_paper_id": "271974248",
      "cited_paper_id": 244346144
    },
    {
      "context_text": "To overcome these limitations, general image restoration methods [16]–[21] have been introduced.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general image restoration methods. No verifiable resources are identified.",
      "processing_time": 21.32819128036499,
      "citing_paper_id": "271974248",
      "cited_paper_id": 231802205
    },
    {
      "context_text": "3) Semantic Guidance Module: Recently, large-scale vision models ( e.g. , DINO family [35], [44] ) have demonstrated their potential in a series of visual downstream tasks in a self-supervised manner.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only models and methods. The citation is focused on the capabilities of large-scale vision models in self-supervised learning.",
      "processing_time": 23.408568382263184,
      "citing_paper_id": "271974248",
      "cited_paper_id": 233444273
    },
    {
      "context_text": "3) Semantic Guidance Module: Recently, large-scale vision models ( e.g. , DINO family [35], [44] ) have demonstrated their potential in a series of visual downstream tasks in a self-supervised manner.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only models and methods. The citation is focused on the capabilities of large-scale vision models in self-supervised learning.",
      "processing_time": 23.408568382263184,
      "citing_paper_id": "271974248",
      "cited_paper_id": 258170077
    },
    {
      "context_text": "Similarly, self-supervised ViT models like DINO [35] and DINO-v2 [44] have proven effective in multiple domains, eliminating the need for labeled input data.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions self-supervised ViT models like DINO and DINO-v2 but does not refer to any specific datasets. The focus is on the effectiveness of these models in multiple domains without labeled data.",
      "processing_time": 24.913984775543213,
      "citing_paper_id": "271974248",
      "cited_paper_id": 233444273
    },
    {
      "context_text": "Similarly, self-supervised ViT models like DINO [35] and DINO-v2 [44] have proven effective in multiple domains, eliminating the need for labeled input data.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions self-supervised ViT models like DINO and DINO-v2 but does not refer to any specific datasets. The focus is on the effectiveness of these models in multiple domains without labeled data.",
      "processing_time": 24.913984775543213,
      "citing_paper_id": "271974248",
      "cited_paper_id": 258170077
    },
    {
      "context_text": "More recent studies [24], [29] have leveraged the feature representation capabilities of large-scale visual models like CLIP [34] and DINO [35] to enhance texture reconstruction and maintain structural consistency.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only models and methods. The cited papers do not provide additional context to identify datasets.",
      "processing_time": 22.203113794326782,
      "citing_paper_id": "271974248",
      "cited_paper_id": 233444273
    },
    {
      "context_text": "More recent studies [24], [29] have leveraged the feature representation capabilities of large-scale visual models like CLIP [34] and DINO [35] to enhance texture reconstruction and maintain structural consistency.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only models and methods. The cited papers do not provide additional context to identify datasets.",
      "processing_time": 22.203113794326782,
      "citing_paper_id": "271974248",
      "cited_paper_id": 259164829
    },
    {
      "context_text": "More recent studies [24], [29] have leveraged the feature representation capabilities of large-scale visual models like CLIP [34] and DINO [35] to enhance texture reconstruction and maintain structural consistency.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only models and methods. The cited papers do not provide additional context to identify datasets.",
      "processing_time": 22.203113794326782,
      "citing_paper_id": "271974248",
      "cited_paper_id": 265609570
    },
    {
      "context_text": "We train Restormer [19] for 100K iterations with a learning rate of 2 × 10 − 4 .",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only the training of a model called Restormer. There are no verifiable resources or datasets mentioned.",
      "processing_time": 22.98919153213501,
      "citing_paper_id": "271974248",
      "cited_paper_id": 244346144
    },
    {
      "context_text": "In the restoration branch, we choose Restormer [19] as backbone.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions Restormer as a backbone for the restoration branch, but it does not refer to a dataset. Restormer is a method, not a dataset.",
      "processing_time": 23.857375860214233,
      "citing_paper_id": "271974248",
      "cited_paper_id": 244346144
    },
    {
      "context_text": "To validate the adaptability of our method for different restoration backbones, we conducted extensive experiments using NAFNet [18], Restormer [19], X-Restormer [74], and GRL [72] as the backbones in the restoration stage.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions several methods/models (NAFNet, Restormer, X-Restormer, GRL) but does not refer to any specific datasets. The focus is on validating the adaptability of the method with different restoration backbones.",
      "processing_time": 25.85547709465027,
      "citing_paper_id": "271974248",
      "cited_paper_id": 244346144
    },
    {
      "context_text": "To validate the adaptability of our method for different restoration backbones, we conducted extensive experiments using NAFNet [18], Restormer [19], X-Restormer [74], and GRL [72] as the backbones in the restoration stage.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions several methods/models (NAFNet, Restormer, X-Restormer, GRL) but does not refer to any specific datasets. The focus is on validating the adaptability of the method with different restoration backbones.",
      "processing_time": 25.85547709465027,
      "citing_paper_id": "271974248",
      "cited_paper_id": 248085491
    },
    {
      "context_text": "To validate the adaptability of our method for different restoration backbones, we conducted extensive experiments using NAFNet [18], Restormer [19], X-Restormer [74], and GRL [72] as the backbones in the restoration stage.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions several methods/models (NAFNet, Restormer, X-Restormer, GRL) but does not refer to any specific datasets. The focus is on validating the adaptability of the method with different restoration backbones.",
      "processing_time": 25.85547709465027,
      "citing_paper_id": "271974248",
      "cited_paper_id": 264289165
    },
    {
      "context_text": "…we select four task-specific methods: LPNet [61], ADFNet [3], DehazeFormer [6], and DRSformer [9]; five general methods: MPRNet [16], Restormer [19], NAFNet [18], FSNet [20], and MambaIR [40], and seven All-in-One methods: DL [62], AirNet [23], IDR [25], NDR [28], PromptIR [27], Gridformer…",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only models and methods. There are no clear identifiers for datasets within the text.",
      "processing_time": 22.42924737930298,
      "citing_paper_id": "271974248",
      "cited_paper_id": 244346144
    },
    {
      "context_text": "Specifically, in the encoder layer, we integrate Multi-Dconv Head Transposed Attention (MDTA) [19] and Gated-Dconv Feed-Forward Network (GDFN) [19] to jointly construct the Transformer Block (TB).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and components of the model architecture.",
      "processing_time": 20.006269216537476,
      "citing_paper_id": "271974248",
      "cited_paper_id": 244346144
    },
    {
      "context_text": "Particularly, in terms of overall performance, our Perceive-IR exhibits superior results compared with PromptIR [27] by over 0.57 dB/0.004 on average in PSNR/SSIM while utilizing the same backbone of Restormer [19].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only comparisons between methods. No dataset names are provided in the context.",
      "processing_time": 28.866592407226562,
      "citing_paper_id": "271974248",
      "cited_paper_id": 244346144
    },
    {
      "context_text": "Then, we train a restoration model, i.e. , Restormer [19] to obtain the restored images I m via cross-validation strategy.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a model (Restormer) which is not included as per instructions.",
      "processing_time": 29.279698610305786,
      "citing_paper_id": "271974248",
      "cited_paper_id": 244346144
    },
    {
      "context_text": "Its effectiveness has led to its widespread application in vision-related tasks [42].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general application in vision-related tasks.",
      "processing_time": 28.359410762786865,
      "citing_paper_id": "271974248",
      "cited_paper_id": 245005809
    },
    {
      "context_text": "Recently, All-in-One image restoration methods [22]–[33] have emerged as a potential solution to these challenges.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only a general trend in image restoration methods. No verifiable resources are named.",
      "processing_time": 29.58366584777832,
      "citing_paper_id": "271974248",
      "cited_paper_id": 247411392
    },
    {
      "context_text": "Recently, All-in-One image restoration methods [22]–[33] have emerged as a potential solution to these challenges.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only a general trend in image restoration methods. No verifiable resources are named.",
      "processing_time": 29.58366584777832,
      "citing_paper_id": "271974248",
      "cited_paper_id": 267320695
    },
    {
      "context_text": "All-in-One restoration [22]–[33], which aims to recover clean images from multiple degradations through a unified model, has grown to be a promising field of low-level vision tasks.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only the concept of All-in-One image restoration. No verifiable resources are identified.",
      "processing_time": 29.73924469947815,
      "citing_paper_id": "271974248",
      "cited_paper_id": 247411392
    },
    {
      "context_text": "All-in-One restoration [22]–[33], which aims to recover clean images from multiple degradations through a unified model, has grown to be a promising field of low-level vision tasks.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only the concept of All-in-One image restoration. No verifiable resources are identified.",
      "processing_time": 29.73924469947815,
      "citing_paper_id": "271974248",
      "cited_paper_id": 248562703
    },
    {
      "context_text": "All-in-One restoration [22]–[33], which aims to recover clean images from multiple degradations through a unified model, has grown to be a promising field of low-level vision tasks.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only the concept of All-in-One image restoration. No verifiable resources are identified.",
      "processing_time": 29.73924469947815,
      "citing_paper_id": "271974248",
      "cited_paper_id": 267320695
    },
    {
      "context_text": "In the All-in-One (“N+H+R+B+L”) setting, we add task-specific methods like HI-Diff [63] and Retinexformer [15]; general methods such as DGUNet [39], and MIRNet-v2 [17]; as well as All-in-One methods like Transweather [64], and TAPE [22].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions several methods and models but does not explicitly refer to any datasets. The cited papers' titles do not indicate the presence of datasets.",
      "processing_time": 30.30091404914856,
      "citing_paper_id": "271974248",
      "cited_paper_id": 247411392
    },
    {
      "context_text": "In the All-in-One (“N+H+R+B+L”) setting, we add task-specific methods like HI-Diff [63] and Retinexformer [15]; general methods such as DGUNet [39], and MIRNet-v2 [17]; as well as All-in-One methods like Transweather [64], and TAPE [22].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions several methods and models but does not explicitly refer to any datasets. The cited papers' titles do not indicate the presence of datasets.",
      "processing_time": 30.30091404914856,
      "citing_paper_id": "271974248",
      "cited_paper_id": 248426764
    },
    {
      "context_text": "In the All-in-One (“N+H+R+B+L”) setting, we add task-specific methods like HI-Diff [63] and Retinexformer [15]; general methods such as DGUNet [39], and MIRNet-v2 [17]; as well as All-in-One methods like Transweather [64], and TAPE [22].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions several methods and models but does not explicitly refer to any datasets. The cited papers' titles do not indicate the presence of datasets.",
      "processing_time": 30.30091404914856,
      "citing_paper_id": "271974248",
      "cited_paper_id": 258832675
    },
    {
      "context_text": "For image deblurring, we leverage DeblurGAN [10], Stripformer [12], and HI-Diff [63].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions models and methods (DeblurGAN, Stripformer, HI-Diff) but does not reference any specific datasets. The cited papers' titles confirm these are methods, not datasets.",
      "processing_time": 32.93927788734436,
      "citing_paper_id": "271974248",
      "cited_paper_id": 248085101
    },
    {
      "context_text": "For image deblurring, we leverage DeblurGAN [10], Stripformer [12], and HI-Diff [63].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions models and methods (DeblurGAN, Stripformer, HI-Diff) but does not reference any specific datasets. The cited papers' titles confirm these are methods, not datasets.",
      "processing_time": 32.93927788734436,
      "citing_paper_id": "271974248",
      "cited_paper_id": 258832675
    },
    {
      "context_text": "…four task-specific methods: LPNet [61], ADFNet [3], DehazeFormer [6], and DRSformer [9]; five general methods: MPRNet [16], Restormer [19], NAFNet [18], FSNet [20], and MambaIR [40], and seven All-in-One methods: DL [62], AirNet [23], IDR [25], NDR [28], PromptIR [27], Gridformer [32] and…",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various methods and models. There are no clear identifiers for datasets within the given context.",
      "processing_time": 30.294705152511597,
      "citing_paper_id": "271974248",
      "cited_paper_id": 248085491
    },
    {
      "context_text": "Specifically, we select noise, haze, and rain as benchmark degradation types and evaluate them on their corresponding real-world datasets: SIDD val [68] (for noise), RTTS [52] (for haze), and RealRain-1k-L test datasets [69] (for rain).",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "SIDD val",
        "RTTS",
        "RealRain-1k-L"
      ],
      "dataset_descriptions": {
        "SIDD val": "Used to evaluate noise degradation in images, focusing on real-world noise patterns and their impact on image quality.",
        "RTTS": "Used to evaluate haze degradation in images, focusing on real-world atmospheric conditions and their effects on visibility.",
        "RealRain-1k-L": "Used to evaluate rain degradation in images, focusing on real-world rain patterns and their impact on visual clarity."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions three specific datasets used for evaluating different types of image degradations in the context of all-in-one image restoration.",
      "processing_time": 49.354196071624756,
      "citing_paper_id": "271974248",
      "cited_paper_id": 249625528
    },
    {
      "context_text": "2) Difficulty-adaptive Perceptual Loss: Using contrastive regularization has been demonstrated as an effective way of improving image restoration performance [45], [46].",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (contrastive regularization) used for image restoration.",
      "processing_time": 28.971760749816895,
      "citing_paper_id": "271974248",
      "cited_paper_id": 257766864
    },
    {
      "context_text": "Recently, CLIP-LIT [43] has shown that initializing textual prompts in CLIP can aid in extracting more accurate low-level image representations.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (CLIP-LIT) for enhancing image representations.",
      "processing_time": 29.409600973129272,
      "citing_paper_id": "271974248",
      "cited_paper_id": 257833938
    },
    {
      "context_text": "For the restoration stage, we chose Restormer as the restoration backbone of Perceive-IR and DINO-v2 [44] base version as the semantic guidance module.",
      "catation_intent": "reusable resource",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions Restormer and DINO-v2 but does not refer to them as datasets. They are described as a restoration backbone and a semantic guidance module, respectively.",
      "processing_time": 32.44751453399658,
      "citing_paper_id": "271974248",
      "cited_paper_id": 258170077
    },
    {
      "context_text": "…demonstrates highly competitive performance in dehazing and deraining tasks, it does not perform as well as the best task-specific methods ( i.e. , HI-Diff [63] and Retinexformer [15]) and the best general methods ( i.e. , FSNet [20] and MIRNet [67]) on deblurring and low-light enhancement tasks.",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only models and methods. The context focuses on comparing performance across different image restoration tasks.",
      "processing_time": 30.279544830322266,
      "citing_paper_id": "271974248",
      "cited_paper_id": 258832675
    },
    {
      "context_text": "…demonstrates highly competitive performance in dehazing and deraining tasks, it does not perform as well as the best task-specific methods ( i.e. , HI-Diff [63] and Retinexformer [15]) and the best general methods ( i.e. , FSNet [20] and MIRNet [67]) on deblurring and low-light enhancement tasks.",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only models and methods. The context focuses on comparing performance across different image restoration tasks.",
      "processing_time": 30.279544830322266,
      "citing_paper_id": "271974248",
      "cited_paper_id": 263784964
    },
    {
      "context_text": "Compared to the latest methods, Gridformer [32] and InstructIR [33], Perceive-IR also demonstrates a PSNR/SSIM improvement of 0.44 dB/0.005 and 0.20 dB/0.004, respectively.",
      "catation_intent": "findings",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only comparisons to methods. No verifiable resources are identified.",
      "processing_time": 28.968804359436035,
      "citing_paper_id": "271974248",
      "cited_paper_id": 258959359
    },
    {
      "context_text": "Compared to the latest methods, Gridformer [32] and InstructIR [33], Perceive-IR also demonstrates a PSNR/SSIM improvement of 0.44 dB/0.005 and 0.20 dB/0.004, respectively.",
      "catation_intent": "findings",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only comparisons to methods. No verifiable resources are identified.",
      "processing_time": 28.968804359436035,
      "citing_paper_id": "271974248",
      "cited_paper_id": 267320695
    },
    {
      "context_text": "Given that these LVMs can extract valuable prior knowledge from external hyper-scale datasets, leveraging pre-trained LVMs has become increasingly popular in the low-level vision community [24], [29], [31].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.2,
      "reasoning": "The context mentions 'external hyper-scale datasets' but does not specify any particular dataset names. The term 'datasets' is too generic and lacks specific identifiers.",
      "processing_time": 31.64522099494934,
      "citing_paper_id": "271974248",
      "cited_paper_id": 259164829
    },
    {
      "context_text": "Given that these LVMs can extract valuable prior knowledge from external hyper-scale datasets, leveraging pre-trained LVMs has become increasingly popular in the low-level vision community [24], [29], [31].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.2,
      "reasoning": "The context mentions 'external hyper-scale datasets' but does not specify any particular dataset names. The term 'datasets' is too generic and lacks specific identifiers.",
      "processing_time": 31.64522099494934,
      "citing_paper_id": "271974248",
      "cited_paper_id": 265609570
    },
    {
      "context_text": "More recently, CLIP-AWR [24], DA-CLIP [31], and DINO-IR [29] leveraged pre-trained large-scale vision models to excel in All-in-One restoration tasks.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions pre-trained models (CLIP-AWR, DA-CLIP, DINO-IR) but does not specify any datasets. The cited paper titles do not provide additional dataset information.",
      "processing_time": 34.353073835372925,
      "citing_paper_id": "271974248",
      "cited_paper_id": 259164829
    },
    {
      "context_text": "More recently, CLIP-AWR [24], DA-CLIP [31], and DINO-IR [29] leveraged pre-trained large-scale vision models to excel in All-in-One restoration tasks.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions pre-trained models (CLIP-AWR, DA-CLIP, DINO-IR) but does not specify any datasets. The cited paper titles do not provide additional dataset information.",
      "processing_time": 34.353073835372925,
      "citing_paper_id": "271974248",
      "cited_paper_id": 265609570
    },
    {
      "context_text": "(c) Our method simultaneously perceives degradation types and severity levels, while being compatible with diverse restoration backbones, offering superior flexibility and versatility.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method and its capabilities. No verifiable resources are identified.",
      "processing_time": 29.710721731185913,
      "citing_paper_id": "271974248",
      "cited_paper_id": 260107992
    },
    {
      "context_text": "In natural language processing, these prompts are usually given in the form of text, whereas in computer vision, they may involve images, text, or other forms of data.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific, verifiable datasets. It only describes general differences in prompts between NLP and computer vision.",
      "processing_time": 30.479492902755737,
      "citing_paper_id": "271974248",
      "cited_paper_id": 266933199
    },
    {
      "context_text": "In noisy scenarios, Perceive-IR produces clear and sharp denoised outputs, preserving fine textures compared to AirNet [23], PromptIR [27], and InstructIR [33] ( e.g. , the head and stomach texture parts in rows 1 and 2 of Fig.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models or methods. The context focuses on comparing the performance of different image restoration techniques.",
      "processing_time": 30.928529739379883,
      "citing_paper_id": "271974248",
      "cited_paper_id": 267320695
    },
    {
      "context_text": "…NAFNet-128 achieves 32.52/29.71 dB PSNR under “N+H+R” and “N+H+R+B+L” settings, surpassing PromptIR [27] by 0.46/0.56 dB and In-structIR [33] with a patch size of 256, the performance of all methods improves further, with the relative performance advantages across different backbones…",
      "catation_intent": "findings",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only performance comparisons between different methods. No verifiable resources are identified.",
      "processing_time": 29.91233253479004,
      "citing_paper_id": "271974248",
      "cited_paper_id": 267320695
    },
    {
      "context_text": "…serves as the anchor, the reference image I g is used as positive sample, the degraded I d is used as easy negative sample, and the augmented images I q from proxy restoration models ( e.g. , InstructIR [33], PromptIR [27], FSNet [20], and MambaIR [40]) as hard or very-hard negative samples.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions proxy restoration models but does not specify any datasets. The cited papers are methods/models, not datasets.",
      "processing_time": 29.92217779159546,
      "citing_paper_id": "271974248",
      "cited_paper_id": 267320695
    },
    {
      "context_text": "…serves as the anchor, the reference image I g is used as positive sample, the degraded I d is used as easy negative sample, and the augmented images I q from proxy restoration models ( e.g. , InstructIR [33], PromptIR [27], FSNet [20], and MambaIR [40]) as hard or very-hard negative samples.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions proxy restoration models but does not specify any datasets. The cited papers are methods/models, not datasets.",
      "processing_time": 29.92217779159546,
      "citing_paper_id": "271974248",
      "cited_paper_id": 267938238
    },
    {
      "context_text": "Models like WeatherDiff [69] and TK-JOURNAL [176] Rain100L [32] BSD68 [26] Gopro [33] LOL [35] S i ng l e MANet [100] also perform well, particularly in handling snow and foggy conditions.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Rain100L",
        "BSD68",
        "Gopro"
      ],
      "dataset_descriptions": {
        "Rain100L": "Used to evaluate rain removal performance, focusing on synthetic rain images with varying intensities and complexities.",
        "BSD68": "Used to assess image denoising capabilities, specifically targeting grayscale images with Gaussian noise.",
        "Gopro": "Used to evaluate deblurring performance, focusing on real-world motion blur in high-resolution video frames."
      },
      "confidence_score": 0.9,
      "reasoning": "The context mentions several datasets and models, but only Rain100L, BSD68, and Gopro are identified as datasets. The others are either models or not clearly specified.",
      "processing_time": 50.489468812942505,
      "citing_paper_id": "273502246",
      "cited_paper_id": 64193
    },
    {
      "context_text": "Models like WeatherDiff [69] and TK-JOURNAL [176] Rain100L [32] BSD68 [26] Gopro [33] LOL [35] S i ng l e MANet [100] also perform well, particularly in handling snow and foggy conditions.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Rain100L",
        "BSD68",
        "Gopro"
      ],
      "dataset_descriptions": {
        "Rain100L": "Used to evaluate rain removal performance, focusing on synthetic rain images with varying intensities and complexities.",
        "BSD68": "Used to assess image denoising capabilities, specifically targeting grayscale images with Gaussian noise.",
        "Gopro": "Used to evaluate deblurring performance, focusing on real-world motion blur in high-resolution video frames."
      },
      "confidence_score": 0.9,
      "reasoning": "The context mentions several datasets and models, but only Rain100L, BSD68, and Gopro are identified as datasets. The others are either models or not clearly specified.",
      "processing_time": 50.489468812942505,
      "citing_paper_id": "273502246",
      "cited_paper_id": 15443600
    },
    {
      "context_text": "With the rapid development of AiOIR, numerous researchers collected a series of datasets tailored for various IR tasks, e.g. , BSD100 [26], Manga109 [27] and Urban100 [28] for SR, RainDrop [29], Outdoor-Rain [30], SPA [31] and Rain-100H [32] for draining, GoPro [33] and HIDE [34] for deblurring,…",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "BSD100",
        "Manga109",
        "Urban100",
        "RainDrop",
        "Outdoor-Rain",
        "SPA",
        "Rain-100H",
        "GoPro",
        "HIDE"
      ],
      "dataset_descriptions": {
        "BSD100": "Used for super-resolution tasks, providing high-quality images for training and evaluation.",
        "Manga109": "Used for super-resolution tasks, containing manga images to test the performance on line art and text.",
        "Urban100": "Used for super-resolution tasks, featuring urban scenes to assess the quality of upscaled images.",
        "RainDrop": "Used for rain removal tasks, providing synthetic rain images to train and evaluate models.",
        "Outdoor-Rain": "Used for rain removal tasks, containing real outdoor rain images to test the robustness of models.",
        "SPA": "Used for rain removal tasks, offering synthetic and real rain images to evaluate the effectiveness of rain removal algorithms.",
        "Rain-100H": "Used for rain removal tasks, providing a challenging set of heavy rain images for model evaluation.",
        "GoPro": "Used for deblurring tasks, featuring real motion-blurred images to test the performance of deblurring algorithms.",
        "HIDE": "Used for deblurring tasks, containing a diverse set of blurred images to evaluate the generalization of deblurring models."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions several datasets specifically designed for image restoration tasks, which align with the research topic of All-in-One Image Restoration.",
      "processing_time": 80.64405012130737,
      "citing_paper_id": "273502246",
      "cited_paper_id": 64193
    },
    {
      "context_text": "…Science and Technology, Harbin Institute of Technology, Harbin 150001, China. focuses on correcting a specific type of image degradation( e.g. , denoising [4]–[9], dehazing [10]–[14], desnowing [15]–[17], deraining [18]–[20], deblurring [21], [22] and low-light image enhancement [23]–[25]).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various types of image restoration tasks. The cited papers do not provide additional context about datasets.",
      "processing_time": 31.617583990097046,
      "citing_paper_id": "273502246",
      "cited_paper_id": 1887989
    },
    {
      "context_text": "…Science and Technology, Harbin Institute of Technology, Harbin 150001, China. focuses on correcting a specific type of image degradation( e.g. , denoising [4]–[9], dehazing [10]–[14], desnowing [15]–[17], deraining [18]–[20], deblurring [21], [22] and low-light image enhancement [23]–[25]).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various types of image restoration tasks. The cited papers do not provide additional context about datasets.",
      "processing_time": 31.617583990097046,
      "citing_paper_id": "273502246",
      "cited_paper_id": 5778488
    },
    {
      "context_text": "…Science and Technology, Harbin Institute of Technology, Harbin 150001, China. focuses on correcting a specific type of image degradation( e.g. , denoising [4]–[9], dehazing [10]–[14], desnowing [15]–[17], deraining [18]–[20], deblurring [21], [22] and low-light image enhancement [23]–[25]).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various types of image restoration tasks. The cited papers do not provide additional context about datasets.",
      "processing_time": 31.617583990097046,
      "citing_paper_id": "273502246",
      "cited_paper_id": 10878983
    },
    {
      "context_text": "…Science and Technology, Harbin Institute of Technology, Harbin 150001, China. focuses on correcting a specific type of image degradation( e.g. , denoising [4]–[9], dehazing [10]–[14], desnowing [15]–[17], deraining [18]–[20], deblurring [21], [22] and low-light image enhancement [23]–[25]).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various types of image restoration tasks. The cited papers do not provide additional context about datasets.",
      "processing_time": 31.617583990097046,
      "citing_paper_id": "273502246",
      "cited_paper_id": 14092238
    },
    {
      "context_text": "…Science and Technology, Harbin Institute of Technology, Harbin 150001, China. focuses on correcting a specific type of image degradation( e.g. , denoising [4]–[9], dehazing [10]–[14], desnowing [15]–[17], deraining [18]–[20], deblurring [21], [22] and low-light image enhancement [23]–[25]).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various types of image restoration tasks. The cited papers do not provide additional context about datasets.",
      "processing_time": 31.617583990097046,
      "citing_paper_id": "273502246",
      "cited_paper_id": 220265496
    },
    {
      "context_text": "…Science and Technology, Harbin Institute of Technology, Harbin 150001, China. focuses on correcting a specific type of image degradation( e.g. , denoising [4]–[9], dehazing [10]–[14], desnowing [15]–[17], deraining [18]–[20], deblurring [21], [22] and low-light image enhancement [23]–[25]).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various types of image restoration tasks. The cited papers do not provide additional context about datasets.",
      "processing_time": 31.617583990097046,
      "citing_paper_id": "273502246",
      "cited_paper_id": 236171006
    },
    {
      "context_text": "…Science and Technology, Harbin Institute of Technology, Harbin 150001, China. focuses on correcting a specific type of image degradation( e.g. , denoising [4]–[9], dehazing [10]–[14], desnowing [15]–[17], deraining [18]–[20], deblurring [21], [22] and low-light image enhancement [23]–[25]).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various types of image restoration tasks. The cited papers do not provide additional context about datasets.",
      "processing_time": 31.617583990097046,
      "citing_paper_id": "273502246",
      "cited_paper_id": 244461440
    },
    {
      "context_text": "…Science and Technology, Harbin Institute of Technology, Harbin 150001, China. focuses on correcting a specific type of image degradation( e.g. , denoising [4]–[9], dehazing [10]–[14], desnowing [15]–[17], deraining [18]–[20], deblurring [21], [22] and low-light image enhancement [23]–[25]).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various types of image restoration tasks. The cited papers do not provide additional context about datasets.",
      "processing_time": 31.617583990097046,
      "citing_paper_id": "273502246",
      "cited_paper_id": 257636509
    },
    {
      "context_text": "…Science and Technology, Harbin Institute of Technology, Harbin 150001, China. focuses on correcting a specific type of image degradation( e.g. , denoising [4]–[9], dehazing [10]–[14], desnowing [15]–[17], deraining [18]–[20], deblurring [21], [22] and low-light image enhancement [23]–[25]).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various types of image restoration tasks. The cited papers do not provide additional context about datasets.",
      "processing_time": 31.617583990097046,
      "citing_paper_id": "273502246",
      "cited_paper_id": 260871139
    },
    {
      "context_text": "…researchers collected a series of datasets tailored for various IR tasks, e.g. , BSD100 [26], Manga109 [27] and Urban100 [28] for SR, RainDrop [29], Outdoor-Rain [30], SPA [31] and Rain-100H [32] for draining, GoPro [33] and HIDE [34] for deblurring, LOL [35] for low-light image enhancement,…",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "BSD100",
        "Manga109",
        "Urban100",
        "RainDrop",
        "Outdoor-Rain",
        "SPA",
        "Rain-100H",
        "GoPro",
        "HIDE",
        "LOL"
      ],
      "dataset_descriptions": {
        "BSD100": "Used for super-resolution tasks, providing high-quality images for training and evaluation.",
        "Manga109": "Used for super-resolution tasks, focusing on manga images to enhance detail and clarity.",
        "Urban100": "Used for super-resolution tasks, featuring urban scenes to improve resolution and texture.",
        "RainDrop": "Used for rain removal tasks, providing images with raindrops to train and test rain removal algorithms.",
        "Outdoor-Rain": "Used for rain removal tasks, containing outdoor scenes with rain to evaluate rain removal performance.",
        "SPA": "Used for rain removal tasks, offering synthetic and real rain images to assess rain removal effectiveness.",
        "Rain-100H": "Used for rain removal tasks, consisting of 100 pairs of rainy and clean images for training and testing.",
        "GoPro": "Used for deblurring tasks, featuring sharp and blurred image pairs to train and evaluate deblurring models.",
        "HIDE": "Used for deblurring tasks, providing a large dataset of blurred and clear images for deblurring research.",
        "LOL": "Used for low-light image enhancement, containing low-light and normal-light image pairs to improve low-light visibility."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions several datasets used for different image restoration tasks, which align with the research topic of all-in-one image restoration.",
      "processing_time": 85.44339156150818,
      "citing_paper_id": "273502246",
      "cited_paper_id": 4539586
    },
    {
      "context_text": "One promising solution to address this challenge is machine unlearning [136].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a concept of machine unlearning.",
      "processing_time": 28.93244242668152,
      "citing_paper_id": "273502246",
      "cited_paper_id": 5945696
    },
    {
      "context_text": "Although these works achieve superior progress in the objective quality( e.g. , PSNR, SSIM [36], NIQE [37] and FID [38]), the restored images still suffer from unsatisfied texture generation, hindering the application of IR methods in real-world scenarios.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only metrics (PSNR, SSIM, NIQE, FID) which are excluded according to the rules.",
      "processing_time": 32.13389587402344,
      "citing_paper_id": "273502246",
      "cited_paper_id": 7492030
    },
    {
      "context_text": "…of AiOIR, numerous researchers collected a series of datasets tailored for various IR tasks, e.g. , BSD100 [26], Manga109 [27] and Urban100 [28] for SR, RainDrop [29], Outdoor-Rain [30], SPA [31] and Rain-100H [32] for draining, GoPro [33] and HIDE [34] for deblurring, LOL [35] for…",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "BSD100",
        "Manga109",
        "Urban100",
        "RainDrop",
        "Outdoor-Rain",
        "SPA",
        "Rain-100H",
        "GoPro",
        "HIDE",
        "LOL"
      ],
      "dataset_descriptions": {
        "BSD100": "Used for single image super-resolution, focusing on enhancing image quality and detail.",
        "Manga109": "Used for single image super-resolution, specifically targeting manga images to improve resolution and clarity.",
        "Urban100": "Used for single image super-resolution, focusing on urban scenes to enhance details and textures.",
        "RainDrop": "Used for rain removal, specifically addressing the challenge of removing raindrops from images.",
        "Outdoor-Rain": "Used for rain removal, focusing on outdoor scenes to improve image clarity and remove rain effects.",
        "SPA": "Used for rain removal, specifically designed to handle synthetic and real rain patterns.",
        "Rain-100H": "Used for rain removal, focusing on heavy rain conditions to improve image quality.",
        "GoPro": "Used for deblurring, specifically targeting motion blur in action shots to enhance clarity.",
        "HIDE": "Used for deblurring, focusing on handling various types of blur to improve image sharpness.",
        "LOL": "Used for low-light enhancement, specifically addressing the challenge of improving visibility in low-light conditions."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions several datasets used for specific image restoration tasks, which align with the topic of All-in-One Image Restoration.",
      "processing_time": 81.11903953552246,
      "citing_paper_id": "273502246",
      "cited_paper_id": 8282555
    },
    {
      "context_text": "With the rise of deep learning, image processing has achieved remarkable results, especially driven by convolutional neural networks (CNNs) [1], Transformer [2] and StableDiffusion [3].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only models and methods. No dataset names are present in the text.",
      "processing_time": 30.436253547668457,
      "citing_paper_id": "273502246",
      "cited_paper_id": 13756489
    },
    {
      "context_text": "With the rise of deep learning, image processing has achieved remarkable results, especially driven by convolutional neural networks (CNNs) [1], Transformer [2] and StableDiffusion [3].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only models and methods. No dataset names are present in the text.",
      "processing_time": 30.436253547668457,
      "citing_paper_id": "273502246",
      "cited_paper_id": 195908774
    },
    {
      "context_text": "Using the Half-Quadratic Splitting (HQS) algorithm [157], the equation can be decomposed into two separate subproblems, each addressing the data term and the prior term individually.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (HQS algorithm) used for solving a problem in image recovery.",
      "processing_time": 30.729967832565308,
      "citing_paper_id": "273502246",
      "cited_paper_id": 14511898
    },
    {
      "context_text": "…researchers collected a series of datasets tailored for various IR tasks, e.g. , BSD100 [26], Manga109 [27] and Urban100 [28] for SR, RainDrop [29], Outdoor-Rain [30], SPA [31] and Rain-100H [32] for draining, GoPro [33] and HIDE [34] for deblurring, LOL [35] for low-light image enhancement, etc .",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "BSD100",
        "Manga109",
        "Urban100",
        "RainDrop",
        "Outdoor-Rain",
        "SPA",
        "Rain-100H",
        "GoPro",
        "HIDE",
        "LOL"
      ],
      "dataset_descriptions": {
        "BSD100": "Used for super-resolution tasks, providing high-quality images for training and evaluation.",
        "Manga109": "Used for super-resolution tasks, focusing on manga images to enhance detail and clarity.",
        "Urban100": "Used for super-resolution tasks, containing urban scenes to improve resolution in complex environments.",
        "RainDrop": "Used for deraining tasks, providing synthetic rain images to train models for rain removal.",
        "Outdoor-Rain": "Used for deraining tasks, containing real outdoor rain images to test model robustness.",
        "SPA": "Used for deraining tasks, offering a diverse set of rain images to evaluate deraining performance.",
        "Rain-100H": "Used for deraining tasks, featuring high-quality rain images to assess deraining accuracy.",
        "GoPro": "Used for deblurring tasks, providing sharp and blurred image pairs to train and evaluate deblurring models.",
        "HIDE": "Used for deblurring tasks, containing a variety of blurred images to test deblurring effectiveness.",
        "LOL": "Used for low-light image enhancement, providing low-light images to improve brightness and clarity."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions several datasets used for various image restoration tasks, including super-resolution, deraining, deblurring, and low-light image enhancement. These datasets are clearly identified and used for training and evaluation.",
      "processing_time": 86.49016284942627,
      "citing_paper_id": "273502246",
      "cited_paper_id": 15443600
    },
    {
      "context_text": "…researchers collected a series of datasets tailored for various IR tasks, e.g. , BSD100 [26], Manga109 [27] and Urban100 [28] for SR, RainDrop [29], Outdoor-Rain [30], SPA [31] and Rain-100H [32] for draining, GoPro [33] and HIDE [34] for deblurring, LOL [35] for low-light image enhancement, etc .",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "BSD100",
        "Manga109",
        "Urban100",
        "RainDrop",
        "Outdoor-Rain",
        "SPA",
        "Rain-100H",
        "GoPro",
        "HIDE",
        "LOL"
      ],
      "dataset_descriptions": {
        "BSD100": "Used for super-resolution tasks, providing high-quality images for training and evaluation.",
        "Manga109": "Used for super-resolution tasks, focusing on manga images to enhance detail and clarity.",
        "Urban100": "Used for super-resolution tasks, containing urban scenes to improve resolution in complex environments.",
        "RainDrop": "Used for deraining tasks, providing synthetic rain images to train models for rain removal.",
        "Outdoor-Rain": "Used for deraining tasks, containing real outdoor rain images to test model robustness.",
        "SPA": "Used for deraining tasks, offering a diverse set of rain images to evaluate deraining performance.",
        "Rain-100H": "Used for deraining tasks, featuring high-quality rain images to assess deraining accuracy.",
        "GoPro": "Used for deblurring tasks, providing sharp and blurred image pairs to train and evaluate deblurring models.",
        "HIDE": "Used for deblurring tasks, containing a variety of blurred images to test deblurring effectiveness.",
        "LOL": "Used for low-light image enhancement, providing low-light images to improve brightness and clarity."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions several datasets used for various image restoration tasks, including super-resolution, deraining, deblurring, and low-light image enhancement. These datasets are clearly identified and used for training and evaluation.",
      "processing_time": 86.49016284942627,
      "citing_paper_id": "273502246",
      "cited_paper_id": 91184545
    },
    {
      "context_text": "Mask Image Modeling (MIM) ( e.g. , [148], [180]) is a technique in computer vision that involves training models to predict masked portions of images based on their surrounding context inspired by Mask Language Modeling [151], [181].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only techniques and methods. The cited papers are about language models, not image restoration datasets.",
      "processing_time": 31.3372540473938,
      "citing_paper_id": "273502246",
      "cited_paper_id": 49313245
    },
    {
      "context_text": "Mask Image Modeling (MIM) ( e.g. , [148], [180]) is a technique in computer vision that involves training models to predict masked portions of images based on their surrounding context inspired by Mask Language Modeling [151], [181].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only techniques and methods. The cited papers are about language models, not image restoration datasets.",
      "processing_time": 31.3372540473938,
      "citing_paper_id": "273502246",
      "cited_paper_id": 52967399
    },
    {
      "context_text": "This integration would provide models with rich contextual and structural insights, enhancing their ability to accurately recover images with varied degradations [233].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general reference to image recovery methods.",
      "processing_time": 29.343536615371704,
      "citing_paper_id": "273502246",
      "cited_paper_id": 52073194
    },
    {
      "context_text": "For example, TextPromptIR [94] utilizes a task-specific fine-tuned BERT [151] to accurately understand textual instructions and generating semantic prompts for concerned all-in-one tasks.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (BERT) which is excluded according to the rules.",
      "processing_time": 30.21750283241272,
      "citing_paper_id": "273502246",
      "cited_paper_id": 52967399
    },
    {
      "context_text": "In addition, these approaches often necessitate prior knowledge regarding the type or extent of degradation involved limiting their applicability in real-world scenarios where such information is unknown or variable.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general limitations of existing approaches. No verifiable resources are identified.",
      "processing_time": 30.428113222122192,
      "citing_paper_id": "273502246",
      "cited_paper_id": 173188673
    },
    {
      "context_text": "In addition, these approaches often necessitate prior knowledge regarding the type or extent of degradation involved limiting their applicability in real-world scenarios where such information is unknown or variable.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general limitations of existing approaches. No verifiable resources are identified.",
      "processing_time": 30.428113222122192,
      "citing_paper_id": "273502246",
      "cited_paper_id": 236171006
    },
    {
      "context_text": "In addition, these approaches often necessitate prior knowledge regarding the type or extent of degradation involved limiting their applicability in real-world scenarios where such information is unknown or variable.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general limitations of existing approaches. No verifiable resources are identified.",
      "processing_time": 30.428113222122192,
      "citing_paper_id": "273502246",
      "cited_paper_id": 275789940
    },
    {
      "context_text": "An illustration of transfer learning scenarios [117], [118] is shown in Fig.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only transfer learning scenarios. No verifiable resources are identified.",
      "processing_time": 29.877497673034668,
      "citing_paper_id": "273502246",
      "cited_paper_id": 186206211
    },
    {
      "context_text": "While there are comprehensive surveys for single-task IR methods, such as image super-resolution [39]–[42], deraining [43], [44], dehazing [45], de-noising [46]–[48], deblurring [49], [50], low-light enhancement [51]–[53] and reviews covering diffusion-based image restoration [54], there is…",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to surveys and methods. No dataset names are present in the text.",
      "processing_time": 31.07138752937317,
      "citing_paper_id": "273502246",
      "cited_paper_id": 202660933
    },
    {
      "context_text": "While there are comprehensive surveys for single-task IR methods, such as image super-resolution [39]–[42], deraining [43], [44], dehazing [45], de-noising [46]–[48], deblurring [49], [50], low-light enhancement [51]–[53] and reviews covering diffusion-based image restoration [54], there is…",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to surveys and methods. No dataset names are present in the text.",
      "processing_time": 31.07138752937317,
      "citing_paper_id": "273502246",
      "cited_paper_id": 235755417
    },
    {
      "context_text": "While there are comprehensive surveys for single-task IR methods, such as image super-resolution [39]–[42], deraining [43], [44], dehazing [45], de-noising [46]–[48], deblurring [49], [50], low-light enhancement [51]–[53] and reviews covering diffusion-based image restoration [54], there is…",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to surveys and methods. No dataset names are present in the text.",
      "processing_time": 31.07138752937317,
      "citing_paper_id": "273502246",
      "cited_paper_id": 263311190
    },
    {
      "context_text": "…are comprehensive surveys for single-task IR methods, such as image super-resolution [39]–[42], deraining [43], [44], dehazing [45], de-noising [46]–[48], deblurring [49], [50], low-light enhancement [51]–[53] and reviews covering diffusion-based image restoration [54], there is currently no…",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various image restoration tasks and a review of diffusion-based methods. No verifiable resources are identified.",
      "processing_time": 32.09119486808777,
      "citing_paper_id": "273502246",
      "cited_paper_id": 209515898
    },
    {
      "context_text": "It is widely used in various domains [131]–[133], where related tasks can benefit from shared information.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only a general statement about shared information in related tasks.",
      "processing_time": 29.867992162704468,
      "citing_paper_id": "273502246",
      "cited_paper_id": 210839011
    },
    {
      "context_text": "Single degradation image restoration (SDIR) focuses on recovering clean images from observations that have been corrupted by a specific type of degradation, such as noise, blur, or rain.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a general description of single degradation image restoration. No verifiable resources are identified.",
      "processing_time": 31.052119255065918,
      "citing_paper_id": "273502246",
      "cited_paper_id": 221173039
    },
    {
      "context_text": "Existing methodologies for unlearning can be broadly categorized into two types: exact unlearning [137], which seeks to completely eliminate the impact of specific data points, and approximate unlearning [138], which aims to diminish the influence of such data to a certain degree.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methodologies for unlearning. There are no verifiable resources or datasets mentioned.",
      "processing_time": 31.05925989151001,
      "citing_paper_id": "273502246",
      "cited_paper_id": 225067129
    },
    {
      "context_text": "…methods, such as image super-resolution [39]–[42], deraining [43], [44], dehazing [45], de-noising [46]–[48], deblurring [49], [50], low-light enhancement [51]–[53] and reviews covering diffusion-based image restoration [54], there is currently no review that specifically addresses the AiOIR field.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only various image processing methods and a gap in the literature for AiOIR reviews.",
      "processing_time": 31.56307864189148,
      "citing_paper_id": "273502246",
      "cited_paper_id": 232359322
    },
    {
      "context_text": "…methods, such as image super-resolution [39]–[42], deraining [43], [44], dehazing [45], de-noising [46]–[48], deblurring [49], [50], low-light enhancement [51]–[53] and reviews covering diffusion-based image restoration [54], there is currently no review that specifically addresses the AiOIR field.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only various image processing methods and a gap in the literature for AiOIR reviews.",
      "processing_time": 31.56307864189148,
      "citing_paper_id": "273502246",
      "cited_paper_id": 261031011
    },
    {
      "context_text": "This involves creating models that are not only accurate but also efficient in terms of computational resources, making them suitable for deployment on various platforms, including mobile devices and embedded systems [222], [223].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only discusses the efficiency and deployment of models on mobile devices.",
      "processing_time": 30.17794179916382,
      "citing_paper_id": "273502246",
      "cited_paper_id": 237497252
    },
    {
      "context_text": "1) Prompt Learning: Prompt learning, initially successful in Natural Language Processing (NLP) [140]–[142], aims to tap into the knowledge that the language model itself possesses by providing instructions or relevant information.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a method (prompt learning) and its application in NLP. No verifiable resources are identified.",
      "processing_time": 32.35122728347778,
      "citing_paper_id": "273502246",
      "cited_paper_id": 238857040
    },
    {
      "context_text": "More recently, several prompt-learning-based approaches have emerged [61], [65], [79], [79]–[84].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to prompt-learning-based approaches. No verifiable resources are identified.",
      "processing_time": 31.306092500686646,
      "citing_paper_id": "273502246",
      "cited_paper_id": 245218925
    },
    {
      "context_text": "For instance, Zero-shot methods ( e.g. , [90]–[93]) leverage pre-trained diffusion models as generative priors, seamlessly integrating degraded images as conditions into the sampling process.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only methods and models. No dataset names are present in the text.",
      "processing_time": 31.043694972991943,
      "citing_paper_id": "273502246",
      "cited_paper_id": 246411364
    },
    {
      "context_text": "For instance, Zero-shot methods ( e.g. , [90]–[93]) leverage pre-trained diffusion models as generative priors, seamlessly integrating degraded images as conditions into the sampling process.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only methods and models. No dataset names are present in the text.",
      "processing_time": 31.043694972991943,
      "citing_paper_id": "273502246",
      "cited_paper_id": 249282628
    },
    {
      "context_text": "Recent works have demonstrated the potential of pretrained vision-language models (VLMs) to enhance downstream tasks using generic visual and text representations [177]–[179].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only the use of pretrained vision-language models (VLMs).",
      "processing_time": 30.855839014053345,
      "citing_paper_id": "273502246",
      "cited_paper_id": 246411402
    },
    {
      "context_text": "BLIP [179] improves this by eliminating noisy web data with synthetic captions.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific dataset names, only a method (BLIP) and a general reference to 'noisy web data'. No specific, verifiable datasets are identified.",
      "processing_time": 34.670795917510986,
      "citing_paper_id": "273502246",
      "cited_paper_id": 246411402
    },
    {
      "context_text": "Visual prompting has been extensively explored in various studies [61], [80], [145], [146], addressing both high-level and low-level vision problems, which has sparked significant interest across different visual domains.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general visual domains and problems. No clear, verifiable datasets are identified.",
      "processing_time": 31.542619705200195,
      "citing_paper_id": "273502246",
      "cited_paper_id": 249674493
    },
    {
      "context_text": "Visual prompting has been extensively explored in various studies [61], [80], [145], [146], addressing both high-level and low-level vision problems, which has sparked significant interest across different visual domains.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general visual domains and problems. No clear, verifiable datasets are identified.",
      "processing_time": 31.542619705200195,
      "citing_paper_id": "273502246",
      "cited_paper_id": 264146360
    },
    {
      "context_text": "To achieve blind AiOIR, AirNet [63] learns degradation representations from corrupted images using a contrastive learning strategy.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (AirNet) and a general approach (contrastive learning).",
      "processing_time": 31.538840770721436,
      "citing_paper_id": "273502246",
      "cited_paper_id": 250551851
    },
    {
      "context_text": "…MiOIR [121] rain-haze-noise[63] 3 Rain, Haze, Noise-σ 15 , Noise-σ 25 , Noise-σ 50 BSD400, WED, Urban100, Rain100L, RESIDE-OTS, RESIDE-SOTS AirNet [63], PromptIR [65],PIP [72], Textpromp-tIR [94], NDR [96], InstructIR [62], AdaIR [78], U-WADN [186], DyNet [185], DaAIR [187], AnyIR [188], MEASNet…",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "BSD400",
        "WED",
        "Urban100",
        "Rain100L",
        "RESIDE-OTS",
        "RESIDE-SOTS"
      ],
      "dataset_descriptions": {
        "BSD400": "Used for evaluating image restoration performance across various corruptions, including rain, haze, and noise.",
        "WED": "Used for evaluating image restoration performance across various corruptions, including rain, haze, and noise.",
        "Urban100": "Used for evaluating image restoration performance across various corruptions, including rain, haze, and noise.",
        "Rain100L": "Used for evaluating image restoration performance specifically under rain corruption.",
        "RESIDE-OTS": "Used for evaluating image restoration performance specifically under haze corruption.",
        "RESIDE-SOTS": "Used for evaluating image restoration performance specifically under haze corruption."
      },
      "confidence_score": 1.0,
      "reasoning": "The citation mentions several datasets and methods. Only datasets are extracted, and their usage is described based on the context provided.",
      "processing_time": 61.50979781150818,
      "citing_paper_id": "273502246",
      "cited_paper_id": 250551851
    },
    {
      "context_text": "It is worthy to note that a more challenging scenario is the opposite case [63]–[78], where the degradation affecting the input image is unknown—this defines blind AiOIR, which has shown promising potential for real-world photographs.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a research scenario and a term 'blind AiOIR'. No verifiable resources are identified.",
      "processing_time": 32.32767915725708,
      "citing_paper_id": "273502246",
      "cited_paper_id": 250551851
    },
    {
      "context_text": "It is worthy to note that a more challenging scenario is the opposite case [63]–[78], where the degradation affecting the input image is unknown—this defines blind AiOIR, which has shown promising potential for real-world photographs.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a research scenario and a term 'blind AiOIR'. No verifiable resources are identified.",
      "processing_time": 32.32767915725708,
      "citing_paper_id": "273502246",
      "cited_paper_id": 268553835
    },
    {
      "context_text": "They serve as the foundation for prompt-based methods like TransWeather [64], AirNet [63], and TANet [109].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models and methods. There are no verifiable resources that meet the criteria for inclusion.",
      "processing_time": 32.05613088607788,
      "citing_paper_id": "273502246",
      "cited_paper_id": 250551851
    },
    {
      "context_text": "Unlike unified models ( e.g. , [63]–[65], [121]) that employ mixed training for multiple-degradation restoration, some novel studies approach AiOIR from the perspective of MTL to address the inconsistencies and conflicts among different degradations, treating each degradation as an independent task.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only approaches and models. The context focuses on methodologies and does not reference any particular dataset.",
      "processing_time": 32.05239009857178,
      "citing_paper_id": "273502246",
      "cited_paper_id": 250551851
    },
    {
      "context_text": "Moreover, contrastive learning-based loss functions have been proposed to obtain discriminative degradation representations in the latent space [63], [66], [72], further improving the model’s ability to distinguish between different types of degradations and to generalize to unseen ones.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods and approaches. The context is about contrastive learning-based loss functions and their role in image restoration.",
      "processing_time": 33.29955077171326,
      "citing_paper_id": "273502246",
      "cited_paper_id": 250551851
    },
    {
      "context_text": "Moreover, contrastive learning-based loss functions have been proposed to obtain discriminative degradation representations in the latent space [63], [66], [72], further improving the model’s ability to distinguish between different types of degradations and to generalize to unseen ones.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods and approaches. The context is about contrastive learning-based loss functions and their role in image restoration.",
      "processing_time": 33.29955077171326,
      "citing_paper_id": "273502246",
      "cited_paper_id": 260085391
    },
    {
      "context_text": "Moreover, contrastive learning-based loss functions have been proposed to obtain discriminative degradation representations in the latent space [63], [66], [72], further improving the model’s ability to distinguish between different types of degradations and to generalize to unseen ones.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods and approaches. The context is about contrastive learning-based loss functions and their role in image restoration.",
      "processing_time": 33.29955077171326,
      "citing_paper_id": "273502246",
      "cited_paper_id": 266149517
    },
    {
      "context_text": "For instance, AirNet [63] utilizes degradation encoder representations to guide network recovery, while Tran-sweather [64] applies queries to direct the recovery process.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods/models. No dataset names are present in the context.",
      "processing_time": 30.828596353530884,
      "citing_paper_id": "273502246",
      "cited_paper_id": 250551851
    },
    {
      "context_text": "Meanwhile, MiOIR [121] rain-haze-noise[63] 3 Rain, Haze, Noise-σ 15 , Noise-σ 25 , Noise-σ 50 BSD400, WED, Urban100, Rain100L, RESIDE-OTS, RESIDE-SOTS AirNet [63], PromptIR [65],PIP [72], Textpromp-tIR [94], NDR [96], InstructIR [62], AdaIR [78], U-WADN [186], DyNet [185], DaAIR [187], AnyIR [188],…",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "BSD400",
        "WED",
        "Urban100",
        "Rain100L",
        "RESIDE-OTS",
        "RESIDE-SOTS"
      ],
      "dataset_descriptions": {
        "BSD400": "Used to evaluate image restoration methods under rain, haze, and noise degradations, focusing on general image quality and detail preservation.",
        "WED": "Used to assess the performance of image restoration techniques under various degradations, emphasizing edge and texture preservation.",
        "Urban100": "Used to test image restoration methods on urban scenes, specifically evaluating the restoration of fine details and structures.",
        "Rain100L": "Used to evaluate the effectiveness of image restoration methods in removing rain streaks, focusing on natural image appearance and detail recovery.",
        "RESIDE-OTS": "Used to assess the performance of image restoration techniques in removing haze, emphasizing natural image appearance and color fidelity.",
        "RESIDE-SOTS": "Used to evaluate the effectiveness of image restoration methods in synthetic outdoor scenes, focusing on realistic haze removal and image clarity."
      },
      "confidence_score": 1.0,
      "reasoning": "The citation mentions several datasets used for evaluating image restoration methods under various degradations. These datasets are specific and relevant to the topic of all-in-one image restoration.",
      "processing_time": 70.43591642379761,
      "citing_paper_id": "273502246",
      "cited_paper_id": 250551851
    },
    {
      "context_text": "Various AiOIR approaches have been developed to address these degradations, including techniques outlined in studies such as [55], [61], [63], [65], [66], [70], [72], [73], [94]–[99].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to studies. No dataset names are present in the context.",
      "processing_time": 31.26659083366394,
      "citing_paper_id": "273502246",
      "cited_paper_id": 250551851
    },
    {
      "context_text": "Various AiOIR approaches have been developed to address these degradations, including techniques outlined in studies such as [55], [61], [63], [65], [66], [70], [72], [73], [94]–[99].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to studies. No dataset names are present in the context.",
      "processing_time": 31.26659083366394,
      "citing_paper_id": "273502246",
      "cited_paper_id": 259937812
    },
    {
      "context_text": "Various AiOIR approaches have been developed to address these degradations, including techniques outlined in studies such as [55], [61], [63], [65], [66], [70], [72], [73], [94]–[99].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to studies. No dataset names are present in the context.",
      "processing_time": 31.26659083366394,
      "citing_paper_id": "273502246",
      "cited_paper_id": 260085391
    },
    {
      "context_text": "Various AiOIR approaches have been developed to address these degradations, including techniques outlined in studies such as [55], [61], [63], [65], [66], [70], [72], [73], [94]–[99].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to studies. No dataset names are present in the context.",
      "processing_time": 31.26659083366394,
      "citing_paper_id": "273502246",
      "cited_paper_id": 266149517
    },
    {
      "context_text": "We hope that this review not only maps the current state-of-the-art but also inspires further innovations and breakthroughs in the pursuit of more sophisticated, efficient, and versatile AiOIR models.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, models, or methods. It is a general statement about the review's purpose.",
      "processing_time": 31.497289657592773,
      "citing_paper_id": "273502246",
      "cited_paper_id": 252683961
    },
    {
      "context_text": "We hope that this review not only maps the current state-of-the-art but also inspires further innovations and breakthroughs in the pursuit of more sophisticated, efficient, and versatile AiOIR models.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, models, or methods. It is a general statement about the review's purpose.",
      "processing_time": 31.497289657592773,
      "citing_paper_id": "273502246",
      "cited_paper_id": 277502494
    },
    {
      "context_text": "Inspired by the ability of effectively modeling task-specific context, prompts have been used for finetuning to vision tasks [143], [144].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only the use of prompts for fine-tuning vision tasks.",
      "processing_time": 30.805743932724,
      "citing_paper_id": "273502246",
      "cited_paper_id": 252735181
    },
    {
      "context_text": "In image restoration field, Painter [183] also leverages MIM pretraining.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.3,
      "reasoning": "The context mentions 'MIM pretraining' which could refer to a method or a dataset. However, without more specific information about the nature of MIM, it is not clear if it is a dataset or a method. The title does not provide additional clarity.",
      "processing_time": 37.76474690437317,
      "citing_paper_id": "273502246",
      "cited_paper_id": 254246343
    },
    {
      "context_text": "…comprehensive surveys for single-task IR methods, such as image super-resolution [39]–[42], deraining [43], [44], dehazing [45], de-noising [46]–[48], deblurring [49], [50], low-light enhancement [51]–[53] and reviews covering diffusion-based image restoration [54], there is currently no review…",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various image restoration tasks and a survey paper. No verifiable resources are identified.",
      "processing_time": 32.29807376861572,
      "citing_paper_id": "273502246",
      "cited_paper_id": 255546477
    },
    {
      "context_text": "Test-time adaptation (TTA) ( e.g. , [86]–[88]) has emerged as a key technique in addressing this issue, allowing models to dynamically adjust their parameters during the testing phase to better align with the characteristics of the degraded input images.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a technique called test-time adaptation (TTA). No dataset names are present in the citation span.",
      "processing_time": 32.97211813926697,
      "citing_paper_id": "273502246",
      "cited_paper_id": 257206115
    },
    {
      "context_text": "…Perceive-IR [111] rain-noise-blur-dark[61] 4 Rain, Noise, Blur, Low-light SIDD, merged deraining, merged debluring,LOL ProRes [61] rain-haze-snow[67] 3 Rain, Haze, Snow SPA+,REVIDE,RealSnow WGWS-Net [67], TKMANet [100], Art [135] rain-haze-snow[64] 3 Raindrop, Rain+Fog, Snow All-weather…",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "SIDD",
        "LOL",
        "SPA+",
        "REVIDE",
        "RealSnow"
      ],
      "dataset_descriptions": {
        "SIDD": "Used for training and evaluating image restoration models under low-light conditions, focusing on noise reduction and image enhancement.",
        "LOL": "Used for training and evaluating image restoration models under low-light conditions, focusing on brightness and contrast enhancement.",
        "SPA+": "Used for training and evaluating image restoration models under rain conditions, focusing on rain removal and image clarity.",
        "REVIDE": "Used for training and evaluating image restoration models under haze conditions, focusing on dehazing and visibility improvement.",
        "RealSnow": "Used for training and evaluating image restoration models under snow conditions, focusing on snow removal and image clarity."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions several datasets used for image restoration under various weather conditions. These datasets are specifically named and used for training and evaluation.",
      "processing_time": 58.19025015830994,
      "citing_paper_id": "273502246",
      "cited_paper_id": 259144110
    },
    {
      "context_text": "The AiOIR field has seen the emergence of multiple solutions aimed at addressing these challenges, including works like [56], [64], [67]–[69], [75], [100]–[105].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to other works. No dataset names are present in the context.",
      "processing_time": 31.82873511314392,
      "citing_paper_id": "273502246",
      "cited_paper_id": 259144110
    },
    {
      "context_text": "The AiOIR field has seen the emergence of multiple solutions aimed at addressing these challenges, including works like [56], [64], [67]–[69], [75], [100]–[105].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to other works. No dataset names are present in the context.",
      "processing_time": 31.82873511314392,
      "citing_paper_id": "273502246",
      "cited_paper_id": 266643865
    },
    {
      "context_text": "…Noise, Blur, Low-light SIDD, merged deraining, merged debluring,LOL ProRes [61] rain-haze-snow[67] 3 Rain, Haze, Snow SPA+,REVIDE,RealSnow WGWS-Net [67], TKMANet [100], Art [135] rain-haze-snow[64] 3 Raindrop, Rain+Fog, Snow All-weather (Outdoor-Rain,Snow100k,Raindrop) Transweather [64],…",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "SIDD",
        "LOL ProRes",
        "SPA+",
        "REVIDE",
        "RealSnow",
        "Snow100k",
        "Raindrop"
      ],
      "dataset_descriptions": {
        "SIDD": "Used for evaluating image denoising performance, focusing on noise reduction in low-light conditions.",
        "LOL ProRes": "Used for low-light image enhancement, specifically improving brightness and clarity in dark images.",
        "SPA+": "Used for rain removal, focusing on eliminating rain streaks and improving visibility in rainy scenes.",
        "REVIDE": "Used for deblurring and deraining, specifically addressing motion blur and rain streaks in videos.",
        "RealSnow": "Used for snow removal, focusing on eliminating snowflakes and improving visibility in snowy scenes.",
        "Snow100k": "Used for snow removal, specifically addressing heavy snowfall and improving image clarity in snowy conditions.",
        "Raindrop": "Used for raindrop removal, focusing on eliminating water droplets and improving visibility in rainy scenes."
      },
      "confidence_score": 0.9,
      "reasoning": "The context mentions several datasets and methods, but only specific datasets with clear identifiers are included. The usage is focused on image restoration under various weather conditions.",
      "processing_time": 68.52809953689575,
      "citing_paper_id": "273502246",
      "cited_paper_id": 259144110
    },
    {
      "context_text": "3) Zero-shot: Zero-shot image restoration involves restoring images with distortions not encountered during training, requiring the model to classify unknown classes into specific categories, unlike Open-set methods [89].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a methodological approach. No verifiable resources are identified.",
      "processing_time": 30.98182964324951,
      "citing_paper_id": "273502246",
      "cited_paper_id": 259274648
    },
    {
      "context_text": "DRM-IR [73] enhances the flexibility of All-In-One scenarios by introducing a reference-based, task-adaptive modeling paradigm.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method or model (DRM-IR). The context focuses on the method's capabilities and enhancements.",
      "processing_time": 33.494362592697144,
      "citing_paper_id": "273502246",
      "cited_paper_id": 259937812
    },
    {
      "context_text": "Subsequently, IDR [66] models various degradations based on underlying physical principles and accomplishes AiOIR in two stages.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (IDR) for image restoration. No verifiable resources are identified.",
      "processing_time": 32.54716897010803,
      "citing_paper_id": "273502246",
      "cited_paper_id": 260085391
    },
    {
      "context_text": "The development of a robust multi-task learning theory for AiOIR is still in its early stages, offering significant research opportunities [66], [217].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only references to research opportunities in multi-task learning for AiOIR.",
      "processing_time": 31.480202198028564,
      "citing_paper_id": "273502246",
      "cited_paper_id": 260085391
    },
    {
      "context_text": "…rain-haze-noise-blur-dark[66] 5 Rain, Haze, Noise-σ 25 , Blur, Low-Light BSD400, WED, Urban100, Rain100L, RESIDE-OTS, RESIDE-SOTS, Gopro, LOL IDR [66], AdaIR [78], InstructIR [62], PIP [72], DaAIR [187], AnyIR [188], MEASNet [110], HAIR [189], Perceive-IR [111] rain-noise-blur-dark[61] 4 Rain,…",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "BSD400",
        "WED",
        "Urban100",
        "Rain100L",
        "RESIDE-OTS",
        "RESIDE-SOTS",
        "Gopro",
        "LOL"
      ],
      "dataset_descriptions": {
        "BSD400": "Used to evaluate image restoration methods, focusing on rain, haze, noise, blur, and low-light conditions.",
        "WED": "Used to assess image restoration techniques, particularly under rain, haze, noise, blur, and low-light scenarios.",
        "Urban100": "Used to test image restoration algorithms, emphasizing rain, haze, noise, blur, and low-light environments.",
        "Rain100L": "Used to evaluate rain removal techniques, focusing on heavy rain conditions.",
        "RESIDE-OTS": "Used to assess dehazing methods, focusing on outdoor synthetic hazy images.",
        "RESIDE-SOTS": "Used to evaluate dehazing techniques, focusing on synthetic outdoor hazy images.",
        "Gopro": "Used to test motion deblurring methods, focusing on real-world blurry images.",
        "LOL": "Used to evaluate low-light image enhancement techniques, focusing on low-light conditions."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions several datasets used for image restoration, which are relevant to the research topic of All-in-One Image Restoration.",
      "processing_time": 76.09487700462341,
      "citing_paper_id": "273502246",
      "cited_paper_id": 260085391
    },
    {
      "context_text": "…[62], AdaIR [78], U-WADN [186], DyNet [185], DaAIR [187], AnyIR [188], MEASNet [110], HAIR [189], Perceive-IR [111] rain-haze-noise-blur-dark[66] 5 Rain, Haze, Noise-σ 25 , Blur, Low-Light BSD400, WED, Urban100, Rain100L, RESIDE-OTS, RESIDE-SOTS, Gopro, LOL IDR [66], AdaIR [78], InstructIR…",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "BSD400",
        "WED",
        "Urban100",
        "Rain100L",
        "RESIDE-OTS",
        "RESIDE-SOTS",
        "Gopro",
        "LOL"
      ],
      "dataset_descriptions": {
        "BSD400": "Used to evaluate image restoration methods under various degradations, focusing on benchmarking performance across diverse image content.",
        "WED": "Used to assess the effectiveness of image restoration techniques in handling real-world degradations, emphasizing practical application scenarios.",
        "Urban100": "Used to test image restoration algorithms on high-resolution urban scenes, evaluating their ability to recover fine details and textures.",
        "Rain100L": "Used to evaluate the performance of image restoration methods in removing rain streaks from images, focusing on realistic rain patterns.",
        "RESIDE-OTS": "Used to assess the capability of image restoration models in reducing haze, emphasizing outdoor scenes with varying levels of atmospheric conditions.",
        "RESIDE-SOTS": "Used to evaluate the effectiveness of image restoration techniques in synthetic outdoor scenes with controlled haze levels.",
        "Gopro": "Used to test the performance of image restoration methods in deblurring images, focusing on motion blur in dynamic scenes.",
        "LOL": "Used to evaluate the ability of image restoration algorithms to enhance low-light images, focusing on improving visibility and color fidelity."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions several datasets used for evaluating image restoration methods under various degradations such as rain, haze, noise, blur, and low-light conditions.",
      "processing_time": 81.43886876106262,
      "citing_paper_id": "273502246",
      "cited_paper_id": 260085391
    },
    {
      "context_text": "There is a need to focus on more practical tasks and datasets that reflect the complexities of real-world image degradations [224], [225].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "LSDIR"
      ],
      "dataset_descriptions": {
        "LSDIR": "Used to address practical tasks in image restoration, focusing on real-world image degradations. The dataset provides a large-scale resource for training and evaluating restoration models."
      },
      "confidence_score": 0.9,
      "reasoning": "The context mentions the need for practical tasks and datasets reflecting real-world image degradations, which aligns with the cited paper's title 'LSDIR: A Large Scale Dataset for Image Restoration'.",
      "processing_time": 42.16363716125488,
      "citing_paper_id": "273502246",
      "cited_paper_id": 260843368
    },
    {
      "context_text": "(j) Question-Answering Paradigm: Finally, models such as promptGIP [80] and AutoDIR [70] employ a question-answering framework.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models and methods. The context is about using a question-answering framework for image processing.",
      "processing_time": 33.24260115623474,
      "citing_paper_id": "273502246",
      "cited_paper_id": 264146360
    },
    {
      "context_text": "Building on these ideas, PromptGIP [80] autoencoding [148], where certain portions of question and answer images are randomly masked.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method or model (PromptGIP).",
      "processing_time": 30.77248191833496,
      "citing_paper_id": "273502246",
      "cited_paper_id": 264146360
    },
    {
      "context_text": "…15 , Noise-σ 25 , Noise-σ 50 BSD400, WED, Urban100, Rain100L, RESIDE-OTS, RESIDE-SOTS AirNet [63], PromptIR [65],PIP [72], Textpromp-tIR [94], NDR [96], InstructIR [62], AdaIR [78], U-WADN [186], DyNet [185], DaAIR [187], AnyIR [188], MEASNet [110], HAIR [189], Perceive-IR [111]…",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "BSD400",
        "WED",
        "Urban100",
        "Rain100L",
        "RESIDE-OTS",
        "RESIDE-SOTS"
      ],
      "dataset_descriptions": {
        "BSD400": "Used to evaluate image restoration techniques, focusing on denoising and deblurring performance across diverse images.",
        "WED": "Used to assess the effectiveness of image restoration methods, particularly in handling various types of noise and degradation.",
        "Urban100": "Used to evaluate image restoration techniques, emphasizing high-resolution urban scene images with complex textures.",
        "Rain100L": "Used to test the performance of image restoration methods in removing rain streaks from images.",
        "RESIDE-OTS": "Used to evaluate image deraining techniques, providing a benchmark for outdoor scenes with synthetic rain.",
        "RESIDE-SOTS": "Used to assess the robustness of image deraining methods, offering a more challenging set of real-world rain scenes."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions several datasets and methods, but only datasets are considered. The datasets are used for evaluating image restoration techniques.",
      "processing_time": 65.84054112434387,
      "citing_paper_id": "273502246",
      "cited_paper_id": 264305756
    },
    {
      "context_text": "Another promising avenue is exploiting large multimodal pretrained models, particularly those incorporating generative models, to enhance AiOIR tasks [226]– [229].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only the use of large multimodal pretrained models and generative models to enhance AiOIR tasks.",
      "processing_time": 33.21502494812012,
      "citing_paper_id": "273502246",
      "cited_paper_id": 265149704
    },
    {
      "context_text": "Another promising avenue is exploiting large multimodal pretrained models, particularly those incorporating generative models, to enhance AiOIR tasks [226]– [229].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only the use of large multimodal pretrained models and generative models to enhance AiOIR tasks.",
      "processing_time": 33.21502494812012,
      "citing_paper_id": "273502246",
      "cited_paper_id": 270045122
    },
    {
      "context_text": "5 [152] becomes increasingly popular, Clarity ChatGPT [150], combined with advanced visual models, offers users a simple and efficient way to perform complex image manipulation and enhancement through natural language interaction.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a system called Clarity ChatGPT which is not a dataset but a method or tool.",
      "processing_time": 33.21179986000061,
      "citing_paper_id": "273502246",
      "cited_paper_id": 265295126
    },
    {
      "context_text": "Here, we summarize a variety of approaches to multimodal AiOIR methods ( e.g. , Clarity-ChatGPT [150], AutoDIR [70], Instruct-IPT [116], InstructIR [62]).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various methods and systems for image restoration and enhancement. The cited paper titles do not provide additional context about datasets.",
      "processing_time": 34.57745623588562,
      "citing_paper_id": "273502246",
      "cited_paper_id": 265295126
    },
    {
      "context_text": "Here, we summarize a variety of approaches to multimodal AiOIR methods ( e.g. , Clarity-ChatGPT [150], AutoDIR [70], Instruct-IPT [116], InstructIR [62]).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various methods and systems for image restoration and enhancement. The cited paper titles do not provide additional context about datasets.",
      "processing_time": 34.57745623588562,
      "citing_paper_id": "273502246",
      "cited_paper_id": 267320695
    },
    {
      "context_text": "Here, we summarize a variety of approaches to multimodal AiOIR methods ( e.g. , Clarity-ChatGPT [150], AutoDIR [70], Instruct-IPT [116], InstructIR [62]).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various methods and systems for image restoration and enhancement. The cited paper titles do not provide additional context about datasets.",
      "processing_time": 34.57745623588562,
      "citing_paper_id": "273502246",
      "cited_paper_id": 270869793
    },
    {
      "context_text": "In this context, several innovative studies [62], [70], [74], [94], [116], [150] have introduced textual prompts, advancing the field of AiOIR.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to studies that have introduced textual prompts for image restoration. No clear, verifiable datasets are named.",
      "processing_time": 34.38187503814697,
      "citing_paper_id": "273502246",
      "cited_paper_id": 265295126
    },
    {
      "context_text": "In this context, several innovative studies [62], [70], [74], [94], [116], [150] have introduced textual prompts, advancing the field of AiOIR.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to studies that have introduced textual prompts for image restoration. No clear, verifiable datasets are named.",
      "processing_time": 34.38187503814697,
      "citing_paper_id": "273502246",
      "cited_paper_id": 267320695
    },
    {
      "context_text": "In this context, several innovative studies [62], [70], [74], [94], [116], [150] have introduced textual prompts, advancing the field of AiOIR.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to studies that have introduced textual prompts for image restoration. No clear, verifiable datasets are named.",
      "processing_time": 34.38187503814697,
      "citing_paper_id": "273502246",
      "cited_paper_id": 270869793
    },
    {
      "context_text": "This structured approach ensures a thorough evaluation of the performance and capabilities of different AiOIR methods.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only a general approach to evaluating AiOIR methods. No dataset names are present in the text.",
      "processing_time": 33.203367471694946,
      "citing_paper_id": "273502246",
      "cited_paper_id": 265609570
    },
    {
      "context_text": "This structured approach ensures a thorough evaluation of the performance and capabilities of different AiOIR methods.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only a general approach to evaluating AiOIR methods. No dataset names are present in the text.",
      "processing_time": 33.203367471694946,
      "citing_paper_id": "273502246",
      "cited_paper_id": 268876071
    },
    {
      "context_text": "This structured approach ensures a thorough evaluation of the performance and capabilities of different AiOIR methods.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only a general approach to evaluating AiOIR methods. No dataset names are present in the text.",
      "processing_time": 33.203367471694946,
      "citing_paper_id": "273502246",
      "cited_paper_id": 272367682
    },
    {
      "context_text": "This structured approach ensures a thorough evaluation of the performance and capabilities of different AiOIR methods.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only a general approach to evaluating AiOIR methods. No dataset names are present in the text.",
      "processing_time": 33.203367471694946,
      "citing_paper_id": "273502246",
      "cited_paper_id": 273323238
    },
    {
      "context_text": "This structured approach ensures a thorough evaluation of the performance and capabilities of different AiOIR methods.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only a general approach to evaluating AiOIR methods. No dataset names are present in the text.",
      "processing_time": 33.203367471694946,
      "citing_paper_id": "273502246",
      "cited_paper_id": 273532643
    },
    {
      "context_text": "This structured approach ensures a thorough evaluation of the performance and capabilities of different AiOIR methods.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only a general approach to evaluating AiOIR methods. No dataset names are present in the text.",
      "processing_time": 33.203367471694946,
      "citing_paper_id": "273502246",
      "cited_paper_id": 274281468
    },
    {
      "context_text": "This structured approach ensures a thorough evaluation of the performance and capabilities of different AiOIR methods.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only a general approach to evaluating AiOIR methods. No dataset names are present in the text.",
      "processing_time": 33.203367471694946,
      "citing_paper_id": "273502246",
      "cited_paper_id": 276960871
    },
    {
      "context_text": "This structured approach ensures a thorough evaluation of the performance and capabilities of different AiOIR methods.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only a general approach to evaluating AiOIR methods. No dataset names are present in the text.",
      "processing_time": 33.203367471694946,
      "citing_paper_id": "273502246",
      "cited_paper_id": 277244267
    },
    {
      "context_text": "This structured approach ensures a thorough evaluation of the performance and capabilities of different AiOIR methods.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only a general approach to evaluating AiOIR methods. No dataset names are present in the text.",
      "processing_time": 33.203367471694946,
      "citing_paper_id": "273502246",
      "cited_paper_id": 277667714
    },
    {
      "context_text": "This structured approach ensures a thorough evaluation of the performance and capabilities of different AiOIR methods.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only a general approach to evaluating AiOIR methods. No dataset names are present in the text.",
      "processing_time": 33.203367471694946,
      "citing_paper_id": "273502246",
      "cited_paper_id": 277755777
    },
    {
      "context_text": "Meanwhile, MiOIR [121] rain-haze-noise[63] 3 Rain, Haze, Noise-σ 15 , Noise-σ 25 , Noise-σ 50 BSD400, WED, Urban100, Rain100L, RESIDE-OTS, RESIDE-SOTS AirNet [63], PromptIR [65],PIP [72], Textpromp-tIR [94], NDR [96], InstructIR [62], AdaIR [78], U-WADN [186], DyNet [185], DaAIR [187], AnyIR [188], MEASNet [110], HAIR [189], Perceive-IR [111] rain-haze-noise-blur-dark[66] 5 Rain, Haze, Noise-σ 25 , Blur, Low-Light BSD400, WED, Urban100, Rain100L, RESIDE-OTS, RESIDE-SOTS, Gopro, LOL IDR [66], AdaIR [78], InstructIR [62], PIP [72], DaAIR [187], AnyIR [188], MEASNet [110], HAIR [189], Perceive-IR [111] rain-noise-blur-dark[61] 4 Rain, Noise, Blur, Low-light SIDD, merged deraining, merged debluring,LOL ProRes [61] rain-haze-snow[67] 3 Rain, Haze, Snow SPA+,REVIDE,RealSnow WGWS-Net [67], TKMANet [100], Art [135] rain-haze-snow[64] 3 Raindrop, Rain+Fog, Snow All-weather (Outdoor-Rain,Snow100k,Raindrop) Transweather [64], WeatherDiff [69], AWRCP [68], AirNet [56] rain-haze-snow[173] 3 Rain, Haze, Snow WeatherStream (176, 100 training images and 11, 400 testing images) LDR [74] SimMIM [180] introduced a generalized masked image modeling approach based on the Swin-ViT [182] architecture.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "BSD400",
        "WED",
        "Urban100",
        "Rain100L",
        "RESIDE-OTS",
        "RESIDE-SOTS",
        "Gopro",
        "LOL",
        "SIDD",
        "SPA+",
        "REVIDE",
        "RealSnow",
        "All-weather",
        "WeatherStream"
      ],
      "dataset_descriptions": {
        "BSD400": "Used for evaluating image restoration models, focusing on various degradation types including rain, haze, and noise.",
        "WED": "Used for evaluating image restoration models, focusing on various degradation types including rain, haze, and noise.",
        "Urban100": "Used for evaluating image restoration models, focusing on various degradation types including rain, haze, and noise.",
        "Rain100L": "Used for evaluating image restoration models, focusing on rain removal and other degradation types.",
        "RESIDE-OTS": "Used for evaluating image restoration models, focusing on haze removal and other degradation types.",
        "RESIDE-SOTS": "Used for evaluating image restoration models, focusing on haze removal and other degradation types.",
        "Gopro": "Used for evaluating image restoration models, focusing on deblurring and other degradation types.",
        "LOL": "Used for evaluating image restoration models, focusing on low-light enhancement and other degradation types.",
        "SIDD": "Used for evaluating image restoration models, focusing on noise reduction and other degradation types.",
        "SPA+": "Used for evaluating image restoration models, focusing on rain, haze, and snow removal.",
        "REVIDE": "Used for evaluating image restoration models, focusing on rain, haze, and snow removal.",
        "RealSnow": "Used for evaluating image restoration models, focusing on snow removal and other degradation types.",
        "All-weather": "Used for evaluating image restoration models, focusing on various weather conditions including rain, snow, and fog.",
        "WeatherStream": "Used for evaluating image restoration models, focusing on various weather conditions including rain, snow, and fog, with a large number of training and testing images."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions several datasets used for evaluating and training image restoration models, which are directly relevant to the topic of all-in-one image restoration.",
      "processing_time": 108.75783443450928,
      "citing_paper_id": "273502246",
      "cited_paper_id": 265609570
    },
    {
      "context_text": "Perceive-IR [111] utilizes the semantic prior knowledge and structural information mined by a DINO-v2-based guidance module to enhance the restoration process.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (DINO-v2) and a general task (image restoration).",
      "processing_time": 32.89612555503845,
      "citing_paper_id": "273502246",
      "cited_paper_id": 265609570
    },
    {
      "context_text": "(g) Pretrained Model Prior: Models like Perceive-IR [111] and DA-CLIP [76] utilize frozen vision-language models, such as CLIP [112], DINO [113], and DINO-v2 [114].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models and methods. The context focuses on pretrained models and their application in image restoration.",
      "processing_time": 32.88277339935303,
      "citing_paper_id": "273502246",
      "cited_paper_id": 265609570
    },
    {
      "context_text": "Degradation-awareness seems to be a key factor, with models like DaAIR and Perceive-IR [111] showing that tailoring the restoration process based on the specific type of degradation improves overall performance.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models and methods. The context focuses on the importance of degradation-awareness in image restoration models.",
      "processing_time": 33.456570625305176,
      "citing_paper_id": "273502246",
      "cited_paper_id": 265609570
    },
    {
      "context_text": "Examples include works by Potlapalli et al. [65], Li et al. [72], and Fan et al. [115].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to other works. There is no information about the actual use of datasets in the given context.",
      "processing_time": 34.36347985267639,
      "citing_paper_id": "273502246",
      "cited_paper_id": 266149517
    },
    {
      "context_text": "PIP [72] introduces a novel Prompt-In-Prompt learning framework for universal image restoration, which employs two innovative prompts: a degradation-aware prompt and a basic restoration prompt.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method or framework. The context focuses on the introduction of a novel learning framework for image restoration.",
      "processing_time": 34.14130520820618,
      "citing_paper_id": "273502246",
      "cited_paper_id": 266149517
    },
    {
      "context_text": "…Haze, Noise-σ 25 , Blur, Low-Light BSD400, WED, Urban100, Rain100L, RESIDE-OTS, RESIDE-SOTS, Gopro, LOL IDR [66], AdaIR [78], InstructIR [62], PIP [72], DaAIR [187], AnyIR [188], MEASNet [110], HAIR [189], Perceive-IR [111] rain-noise-blur-dark[61] 4 Rain, Noise, Blur, Low-light SIDD, merged…",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "BSD400",
        "WED",
        "Urban100",
        "Rain100L",
        "RESIDE-OTS",
        "RESIDE-SOTS",
        "Gopro",
        "LOL",
        "SIDD"
      ],
      "dataset_descriptions": {
        "BSD400": "Used to evaluate image restoration techniques, focusing on haze removal and other degradations. The dataset provides diverse images for benchmarking.",
        "WED": "Used to assess image restoration performance, particularly for noise reduction and low-light enhancement. The dataset includes challenging real-world images.",
        "Urban100": "Used to evaluate image restoration techniques, focusing on urban scenes with various degradations. The dataset provides high-resolution images for benchmarking.",
        "Rain100L": "Used to test rain removal algorithms, providing synthetic rain images with varying intensities. The dataset is designed to challenge restoration models.",
        "RESIDE-OTS": "Used to evaluate haze removal techniques, providing outdoor scenes with synthetic haze. The dataset is designed to test the robustness of restoration models.",
        "RESIDE-SOTS": "Used to evaluate haze removal techniques, providing synthetic and real-world hazy images. The dataset is designed to test the generalizability of restoration models.",
        "Gopro": "Used to assess deblurring performance, providing sharp and blurred image pairs. The dataset is designed to challenge motion blur restoration algorithms.",
        "LOL": "Used to evaluate low-light image enhancement, providing pairs of low-light and normal-light images. The dataset is designed to test the effectiveness of enhancement techniques.",
        "SIDD": "Used to evaluate noise reduction techniques, providing noisy and clean image pairs. The dataset is designed to test the robustness of denoising algorithms."
      },
      "confidence_score": 0.9,
      "reasoning": "The context mentions several datasets and methods, but only datasets are extracted. The datasets are used for evaluating image restoration techniques under various degradations.",
      "processing_time": 96.93920660018921,
      "citing_paper_id": "273502246",
      "cited_paper_id": 266149517
    },
    {
      "context_text": "III, PIP [72] and TextPromptIR [94] achieve the best average performance, indicating strong generalization across different degradation types.",
      "catation_intent": "findings",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods or models. The context focuses on the performance of different approaches to image restoration.",
      "processing_time": 32.877482414245605,
      "citing_paper_id": "273502246",
      "cited_paper_id": 266149517
    },
    {
      "context_text": "…3 Rain, Haze, Noise-σ 15 , Noise-σ 25 , Noise-σ 50 BSD400, WED, Urban100, Rain100L, RESIDE-OTS, RESIDE-SOTS AirNet [63], PromptIR [65],PIP [72], Textpromp-tIR [94], NDR [96], InstructIR [62], AdaIR [78], U-WADN [186], DyNet [185], DaAIR [187], AnyIR [188], MEASNet [110], HAIR [189],…",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "BSD400",
        "WED",
        "Urban100",
        "Rain100L",
        "RESIDE-OTS",
        "RESIDE-SOTS"
      ],
      "dataset_descriptions": {
        "BSD400": "Used to evaluate image restoration models, focusing on denoising and deblurring tasks with a diverse set of images.",
        "WED": "Used to assess the performance of image restoration models on watermarked and encrypted images, emphasizing robustness.",
        "Urban100": "Used to evaluate super-resolution and denoising models on urban scenes, highlighting fine details and textures.",
        "Rain100L": "Used to test rain removal algorithms, focusing on heavy rain conditions and their impact on image quality.",
        "RESIDE-OTS": "Used to evaluate dehazing models on outdoor scenes, assessing the ability to restore visibility in hazy conditions.",
        "RESIDE-SOTS": "Used to evaluate dehazing models on synthetic outdoor scenes, focusing on the realism and effectiveness of haze removal."
      },
      "confidence_score": 1.0,
      "reasoning": "The citation mentions several datasets and methods, but only the datasets are considered. The datasets are used for evaluating image restoration models under various conditions.",
      "processing_time": 68.87697505950928,
      "citing_paper_id": "273502246",
      "cited_paper_id": 266149517
    },
    {
      "context_text": "For instance, PromptIR [65] introduces a series of learnable prompts to encode discriminative information about different degradation types, involving a substantial number of parameters.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (PromptIR) and its application. No verifiable resources are identified.",
      "processing_time": 32.87182402610779,
      "citing_paper_id": "273502246",
      "cited_paper_id": 266191538
    },
    {
      "context_text": "The MoFME [155] framework outperforms previous approaches in both image restoration and down-stream tasks, demonstrating its effectiveness and efficiency.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a framework (MoFME) and its performance. No verifiable resources are identified.",
      "processing_time": 33.16978073120117,
      "citing_paper_id": "273502246",
      "cited_paper_id": 266573162
    },
    {
      "context_text": "The MoFME [155] framework outperforms previous approaches in both image restoration and down-stream tasks, demonstrating its effectiveness and efficiency.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a framework (MoFME) and its performance. No verifiable resources are identified.",
      "processing_time": 33.16978073120117,
      "citing_paper_id": "273502246",
      "cited_paper_id": 267061016
    },
    {
      "context_text": "The MoFME [155] framework outperforms previous approaches in both image restoration and down-stream tasks, demonstrating its effectiveness and efficiency.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a framework (MoFME) and its performance. No verifiable resources are identified.",
      "processing_time": 33.16978073120117,
      "citing_paper_id": "273502246",
      "cited_paper_id": 278326960
    },
    {
      "context_text": "A significant trend in recent research is the incorporation of prompt-based techniques—visual, textual, or multimodal—that guide the restoration process and enhance adaptability.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only a trend in research methods. No verifiable resources are identified.",
      "processing_time": 32.223894357681274,
      "citing_paper_id": "273502246",
      "cited_paper_id": 267061016
    },
    {
      "context_text": "…Urban100, Rain100L, RESIDE-OTS, RESIDE-SOTS AirNet [63], PromptIR [65],PIP [72], Textpromp-tIR [94], NDR [96], InstructIR [62], AdaIR [78], U-WADN [186], DyNet [185], DaAIR [187], AnyIR [188], MEASNet [110], HAIR [189], Perceive-IR [111] rain-haze-noise-blur-dark[66] 5 Rain, Haze, Noise-σ 25 ,…",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Urban100",
        "Rain100L",
        "RESIDE-OTS",
        "RESIDE-SOTS",
        "AirNet",
        "PromptIR",
        "PIP",
        "Textprompt-IR",
        "NDR",
        "InstructIR",
        "AdaIR",
        "U-WADN",
        "DyNet",
        "DaAIR",
        "AnyIR",
        "MEASNet",
        "HAIR",
        "Perceive-IR"
      ],
      "dataset_descriptions": {
        "Urban100": "Used to evaluate image restoration techniques, focusing on urban scenes with various degradations.",
        "Rain100L": "Used to assess performance in removing rain streaks from images, emphasizing realistic rain effects.",
        "RESIDE-OTS": "Used to test haze removal algorithms, providing outdoor scenes with synthetic haze.",
        "RESIDE-SOTS": "Used to evaluate haze removal, featuring real-world outdoor scenes with varying levels of haze.",
        "AirNet": "Used to assess image restoration quality, particularly in aerial imagery with diverse degradations.",
        "PromptIR": "Used to evaluate image restoration, focusing on prompt-based methods for various degradations.",
        "PIP": "Used to test image restoration techniques, particularly for handling multiple degradations simultaneously.",
        "Textprompt-IR": "Used to evaluate text-prompt guided image restoration, focusing on integrating textual information.",
        "NDR": "Used to assess noise reduction in images, particularly for high-frequency noise.",
        "InstructIR": "Used to evaluate instruction-based image restoration, focusing on user-guided restoration.",
        "AdaIR": "Used to test adaptive image restoration techniques, emphasizing flexibility across different degradations.",
        "U-WADN": "Used to evaluate unified-width adaptive dynamic networks for image restoration, focusing on efficiency and adaptability.",
        "DyNet": "Used to assess dynamic network approaches for image restoration, emphasizing real-time performance.",
        "DaAIR": "Used to evaluate data-augmented image restoration, focusing on enhancing robustness through data augmentation.",
        "AnyIR": "Used to test all-in-one image restoration techniques, covering a wide range of degradations.",
        "MEASNet": "Used to evaluate multi-exposure adaptive synthesis networks for image restoration, focusing on dynamic range enhancement.",
        "HAIR": "Used to assess hybrid adaptive image restoration, combining multiple techniques for improved results.",
        "Perceive-IR": "Used to evaluate perception-driven image restoration, focusing on human visual quality."
      },
      "confidence_score": 0.9,
      "reasoning": "The context mentions multiple datasets and methods, but only datasets with clear identifiers are included. The context suggests these datasets are used for evaluating image restoration techniques.",
      "processing_time": 125.66061329841614,
      "citing_paper_id": "273502246",
      "cited_paper_id": 267199899
    },
    {
      "context_text": "…, Noise-σ 50 BSD400, WED, Urban100, Rain100L, RESIDE-OTS, RESIDE-SOTS AirNet [63], PromptIR [65],PIP [72], Textpromp-tIR [94], NDR [96], InstructIR [62], AdaIR [78], U-WADN [186], DyNet [185], DaAIR [187], AnyIR [188], MEASNet [110], HAIR [189], Perceive-IR [111] rain-haze-noise-blur-dark[66] 5…",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "BSD400",
        "WED",
        "Urban100",
        "Rain100L",
        "RESIDE-OTS",
        "RESIDE-SOTS",
        "AirNet"
      ],
      "dataset_descriptions": {
        "BSD400": "Used to evaluate image denoising performance, focusing on noise reduction at σ 50.",
        "WED": "Used to assess image restoration quality, particularly for weighted error distribution.",
        "Urban100": "Used to evaluate image restoration on urban scenes, focusing on detail preservation and artifact reduction.",
        "Rain100L": "Used to test rain removal algorithms, focusing on heavy rain conditions.",
        "RESIDE-OTS": "Used to evaluate dehazing performance, focusing on outdoor synthetic images.",
        "RESIDE-SOTS": "Used to evaluate dehazing performance, focusing on outdoor real-world images.",
        "AirNet": "Used to test air-light estimation in hazy images, focusing on realistic haze simulation."
      },
      "confidence_score": 0.9,
      "reasoning": "The context mentions several datasets and methods, but only datasets are considered. The datasets are used for evaluating image restoration techniques.",
      "processing_time": 66.58208441734314,
      "citing_paper_id": "273502246",
      "cited_paper_id": 267320695
    },
    {
      "context_text": "In-structIR [62] trains models using common image datasets and prompts generated using GPT-4, note that this generalizes to human-written instructions .",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.3,
      "reasoning": "The context mentions 'common image datasets' but does not specify any particular dataset names. The term 'common image datasets' is too generic and lacks specific identifiers.",
      "processing_time": 35.166489124298096,
      "citing_paper_id": "273502246",
      "cited_paper_id": 267320695
    },
    {
      "context_text": "Some works [55]–[62] require prior knowledge of the degradation in the corrupted image to feed it to the appropriate restoration head or guide the design of the image restoration network.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general reference to prior knowledge of degradation in images. No verifiable resources are identified.",
      "processing_time": 33.42048621177673,
      "citing_paper_id": "273502246",
      "cited_paper_id": 267320695
    },
    {
      "context_text": "(i) Textual and Multimodal Prompting: Extending the concept of prompting, models like Yan et al. [94], Conde et al. [62], and Tian et al. [116] incorporate textual or multimodal prompts.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only models and methods. The cited papers' titles suggest a focus on image restoration and processing, but do not introduce specific datasets.",
      "processing_time": 35.63749289512634,
      "citing_paper_id": "273502246",
      "cited_paper_id": 267320695
    },
    {
      "context_text": "(i) Textual and Multimodal Prompting: Extending the concept of prompting, models like Yan et al. [94], Conde et al. [62], and Tian et al. [116] incorporate textual or multimodal prompts.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only models and methods. The cited papers' titles suggest a focus on image restoration and processing, but do not introduce specific datasets.",
      "processing_time": 35.63749289512634,
      "citing_paper_id": "273502246",
      "cited_paper_id": 270869793
    },
    {
      "context_text": "…5 Rain, Haze, Noise-σ 25 , Blur, Low-Light BSD400, WED, Urban100, Rain100L, RESIDE-OTS, RESIDE-SOTS, Gopro, LOL IDR [66], AdaIR [78], InstructIR [62], PIP [72], DaAIR [187], AnyIR [188], MEASNet [110], HAIR [189], Perceive-IR [111] rain-noise-blur-dark[61] 4 Rain, Noise, Blur, Low-light SIDD,…",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "BSD400",
        "WED",
        "Urban100",
        "Rain100L",
        "RESIDE-OTS",
        "RESIDE-SOTS",
        "Gopro",
        "LOL"
      ],
      "dataset_descriptions": {
        "BSD400": "Used to evaluate image restoration methods, focusing on various degradations such as rain, haze, noise, blur, and low-light conditions.",
        "WED": "Used to assess image restoration performance, particularly under complex degradations including rain, noise, blur, and low-light scenarios.",
        "Urban100": "Used to test image restoration algorithms, emphasizing urban scenes with rain, noise, blur, and low-light challenges.",
        "Rain100L": "Used to evaluate rain removal techniques, providing a large set of images with varying rain intensities and other degradations.",
        "RESIDE-OTS": "Used to assess dehazing algorithms, containing outdoor scenes with synthetic haze and additional degradations.",
        "RESIDE-SOTS": "Used to evaluate dehazing methods, featuring synthetic outdoor scenes with realistic haze and other image degradations.",
        "Gopro": "Used to test motion deblurring algorithms, providing sharp and blurred image pairs for evaluation.",
        "LOL": "Used to evaluate low-light image enhancement, offering pairs of low-light and well-lit images for benchmarking."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions multiple datasets used for image restoration experiments, including BSD400, WED, Urban100, Rain100L, RESIDE-OTS, RESIDE-SOTS, Gopro, and LOL. These datasets are specific and relevant to the topic of all-in-one image restoration.",
      "processing_time": 89.06170892715454,
      "citing_paper_id": "273502246",
      "cited_paper_id": 267320695
    },
    {
      "context_text": "…BSD400, WED, Urban100, Rain100L, RESIDE-OTS, RESIDE-SOTS AirNet [63], PromptIR [65],PIP [72], Textpromp-tIR [94], NDR [96], InstructIR [62], AdaIR [78], U-WADN [186], DyNet [185], DaAIR [187], AnyIR [188], MEASNet [110], HAIR [189], Perceive-IR [111] rain-haze-noise-blur-dark[66] 5 Rain, Haze,…",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "BSD400",
        "WED",
        "Urban100",
        "Rain100L",
        "RESIDE-OTS",
        "RESIDE-SOTS",
        "AirNet",
        "PromptIR",
        "PIP",
        "TextpromptIR",
        "NDR",
        "InstructIR",
        "AdaIR",
        "U-WADN",
        "DyNet",
        "DaAIR",
        "AnyIR",
        "MEASNet",
        "HAIR",
        "Perceive-IR",
        "rain-haze-noise-blur-dark"
      ],
      "dataset_descriptions": {
        "BSD400": "Used to evaluate image restoration techniques, focusing on denoising and deblurring performance.",
        "WED": "Used to assess the effectiveness of image restoration methods, particularly in handling various degradations.",
        "Urban100": "Used to evaluate image restoration techniques, emphasizing high-resolution urban scene restoration.",
        "Rain100L": "Used to test rain removal algorithms, focusing on light rain conditions.",
        "RESIDE-OTS": "Used to evaluate dehazing algorithms, focusing on outdoor synthetic hazy images.",
        "RESIDE-SOTS": "Used to evaluate dehazing algorithms, focusing on outdoor real-world hazy images.",
        "AirNet": "Used to test image restoration methods, particularly for aerial images with various degradations.",
        "PromptIR": "Used to evaluate image restoration techniques, focusing on adaptive restoration methods.",
        "PIP": "Used to assess the performance of image restoration methods, particularly in handling complex degradations.",
        "TextpromptIR": "Used to evaluate text-based image restoration techniques, focusing on integrating textual prompts.",
        "NDR": "Used to test noise reduction algorithms, focusing on natural image datasets.",
        "InstructIR": "Used to evaluate instruction-based image restoration methods, focusing on adaptive restoration.",
        "AdaIR": "Used to test adaptive all-in-one image restoration techniques, focusing on frequency mining and modulation.",
        "U-WADN": "Used to evaluate image restoration methods, particularly for underwater scenes.",
        "DyNet": "Used to test dynamic image restoration techniques, focusing on real-time performance.",
        "DaAIR": "Used to evaluate domain-adaptive image restoration methods, focusing on cross-domain generalization.",
        "AnyIR": "Used to test all-in-one image restoration techniques, focusing on versatility across degradations.",
        "MEASNet": "Used to evaluate multi-exposure image restoration methods, focusing on dynamic range enhancement.",
        "HAIR": "Used to test hybrid image restoration techniques, combining multiple restoration approaches.",
        "Perceive-IR": "Used to evaluate perceptual image restoration methods, focusing on visual quality.",
        "rain-haze-noise-blur-dark": "Used to test comprehensive image restoration techniques, covering multiple degradation types."
      },
      "confidence_score": 0.9,
      "reasoning": "The context mentions multiple datasets and methods, but only datasets are extracted. The datasets are used for evaluating image restoration techniques.",
      "processing_time": 137.77917742729187,
      "citing_paper_id": "273502246",
      "cited_paper_id": 268553835
    },
    {
      "context_text": "…5 Rain, Haze, Noise-σ 25 , Blur, Low-Light BSD400, WED, Urban100, Rain100L, RESIDE-OTS, RESIDE-SOTS, Gopro, LOL IDR [66], AdaIR [78], InstructIR [62], PIP [72], DaAIR [187], AnyIR [188], MEASNet [110], HAIR [189], Perceive-IR [111] rain-noise-blur-dark[61] 4 Rain, Noise, Blur,…",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "BSD400",
        "WED",
        "Urban100",
        "Rain100L",
        "RESIDE-OTS",
        "RESIDE-SOTS",
        "Gopro",
        "LOL"
      ],
      "dataset_descriptions": {
        "BSD400": "Used to evaluate image restoration methods under various degradations, focusing on benchmarking performance across different image qualities.",
        "WED": "Used to assess the effectiveness of image restoration techniques in handling various types of image degradations, including rain, haze, noise, and blur.",
        "Urban100": "Used to test image restoration methods on high-resolution urban scenes, emphasizing the preservation of fine details and textures.",
        "Rain100L": "Used to evaluate the performance of image restoration algorithms in removing rain streaks from images, focusing on realistic rain effects.",
        "RESIDE-OTS": "Used to assess the ability of image restoration methods to remove haze from outdoor scenes, focusing on synthetic hazy images.",
        "RESIDE-SOTS": "Used to evaluate the effectiveness of image restoration techniques in removing haze from outdoor scenes, focusing on real-world hazy images.",
        "Gopro": "Used to test the performance of image restoration methods in deblurring images, focusing on motion blur in video frames.",
        "LOL": "Used to evaluate the ability of image restoration algorithms to enhance low-light images, focusing on improving visibility and color fidelity."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions multiple datasets used for evaluating image restoration methods under various degradations. These datasets are specific and relevant to the research topic.",
      "processing_time": 83.48391270637512,
      "citing_paper_id": "273502246",
      "cited_paper_id": 268553835
    },
    {
      "context_text": "For dehazing, Instruct-IPT [116] achieves the highest PSNR of 39.95, significantly outperforming others, followed by MEASNet [110] and DyNet [185].",
      "catation_intent": "findings",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models and their performance metrics. The context focuses on comparing the performance of different models for dehazing.",
      "processing_time": 34.497586727142334,
      "citing_paper_id": "273502246",
      "cited_paper_id": 268856875
    },
    {
      "context_text": "For dehazing, Instruct-IPT [116] achieves the highest PSNR of 39.95, significantly outperforming others, followed by MEASNet [110] and DyNet [185].",
      "catation_intent": "findings",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models and their performance metrics. The context focuses on comparing the performance of different models for dehazing.",
      "processing_time": 34.497586727142334,
      "citing_paper_id": "273502246",
      "cited_paper_id": 270869793
    },
    {
      "context_text": "…RESIDE-OTS, RESIDE-SOTS AirNet [63], PromptIR [65],PIP [72], Textpromp-tIR [94], NDR [96], InstructIR [62], AdaIR [78], U-WADN [186], DyNet [185], DaAIR [187], AnyIR [188], MEASNet [110], HAIR [189], Perceive-IR [111] rain-haze-noise-blur-dark[66] 5 Rain, Haze, Noise-σ 25 , Blur, Low-Light…",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "RESIDE-OTS",
        "RESIDE-SOTS",
        "rain-haze-noise-blur-dark"
      ],
      "dataset_descriptions": {
        "RESIDE-OTS": "Used to evaluate all-in-one image restoration models, focusing on outdoor scene degradation including rain, haze, noise, blur, and low-light conditions.",
        "RESIDE-SOTS": "Used to evaluate all-in-one image restoration models, focusing on synthetic outdoor scene degradation including rain, haze, noise, blur, and low-light conditions.",
        "rain-haze-noise-blur-dark": "Used to evaluate all-in-one image restoration models, focusing on combined degradations including rain, haze, noise, blur, and low-light conditions."
      },
      "confidence_score": 0.9,
      "reasoning": "The context mentions several datasets and methods, but only 'RESIDE-OTS', 'RESIDE-SOTS', and 'rain-haze-noise-blur-dark' appear to be specific datasets used for image restoration tasks.",
      "processing_time": 59.93389081954956,
      "citing_paper_id": "273502246",
      "cited_paper_id": 268856875
    },
    {
      "context_text": "DyNet [185] are also trained in parallel branches to reconstruct the clean image from masked degraded inputs.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (DyNet) used for image restoration.",
      "processing_time": 31.90347385406494,
      "citing_paper_id": "273502246",
      "cited_paper_id": 268856875
    },
    {
      "context_text": "Contrastive learning approaches typically serve as an additional form of regularization to improve the generalization of single-task restoration models [126]–[130].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only contrastive learning approaches as a form of regularization.",
      "processing_time": 31.713918924331665,
      "citing_paper_id": "273502246",
      "cited_paper_id": 269214582
    },
    {
      "context_text": "Currently, there is only one notable article focused on All-in-One document image restoration, which is DocRes [108].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions 'DocRes' but does not refer to it as a dataset. It is described as a model or method for document image restoration tasks.",
      "processing_time": 34.88567304611206,
      "citing_paper_id": "273502246",
      "cited_paper_id": 269614217
    },
    {
      "context_text": "MEASNet and DaAIR [187] also show strong results, with high average scores, indicating their robustness across multiple degradation types.",
      "catation_intent": "findings",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models or methods. The context focuses on the performance of MEASNet and DaAIR across multiple degradation types.",
      "processing_time": 35.08280634880066,
      "citing_paper_id": "273502246",
      "cited_paper_id": 270045495
    },
    {
      "context_text": "…RESIDE-SOTS AirNet [63], PromptIR [65],PIP [72], Textpromp-tIR [94], NDR [96], InstructIR [62], AdaIR [78], U-WADN [186], DyNet [185], DaAIR [187], AnyIR [188], MEASNet [110], HAIR [189], Perceive-IR [111] rain-haze-noise-blur-dark[66] 5 Rain, Haze, Noise-σ 25 , Blur, Low-Light BSD400, WED,…",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "BSD400",
        "WED"
      ],
      "dataset_descriptions": {
        "BSD400": "Used to evaluate image restoration models, focusing on various degradations including rain, haze, noise, blur, and low-light conditions.",
        "WED": "Used to evaluate image restoration models, focusing on various degradations including rain, haze, noise, blur, and low-light conditions."
      },
      "confidence_score": 0.9,
      "reasoning": "The context mentions several datasets and methods, but only 'BSD400' and 'WED' are specific datasets. Other names are methods or models.",
      "processing_time": 48.114492654800415,
      "citing_paper_id": "273502246",
      "cited_paper_id": 270045495
    },
    {
      "context_text": "…25 , Blur, Low-Light BSD400, WED, Urban100, Rain100L, RESIDE-OTS, RESIDE-SOTS, Gopro, LOL IDR [66], AdaIR [78], InstructIR [62], PIP [72], DaAIR [187], AnyIR [188], MEASNet [110], HAIR [189], Perceive-IR [111] rain-noise-blur-dark[61] 4 Rain, Noise, Blur, Low-light SIDD, merged deraining,…",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "BSD400",
        "WED",
        "Urban100",
        "Rain100L",
        "RESIDE-OTS",
        "RESIDE-SOTS",
        "Gopro",
        "LOL",
        "SIDD"
      ],
      "dataset_descriptions": {
        "BSD400": "Used for evaluating image restoration models, focusing on blur degradation. The dataset provides high-quality images for benchmarking.",
        "WED": "Used for evaluating image restoration models, focusing on various degradations. The dataset includes diverse images for comprehensive testing.",
        "Urban100": "Used for evaluating image restoration models, focusing on urban scenes. The dataset provides high-resolution images of cityscapes.",
        "Rain100L": "Used for evaluating image restoration models, focusing on rain degradation. The dataset includes synthetic rain effects on real images.",
        "RESIDE-OTS": "Used for evaluating image restoration models, focusing on haze removal. The dataset provides outdoor scenes with varying levels of haze.",
        "RESIDE-SOTS": "Used for evaluating image restoration models, focusing on haze removal. The dataset provides synthetic and real-world hazy images.",
        "Gopro": "Used for evaluating image restoration models, focusing on motion blur. The dataset includes sharp and blurred pairs of images.",
        "LOL": "Used for evaluating image restoration models, focusing on low-light conditions. The dataset provides pairs of low-light and normal-light images.",
        "SIDD": "Used for evaluating image restoration models, focusing on noise reduction. The dataset includes raw and denoised image pairs."
      },
      "confidence_score": 0.9,
      "reasoning": "The context mentions multiple datasets and methods, but only datasets are extracted. The datasets are used for image restoration tasks, particularly for various degradations such as blur, low-light, rain, and noise.",
      "processing_time": 90.94123888015747,
      "citing_paper_id": "273502246",
      "cited_paper_id": 270045495
    },
    {
      "context_text": "Notable approaches in this area include AMIR [106] and ProCT [107].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods (AMIR and ProCT). The context does not provide information about the usage of datasets.",
      "processing_time": 34.840869665145874,
      "citing_paper_id": "273502246",
      "cited_paper_id": 270123740
    },
    {
      "context_text": "It is worth noting that diverse and unforeseen degradations may arise in the real-world scenarios jointly, hence the need for AiOIR models further developing.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only the need for AiOIR models. No verifiable resources are identified.",
      "processing_time": 33.068023920059204,
      "citing_paper_id": "273502246",
      "cited_paper_id": 271039782
    },
    {
      "context_text": "Future research should focus on developing semi-supervised and unsupervised learning approaches that can learn effective representations from unlabeled or partially labeled data [220], [221].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a direction for future research. No verifiable resources are identified.",
      "processing_time": 32.3762583732605,
      "citing_paper_id": "273502246",
      "cited_paper_id": 272724496
    },
    {
      "context_text": "Qin et al. [184] introduces RAM into AiOIR, a framework designed to extract intrinsic image information from corrupted images through the use of MIM pre-training, along with a fine-tuning algorithm that facilitates the transition from masked images to fully restored ones.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions 'MIM pre-training', which likely refers to a method or technique rather than a specific dataset. No other specific datasets are mentioned.",
      "processing_time": 34.46197557449341,
      "citing_paper_id": "273502246",
      "cited_paper_id": 272987034
    },
    {
      "context_text": "To facilitate a comprehensive and efficient comparison of various AiOIR methods, we begin by summarizing the key datasets, experimental setups, and evaluation metrics commonly used across different tasks.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a general reference to 'key datasets'. No specific names or identifiers are provided.",
      "processing_time": 32.76985549926758,
      "citing_paper_id": "273502246",
      "cited_paper_id": 273532643
    },
    {
      "context_text": "To facilitate a comprehensive and efficient comparison of various AiOIR methods, we begin by summarizing the key datasets, experimental setups, and evaluation metrics commonly used across different tasks.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a general reference to 'key datasets'. No specific names or identifiers are provided.",
      "processing_time": 32.76985549926758,
      "citing_paper_id": "273502246",
      "cited_paper_id": 277667714
    },
    {
      "context_text": "These learned representations are then employed to restore the clean image.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only a general process of using learned representations for image restoration.",
      "processing_time": 31.853206872940063,
      "citing_paper_id": "273502246",
      "cited_paper_id": 275789940
    },
    {
      "context_text": "Furthermore, AutoDIR [70] can handle images with unknown degradations in unseen task via an intuitive text prompt, empowering users to refine restoration results in alignment with their visual preferences.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (AutoDIR) and its capabilities. No verifiable resources are identified.",
      "processing_time": 33.32963490486145,
      "citing_paper_id": "273502246",
      "cited_paper_id": 277244267
    },
    {
      "context_text": "This prompts the model to reconstruct these patches from the unmasked areas, and during inference, input-output pairs are assembled as task prompts to facilitate image restoration.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general process for image restoration. No verifiable resources are identified.",
      "processing_time": 32.77715826034546,
      "citing_paper_id": "273502246",
      "cited_paper_id": 278528939
    },
    {
      "context_text": "‘rain-haze-snow’setting :Following[31],thetrainingdataset, termed “AllWeather”, includes images from Snow100K [22], Raindrop [28], and Outdoor-Rain [17], encompassing a variety of weather-related degradations.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Snow100K",
        "Raindrop",
        "Outdoor-Rain"
      ],
      "dataset_descriptions": {
        "Snow100K": "Used to train the AllWeather dataset for snow removal, providing a large set of synthetic snow images to enhance robustness.",
        "Raindrop": "Used to train the AllWeather dataset for raindrop removal, offering a diverse set of raindrop images to improve model generalization.",
        "Outdoor-Rain": "Used to train the AllWeather dataset for outdoor rain removal, contributing real-world rain images to address various rain intensities and conditions."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions specific datasets used for training in the 'rain-haze-snow' setting, which are relevant to the topic of image restoration.",
      "processing_time": 53.206286907196045,
      "citing_paper_id": "273643057",
      "cited_paper_id": 837707
    },
    {
      "context_text": "Specifically, we employ Rain100L [35] for deraining, the Indoor Training Set (ITS) from the RESIDE [16] dataset for dehazing, a combination of BSD400 [2] and WED [23] for denoising, GoPro [24] for deblurring, and LOL [33] for low-light enhancement.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Rain100L",
        "RESIDE",
        "BSD400",
        "WED",
        "GoPro",
        "LOL"
      ],
      "dataset_descriptions": {
        "Rain100L": "Used for deraining, focusing on removing rain streaks from images using deep learning methods.",
        "RESIDE": "Used for dehazing, specifically employing the Indoor Training Set (ITS) to improve visibility in hazy images.",
        "BSD400": "Used for denoising, combined with WED to enhance image quality by reducing noise.",
        "WED": "Used for denoising, combined with BSD400 to enhance image quality by reducing noise.",
        "GoPro": "Used for deblurring, focusing on restoring sharpness in blurred images using deep learning techniques.",
        "LOL": "Used for low-light enhancement, focusing on improving visibility in low-light conditions."
      },
      "confidence_score": 1.0,
      "reasoning": "The citation clearly mentions specific datasets used for various image restoration tasks, which are directly relevant to the research topic of all-in-one image restoration.",
      "processing_time": 64.76561665534973,
      "citing_paper_id": "273643057",
      "cited_paper_id": 15443600
    },
    {
      "context_text": "Specifically, we employ Rain100L [35] for deraining, the Indoor Training Set (ITS) from the RESIDE [16] dataset for dehazing, a combination of BSD400 [2] and WED [23] for denoising, GoPro [24] for deblurring, and LOL [33] for low-light enhancement.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Rain100L",
        "RESIDE",
        "BSD400",
        "WED",
        "GoPro",
        "LOL"
      ],
      "dataset_descriptions": {
        "Rain100L": "Used for deraining, focusing on removing rain streaks from images using deep learning methods.",
        "RESIDE": "Used for dehazing, specifically employing the Indoor Training Set (ITS) to improve visibility in hazy images.",
        "BSD400": "Used for denoising, combined with WED to enhance image quality by reducing noise.",
        "WED": "Used for denoising, combined with BSD400 to enhance image quality by reducing noise.",
        "GoPro": "Used for deblurring, focusing on restoring sharpness in blurred images using deep learning techniques.",
        "LOL": "Used for low-light enhancement, focusing on improving visibility in low-light conditions."
      },
      "confidence_score": 1.0,
      "reasoning": "The citation clearly mentions specific datasets used for various image restoration tasks, which are directly relevant to the research topic of all-in-one image restoration.",
      "processing_time": 64.76561665534973,
      "citing_paper_id": "273643057",
      "cited_paper_id": 206764694
    },
    {
      "context_text": "Recent research has shifted attention to unified models, which is achieved by harnessing task-oriented information to inform and guide the learning process of the unified model [15, 21, 38].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only a shift in research focus towards unified models. No verifiable resources are identified.",
      "processing_time": 34.008089542388916,
      "citing_paper_id": "273643057",
      "cited_paper_id": 227239228
    },
    {
      "context_text": "Recent research has shifted attention to unified models, which is achieved by harnessing task-oriented information to inform and guide the learning process of the unified model [15, 21, 38].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only a shift in research focus towards unified models. No verifiable resources are identified.",
      "processing_time": 34.008089542388916,
      "citing_paper_id": "273643057",
      "cited_paper_id": 260085391
    },
    {
      "context_text": "…detail, we assume that the degradation type of each training sample is known, and we denote the input as 𝑥 𝑡𝑖 , where 𝑖 ∈ 1 , . . . , 𝑁 represents the sample index, 𝑡 ∈ 1 , . . . ,𝐾 indicates the degradation type, 𝑁 is the total number of samples, and 𝐾 is the number of degradation types.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific, verifiable datasets. It only describes the structure of the training data without naming any particular dataset.",
      "processing_time": 33.31484580039978,
      "citing_paper_id": "273643057",
      "cited_paper_id": 227239228
    },
    {
      "context_text": "However, these task-specific methods face a significant challenge in real-world scenarios: when they excel in a certain task, they may completely fail in other tasks.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only the challenges faced by task-specific methods in real-world scenarios.",
      "processing_time": 32.10322189331055,
      "citing_paper_id": "273643057",
      "cited_paper_id": 235358213
    },
    {
      "context_text": "This comprehensive dataset comprises the SPA+ [42] dataset for image deraining, RealSnow [42] for image desnowing, and REVIDE [41] for image dehazing, allowing us to assess the effectiveness of our method across diverse real-world degradation scenarios. iv.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "SPA+",
        "RealSnow",
        "REVIDE"
      ],
      "dataset_descriptions": {
        "SPA+": "Used for image deraining, assessing the method's effectiveness in removing rain artifacts from images.",
        "RealSnow": "Used for image desnowing, evaluating the method's performance in removing snow artifacts from images.",
        "REVIDE": "Used for image dehazing, testing the method's capability to restore images degraded by haze."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions three specific datasets used for evaluating the method across different image restoration tasks.",
      "processing_time": 46.69212460517883,
      "citing_paper_id": "273643057",
      "cited_paper_id": 235719890
    },
    {
      "context_text": "Recently, deep-learning based single-task models have demonstrated exceptional performance in the respective domains, such as denoising [20, 36, 40], deblurring [5, 26, 32], deraining [7, 8, 12, 37], and dehazing [9, 29].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to various deep-learning based models and their applications in image restoration tasks.",
      "processing_time": 33.996752977371216,
      "citing_paper_id": "273643057",
      "cited_paper_id": 244346144
    },
    {
      "context_text": "Recently, deep-learning based single-task models have demonstrated exceptional performance in the respective domains, such as denoising [20, 36, 40], deblurring [5, 26, 32], deraining [7, 8, 12, 37], and dehazing [9, 29].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to various deep-learning based models and their applications in image restoration tasks.",
      "processing_time": 33.996752977371216,
      "citing_paper_id": "273643057",
      "cited_paper_id": 247134149
    },
    {
      "context_text": "Recently, deep-learning based single-task models have demonstrated exceptional performance in the respective domains, such as denoising [20, 36, 40], deblurring [5, 26, 32], deraining [7, 8, 12, 37], and dehazing [9, 29].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to various deep-learning based models and their applications in image restoration tasks.",
      "processing_time": 33.996752977371216,
      "citing_paper_id": "273643057",
      "cited_paper_id": 257636509
    },
    {
      "context_text": "[27] and Prompt-In-Prompt [19] further simplify the task-oriented guidance by an adaptive prompt learning approach, enabling end-to-end multi-task training and achieving robust performance.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and approaches. The context is focused on the methodologies used for image restoration.",
      "processing_time": 33.98118019104004,
      "citing_paper_id": "273643057",
      "cited_paper_id": 260085391
    },
    {
      "context_text": "It is worth noting that even with suboptimal hyper-parameter settings, our proposed method still outperforms the baseline models presented in [14], which achieve a PSNR of 29.51 dB.",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a performance metric (PSNR) and a comparison to baseline models. No verifiable resources are identified.",
      "processing_time": 35.35255002975464,
      "citing_paper_id": "273643057",
      "cited_paper_id": 266844752
    },
    {
      "context_text": "Although MioIR [14] has improved the performance of all-in-one image restoration by elaborately changing the task settings, it fundamentally does not solve the conflict between tasks.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions MioIR but does not refer to it as a dataset. It is described as a method or approach, not a reusable data resource.",
      "processing_time": 35.34125280380249,
      "citing_paper_id": "273643057",
      "cited_paper_id": 266844752
    },
    {
      "context_text": "Most recently, MioIR [14] adopts a novel training scheme to improve the all-in-one image restoration problem by a sequential and prompt learning strategy.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions MioIR but does not refer to it as a dataset. It is described as a method or approach, not a reusable dataset.",
      "processing_time": 35.036712408065796,
      "citing_paper_id": "273643057",
      "cited_paper_id": 266844752
    },
    {
      "context_text": "Detailed experimental settings are as follows: i. ‘multiple degradation’ setting : Following [14], there are 7 different image restoration tasks.",
      "catation_intent": "reusable resource",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a setting for multiple degradation tasks. No clear, verifiable dataset names are provided.",
      "processing_time": 34.39527082443237,
      "citing_paper_id": "273643057",
      "cited_paper_id": 266844752
    },
    {
      "context_text": "Count of Tasks Detail Degradation multiple degradation [14] 7 SR, Blur, Noise, JPEG, Rain, Haze, Low-Light rain-haze-noise [15] 5 Rain, Haze, Noise-𝜎 15, Noise-𝜎 25, Noise-𝜎 50 rain-haze-snow [31] 3 Rain, Haze, Snow rain-haze-noise-blur-dark [38] 5 Rain, Haze, Noise, Blur, Low-Light",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions several degradation types but does not specify any named datasets. The degradation types are listed as part of task counts and are not identified as specific, verifiable datasets.",
      "processing_time": 35.875290870666504,
      "citing_paper_id": "273643057",
      "cited_paper_id": 266844752
    },
    {
      "context_text": "Table 5 demonstrates the superior performance of our proposed multi-task balanced learning approach, Art, compared to the vanilla mixed Training and the state-of-the-art sequential training strategy introduced by MiOIR [14].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a comparison of training strategies. No verifiable resources are identified.",
      "processing_time": 33.019054651260376,
      "citing_paper_id": "273643057",
      "cited_paper_id": 266844752
    },
    {
      "context_text": "The optimization of the network is guided by an L 1 loss function, employing the AdamW optimizer [48] with parameters β 1 = 0 .",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only an optimizer method. No datasets are referenced or used in the context provided.",
      "processing_time": 33.28243708610535,
      "citing_paper_id": "271874811",
      "cited_paper_id": 3312944
    },
    {
      "context_text": "Nonetheless, the predominant approach in current research is to employ task-specific models, each tailored to address a particular type of degradation [16, 7, 12, 13, 8].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only a general approach in research. No dataset names are present in the text.",
      "processing_time": 34.18174457550049,
      "citing_paper_id": "271874811",
      "cited_paper_id": 3619954
    },
    {
      "context_text": "Nonetheless, the predominant approach in current research is to employ task-specific models, each tailored to address a particular type of degradation [16, 7, 12, 13, 8].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only a general approach in research. No dataset names are present in the text.",
      "processing_time": 34.18174457550049,
      "citing_paper_id": "271874811",
      "cited_paper_id": 254018182
    },
    {
      "context_text": "While single degradation methods do achieve great success [9, 10, 11, 7, 12, 13], blind all-in-one image restoration, which aims to utilize a single deep blind restoration model to tackle multiple types of degradation simultaneously without the prior information of the degradation of the input…",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general methods and approaches. The cited papers' titles also do not provide clear dataset names.",
      "processing_time": 35.04407715797424,
      "citing_paper_id": "271874811",
      "cited_paper_id": 3846544
    },
    {
      "context_text": "While single degradation methods do achieve great success [9, 10, 11, 7, 12, 13], blind all-in-one image restoration, which aims to utilize a single deep blind restoration model to tackle multiple types of degradation simultaneously without the prior information of the degradation of the input…",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general methods and approaches. The cited papers' titles also do not provide clear dataset names.",
      "processing_time": 35.04407715797424,
      "citing_paper_id": "271874811",
      "cited_paper_id": 8550762
    },
    {
      "context_text": "While single degradation methods do achieve great success [9, 10, 11, 7, 12, 13], blind all-in-one image restoration, which aims to utilize a single deep blind restoration model to tackle multiple types of degradation simultaneously without the prior information of the degradation of the input…",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general methods and approaches. The cited papers' titles also do not provide clear dataset names.",
      "processing_time": 35.04407715797424,
      "citing_paper_id": "271874811",
      "cited_paper_id": 85501306
    },
    {
      "context_text": "While single degradation methods do achieve great success [9, 10, 11, 7, 12, 13], blind all-in-one image restoration, which aims to utilize a single deep blind restoration model to tackle multiple types of degradation simultaneously without the prior information of the degradation of the input…",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general methods and approaches. The cited papers' titles also do not provide clear dataset names.",
      "processing_time": 35.04407715797424,
      "citing_paper_id": "271874811",
      "cited_paper_id": 254018182
    },
    {
      "context_text": "The use of deep learning in this domain has made remarkable progress, as evidenced by a suite of recent methodologies [6, 7, 8, 9, 10, 11, 12, 13, 14, 15].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only a suite of methodologies. The cited papers' titles do not provide additional context about datasets.",
      "processing_time": 34.9987952709198,
      "citing_paper_id": "271874811",
      "cited_paper_id": 3846544
    },
    {
      "context_text": "The use of deep learning in this domain has made remarkable progress, as evidenced by a suite of recent methodologies [6, 7, 8, 9, 10, 11, 12, 13, 14, 15].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only a suite of methodologies. The cited papers' titles do not provide additional context about datasets.",
      "processing_time": 34.9987952709198,
      "citing_paper_id": "271874811",
      "cited_paper_id": 8550762
    },
    {
      "context_text": "The use of deep learning in this domain has made remarkable progress, as evidenced by a suite of recent methodologies [6, 7, 8, 9, 10, 11, 12, 13, 14, 15].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only a suite of methodologies. The cited papers' titles do not provide additional context about datasets.",
      "processing_time": 34.9987952709198,
      "citing_paper_id": "271874811",
      "cited_paper_id": 85501306
    },
    {
      "context_text": "The use of deep learning in this domain has made remarkable progress, as evidenced by a suite of recent methodologies [6, 7, 8, 9, 10, 11, 12, 13, 14, 15].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only a suite of methodologies. The cited papers' titles do not provide additional context about datasets.",
      "processing_time": 34.9987952709198,
      "citing_paper_id": "271874811",
      "cited_paper_id": 254018182
    },
    {
      "context_text": "The use of deep learning in this domain has made remarkable progress, as evidenced by a suite of recent methodologies [6, 7, 8, 9, 10, 11, 12, 13, 14, 15].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only a suite of methodologies. The cited papers' titles do not provide additional context about datasets.",
      "processing_time": 34.9987952709198,
      "citing_paper_id": "271874811",
      "cited_paper_id": 268856875
    },
    {
      "context_text": "It becomes evident that HAIR excels not only in recognizing various degradation types, such as noise, rain, and haze, but also in distinguishing between the same type of degradation at varying intensities, exemplified by noise with standard deviations of [15, 25, 50].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets by name, only describing the capabilities of HAIR in recognizing various degradation types. The cited papers' titles do not provide additional dataset names.",
      "processing_time": 36.51739764213562,
      "citing_paper_id": "271874811",
      "cited_paper_id": 4840263
    },
    {
      "context_text": "It becomes evident that HAIR excels not only in recognizing various degradation types, such as noise, rain, and haze, but also in distinguishing between the same type of degradation at varying intensities, exemplified by noise with standard deviations of [15, 25, 50].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets by name, only describing the capabilities of HAIR in recognizing various degradation types. The cited papers' titles do not provide additional dataset names.",
      "processing_time": 36.51739764213562,
      "citing_paper_id": "271874811",
      "cited_paper_id": 268856875
    },
    {
      "context_text": "For single-task image denoising, we use the BSD400 [49] and WED [50] datasets, adding Gaussian noise at levels σ ∈ { 15 , 25 , 50 } to generate training images.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "BSD400",
        "WED"
      ],
      "dataset_descriptions": {
        "BSD400": "Used for single-task image denoising, generating training images by adding Gaussian noise at levels σ ∈ {15, 25, 50}.",
        "WED": "Used for single-task image denoising, generating training images by adding Gaussian noise at levels σ ∈ {15, 25, 50}."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions two specific datasets, BSD400 and WED, which are used for generating training images by adding Gaussian noise for single-task image denoising.",
      "processing_time": 51.03154110908508,
      "citing_paper_id": "271874811",
      "cited_paper_id": 4840263
    },
    {
      "context_text": "Additionally, the Weights Box parameter N is set according to the sequence [5, 7, 7, 9] for each respective level.",
      "catation_intent": "none",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only a parameter setting for a method. No dataset names are present in the text.",
      "processing_time": 34.38742446899414,
      "citing_paper_id": "271874811",
      "cited_paper_id": 8550762
    },
    {
      "context_text": "Image classification is a fundamental task in Computer Vision, with the goal of categorizing images based on their content [33].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general task in Computer Vision.",
      "processing_time": 31.78123164176941,
      "citing_paper_id": "271874811",
      "cited_paper_id": 9985555
    },
    {
      "context_text": "Dehazing experiments utilize the SOTS dataset [54].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "SOTS"
      ],
      "dataset_descriptions": {
        "SOTS": "Used for dehazing experiments, specifically evaluating single-image dehazing methods and their performance on synthetic and real-world images."
      },
      "confidence_score": 1.0,
      "reasoning": "The context clearly mentions the use of the SOTS dataset for dehazing experiments, which is directly relevant to the topic of image restoration.",
      "processing_time": 40.32190728187561,
      "citing_paper_id": "271874811",
      "cited_paper_id": 39760169
    },
    {
      "context_text": "Deblurring and low-light enhancement tasks use the GoPro [55] and LOL-v1 [56] datasets respectively.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "GoPro",
        "LOL-v1"
      ],
      "dataset_descriptions": {
        "GoPro": "Used for deblurring tasks, providing a benchmark for evaluating image restoration algorithms in motion blur scenarios.",
        "LOL-v1": "Used for low-light enhancement tasks, offering a dataset to test and improve the performance of algorithms in low-light conditions."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions two specific datasets, GoPro and LOL-v1, which are used for deblurring and low-light enhancement tasks, respectively.",
      "processing_time": 45.72002911567688,
      "citing_paper_id": "271874811",
      "cited_paper_id": 52008443
    },
    {
      "context_text": "The presence of adverse conditions such as noise, haze, or rain can severely diminish the effectiveness of images for a variety of applications, such as autonomous navigation [1, 2], augmented reality [3, 4, 5].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only general applications affected by image degradation. The cited papers do not provide additional dataset information.",
      "processing_time": 34.75319790840149,
      "citing_paper_id": "271874811",
      "cited_paper_id": 55358798
    },
    {
      "context_text": "The presence of adverse conditions such as noise, haze, or rain can severely diminish the effectiveness of images for a variety of applications, such as autonomous navigation [1, 2], augmented reality [3, 4, 5].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only general applications affected by image degradation. The cited papers do not provide additional dataset information.",
      "processing_time": 34.75319790840149,
      "citing_paper_id": "271874811",
      "cited_paper_id": 244714491
    },
    {
      "context_text": "Given that these vectors are capable of classifying hundreds of categories [39], they must contain the essential information of the images.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific dataset name, only a general capability of vectors. The cited paper title 'ImageNet: A large-scale hierarchical image database' suggests a potential dataset, but it is not explicitly used in the given context.",
      "processing_time": 38.14546227455139,
      "citing_paper_id": "271874811",
      "cited_paper_id": 57246310
    },
    {
      "context_text": "Pioneering efforts in this area [20, 21, 1, 22, 23, 24, 25, 26, 15] have utilized a variety of advanced techniques such as contrastive learning [20], meta-learning [25], visual prompting methods [24, 27, 28, 14].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only techniques and methods. The cited papers' titles do not provide additional context to identify datasets.",
      "processing_time": 35.29809093475342,
      "citing_paper_id": "271874811",
      "cited_paper_id": 195787503
    },
    {
      "context_text": "Pioneering efforts in this area [20, 21, 1, 22, 23, 24, 25, 26, 15] have utilized a variety of advanced techniques such as contrastive learning [20], meta-learning [25], visual prompting methods [24, 27, 28, 14].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only techniques and methods. The cited papers' titles do not provide additional context to identify datasets.",
      "processing_time": 35.29809093475342,
      "citing_paper_id": "271874811",
      "cited_paper_id": 244714491
    },
    {
      "context_text": "Pioneering efforts in this area [20, 21, 1, 22, 23, 24, 25, 26, 15] have utilized a variety of advanced techniques such as contrastive learning [20], meta-learning [25], visual prompting methods [24, 27, 28, 14].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only techniques and methods. The cited papers' titles do not provide additional context to identify datasets.",
      "processing_time": 35.29809093475342,
      "citing_paper_id": "271874811",
      "cited_paper_id": 247411392
    },
    {
      "context_text": "Pioneering efforts in this area [20, 21, 1, 22, 23, 24, 25, 26, 15] have utilized a variety of advanced techniques such as contrastive learning [20], meta-learning [25], visual prompting methods [24, 27, 28, 14].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only techniques and methods. The cited papers' titles do not provide additional context to identify datasets.",
      "processing_time": 35.29809093475342,
      "citing_paper_id": "271874811",
      "cited_paper_id": 250551851
    },
    {
      "context_text": "Pioneering efforts in this area [20, 21, 1, 22, 23, 24, 25, 26, 15] have utilized a variety of advanced techniques such as contrastive learning [20], meta-learning [25], visual prompting methods [24, 27, 28, 14].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only techniques and methods. The cited papers' titles do not provide additional context to identify datasets.",
      "processing_time": 35.29809093475342,
      "citing_paper_id": "271874811",
      "cited_paper_id": 268856875
    },
    {
      "context_text": "We conducted a comparative evaluation of our All-in-One image restoration framework against several state-of-the-art specialized methods, including BRDNet [57], LPNet [58], FDGAN [59], DL [23], MPRNet [21], AirNet [20], PromptIR [24], and DaAIR [26].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only models and methods used for comparison. The cited papers also do not provide additional information about datasets.",
      "processing_time": 35.29533624649048,
      "citing_paper_id": "271874811",
      "cited_paper_id": 195787503
    },
    {
      "context_text": "We conducted a comparative evaluation of our All-in-One image restoration framework against several state-of-the-art specialized methods, including BRDNet [57], LPNet [58], FDGAN [59], DL [23], MPRNet [21], AirNet [20], PromptIR [24], and DaAIR [26].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only models and methods used for comparison. The cited papers also do not provide additional information about datasets.",
      "processing_time": 35.29533624649048,
      "citing_paper_id": "271874811",
      "cited_paper_id": 203042751
    },
    {
      "context_text": "We conducted a comparative evaluation of our All-in-One image restoration framework against several state-of-the-art specialized methods, including BRDNet [57], LPNet [58], FDGAN [59], DL [23], MPRNet [21], AirNet [20], PromptIR [24], and DaAIR [26].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only models and methods used for comparison. The cited papers also do not provide additional information about datasets.",
      "processing_time": 35.29533624649048,
      "citing_paper_id": "271874811",
      "cited_paper_id": 210838848
    },
    {
      "context_text": "We conducted a comparative evaluation of our All-in-One image restoration framework against several state-of-the-art specialized methods, including BRDNet [57], LPNet [58], FDGAN [59], DL [23], MPRNet [21], AirNet [20], PromptIR [24], and DaAIR [26].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only models and methods used for comparison. The cited papers also do not provide additional information about datasets.",
      "processing_time": 35.29533624649048,
      "citing_paper_id": "271874811",
      "cited_paper_id": 250551851
    },
    {
      "context_text": "Moreover, the backbone can be substituted with other straightforward image classification networks such as VGG [44] and Inception [45], without any detrimental impact on performance, given that the classification of degradation types is a relatively simple task.",
      "catation_intent": "none",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models (VGG and Inception). The context focuses on the interchangeability of backbone networks for image classification tasks.",
      "processing_time": 35.82712173461914,
      "citing_paper_id": "271874811",
      "cited_paper_id": 206593880
    },
    {
      "context_text": "We start by entering into the backbone, which is composed of multiple ResNet blocks [35] followed by downsampling × 2 .",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a method (ResNet blocks).",
      "processing_time": 32.0287709236145,
      "citing_paper_id": "271874811",
      "cited_paper_id": 206594692
    },
    {
      "context_text": "While traditional methods relied on handcrafted techniques [34], the advent of neural networks has revolutionized feature extraction [35, 36, 37, 38].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only the shift from handcrafted techniques to neural networks for feature extraction.",
      "processing_time": 34.14451289176941,
      "citing_paper_id": "271874811",
      "cited_paper_id": 206594692
    },
    {
      "context_text": "While traditional methods relied on handcrafted techniques [34], the advent of neural networks has revolutionized feature extraction [35, 36, 37, 38].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only the shift from handcrafted techniques to neural networks for feature extraction.",
      "processing_time": 34.14451289176941,
      "citing_paper_id": "271874811",
      "cited_paper_id": 225039882
    },
    {
      "context_text": "While traditional methods relied on handcrafted techniques [34], the advent of neural networks has revolutionized feature extraction [35, 36, 37, 38].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only the shift from handcrafted techniques to neural networks for feature extraction.",
      "processing_time": 34.14451289176941,
      "citing_paper_id": "271874811",
      "cited_paper_id": 232352874
    },
    {
      "context_text": "This tailored approach, while precise, presents a constraint in terms of universality, as it restricts the applicability of models to scenarios with varied or unknown degradations [17, 18, 19].",
      "catation_intent": "limitation",
      "resource_type": "limitation",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a limitation of existing approaches.",
      "processing_time": 30.971673250198364,
      "citing_paper_id": "271874811",
      "cited_paper_id": 212737191
    },
    {
      "context_text": "…10, 11, 7, 12, 13], blind all-in-one image restoration, which aims to utilize a single deep blind restoration model to tackle multiple types of degradation simultaneously without the prior information of the degradation of the input image, has gained more attention recently [1, 2, 30, 24, 25, 26].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general concepts and methods related to all-in-one image restoration.",
      "processing_time": 33.91939663887024,
      "citing_paper_id": "271874811",
      "cited_paper_id": 244714491
    },
    {
      "context_text": "…10, 11, 7, 12, 13], blind all-in-one image restoration, which aims to utilize a single deep blind restoration model to tackle multiple types of degradation simultaneously without the prior information of the degradation of the input image, has gained more attention recently [1, 2, 30, 24, 25, 26].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general concepts and methods related to all-in-one image restoration.",
      "processing_time": 33.91939663887024,
      "citing_paper_id": "271874811",
      "cited_paper_id": 264145824
    },
    {
      "context_text": "As depicted in Table Table 6, we selected three efficacious image restoration models—Transweather [1], AirNet [20], and Restormer [12]—for integration with HAIR.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only models. The cited papers' titles do not provide additional dataset information.",
      "processing_time": 34.34826707839966,
      "citing_paper_id": "271874811",
      "cited_paper_id": 244714491
    },
    {
      "context_text": "As depicted in Table Table 6, we selected three efficacious image restoration models—Transweather [1], AirNet [20], and Restormer [12]—for integration with HAIR.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only models. The cited papers' titles do not provide additional dataset information.",
      "processing_time": 34.34826707839966,
      "citing_paper_id": "271874811",
      "cited_paper_id": 250551851
    },
    {
      "context_text": "We follow the approaches in previous studies [20, 24, 25] for our All-in-One and single-task experiments, using the following datasets.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.3,
      "reasoning": "The citation span mentions using datasets for All-in-One and single-task experiments but does not specify the names of the datasets. The context is too vague to identify specific datasets.",
      "processing_time": 36.67102766036987,
      "citing_paper_id": "271874811",
      "cited_paper_id": 250551851
    },
    {
      "context_text": "The seminal work AirNet [20] achieves blind All-in-One image restoration by employing contrastive learning to extract degradation representations from corrupted images, which are subsequently utilized to restore the clean image.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions a method (AirNet) rather than a specific dataset. No dataset names are mentioned in the context.",
      "processing_time": 34.322662353515625,
      "citing_paper_id": "271874811",
      "cited_paper_id": 250551851
    },
    {
      "context_text": "In Section 2.3, we claim that our Hypernetworks-based method can work better than conditional embedding-based methods like AirNet [20] and PromptIR [24].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only other methods (AirNet, PromptIR) for comparison. No verifiable resources are identified.",
      "processing_time": 35.62394046783447,
      "citing_paper_id": "271874811",
      "cited_paper_id": 250551851
    },
    {
      "context_text": "• We theoretically prove that, for a given small error threshold ϵ in image restoration tasks, HAIR requires fewer parameters compared to mainstream embedding based All-in-One methods like [20, 24, 26].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only theoretical comparisons between methods.",
      "processing_time": 31.733697175979614,
      "citing_paper_id": "271874811",
      "cited_paper_id": 250551851
    },
    {
      "context_text": "Despite previous attempts to integrate Hypernetworks into Image Restoration [43], these have primarily leveraged Task-conditioned hypernetworks.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method or approach. The context focuses on the integration of Hypernetworks into Image Restoration, which is a methodological discussion.",
      "processing_time": 36.9276237487793,
      "citing_paper_id": "271874811",
      "cited_paper_id": 257219872
    },
    {
      "context_text": "With the rise of LLM [31], prompt-based learning [24, 27, 14] has also emerged as a promising direction in computer vision tasks.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general trend in computer vision tasks. No verifiable resources are identified.",
      "processing_time": 34.951653480529785,
      "citing_paper_id": "271874811",
      "cited_paper_id": 257900969
    },
    {
      "context_text": "Given that core operations such as convolution and matrix multiplication follow the distributive law over addition, HAIR is versatile and can be integrated into various architectures like Transformer [46] and Mamba [47].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models and methods. There are no verifiable resources that meet the criteria for inclusion.",
      "processing_time": 35.424429416656494,
      "citing_paper_id": "271874811",
      "cited_paper_id": 265551773
    },
    {
      "context_text": "Despite the notable successes of these methods, they share a common drawback, namely, they rely on a single model with fixed parameters to address a variety of degradations.",
      "catation_intent": "limitation",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, models, or resources. It only discusses a limitation of existing methods.",
      "processing_time": 34.70261001586914,
      "citing_paper_id": "271874811",
      "cited_paper_id": 267320695
    },
    {
      "context_text": "Assume that the first layer of any q ∈ q is bounded ∥ W 1 ∥ 1 ≤ c , for some constant c > 0 .",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, models, or methods. It only discusses a mathematical condition for a layer in a neural network.",
      "processing_time": 35.792306661605835,
      "citing_paper_id": "271874811",
      "cited_paper_id": 267320695
    },
    {
      "context_text": "Specifically, the Classifier is a simple image classification network with Global Average Pooling, designed to produce a Global Information Vector (GIV) that encapsulates the global information from the input image.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only a method (Classifier) and its purpose. There are no verifiable resources or datasets mentioned.",
      "processing_time": 36.02042865753174,
      "citing_paper_id": "271874811",
      "cited_paper_id": 267320695
    },
    {
      "context_text": "Therefore, our proposed method employs a Classifier to obtain a Global Information Vector (GIV), that captures the rich global information of the image, including its degradation characteristics.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only a method for image restoration. No verifiable resources are identified.",
      "processing_time": 34.936192750930786,
      "citing_paper_id": "271874811",
      "cited_paper_id": 267320695
    },
    {
      "context_text": "The primary challenge in this task is feature extraction, which involves converting an image into a vector that encapsulates its most critical information.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only a general challenge in feature extraction. No verifiable resources are identified.",
      "processing_time": 34.93346834182739,
      "citing_paper_id": "271874811",
      "cited_paper_id": 267320695
    },
    {
      "context_text": "This process is aimed at downsizing the spatial resolution while distilling the essential information, as depicted by the following equations: After three iterations, we obtain F , which serves as the input for generating the GIV V g ∈ R 2 C : where GAP denotes Global Average Pooling.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The provided context does not mention any specific datasets, only a methodological process involving equations and operations like Global Average Pooling.",
      "processing_time": 34.92950630187988,
      "citing_paper_id": "271874811",
      "cited_paper_id": 267320695
    },
    {
      "context_text": "2 Related Preliminaries",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general preliminaries. No dataset names are present in the text.",
      "processing_time": 34.9270133972168,
      "citing_paper_id": "271874811",
      "cited_paper_id": 267320695
    },
    {
      "context_text": "2) This GIV is then utilized to generate parameters tailored to the specific degradation present in the image.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only a method for generating parameters for image degradation.",
      "processing_time": 33.18105125427246,
      "citing_paper_id": "271874811",
      "cited_paper_id": 270869793
    },
    {
      "context_text": "For super-resolution, we follow previous practice and train AutoDIR upon DIV2K [2] and Flickr2K [32] training sets, with jointly downsampled images at x8 and x4 scales as inputs.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "DIV2K",
        "Flickr2K"
      ],
      "dataset_descriptions": {
        "DIV2K": "Used for training AutoDIR for super-resolution, focusing on enhancing image resolution at x8 and x4 scales.",
        "Flickr2K": "Used for training AutoDIR for super-resolution, focusing on enhancing image resolution at x8 and x4 scales."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions specific datasets used for training a model for super-resolution, which are relevant to the topic of image restoration.",
      "processing_time": 46.077181339263916,
      "citing_paper_id": "264145824",
      "cited_paper_id": 6540453
    },
    {
      "context_text": "For the denoising task, we leverage the SIDD [1] as real image denoising training dataset and synthetic noise dataset with the DIV2K [2] and Flickr2K [32] for synthetic image denoising.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "SIDD",
        "DIV2K",
        "Flickr2K"
      ],
      "dataset_descriptions": {
        "SIDD": "Used as a real image denoising training dataset, focusing on enhancing the model's ability to handle real-world noise patterns.",
        "DIV2K": "Used as a synthetic image denoising dataset, providing high-quality images for training and evaluating denoising algorithms.",
        "Flickr2K": "Used as a synthetic image denoising dataset, complementing DIV2K with additional diverse images for robust model training."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions specific datasets used for training and evaluation in the denoising task, which are directly relevant to the research topic of all-in-one image restoration.",
      "processing_time": 53.880188941955566,
      "citing_paper_id": "264145824",
      "cited_paper_id": 6540453
    },
    {
      "context_text": "Specifically, we use GoPro [36], LOL [65], RESIDE [28], Rain200L [68], and Raindrop datasets [42] for deblurring, low-light enhancement, dehazing, deraining, and derain-drop tasks respectively.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "GoPro",
        "LOL",
        "RESIDE",
        "Rain200L",
        "Raindrop"
      ],
      "dataset_descriptions": {
        "GoPro": "Used for deblurring dynamic scenes, focusing on improving clarity and sharpness in video frames using a deep multi-scale convolutional neural network.",
        "LOL": "Used for low-light enhancement, specifically addressing the challenge of improving visibility in underexposed images.",
        "RESIDE": "Used for dehazing, focusing on removing atmospheric effects to enhance image clarity and color fidelity.",
        "Rain200L": "Used for deraining, specifically targeting the removal of rain streaks from images to improve visual quality.",
        "Raindrop": "Used for derain-drop tasks, focusing on removing raindrops from images to enhance overall image clarity and detail."
      },
      "confidence_score": 1.0,
      "reasoning": "The context explicitly mentions several datasets used for specific image restoration tasks, which align with the research topic of all-in-one image restoration.",
      "processing_time": 62.75977683067322,
      "citing_paper_id": "264145824",
      "cited_paper_id": 8671030
    },
    {
      "context_text": "Specifically, we use GoPro [36], LOL [65], RESIDE [28], Rain200L [68], and Raindrop datasets [42] for deblurring, low-light enhancement, dehazing, deraining, and derain-drop tasks respectively.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "GoPro",
        "LOL",
        "RESIDE",
        "Rain200L",
        "Raindrop"
      ],
      "dataset_descriptions": {
        "GoPro": "Used for deblurring dynamic scenes, focusing on improving clarity and sharpness in video frames using a deep multi-scale convolutional neural network.",
        "LOL": "Used for low-light enhancement, specifically addressing the challenge of improving visibility in underexposed images.",
        "RESIDE": "Used for dehazing, focusing on removing atmospheric effects to enhance image clarity and color fidelity.",
        "Rain200L": "Used for deraining, specifically targeting the removal of rain streaks from images to improve visual quality.",
        "Raindrop": "Used for derain-drop tasks, focusing on removing raindrops from images to enhance overall image clarity and detail."
      },
      "confidence_score": 1.0,
      "reasoning": "The context explicitly mentions several datasets used for specific image restoration tasks, which align with the research topic of all-in-one image restoration.",
      "processing_time": 62.75977683067322,
      "citing_paper_id": "264145824",
      "cited_paper_id": 15443600
    },
    {
      "context_text": "These include the SIDD validation set [1], CBSD68 [33], DIV2K validation set [2], GoPro test set [36], HIDE [53], LOL [65], SOTS-Outdoor [28], Rain100 [68], and Raindrop datasets [42], each corresponding to their respective tasks.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "SIDD validation set",
        "CBSD68",
        "DIV2K validation set",
        "GoPro test set",
        "HIDE",
        "LOL",
        "SOTS-Outdoor",
        "Rain100",
        "Raindrop datasets"
      ],
      "dataset_descriptions": {
        "SIDD validation set": "Used for validating deblurring algorithms, focusing on dynamic scene deblurring with real-world motion blur.",
        "CBSD68": "Used for evaluating denoising algorithms, focusing on color image denoising with a variety of noise levels.",
        "DIV2K validation set": "Used for super-resolution tasks, focusing on high-quality image upscaling with diverse content.",
        "GoPro test set": "Used for motion deblurring, focusing on real-world video frames with camera shake and motion blur.",
        "HIDE": "Used for haze removal, focusing on outdoor scenes with varying degrees of atmospheric conditions.",
        "LOL": "Used for low-light image enhancement, focusing on improving visibility in dark environments.",
        "SOTS-Outdoor": "Used for dehazing, focusing on outdoor scenes with synthetic and real-world haze.",
        "Rain100": "Used for rain removal, focusing on synthetic rain streaks on images.",
        "Raindrop datasets": "Used for raindrop removal, focusing on realistic raindrop effects on images."
      },
      "confidence_score": 1.0,
      "reasoning": "The context lists several datasets used for various image restoration tasks, which align with the research topic of all-in-one image restoration.",
      "processing_time": 84.30856895446777,
      "citing_paper_id": "264145824",
      "cited_paper_id": 8671030
    },
    {
      "context_text": "These include the SIDD validation set [1], CBSD68 [33], DIV2K validation set [2], GoPro test set [36], HIDE [53], LOL [65], SOTS-Outdoor [28], Rain100 [68], and Raindrop datasets [42], each corresponding to their respective tasks.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "SIDD validation set",
        "CBSD68",
        "DIV2K validation set",
        "GoPro test set",
        "HIDE",
        "LOL",
        "SOTS-Outdoor",
        "Rain100",
        "Raindrop datasets"
      ],
      "dataset_descriptions": {
        "SIDD validation set": "Used for validating deblurring algorithms, focusing on dynamic scene deblurring with real-world motion blur.",
        "CBSD68": "Used for evaluating denoising algorithms, focusing on color image denoising with a variety of noise levels.",
        "DIV2K validation set": "Used for super-resolution tasks, focusing on high-quality image upscaling with diverse content.",
        "GoPro test set": "Used for motion deblurring, focusing on real-world video frames with camera shake and motion blur.",
        "HIDE": "Used for haze removal, focusing on outdoor scenes with varying degrees of atmospheric conditions.",
        "LOL": "Used for low-light image enhancement, focusing on improving visibility in dark environments.",
        "SOTS-Outdoor": "Used for dehazing, focusing on outdoor scenes with synthetic and real-world haze.",
        "Rain100": "Used for rain removal, focusing on synthetic rain streaks on images.",
        "Raindrop datasets": "Used for raindrop removal, focusing on realistic raindrop effects on images."
      },
      "confidence_score": 1.0,
      "reasoning": "The context lists several datasets used for various image restoration tasks, which align with the research topic of all-in-one image restoration.",
      "processing_time": 84.30856895446777,
      "citing_paper_id": "264145824",
      "cited_paper_id": 15443600
    },
    {
      "context_text": "These include the SIDD validation set [1], CBSD68 [33], DIV2K validation set [2], GoPro test set [36], HIDE [53], LOL [65], SOTS-Outdoor [28], Rain100 [68], and Raindrop datasets [42], each corresponding to their respective tasks.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "SIDD validation set",
        "CBSD68",
        "DIV2K validation set",
        "GoPro test set",
        "HIDE",
        "LOL",
        "SOTS-Outdoor",
        "Rain100",
        "Raindrop datasets"
      ],
      "dataset_descriptions": {
        "SIDD validation set": "Used for validating deblurring algorithms, focusing on dynamic scene deblurring with real-world motion blur.",
        "CBSD68": "Used for evaluating denoising algorithms, focusing on color image denoising with a variety of noise levels.",
        "DIV2K validation set": "Used for super-resolution tasks, focusing on high-quality image upscaling with diverse content.",
        "GoPro test set": "Used for motion deblurring, focusing on real-world video frames with camera shake and motion blur.",
        "HIDE": "Used for haze removal, focusing on outdoor scenes with varying degrees of atmospheric conditions.",
        "LOL": "Used for low-light image enhancement, focusing on improving visibility in dark environments.",
        "SOTS-Outdoor": "Used for dehazing, focusing on outdoor scenes with synthetic and real-world haze.",
        "Rain100": "Used for rain removal, focusing on synthetic rain streaks on images.",
        "Raindrop datasets": "Used for raindrop removal, focusing on realistic raindrop effects on images."
      },
      "confidence_score": 1.0,
      "reasoning": "The context lists several datasets used for various image restoration tasks, which align with the research topic of all-in-one image restoration.",
      "processing_time": 84.30856895446777,
      "citing_paper_id": "264145824",
      "cited_paper_id": 201624746
    },
    {
      "context_text": "This is achieved by utilizing recent advancements in vision-language models [20,30,45,55] and text-to-image diffusion models [56, 66, 69, 70], which have shown significant progress in text-to-image generation and editing but have primarily focused on semantic edits and not image quality tasks.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only models and methods. The cited papers' titles do not introduce any datasets either.",
      "processing_time": 35.9904408454895,
      "citing_paper_id": "264145824",
      "cited_paper_id": 8858625
    },
    {
      "context_text": "This is achieved by utilizing recent advancements in vision-language models [20,30,45,55] and text-to-image diffusion models [56, 66, 69, 70], which have shown significant progress in text-to-image generation and editing but have primarily focused on semantic edits and not image quality tasks.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only models and methods. The cited papers' titles do not introduce any datasets either.",
      "processing_time": 35.9904408454895,
      "citing_paper_id": "264145824",
      "cited_paper_id": 201103729
    },
    {
      "context_text": "This is achieved by utilizing recent advancements in vision-language models [20,30,45,55] and text-to-image diffusion models [56, 66, 69, 70], which have shown significant progress in text-to-image generation and editing but have primarily focused on semantic edits and not image quality tasks.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only models and methods. The cited papers' titles do not introduce any datasets either.",
      "processing_time": 35.9904408454895,
      "citing_paper_id": "264145824",
      "cited_paper_id": 231591445
    },
    {
      "context_text": "This is achieved by utilizing recent advancements in vision-language models [20,30,45,55] and text-to-image diffusion models [56, 66, 69, 70], which have shown significant progress in text-to-image generation and editing but have primarily focused on semantic edits and not image quality tasks.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only models and methods. The cited papers' titles do not introduce any datasets either.",
      "processing_time": 35.9904408454895,
      "citing_paper_id": "264145824",
      "cited_paper_id": 246411402
    },
    {
      "context_text": "This is achieved by utilizing recent advancements in vision-language models [20,30,45,55] and text-to-image diffusion models [56, 66, 69, 70], which have shown significant progress in text-to-image generation and editing but have primarily focused on semantic edits and not image quality tasks.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only models and methods. The cited papers' titles do not introduce any datasets either.",
      "processing_time": 35.9904408454895,
      "citing_paper_id": "264145824",
      "cited_paper_id": 247778989
    },
    {
      "context_text": "Early works [56,66,69,70,78] on text-to-image synthesis and editing primarily relied on generative adversarial networks (GANs) [12].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only generative adversarial networks (GANs) and their application in text-to-image synthesis and editing.",
      "processing_time": 36.616626501083374,
      "citing_paper_id": "264145824",
      "cited_paper_id": 8858625
    },
    {
      "context_text": "Early works [56,66,69,70,78] on text-to-image synthesis and editing primarily relied on generative adversarial networks (GANs) [12].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only generative adversarial networks (GANs) and their application in text-to-image synthesis and editing.",
      "processing_time": 36.616626501083374,
      "citing_paper_id": "264145824",
      "cited_paper_id": 91183909
    },
    {
      "context_text": "Early works [56,66,69,70,78] on text-to-image synthesis and editing primarily relied on generative adversarial networks (GANs) [12].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only generative adversarial networks (GANs) and their application in text-to-image synthesis and editing.",
      "processing_time": 36.616626501083374,
      "citing_paper_id": "264145824",
      "cited_paper_id": 247778989
    },
    {
      "context_text": "Moreover, to support conditional generation, LDM augments the UNet backbone [49] with a cross-attention mechanism [60].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models and methods. The context focuses on the augmentation of the UNet backbone with a cross-attention mechanism.",
      "processing_time": 36.87610721588135,
      "citing_paper_id": "264145824",
      "cited_paper_id": 13756489
    },
    {
      "context_text": "This finding further validates the effectiveness of our BIQA model.",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.2,
      "reasoning": "The citation span does not mention any specific datasets, only a validation of a BIQA model. The cited papers' titles suggest potential datasets, but they are not explicitly mentioned in the citation context.",
      "processing_time": 38.05917000770569,
      "citing_paper_id": "264145824",
      "cited_paper_id": 13911460
    },
    {
      "context_text": "This finding further validates the effectiveness of our BIQA model.",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.2,
      "reasoning": "The citation span does not mention any specific datasets, only a validation of a BIQA model. The cited papers' titles suggest potential datasets, but they are not explicitly mentioned in the citation context.",
      "processing_time": 38.05917000770569,
      "citing_paper_id": "264145824",
      "cited_paper_id": 220713808
    },
    {
      "context_text": "This finding further validates the effectiveness of our BIQA model.",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.2,
      "reasoning": "The citation span does not mention any specific datasets, only a validation of a BIQA model. The cited papers' titles suggest potential datasets, but they are not explicitly mentioned in the citation context.",
      "processing_time": 38.05917000770569,
      "citing_paper_id": "264145824",
      "cited_paper_id": 267199774
    },
    {
      "context_text": "While these single-task approaches yield promising results within their respective tasks, they encounter difficulties when applied to complex real-world situations due to: (i) Real-world images often contain multiple unknown degradations or require multiple enhancements, which cannot be addressed…",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general challenges in image restoration. No verifiable resources are identified.",
      "processing_time": 35.5609929561615,
      "citing_paper_id": "264145824",
      "cited_paper_id": 20282961
    },
    {
      "context_text": "While these single-task approaches yield promising results within their respective tasks, they encounter difficulties when applied to complex real-world situations due to: (i) Real-world images often contain multiple unknown degradations or require multiple enhancements, which cannot be addressed…",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general challenges in image restoration. No verifiable resources are identified.",
      "processing_time": 35.5609929561615,
      "citing_paper_id": "264145824",
      "cited_paper_id": 259137590
    },
    {
      "context_text": "In addition to automatic degradation detection, we also provide a user-friendly interface that allows users to manually adjust the type of image recovery based on their specific requirements.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only a user-friendly interface for image recovery. No verifiable resources are identified.",
      "processing_time": 35.969504833221436,
      "citing_paper_id": "264145824",
      "cited_paper_id": 69993921
    },
    {
      "context_text": "Finally, a Structure Correction Module (SCM) is proposed",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The provided context does not mention any specific datasets, only a module proposal. The cited papers' titles do not provide additional context about datasets.",
      "processing_time": 36.60143327713013,
      "citing_paper_id": "264145824",
      "cited_paper_id": 69993921
    },
    {
      "context_text": "Finally, a Structure Correction Module (SCM) is proposed",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The provided context does not mention any specific datasets, only a module proposal. The cited papers' titles do not provide additional context about datasets.",
      "processing_time": 36.60143327713013,
      "citing_paper_id": "264145824",
      "cited_paper_id": 227239228
    },
    {
      "context_text": "Finally, a Structure Correction Module (SCM) is proposed",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The provided context does not mention any specific datasets, only a module proposal. The cited papers' titles do not provide additional context about datasets.",
      "processing_time": 36.60143327713013,
      "citing_paper_id": "264145824",
      "cited_paper_id": 237266491
    },
    {
      "context_text": "The utilization of generative priors [3,5,17,34] has been a common technique in image restoration tasks.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a technique used in image restoration tasks.",
      "processing_time": 34.88748264312744,
      "citing_paper_id": "264145824",
      "cited_paper_id": 196834421
    },
    {
      "context_text": "Given the diverse nature of these subtasks, which necessitate the estimation of various image operations, prior methods [7, 19, 21, 29, 35, 41, 43, 44, 47, 54, 62, 74, 75] often train individual single-task models for each specific task to determine the currently a r X i v : 2310 .",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to prior methods and models. The context is about the diversity of subtasks in image restoration and the need for single-task models.",
      "processing_time": 38.38067317008972,
      "citing_paper_id": "264145824",
      "cited_paper_id": 208138077
    },
    {
      "context_text": "Given the diverse nature of these subtasks, which necessitate the estimation of various image operations, prior methods [7, 19, 21, 29, 35, 41, 43, 44, 47, 54, 62, 74, 75] often train individual single-task models for each specific task to determine the currently a r X i v : 2310 .",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to prior methods and models. The context is about the diversity of subtasks in image restoration and the need for single-task models.",
      "processing_time": 38.38067317008972,
      "citing_paper_id": "264145824",
      "cited_paper_id": 248069347
    },
    {
      "context_text": "Given the diverse nature of these subtasks, which necessitate the estimation of various image operations, prior methods [7, 19, 21, 29, 35, 41, 43, 44, 47, 54, 62, 74, 75] often train individual single-task models for each specific task to determine the currently a r X i v : 2310 .",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to prior methods and models. The context is about the diversity of subtasks in image restoration and the need for single-task models.",
      "processing_time": 38.38067317008972,
      "citing_paper_id": "264145824",
      "cited_paper_id": 250615985
    },
    {
      "context_text": "Given the diverse nature of these subtasks, which necessitate the estimation of various image operations, prior methods [7, 19, 21, 29, 35, 41, 43, 44, 47, 54, 62, 74, 75] often train individual single-task models for each specific task to determine the currently a r X i v : 2310 .",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to prior methods and models. The context is about the diversity of subtasks in image restoration and the need for single-task models.",
      "processing_time": 38.38067317008972,
      "citing_paper_id": "264145824",
      "cited_paper_id": 257804691
    },
    {
      "context_text": "Given the diverse nature of these subtasks, which necessitate the estimation of various image operations, prior methods [7, 19, 21, 29, 35, 41, 43, 44, 47, 54, 62, 74, 75] often train individual single-task models for each specific task to determine the currently a r X i v : 2310 .",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to prior methods and models. The context is about the diversity of subtasks in image restoration and the need for single-task models.",
      "processing_time": 38.38067317008972,
      "citing_paper_id": "264145824",
      "cited_paper_id": 258615282
    },
    {
      "context_text": "Given the diverse nature of these subtasks, which necessitate the estimation of various image operations, prior methods [7, 19, 21, 29, 35, 41, 43, 44, 47, 54, 62, 74, 75] often train individual single-task models for each specific task to determine the currently a r X i v : 2310 .",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to prior methods and models. The context is about the diversity of subtasks in image restoration and the need for single-task models.",
      "processing_time": 38.38067317008972,
      "citing_paper_id": "264145824",
      "cited_paper_id": 261244427
    },
    {
      "context_text": "Moreover, we can leverage generative models like Variational Autoencoders (VAEs) [25,58], Generative Adversarial Networks (GANs) [12], and Diffusion Models [16, 48] to capture the complex empirical distributions of natural images [52].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only generative models and their ability to capture the distribution of natural images. No verifiable datasets are named.",
      "processing_time": 37.499470710754395,
      "citing_paper_id": "264145824",
      "cited_paper_id": 211146177
    },
    {
      "context_text": "Moreover, we can leverage generative models like Variational Autoencoders (VAEs) [25,58], Generative Adversarial Networks (GANs) [12], and Diffusion Models [16, 48] to capture the complex empirical distributions of natural images [52].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only generative models and their ability to capture the distribution of natural images. No verifiable datasets are named.",
      "processing_time": 37.499470710754395,
      "citing_paper_id": "264145824",
      "cited_paper_id": 219955663
    },
    {
      "context_text": "Moreover, we can leverage generative models like Variational Autoencoders (VAEs) [25,58], Generative Adversarial Networks (GANs) [12], and Diffusion Models [16, 48] to capture the complex empirical distributions of natural images [52].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only generative models and their ability to capture the distribution of natural images. No verifiable datasets are named.",
      "processing_time": 37.499470710754395,
      "citing_paper_id": "264145824",
      "cited_paper_id": 233241040
    },
    {
      "context_text": "Preliminaries The Latent Diffusion Model [48] is a variant of Denoising Diffusion Probabilistic Models [16], that conducts forward and reverse processes in the latent space of a pre-trained Variational Autoencoder (VAE) [25] with an encoder E ldm and decoder D .",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only models and methods. The cited papers are used to explain the background of the Latent Diffusion Model, which is not a dataset.",
      "processing_time": 38.524166107177734,
      "citing_paper_id": "264145824",
      "cited_paper_id": 211146177
    },
    {
      "context_text": "Preliminaries The Latent Diffusion Model [48] is a variant of Denoising Diffusion Probabilistic Models [16], that conducts forward and reverse processes in the latent space of a pre-trained Variational Autoencoder (VAE) [25] with an encoder E ldm and decoder D .",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only models and methods. The cited papers are used to explain the background of the Latent Diffusion Model, which is not a dataset.",
      "processing_time": 38.524166107177734,
      "citing_paper_id": "264145824",
      "cited_paper_id": 219955663
    },
    {
      "context_text": "Step 1: “A photo needs undereposure artifact reduction” Step 2: “A photo needs blur artifact reduction” [76] and unseen Enhancing Under-water Visual Perception (EUVP) dataset [18].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Enhancing Under-water Visual Perception (EUVP)"
      ],
      "dataset_descriptions": {
        "Enhancing Under-water Visual Perception (EUVP)": "Used to evaluate underexposure and blur artifact reduction techniques, focusing on enhancing visual perception in underwater images."
      },
      "confidence_score": 0.9,
      "reasoning": "The context mentions the EUVP dataset, which is relevant to image restoration, particularly for underexposure and blur artifact reduction.",
      "processing_time": 42.942984104156494,
      "citing_paper_id": "264145824",
      "cited_paper_id": 212647851
    },
    {
      "context_text": "Recently, developments in diffusion models [16,48] and language-vision models [45] have led to significant progress in image synthesis and editing [6, 37, 46, 51].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only models and methods. The cited papers' titles do not introduce any datasets either.",
      "processing_time": 37.00170350074768,
      "citing_paper_id": "264145824",
      "cited_paper_id": 219955663
    },
    {
      "context_text": "Recently, developments in diffusion models [16,48] and language-vision models [45] have led to significant progress in image synthesis and editing [6, 37, 46, 51].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only models and methods. The cited papers' titles do not introduce any datasets either.",
      "processing_time": 37.00170350074768,
      "citing_paper_id": "264145824",
      "cited_paper_id": 231591445
    },
    {
      "context_text": "Recently, developments in diffusion models [16,48] and language-vision models [45] have led to significant progress in image synthesis and editing [6, 37, 46, 51].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only models and methods. The cited papers' titles do not introduce any datasets either.",
      "processing_time": 37.00170350074768,
      "citing_paper_id": "264145824",
      "cited_paper_id": 245335086
    },
    {
      "context_text": "Recently, developments in diffusion models [16,48] and language-vision models [45] have led to significant progress in image synthesis and editing [6, 37, 46, 51].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only models and methods. The cited papers' titles do not introduce any datasets either.",
      "processing_time": 37.00170350074768,
      "citing_paper_id": "264145824",
      "cited_paper_id": 248097655
    },
    {
      "context_text": "Recently, developments in diffusion models [16,48] and language-vision models [45] have led to significant progress in image synthesis and editing [6, 37, 46, 51].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only models and methods. The cited papers' titles do not introduce any datasets either.",
      "processing_time": 37.00170350074768,
      "citing_paper_id": "264145824",
      "cited_paper_id": 256416326
    },
    {
      "context_text": "Recent literature has proposed some unified approaches to address the multi-task image restoration problem, which fall into two categories: The first category of methods [22,22,39,64] leverages pre-trained generative models, including GANs [12] and Diffusion Models [16, 48], as priors.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and models. The context focuses on the use of pre-trained generative models for multi-task image restoration.",
      "processing_time": 37.81387686729431,
      "citing_paper_id": "264145824",
      "cited_paper_id": 219955663
    },
    {
      "context_text": "Specifically, our proposed pipeline comprises three stages as shown in Figure 1: (i) We fine-tune a pre-trained language-vision model CLIP [45] as a blind image quality assessment (BIQA) module to detect basic image degradation types and generate text embeddings for each image quality issue.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a pre-trained model (CLIP) which is excluded according to the rules.",
      "processing_time": 36.99545931816101,
      "citing_paper_id": "264145824",
      "cited_paper_id": 231591445
    },
    {
      "context_text": "We adopt a pre-trained CLIP model [45], which consists of an image encoder E I and a text encoder E T .",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions a pre-trained CLIP model but does not refer to any specific dataset. The context is about using the model, not a dataset.",
      "processing_time": 37.48125433921814,
      "citing_paper_id": "264145824",
      "cited_paper_id": 231591445
    },
    {
      "context_text": "These models have already demonstrated significant advancements in restoring image details across various image restoration tasks [11,14,38,52,62,79].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only advancements in image restoration tasks. No dataset names are provided in the context or titles.",
      "processing_time": 37.8046498298645,
      "citing_paper_id": "264145824",
      "cited_paper_id": 233241040
    },
    {
      "context_text": "These models have already demonstrated significant advancements in restoring image details across various image restoration tasks [11,14,38,52,62,79].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only advancements in image restoration tasks. No dataset names are provided in the context or titles.",
      "processing_time": 37.8046498298645,
      "citing_paper_id": "264145824",
      "cited_paper_id": 254535902
    },
    {
      "context_text": "These models have already demonstrated significant advancements in restoring image details across various image restoration tasks [11,14,38,52,62,79].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only advancements in image restoration tasks. No dataset names are provided in the context or titles.",
      "processing_time": 37.8046498298645,
      "citing_paper_id": "264145824",
      "cited_paper_id": 258615282
    },
    {
      "context_text": "Moreover, unlike generative-based image restoration methods [50, 52, 63], they lack the capability to hallucinate missing or deteriorated details in the images.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only generative-based image restoration methods. No dataset names are present in the text.",
      "processing_time": 38.35398745536804,
      "citing_paper_id": "264145824",
      "cited_paper_id": 233241040
    },
    {
      "context_text": "Moreover, unlike generative-based image restoration methods [50, 52, 63], they lack the capability to hallucinate missing or deteriorated details in the images.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only generative-based image restoration methods. No dataset names are present in the text.",
      "processing_time": 38.35398745536804,
      "citing_paper_id": "264145824",
      "cited_paper_id": 243938678
    },
    {
      "context_text": "For the super-resolution and debluring task, we adopt the widely used blind perceptual metrics including CLIP-IQA [61] and MUSIQ [24] to evaluate the quality of generated images.",
      "catation_intent": "reusable resource",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions using MUSIQ for evaluating image quality in super-resolution and deblurring tasks, but MUSIQ is a method, not a dataset.",
      "processing_time": 39.53761434555054,
      "citing_paper_id": "264145824",
      "cited_paper_id": 237048383
    },
    {
      "context_text": "Previous approaches to unified image restoration can be categorized into two main groups: unsupervised generative prior-based methods [10,22,39,64,67] and end-to-end learning-based methods [9,26,31,40,59,71].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only categories of methods. No dataset names are present in the text.",
      "processing_time": 38.893282651901245,
      "citing_paper_id": "264145824",
      "cited_paper_id": 244714491
    },
    {
      "context_text": "Previous approaches to unified image restoration can be categorized into two main groups: unsupervised generative prior-based methods [10,22,39,64,67] and end-to-end learning-based methods [9,26,31,40,59,71].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only categories of methods. No dataset names are present in the text.",
      "processing_time": 38.893282651901245,
      "citing_paper_id": "264145824",
      "cited_paper_id": 257921922
    },
    {
      "context_text": "Previous approaches to unified image restoration can be categorized into two main groups: unsupervised generative prior-based methods [10,22,39,64,67] and end-to-end learning-based methods [9,26,31,40,59,71].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only categories of methods. No dataset names are present in the text.",
      "processing_time": 38.893282651901245,
      "citing_paper_id": "264145824",
      "cited_paper_id": 260085391
    },
    {
      "context_text": "Previous approaches to unified image restoration can be categorized into two main groups: unsupervised generative prior-based methods [10,22,39,64,67] and end-to-end learning-based methods [9,26,31,40,59,71].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only categories of methods. No dataset names are present in the text.",
      "processing_time": 38.893282651901245,
      "citing_paper_id": "264145824",
      "cited_paper_id": 260107992
    },
    {
      "context_text": "For example, Vala-narasu et al. [59] leverage a transformer encoder to capture hierarchical features of haze, while Li et al. [27] employ a degradation classifier trained using contrastive learning.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and models. The context focuses on the use of a transformer encoder and a degradation classifier, which are not datasets.",
      "processing_time": 40.33765482902527,
      "citing_paper_id": "264145824",
      "cited_paper_id": 244714491
    },
    {
      "context_text": "The other category is end-to-end learning-based meth-ods, which typically utilize image embeddings extracted by an auxiliary degradation predictor to guide the image restoration model [26, 31, 40, 59, 71].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only methods and approaches. The cited papers' titles do not provide additional dataset information.",
      "processing_time": 39.82710242271423,
      "citing_paper_id": "264145824",
      "cited_paper_id": 244714491
    },
    {
      "context_text": "The other category is end-to-end learning-based meth-ods, which typically utilize image embeddings extracted by an auxiliary degradation predictor to guide the image restoration model [26, 31, 40, 59, 71].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only methods and approaches. The cited papers' titles do not provide additional dataset information.",
      "processing_time": 39.82710242271423,
      "citing_paper_id": "264145824",
      "cited_paper_id": 260085391
    },
    {
      "context_text": "The other category is end-to-end learning-based meth-ods, which typically utilize image embeddings extracted by an auxiliary degradation predictor to guide the image restoration model [26, 31, 40, 59, 71].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only methods and approaches. The cited papers' titles do not provide additional dataset information.",
      "processing_time": 39.82710242271423,
      "citing_paper_id": "264145824",
      "cited_paper_id": 260107992
    },
    {
      "context_text": "Specifically, we conduct comparisons of our re-sults with two state-of-the-art methods: the CNN-based single-task image restoration pipeline NAFNet [8] and the powerful generative backbone Stable Diffusion [48].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions methods (NAFNet, Stable Diffusion) but does not reference any specific datasets. The context focuses on comparing methods rather than using datasets.",
      "processing_time": 40.5076687335968,
      "citing_paper_id": "264145824",
      "cited_paper_id": 248085491
    },
    {
      "context_text": "As demonstrated, AutoDIR achieves competitive performance across all seven tasks and significantly outperforms both NAFNet [8] and Stable Diffusion [48], as indicated by a substantial margin in the average score.",
      "catation_intent": "findings",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only comparisons with other methods. No dataset names are present in the citation span.",
      "processing_time": 39.8208703994751,
      "citing_paper_id": "264145824",
      "cited_paper_id": 248085491
    },
    {
      "context_text": "The emergence of diffusion-based methods has offered new avenues for text-to-image editing, which can be broadly categorized into data-driven [4] and no-extra-data-required approaches [15, 23, 73].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only categories of approaches for text-to-image editing.",
      "processing_time": 39.524030447006226,
      "citing_paper_id": "264145824",
      "cited_paper_id": 251252882
    },
    {
      "context_text": "The emergence of diffusion-based methods has offered new avenues for text-to-image editing, which can be broadly categorized into data-driven [4] and no-extra-data-required approaches [15, 23, 73].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only categories of approaches for text-to-image editing.",
      "processing_time": 39.524030447006226,
      "citing_paper_id": "264145824",
      "cited_paper_id": 253581213
    },
    {
      "context_text": "Hertz et al. [15] improves editing efficiency by directly manipulating cross-attention maps without the need of per-image finetuning.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for image editing. No dataset names are present in the citation span.",
      "processing_time": 40.154279470443726,
      "citing_paper_id": "264145824",
      "cited_paper_id": 251252882
    },
    {
      "context_text": "In the data-driven category, Brooks et al. [4] employs a stable diffusion model fine-tuned with a large dataset of prompt and image pairs.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.2,
      "reasoning": "The context mentions a 'large dataset of prompt and image pairs' but does not provide a specific name. The dataset is described generically, so it does not meet the criteria for inclusion.",
      "processing_time": 41.88640332221985,
      "citing_paper_id": "264145824",
      "cited_paper_id": 253581213
    },
    {
      "context_text": "To address non-linear image restoration, Fei et al. [10] estimate a degradation function using gradients during the reverse process, but their applicability is limited when dealing with complex degradations like motion blur.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for estimating a degradation function. No verifiable resources are identified.",
      "processing_time": 40.80359506607056,
      "citing_paper_id": "264145824",
      "cited_paper_id": 257921922
    },
    {
      "context_text": "Unlike the above methods, our AutoDIR supports real image enhancement without per-image fine-tuning.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, models, or resources. It only describes a feature of the method being developed.",
      "processing_time": 41.37650942802429,
      "citing_paper_id": "264145824",
      "cited_paper_id": 258762187
    },
    {
      "context_text": "However, it requires interior maps in the reversing process and is thus not applicable to real image editing.",
      "catation_intent": "limitation",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a limitation of a method.",
      "processing_time": 40.31504416465759,
      "citing_paper_id": "264145824",
      "cited_paper_id": 258762187
    },
    {
      "context_text": "The second category comprises end-to-end discriminative methods [26,40,71].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods. The context is about end-to-end discriminative methods for image restoration, which aligns with the research topic but does not specify any datasets.",
      "processing_time": 44.04072093963623,
      "citing_paper_id": "264145824",
      "cited_paper_id": 260085391
    },
    {
      "context_text": "The second category comprises end-to-end discriminative methods [26,40,71].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods. The context is about end-to-end discriminative methods for image restoration, which aligns with the research topic but does not specify any datasets.",
      "processing_time": 44.04072093963623,
      "citing_paper_id": "264145824",
      "cited_paper_id": 260107992
    },
    {
      "context_text": "Park et al. [40] design a degradation classifier for multiple types of degradation to select appropriate Adaptive Discriminant filters, altering network parameters based on specific degradations.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions a degradation classifier and Adaptive Discriminant filters but does not specify a dataset. The context focuses on methodological aspects rather than data sources.",
      "processing_time": 41.49250912666321,
      "citing_paper_id": "264145824",
      "cited_paper_id": 260107992
    },
    {
      "context_text": "To demonstrate this, we conduct experiments on an unseen under-display-camera dataset [77] and unseen Enhancing Underwater Visual Perception (EUVP) dataset [18] containing various unknown artifacts.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "unseen under-display-camera dataset",
        "Enhancing Underwater Visual Perception (EUVP) dataset"
      ],
      "dataset_descriptions": {
        "unseen under-display-camera dataset": "Used to test image restoration techniques on under-display-camera images, focusing on artifact reduction and image quality enhancement.",
        "Enhancing Underwater Visual Perception (EUVP) dataset": "Used to evaluate image restoration methods on underwater images, addressing issues like color correction and clarity improvement."
      },
      "confidence_score": 0.9,
      "reasoning": "The context mentions two specific datasets used for experiments: an 'unseen under-display-camera dataset' and the 'Enhancing Underwater Visual Perception (EUVP) dataset'. Both are relevant to image restoration.",
      "processing_time": 58.94893836975098,
      "citing_paper_id": "264145824",
      "cited_paper_id": 261697392
    },
    {
      "context_text": "Recently, deep learning-based approaches have demonstrated remarkable performance with fast computations, but a majority of them have been focusing on known single degradation such as denoising [2, 4, 20, 25, 65, 67, 68], deraining [5, 16, 30, 51, 52, 59, 60], deblurring [19, 23, 24, 36, 38, 43, 46, 55], desnowing [11, 31, 33, 53] and dehazing [9, 18, 27, 32, 40, 45, 56, 63, 66].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various image restoration tasks. No dataset names are provided.",
      "processing_time": 41.490931034088135,
      "citing_paper_id": "260107992",
      "cited_paper_id": 4054776
    },
    {
      "context_text": "Recently, deep learning-based approaches have demonstrated remarkable performance with fast computations, but a majority of them have been focusing on known single degradation such as denoising [2, 4, 20, 25, 65, 67, 68], deraining [5, 16, 30, 51, 52, 59, 60], deblurring [19, 23, 24, 36, 38, 43, 46, 55], desnowing [11, 31, 33, 53] and dehazing [9, 18, 27, 32, 40, 45, 56, 63, 66].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various image restoration tasks. No dataset names are provided.",
      "processing_time": 41.490931034088135,
      "citing_paper_id": "260107992",
      "cited_paper_id": 5872410
    },
    {
      "context_text": "Recently, deep learning-based approaches have demonstrated remarkable performance with fast computations, but a majority of them have been focusing on known single degradation such as denoising [2, 4, 20, 25, 65, 67, 68], deraining [5, 16, 30, 51, 52, 59, 60], deblurring [19, 23, 24, 36, 38, 43, 46, 55], desnowing [11, 31, 33, 53] and dehazing [9, 18, 27, 32, 40, 45, 56, 63, 66].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various image restoration tasks. No dataset names are provided.",
      "processing_time": 41.490931034088135,
      "citing_paper_id": "260107992",
      "cited_paper_id": 208138077
    },
    {
      "context_text": "Recently, deep learning-based approaches have demonstrated remarkable performance with fast computations, but a majority of them have been focusing on known single degradation such as denoising [2, 4, 20, 25, 65, 67, 68], deraining [5, 16, 30, 51, 52, 59, 60], deblurring [19, 23, 24, 36, 38, 43, 46, 55], desnowing [11, 31, 33, 53] and dehazing [9, 18, 27, 32, 40, 45, 56, 63, 66].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various image restoration tasks. No dataset names are provided.",
      "processing_time": 41.490931034088135,
      "citing_paper_id": "260107992",
      "cited_paper_id": 208138230
    },
    {
      "context_text": "Recently, deep learning-based approaches have demonstrated remarkable performance with fast computations, but a majority of them have been focusing on known single degradation such as denoising [2, 4, 20, 25, 65, 67, 68], deraining [5, 16, 30, 51, 52, 59, 60], deblurring [19, 23, 24, 36, 38, 43, 46, 55], desnowing [11, 31, 33, 53] and dehazing [9, 18, 27, 32, 40, 45, 56, 63, 66].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various image restoration tasks. No dataset names are provided.",
      "processing_time": 41.490931034088135,
      "citing_paper_id": "260107992",
      "cited_paper_id": 209376714
    },
    {
      "context_text": "Recently, deep learning-based approaches have demonstrated remarkable performance with fast computations, but a majority of them have been focusing on known single degradation such as denoising [2, 4, 20, 25, 65, 67, 68], deraining [5, 16, 30, 51, 52, 59, 60], deblurring [19, 23, 24, 36, 38, 43, 46, 55], desnowing [11, 31, 33, 53] and dehazing [9, 18, 27, 32, 40, 45, 56, 63, 66].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various image restoration tasks. No dataset names are provided.",
      "processing_time": 41.490931034088135,
      "citing_paper_id": "260107992",
      "cited_paper_id": 218487055
    },
    {
      "context_text": "Recently, deep learning-based approaches have demonstrated remarkable performance with fast computations, but a majority of them have been focusing on known single degradation such as denoising [2, 4, 20, 25, 65, 67, 68], deraining [5, 16, 30, 51, 52, 59, 60], deblurring [19, 23, 24, 36, 38, 43, 46, 55], desnowing [11, 31, 33, 53] and dehazing [9, 18, 27, 32, 40, 45, 56, 63, 66].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various image restoration tasks. No dataset names are provided.",
      "processing_time": 41.490931034088135,
      "citing_paper_id": "260107992",
      "cited_paper_id": 231419143
    },
    {
      "context_text": "Recently, deep learning-based approaches have demonstrated remarkable performance with fast computations, but a majority of them have been focusing on known single degradation such as denoising [2, 4, 20, 25, 65, 67, 68], deraining [5, 16, 30, 51, 52, 59, 60], deblurring [19, 23, 24, 36, 38, 43, 46, 55], desnowing [11, 31, 33, 53] and dehazing [9, 18, 27, 32, 40, 45, 56, 63, 66].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various image restoration tasks. No dataset names are provided.",
      "processing_time": 41.490931034088135,
      "citing_paper_id": "260107992",
      "cited_paper_id": 231728368
    },
    {
      "context_text": "Recently, deep learning-based approaches have demonstrated remarkable performance with fast computations, but a majority of them have been focusing on known single degradation such as denoising [2, 4, 20, 25, 65, 67, 68], deraining [5, 16, 30, 51, 52, 59, 60], deblurring [19, 23, 24, 36, 38, 43, 46, 55], desnowing [11, 31, 33, 53] and dehazing [9, 18, 27, 32, 40, 45, 56, 63, 66].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various image restoration tasks. No dataset names are provided.",
      "processing_time": 41.490931034088135,
      "citing_paper_id": "260107992",
      "cited_paper_id": 233296013
    },
    {
      "context_text": "Recently, deep learning-based approaches have demonstrated remarkable performance with fast computations, but a majority of them have been focusing on known single degradation such as denoising [2, 4, 20, 25, 65, 67, 68], deraining [5, 16, 30, 51, 52, 59, 60], deblurring [19, 23, 24, 36, 38, 43, 46, 55], desnowing [11, 31, 33, 53] and dehazing [9, 18, 27, 32, 40, 45, 56, 63, 66].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various image restoration tasks. No dataset names are provided.",
      "processing_time": 41.490931034088135,
      "citing_paper_id": "260107992",
      "cited_paper_id": 235693182
    },
    {
      "context_text": "Recently, deep learning-based approaches have demonstrated remarkable performance with fast computations, but a majority of them have been focusing on known single degradation such as denoising [2, 4, 20, 25, 65, 67, 68], deraining [5, 16, 30, 51, 52, 59, 60], deblurring [19, 23, 24, 36, 38, 43, 46, 55], desnowing [11, 31, 33, 53] and dehazing [9, 18, 27, 32, 40, 45, 56, 63, 66].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various image restoration tasks. No dataset names are provided.",
      "processing_time": 41.490931034088135,
      "citing_paper_id": "260107992",
      "cited_paper_id": 244908890
    },
    {
      "context_text": "Recently, deep learning-based approaches have demonstrated remarkable performance with fast computations, but a majority of them have been focusing on known single degradation such as denoising [2, 4, 20, 25, 65, 67, 68], deraining [5, 16, 30, 51, 52, 59, 60], deblurring [19, 23, 24, 36, 38, 43, 46, 55], desnowing [11, 31, 33, 53] and dehazing [9, 18, 27, 32, 40, 45, 56, 63, 66].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various image restoration tasks. No dataset names are provided.",
      "processing_time": 41.490931034088135,
      "citing_paper_id": "260107992",
      "cited_paper_id": 246904639
    },
    {
      "context_text": "Xie et al. [57] proposed FAIG that can identify discriminative filters of specific degradation in blind super-resolution (SR) by computing integrated gradient (IG) [47, 48] between the baseline and target models.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and models. The context focuses on the use of FAIG and integrated gradient for identifying filters in blind super-resolution.",
      "processing_time": 43.59826397895813,
      "citing_paper_id": "260107992",
      "cited_paper_id": 11880723
    },
    {
      "context_text": "Xie et al. [57] proposed FAIG that can identify discriminative filters of specific degradation in blind super-resolution (SR) by computing integrated gradient (IG) [47, 48] between the baseline and target models.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and models. The context focuses on the use of FAIG and integrated gradient for identifying filters in blind super-resolution.",
      "processing_time": 43.59826397895813,
      "citing_paper_id": "260107992",
      "cited_paper_id": 16747630
    },
    {
      "context_text": "These prior works can † Corresponding author. enhance the visual quality of deteriorated images as well as may improve the performance of down-stream tasks including classification [17,22], object detection [41,42] and autonomous driving [3,34].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general improvements in image restoration and downstream tasks. No verifiable resources are named.",
      "processing_time": 42.21825909614563,
      "citing_paper_id": "260107992",
      "cited_paper_id": 15780954
    },
    {
      "context_text": "These prior works can † Corresponding author. enhance the visual quality of deteriorated images as well as may improve the performance of down-stream tasks including classification [17,22], object detection [41,42] and autonomous driving [3,34].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general improvements in image restoration and downstream tasks. No verifiable resources are named.",
      "processing_time": 42.21825909614563,
      "citing_paper_id": "260107992",
      "cited_paper_id": 195908774
    },
    {
      "context_text": "…fast computations, but a majority of them have been focusing on known single degradation such as denoising [2, 4, 20, 25, 65, 67, 68], de-raining [5, 16, 30, 51, 52, 59, 60], deblurring [19, 23, 24, 36, 38, 43, 46, 55], desnowing [11, 31, 33, 53] and dehazing [9, 18, 27, 32, 40, 45, 56, 63, 66].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various image restoration tasks. No verifiable resources are identified.",
      "processing_time": 41.608094453811646,
      "citing_paper_id": "260107992",
      "cited_paper_id": 207977878
    },
    {
      "context_text": "Recently, all-in-one image restoration for unknown multiple degradations has been proposed [12,26,29] with different approaches.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to papers proposing different approaches for all-in-one image restoration.",
      "processing_time": 41.860554695129395,
      "citing_paper_id": "260107992",
      "cited_paper_id": 219978541
    },
    {
      "context_text": "The first way to handle unknown multiple degradations is to use a shared decoder with multiple independent encoders for multiple degradations (will denote it as Model with independent Encoders or ME) [29] to take the advantage of using small network parameters as compared to IM as illustrated in Figure 1(b).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models and methods. The context focuses on the architectural design of models for handling multiple degradations.",
      "processing_time": 42.872260332107544,
      "citing_paper_id": "260107992",
      "cited_paper_id": 219978541
    },
    {
      "context_text": "[29] proposed a neural architecture search for all-in-one image restoration focusing on Rain-Haze-Snow dataset using task-specific encoders Chen et al.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Rain-Haze-Snow"
      ],
      "dataset_descriptions": {
        "Rain-Haze-Snow": "Used to train and evaluate a neural architecture search method for all-in-one image restoration, focusing on rain, haze, and snow removal tasks."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions the 'Rain-Haze-Snow dataset' which is a specific dataset used for image restoration tasks involving rain, haze, and snow. The dataset is used for training and evaluating a neural architecture search method.",
      "processing_time": 53.75472569465637,
      "citing_paper_id": "260107992",
      "cited_paper_id": 219978541
    },
    {
      "context_text": "We investigated the following selection methods for the masks M : Random selection method (Ran), Encoder selection method only (En) following Li [29], |θ− θ̂| selection method (θ), and the proposed FAIG-SD method (described in Section 4.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods for mask selection. No verifiable resources are identified.",
      "processing_time": 41.852126121520996,
      "citing_paper_id": "260107992",
      "cited_paper_id": 219978541
    },
    {
      "context_text": "In the case of blur, a blur image was generated using a centered blur kernel [43, 44], and the kernel size was in between 5 and 29.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions generating blur images using a blur kernel, which is a method rather than a specific dataset. No dataset names are explicitly mentioned.",
      "processing_time": 42.66197752952576,
      "citing_paper_id": "260107992",
      "cited_paper_id": 222104551
    },
    {
      "context_text": "In the case of blur, a blur image was generated using a centered blur kernel [43, 44], and the kernel size was in between 5 and 29.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions generating blur images using a blur kernel, which is a method rather than a specific dataset. No dataset names are explicitly mentioned.",
      "processing_time": 42.66197752952576,
      "citing_paper_id": "260107992",
      "cited_paper_id": 246904639
    },
    {
      "context_text": "A vision transformerbased Image Processing Transformer (IPT) was proposed in [6] and Restormer, an efficient transformer model that can be applied to large images while capturing long-range pixel interactions using a multi-head attention and feedforward network, was also proposed in [61].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions models (IPT and Restormer) but does not reference any specific datasets. The cited papers' titles confirm that the focus is on models rather than datasets.",
      "processing_time": 44.89163041114807,
      "citing_paper_id": "260107992",
      "cited_paper_id": 227239228
    },
    {
      "context_text": "A vision transformerbased Image Processing Transformer (IPT) was proposed in [6] and Restormer, an efficient transformer model that can be applied to large images while capturing long-range pixel interactions using a multi-head attention and feedforward network, was also proposed in [61].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions models (IPT and Restormer) but does not reference any specific datasets. The cited papers' titles confirm that the focus is on models rather than datasets.",
      "processing_time": 44.89163041114807,
      "citing_paper_id": "260107992",
      "cited_paper_id": 244346144
    },
    {
      "context_text": "Recently, MPRNet was proposed based on multi-stage architecture to progressively learn the image restoration functions for known multiple degradations [62].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions MPRNet, which is a method, not a dataset. No specific datasets are named or described in the citation context.",
      "processing_time": 43.10329055786133,
      "citing_paper_id": "260107992",
      "cited_paper_id": 231802205
    },
    {
      "context_text": "This approach has achieved state-of-the-art performance for known multiple degradations [7,8,13,35,39,50,54,61,62].",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to other works that have achieved state-of-the-art performance. No dataset names are provided.",
      "processing_time": 43.56853675842285,
      "citing_paper_id": "260107992",
      "cited_paper_id": 231802205
    },
    {
      "context_text": "This approach has achieved state-of-the-art performance for known multiple degradations [7,8,13,35,39,50,54,61,62].",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to other works that have achieved state-of-the-art performance. No dataset names are provided.",
      "processing_time": 43.56853675842285,
      "citing_paper_id": "260107992",
      "cited_paper_id": 237213661
    },
    {
      "context_text": "This approach has achieved state-of-the-art performance for known multiple degradations [7,8,13,35,39,50,54,61,62].",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to other works that have achieved state-of-the-art performance. No dataset names are provided.",
      "processing_time": 43.56853675842285,
      "citing_paper_id": "260107992",
      "cited_paper_id": 244346144
    },
    {
      "context_text": "This approach has achieved state-of-the-art performance for known multiple degradations [7,8,13,35,39,50,54,61,62].",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to other works that have achieved state-of-the-art performance. No dataset names are provided.",
      "processing_time": 43.56853675842285,
      "citing_paper_id": "260107992",
      "cited_paper_id": 248085491
    },
    {
      "context_text": "This approach has achieved state-of-the-art performance for known multiple degradations [7,8,13,35,39,50,54,61,62].",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to other works that have achieved state-of-the-art performance. No dataset names are provided.",
      "processing_time": 43.56853675842285,
      "citing_paper_id": "260107992",
      "cited_paper_id": 250451699
    },
    {
      "context_text": "Class-SR [21] proposed an SR inference using different networks for each patch, similar to the IM technique.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (Class-SR) and a technique (IM).",
      "processing_time": 42.43576002120972,
      "citing_paper_id": "260107992",
      "cited_paper_id": 232146599
    },
    {
      "context_text": "enhance the visual quality of deteriorated images as well as may improve the performance of down-stream tasks including classification [17, 22], object detection [41, 42] and autonomous driving [3, 34].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general tasks and applications. No clear, verifiable resource names are present.",
      "processing_time": 42.64669489860535,
      "citing_paper_id": "260107992",
      "cited_paper_id": 233296254
    },
    {
      "context_text": "SPAIR, a network design that dynamically adjusts calculations for difficult areas of an image, was proposed in [39] and an efficient and simple model without nonlinear functions was proposed in [7].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions two models, SPAIR and an efficient and simple model, but does not refer to any specific datasets. The cited papers' titles do not mention datasets either.",
      "processing_time": 44.89222526550293,
      "citing_paper_id": "260107992",
      "cited_paper_id": 237213661
    },
    {
      "context_text": "SPAIR, a network design that dynamically adjusts calculations for difficult areas of an image, was proposed in [39] and an efficient and simple model without nonlinear functions was proposed in [7].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions two models, SPAIR and an efficient and simple model, but does not refer to any specific datasets. The cited papers' titles do not mention datasets either.",
      "processing_time": 44.89222526550293,
      "citing_paper_id": "260107992",
      "cited_paper_id": 248085491
    },
    {
      "context_text": "We adopt an architecture similar to MSBDN [15], just like Chen [12] and NAFNet [7].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models or methods. There are no verifiable resources that meet the criteria for inclusion.",
      "processing_time": 43.072370529174805,
      "citing_paper_id": "260107992",
      "cited_paper_id": 248085491
    },
    {
      "context_text": "…[20] for dehazing, Rain13k-Test (the combination of Rain100L [53], Rain100H [53], Test100 [59], Test1200 [58] and Test2800 [10]) for deraining, GoPro for motion deblurring, LOL [49] for low-light enhancement, BSD68 [38] for denoising, LSDIR-val for kernel deblurring and jpeg artifact removal.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Rain13k-Test",
        "GoPro",
        "LOL",
        "BSD68",
        "LSDIR-val"
      ],
      "dataset_descriptions": {
        "Rain13k-Test": "Used for deraining, combining multiple datasets to evaluate the effectiveness of deraining algorithms in diverse conditions.",
        "GoPro": "Used for motion deblurring, assessing the performance of deblurring algorithms on real-world motion blur scenarios.",
        "LOL": "Used for low-light enhancement, testing the ability of models to improve image quality in low-light conditions.",
        "BSD68": "Used for denoising, evaluating the effectiveness of denoising algorithms on a standard benchmark dataset.",
        "LSDIR-val": "Used for kernel deblurring and JPEG artifact removal, assessing the performance of models in handling specific image degradation issues."
      },
      "confidence_score": 1.0,
      "reasoning": "The citation mentions several datasets used for various image restoration tasks, including dehazing, deraining, motion deblurring, low-light enhancement, denoising, kernel deblurring, and JPEG artifact removal.",
      "processing_time": 76.10432720184326,
      "citing_paper_id": "272987034",
      "cited_paper_id": 64193
    },
    {
      "context_text": "For high-cost tasks that degradations are difficult to synthesize, we leverage existing paired datasets, including RE-SIDE [20] for dehazing, Rain13k [9, 25, 27, 35, 54] for deraining, GoPro [40] for motion deblurring, and LOL-v2 [55] for low-light image enhancement (LLIE).",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "RE-SIDE",
        "Rain13k",
        "GoPro",
        "LOL-v2"
      ],
      "dataset_descriptions": {
        "RE-SIDE": "Used for dehazing in single-image rain removal, leveraging paired images to train and evaluate dehazing models.",
        "Rain13k": "Used for deraining, providing a large dataset of paired images to train and test deraining algorithms.",
        "GoPro": "Used for motion deblurring, offering paired sharp and blurred images to train and evaluate deblurring models.",
        "LOL-v2": "Used for low-light image enhancement, providing paired low-light and normal-light images to train and test LLIE methods."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions specific datasets used for various image restoration tasks, which are directly relevant to the research topic of all-in-one image restoration.",
      "processing_time": 67.4995059967041,
      "citing_paper_id": "272987034",
      "cited_paper_id": 1372266
    },
    {
      "context_text": "For high-cost tasks that degradations are difficult to synthesize, we leverage existing paired datasets, including RE-SIDE [20] for dehazing, Rain13k [9, 25, 27, 35, 54] for deraining, GoPro [40] for motion deblurring, and LOL-v2 [55] for low-light image enhancement (LLIE).",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "RE-SIDE",
        "Rain13k",
        "GoPro",
        "LOL-v2"
      ],
      "dataset_descriptions": {
        "RE-SIDE": "Used for dehazing in single-image rain removal, leveraging paired images to train and evaluate dehazing models.",
        "Rain13k": "Used for deraining, providing a large dataset of paired images to train and test deraining algorithms.",
        "GoPro": "Used for motion deblurring, offering paired sharp and blurred images to train and evaluate deblurring models.",
        "LOL-v2": "Used for low-light image enhancement, providing paired low-light and normal-light images to train and test LLIE methods."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions specific datasets used for various image restoration tasks, which are directly relevant to the research topic of all-in-one image restoration.",
      "processing_time": 67.4995059967041,
      "citing_paper_id": "272987034",
      "cited_paper_id": 8671030
    },
    {
      "context_text": "For high-cost tasks that degradations are difficult to synthesize, we leverage existing paired datasets, including RE-SIDE [20] for dehazing, Rain13k [9, 25, 27, 35, 54] for deraining, GoPro [40] for motion deblurring, and LOL-v2 [55] for low-light image enhancement (LLIE).",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "RE-SIDE",
        "Rain13k",
        "GoPro",
        "LOL-v2"
      ],
      "dataset_descriptions": {
        "RE-SIDE": "Used for dehazing in single-image rain removal, leveraging paired images to train and evaluate dehazing models.",
        "Rain13k": "Used for deraining, providing a large dataset of paired images to train and test deraining algorithms.",
        "GoPro": "Used for motion deblurring, offering paired sharp and blurred images to train and evaluate deblurring models.",
        "LOL-v2": "Used for low-light image enhancement, providing paired low-light and normal-light images to train and test LLIE methods."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions specific datasets used for various image restoration tasks, which are directly relevant to the research topic of all-in-one image restoration.",
      "processing_time": 67.4995059967041,
      "citing_paper_id": "272987034",
      "cited_paper_id": 39760169
    },
    {
      "context_text": "For high-cost tasks that degradations are difficult to synthesize, we leverage existing paired datasets, including RE-SIDE [20] for dehazing, Rain13k [9, 25, 27, 35, 54] for deraining, GoPro [40] for motion deblurring, and LOL-v2 [55] for low-light image enhancement (LLIE).",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "RE-SIDE",
        "Rain13k",
        "GoPro",
        "LOL-v2"
      ],
      "dataset_descriptions": {
        "RE-SIDE": "Used for dehazing in single-image rain removal, leveraging paired images to train and evaluate dehazing models.",
        "Rain13k": "Used for deraining, providing a large dataset of paired images to train and test deraining algorithms.",
        "GoPro": "Used for motion deblurring, offering paired sharp and blurred images to train and evaluate deblurring models.",
        "LOL-v2": "Used for low-light image enhancement, providing paired low-light and normal-light images to train and test LLIE methods."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions specific datasets used for various image restoration tasks, which are directly relevant to the research topic of all-in-one image restoration.",
      "processing_time": 67.4995059967041,
      "citing_paper_id": "272987034",
      "cited_paper_id": 209376714
    },
    {
      "context_text": "For evaluation, we use SOTS-outdoor [20] for dehazing, Rain13k-Test (the combination of Rain100L [53], Rain100H [53], Test100 [59], Test1200 [58] and Test2800 [10]) for deraining, GoPro for motion deblurring, LOL [49] for low-light enhancement, BSD68 [38] for denoising, LSDIR-val for kernel…",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "SOTS-outdoor",
        "Rain13k-Test",
        "GoPro",
        "LOL",
        "BSD68",
        "LSDIR-val"
      ],
      "dataset_descriptions": {
        "SOTS-outdoor": "Used for evaluating dehazing algorithms, focusing on outdoor scenes to assess performance in removing atmospheric haze.",
        "Rain13k-Test": "Used for evaluating deraining algorithms, combining multiple test sets to assess performance in removing rain streaks from images.",
        "GoPro": "Used for evaluating motion deblurring algorithms, focusing on real-world motion blur to assess the clarity of restored images.",
        "LOL": "Used for evaluating low-light enhancement algorithms, focusing on improving visibility in low-light conditions.",
        "BSD68": "Used for evaluating denoising algorithms, focusing on reducing noise while preserving image details.",
        "LSDIR-val": "Used for evaluating kernel estimation in image restoration, focusing on the accuracy of kernel prediction for various degradation types."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions several datasets used for evaluating different image restoration tasks, which are specific and relevant to the topic of all-in-one image restoration.",
      "processing_time": 78.55203294754028,
      "citing_paper_id": "272987034",
      "cited_paper_id": 3406592
    },
    {
      "context_text": "For evaluation, we use SOTS-outdoor [20] for dehazing, Rain13k-Test (the combination of Rain100L [53], Rain100H [53], Test100 [59], Test1200 [58] and Test2800 [10]) for deraining, GoPro for motion deblurring, LOL [49] for low-light enhancement, BSD68 [38] for denoising, LSDIR-val for kernel…",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "SOTS-outdoor",
        "Rain13k-Test",
        "GoPro",
        "LOL",
        "BSD68",
        "LSDIR-val"
      ],
      "dataset_descriptions": {
        "SOTS-outdoor": "Used for evaluating dehazing algorithms, focusing on outdoor scenes to assess performance in removing atmospheric haze.",
        "Rain13k-Test": "Used for evaluating deraining algorithms, combining multiple test sets to assess performance in removing rain streaks from images.",
        "GoPro": "Used for evaluating motion deblurring algorithms, focusing on real-world motion blur to assess the clarity of restored images.",
        "LOL": "Used for evaluating low-light enhancement algorithms, focusing on improving visibility in low-light conditions.",
        "BSD68": "Used for evaluating denoising algorithms, focusing on reducing noise while preserving image details.",
        "LSDIR-val": "Used for evaluating kernel estimation in image restoration, focusing on the accuracy of kernel prediction for various degradation types."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions several datasets used for evaluating different image restoration tasks, which are specific and relevant to the topic of all-in-one image restoration.",
      "processing_time": 78.55203294754028,
      "citing_paper_id": "272987034",
      "cited_paper_id": 11922819
    },
    {
      "context_text": "For evaluation, we use SOTS-outdoor [20] for dehazing, Rain13k-Test (the combination of Rain100L [53], Rain100H [53], Test100 [59], Test1200 [58] and Test2800 [10]) for deraining, GoPro for motion deblurring, LOL [49] for low-light enhancement, BSD68 [38] for denoising, LSDIR-val for kernel…",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "SOTS-outdoor",
        "Rain13k-Test",
        "GoPro",
        "LOL",
        "BSD68",
        "LSDIR-val"
      ],
      "dataset_descriptions": {
        "SOTS-outdoor": "Used for evaluating dehazing algorithms, focusing on outdoor scenes to assess performance in removing atmospheric haze.",
        "Rain13k-Test": "Used for evaluating deraining algorithms, combining multiple test sets to assess performance in removing rain streaks from images.",
        "GoPro": "Used for evaluating motion deblurring algorithms, focusing on real-world motion blur to assess the clarity of restored images.",
        "LOL": "Used for evaluating low-light enhancement algorithms, focusing on improving visibility in low-light conditions.",
        "BSD68": "Used for evaluating denoising algorithms, focusing on reducing noise while preserving image details.",
        "LSDIR-val": "Used for evaluating kernel estimation in image restoration, focusing on the accuracy of kernel prediction for various degradation types."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions several datasets used for evaluating different image restoration tasks, which are specific and relevant to the topic of all-in-one image restoration.",
      "processing_time": 78.55203294754028,
      "citing_paper_id": "272987034",
      "cited_paper_id": 39760169
    },
    {
      "context_text": "Methods [5, 19, 45, 46] based on integrated gradient attribution often require specifying an integration path, which describes the process of input changes.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only methods related to integrated gradient attribution. No verifiable resources are identified.",
      "processing_time": 42.63581991195679,
      "citing_paper_id": "272987034",
      "cited_paper_id": 3652822
    },
    {
      "context_text": "Methods [5, 19, 45, 46] based on integrated gradient attribution often require specifying an integration path, which describes the process of input changes.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only methods related to integrated gradient attribution. No verifiable resources are identified.",
      "processing_time": 42.63581991195679,
      "citing_paper_id": "272987034",
      "cited_paper_id": 16747630
    },
    {
      "context_text": "For example, Integrated Gradients (IG) [45] aims to attribute the impact of the original input, defining a linear path (which we refer to γ ( α ) in our paper) from an all-black image to the input image.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a method (Integrated Gradients). No dataset names are present in the citation span.",
      "processing_time": 43.0580530166626,
      "citing_paper_id": "272987034",
      "cited_paper_id": 16747630
    },
    {
      "context_text": "To verify the effectiveness of our fine-tuning strategy, we fine-tune 10% of the network layers selected through MAC analysis, IG [45], and uniform sampling, respectively, and the results are shown in Tab.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods for selecting network layers for fine-tuning.",
      "processing_time": 41.80973148345947,
      "citing_paper_id": "272987034",
      "cited_paper_id": 16747630
    },
    {
      "context_text": "Before giving the definition of Mask Attribute Conductance (MAC), we briefly recall the definition of integrate gradient [45] (IG) and neuron conductance [5] (Cond).",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods (integrated gradients and neuron conductance).",
      "processing_time": 42.03371715545654,
      "citing_paper_id": "272987034",
      "cited_paper_id": 16747630
    },
    {
      "context_text": "1] , and introducing JPEG artifacts with a random quality parameter q ∈ [20 , 90] .",
      "catation_intent": "none",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only a method for introducing JPEG artifacts. There are no verifiable resources or datasets mentioned.",
      "processing_time": 43.3104510307312,
      "citing_paper_id": "272987034",
      "cited_paper_id": 39760169
    },
    {
      "context_text": "Therefore, we randomly applied JPEG compression to the images at different qualities (sampled in [20 , 90] ), resulting in corrupted images I J with varying degrees of compression artifacts:",
      "catation_intent": "none",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific, verifiable datasets. It describes a method of corrupting images using JPEG compression, which is not a dataset.",
      "processing_time": 42.81447148323059,
      "citing_paper_id": "272987034",
      "cited_paper_id": 39760169
    },
    {
      "context_text": "Inspired by Mask Language Modeling [18,43], Mask Image Modeling (MIM) [14, 52] is introduced as a pretraining approach to learn general representations in high-level vision.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only methods and models. The cited papers are also about models and do not introduce specific datasets.",
      "processing_time": 43.78152251243591,
      "citing_paper_id": "272987034",
      "cited_paper_id": 49313245
    },
    {
      "context_text": "Inspired by Mask Language Modeling [18,43], Mask Image Modeling (MIM) [14, 52] is introduced as a pretraining approach to learn general representations in high-level vision.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only methods and models. The cited papers are also about models and do not introduce specific datasets.",
      "processing_time": 43.78152251243591,
      "citing_paper_id": "272987034",
      "cited_paper_id": 52967399
    },
    {
      "context_text": "Inspired by Mask Language Modeling [18,43], Mask Image Modeling (MIM) [14, 52] is introduced as a pretraining approach to learn general representations in high-level vision.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only methods and models. The cited papers are also about models and do not introduce specific datasets.",
      "processing_time": 43.78152251243591,
      "citing_paper_id": "272987034",
      "cited_paper_id": 243985980
    },
    {
      "context_text": "Following the Bert [18] and MAE [14], we choose L1 loss to supervise the masked part.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only models and methods. The context is about choosing a loss function for supervising the masked part of the model.",
      "processing_time": 44.839704513549805,
      "citing_paper_id": "272987034",
      "cited_paper_id": 52967399
    },
    {
      "context_text": "Following the Bert [18] and MAE [14], we choose L1 loss to supervise the masked part.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only models and methods. The context is about choosing a loss function for supervising the masked part of the model.",
      "processing_time": 44.839704513549805,
      "citing_paper_id": "272987034",
      "cited_paper_id": 243985980
    },
    {
      "context_text": "Four general architecture-based image restoration methods [3, 28, 39, 56] and four all-in-one methods [7, 21, 31, 42] are considered for comparison.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods and architectures. The cited paper titles do not provide additional context about datasets.",
      "processing_time": 43.52955627441406,
      "citing_paper_id": "272987034",
      "cited_paper_id": 226282220
    },
    {
      "context_text": "Four general architecture-based image restoration methods [3, 28, 39, 56] and four all-in-one methods [7, 21, 31, 42] are considered for comparison.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods and architectures. The cited paper titles do not provide additional context about datasets.",
      "processing_time": 43.52955627441406,
      "citing_paper_id": "272987034",
      "cited_paper_id": 237266491
    },
    {
      "context_text": "Four general architecture-based image restoration methods [3, 28, 39, 56] and four all-in-one methods [7, 21, 31, 42] are considered for comparison.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods and architectures. The cited paper titles do not provide additional context about datasets.",
      "processing_time": 43.52955627441406,
      "citing_paper_id": "272987034",
      "cited_paper_id": 247411392
    },
    {
      "context_text": "Four general architecture-based image restoration methods [3, 28, 39, 56] and four all-in-one methods [7, 21, 31, 42] are considered for comparison.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods and architectures. The cited paper titles do not provide additional context about datasets.",
      "processing_time": 43.52955627441406,
      "citing_paper_id": "272987034",
      "cited_paper_id": 250551851
    },
    {
      "context_text": "A group of methods [3,28,39,48,56] aims at designing a general architecture that can effectively learn each degradation pattern.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods and architectures. No dataset names are present in the text.",
      "processing_time": 42.792038679122925,
      "citing_paper_id": "272987034",
      "cited_paper_id": 226282220
    },
    {
      "context_text": "A group of methods [3,28,39,48,56] aims at designing a general architecture that can effectively learn each degradation pattern.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods and architectures. No dataset names are present in the text.",
      "processing_time": 42.792038679122925,
      "citing_paper_id": "272987034",
      "cited_paper_id": 237266491
    },
    {
      "context_text": "Some works [29, 30,61] utilize task-specific priors to solve a certain degradation problem, while another research line [3, 28, 39, 48, 56] tries to design a general network architecture that can effectively learn each degradation pattern.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general approaches and research lines. No dataset names are present in the text.",
      "processing_time": 43.292322635650635,
      "citing_paper_id": "272987034",
      "cited_paper_id": 226282220
    },
    {
      "context_text": "Some works [29, 30,61] utilize task-specific priors to solve a certain degradation problem, while another research line [3, 28, 39, 48, 56] tries to design a general network architecture that can effectively learn each degradation pattern.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general approaches and research lines. No dataset names are present in the text.",
      "processing_time": 43.292322635650635,
      "citing_paper_id": "272987034",
      "cited_paper_id": 237266491
    },
    {
      "context_text": "Some works [29, 30,61] utilize task-specific priors to solve a certain degradation problem, while another research line [3, 28, 39, 48, 56] tries to design a general network architecture that can effectively learn each degradation pattern.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general approaches and research lines. No dataset names are present in the text.",
      "processing_time": 43.292322635650635,
      "citing_paper_id": "272987034",
      "cited_paper_id": 258557157
    },
    {
      "context_text": "Some works [29, 30,61] utilize task-specific priors to solve a certain degradation problem, while another research line [3, 28, 39, 48, 56] tries to design a general network architecture that can effectively learn each degradation pattern.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general approaches and research lines. No dataset names are present in the text.",
      "processing_time": 43.292322635650635,
      "citing_paper_id": "272987034",
      "cited_paper_id": 260887508
    },
    {
      "context_text": "Several methods [1,24] leverage multiple input and output heads to empower the network to restore 2) The Fine-tuning stage is followed to overcome the input integrity gap caused by changing masked input during pre-training into the whole image during inference.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and stages of training. There are no clear identifiers for datasets in the given context.",
      "processing_time": 43.94103407859802,
      "citing_paper_id": "272987034",
      "cited_paper_id": 227239228
    },
    {
      "context_text": "Gradient-based attribution methods [5,11,44–46 , 51] are often used to",
      "catation_intent": "none",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only gradient-based attribution methods. No dataset names are present in the context.",
      "processing_time": 43.02403020858765,
      "citing_paper_id": "272987034",
      "cited_paper_id": 236772838
    },
    {
      "context_text": "We first find the best choice of patch size by pre-training SwinIR [28] on 1 × 1 , 4 × 4 , and 8 × 8 , as shown in Fig.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only patch sizes used for pre-training a model. No verifiable resource names are present.",
      "processing_time": 43.7670681476593,
      "citing_paper_id": "272987034",
      "cited_paper_id": 237266491
    },
    {
      "context_text": "SwinIR [28] employs a window attention mechanism to convert global attention into a localized approach, effectively reducing computational overhead.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (SwinIR) and its mechanism. No verifiable resources are referenced.",
      "processing_time": 43.93036651611328,
      "citing_paper_id": "272987034",
      "cited_paper_id": 237266491
    },
    {
      "context_text": "We apply our proposed RAM to SwinIR [28] and PromptIR [42].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions applying the proposed RAM to SwinIR and PromptIR, which are methods/models, not datasets. No specific datasets are mentioned.",
      "processing_time": 44.315656900405884,
      "citing_paper_id": "272987034",
      "cited_paper_id": 237266491
    },
    {
      "context_text": "In high-level tasks, MAE [14] masks 75% of an image with 16 × 16 patch size.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a method (MAE) and its parameters. No verifiable resources are identified.",
      "processing_time": 43.75102949142456,
      "citing_paper_id": "272987034",
      "cited_paper_id": 243985980
    },
    {
      "context_text": "Besides, since our MIM pre-training has a similar target to subsequent low-level tasks, we do not need to change the decoder like MAE [14] does but just fine-tune it.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.0,
      "reasoning": "The context mentions 'MIM pre-training' but does not refer to any specific dataset. The cited paper title 'Masked Autoencoders Are Scalable Vision Learners' does not help in identifying a dataset.",
      "processing_time": 47.446903467178345,
      "citing_paper_id": "272987034",
      "cited_paper_id": 243985980
    },
    {
      "context_text": "MAE [14] effectively utilizes MIM for predicting hidden tokens, demonstrating strong performance and generalization across various downstream tasks.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific dataset names, only a method (Masked Autoencoders) and its performance. The term 'MIM' likely refers to a technique (Masked Image Modeling) rather than a dataset.",
      "processing_time": 48.12414908409119,
      "citing_paper_id": "272987034",
      "cited_paper_id": 243985980
    },
    {
      "context_text": "It allows models to acquire a generic representation of images and thus achieve good pre-training, which is verified in many high-level tasks [14, 52].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general capability of models to pre-train on images.",
      "processing_time": 42.99267911911011,
      "citing_paper_id": "272987034",
      "cited_paper_id": 243985980
    },
    {
      "context_text": "1: Our RAM achieves more balanced and more powerful performance than the state-of-the-art methods (Air-Net [21], TAPE [31], PromptIR [42]) for all-in-one blind image restoration.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only models and methods. The cited papers' titles do not provide additional dataset information.",
      "processing_time": 43.91288185119629,
      "citing_paper_id": "272987034",
      "cited_paper_id": 247411392
    },
    {
      "context_text": "1: Our RAM achieves more balanced and more powerful performance than the state-of-the-art methods (Air-Net [21], TAPE [31], PromptIR [42]) for all-in-one blind image restoration.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only models and methods. The cited papers' titles do not provide additional dataset information.",
      "processing_time": 43.91288185119629,
      "citing_paper_id": "272987034",
      "cited_paper_id": 250551851
    },
    {
      "context_text": "It is worth noting that TAPE [31] similarly suggests that understanding normal image nature aids restoration by introducing a natural image prior.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions TAPE, which is a method or model, not a dataset. No specific dataset is mentioned in the context.",
      "processing_time": 42.99678993225098,
      "citing_paper_id": "272987034",
      "cited_paper_id": 247411392
    },
    {
      "context_text": "However, some studies verify that semantic information is not as important for image restoration as it is in pattern recognition tasks [33, 37].",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general statement about the importance of semantic information in image restoration.",
      "processing_time": 43.257078647613525,
      "citing_paper_id": "272987034",
      "cited_paper_id": 248986859
    },
    {
      "context_text": "Recently, several subsequent meth-ods [4, 21, 36, 41, 42, 57, 62] have been proposed to employ a unified network to address multiple restoration issues.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods and approaches. The cited papers' titles also do not provide specific dataset names.",
      "processing_time": 44.302717208862305,
      "citing_paper_id": "272987034",
      "cited_paper_id": 250551851
    },
    {
      "context_text": "Recently, several subsequent meth-ods [4, 21, 36, 41, 42, 57, 62] have been proposed to employ a unified network to address multiple restoration issues.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods and approaches. The cited papers' titles also do not provide specific dataset names.",
      "processing_time": 44.302717208862305,
      "citing_paper_id": "272987034",
      "cited_paper_id": 250581068
    },
    {
      "context_text": "Recently, several subsequent meth-ods [4, 21, 36, 41, 42, 57, 62] have been proposed to employ a unified network to address multiple restoration issues.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods and approaches. The cited papers' titles also do not provide specific dataset names.",
      "processing_time": 44.302717208862305,
      "citing_paper_id": "272987034",
      "cited_paper_id": 260107992
    },
    {
      "context_text": "Recently, several subsequent meth-ods [4, 21, 36, 41, 42, 57, 62] have been proposed to employ a unified network to address multiple restoration issues.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods and approaches. The cited papers' titles also do not provide specific dataset names.",
      "processing_time": 44.302717208862305,
      "citing_paper_id": "272987034",
      "cited_paper_id": 260680793
    },
    {
      "context_text": "Recently, several subsequent meth-ods [4, 21, 36, 41, 42, 57, 62] have been proposed to employ a unified network to address multiple restoration issues.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods and approaches. The cited papers' titles also do not provide specific dataset names.",
      "processing_time": 44.302717208862305,
      "citing_paper_id": "272987034",
      "cited_paper_id": null
    },
    {
      "context_text": "Most of these approaches tend to utilize explicit priors ( e.g ., Air-Net [21]) or introduce an extra module ( e.g ., PromptIR [42]) to discern image degradation patterns, thereby assisting the model in performing the restoration.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and models. There are no clear identifiers for datasets in the provided context.",
      "processing_time": 43.901509523391724,
      "citing_paper_id": "272987034",
      "cited_paper_id": 250551851
    },
    {
      "context_text": "Typically, AirNet [21] first proposed an all-in-one image restoration task.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method called AirNet. The context is about proposing an all-in-one image restoration task, which is not a dataset.",
      "processing_time": 46.08787417411804,
      "citing_paper_id": "272987034",
      "cited_paper_id": 250551851
    },
    {
      "context_text": "While neural networks have demonstrated impressive performance in single degradation image restoration [8, 12, 13, 17, 22, 23, 29, 30, 50, 61], recent works have shifted their focus towards addressing the more challenging domain of multi-degradation image restoration.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only a shift in research focus within the field of image restoration.",
      "processing_time": 43.47509813308716,
      "citing_paper_id": "272987034",
      "cited_paper_id": 257102476
    },
    {
      "context_text": "While neural networks have demonstrated impressive performance in single degradation image restoration [8, 12, 13, 17, 22, 23, 29, 30, 50, 61], recent works have shifted their focus towards addressing the more challenging domain of multi-degradation image restoration.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only a shift in research focus within the field of image restoration.",
      "processing_time": 43.47509813308716,
      "citing_paper_id": "272987034",
      "cited_paper_id": 258048853
    },
    {
      "context_text": "While neural networks have demonstrated impressive performance in single degradation image restoration [8, 12, 13, 17, 22, 23, 29, 30, 50, 61], recent works have shifted their focus towards addressing the more challenging domain of multi-degradation image restoration.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only a shift in research focus within the field of image restoration.",
      "processing_time": 43.47509813308716,
      "citing_paper_id": "272987034",
      "cited_paper_id": 258557157
    },
    {
      "context_text": "While neural networks have demonstrated impressive performance in single degradation image restoration [8, 12, 13, 17, 22, 23, 29, 30, 50, 61], recent works have shifted their focus towards addressing the more challenging domain of multi-degradation image restoration.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only a shift in research focus within the field of image restoration.",
      "processing_time": 43.47509813308716,
      "citing_paper_id": "272987034",
      "cited_paper_id": 260887508
    },
    {
      "context_text": "While neural networks have demonstrated impressive performance in single degradation image restoration [8, 12, 13, 17, 22, 23, 29, 30, 50, 61], recent works have shifted their focus towards addressing the more challenging domain of multi-degradation image restoration.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only a shift in research focus within the field of image restoration.",
      "processing_time": 43.47509813308716,
      "citing_paper_id": "272987034",
      "cited_paper_id": 261081748
    },
    {
      "context_text": "[2] demonstrate that pair-wise MIM training enhances the generalization performance over different types of noisy images.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (pair-wise MIM training). The context is about the effectiveness of a training method, not the use of a dataset.",
      "processing_time": 46.48959827423096,
      "citing_paper_id": "272987034",
      "cited_paper_id": 257687687
    },
    {
      "context_text": "Some attempts [2, 6] by Mask Image Modeling (MIM) in low-level vision have caught our attention.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (Mask Image Modeling). No dataset names are provided in the context.",
      "processing_time": 44.14804530143738,
      "citing_paper_id": "272987034",
      "cited_paper_id": 257687687
    },
    {
      "context_text": "[2 ] employs the MIM model to enhance the model’s generalization for denoising tasks but has not explored its potential in multi-task scenarios.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions the MIM model but does not refer to a specific dataset. The context is about enhancing model generalization for denoising tasks.",
      "processing_time": 45.02738809585571,
      "citing_paper_id": "272987034",
      "cited_paper_id": 257687687
    },
    {
      "context_text": "Among them, [2] and [6] are the most closely aligned with our focus.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to other papers. No dataset names are present in the text.",
      "processing_time": 42.970452308654785,
      "citing_paper_id": "272987034",
      "cited_paper_id": 257687687
    },
    {
      "context_text": "We combine datasets from various restoration tasks to form the training set, following [60].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.3,
      "reasoning": "The context mentions combining datasets from various restoration tasks but does not specify any particular dataset names. The cited paper title does not provide additional specific dataset names.",
      "processing_time": 45.36892867088318,
      "citing_paper_id": "272987034",
      "cited_paper_id": 260085391
    },
    {
      "context_text": "The conventional image restoration pipeline uses a predefined model to represent degradation, e.g., Gaussian noise [2], [52] or motion blur [53], [54].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific, verifiable datasets. It only refers to types of degradation models such as Gaussian noise and motion blur.",
      "processing_time": 44.754170417785645,
      "citing_paper_id": "264305756",
      "cited_paper_id": 684325
    },
    {
      "context_text": "The conventional image restoration pipeline uses a predefined model to represent degradation, e.g., Gaussian noise [2], [52] or motion blur [53], [54].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific, verifiable datasets. It only refers to types of degradation models such as Gaussian noise and motion blur.",
      "processing_time": 44.754170417785645,
      "citing_paper_id": "264305756",
      "cited_paper_id": 996788
    },
    {
      "context_text": "Following [2], [3], [4], [5], [23], Gaussian noise is added with standard deviation values of 15, 25, and 50 on the clean images.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only the method of adding Gaussian noise to clean images. No verifiable resources are identified.",
      "processing_time": 44.273226261138916,
      "citing_paper_id": "264305756",
      "cited_paper_id": 996788
    },
    {
      "context_text": "Following [2], [3], [4], [5], [23], Gaussian noise is added with standard deviation values of 15, 25, and 50 on the clean images.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only the method of adding Gaussian noise to clean images. No verifiable resources are identified.",
      "processing_time": 44.273226261138916,
      "citing_paper_id": "264305756",
      "cited_paper_id": 203042751
    },
    {
      "context_text": "In recent years, researchers have proposed tremendous neural networks that are tailored to specific tasks, such as denoising [2], [4], [27], [28], deraining [6], [7], [8], [9], [29], [29], [30], [31], [32], [33], dehazing [11], [34], [35], [36], and SR [37], [38], [39], [40].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various tasks and methods. No dataset names are present in the text.",
      "processing_time": 44.12979483604431,
      "citing_paper_id": "264305756",
      "cited_paper_id": 996788
    },
    {
      "context_text": "In recent years, researchers have proposed tremendous neural networks that are tailored to specific tasks, such as denoising [2], [4], [27], [28], deraining [6], [7], [8], [9], [29], [29], [30], [31], [32], [33], dehazing [11], [34], [35], [36], and SR [37], [38], [39], [40].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various tasks and methods. No dataset names are present in the text.",
      "processing_time": 44.12979483604431,
      "citing_paper_id": "264305756",
      "cited_paper_id": 3619954
    },
    {
      "context_text": "In recent years, researchers have proposed tremendous neural networks that are tailored to specific tasks, such as denoising [2], [4], [27], [28], deraining [6], [7], [8], [9], [29], [29], [30], [31], [32], [33], dehazing [11], [34], [35], [36], and SR [37], [38], [39], [40].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various tasks and methods. No dataset names are present in the text.",
      "processing_time": 44.12979483604431,
      "citing_paper_id": "264305756",
      "cited_paper_id": 49864080
    },
    {
      "context_text": "In recent years, researchers have proposed tremendous neural networks that are tailored to specific tasks, such as denoising [2], [4], [27], [28], deraining [6], [7], [8], [9], [29], [29], [30], [31], [32], [33], dehazing [11], [34], [35], [36], and SR [37], [38], [39], [40].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various tasks and methods. No dataset names are present in the text.",
      "processing_time": 44.12979483604431,
      "citing_paper_id": "264305756",
      "cited_paper_id": 52952008
    },
    {
      "context_text": "In recent years, researchers have proposed tremendous neural networks that are tailored to specific tasks, such as denoising [2], [4], [27], [28], deraining [6], [7], [8], [9], [29], [29], [30], [31], [32], [33], dehazing [11], [34], [35], [36], and SR [37], [38], [39], [40].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various tasks and methods. No dataset names are present in the text.",
      "processing_time": 44.12979483604431,
      "citing_paper_id": "264305756",
      "cited_paper_id": 247218518
    },
    {
      "context_text": "In recent years, researchers have proposed tremendous neural networks that are tailored to specific tasks, such as denoising [2], [4], [27], [28], deraining [6], [7], [8], [9], [29], [29], [30], [31], [32], [33], dehazing [11], [34], [35], [36], and SR [37], [38], [39], [40].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various tasks and methods. No dataset names are present in the text.",
      "processing_time": 44.12979483604431,
      "citing_paper_id": "264305756",
      "cited_paper_id": 248572065
    },
    {
      "context_text": "To process different types of degradations, various image restoration methods have been proposed, e.g., denoising [1], [2], [3], [4], [5], deraining [6], [7], [8], [9], [10], dehazing [11], [12], [13], [14], [15], and super-resolution (SR) [16], [17], [18], [19], [20].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various image restoration methods and their applications. No verifiable resources are identified.",
      "processing_time": 44.26741099357605,
      "citing_paper_id": "264305756",
      "cited_paper_id": 996788
    },
    {
      "context_text": "To process different types of degradations, various image restoration methods have been proposed, e.g., denoising [1], [2], [3], [4], [5], deraining [6], [7], [8], [9], [10], dehazing [11], [12], [13], [14], [15], and super-resolution (SR) [16], [17], [18], [19], [20].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various image restoration methods and their applications. No verifiable resources are identified.",
      "processing_time": 44.26741099357605,
      "citing_paper_id": "264305756",
      "cited_paper_id": 6540453
    },
    {
      "context_text": "To process different types of degradations, various image restoration methods have been proposed, e.g., denoising [1], [2], [3], [4], [5], deraining [6], [7], [8], [9], [10], dehazing [11], [12], [13], [14], [15], and super-resolution (SR) [16], [17], [18], [19], [20].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various image restoration methods and their applications. No verifiable resources are identified.",
      "processing_time": 44.26741099357605,
      "citing_paper_id": "264305756",
      "cited_paper_id": 6593498
    },
    {
      "context_text": "To process different types of degradations, various image restoration methods have been proposed, e.g., denoising [1], [2], [3], [4], [5], deraining [6], [7], [8], [9], [10], dehazing [11], [12], [13], [14], [15], and super-resolution (SR) [16], [17], [18], [19], [20].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various image restoration methods and their applications. No verifiable resources are identified.",
      "processing_time": 44.26741099357605,
      "citing_paper_id": "264305756",
      "cited_paper_id": 17763780
    },
    {
      "context_text": "To process different types of degradations, various image restoration methods have been proposed, e.g., denoising [1], [2], [3], [4], [5], deraining [6], [7], [8], [9], [10], dehazing [11], [12], [13], [14], [15], and super-resolution (SR) [16], [17], [18], [19], [20].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various image restoration methods and their applications. No verifiable resources are identified.",
      "processing_time": 44.26741099357605,
      "citing_paper_id": "264305756",
      "cited_paper_id": 21712570
    },
    {
      "context_text": "To process different types of degradations, various image restoration methods have been proposed, e.g., denoising [1], [2], [3], [4], [5], deraining [6], [7], [8], [9], [10], dehazing [11], [12], [13], [14], [15], and super-resolution (SR) [16], [17], [18], [19], [20].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various image restoration methods and their applications. No verifiable resources are identified.",
      "processing_time": 44.26741099357605,
      "citing_paper_id": "264305756",
      "cited_paper_id": 203042751
    },
    {
      "context_text": "To process different types of degradations, various image restoration methods have been proposed, e.g., denoising [1], [2], [3], [4], [5], deraining [6], [7], [8], [9], [10], dehazing [11], [12], [13], [14], [15], and super-resolution (SR) [16], [17], [18], [19], [20].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various image restoration methods and their applications. No verifiable resources are identified.",
      "processing_time": 44.26741099357605,
      "citing_paper_id": "264305756",
      "cited_paper_id": 260870102
    },
    {
      "context_text": "For denoising, we compare our methods with CBM3D [1], DnCNN [2], IRCNN [3], FFDNet [4], and BRDNet [5].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions several methods/models for image denoising but does not reference any specific datasets. The cited papers are also focused on methods rather than datasets.",
      "processing_time": 45.35368537902832,
      "citing_paper_id": "264305756",
      "cited_paper_id": 996788
    },
    {
      "context_text": "For denoising, we compare our methods with CBM3D [1], DnCNN [2], IRCNN [3], FFDNet [4], and BRDNet [5].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions several methods/models for image denoising but does not reference any specific datasets. The cited papers are also focused on methods rather than datasets.",
      "processing_time": 45.35368537902832,
      "citing_paper_id": "264305756",
      "cited_paper_id": 203042751
    },
    {
      "context_text": "For example, DnCNN [2] utilizes a deep convolutional architecture to learn the mapping between noisy and clean images, effectively suppressing the noise while preserving image details.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific dataset names, only a method (DnCNN) and its application to image denoising.",
      "processing_time": 44.515684604644775,
      "citing_paper_id": "264305756",
      "cited_paper_id": 996788
    },
    {
      "context_text": "For SR, we compare our method with SR methods including EDSR [17], RCAN [37], HAN + [19], SwinIR [72], HAT [39], and Restormer [21].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context mentions several methods/models for image super-resolution but does not refer to any specific datasets. The context is focused on comparing different methods, not on using datasets.",
      "processing_time": 46.26998043060303,
      "citing_paper_id": "264305756",
      "cited_paper_id": 3619954
    },
    {
      "context_text": "For SR, we compare our method with SR methods including EDSR [17], RCAN [37], HAN + [19], SwinIR [72], HAT [39], and Restormer [21].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context mentions several methods/models for image super-resolution but does not refer to any specific datasets. The context is focused on comparing different methods, not on using datasets.",
      "processing_time": 46.26998043060303,
      "citing_paper_id": "264305756",
      "cited_paper_id": 6540453
    },
    {
      "context_text": "For SR, we compare our method with SR methods including EDSR [17], RCAN [37], HAN + [19], SwinIR [72], HAT [39], and Restormer [21].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context mentions several methods/models for image super-resolution but does not refer to any specific datasets. The context is focused on comparing different methods, not on using datasets.",
      "processing_time": 46.26998043060303,
      "citing_paper_id": "264305756",
      "cited_paper_id": 248572065
    },
    {
      "context_text": "For denoising, we take the widely-used BSD400 [61] and WEB [70] datasets as training sets, and the BSD68 [61] and Urban100 [62] datasets as testing sets.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "BSD400",
        "WEB",
        "BSD68",
        "Urban100"
      ],
      "dataset_descriptions": {
        "BSD400": "Used for training denoising models, providing a diverse set of images to improve generalization.",
        "WEB": "Used for training denoising models, offering web images to enhance real-world applicability.",
        "BSD68": "Used for testing denoising models, evaluating performance on a standardized set of images.",
        "Urban100": "Used for testing denoising models, assessing performance on high-resolution urban scenes."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions specific datasets used for training and testing in the context of image denoising. These datasets are clearly identified and their usage is described.",
      "processing_time": 65.51284837722778,
      "citing_paper_id": "264305756",
      "cited_paper_id": 4840263
    },
    {
      "context_text": "To be detailed, BSD400 [61], WED [70], BSD68 [61], and Urban100 [62] datasets consist of 400, 4,744, 68, and 100 clean natural images, respectively.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "BSD400",
        "WED",
        "BSD68",
        "Urban100"
      ],
      "dataset_descriptions": {
        "BSD400": "Used for image restoration, consisting of 400 clean natural images to evaluate restoration algorithms.",
        "WED": "Used for image quality assessment, containing 4,744 images to test and validate image restoration models.",
        "BSD68": "Used for image restoration, comprising 68 clean natural images to assess restoration performance.",
        "Urban100": "Used for image restoration, including 100 urban scene images to evaluate restoration techniques."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions specific image datasets used for image restoration, providing the number of images in each dataset.",
      "processing_time": 65.24648213386536,
      "citing_paper_id": "264305756",
      "cited_paper_id": 4840263
    },
    {
      "context_text": "For dehazing, we compare our methods with DehazeNet [11], MSCNN [12], AODNet [13], EPDN [14], FDGAN [15], and DehazeFormer [67].",
      "catation_intent": "reusable resource",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span mentions several methods/models but does not reference any specific datasets. No dataset names are present in the context.",
      "processing_time": 44.49417972564697,
      "citing_paper_id": "264305756",
      "cited_paper_id": 17763780
    },
    {
      "context_text": "For deraining, we compare our methods with DIDMDN [6], UMRL [7], SIRR [8], MSPFN [9], and LPNet [10].",
      "catation_intent": "reusable resource",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only other methods for image deraining.",
      "processing_time": 43.196839570999146,
      "citing_paper_id": "264305756",
      "cited_paper_id": 21712570
    },
    {
      "context_text": "For noisy images, we conduct experiments using the SIDD [74] dataset, which contains 200 images captured with smartphones.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "SIDD"
      ],
      "dataset_descriptions": {
        "SIDD": "Used to conduct experiments on denoising noisy images captured with smartphones, focusing on improving image quality through restoration techniques."
      },
      "confidence_score": 1.0,
      "reasoning": "The context clearly mentions the use of the SIDD dataset for conducting experiments on noisy images captured with smartphones.",
      "processing_time": 49.97191023826599,
      "citing_paper_id": "264305756",
      "cited_paper_id": 52059988
    },
    {
      "context_text": "N2V [42] proposes an self-supervised learning-based method with blind-spot convolution.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method called Noise2Void. The context focuses on the method's capabilities and does not reference any dataset.",
      "processing_time": 45.77567529678345,
      "citing_paper_id": "264305756",
      "cited_paper_id": 53751136
    },
    {
      "context_text": "Each image’s degradation is randomly generated according to physical degradation processes [64], [66], [71], resulting in different degradation patterns.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only general degradation processes. No dataset names are provided in the context.",
      "processing_time": 43.83747434616089,
      "citing_paper_id": "264305756",
      "cited_paper_id": 59523708
    },
    {
      "context_text": "3) Quantatitive Results: To further demonstrate the effectiveness of our method in real-captured images, we calculate non-reference image quality assessment metrics (DBCNN [76] and MUSIQ [77]) in Table VII.",
      "catation_intent": "reusable resource",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions DBCNN and MUSIQ as non-reference image quality assessment metrics, but they are not datasets. They are methods/models used to assess image quality.",
      "processing_time": 46.63132452964783,
      "citing_paper_id": "264305756",
      "cited_paper_id": 69993921
    },
    {
      "context_text": "3) Quantatitive Results: To further demonstrate the effectiveness of our method in real-captured images, we calculate non-reference image quality assessment metrics (DBCNN [76] and MUSIQ [77]) in Table VII.",
      "catation_intent": "reusable resource",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions DBCNN and MUSIQ as non-reference image quality assessment metrics, but they are not datasets. They are methods/models used to assess image quality.",
      "processing_time": 46.63132452964783,
      "citing_paper_id": "264305756",
      "cited_paper_id": 237048383
    },
    {
      "context_text": "This phenomenon aligns with how convolutional neural networks emphasize local regions and features like edges and textures [78], [79].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only references to how convolutional neural networks process features. No verifiable resources are identified.",
      "processing_time": 45.19033765792847,
      "citing_paper_id": "264305756",
      "cited_paper_id": 119308964
    },
    {
      "context_text": "This phenomenon aligns with how convolutional neural networks emphasize local regions and features like edges and textures [78], [79].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only references to how convolutional neural networks process features. No verifiable resources are identified.",
      "processing_time": 45.19033765792847,
      "citing_paper_id": "264305756",
      "cited_paper_id": 234338988
    },
    {
      "context_text": "The CP-Conv consists of a dimension projection and a Kronecker product [59], [60].",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only methods or models. The cited papers are about mathematical operations and do not introduce datasets.",
      "processing_time": 44.97061252593994,
      "citing_paper_id": "264305756",
      "cited_paper_id": 122775231
    },
    {
      "context_text": "The CP-Conv consists of a dimension projection and a Kronecker product [59], [60].",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only methods or models. The cited papers are about mathematical operations and do not introduce datasets.",
      "processing_time": 44.97061252593994,
      "citing_paper_id": "264305756",
      "cited_paper_id": 250296280
    },
    {
      "context_text": "3), we conduct matrix multiplications similar to the weighted summation process of 1 × 1 convolution.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, models, or methods. It only describes a technical process related to matrix multiplications and 1x1 convolutions.",
      "processing_time": 46.4152991771698,
      "citing_paper_id": "264305756",
      "cited_paper_id": 210838848
    },
    {
      "context_text": "In practical applications, however, the degradation is often unknown, and the mismatch between the model and the degradation will result in a severe performance drop.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only a general issue in image restoration models.",
      "processing_time": 43.40349078178406,
      "citing_paper_id": "264305756",
      "cited_paper_id": 210838848
    },
    {
      "context_text": "This phenomenon may be related to the coupling between different degradations, known as task dependency and conflict in multi-task learning [80], which we will explore in future work.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a concept related to multi-task learning.",
      "processing_time": 43.1694552898407,
      "citing_paper_id": "264305756",
      "cited_paper_id": 221771219
    },
    {
      "context_text": "DAN [56] uses an unfolding algorithm to learn a degradation kernel, capturing the intrinsic features of spatial resolution degradation.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (unfolding algorithm) used for blind super resolution.",
      "processing_time": 44.204787254333496,
      "citing_paper_id": "264305756",
      "cited_paper_id": 222141081
    },
    {
      "context_text": "Early attempts [21], [46] utilize the same network architecture trained on different tasks with different parameters.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only different tasks and network architectures.",
      "processing_time": 42.669798612594604,
      "citing_paper_id": "264305756",
      "cited_paper_id": 226282220
    },
    {
      "context_text": "We compare our method with NAFNet [73], MPRNet [46], AirNet [23] and Restormer [21].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only other methods for image restoration. No verifiable resources are identified.",
      "processing_time": 44.45480465888977,
      "citing_paper_id": "264305756",
      "cited_paper_id": 226282220
    },
    {
      "context_text": "We compare our method with NAFNet [73], MPRNet [46], AirNet [23] and Restormer [21].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only other methods for image restoration. No verifiable resources are identified.",
      "processing_time": 44.45480465888977,
      "citing_paper_id": "264305756",
      "cited_paper_id": 248085491
    },
    {
      "context_text": "We choose NAFNet [73], MPRNet [46] and Restormer [21] as baselines.",
      "catation_intent": "none",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions NAFNet, MPRNet, and Restormer as baselines but does not refer to any specific datasets. These are models or methods, not datasets.",
      "processing_time": 46.94210171699524,
      "citing_paper_id": "264305756",
      "cited_paper_id": 226282220
    },
    {
      "context_text": "We choose NAFNet [73], MPRNet [46] and Restormer [21] as baselines.",
      "catation_intent": "none",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions NAFNet, MPRNet, and Restormer as baselines but does not refer to any specific datasets. These are models or methods, not datasets.",
      "processing_time": 46.94210171699524,
      "citing_paper_id": "264305756",
      "cited_paper_id": 248085491
    },
    {
      "context_text": "Different from previous works [58] that directly apply CP decomposition to a single feature, we leverage it to simultaneously capture the essential representation of both image and degradation features, thereby mapping them into the same space.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (CP decomposition) and a general approach to image and degradation features.",
      "processing_time": 44.945385694503784,
      "citing_paper_id": "264305756",
      "cited_paper_id": 248006282
    },
    {
      "context_text": "We compare our method with various image restoration techniques, including NAFNet [73], MPRNet [63], Restormer [21], and AirNet [23].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span mentions several methods/models but does not refer to any specific datasets. The context is focused on comparing different image restoration techniques.",
      "processing_time": 45.302483797073364,
      "citing_paper_id": "264305756",
      "cited_paper_id": 248085491
    },
    {
      "context_text": "Additionally, we notice that the data proportion for different tasks greatly affects the model’s performance, which means increasing data for a specific task improves its performance but may reduce performance in other tasks [84], [85].",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general observation about data proportions affecting model performance.",
      "processing_time": 43.799338817596436,
      "citing_paper_id": "264305756",
      "cited_paper_id": 248498308
    },
    {
      "context_text": "Meanwhile, unified weather restoration methods [49], [50], [51] are also proposed.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to papers proposing unified weather restoration methods.",
      "processing_time": 43.79604125022888,
      "citing_paper_id": "264305756",
      "cited_paper_id": 250581068
    },
    {
      "context_text": "For perception-oriented methods ( i.e., DiffBIR [82] and StableIR [83]), we focus on visual quality, as shown in Fig.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods (DiffBIR and StableIR). No dataset names are present in the text.",
      "processing_time": 45.72801899909973,
      "citing_paper_id": "264305756",
      "cited_paper_id": 258615282
    },
    {
      "context_text": "We explore fidelity-and perception-oriented all-in-one image restoration by comparing our approach with recent methods [44], [81], [82], [83].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only comparisons with recent methods. No verifiable resources are identified.",
      "processing_time": 44.44388747215271,
      "citing_paper_id": "264305756",
      "cited_paper_id": 258615282
    },
    {
      "context_text": "We explore fidelity-and perception-oriented all-in-one image restoration by comparing our approach with recent methods [44], [81], [82], [83].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only comparisons with recent methods. No verifiable resources are identified.",
      "processing_time": 44.44388747215271,
      "citing_paper_id": "264305756",
      "cited_paper_id": 259224666
    },
    {
      "context_text": "All-in-one image restoration, which utilizes a single network to handle different restoration tasks, has emerged as a promising direction [23], [43], [44], [45].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only the concept of all-in-one image restoration. No dataset names are present in the text.",
      "processing_time": 45.97710657119751,
      "citing_paper_id": "264305756",
      "cited_paper_id": 259224666
    },
    {
      "context_text": "For fidelity-oriented methods, we follow the training configuration of PromptIR [44] to train both HAT [81] and our model, and we directly take the results for PromptIR from the officially published version.",
      "catation_intent": "reusable resource",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions using the training configuration of PromptIR but does not specify any datasets. The cited paper title confirms PromptIR is a method, not a dataset.",
      "processing_time": 45.97460198402405,
      "citing_paper_id": "264305756",
      "cited_paper_id": 259224666
    },
    {
      "context_text": "PromptIR [44] has recently been proposed utilizing learnable degradation-related parameters.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method called PromptIR. No verifiable resources are identified.",
      "processing_time": 44.66780948638916,
      "citing_paper_id": "264305756",
      "cited_paper_id": 259224666
    },
    {
      "context_text": "ADMS [47] utilizes adaptive discriminative filters to handle different degradations and IDR [48] proposes a two-stage Fig.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods or models. The context focuses on techniques for handling image degradation.",
      "processing_time": 44.91829252243042,
      "citing_paper_id": "264305756",
      "cited_paper_id": 260085391
    },
    {
      "context_text": "S2: Comparative analysis of image denoising by all-in-one methods on the BSD68 dataset [30].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "BSD68"
      ],
      "dataset_descriptions": {
        "BSD68": "Used to evaluate image denoising methods, focusing on performance metrics and visual quality of restored images."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions the BSD68 dataset, which is a specific dataset used for image denoising evaluation. The dataset is clearly named and relevant to the research topic.",
      "processing_time": 52.519200801849365,
      "citing_paper_id": "268856875",
      "cited_paper_id": 64193
    },
    {
      "context_text": "Table 4: Denoising comparisons in the single-task setting on BSD68 [30] and Ur-ban100 [20] datasets.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "BSD68",
        "Urban100"
      ],
      "dataset_descriptions": {
        "BSD68": "Used for denoising comparisons in a single-task setting, focusing on image restoration performance and algorithm evaluation.",
        "Urban100": "Used for denoising comparisons in a single-task setting, focusing on image restoration performance and algorithm evaluation."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions two specific datasets, BSD68 and Urban100, which are used for denoising comparisons in a single-task setting.",
      "processing_time": 57.814449310302734,
      "citing_paper_id": "268856875",
      "cited_paper_id": 64193
    },
    {
      "context_text": "Table 4: Denoising comparisons in the single-task setting on BSD68 [30] and Ur-ban100 [20] datasets.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "BSD68",
        "Urban100"
      ],
      "dataset_descriptions": {
        "BSD68": "Used for denoising comparisons in a single-task setting, focusing on image restoration performance and algorithm evaluation.",
        "Urban100": "Used for denoising comparisons in a single-task setting, focusing on image restoration performance and algorithm evaluation."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions two specific datasets, BSD68 and Urban100, which are used for denoising comparisons in a single-task setting.",
      "processing_time": 57.814449310302734,
      "citing_paper_id": "268856875",
      "cited_paper_id": 8282555
    },
    {
      "context_text": "5: Comparative analysis of image denoising by all-in-one methods on the BSD68 dataset [30].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "BSD68"
      ],
      "dataset_descriptions": {
        "BSD68": "Used for comparative analysis of image denoising methods, focusing on evaluating the performance of all-in-one restoration techniques."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions the BSD68 dataset, which is a specific dataset used for image denoising evaluation. The dataset is clearly identified and used for comparative analysis.",
      "processing_time": 52.8109564781189,
      "citing_paper_id": "268856875",
      "cited_paper_id": 64193
    },
    {
      "context_text": "Testing is conducted on the BSD68 [30] and Urban100 [20] datasets.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "BSD68",
        "Urban100"
      ],
      "dataset_descriptions": {
        "BSD68": "Used to test image restoration methods, focusing on natural image denoising and evaluating algorithm performance.",
        "Urban100": "Used to test image restoration methods, focusing on urban scene images and evaluating super-resolution techniques."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions two specific datasets, BSD68 and Urban100, which are commonly used in image restoration and super-resolution tasks.",
      "processing_time": 56.479716777801514,
      "citing_paper_id": "268856875",
      "cited_paper_id": 64193
    },
    {
      "context_text": "Testing is conducted on the BSD68 [30] and Urban100 [20] datasets.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "BSD68",
        "Urban100"
      ],
      "dataset_descriptions": {
        "BSD68": "Used to test image restoration methods, focusing on natural image denoising and evaluating algorithm performance.",
        "Urban100": "Used to test image restoration methods, focusing on urban scene images and evaluating super-resolution techniques."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions two specific datasets, BSD68 and Urban100, which are commonly used in image restoration and super-resolution tasks.",
      "processing_time": 56.479716777801514,
      "citing_paper_id": "268856875",
      "cited_paper_id": 8282555
    },
    {
      "context_text": "Research in image restoration has predominantly concentrated on addressing single degradation challenges [11, 34, 38, 40, 45, 52, 53, 58, 60].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only a general trend in research focus. No dataset names are present in the text.",
      "processing_time": 45.696579933166504,
      "citing_paper_id": "268856875",
      "cited_paper_id": 1900475
    },
    {
      "context_text": "Research in image restoration has predominantly concentrated on addressing single degradation challenges [11, 34, 38, 40, 45, 52, 53, 58, 60].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only a general trend in research focus. No dataset names are present in the text.",
      "processing_time": 45.696579933166504,
      "citing_paper_id": "268856875",
      "cited_paper_id": 202733344
    },
    {
      "context_text": "Research in image restoration has predominantly concentrated on addressing single degradation challenges [11, 34, 38, 40, 45, 52, 53, 58, 60].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only a general trend in research focus. No dataset names are present in the text.",
      "processing_time": 45.696579933166504,
      "citing_paper_id": "268856875",
      "cited_paper_id": 212737191
    },
    {
      "context_text": "Research in image restoration has predominantly concentrated on addressing single degradation challenges [11, 34, 38, 40, 45, 52, 53, 58, 60].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only a general trend in research focus. No dataset names are present in the text.",
      "processing_time": 45.696579933166504,
      "citing_paper_id": "268856875",
      "cited_paper_id": 214802286
    },
    {
      "context_text": "Research in image restoration has predominantly concentrated on addressing single degradation challenges [11, 34, 38, 40, 45, 52, 53, 58, 60].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only a general trend in research focus. No dataset names are present in the text.",
      "processing_time": 45.696579933166504,
      "citing_paper_id": "268856875",
      "cited_paper_id": 216562731
    },
    {
      "context_text": "Research in image restoration has predominantly concentrated on addressing single degradation challenges [11, 34, 38, 40, 45, 52, 53, 58, 60].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only a general trend in research focus. No dataset names are present in the text.",
      "processing_time": 45.696579933166504,
      "citing_paper_id": "268856875",
      "cited_paper_id": 235702920
    },
    {
      "context_text": "Research in image restoration has predominantly concentrated on addressing single degradation challenges [11, 34, 38, 40, 45, 52, 53, 58, 60].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only a general trend in research focus. No dataset names are present in the text.",
      "processing_time": 45.696579933166504,
      "citing_paper_id": "268856875",
      "cited_paper_id": 244346144
    },
    {
      "context_text": "Research in image restoration has predominantly concentrated on addressing single degradation challenges [11, 34, 38, 40, 45, 52, 53, 58, 60].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only a general trend in research focus. No dataset names are present in the text.",
      "processing_time": 45.696579933166504,
      "citing_paper_id": "268856875",
      "cited_paper_id": 247939726
    },
    {
      "context_text": "Given the impractical-ity of manual sorting of 170M images, we employ various image quality metrics NIQE [33], BRISQUE [32], and NIMA [43].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions image quality metrics (NIQE, BRISQUE, NIMA) but does not refer to them as datasets. These are metrics, not datasets, and thus do not meet the criteria for inclusion.",
      "processing_time": 49.00348472595215,
      "citing_paper_id": "268856875",
      "cited_paper_id": 2927709
    },
    {
      "context_text": "Given the impractical-ity of manual sorting of 170M images, we employ various image quality metrics NIQE [33], BRISQUE [32], and NIMA [43].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions image quality metrics (NIQE, BRISQUE, NIMA) but does not refer to them as datasets. These are metrics, not datasets, and thus do not meet the criteria for inclusion.",
      "processing_time": 49.00348472595215,
      "citing_paper_id": "268856875",
      "cited_paper_id": 16892725
    },
    {
      "context_text": "Specifically, for image denoising, we combine the BSD400 [5] (400 training images) and WED [29] (4,744 images) datasets, adding Gaussian noise at levels σ ∈ 15 , 25 , 50 to create noisy images.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "BSD400",
        "WED"
      ],
      "dataset_descriptions": {
        "BSD400": "Used for image denoising, specifically by adding Gaussian noise at levels σ ∈ 15, 25, 50 to create noisy images for training.",
        "WED": "Used for image denoising, specifically by adding Gaussian noise at levels σ ∈ 15, 25, 50 to create noisy images for training."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions two specific datasets, BSD400 and WED, which are used for image denoising by adding Gaussian noise at specific levels.",
      "processing_time": 62.37249231338501,
      "citing_paper_id": "268856875",
      "cited_paper_id": 4840263
    },
    {
      "context_text": "For the challenging noise level of σ = 50 on Urban100 [20], our DyNet-L obtains 0 .",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Urban100"
      ],
      "dataset_descriptions": {
        "Urban100": "Used to evaluate the performance of the DyNet-L model under high noise conditions (σ = 50), focusing on image restoration quality."
      },
      "confidence_score": 1.0,
      "reasoning": "The citation mentions 'Urban100' which is a known dataset used for image restoration tasks. The context indicates that the dataset is used to evaluate the performance of the DyNet-L model under high noise conditions.",
      "processing_time": 55.316916942596436,
      "citing_paper_id": "268856875",
      "cited_paper_id": 8282555
    },
    {
      "context_text": "Table 2: Dehazing results in the single-task setting on the SOTS benchmark dataset [25].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "SOTS"
      ],
      "dataset_descriptions": {
        "SOTS": "Used to evaluate dehazing algorithms in a single-task setting, focusing on image quality and restoration performance."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions the SOTS benchmark dataset, which is a specific dataset used for evaluating dehazing algorithms.",
      "processing_time": 51.026840925216675,
      "citing_paper_id": "268856875",
      "cited_paper_id": 39760169
    },
    {
      "context_text": "S4: Comparative analysis of image dehazing by all-in-one methods on the SOTS dataset [25].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "SOTS"
      ],
      "dataset_descriptions": {
        "SOTS": "Used to compare the performance of all-in-one image dehazing methods, focusing on the effectiveness of different approaches in removing haze from images."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions the SOTS dataset, which is a specific dataset used for benchmarking single-image dehazing methods.",
      "processing_time": 52.01850390434265,
      "citing_paper_id": "268856875",
      "cited_paper_id": 39760169
    },
    {
      "context_text": "Dehazing employs the SOTS [25] dataset, featuring 72,135 training and 500 testing images.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "SOTS"
      ],
      "dataset_descriptions": {
        "SOTS": "Used for benchmarking single-image dehazing methods, featuring 72,135 training and 500 testing images to evaluate dehazing performance."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions the SOTS dataset, which is used for dehazing experiments. The dataset is clearly identified and its usage is described.",
      "processing_time": 53.6405885219574,
      "citing_paper_id": "268856875",
      "cited_paper_id": 39760169
    },
    {
      "context_text": "7: Comparative analysis of image dehazing by all-in-one methods on the SOTS dataset [25].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "SOTS"
      ],
      "dataset_descriptions": {
        "SOTS": "Used to compare the performance of all-in-one image dehazing methods, focusing on the effectiveness of different techniques in removing haze from images."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions the SOTS dataset, which is a specific dataset used for benchmarking single-image dehazing methods.",
      "processing_time": 52.0122504234314,
      "citing_paper_id": "268856875",
      "cited_paper_id": 39760169
    },
    {
      "context_text": "DehazeNet [6] MSCNN [39] AODNet [24] EPDN [37] FDGAN [12] AirNet [26] Restormer [52] PromptIR [36] in removing rain and haze, respectively, producing more cleaner images than PromptIR [36].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions several methods/models (DehazeNet, MSCNN, AODNet, EPDN, FDGAN, AirNet, Restormer, PromptIR) but does not refer to any specific datasets. The focus is on comparing the performance of these methods in image restoration tasks.",
      "processing_time": 51.48492479324341,
      "citing_paper_id": "268856875",
      "cited_paper_id": 195489762
    },
    {
      "context_text": "DehazeNet [6] MSCNN [39] AODNet [24] EPDN [37] FDGAN [12] AirNet [26] Restormer [52] PromptIR [36] in removing rain and haze, respectively, producing more cleaner images than PromptIR [36].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions several methods/models (DehazeNet, MSCNN, AODNet, EPDN, FDGAN, AirNet, Restormer, PromptIR) but does not refer to any specific datasets. The focus is on comparing the performance of these methods in image restoration tasks.",
      "processing_time": 51.48492479324341,
      "citing_paper_id": "268856875",
      "cited_paper_id": 244346144
    },
    {
      "context_text": "DehazeNet [6] MSCNN [39] AODNet [24] EPDN [37] FDGAN [12] AirNet [26] Restormer [52] PromptIR [36] in removing rain and haze, respectively, producing more cleaner images than PromptIR [36].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions several methods/models (DehazeNet, MSCNN, AODNet, EPDN, FDGAN, AirNet, Restormer, PromptIR) but does not refer to any specific datasets. The focus is on comparing the performance of these methods in image restoration tasks.",
      "processing_time": 51.48492479324341,
      "citing_paper_id": "268856875",
      "cited_paper_id": 250551851
    },
    {
      "context_text": "We benchmark our DyNet against a range of general image restoration models [12,17,44,54] and specific all-in-one solutions [16,26,36], as shown in Table 1.",
      "catation_intent": "reusable resource",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only models and methods. The cited papers do not provide additional dataset information.",
      "processing_time": 45.48274207115173,
      "citing_paper_id": "268856875",
      "cited_paper_id": 198185751
    },
    {
      "context_text": "We benchmark our DyNet against a range of general image restoration models [12,17,44,54] and specific all-in-one solutions [16,26,36], as shown in Table 1.",
      "catation_intent": "reusable resource",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only models and methods. The cited papers do not provide additional dataset information.",
      "processing_time": 45.48274207115173,
      "citing_paper_id": "268856875",
      "cited_paper_id": 250551851
    },
    {
      "context_text": "For deraining, we use Rain100L [50] with 200 training and 100 testing clean-rainy image pairs.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Rain100L"
      ],
      "dataset_descriptions": {
        "Rain100L": "Used for training and testing deraining models, specifically with 200 training and 100 testing clean-rainy image pairs."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions a specific dataset, Rain100L, which is used for training and testing deraining models. The dataset is clearly identified and its usage is described.",
      "processing_time": 54.579097747802734,
      "citing_paper_id": "268856875",
      "cited_paper_id": 219530930
    },
    {
      "context_text": "However, opting for lightweight models introduces an inevitable accuracy-efficiency trade-off [56].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a trade-off in model efficiency and accuracy.",
      "processing_time": 44.12845802307129,
      "citing_paper_id": "268856875",
      "cited_paper_id": 221703405
    },
    {
      "context_text": "However, choosing lightweight models invariably leads to a trade-off between accuracy and efficiency [28,56].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a trade-off between accuracy and efficiency in lightweight models.",
      "processing_time": 44.614487171173096,
      "citing_paper_id": "268856875",
      "cited_paper_id": 221703405
    },
    {
      "context_text": "The IPT model [7], for instance, relies on prior knowledge of the corruption affecting the input image, utilizing a pre-trained transformer backbone equipped with distinct encoders and decoder heads tailored for multi-task restoration.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a model (IPT) and its components. No verifiable resources are identified.",
      "processing_time": 46.14209294319153,
      "citing_paper_id": "268856875",
      "cited_paper_id": 227239228
    },
    {
      "context_text": "We combine the existing high-quality, high-resolution natural image datasets such as LSDIR [27], DIV2K [2], Flickr2K [35], and NTIRE [4].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "LSDIR",
        "DIV2K",
        "Flickr2K",
        "NTIRE"
      ],
      "dataset_descriptions": {
        "LSDIR": "Used to provide high-quality, high-resolution natural images for training and evaluation in image restoration tasks.",
        "DIV2K": "Used to provide high-quality, high-resolution natural images for training and evaluation in image restoration tasks.",
        "Flickr2K": "Used to provide high-quality, high-resolution natural images for training and evaluation in image restoration tasks.",
        "NTIRE": "Used to provide high-quality, high-resolution natural images for training and evaluation in image restoration tasks."
      },
      "confidence_score": 1.0,
      "reasoning": "The citation mentions specific datasets used for high-quality, high-resolution natural images, which are relevant to the topic of image restoration.",
      "processing_time": 67.02686929702759,
      "citing_paper_id": "268856875",
      "cited_paper_id": 235703693
    },
    {
      "context_text": "Dataset NTIRE [4] DIV2K [2] Flikr2K [35] LSDIR [27] Laion-HR [42] #Images 56 2,000 2,000 84,991 2M Fig.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "DIV2K",
        "Flikr2K",
        "LSDIR",
        "Laion-HR"
      ],
      "dataset_descriptions": {
        "DIV2K": "Used for training and evaluation in image restoration, containing 2,000 high-resolution images for dehazing and other restoration tasks.",
        "Flikr2K": "Used for training and evaluation in image restoration, containing 2,000 high-resolution images from Flickr for dehazing and other restoration tasks.",
        "LSDIR": "Used for training and evaluation in image restoration, containing 84,991 high-resolution images for dehazing and other restoration tasks.",
        "Laion-HR": "Used for training and evaluation in image restoration, containing 2 million high-resolution images for dehazing and other restoration tasks."
      },
      "confidence_score": 1.0,
      "reasoning": "The citation mentions multiple datasets used for image restoration, providing a clear count of images for each. These datasets are specific and relevant to the research topic.",
      "processing_time": 75.57086443901062,
      "citing_paper_id": "268856875",
      "cited_paper_id": 235703693
    },
    {
      "context_text": "DyNet comprises four encoder-decoder levels, with the bulky variant (DyNet-L) employing reuse frequencies of f = [4 , 6 , 6 , 8] at encoder-decoder level 1 to level 4, respectively.",
      "catation_intent": "none",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only model architecture details. No dataset names are present in the text.",
      "processing_time": 45.21446990966797,
      "citing_paper_id": "268856875",
      "cited_paper_id": 235703693
    },
    {
      "context_text": "For the encoder-decoder levels 1 to 4, we set transformer block weights reuse frequencies of [4,6,6,8] for DyNet-L and [2,3,3,4] for DyNet-S.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only model configurations. The cited paper titles do not provide additional context to identify datasets.",
      "processing_time": 45.646050453186035,
      "citing_paper_id": "268856875",
      "cited_paper_id": 235703693
    },
    {
      "context_text": "For the encoder-decoder levels 1 to 4, we set transformer block weights reuse frequencies of [4,6,6,8] for DyNet-L and [2,3,3,4] for DyNet-S.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only model configurations. The cited paper titles do not provide additional context to identify datasets.",
      "processing_time": 45.646050453186035,
      "citing_paper_id": "268856875",
      "cited_paper_id": 264307339
    },
    {
      "context_text": "More importantly, we are motivated by the scaling laws in high-level tasks that demonstrate the large-scale pre-taining can enable even lightweight, efficient model designs to reach the performance mark of much heavier models [1].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only the concept of large-scale pre-training. There are no clear identifiers for datasets in the given context.",
      "processing_time": 46.84310507774353,
      "citing_paper_id": "268856875",
      "cited_paper_id": 238354065
    },
    {
      "context_text": "Moreover, we improve the generalization capability of our DyNet variants by adopting an input masking strategy akin to that proposed in masked auto-encoders [19].",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (masked auto-encoders).",
      "processing_time": 44.59362435340881,
      "citing_paper_id": "268856875",
      "cited_paper_id": 243985980
    },
    {
      "context_text": "The majority of existing methods [3,8,13–15,31,46,47,52] learn image priors implicitly, requiring separate network training for different degradation types, levels, and datasets.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general references to datasets. No multi-word proper nouns, acronyms, or hyphenated names with digits are present.",
      "processing_time": 48.408414125442505,
      "citing_paper_id": "268856875",
      "cited_paper_id": 244346144
    },
    {
      "context_text": "The majority of existing methods [3,8,13–15,31,46,47,52] learn image priors implicitly, requiring separate network training for different degradation types, levels, and datasets.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general references to datasets. No multi-word proper nouns, acronyms, or hyphenated names with digits are present.",
      "processing_time": 48.408414125442505,
      "citing_paper_id": "268856875",
      "cited_paper_id": 264307339
    },
    {
      "context_text": "At each encoder-decoder level, we initialize weights ( w 1 ) for the first transformer block [52] and reuse them for the subsequent blocks.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only the initialization of weights for transformer blocks.",
      "processing_time": 44.3536012172699,
      "citing_paper_id": "268856875",
      "cited_paper_id": 244346144
    },
    {
      "context_text": "2 dB over Restormer [52] in dehazing.",
      "catation_intent": "findings",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a comparison in performance with another method (Restormer).",
      "processing_time": 45.45066976547241,
      "citing_paper_id": "268856875",
      "cited_paper_id": 244346144
    },
    {
      "context_text": "We utilize the existing Restormer’s [52] Transformer block as a fundamental feature extraction module.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions Restormer but does not refer to it as a dataset. It is described as a method or model, not a dataset.",
      "processing_time": 46.473045349121094,
      "citing_paper_id": "268856875",
      "cited_paper_id": 244346144
    },
    {
      "context_text": "Recent advancements, such as AirNet [26] and PromptIR [36], have addressed the all-in-one restoration challenge by employing contrastive learning and implicit visual prompting techniques, respectively.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions methods (AirNet, PromptIR) but does not reference any specific datasets. The context focuses on methodologies rather than datasets.",
      "processing_time": 46.459028244018555,
      "citing_paper_id": "268856875",
      "cited_paper_id": 250551851
    },
    {
      "context_text": "[ 26] propose a unified network for multiple tasks such as denoising, deraining, and dehazing, employing an image encoder refined through contrastive learning.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for image restoration. No dataset names are provided in the context.",
      "processing_time": 45.87773370742798,
      "citing_paper_id": "268856875",
      "cited_paper_id": 250551851
    },
    {
      "context_text": "64 dB gain compared to AirNet [26].",
      "catation_intent": "none",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a comparison to AirNet, which appears to be a method or model.",
      "processing_time": 31.762407302856445,
      "citing_paper_id": "268856875",
      "cited_paper_id": 250551851
    },
    {
      "context_text": "Following [26, 36], we prepare datasets for various restoration tasks.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.2,
      "reasoning": "The context does not provide specific dataset names, only a generic reference to 'datasets'. No clear, verifiable datasets are mentioned.",
      "processing_time": 46.45795440673828,
      "citing_paper_id": "268856875",
      "cited_paper_id": 250551851
    },
    {
      "context_text": "76 dB over PromptIR [26].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a reference to another paper. No verifiable resources are identified.",
      "processing_time": 45.6103994846344,
      "citing_paper_id": "268856875",
      "cited_paper_id": 250551851
    },
    {
      "context_text": "…the frequency domain to reduce the computational demands linked to attention mechanisms [62] or non-local operations [18] while few methods like [9, 49] partition the feature space for efficient processing, or split the attention across dimensions to improve the computational efficiency [9,61].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods and approaches for efficient image restoration.",
      "processing_time": 44.814016580581665,
      "citing_paper_id": "268856875",
      "cited_paper_id": 260397544
    },
    {
      "context_text": "Furthermore, methods like [9, 49] partition the feature space for efficient processing, while [9, 61] split the attention across dimensions to improve the computational efficiency.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and approaches. There are no clear identifiers for datasets in the provided context.",
      "processing_time": 46.449532985687256,
      "citing_paper_id": "268856875",
      "cited_paper_id": 260397544
    },
    {
      "context_text": "For Setting III, we use RESIDE [21] for dehazing, WED [39] and BSD [41] for denoising, Rain100L [74] for deraining.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "RESIDE",
        "WED",
        "BSD",
        "Rain100L"
      ],
      "dataset_descriptions": {
        "RESIDE": "Used for dehazing in image restoration, providing a benchmark for evaluating dehazing algorithms.",
        "WED": "Used for denoising in image restoration, offering challenges for image quality assessment models.",
        "BSD": "Used for denoising in image restoration, providing a benchmark for evaluating denoising algorithms.",
        "Rain100L": "Used for deraining in image restoration, providing a dataset for evaluating deraining algorithms."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions specific datasets used for different image restoration tasks. RESIDE is used for dehazing, WED and BSD for denoising, and Rain100L for deraining. These datasets are clearly identified and their usage is specified.",
      "processing_time": 70.31846475601196,
      "citing_paper_id": "273501673",
      "cited_paper_id": 64193
    },
    {
      "context_text": "For Setting III, we use RESIDE [21] for dehazing, WED [39] and BSD [41] for denoising, Rain100L [74] for deraining.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "RESIDE",
        "WED",
        "BSD",
        "Rain100L"
      ],
      "dataset_descriptions": {
        "RESIDE": "Used for dehazing in image restoration, providing a benchmark for evaluating dehazing algorithms.",
        "WED": "Used for denoising in image restoration, offering challenges for image quality assessment models.",
        "BSD": "Used for denoising in image restoration, providing a benchmark for evaluating denoising algorithms.",
        "Rain100L": "Used for deraining in image restoration, providing a dataset for evaluating deraining algorithms."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions specific datasets used for different image restoration tasks. RESIDE is used for dehazing, WED and BSD for denoising, and Rain100L for deraining. These datasets are clearly identified and their usage is specified.",
      "processing_time": 70.31846475601196,
      "citing_paper_id": "273501673",
      "cited_paper_id": 4840263
    },
    {
      "context_text": "For Setting III, we use RESIDE [21] for dehazing, WED [39] and BSD [41] for denoising, Rain100L [74] for deraining.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "RESIDE",
        "WED",
        "BSD",
        "Rain100L"
      ],
      "dataset_descriptions": {
        "RESIDE": "Used for dehazing in image restoration, providing a benchmark for evaluating dehazing algorithms.",
        "WED": "Used for denoising in image restoration, offering challenges for image quality assessment models.",
        "BSD": "Used for denoising in image restoration, providing a benchmark for evaluating denoising algorithms.",
        "Rain100L": "Used for deraining in image restoration, providing a dataset for evaluating deraining algorithms."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions specific datasets used for different image restoration tasks. RESIDE is used for dehazing, WED and BSD for denoising, and Rain100L for deraining. These datasets are clearly identified and their usage is specified.",
      "processing_time": 70.31846475601196,
      "citing_paper_id": "273501673",
      "cited_paper_id": 39760169
    },
    {
      "context_text": "For Setting IV, we use a merged dataset [78, 89] for de-raining, LOL [68], DCIE [19], MEF [38], and NPE [62] for low-light enhancement, Snow100K [34] for desnowing, RESIDE [21] for dehazing, GoPro [44], HIDE [55], Real-Blur [52] for deblurring.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "LOL",
        "DCIE",
        "MEF",
        "NPE",
        "Snow100K",
        "RESIDE",
        "GoPro",
        "HIDE",
        "Real-Blur"
      ],
      "dataset_descriptions": {
        "LOL": "Used for low-light enhancement, focusing on improving image quality in low-light conditions through deep learning techniques.",
        "DCIE": "Used for low-light enhancement, specifically enhancing contrast and color in dark images using layered difference representation.",
        "MEF": "Used for low-light enhancement, focusing on multi-exposure fusion to improve image quality in low-light scenarios.",
        "NPE": "Used for low-light enhancement, specifically addressing noise reduction and detail preservation in low-light images.",
        "Snow100K": "Used for desnowing, focusing on removing snow artifacts from images using a large-scale dataset of snowy scenes.",
        "RESIDE": "Used for dehazing, focusing on improving visibility in hazy images through a comprehensive benchmark dataset.",
        "GoPro": "Used for deblurring, focusing on motion blur correction in high-speed video sequences.",
        "HIDE": "Used for deblurring, specifically addressing real-world motion blur in images and videos.",
        "Real-Blur": "Used for deblurring, focusing on realistic motion blur correction using a dataset of real-world blurred images."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions several datasets used for various image restoration tasks, including de-raining, low-light enhancement, desnowing, dehazing, and deblurring. These datasets are clearly identified and used for training and evaluation.",
      "processing_time": 97.40021848678589,
      "citing_paper_id": "273501673",
      "cited_paper_id": 837707
    },
    {
      "context_text": "For Setting IV, we use a merged dataset [78, 89] for de-raining, LOL [68], DCIE [19], MEF [38], and NPE [62] for low-light enhancement, Snow100K [34] for desnowing, RESIDE [21] for dehazing, GoPro [44], HIDE [55], Real-Blur [52] for deblurring.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "LOL",
        "DCIE",
        "MEF",
        "NPE",
        "Snow100K",
        "RESIDE",
        "GoPro",
        "HIDE",
        "Real-Blur"
      ],
      "dataset_descriptions": {
        "LOL": "Used for low-light enhancement, focusing on improving image quality in low-light conditions through deep learning techniques.",
        "DCIE": "Used for low-light enhancement, specifically enhancing contrast and color in dark images using layered difference representation.",
        "MEF": "Used for low-light enhancement, focusing on multi-exposure fusion to improve image quality in low-light scenarios.",
        "NPE": "Used for low-light enhancement, specifically addressing noise reduction and detail preservation in low-light images.",
        "Snow100K": "Used for desnowing, focusing on removing snow artifacts from images using a large-scale dataset of snowy scenes.",
        "RESIDE": "Used for dehazing, focusing on improving visibility in hazy images through a comprehensive benchmark dataset.",
        "GoPro": "Used for deblurring, focusing on motion blur correction in high-speed video sequences.",
        "HIDE": "Used for deblurring, specifically addressing real-world motion blur in images and videos.",
        "Real-Blur": "Used for deblurring, focusing on realistic motion blur correction using a dataset of real-world blurred images."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions several datasets used for various image restoration tasks, including de-raining, low-light enhancement, desnowing, dehazing, and deblurring. These datasets are clearly identified and used for training and evaluation.",
      "processing_time": 97.40021848678589,
      "citing_paper_id": "273501673",
      "cited_paper_id": 18235798
    },
    {
      "context_text": "For Setting IV, we use a merged dataset [78, 89] for de-raining, LOL [68], DCIE [19], MEF [38], and NPE [62] for low-light enhancement, Snow100K [34] for desnowing, RESIDE [21] for dehazing, GoPro [44], HIDE [55], Real-Blur [52] for deblurring.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "LOL",
        "DCIE",
        "MEF",
        "NPE",
        "Snow100K",
        "RESIDE",
        "GoPro",
        "HIDE",
        "Real-Blur"
      ],
      "dataset_descriptions": {
        "LOL": "Used for low-light enhancement, focusing on improving image quality in low-light conditions through deep learning techniques.",
        "DCIE": "Used for low-light enhancement, specifically enhancing contrast and color in dark images using layered difference representation.",
        "MEF": "Used for low-light enhancement, focusing on multi-exposure fusion to improve image quality in low-light scenarios.",
        "NPE": "Used for low-light enhancement, specifically addressing noise reduction and detail preservation in low-light images.",
        "Snow100K": "Used for desnowing, focusing on removing snow artifacts from images using a large-scale dataset of snowy scenes.",
        "RESIDE": "Used for dehazing, focusing on improving visibility in hazy images through a comprehensive benchmark dataset.",
        "GoPro": "Used for deblurring, focusing on motion blur correction in high-speed video sequences.",
        "HIDE": "Used for deblurring, specifically addressing real-world motion blur in images and videos.",
        "Real-Blur": "Used for deblurring, focusing on realistic motion blur correction using a dataset of real-world blurred images."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions several datasets used for various image restoration tasks, including de-raining, low-light enhancement, desnowing, dehazing, and deblurring. These datasets are clearly identified and used for training and evaluation.",
      "processing_time": 97.40021848678589,
      "citing_paper_id": "273501673",
      "cited_paper_id": 39760169
    },
    {
      "context_text": "For Setting IV, we use a merged dataset [78, 89] for de-raining, LOL [68], DCIE [19], MEF [38], and NPE [62] for low-light enhancement, Snow100K [34] for desnowing, RESIDE [21] for dehazing, GoPro [44], HIDE [55], Real-Blur [52] for deblurring.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "LOL",
        "DCIE",
        "MEF",
        "NPE",
        "Snow100K",
        "RESIDE",
        "GoPro",
        "HIDE",
        "Real-Blur"
      ],
      "dataset_descriptions": {
        "LOL": "Used for low-light enhancement, focusing on improving image quality in low-light conditions through deep learning techniques.",
        "DCIE": "Used for low-light enhancement, specifically enhancing contrast and color in dark images using layered difference representation.",
        "MEF": "Used for low-light enhancement, focusing on multi-exposure fusion to improve image quality in low-light scenarios.",
        "NPE": "Used for low-light enhancement, specifically addressing noise reduction and detail preservation in low-light images.",
        "Snow100K": "Used for desnowing, focusing on removing snow artifacts from images using a large-scale dataset of snowy scenes.",
        "RESIDE": "Used for dehazing, focusing on improving visibility in hazy images through a comprehensive benchmark dataset.",
        "GoPro": "Used for deblurring, focusing on motion blur correction in high-speed video sequences.",
        "HIDE": "Used for deblurring, specifically addressing real-world motion blur in images and videos.",
        "Real-Blur": "Used for deblurring, focusing on realistic motion blur correction using a dataset of real-world blurred images."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions several datasets used for various image restoration tasks, including de-raining, low-light enhancement, desnowing, dehazing, and deblurring. These datasets are clearly identified and used for training and evaluation.",
      "processing_time": 97.40021848678589,
      "citing_paper_id": "273501673",
      "cited_paper_id": 52008443
    },
    {
      "context_text": "In recent years, significant progress has been achieved with specialized restoration networks targeting specific degradations [7, 58, 70, 83].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to specialized restoration networks. No dataset names are provided in the context.",
      "processing_time": 46.44034671783447,
      "citing_paper_id": "273501673",
      "cited_paper_id": 996788
    },
    {
      "context_text": "In recent years, significant progress has been achieved with specialized restoration networks targeting specific degradations [7, 58, 70, 83].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to specialized restoration networks. No dataset names are provided in the context.",
      "processing_time": 46.44034671783447,
      "citing_paper_id": "273501673",
      "cited_paper_id": 248085101
    },
    {
      "context_text": "In recent years, significant progress has been achieved with specialized restoration networks targeting specific degradations [7, 58, 70, 83].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to specialized restoration networks. No dataset names are provided in the context.",
      "processing_time": 46.44034671783447,
      "citing_paper_id": "273501673",
      "cited_paper_id": 257636509
    },
    {
      "context_text": "For benchmarks that do not include ground truth images, we use NIQE [42], LOE [61] and IL-NIQE [85] as no-reference metrics.",
      "catation_intent": "reusable resource",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions no-reference metrics (NIQE, LOE, IL-NIQE) but does not specify any datasets. These metrics are used for evaluating image quality without ground truth images, which is relevant to the topic of image restoration.",
      "processing_time": 50.58278703689575,
      "citing_paper_id": "273501673",
      "cited_paper_id": 2972940
    },
    {
      "context_text": "For benchmarks that do not include ground truth images, we use NIQE [42], LOE [61] and IL-NIQE [85] as no-reference metrics.",
      "catation_intent": "reusable resource",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions no-reference metrics (NIQE, LOE, IL-NIQE) but does not specify any datasets. These metrics are used for evaluating image quality without ground truth images, which is relevant to the topic of image restoration.",
      "processing_time": 50.58278703689575,
      "citing_paper_id": "273501673",
      "cited_paper_id": 262546756
    },
    {
      "context_text": "RainDrop [49] PSNR SSIM pix2pix [16] 28.02 0.8547 DuRN [32] 31.24 0.9259 RaindropAttn [50] 31.44 0.9263 AttentiveGAN [49] 31.59 0.9170 IDT [72] 31.87 0.9313 MAXIM [59] 31.87 0.9352 Restormer [78] 32.18 0.9408 All-in-One [24] 31.12 0.9268 TransWeather [60] 30.17 0.9157 Chen et al .",
      "catation_intent": "findings",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only models and their performance metrics. No verifiable resources are identified.",
      "processing_time": 45.8368079662323,
      "citing_paper_id": "273501673",
      "cited_paper_id": 4539586
    },
    {
      "context_text": "RainDrop [49] PSNR SSIM pix2pix [16] 28.02 0.8547 DuRN [32] 31.24 0.9259 RaindropAttn [50] 31.44 0.9263 AttentiveGAN [49] 31.59 0.9170 IDT [72] 31.87 0.9313 MAXIM [59] 31.87 0.9352 Restormer [78] 32.18 0.9408 All-in-One [24] 31.12 0.9268 TransWeather [60] 30.17 0.9157 Chen et al .",
      "catation_intent": "findings",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only models and their performance metrics. No verifiable resources are identified.",
      "processing_time": 45.8368079662323,
      "citing_paper_id": "273501673",
      "cited_paper_id": 6200260
    },
    {
      "context_text": "RainDrop [49] PSNR SSIM pix2pix [16] 28.02 0.8547 DuRN [32] 31.24 0.9259 RaindropAttn [50] 31.44 0.9263 AttentiveGAN [49] 31.59 0.9170 IDT [72] 31.87 0.9313 MAXIM [59] 31.87 0.9352 Restormer [78] 32.18 0.9408 All-in-One [24] 31.12 0.9268 TransWeather [60] 30.17 0.9157 Chen et al .",
      "catation_intent": "findings",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only models and their performance metrics. No verifiable resources are identified.",
      "processing_time": 45.8368079662323,
      "citing_paper_id": "273501673",
      "cited_paper_id": 244714491
    },
    {
      "context_text": "RainDrop [49] PSNR SSIM pix2pix [16] 28.02 0.8547 DuRN [32] 31.24 0.9259 RaindropAttn [50] 31.44 0.9263 AttentiveGAN [49] 31.59 0.9170 IDT [72] 31.87 0.9313 MAXIM [59] 31.87 0.9352 Restormer [78] 32.18 0.9408 All-in-One [24] 31.12 0.9268 TransWeather [60] 30.17 0.9157 Chen et al .",
      "catation_intent": "findings",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only models and their performance metrics. No verifiable resources are identified.",
      "processing_time": 45.8368079662323,
      "citing_paper_id": "273501673",
      "cited_paper_id": 245837508
    },
    {
      "context_text": "[16] 19.09 0.7100 HRGAN [23] 21.56 0.8550 PCNet [17] 26.19 0.9015 MPRNet[77] 28.03 0.9192 NAFNet [4] 29.59 0.9027 Restormer [78] 30.03 0.9215 All-in-One [24] 24.71 0.8980 TransWeather [60] 28.83 0.9000 Chen et al .",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only model names and performance metrics. No verifiable resources are identified.",
      "processing_time": 46.070317029953,
      "citing_paper_id": "273501673",
      "cited_paper_id": 6200260
    },
    {
      "context_text": "[16] 19.09 0.7100 HRGAN [23] 21.56 0.8550 PCNet [17] 26.19 0.9015 MPRNet[77] 28.03 0.9192 NAFNet [4] 29.59 0.9027 Restormer [78] 30.03 0.9215 All-in-One [24] 24.71 0.8980 TransWeather [60] 28.83 0.9000 Chen et al .",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only model names and performance metrics. No verifiable resources are identified.",
      "processing_time": 46.070317029953,
      "citing_paper_id": "273501673",
      "cited_paper_id": 244714491
    },
    {
      "context_text": "[16] 19.09 0.7100 HRGAN [23] 21.56 0.8550 PCNet [17] 26.19 0.9015 MPRNet[77] 28.03 0.9192 NAFNet [4] 29.59 0.9027 Restormer [78] 30.03 0.9215 All-in-One [24] 24.71 0.8980 TransWeather [60] 28.83 0.9000 Chen et al .",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only model names and performance metrics. No verifiable resources are identified.",
      "processing_time": 46.070317029953,
      "citing_paper_id": "273501673",
      "cited_paper_id": 248085491
    },
    {
      "context_text": "However, in practical applications like autonomous driving and outdoor surveillance [40, 96], images are often simultaneously affected by multiple complex degradations, including haze, rain, low-light conditions, motion blur, etc .",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only general application areas and types of image degradations.",
      "processing_time": 45.15004873275757,
      "citing_paper_id": "273501673",
      "cited_paper_id": 29436769
    },
    {
      "context_text": "DAM adopts a structure similar to the channel attention block [88], modulating degradation information along the channel dimension, which can be formulated as where ⊙ denotes the channel-wise multiplication, MLP shared denotes the MLP projector shared across different blocks, LN denotes LayerNorm,…",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (channel attention block) used in the context of image super-resolution.",
      "processing_time": 46.24418663978577,
      "citing_paper_id": "273501673",
      "cited_paper_id": 49657846
    },
    {
      "context_text": "[Setting IV] Visual results on HIDE [55] for training-seen tasks generalization evaluation (top row) and TOLED [92] for training-unseen tasks generalization evaluation (bottom row).",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "HIDE",
        "TOLED"
      ],
      "dataset_descriptions": {
        "HIDE": "Used for evaluating generalization on training-seen tasks in image restoration, focusing on under-display camera images.",
        "TOLED": "Used for evaluating generalization on training-unseen tasks in image restoration, focusing on under-display camera images."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions two specific datasets, HIDE and TOLED, which are used for evaluating the generalization of image restoration tasks.",
      "processing_time": 56.72831630706787,
      "citing_paper_id": "273501673",
      "cited_paper_id": 212647851
    },
    {
      "context_text": "Among these, prompt learning [20, 90] and Low-Rank Adaptation (LoRA) [15] are two prominent and widely used PEFT methods.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only methods (prompt learning and LoRA). No verifiable resources are identified.",
      "processing_time": 46.43653059005737,
      "citing_paper_id": "273501673",
      "cited_paper_id": 233296808
    },
    {
      "context_text": "Among these, prompt learning [20, 90] and Low-Rank Adaptation (LoRA) [15] are two prominent and widely used PEFT methods.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only methods (prompt learning and LoRA). No verifiable resources are identified.",
      "processing_time": 46.43653059005737,
      "citing_paper_id": "273501673",
      "cited_paper_id": 235458009
    },
    {
      "context_text": "Image restoration for known degradations has been extensively studied [4, 10, 26, 27, 65, 71, 77, 78, 82].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general references to image restoration studies. No verifiable resources are identified.",
      "processing_time": 46.41726350784302,
      "citing_paper_id": "273501673",
      "cited_paper_id": 235358213
    },
    {
      "context_text": "Image restoration for known degradations has been extensively studied [4, 10, 26, 27, 65, 71, 77, 78, 82].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general references to image restoration studies. No verifiable resources are identified.",
      "processing_time": 46.41726350784302,
      "citing_paper_id": "273501673",
      "cited_paper_id": 237266491
    },
    {
      "context_text": "Image restoration for known degradations has been extensively studied [4, 10, 26, 27, 65, 71, 77, 78, 82].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general references to image restoration studies. No verifiable resources are identified.",
      "processing_time": 46.41726350784302,
      "citing_paper_id": "273501673",
      "cited_paper_id": 248085491
    },
    {
      "context_text": "Image restoration for known degradations has been extensively studied [4, 10, 26, 27, 65, 71, 77, 78, 82].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general references to image restoration studies. No verifiable resources are identified.",
      "processing_time": 46.41726350784302,
      "citing_paper_id": "273501673",
      "cited_paper_id": 252693111
    },
    {
      "context_text": "Image restoration for known degradations has been extensively studied [4, 10, 26, 27, 65, 71, 77, 78, 82].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general references to image restoration studies. No verifiable resources are identified.",
      "processing_time": 46.41726350784302,
      "citing_paper_id": "273501673",
      "cited_paper_id": 257255385
    },
    {
      "context_text": "Motivated by the success of Low-Rank Adaptation (LoRA) [15] in parameter-efficient fine-tuning, we explore the use of diverse low-rank experts to model degradation characteristics and correlations efficiently.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a method (LoRA) for parameter-efficient fine-tuning. No datasets are referenced for training or evaluation.",
      "processing_time": 47.83975791931152,
      "citing_paper_id": "273501673",
      "cited_paper_id": 235458009
    },
    {
      "context_text": "† Corresponding author.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only a general reference to semi-supervised image deraining. No clear, verifiable dataset names are present.",
      "processing_time": 48.128289222717285,
      "citing_paper_id": "273501673",
      "cited_paper_id": 235702840
    },
    {
      "context_text": "For Setting I, we use the All-Weather [56, 60] dataset to evaluate our method.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "All-Weather"
      ],
      "dataset_descriptions": {
        "All-Weather": "Used to evaluate the method's performance in restoring images degraded by adverse weather conditions, focusing on the effectiveness of the restoration algorithm."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions the 'All-Weather' dataset, which is a specific dataset used for evaluating image restoration methods under adverse weather conditions.",
      "processing_time": 53.191436529159546,
      "citing_paper_id": "273501673",
      "cited_paper_id": 244714491
    },
    {
      "context_text": "Recently, there has been significant interest in all-in-one frameworks within the community [9, 60, 81, 95].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only a general interest in all-in-one frameworks. No verifiable resources are identified.",
      "processing_time": 47.149604082107544,
      "citing_paper_id": "273501673",
      "cited_paper_id": 244714491
    },
    {
      "context_text": "Recently, there has been significant interest in all-in-one frameworks within the community [9, 60, 81, 95].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only a general interest in all-in-one frameworks. No verifiable resources are identified.",
      "processing_time": 47.149604082107544,
      "citing_paper_id": "273501673",
      "cited_paper_id": 267320695
    },
    {
      "context_text": "To comprehensively validate our method, we conduct experiments on the All-Weather [60] and mixed-degradation [45, 91] benchmarks.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "All-Weather"
      ],
      "dataset_descriptions": {
        "All-Weather": "Used to validate the method's effectiveness in restoring images degraded by adverse weather conditions, focusing on the performance under various weather scenarios."
      },
      "confidence_score": 0.8,
      "reasoning": "The citation mentions 'All-Weather' and 'mixed-degradation' benchmarks, which are likely datasets used for validating image restoration methods. However, 'mixed-degradation' is too generic and lacks a specific identifier.",
      "processing_time": 55.55616235733032,
      "citing_paper_id": "273501673",
      "cited_paper_id": 244714491
    },
    {
      "context_text": "4 are the simple convolutional NAFBlocks [4], forming a simple all-in-one CNN baseline.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (NAFBlocks) used in a CNN baseline for image restoration.",
      "processing_time": 46.745179891586304,
      "citing_paper_id": "273501673",
      "cited_paper_id": 248085491
    },
    {
      "context_text": "…removal; (II) 3-task real-world adverse weather removal [73], including deraining, dehazing, and desnow-ing; (III) 3-task image restoration [22], including derain-ing, dehazing, and denoising; (IV) 5-task image restoration [89], including deraining, low-light enhancement, desnowing,…",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions multiple tasks related to image restoration but does not specify any named datasets. The tasks are described generically without referencing specific datasets.",
      "processing_time": 47.41252279281616,
      "citing_paper_id": "273501673",
      "cited_paper_id": 250551851
    },
    {
      "context_text": "AirNet [22] is the pioneering work in all-in-one IR, utilizing contrastive learning to capture degradation information.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions AirNet as a pioneering work in all-in-one image restoration but does not refer to it as a dataset. It is described as a method or approach.",
      "processing_time": 48.603941440582275,
      "citing_paper_id": "273501673",
      "cited_paper_id": 250551851
    },
    {
      "context_text": "With the rise of large foundational models [1, 18, 51] in modern deep learning, the community has increasingly shifted its focus towards parameter-efficient fine-tuning (PEFT) methods for effective model adaptation.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only a shift in focus towards parameter-efficient fine-tuning methods in the context of large foundational models.",
      "processing_time": 47.82613754272461,
      "citing_paper_id": "273501673",
      "cited_paper_id": 257532815
    },
    {
      "context_text": "With the rise of large foundational models [1, 18, 51] in modern deep learning, the community has increasingly shifted its focus towards parameter-efficient fine-tuning (PEFT) methods for effective model adaptation.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only a shift in focus towards parameter-efficient fine-tuning methods in the context of large foundational models.",
      "processing_time": 47.82613754272461,
      "citing_paper_id": "273501673",
      "cited_paper_id": 257952310
    },
    {
      "context_text": "Among them, CLIP [51], as a powerful VLM, has demonstrated impressive zero-shot and few-shot capabilities across various high-level vision tasks [66, 67, 93].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a model (CLIP) and its capabilities. No verifiable resources are identified.",
      "processing_time": 47.120580196380615,
      "citing_paper_id": "273501673",
      "cited_paper_id": 259924765
    },
    {
      "context_text": "More recent state-of-the-art methods adopt prompt-based frameworks [2, 33, 36, 48] (Fig.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to methods and frameworks. The cited paper titles do not provide additional context about datasets.",
      "processing_time": 47.62703847885132,
      "citing_paper_id": "273501673",
      "cited_paper_id": 264146360
    },
    {
      "context_text": "More recent state-of-the-art methods adopt prompt-based frameworks [2, 33, 36, 48] (Fig.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to methods and frameworks. The cited paper titles do not provide additional context about datasets.",
      "processing_time": 47.62703847885132,
      "citing_paper_id": "273501673",
      "cited_paper_id": 268064460
    },
    {
      "context_text": "More recent state-of-the-art methods adopt prompt-based frameworks [2, 33, 36, 48] (Fig.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to methods and frameworks. The cited paper titles do not provide additional context about datasets.",
      "processing_time": 47.62703847885132,
      "citing_paper_id": "273501673",
      "cited_paper_id": 272724853
    },
    {
      "context_text": "As discussed in [69, 76], diffusion-based IR meth-ods typically have an advantage in no-reference metrics like NIQE.",
      "catation_intent": "findings",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a methodological advantage in no-reference metrics.",
      "processing_time": 45.34886431694031,
      "citing_paper_id": "273501673",
      "cited_paper_id": 267199774
    },
    {
      "context_text": "PromptIR [48] DACLIP-UIR [36] DiffUIR [89] LoRA-IR (Ours) GT Rainy&Hazy Input PromptIR [48] DACLIP-UIR [36] DiffUIR [89] LoRA-IR (Ours) GT Figure 6.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only model names and a figure reference. There is no indication of dataset usage.",
      "processing_time": 46.732444047927856,
      "citing_paper_id": "273501673",
      "cited_paper_id": 268064460
    },
    {
      "context_text": "Our method surpasses PromptIR [48] by 1.38 dB in PSNR on the Rain100L dataset, with an average improvement of 0.36 dB across the three tasks.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Rain100L"
      ],
      "dataset_descriptions": {
        "Rain100L": "Used to evaluate the performance of the proposed method against PromptIR, focusing on image restoration tasks, specifically measuring PSNR improvements."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions the Rain100L dataset, which is used to evaluate the performance of the method against PromptIR. The dataset is clearly identified and used for benchmarking.",
      "processing_time": 55.158671379089355,
      "citing_paper_id": "273501673",
      "cited_paper_id": 268064460
    },
    {
      "context_text": "PromptIR [48] DiffUIR [89] LoRA-IR (Ours) GT LQ Image Input PromptIR [48] DiffUIR [89] LoRA-IR (Ours) GT Figure 5.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only model names and a figure reference. There is no information about the usage of datasets.",
      "processing_time": 47.38870596885681,
      "citing_paper_id": "273501673",
      "cited_paper_id": 268064460
    },
    {
      "context_text": "PromptIR [48] proposes a plug-and-play prompt module to guide the restoration process.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method called PromptIR. The context is about a method or tool, not a dataset.",
      "processing_time": 47.80451798439026,
      "citing_paper_id": "273501673",
      "cited_paper_id": 268064460
    },
    {
      "context_text": "Compared to SOTA methods like MPerceiver [2] and Histoformer [56], our approach shows significant improvements across all benchmarks and metrics.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only comparisons to other methods. No verifiable resources are identified.",
      "processing_time": 46.17586302757263,
      "citing_paper_id": "273501673",
      "cited_paper_id": 272724853
    },
    {
      "context_text": "[6] 31.81 0.9309 WGWSNet [95] 32.38 0.9378 WeatherDiff 64 [46] 30.71 0.9312 WeatherDiff 128 [46] 29.66 0.9225 AWRCP [75] 31.93 0.9314 MPerceiver [2] 33.21 0.9294 Histoformer [56] 33.06 0.9441 LoRA-IR 33.39 0.9489",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only model names and performance metrics. There are no clear identifiers for datasets.",
      "processing_time": 47.10825705528259,
      "citing_paper_id": "273501673",
      "cited_paper_id": 272724853
    },
    {
      "context_text": "[6] 29.27 0.9147 WGWSNet [95] 29.32 0.9207 WeatherDiff 64 [46] 29.64 0.9312 WeatherDiff 128 [46] 29.72 0.9216 AWRCP [75] 31.39 0.9329 MPerceiver [2] 31.25 0.9246 Histoformer [56] 32.08 0.9389 LoRA-IR 32.62 0.9447",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only model names and performance metrics. There are no clear identifiers for datasets.",
      "processing_time": 47.37702465057373,
      "citing_paper_id": "273501673",
      "cited_paper_id": 272724853
    },
    {
      "context_text": "MPerceiver [2] introduces a multi-modal prompt learning approach to harness Stable Diffusion priors.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a method (MPerceiver) and a model (Stable Diffusion).",
      "processing_time": 47.79289484024048,
      "citing_paper_id": "273501673",
      "cited_paper_id": 272724853
    },
    {
      "context_text": "[44] and bottom row from LIVE1 [54]), which reveals significant loss of degradation information after processing.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "LIVE1"
      ],
      "dataset_descriptions": {
        "LIVE1": "Used to demonstrate the loss of degradation information after processing, focusing on image quality assessment and degradation analysis."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions 'LIVE1', which appears to be a dataset used for image quality assessment. It is used to demonstrate the loss of degradation information after processing.",
      "processing_time": 53.68574023246765,
      "citing_paper_id": "273501673",
      "cited_paper_id": null
    },
    {
      "context_text": "We can then move one step forward, the output feature map in layer i + 2 can be denoted as Substitute F ′ i +1 in Eq 9 with Eq 8, we can get 5], DnCNN [37], IRCNN [38], FFDNet [39], AirNet [18] and U-WADN, respectively.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only models and methods. No verifiable resources are identified.",
      "processing_time": 46.706281900405884,
      "citing_paper_id": "267199899",
      "cited_paper_id": 1900475
    },
    {
      "context_text": "We can then move one step forward, the output feature map in layer i + 2 can be denoted as Substitute F ′ i +1 in Eq 9 with Eq 8, we can get 5], DnCNN [37], IRCNN [38], FFDNet [39], AirNet [18] and U-WADN, respectively.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only models and methods. No verifiable resources are identified.",
      "processing_time": 46.706281900405884,
      "citing_paper_id": "267199899",
      "cited_paper_id": 250551851
    },
    {
      "context_text": "C ( · , ξ 1 ) and C ( · , ξ 2 ) , to obtain the decision logits The [35], UMRL [31], SIRR [29], LPNet [10], AirNet [18], and U-WADN, respectively.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions several methods/models (UMRL, SIRR, LPNet, AirNet, U-WADN) but does not specify any datasets. The cited papers are related to image restoration, but do not mention specific datasets.",
      "processing_time": 51.10013175010681,
      "citing_paper_id": "267199899",
      "cited_paper_id": 3406592
    },
    {
      "context_text": "C ( · , ξ 1 ) and C ( · , ξ 2 ) , to obtain the decision logits The [35], UMRL [31], SIRR [29], LPNet [10], AirNet [18], and U-WADN, respectively.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions several methods/models (UMRL, SIRR, LPNet, AirNet, U-WADN) but does not specify any datasets. The cited papers are related to image restoration, but do not mention specific datasets.",
      "processing_time": 51.10013175010681,
      "citing_paper_id": "267199899",
      "cited_paper_id": 21712570
    },
    {
      "context_text": "C ( · , ξ 1 ) and C ( · , ξ 2 ) , to obtain the decision logits The [35], UMRL [31], SIRR [29], LPNet [10], AirNet [18], and U-WADN, respectively.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions several methods/models (UMRL, SIRR, LPNet, AirNet, U-WADN) but does not specify any datasets. The cited papers are related to image restoration, but do not mention specific datasets.",
      "processing_time": 51.10013175010681,
      "citing_paper_id": "267199899",
      "cited_paper_id": 195657934
    },
    {
      "context_text": "C ( · , ξ 1 ) and C ( · , ξ 2 ) , to obtain the decision logits The [35], UMRL [31], SIRR [29], LPNet [10], AirNet [18], and U-WADN, respectively.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions several methods/models (UMRL, SIRR, LPNet, AirNet, U-WADN) but does not specify any datasets. The cited papers are related to image restoration, but do not mention specific datasets.",
      "processing_time": 51.10013175010681,
      "citing_paper_id": "267199899",
      "cited_paper_id": 250551851
    },
    {
      "context_text": "Consequently, our experimentation necessitates the utilization of five corresponding datasets, which comprise BSD400, BSD68 [22], WED [21] for the purpose of image denoising, Rain100L [30] for image deraining, and RESIDE [17] for image dehazing.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "BSD400",
        "BSD68",
        "WED",
        "Rain100L",
        "RESIDE"
      ],
      "dataset_descriptions": {
        "BSD400": "Used for image denoising, providing a benchmark for evaluating denoising algorithms.",
        "BSD68": "Used for image denoising, serving as a smaller subset for validation and testing.",
        "WED": "Used for image denoising, offering diverse images to test denoising performance.",
        "Rain100L": "Used for image deraining, containing synthetic rain images to evaluate deraining algorithms.",
        "RESIDE": "Used for image dehazing, providing a comprehensive dataset for training and testing dehazing methods."
      },
      "confidence_score": 1.0,
      "reasoning": "The context clearly mentions five datasets used for specific image restoration tasks. Each dataset is named and its purpose is specified.",
      "processing_time": 72.86241269111633,
      "citing_paper_id": "267199899",
      "cited_paper_id": 4840263
    },
    {
      "context_text": "Consequently, our experimentation necessitates the utilization of five corresponding datasets, which comprise BSD400, BSD68 [22], WED [21] for the purpose of image denoising, Rain100L [30] for image deraining, and RESIDE [17] for image dehazing.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "BSD400",
        "BSD68",
        "WED",
        "Rain100L",
        "RESIDE"
      ],
      "dataset_descriptions": {
        "BSD400": "Used for image denoising, providing a benchmark for evaluating denoising algorithms.",
        "BSD68": "Used for image denoising, serving as a smaller subset for validation and testing.",
        "WED": "Used for image denoising, offering diverse images to test denoising performance.",
        "Rain100L": "Used for image deraining, containing synthetic rain images to evaluate deraining algorithms.",
        "RESIDE": "Used for image dehazing, providing a comprehensive dataset for training and testing dehazing methods."
      },
      "confidence_score": 1.0,
      "reasoning": "The context clearly mentions five datasets used for specific image restoration tasks. Each dataset is named and its purpose is specified.",
      "processing_time": 72.86241269111633,
      "citing_paper_id": "267199899",
      "cited_paper_id": 73439498
    },
    {
      "context_text": "For image de-noising, BSD400 and WED [21] are used for training while BSD68 [22] is the testing set with 68 ground truth images.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "BSD400",
        "WED",
        "BSD68"
      ],
      "dataset_descriptions": {
        "BSD400": "Used for training image de-noising models, providing a diverse set of images to improve model robustness.",
        "WED": "Used for training image de-noising models, offering additional challenges and variations in image quality.",
        "BSD68": "Used as the testing set for evaluating image de-noising models, containing 68 ground truth images for performance assessment."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions specific datasets used for training and testing in the context of image de-noising. BSD400 and WED are used for training, and BSD68 is used for testing.",
      "processing_time": 66.8763816356659,
      "citing_paper_id": "267199899",
      "cited_paper_id": 4840263
    },
    {
      "context_text": "The importance of image restoration tasks * ∗ Corresponding Author is underscored by their pivotal role in facilitating subsequent downstream applications, notably encompassing image classification [13, 32], object detection [25], and semantic image segmentation [26].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general tasks and applications. No verifiable resources are identified.",
      "processing_time": 47.08429765701294,
      "citing_paper_id": "267199899",
      "cited_paper_id": 10328909
    },
    {
      "context_text": "The importance of image restoration tasks * ∗ Corresponding Author is underscored by their pivotal role in facilitating subsequent downstream applications, notably encompassing image classification [13, 32], object detection [25], and semantic image segmentation [26].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general tasks and applications. No verifiable resources are identified.",
      "processing_time": 47.08429765701294,
      "citing_paper_id": "267199899",
      "cited_paper_id": 56657799
    },
    {
      "context_text": "The importance of image restoration tasks * ∗ Corresponding Author is underscored by their pivotal role in facilitating subsequent downstream applications, notably encompassing image classification [13, 32], object detection [25], and semantic image segmentation [26].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general tasks and applications. No verifiable resources are identified.",
      "processing_time": 47.08429765701294,
      "citing_paper_id": "267199899",
      "cited_paper_id": 206594692
    },
    {
      "context_text": "Conventional image restoration techniques [6, 12, 27] rely on manually crafted priors for the reconstruction of deteriorated images.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only general techniques and methods.",
      "processing_time": 45.51255750656128,
      "citing_paper_id": "267199899",
      "cited_paper_id": 13133466
    },
    {
      "context_text": "Image restoration is a fundamental endeavor focused on ameliorating the quality of images afflicted by an array of degradations, spanning from noise [5, 20] to environmental factors such as rain [9, 14] and atmospheric hazi-ness [1, 24].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only general types of image degradations. No verifiable resources are named.",
      "processing_time": 48.05719757080078,
      "citing_paper_id": "267199899",
      "cited_paper_id": 14092238
    },
    {
      "context_text": "Image restoration is a fundamental endeavor focused on ameliorating the quality of images afflicted by an array of degradations, spanning from noise [5, 20] to environmental factors such as rain [9, 14] and atmospheric hazi-ness [1, 24].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only general types of image degradations. No verifiable resources are named.",
      "processing_time": 48.05719757080078,
      "citing_paper_id": "267199899",
      "cited_paper_id": 17115407
    },
    {
      "context_text": "Additionally, we have included the number of parameters, Floating-Figure 1], AOD-Net [16], EPDN [24], FDGAN [7], AirNet [18], and U-WADN, respectively.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only models and methods. The cited papers' titles do not provide additional dataset information.",
      "processing_time": 48.41335725784302,
      "citing_paper_id": "267199899",
      "cited_paper_id": 14092238
    },
    {
      "context_text": "Additionally, we have included the number of parameters, Floating-Figure 1], AOD-Net [16], EPDN [24], FDGAN [7], AirNet [18], and U-WADN, respectively.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only models and methods. The cited papers' titles do not provide additional dataset information.",
      "processing_time": 48.41335725784302,
      "citing_paper_id": "267199899",
      "cited_paper_id": 30151664
    },
    {
      "context_text": "Additionally, we have included the number of parameters, Floating-Figure 1], AOD-Net [16], EPDN [24], FDGAN [7], AirNet [18], and U-WADN, respectively.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only models and methods. The cited papers' titles do not provide additional dataset information.",
      "processing_time": 48.41335725784302,
      "citing_paper_id": "267199899",
      "cited_paper_id": 250551851
    },
    {
      "context_text": "Building upon the foundations laid by Li et al.[18], our analysis positions the novel U-WADN framework with six state-of-the-art image restoration methodologies, namely, BRDNet[28], LPNet [10], FDGAN [7], MPRNet [34], DL [8], and AirNet [18].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methodologies and frameworks. There are no clear identifiers for datasets in the provided context.",
      "processing_time": 48.79179787635803,
      "citing_paper_id": "267199899",
      "cited_paper_id": 21712570
    },
    {
      "context_text": "Building upon the foundations laid by Li et al.[18], our analysis positions the novel U-WADN framework with six state-of-the-art image restoration methodologies, namely, BRDNet[28], LPNet [10], FDGAN [7], MPRNet [34], DL [8], and AirNet [18].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methodologies and frameworks. There are no clear identifiers for datasets in the provided context.",
      "processing_time": 48.79179787635803,
      "citing_paper_id": "267199899",
      "cited_paper_id": 195787503
    },
    {
      "context_text": "Building upon the foundations laid by Li et al.[18], our analysis positions the novel U-WADN framework with six state-of-the-art image restoration methodologies, namely, BRDNet[28], LPNet [10], FDGAN [7], MPRNet [34], DL [8], and AirNet [18].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methodologies and frameworks. There are no clear identifiers for datasets in the provided context.",
      "processing_time": 48.79179787635803,
      "citing_paper_id": "267199899",
      "cited_paper_id": 203042751
    },
    {
      "context_text": "For image deraining, we adopt the Rain100L [30] dataset which consists of 200 rainy-clean training pairs and 100 testing image pairs.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Rain100L"
      ],
      "dataset_descriptions": {
        "Rain100L": "Used for training and testing image deraining models, consisting of 200 rainy-clean training pairs and 100 testing image pairs."
      },
      "confidence_score": 1.0,
      "reasoning": "The context clearly mentions the Rain100L dataset, which is used for training and testing image deraining models.",
      "processing_time": 54.39870476722717,
      "citing_paper_id": "267199899",
      "cited_paper_id": 73439498
    },
    {
      "context_text": "Other methodologies, including path selection [33] and dynamic selection [11], have also demonstrated superior efficacy-efficiency tradeoffs in the context of image restoration tasks.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only methodologies for image restoration tasks.",
      "processing_time": 46.92433452606201,
      "citing_paper_id": "267199899",
      "cited_paper_id": 128362447
    },
    {
      "context_text": "Other methodologies, including path selection [33] and dynamic selection [11], have also demonstrated superior efficacy-efficiency tradeoffs in the context of image restoration tasks.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only methodologies for image restoration tasks.",
      "processing_time": 46.92433452606201,
      "citing_paper_id": "267199899",
      "cited_paper_id": 221852418
    },
    {
      "context_text": "Building upon this foundation, Chen et al. [2] presented a streamlined model by amalgamating these convolution branches into a versatile, reusable architecture, drawing inspiration from the principles of ClassSR.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method or model. The context focuses on the architectural aspects of the model and its inspiration.",
      "processing_time": 49.26655554771423,
      "citing_paper_id": "267199899",
      "cited_paper_id": 247594264
    },
    {
      "context_text": "Among these all-in-one methods, corruption-agnostic approaches like AirNet [18] and IDR [36] are gaining increased attention to relieve the need for predefined degradation types.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and approaches. The context focuses on describing the methods used in image restoration.",
      "processing_time": 49.11078214645386,
      "citing_paper_id": "267199899",
      "cited_paper_id": 250551851
    },
    {
      "context_text": "Among these all-in-one methods, corruption-agnostic approaches like AirNet [18] and IDR [36] are gaining increased attention to relieve the need for predefined degradation types.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and approaches. The context focuses on describing the methods used in image restoration.",
      "processing_time": 49.11078214645386,
      "citing_paper_id": "267199899",
      "cited_paper_id": 260085391
    },
    {
      "context_text": "The Width Adaptive Backbone (WAB), is formed by sub-stituting the standard convolutions in [18] with width adaptive convolutions.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method or model. The context is about substituting standard convolutions with width adaptive convolutions in a model.",
      "processing_time": 50.06036949157715,
      "citing_paper_id": "267199899",
      "cited_paper_id": 250551851
    },
    {
      "context_text": "To be more specific, we follow [18] of the division of training and testing sets.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.2,
      "reasoning": "The citation does not mention any specific dataset names, only a general reference to training and testing sets. No clear, verifiable resource is identified.",
      "processing_time": 48.9786102771759,
      "citing_paper_id": "267199899",
      "cited_paper_id": 250551851
    },
    {
      "context_text": "As the U-WADN is adaptive to all network structures, we simply borrow the network from AirNet [18] to demonstrate the efficacy and efficiency of the proposed methods.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a method (AirNet) which is not a dataset.",
      "processing_time": 48.517282009124756,
      "citing_paper_id": "267199899",
      "cited_paper_id": 250551851
    },
    {
      "context_text": "Given the inherent adaptability of the WAB to various network structures, we have adopted a backbone structure from [18] in our paper.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a backbone structure from another paper. No verifiable resources are identified.",
      "processing_time": 49.251978397369385,
      "citing_paper_id": "267199899",
      "cited_paper_id": 250551851
    },
    {
      "context_text": "As deep learning methods have exhibited remarkable efficacy in image restoration tasks, all-in-one image restoration methodologies [18, 23] have gained prominence for their capacity to address multiple degradations in a single model.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methodologies and their effectiveness in image restoration tasks.",
      "processing_time": 48.383912801742554,
      "citing_paper_id": "267199899",
      "cited_paper_id": 250551851
    },
    {
      "context_text": "Building upon the work of [18], we undertake comprehensive experimental evaluations across five distinct image restoration tasks, specifically focusing on image denoising at noise levels σ = 15 , 25 , 50 , image deraining, and image dehazing.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only general tasks. No clear, verifiable datasets are identified.",
      "processing_time": 49.0927517414093,
      "citing_paper_id": "267199899",
      "cited_paper_id": 250551851
    },
    {
      "context_text": "As depicted in previous work [36], the image degradation process is defined as where we denote the corrupted image as y ∈ R H × W × C , while x ∈ R H × W × C represents the underlying clean image.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only the image degradation process. No dataset names are present in the citation span.",
      "processing_time": 49.65194368362427,
      "citing_paper_id": "267199899",
      "cited_paper_id": 260085391
    },
    {
      "context_text": "[36] further views the various degradation in an ingredient-oriented manner, enhancing the scalability when more image restoration tasks are involved.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method or approach for image restoration.",
      "processing_time": 48.511061668395996,
      "citing_paper_id": "267199899",
      "cited_paper_id": 260085391
    },
    {
      "context_text": "The dataset contains 18,609 training images and 17,609 testing images across three subsets: Test1 [83], Snow100K-L [84], and Raindrop [85].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Test1",
        "Snow100K-L",
        "Raindrop"
      ],
      "dataset_descriptions": {
        "Test1": "Used for evaluating image restoration models, focusing on general image quality and restoration accuracy.",
        "Snow100K-L": "Used for evaluating snow removal techniques, specifically assessing the effectiveness of models in restoring images degraded by snow.",
        "Raindrop": "Used for evaluating raindrop removal methods, focusing on the ability to restore images with raindrop distortions."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions specific datasets used for training and testing in the image restoration domain, which are directly relevant to the research topic.",
      "processing_time": 65.37785291671753,
      "citing_paper_id": "271533444",
      "cited_paper_id": 4539586
    },
    {
      "context_text": "The dataset contains 18,609 training images and 17,609 testing images across three subsets: Test1 [83], Snow100K-L [84], and Raindrop [85].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Test1",
        "Snow100K-L",
        "Raindrop"
      ],
      "dataset_descriptions": {
        "Test1": "Used for evaluating image restoration models, focusing on general image quality and restoration accuracy.",
        "Snow100K-L": "Used for evaluating snow removal techniques, specifically assessing the effectiveness of models in restoring images degraded by snow.",
        "Raindrop": "Used for evaluating raindrop removal methods, focusing on the ability to restore images with raindrop distortions."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions specific datasets used for training and testing in the image restoration domain, which are directly relevant to the research topic.",
      "processing_time": 65.37785291671753,
      "citing_paper_id": "271533444",
      "cited_paper_id": 131773964
    },
    {
      "context_text": "The dataset contains 18,609 training images and 17,609 testing images across three subsets: Test1 [83], Snow100K-L [84], and Raindrop [85].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Test1",
        "Snow100K-L",
        "Raindrop"
      ],
      "dataset_descriptions": {
        "Test1": "Used for evaluating image restoration models, focusing on general image quality and restoration accuracy.",
        "Snow100K-L": "Used for evaluating snow removal techniques, specifically assessing the effectiveness of models in restoring images degraded by snow.",
        "Raindrop": "Used for evaluating raindrop removal methods, focusing on the ability to restore images with raindrop distortions."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions specific datasets used for training and testing in the image restoration domain, which are directly relevant to the research topic.",
      "processing_time": 65.37785291671753,
      "citing_paper_id": "271533444",
      "cited_paper_id": 195489762
    },
    {
      "context_text": "During the training phase, we used the Adam [86] optimizer to optimize the network, setting the initial learning rate to 2 × 10 4 and adjusting it using the cosine annealing strategy.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions the Adam optimizer, which is a method, not a dataset. No datasets are mentioned in the citation context.",
      "processing_time": 49.64323949813843,
      "citing_paper_id": "271533444",
      "cited_paper_id": 6628106
    },
    {
      "context_text": "Numerous effective methods have been proposed for various tasks, including image deblurring [11], [12], image denoising [9], [47], image dehazing [20], [46], [48], [49], image deraining [14], [15], and image desnowing [50], [51].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various image restoration tasks and methods. The cited paper titles also do not provide specific dataset names.",
      "processing_time": 50.586790323257446,
      "citing_paper_id": "271533444",
      "cited_paper_id": 10514149
    },
    {
      "context_text": "Numerous effective methods have been proposed for various tasks, including image deblurring [11], [12], image denoising [9], [47], image dehazing [20], [46], [48], [49], image deraining [14], [15], and image desnowing [50], [51].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various image restoration tasks and methods. The cited paper titles also do not provide specific dataset names.",
      "processing_time": 50.586790323257446,
      "citing_paper_id": "271533444",
      "cited_paper_id": 214623217
    },
    {
      "context_text": "Numerous effective methods have been proposed for various tasks, including image deblurring [11], [12], image denoising [9], [47], image dehazing [20], [46], [48], [49], image deraining [14], [15], and image desnowing [50], [51].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various image restoration tasks and methods. The cited paper titles also do not provide specific dataset names.",
      "processing_time": 50.586790323257446,
      "citing_paper_id": "271533444",
      "cited_paper_id": 226641165
    },
    {
      "context_text": "Due to the complexity and uncertainty of the degradation process, traditional image restoration methods often rely on manually designed features and prior knowledge to construct restoration models [8], [19], [41], [42], [43].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general methods and approaches. No verifiable resources are identified.",
      "processing_time": 49.62881278991699,
      "citing_paper_id": "271533444",
      "cited_paper_id": 13692444
    },
    {
      "context_text": "Due to the complexity and uncertainty of the degradation process, traditional image restoration methods often rely on manually designed features and prior knowledge to construct restoration models [8], [19], [41], [42], [43].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general methods and approaches. No verifiable resources are identified.",
      "processing_time": 49.62881278991699,
      "citing_paper_id": "271533444",
      "cited_paper_id": 18774783
    },
    {
      "context_text": "Due to the complexity and uncertainty of the degradation process, traditional image restoration methods often rely on manually designed features and prior knowledge to construct restoration models [8], [19], [41], [42], [43].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general methods and approaches. No verifiable resources are identified.",
      "processing_time": 49.62881278991699,
      "citing_paper_id": "271533444",
      "cited_paper_id": 27333569
    },
    {
      "context_text": "Although many methods have shown excellent performance in their respective fields, such as denoising [8], [9], [10], deblurring [11], [12], [13], deraining [14], [15], [16], [17], and dehazing [18], [19], [20], [21], they are often limited to dealing with a single type of degradation problem.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various image restoration tasks and methods. No verifiable resources are identified.",
      "processing_time": 50.02055597305298,
      "citing_paper_id": "271533444",
      "cited_paper_id": 13692444
    },
    {
      "context_text": "Although many methods have shown excellent performance in their respective fields, such as denoising [8], [9], [10], deblurring [11], [12], [13], deraining [14], [15], [16], [17], and dehazing [18], [19], [20], [21], they are often limited to dealing with a single type of degradation problem.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various image restoration tasks and methods. No verifiable resources are identified.",
      "processing_time": 50.02055597305298,
      "citing_paper_id": "271533444",
      "cited_paper_id": 214623217
    },
    {
      "context_text": "Although many methods have shown excellent performance in their respective fields, such as denoising [8], [9], [10], deblurring [11], [12], [13], deraining [14], [15], [16], [17], and dehazing [18], [19], [20], [21], they are often limited to dealing with a single type of degradation problem.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various image restoration tasks and methods. No verifiable resources are identified.",
      "processing_time": 50.02055597305298,
      "citing_paper_id": "271533444",
      "cited_paper_id": 257069681
    },
    {
      "context_text": "Although many methods have shown excellent performance in their respective fields, such as denoising [8], [9], [10], deblurring [11], [12], [13], deraining [14], [15], [16], [17], and dehazing [18], [19], [20], [21], they are often limited to dealing with a single type of degradation problem.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various image restoration tasks and methods. No verifiable resources are identified.",
      "processing_time": 50.02055597305298,
      "citing_paper_id": "271533444",
      "cited_paper_id": 257999356
    },
    {
      "context_text": "…assess its restoration effectiveness, we compared it with various advanced image restoration methods, including general methods such as BRD-Net [65], LPNet [66], FDGAN [67], and MPRNet [69], as well as specialized All-in-One image restoration methods like DL [68], AirNet [62], and PromptIR [36].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only models and methods. No dataset names are present in the text.",
      "processing_time": 49.89537835121155,
      "citing_paper_id": "271533444",
      "cited_paper_id": 195787503
    },
    {
      "context_text": "…assess its restoration effectiveness, we compared it with various advanced image restoration methods, including general methods such as BRD-Net [65], LPNet [66], FDGAN [67], and MPRNet [69], as well as specialized All-in-One image restoration methods like DL [68], AirNet [62], and PromptIR [36].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only models and methods. No dataset names are present in the text.",
      "processing_time": 49.89537835121155,
      "citing_paper_id": "271533444",
      "cited_paper_id": 198185751
    },
    {
      "context_text": "…assess its restoration effectiveness, we compared it with various advanced image restoration methods, including general methods such as BRD-Net [65], LPNet [66], FDGAN [67], and MPRNet [69], as well as specialized All-in-One image restoration methods like DL [68], AirNet [62], and PromptIR [36].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only models and methods. No dataset names are present in the text.",
      "processing_time": 49.89537835121155,
      "citing_paper_id": "271533444",
      "cited_paper_id": 210838848
    },
    {
      "context_text": "…assess its restoration effectiveness, we compared it with various advanced image restoration methods, including general methods such as BRD-Net [65], LPNet [66], FDGAN [67], and MPRNet [69], as well as specialized All-in-One image restoration methods like DL [68], AirNet [62], and PromptIR [36].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only models and methods. No dataset names are present in the text.",
      "processing_time": 49.89537835121155,
      "citing_paper_id": "271533444",
      "cited_paper_id": 231802205
    },
    {
      "context_text": "…assess its restoration effectiveness, we compared it with various advanced image restoration methods, including general methods such as BRD-Net [65], LPNet [66], FDGAN [67], and MPRNet [69], as well as specialized All-in-One image restoration methods like DL [68], AirNet [62], and PromptIR [36].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only models and methods. No dataset names are present in the text.",
      "processing_time": 49.89537835121155,
      "citing_paper_id": "271533444",
      "cited_paper_id": 250551851
    },
    {
      "context_text": "Representative methods include DnCNN for denoising [9], MSPF for deraining [14], DCMPNet for dehazing [20], and DTCW for desnowing [51].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span mentions several methods but does not refer to any specific datasets. The context is focused on describing various image restoration methods rather than datasets.",
      "processing_time": 50.57515573501587,
      "citing_paper_id": "271533444",
      "cited_paper_id": 214623217
    },
    {
      "context_text": "To address the above challenges, researchers have begun exploring and designing universal models that can adapt to various image restoration tasks, achieving some promising research results [22], [23].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only references to research papers and the concept of universal models for image restoration.",
      "processing_time": 50.165985345840454,
      "citing_paper_id": "271533444",
      "cited_paper_id": 244346144
    },
    {
      "context_text": "To address this issue, researchers have introduced Transformers [52] to the field of image restoration, proposing methods such as SwinIR [53] and Restormer [23].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions Restormer, which is a method for high-resolution image restoration, not a dataset. No specific datasets are mentioned in the citation.",
      "processing_time": 51.01679229736328,
      "citing_paper_id": "271533444",
      "cited_paper_id": 244346144
    },
    {
      "context_text": "For instance, Wang et al. proposed Uformer [22], Liang et al. proposed SwinIR [53], and Zamir et al. proposed Restormer [23].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span mentions several methods/models (Uformer, SwinIR, Restormer) but does not reference any specific datasets. The context focuses on model proposals rather than dataset usage.",
      "processing_time": 52.07300043106079,
      "citing_paper_id": "271533444",
      "cited_paper_id": 244346144
    },
    {
      "context_text": "For example, Zhang et al. introduced the MC-Blur dataset [54], which covers various types of blur, such as motion blur, ultra-high-definition blur, and defocus blur, advancing research on deblurring algorithms.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "MC-Blur"
      ],
      "dataset_descriptions": {
        "MC-Blur": "Used to advance research on deblurring algorithms, covering various types of blur including motion, ultra-high-definition, and defocus blur."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions the MC-Blur dataset, which is a specific, verifiable dataset used for advancing research on deblurring algorithms.",
      "processing_time": 56.960999965667725,
      "citing_paper_id": "271533444",
      "cited_paper_id": 249605449
    },
    {
      "context_text": "Interms of feature modulation, Li et al. [62] employed a contrastive learning strategy to learn degradation representations from degraded images.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method or approach used in the research.",
      "processing_time": 50.753713846206665,
      "citing_paper_id": "271533444",
      "cited_paper_id": 250551851
    },
    {
      "context_text": "These methods not only adopt new network architectures but also leverage specially designed datasets (such as RWBI [55], UHD-LOL [56], JRSRD [57], and RainKITTI2012 [58]), further improving the performance of image restoration algorithms in complex degradation scenarios.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "RWBI",
        "UHD-LOL",
        "JRSRD",
        "RainKITTI2012"
      ],
      "dataset_descriptions": {
        "RWBI": "Used to enhance image restoration performance in real-world blur and illumination conditions, focusing on complex degradation scenarios.",
        "UHD-LOL": "Applied to improve low-light image enhancement in ultra-high-definition settings, addressing complex lighting conditions.",
        "JRSRD": "Leveraged to restore images degraded by rain and other real-world factors, enhancing performance in challenging environments.",
        "RainKITTI2012": "Utilized to test and improve rain removal algorithms, specifically addressing rain-induced degradations in image restoration."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions specific datasets used for improving image restoration algorithms in complex degradation scenarios.",
      "processing_time": 74.21943545341492,
      "citing_paper_id": "271533444",
      "cited_paper_id": 254973973
    },
    {
      "context_text": "Additionally, researchers have further proposed methods that combine novel network architectures with specially designed datasets, such as DBGAN proposed by Zhang et al. [55], LLFormer proposed by Wang et al. [56], DAM proposed by Zhang et al. [57], and EPRRNet proposed by Zhang et al. [58].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions several methods but does not specify any datasets. The context focuses on network architectures and methods rather than datasets.",
      "processing_time": 51.52397108078003,
      "citing_paper_id": "271533444",
      "cited_paper_id": 254973973
    },
    {
      "context_text": "…into three categories: Prior Knowledge-Based Image Restoration (PKIR) [24], [25], [26], [27], [28], [29], Architecture Search and Feature Modulation-Based Image Restoration (AS-FM-IR) [30], [31], [32], [33], [34], [35], and Prompt-Based Image Restoration (PromptIR) [36], [37], [38], [39], [40].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only categories of image restoration methods. No dataset names are present in the text.",
      "processing_time": 51.74600386619568,
      "citing_paper_id": "271533444",
      "cited_paper_id": 259243623
    },
    {
      "context_text": "…into three categories: Prior Knowledge-Based Image Restoration (PKIR) [24], [25], [26], [27], [28], [29], Architecture Search and Feature Modulation-Based Image Restoration (AS-FM-IR) [30], [31], [32], [33], [34], [35], and Prompt-Based Image Restoration (PromptIR) [36], [37], [38], [39], [40].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only categories of image restoration methods. No dataset names are present in the text.",
      "processing_time": 51.74600386619568,
      "citing_paper_id": "271533444",
      "cited_paper_id": 264555119
    },
    {
      "context_text": "…into three categories: Prior Knowledge-Based Image Restoration (PKIR) [24], [25], [26], [27], [28], [29], Architecture Search and Feature Modulation-Based Image Restoration (AS-FM-IR) [30], [31], [32], [33], [34], [35], and Prompt-Based Image Restoration (PromptIR) [36], [37], [38], [39], [40].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only categories of image restoration methods. No dataset names are present in the text.",
      "processing_time": 51.74600386619568,
      "citing_paper_id": "271533444",
      "cited_paper_id": 266149517
    },
    {
      "context_text": "…into three categories: Prior Knowledge-Based Image Restoration (PKIR) [24], [25], [26], [27], [28], [29], Architecture Search and Feature Modulation-Based Image Restoration (AS-FM-IR) [30], [31], [32], [33], [34], [35], and Prompt-Based Image Restoration (PromptIR) [36], [37], [38], [39], [40].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only categories of image restoration methods. No dataset names are present in the text.",
      "processing_time": 51.74600386619568,
      "citing_paper_id": "271533444",
      "cited_paper_id": 267320695
    },
    {
      "context_text": "Ma et al. [37] employed degradation-aware visual prompts to encode different types of image degradation information, using linear weighting to control the restoration process.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a method for image restoration. The cited paper title confirms the focus on a method rather than a dataset.",
      "processing_time": 52.855644941329956,
      "citing_paper_id": "271533444",
      "cited_paper_id": 259243623
    },
    {
      "context_text": "To achieve effective separation of high-frequency and low-frequency components, this paper introduces dynamically learnable filters [64].",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for image restoration.",
      "processing_time": 50.731292724609375,
      "citing_paper_id": "271533444",
      "cited_paper_id": 259298517
    },
    {
      "context_text": "Gao et al. [60] effectively extracted and restored low-frequency and high-frequency features through a frequency-oriented encoder and a frequency-refined decoder, achieving all-in-one image restoration from a frequency perspective.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for image restoration. The focus is on the technique rather than a particular dataset.",
      "processing_time": 52.37224364280701,
      "citing_paper_id": "271533444",
      "cited_paper_id": 260271048
    },
    {
      "context_text": "In image restoration based on high-frequency and low-frequency component analysis, existing methods, such as those in [49] and [60], focus on recovering high-frequency and low-frequency information in different manners.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods or approaches. The context focuses on the technique of frequency-oriented efficient transformer for image restoration.",
      "processing_time": 53.018882513046265,
      "citing_paper_id": "271533444",
      "cited_paper_id": 260271048
    },
    {
      "context_text": "…restoration methods can be broadly divided into three categories: Prior Knowledge-Based Image Restoration (PKIR) [24], [25], [26], [27], [28], [29], Architecture Search and Feature Modulation-Based Image Restoration (AS-FM-IR) [30], [31], [32], [33], [34], [35], and Prompt-Based Image…",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only categories of image restoration methods. No verifiable resources are identified.",
      "processing_time": 52.19757127761841,
      "citing_paper_id": "271533444",
      "cited_paper_id": 263605463
    },
    {
      "context_text": "Luo et al. [29] took a different approach by fine-tuning the CLIP image encoder to predict high-quality feature embeddings from various degraded images.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (CLIP image encoder) used for predicting feature embeddings from degraded images.",
      "processing_time": 52.67533755302429,
      "citing_paper_id": "271533444",
      "cited_paper_id": 263605463
    },
    {
      "context_text": "…1) Image Restoration via Knowledge-Based Methods: In All-in-One image restoration frameworks, leveraging external knowledge is a common strategy for addressing conflicts arising from the inconsistent demands of diverse restoration tasks on a unified framework [24], [25], [26], [27], [28], [29].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general strategies for image restoration. No dataset names are present in the text.",
      "processing_time": 52.67213034629822,
      "citing_paper_id": "271533444",
      "cited_paper_id": 263605463
    },
    {
      "context_text": "…1) Image Restoration via Knowledge-Based Methods: In All-in-One image restoration frameworks, leveraging external knowledge is a common strategy for addressing conflicts arising from the inconsistent demands of diverse restoration tasks on a unified framework [24], [25], [26], [27], [28], [29].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general strategies for image restoration. No dataset names are present in the text.",
      "processing_time": 52.67213034629822,
      "citing_paper_id": "271533444",
      "cited_paper_id": 264145824
    },
    {
      "context_text": "…1) Image Restoration via Knowledge-Based Methods: In All-in-One image restoration frameworks, leveraging external knowledge is a common strategy for addressing conflicts arising from the inconsistent demands of diverse restoration tasks on a unified framework [24], [25], [26], [27], [28], [29].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general strategies for image restoration. No dataset names are present in the text.",
      "processing_time": 52.67213034629822,
      "citing_paper_id": "271533444",
      "cited_paper_id": 266690738
    },
    {
      "context_text": "Lastly, Jiang et al. [26] constructed a correlation between degraded images and predefined degradation descriptions, retrieved the text descriptions of degraded images, and used them as generation conditions for diffusion models, ultimately achieving multi-task image restoration.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific, verifiable datasets. It describes a method for image restoration using degradation descriptions and diffusion models.",
      "processing_time": 52.66889262199402,
      "citing_paper_id": "271533444",
      "cited_paper_id": 264145824
    },
    {
      "context_text": "…all-in-one image restoration methods can be broadly divided into three categories: Prior Knowledge-Based Image Restoration (PKIR) [24], [25], [26], [27], [28], [29], Architecture Search and Feature Modulation-Based Image Restoration (AS-FM-IR) [30], [31], [32], [33], [34], [35], and…",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only categories of image restoration methods. No verifiable resources are identified.",
      "processing_time": 52.48347759246826,
      "citing_paper_id": "271533444",
      "cited_paper_id": 264145824
    },
    {
      "context_text": "Chen and Pei [33] effectively extracted information on degradation types and severity by combining edge quality ranking loss with contrast loss.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and losses used for extracting information on degradation types and severity.",
      "processing_time": 52.82215762138367,
      "citing_paper_id": "271533444",
      "cited_paper_id": 264555119
    },
    {
      "context_text": "Yang et al. [61] constructed an expert library containing various convolution kernels.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.0,
      "reasoning": "The citation does not mention any specific dataset, only a library of convolution kernels. The context does not provide enough information to identify a verifiable dataset.",
      "processing_time": 54.613147020339966,
      "citing_paper_id": "271533444",
      "cited_paper_id": 265609786
    },
    {
      "context_text": "Li et al. [38] integrated degradation-aware prompts and restoration prompts into a general restoration prompt and utilized a prompt-feature interaction module to modulate degradation-related features.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and models. The context focuses on the integration of prompts and a prompt-feature interaction module.",
      "processing_time": 54.609946489334106,
      "citing_paper_id": "271533444",
      "cited_paper_id": 266149517
    },
    {
      "context_text": "Similarly, Lin et al. [27] proposed a method that maps degraded images into a text space.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for image restoration.",
      "processing_time": 52.812995195388794,
      "citing_paper_id": "271533444",
      "cited_paper_id": 266690738
    },
    {
      "context_text": "…all-in-one image restoration methods can be broadly divided into three categories: Prior Knowledge-Based Image Restoration (PKIR) [24], [25], [26], [27], [28], [29], Architecture Search and Feature Modulation-Based Image Restoration (AS-FM-IR) [30], [31], [32], [33], [34], [35], and Prompt-Based…",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only categories of image restoration methods. No verifiable resources are identified.",
      "processing_time": 53.97342872619629,
      "citing_paper_id": "271533444",
      "cited_paper_id": 266690738
    },
    {
      "context_text": "Conde et al. [40] proposed a text-guided image restoration model that uses human instructions as prompts to guide the restoration process.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions a model (text-guided image restoration model) but does not specify a dataset. No dataset names are mentioned in the citation context.",
      "processing_time": 54.977476358413696,
      "citing_paper_id": "271533444",
      "cited_paper_id": 267320695
    },
    {
      "context_text": "Given its critical role in numerous downstream tasks, such as image fusion [1], [2], [3], target recognition [4], [5], and detection [6], [7], this technology has attracted widespread attention from researchers.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general applications of image restoration technology. No verifiable resources are identified.",
      "processing_time": 54.138991594314575,
      "citing_paper_id": "271533444",
      "cited_paper_id": 268667967
    },
    {
      "context_text": "When faced with different types of degradation or varying degrees of degradation, these methods often struggle to provide satisfactory results.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, models, or methods. It only discusses the general challenges faced by existing methods in image restoration.",
      "processing_time": 54.98094153404236,
      "citing_paper_id": "271533444",
      "cited_paper_id": 271064424
    },
    {
      "context_text": "Testing is conducted on the CBSD68 [32] and Urban100 [14] datasets.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "CBSD68",
        "Urban100"
      ],
      "dataset_descriptions": {
        "CBSD68": "Used to test image restoration methods, focusing on benchmarking denoising performance on a set of 68 color images.",
        "Urban100": "Used to evaluate image restoration techniques, particularly for super-resolution tasks on urban scenes."
      },
      "confidence_score": 1.0,
      "reasoning": "The citation mentions two specific datasets, CBSD68 and Urban100, which are used for testing image restoration methods.",
      "processing_time": 67.22377634048462,
      "citing_paper_id": "278740510",
      "cited_paper_id": 64193
    },
    {
      "context_text": "Testing is conducted on the CBSD68 [32] and Urban100 [14] datasets.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "CBSD68",
        "Urban100"
      ],
      "dataset_descriptions": {
        "CBSD68": "Used to test image restoration methods, focusing on benchmarking denoising performance on a set of 68 color images.",
        "Urban100": "Used to evaluate image restoration techniques, particularly for super-resolution tasks on urban scenes."
      },
      "confidence_score": 1.0,
      "reasoning": "The citation mentions two specific datasets, CBSD68 and Urban100, which are used for testing image restoration methods.",
      "processing_time": 67.22377634048462,
      "citing_paper_id": "278740510",
      "cited_paper_id": 8282555
    },
    {
      "context_text": "To assist the recovery process, the encoder features are concatenated with the decoder features via skip connections [44].",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (U-Net) which is not included as per instructions.",
      "processing_time": 54.26549530029297,
      "citing_paper_id": "278740510",
      "cited_paper_id": 3719281
    },
    {
      "context_text": "…the performance of various image restoration tasks, such as denoising [13, 26, 35, 36, 42, 43], de-raining [2, 22, 24, 27, 47], dehazing [5, 9, 29, 40, 45, 51], deblurring [4, 7, 18, 19, 37, 37, 60], and low-light enhancement [12, 31, 52], by learning generalizable priors from large-scale datasets.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.3,
      "reasoning": "The context mentions 'large-scale datasets' but does not specify any particular dataset names. The cited papers do not provide additional specific dataset names either.",
      "processing_time": 55.34179091453552,
      "citing_paper_id": "278740510",
      "cited_paper_id": 4552226
    },
    {
      "context_text": "…the performance of various image restoration tasks, such as denoising [13, 26, 35, 36, 42, 43], de-raining [2, 22, 24, 27, 47], dehazing [5, 9, 29, 40, 45, 51], deblurring [4, 7, 18, 19, 37, 37, 60], and low-light enhancement [12, 31, 52], by learning generalizable priors from large-scale datasets.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.3,
      "reasoning": "The context mentions 'large-scale datasets' but does not specify any particular dataset names. The cited papers do not provide additional specific dataset names either.",
      "processing_time": 55.34179091453552,
      "citing_paper_id": "278740510",
      "cited_paper_id": 199543931
    },
    {
      "context_text": "…the performance of various image restoration tasks, such as denoising [13, 26, 35, 36, 42, 43], de-raining [2, 22, 24, 27, 47], dehazing [5, 9, 29, 40, 45, 51], deblurring [4, 7, 18, 19, 37, 37, 60], and low-light enhancement [12, 31, 52], by learning generalizable priors from large-scale datasets.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.3,
      "reasoning": "The context mentions 'large-scale datasets' but does not specify any particular dataset names. The cited papers do not provide additional specific dataset names either.",
      "processing_time": 55.34179091453552,
      "citing_paper_id": "278740510",
      "cited_paper_id": 210839013
    },
    {
      "context_text": "…the performance of various image restoration tasks, such as denoising [13, 26, 35, 36, 42, 43], de-raining [2, 22, 24, 27, 47], dehazing [5, 9, 29, 40, 45, 51], deblurring [4, 7, 18, 19, 37, 37, 60], and low-light enhancement [12, 31, 52], by learning generalizable priors from large-scale datasets.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.3,
      "reasoning": "The context mentions 'large-scale datasets' but does not specify any particular dataset names. The cited papers do not provide additional specific dataset names either.",
      "processing_time": 55.34179091453552,
      "citing_paper_id": "278740510",
      "cited_paper_id": 214802286
    },
    {
      "context_text": "…the performance of various image restoration tasks, such as denoising [13, 26, 35, 36, 42, 43], de-raining [2, 22, 24, 27, 47], dehazing [5, 9, 29, 40, 45, 51], deblurring [4, 7, 18, 19, 37, 37, 60], and low-light enhancement [12, 31, 52], by learning generalizable priors from large-scale datasets.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.3,
      "reasoning": "The context mentions 'large-scale datasets' but does not specify any particular dataset names. The cited papers do not provide additional specific dataset names either.",
      "processing_time": 55.34179091453552,
      "citing_paper_id": "278740510",
      "cited_paper_id": 236976210
    },
    {
      "context_text": "Deblur-ring and low-light enhancement tasks utilize the GoPro [34] and LOL-v1 [34] datasets, respectively.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "GoPro",
        "LOL-v1"
      ],
      "dataset_descriptions": {
        "GoPro": "Used for dynamic scene deblurring tasks, focusing on improving image clarity in motion-blurred images using a deep multi-scale convolutional neural network.",
        "LOL-v1": "Used for low-light image enhancement, specifically addressing the challenge of improving visibility in underexposed images."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions two specific datasets, GoPro and LOL-v1, which are used for deblurring and low-light enhancement tasks, respectively.",
      "processing_time": 68.41714811325073,
      "citing_paper_id": "278740510",
      "cited_paper_id": 8671030
    },
    {
      "context_text": "These encompass datasets from the previously mentioned three-task scenario, alongside additional datasets: GoPro [34] for motion deblurring and LOL [50] for low-light image enhancement.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "GoPro",
        "LOL"
      ],
      "dataset_descriptions": {
        "GoPro": "Used for motion deblurring, specifically to train and evaluate models on dynamic scene deblurring tasks using a multi-scale convolutional neural network.",
        "LOL": "Used for low-light image enhancement, specifically to train and evaluate models on improving image quality in low-light conditions using deep retinex decomposition."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions two specific datasets, GoPro and LOL, which are used for motion deblurring and low-light image enhancement, respectively. These datasets are clearly identified and their usage is described.",
      "processing_time": 71.31549096107483,
      "citing_paper_id": "278740510",
      "cited_paper_id": 8671030
    },
    {
      "context_text": "These encompass datasets from the previously mentioned three-task scenario, alongside additional datasets: GoPro [34] for motion deblurring and LOL [50] for low-light image enhancement.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "GoPro",
        "LOL"
      ],
      "dataset_descriptions": {
        "GoPro": "Used for motion deblurring, specifically to train and evaluate models on dynamic scene deblurring tasks using a multi-scale convolutional neural network.",
        "LOL": "Used for low-light image enhancement, specifically to train and evaluate models on improving image quality in low-light conditions using deep retinex decomposition."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions two specific datasets, GoPro and LOL, which are used for motion deblurring and low-light image enhancement, respectively. These datasets are clearly identified and their usage is described.",
      "processing_time": 71.31549096107483,
      "citing_paper_id": "278740510",
      "cited_paper_id": 52008443
    },
    {
      "context_text": "…have significantly advanced the performance of various image restoration tasks, such as denoising [13, 26, 35, 36, 42, 43], de-raining [2, 22, 24, 27, 47], dehazing [5, 9, 29, 40, 45, 51], deblurring [4, 7, 18, 19, 37, 37, 60], and low-light enhancement [12, 31, 52], by learning…",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various image restoration tasks and related papers. No verifiable resources are named.",
      "processing_time": 55.32152700424194,
      "citing_paper_id": "278740510",
      "cited_paper_id": 49864080
    },
    {
      "context_text": "Recently, deep learning-based approaches have achieved remarkable progress in single-degradation restoration tasks, including denoising [15, 26, 35, 36, 42, 61], deraining [2, 17, 22, 48, 54], dehazing [5, 9, 40, 45, 51], and deblurring [7, 19, 37, 37, 60].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various tasks and methods. The cited papers' titles do not provide additional dataset information.",
      "processing_time": 55.72094964981079,
      "citing_paper_id": "278740510",
      "cited_paper_id": 56475900
    },
    {
      "context_text": "Recently, deep learning-based approaches have achieved remarkable progress in single-degradation restoration tasks, including denoising [15, 26, 35, 36, 42, 61], deraining [2, 17, 22, 48, 54], dehazing [5, 9, 40, 45, 51], and deblurring [7, 19, 37, 37, 60].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various tasks and methods. The cited papers' titles do not provide additional dataset information.",
      "processing_time": 55.72094964981079,
      "citing_paper_id": "278740510",
      "cited_paper_id": 199543931
    },
    {
      "context_text": "Recently, deep learning-based approaches have achieved remarkable progress in single-degradation restoration tasks, including denoising [15, 26, 35, 36, 42, 61], deraining [2, 17, 22, 48, 54], dehazing [5, 9, 40, 45, 51], and deblurring [7, 19, 37, 37, 60].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various tasks and methods. The cited papers' titles do not provide additional dataset information.",
      "processing_time": 55.72094964981079,
      "citing_paper_id": "278740510",
      "cited_paper_id": 209376714
    },
    {
      "context_text": "Recently, deep learning-based approaches have achieved remarkable progress in single-degradation restoration tasks, including denoising [15, 26, 35, 36, 42, 61], deraining [2, 17, 22, 48, 54], dehazing [5, 9, 40, 45, 51], and deblurring [7, 19, 37, 37, 60].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various tasks and methods. The cited papers' titles do not provide additional dataset information.",
      "processing_time": 55.72094964981079,
      "citing_paper_id": "278740510",
      "cited_paper_id": 214623217
    },
    {
      "context_text": "Recently, deep learning-based approaches have achieved remarkable progress in single-degradation restoration tasks, including denoising [15, 26, 35, 36, 42, 61], deraining [2, 17, 22, 48, 54], dehazing [5, 9, 40, 45, 51], and deblurring [7, 19, 37, 37, 60].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various tasks and methods. The cited papers' titles do not provide additional dataset information.",
      "processing_time": 55.72094964981079,
      "citing_paper_id": "278740510",
      "cited_paper_id": 214802286
    },
    {
      "context_text": "Recently, deep learning-based approaches have achieved remarkable progress in single-degradation restoration tasks, including denoising [15, 26, 35, 36, 42, 61], deraining [2, 17, 22, 48, 54], dehazing [5, 9, 40, 45, 51], and deblurring [7, 19, 37, 37, 60].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various tasks and methods. The cited papers' titles do not provide additional dataset information.",
      "processing_time": 55.72094964981079,
      "citing_paper_id": "278740510",
      "cited_paper_id": 218613859
    },
    {
      "context_text": "Recently, deep learning-based approaches have achieved remarkable progress in single-degradation restoration tasks, including denoising [15, 26, 35, 36, 42, 61], deraining [2, 17, 22, 48, 54], dehazing [5, 9, 40, 45, 51], and deblurring [7, 19, 37, 37, 60].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various tasks and methods. The cited papers' titles do not provide additional dataset information.",
      "processing_time": 55.72094964981079,
      "citing_paper_id": "278740510",
      "cited_paper_id": 219488928
    },
    {
      "context_text": "Recently, deep learning-based approaches have achieved remarkable progress in single-degradation restoration tasks, including denoising [15, 26, 35, 36, 42, 61], deraining [2, 17, 22, 48, 54], dehazing [5, 9, 40, 45, 51], and deblurring [7, 19, 37, 37, 60].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various tasks and methods. The cited papers' titles do not provide additional dataset information.",
      "processing_time": 55.72094964981079,
      "citing_paper_id": "278740510",
      "cited_paper_id": 231419143
    },
    {
      "context_text": "Recently, deep learning-based approaches have achieved remarkable progress in single-degradation restoration tasks, including denoising [15, 26, 35, 36, 42, 61], deraining [2, 17, 22, 48, 54], dehazing [5, 9, 40, 45, 51], and deblurring [7, 19, 37, 37, 60].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various tasks and methods. The cited papers' titles do not provide additional dataset information.",
      "processing_time": 55.72094964981079,
      "citing_paper_id": "278740510",
      "cited_paper_id": 235702920
    },
    {
      "context_text": "Recently, deep learning-based approaches have achieved remarkable progress in single-degradation restoration tasks, including denoising [15, 26, 35, 36, 42, 61], deraining [2, 17, 22, 48, 54], dehazing [5, 9, 40, 45, 51], and deblurring [7, 19, 37, 37, 60].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various tasks and methods. The cited papers' titles do not provide additional dataset information.",
      "processing_time": 55.72094964981079,
      "citing_paper_id": "278740510",
      "cited_paper_id": 236976210
    },
    {
      "context_text": "Recently, deep learning-based approaches have achieved remarkable progress in single-degradation restoration tasks, including denoising [15, 26, 35, 36, 42, 61], deraining [2, 17, 22, 48, 54], dehazing [5, 9, 40, 45, 51], and deblurring [7, 19, 37, 37, 60].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various tasks and methods. The cited papers' titles do not provide additional dataset information.",
      "processing_time": 55.72094964981079,
      "citing_paper_id": "278740510",
      "cited_paper_id": 253321998
    },
    {
      "context_text": "Recently, deep learning-based approaches have achieved remarkable progress in single-degradation restoration tasks, including denoising [15, 26, 35, 36, 42, 61], deraining [2, 17, 22, 48, 54], dehazing [5, 9, 40, 45, 51], and deblurring [7, 19, 37, 37, 60].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various tasks and methods. The cited papers' titles do not provide additional dataset information.",
      "processing_time": 55.72094964981079,
      "citing_paper_id": "278740510",
      "cited_paper_id": 260887508
    },
    {
      "context_text": "The image quality metrics—PSNR and SSIM [16]—for the top-performing methods are high-lighted in red and the second-best results are highlighted in blue in the result tables.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions PSNR and SSIM but does not refer to any specific dataset. These are metrics, not datasets.",
      "processing_time": 54.9574921131134,
      "citing_paper_id": "278740510",
      "cited_paper_id": 62732555
    },
    {
      "context_text": "…the performance of various image restoration tasks, such as denoising [13, 26, 35, 36, 42, 43], de-raining [2, 22, 24, 27, 47], dehazing [5, 9, 29, 40, 45, 51], deblurring [4, 7, 18, 19, 37, 37, 60], and low-light enhancement [12, 31, 52], by learning generalizable priors from large-scale…",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various image restoration tasks. No clear identifiers for datasets are present.",
      "processing_time": 55.15390968322754,
      "citing_paper_id": "278740510",
      "cited_paper_id": 199528450
    },
    {
      "context_text": "…the performance of various image restoration tasks, such as denoising [13, 26, 35, 36, 42, 43], de-raining [2, 22, 24, 27, 47], dehazing [5, 9, 29, 40, 45, 51], deblurring [4, 7, 18, 19, 37, 37, 60], and low-light enhancement [12, 31, 52], by learning generalizable priors from large-scale…",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various image restoration tasks. No clear identifiers for datasets are present.",
      "processing_time": 55.15390968322754,
      "citing_paper_id": "278740510",
      "cited_paper_id": 218613859
    },
    {
      "context_text": "…significantly advanced the performance of various image restoration tasks, such as denoising [13, 26, 35, 36, 42, 43], de-raining [2, 22, 24, 27, 47], dehazing [5, 9, 29, 40, 45, 51], deblurring [4, 7, 18, 19, 37, 37, 60], and low-light enhancement [12, 31, 52], by learning generalizable priors…",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various image restoration tasks and methods. No verifiable resources are identified.",
      "processing_time": 55.30976867675781,
      "citing_paper_id": "278740510",
      "cited_paper_id": 207977878
    },
    {
      "context_text": "…advanced the performance of various image restoration tasks, such as denoising [13, 26, 35, 36, 42, 43], de-raining [2, 22, 24, 27, 47], dehazing [5, 9, 29, 40, 45, 51], deblurring [4, 7, 18, 19, 37, 37, 60], and low-light enhancement [12, 31, 52], by learning generalizable priors from…",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various image restoration tasks and methods. No clear, verifiable datasets are identified.",
      "processing_time": 55.891249895095825,
      "citing_paper_id": "278740510",
      "cited_paper_id": 219488928
    },
    {
      "context_text": "For image dehazing, we use the SOTS [20] dataset, while Rain100L [53] is employed for image deraining.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "SOTS",
        "Rain100L"
      ],
      "dataset_descriptions": {
        "SOTS": "Used for image dehazing, specifically evaluating the performance of dehazing algorithms on synthetic and real-world hazy images.",
        "Rain100L": "Employed for image deraining, focusing on the effectiveness of deraining methods on synthetic rain images with varying intensities."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions two specific datasets, SOTS and Rain100L, which are used for image dehazing and deraining respectively.",
      "processing_time": 69.67191934585571,
      "citing_paper_id": "278740510",
      "cited_paper_id": 219530930
    },
    {
      "context_text": "Li et al. [23] introduce a single-encoder, multi-decoder framework targeting weather-based degradations using the Rain-Haze-Snow dataset.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Rain-Haze-Snow"
      ],
      "dataset_descriptions": {
        "Rain-Haze-Snow": "Used to train a single-encoder, multi-decoder framework for removing weather-based degradations, focusing on rain, haze, and snow conditions."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions a specific dataset, 'Rain-Haze-Snow', which is used for training a model to handle weather-based degradations. The dataset is clearly named and relevant to the research topic.",
      "processing_time": 66.12119603157043,
      "citing_paper_id": "278740510",
      "cited_paper_id": 219978541
    },
    {
      "context_text": "…deep learning-based approaches have significantly advanced the performance of various image restoration tasks, such as denoising [13, 26, 35, 36, 42, 43], de-raining [2, 22, 24, 27, 47], dehazing [5, 9, 29, 40, 45, 51], deblurring [4, 7, 18, 19, 37, 37, 60], and low-light enhancement [12, 31,…",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various image restoration tasks and related papers. No verifiable resources are identified.",
      "processing_time": 55.70068979263306,
      "citing_paper_id": "278740510",
      "cited_paper_id": 235702920
    },
    {
      "context_text": "We focus on general purpose restoration models [3, 25, 57], as these architectures can be independently trained for a variety of tasks.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only general-purpose restoration models. The cited papers' titles do not provide additional dataset information.",
      "processing_time": 21.123323917388916,
      "citing_paper_id": "278740510",
      "cited_paper_id": 237266491
    },
    {
      "context_text": "We focus on general purpose restoration models [3, 25, 57], as these architectures can be independently trained for a variety of tasks.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only general-purpose restoration models. The cited papers' titles do not provide additional dataset information.",
      "processing_time": 21.123323917388916,
      "citing_paper_id": "278740510",
      "cited_paper_id": 244346144
    },
    {
      "context_text": "We focus on general purpose restoration models [3, 25, 57], as these architectures can be independently trained for a variety of tasks.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only general-purpose restoration models. The cited papers' titles do not provide additional dataset information.",
      "processing_time": 21.123323917388916,
      "citing_paper_id": "278740510",
      "cited_paper_id": 248085491
    },
    {
      "context_text": "In this study, we adopt Restormer [57] as the backbone for our DFPIR model due to its efficient design and high performance across multiple restoration tasks.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions Restormer but does not refer to it as a dataset. It is described as a method or model used as the backbone for another model.",
      "processing_time": 56.84892249107361,
      "citing_paper_id": "278740510",
      "cited_paper_id": 244346144
    },
    {
      "context_text": "Consequently, recent research has shifted toward multi-degradation restoration techniques, delivering state-of-the-art performance for known combinations of degradations [3, 33, 56, 57].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only a shift in research focus towards multi-degradation restoration techniques.",
      "processing_time": 55.29831910133362,
      "citing_paper_id": "278740510",
      "cited_paper_id": 244346144
    },
    {
      "context_text": "Consequently, recent research has shifted toward multi-degradation restoration techniques, delivering state-of-the-art performance for known combinations of degradations [3, 33, 56, 57].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only a shift in research focus towards multi-degradation restoration techniques.",
      "processing_time": 55.29831910133362,
      "citing_paper_id": "278740510",
      "cited_paper_id": 248085491
    },
    {
      "context_text": "Consequently, recent research has shifted toward multi-degradation restoration techniques, delivering state-of-the-art performance for known combinations of degradations [3, 33, 56, 57].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only a shift in research focus towards multi-degradation restoration techniques.",
      "processing_time": 55.29831910133362,
      "citing_paper_id": "278740510",
      "cited_paper_id": 248426764
    },
    {
      "context_text": "Motivated by Restormer [57], we design a cross-attention mechanism in the channel dimension to aggregate the shuffled and original features.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a method (Restormer) which is not a dataset.",
      "processing_time": 55.29462432861328,
      "citing_paper_id": "278740510",
      "cited_paper_id": 244346144
    },
    {
      "context_text": "Each level of the encoder-decoder employs multiple Transformer blocks [57], where the number of blocks gradually increases from the top level to the bottom level, facilitating a computationally efficient design.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only the architectural details of the model.",
      "processing_time": 54.21237874031067,
      "citing_paper_id": "278740510",
      "cited_paper_id": 244346144
    },
    {
      "context_text": "Meanwhile, Restormer [57] leverages trans-former architectures to enhance low-level restoration tasks while minimizing computational overhead.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions Restormer, which is a method or model, not a dataset. There are no specific datasets mentioned in the citation context.",
      "processing_time": 56.44038653373718,
      "citing_paper_id": "278740510",
      "cited_paper_id": 244346144
    },
    {
      "context_text": "3(a), using channel attention (Method (a) ) improves by 0.36 dB compared to the baseline [57], but it is 0.15 dB lower than channel shuffle (Method (c) ).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and performance comparisons.",
      "processing_time": 53.89530396461487,
      "citing_paper_id": "278740510",
      "cited_paper_id": 244346144
    },
    {
      "context_text": "Notably, our method surpasses Restormer[57] and NAFNet [3] on average PSNR by 3.04 dB and 2.88 dB, respectively, validating the effectiveness of our approach in handling multiple degradations.",
      "catation_intent": "findings",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only comparisons with other methods. The context focuses on performance metrics rather than dataset usage.",
      "processing_time": 55.85577130317688,
      "citing_paper_id": "278740510",
      "cited_paper_id": 244346144
    },
    {
      "context_text": "Notably, our method surpasses Restormer[57] and NAFNet [3] on average PSNR by 3.04 dB and 2.88 dB, respectively, validating the effectiveness of our approach in handling multiple degradations.",
      "catation_intent": "findings",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only comparisons with other methods. The context focuses on performance metrics rather than dataset usage.",
      "processing_time": 55.85577130317688,
      "citing_paper_id": "278740510",
      "cited_paper_id": 248085491
    },
    {
      "context_text": "Our DFPIR is compared with various general image restoration methods, including Restormer [57], FDGAN [10], and MPRNet [56], as well as specialized all-in-one approaches such as DL [11], AirNet [21], PromptIR [39], and InstructIR [8].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only models and methods. There are no clear identifiers for datasets within the text.",
      "processing_time": 55.844130992889404,
      "citing_paper_id": "278740510",
      "cited_paper_id": 244346144
    },
    {
      "context_text": "NAFNet [3] simplifies the network structure with lightweight channel attention and gated mechanisms, offering an alternative to non-linear activations.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (NAFNet) and its features. There are no verifiable resources or datasets mentioned.",
      "processing_time": 56.82564902305603,
      "citing_paper_id": "278740510",
      "cited_paper_id": 248085491
    },
    {
      "context_text": "…recent deep learning-based approaches have significantly advanced the performance of various image restoration tasks, such as denoising [13, 26, 35, 36, 42, 43], de-raining [2, 22, 24, 27, 47], dehazing [5, 9, 29, 40, 45, 51], deblurring [4, 7, 18, 19, 37, 37, 60], and low-light enhancement…",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various image restoration tasks. No verifiable resources are identified.",
      "processing_time": 55.52769136428833,
      "citing_paper_id": "278740510",
      "cited_paper_id": 253321998
    },
    {
      "context_text": "…learning-based approaches have significantly advanced the performance of various image restoration tasks, such as denoising [13, 26, 35, 36, 42, 43], de-raining [2, 22, 24, 27, 47], dehazing [5, 9, 29, 40, 45, 51], deblurring [4, 7, 18, 19, 37, 37, 60], and low-light enhancement [12, 31, 52],…",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various image restoration tasks and related papers. No verifiable resources are identified.",
      "processing_time": 56.04659295082092,
      "citing_paper_id": "278740510",
      "cited_paper_id": 258509678
    },
    {
      "context_text": "Zhang et al. [59] propose an ingredient-oriented strategy that supports up to five restoration tasks within a single model, significantly enhancing scalability.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method or strategy for image restoration.",
      "processing_time": 54.51315903663635,
      "citing_paper_id": "278740510",
      "cited_paper_id": 260085391
    },
    {
      "context_text": "To further validate the method’s effectiveness in addressing a broader range of tasks, building upon the recent research conducted by IDR [59] and InstructIR [8], we extend our investigation into the efficacy of DFPIR by conducting experiments across five restoration tasks: dehazing, derain-ing,…",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only restoration tasks. No clear identifiers for datasets are provided.",
      "processing_time": 55.0913360118866,
      "citing_paper_id": "278740510",
      "cited_paper_id": 260085391
    },
    {
      "context_text": "…image restoration methods can be roughly divided into two categories: (1) one solution is to modify the parameter space to fit the model for different degradations [21, 38, 49, 59]; (2) and the other solution is to modify the feature space to align with the shared parameter space [8, 39, 55].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only categories of image restoration methods. No verifiable resources are identified.",
      "processing_time": 55.835577964782715,
      "citing_paper_id": "278740510",
      "cited_paper_id": 260085391
    },
    {
      "context_text": "…image restoration methods can be roughly divided into two categories: (1) one solution is to modify the parameter space to fit the model for different degradations [21, 38, 49, 59]; (2) and the other solution is to modify the feature space to align with the shared parameter space [8, 39, 55].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only categories of image restoration methods. No verifiable resources are identified.",
      "processing_time": 55.835577964782715,
      "citing_paper_id": "278740510",
      "cited_paper_id": 268064460
    },
    {
      "context_text": "…image restoration methods can be roughly divided into two categories: (1) one solution is to modify the parameter space to fit the model for different degradations [21, 38, 49, 59]; (2) and the other solution is to modify the feature space to align with the shared parameter space [8, 39, 55].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only categories of image restoration methods. No verifiable resources are identified.",
      "processing_time": 55.835577964782715,
      "citing_paper_id": "278740510",
      "cited_paper_id": 270123740
    },
    {
      "context_text": "Recently, all-in-one approaches(also known as multi-degradation or multi-task image restoration) have gained prominence by addressing multiple image degradations within a unified model [6, 21, 38, 39, 49, 55, 59].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to papers discussing all-in-one approaches in image restoration.",
      "processing_time": 55.642226457595825,
      "citing_paper_id": "278740510",
      "cited_paper_id": 260085391
    },
    {
      "context_text": "Recently, all-in-one approaches(also known as multi-degradation or multi-task image restoration) have gained prominence by addressing multiple image degradations within a unified model [6, 21, 38, 39, 49, 55, 59].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to papers discussing all-in-one approaches in image restoration.",
      "processing_time": 55.642226457595825,
      "citing_paper_id": "278740510",
      "cited_paper_id": 268064460
    },
    {
      "context_text": "Recently, all-in-one approaches(also known as multi-degradation or multi-task image restoration) have gained prominence by addressing multiple image degradations within a unified model [6, 21, 38, 39, 49, 55, 59].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to papers discussing all-in-one approaches in image restoration.",
      "processing_time": 55.642226457595825,
      "citing_paper_id": "278740510",
      "cited_paper_id": 270123740
    },
    {
      "context_text": "Multi-task image restoration aims to address multiple tasks using the same network design [8, 21, 38, 39, 49, 55, 59].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only the concept of multi-task image restoration. No dataset names are present in the context.",
      "processing_time": 56.59481191635132,
      "citing_paper_id": "278740510",
      "cited_paper_id": 260085391
    },
    {
      "context_text": "Multi-task image restoration aims to address multiple tasks using the same network design [8, 21, 38, 39, 49, 55, 59].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only the concept of multi-task image restoration. No dataset names are present in the context.",
      "processing_time": 56.59481191635132,
      "citing_paper_id": "278740510",
      "cited_paper_id": 268064460
    },
    {
      "context_text": "Multi-task image restoration aims to address multiple tasks using the same network design [8, 21, 38, 39, 49, 55, 59].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only the concept of multi-task image restoration. No dataset names are present in the context.",
      "processing_time": 56.59481191635132,
      "citing_paper_id": "278740510",
      "cited_paper_id": 270123740
    },
    {
      "context_text": "Similarly, Zhang et al. [58] introduce a representation learning network guided by degradation classification, using its strong classification capabilities to effectively steer the restoration process.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method or network. The context focuses on the introduction of a representation learning network for image restoration.",
      "processing_time": 17.328696966171265,
      "citing_paper_id": "278740510",
      "cited_paper_id": 260680793
    },
    {
      "context_text": "…recent deep learning-based approaches have significantly advanced the performance of various image restoration tasks, such as denoising [13, 26, 35, 36, 42, 43], de-raining [2, 22, 24, 27, 47], dehazing [5, 9, 29, 40, 45, 51], deblurring [4, 7, 18, 19, 37, 37, 60], and low-light…",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various image restoration tasks. No verifiable resources are identified.",
      "processing_time": 55.640186071395874,
      "citing_paper_id": "278740510",
      "cited_paper_id": 260887508
    },
    {
      "context_text": "In line with previous work [21, 39], we prepare datasets tailored for various restoration tasks.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.3,
      "reasoning": "The context mentions preparing datasets for restoration tasks but does not specify any named datasets. The term 'datasets' is too generic.",
      "processing_time": 56.18745517730713,
      "citing_paper_id": "278740510",
      "cited_paper_id": 268064460
    },
    {
      "context_text": "PromptIR [39] encodes degradation-specific information through prompts, using them to dynamically guide the restoration network.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a method called PromptIR. The title confirms that PromptIR is a method, not a dataset.",
      "processing_time": 57.19136452674866,
      "citing_paper_id": "278740510",
      "cited_paper_id": 268064460
    },
    {
      "context_text": "Specifically, PromptIR [39] conducts multi-degradation processing by introducing additional implicit prompts.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method called PromptIR. The context is about the methodology and not about using a particular dataset.",
      "processing_time": 57.188234090805054,
      "citing_paper_id": "278740510",
      "cited_paper_id": 268064460
    },
    {
      "context_text": "In order to fully utilize the inherent features of the image while reducing the influence of different degradation features, motivated by the “hard” routing strategy [55] and PromptIR [39], we propose",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only methods and strategies. The cited papers' titles do not provide additional dataset information.",
      "processing_time": 56.38301062583923,
      "citing_paper_id": "278740510",
      "cited_paper_id": 268064460
    },
    {
      "context_text": "In order to fully utilize the inherent features of the image while reducing the influence of different degradation features, motivated by the “hard” routing strategy [55] and PromptIR [39], we propose",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only methods and strategies. The cited papers' titles do not provide additional dataset information.",
      "processing_time": 56.38301062583923,
      "citing_paper_id": "278740510",
      "cited_paper_id": 270123740
    },
    {
      "context_text": "In this section, we follow the protocols of prior state-of-the-art works [21, 39] to conduct experiments under two settings: (a) all-in-one and (b) single-task.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only experimental settings and protocols.",
      "processing_time": 54.47934412956238,
      "citing_paper_id": "278740510",
      "cited_paper_id": 268064460
    },
    {
      "context_text": "Another approach is to introduce image or degradation prompts to modulate the features, adapting to a unified parameter space [8, 39, 55].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods or approaches. The context is about using prompts for image restoration, which is a methodological approach rather than a dataset.",
      "processing_time": 58.41259574890137,
      "citing_paper_id": "278740510",
      "cited_paper_id": 268064460
    },
    {
      "context_text": "Another approach is to introduce image or degradation prompts to modulate the features, adapting to a unified parameter space [8, 39, 55].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods or approaches. The context is about using prompts for image restoration, which is a methodological approach rather than a dataset.",
      "processing_time": 58.41259574890137,
      "citing_paper_id": "278740510",
      "cited_paper_id": 270123740
    },
    {
      "context_text": "Following the configuration of PromptIR [39], our DFPIR architecture features a 4-level encoder-decoder structure, comprising varying numbers of Transformer blocks at each level, specifically [4, 6, 6, 8] from level-1 to level-4.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only the architecture configuration. The context is focused on the methodological details of the DFPIR architecture.",
      "processing_time": 57.33793783187866,
      "citing_paper_id": "278740510",
      "cited_paper_id": 268064460
    },
    {
      "context_text": "For example, MedIR [55] demonstrates that the gradient update directions between different tasks are inconsistent or even opposite.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method or finding related to task inconsistency in medical image restoration.",
      "processing_time": 56.0069055557251,
      "citing_paper_id": "278740510",
      "cited_paper_id": 270123740
    },
    {
      "context_text": "MedIR [55] introduces multiple experts into the feature space modulation to implement a task-adaptive routing strategy.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method or approach called MedIR. The context focuses on the methodology rather than the data used.",
      "processing_time": 56.55242323875427,
      "citing_paper_id": "278740510",
      "cited_paper_id": 270123740
    },
    {
      "context_text": "Subsequently, the ERFM is independently tested on each degradation scenario, such as ❶ Rain100L [43] for deraining, ❷ SOTS-Outdoor [44] for dehazing, ❸ BSD68 [45], Urban100 [49], and Kodak24 [50] for denoising, ❹ GoPro [47] for deblurring, and ❺ LOL [48] for low-light enhancement.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Rain100L",
        "SOTS-Outdoor",
        "BSD68",
        "Urban100",
        "Kodak24",
        "GoPro",
        "LOL"
      ],
      "dataset_descriptions": {
        "Rain100L": "Used for deraining, evaluating the ERFM's performance in removing rain streaks from images.",
        "SOTS-Outdoor": "Used for dehazing, assessing the ERFM's effectiveness in enhancing visibility in hazy outdoor scenes.",
        "BSD68": "Used for denoising, testing the ERFM's ability to reduce noise in images.",
        "Urban100": "Used for denoising, evaluating the ERFM's performance in urban environments.",
        "Kodak24": "Used for denoising, assessing the ERFM's capability in reducing noise in high-quality images.",
        "GoPro": "Used for deblurring, testing the ERFM's ability to restore sharpness in blurred images.",
        "LOL": "Used for low-light enhancement, evaluating the ERFM's performance in improving visibility in low-light conditions."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions specific datasets used for testing the ERFM under various image degradation scenarios. Each dataset is clearly identified and used for a specific purpose.",
      "processing_time": 98.79749464988708,
      "citing_paper_id": "275218066",
      "cited_paper_id": 8282555
    },
    {
      "context_text": "Subsequently, the ERFM is independently tested on each degradation scenario, such as ❶ Rain100L [43] for deraining, ❷ SOTS-Outdoor [44] for dehazing, ❸ BSD68 [45], Urban100 [49], and Kodak24 [50] for denoising, ❹ GoPro [47] for deblurring, and ❺ LOL [48] for low-light enhancement.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Rain100L",
        "SOTS-Outdoor",
        "BSD68",
        "Urban100",
        "Kodak24",
        "GoPro",
        "LOL"
      ],
      "dataset_descriptions": {
        "Rain100L": "Used for deraining, evaluating the ERFM's performance in removing rain streaks from images.",
        "SOTS-Outdoor": "Used for dehazing, assessing the ERFM's effectiveness in enhancing visibility in hazy outdoor scenes.",
        "BSD68": "Used for denoising, testing the ERFM's ability to reduce noise in images.",
        "Urban100": "Used for denoising, evaluating the ERFM's performance in urban environments.",
        "Kodak24": "Used for denoising, assessing the ERFM's capability in reducing noise in high-quality images.",
        "GoPro": "Used for deblurring, testing the ERFM's ability to restore sharpness in blurred images.",
        "LOL": "Used for low-light enhancement, evaluating the ERFM's performance in improving visibility in low-light conditions."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions specific datasets used for testing the ERFM under various image degradation scenarios. Each dataset is clearly identified and used for a specific purpose.",
      "processing_time": 98.79749464988708,
      "citing_paper_id": "275218066",
      "cited_paper_id": 52008443
    },
    {
      "context_text": "remote sensing [3].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific dataset names, only a general reference to remote sensing. No multi-word proper nouns, acronyms, or hyphenated names with digits are present.",
      "processing_time": 58.84997582435608,
      "citing_paper_id": "275218066",
      "cited_paper_id": 28487645
    },
    {
      "context_text": "The ERFM is trained on a hybrid dataset by combining ❶ Rain200L [43] for deraining, ❷ RESIDE [44] for dehazing, ❸ BSD400 [45] and WED [46] for denoising, ❹ GoPro [47] for deblurring, and ❺ LOL [48] for low-light enhancement.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Rain200L",
        "RESIDE",
        "BSD400",
        "WED",
        "GoPro",
        "LOL"
      ],
      "dataset_descriptions": {
        "Rain200L": "Used for deraining, providing synthetic rain images to train the model on removing rain streaks from images.",
        "RESIDE": "Used for dehazing, offering hazy images to train the model on enhancing visibility in hazy conditions.",
        "BSD400": "Used for denoising, providing noisy images to train the model on reducing noise while preserving details.",
        "WED": "Used for denoising, complementing BSD400 with additional noisy images to enhance denoising capabilities.",
        "GoPro": "Used for deblurring, supplying blurred images to train the model on sharpening and restoring motion-blurred images.",
        "LOL": "Used for low-light enhancement, providing low-light images to train the model on improving brightness and contrast in dark conditions."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions specific datasets used for training the ERFM model on various image restoration tasks. Each dataset is clearly identified and associated with a specific task.",
      "processing_time": 93.01315593719482,
      "citing_paper_id": "275218066",
      "cited_paper_id": 52008443
    },
    {
      "context_text": "Reference [33] simplified the training process of independent models for different degradation types by optimizing task-specific encoders.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for simplifying the training process of independent models for different degradation types.",
      "processing_time": 56.549129009246826,
      "citing_paper_id": "275218066",
      "cited_paper_id": 219978541
    },
    {
      "context_text": "Although [33] improves training efficiency and saves memory resources, its model still contains considerable parameters.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a model or method. The context is about training efficiency and memory resources, which does not indicate the use of a dataset.",
      "processing_time": 58.613917112350464,
      "citing_paper_id": "275218066",
      "cited_paper_id": 219978541
    },
    {
      "context_text": "For the NLP task, Lu et al. [34] pioneered the research on developing a shared systolic array for the multi-head attention and feed-forward network.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a hardware accelerator for NLP tasks. No verifiable resources are identified.",
      "processing_time": 56.754581689834595,
      "citing_paper_id": "275218066",
      "cited_paper_id": 221802736
    },
    {
      "context_text": "This real number pair needs to satisfy the following conditions [41]: By this means, g ( x ) can be formulated by : Since the defined domain of the logarithmic operation g ( x ) is limited to [ 1 , 2 ) , we also utilize a linear approximation log 2 m ≈ m − 1 , m ∈ [ 1 , 2 ) .",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, models, or methods. It focuses on mathematical operations and approximations.",
      "processing_time": 32.99517369270325,
      "citing_paper_id": "275218066",
      "cited_paper_id": 226674012
    },
    {
      "context_text": "Moreover, Transformer-based [4], [5] methods have exhibited better visual effects than convolutional neural networks (CNNs) based methods [6], [7].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods and models. There are no clear identifiers for datasets in the text.",
      "processing_time": 56.97090005874634,
      "citing_paper_id": "275218066",
      "cited_paper_id": 231802205
    },
    {
      "context_text": "Moreover, Transformer-based [4], [5] methods have exhibited better visual effects than convolutional neural networks (CNNs) based methods [6], [7].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods and models. There are no clear identifiers for datasets in the text.",
      "processing_time": 56.97090005874634,
      "citing_paper_id": "275218066",
      "cited_paper_id": 234482841
    },
    {
      "context_text": "2) Setting of the Baselines: To accurately evaluate the algorithm performance, several IR baselines, including task-specific methods [4], [5], [6], [7], [8], [9], [10] and all-in-one fashion methods [11], [12], [13], [14], [15], are chosen on seven challenging IR tasks, such as deraining, dehazing,…",
      "catation_intent": "reusable resource",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to various image restoration methods and tasks. No clear, verifiable datasets are identified.",
      "processing_time": 57.61144304275513,
      "citing_paper_id": "275218066",
      "cited_paper_id": 231802205
    },
    {
      "context_text": "2) Setting of the Baselines: To accurately evaluate the algorithm performance, several IR baselines, including task-specific methods [4], [5], [6], [7], [8], [9], [10] and all-in-one fashion methods [11], [12], [13], [14], [15], are chosen on seven challenging IR tasks, such as deraining, dehazing,…",
      "catation_intent": "reusable resource",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to various image restoration methods and tasks. No clear, verifiable datasets are identified.",
      "processing_time": 57.61144304275513,
      "citing_paper_id": "275218066",
      "cited_paper_id": 234482841
    },
    {
      "context_text": "…Baselines: To accurately evaluate the algorithm performance, several IR baselines, including task-specific methods [4], [5], [6], [7], [8], [9], [10] and all-in-one fashion methods [11], [12], [13], [14], [15], are chosen on seven challenging IR tasks, such as deraining, dehazing, denoising ( σ…",
      "catation_intent": "reusable resource",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.0,
      "reasoning": "The citation span does not mention any specific datasets, only references to various image restoration (IR) tasks and methods. No clear, verifiable datasets are identified.",
      "processing_time": 58.35960578918457,
      "citing_paper_id": "275218066",
      "cited_paper_id": 237266491
    },
    {
      "context_text": "(ii) As for the comparison with some task-specific methods, we can achieve better IR results ( e.g. , up to ↑ 4.1 ∼↑ 4.6 dB PSNR on average over SwinIR [10]).",
      "catation_intent": "findings",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions 'SwinIR' but does not refer to it as a dataset. It is likely a method or model, and thus excluded.",
      "processing_time": 57.61589574813843,
      "citing_paper_id": "275218066",
      "cited_paper_id": 237266491
    },
    {
      "context_text": "To break through this limitation, SwinIR [10] leveraged local attention and a sliding window scheme to aggregate multi-scale information, showing great promise in the field of IR.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets used in the research. It focuses on the method (SwinIR) and its capabilities.",
      "processing_time": 57.1287784576416,
      "citing_paper_id": "275218066",
      "cited_paper_id": 237266491
    },
    {
      "context_text": "Some approaches [8], [9], [10] integrate known prior into the IR models to address specific tasks.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to prior work. The cited paper title is about a method (SwinIR) rather than a dataset.",
      "processing_time": 58.81788754463196,
      "citing_paper_id": "275218066",
      "cited_paper_id": 237266491
    },
    {
      "context_text": "Some studies [12], [32] have focused on all-in-one IR by constructing multi-input-output networks.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a focus on all-in-one image restoration using multi-input-output networks.",
      "processing_time": 56.729620695114136,
      "citing_paper_id": "275218066",
      "cited_paper_id": 244714491
    },
    {
      "context_text": "9 illustrates the comparative visual effects of the ERFM and five other IR methods [4], [9], [12], [13], [14] on restored images or cropped regions.",
      "catation_intent": "reusable resource",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods and visual effects comparisons. No dataset names are provided.",
      "processing_time": 56.714603424072266,
      "citing_paper_id": "275218066",
      "cited_paper_id": 244714491
    },
    {
      "context_text": "9 illustrates the comparative visual effects of the ERFM and five other IR methods [4], [9], [12], [13], [14] on restored images or cropped regions.",
      "catation_intent": "reusable resource",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods and visual effects comparisons. No dataset names are provided.",
      "processing_time": 56.714603424072266,
      "citing_paper_id": "275218066",
      "cited_paper_id": 247411392
    },
    {
      "context_text": "9 illustrates the comparative visual effects of the ERFM and five other IR methods [4], [9], [12], [13], [14] on restored images or cropped regions.",
      "catation_intent": "reusable resource",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods and visual effects comparisons. No dataset names are provided.",
      "processing_time": 56.714603424072266,
      "citing_paper_id": "275218066",
      "cited_paper_id": 250551851
    },
    {
      "context_text": "To address this problem, [12] further simplified multiple specialized CNN-based encoders into a single Transformer-based encoder and introduced weather-related queries to assist in weather removal.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for image restoration using a Transformer-based encoder.",
      "processing_time": 56.11081385612488,
      "citing_paper_id": "275218066",
      "cited_paper_id": 244714491
    },
    {
      "context_text": "Recently, various all-in-one methods [11], [12], [13], [14], [15] have emerged for multi-task IR.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to various all-in-one methods for multi-task image restoration. No dataset names are provided.",
      "processing_time": 57.95044183731079,
      "citing_paper_id": "275218066",
      "cited_paper_id": 244714491
    },
    {
      "context_text": "Recently, various all-in-one methods [11], [12], [13], [14], [15] have emerged for multi-task IR.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to various all-in-one methods for multi-task image restoration. No dataset names are provided.",
      "processing_time": 57.95044183731079,
      "citing_paper_id": "275218066",
      "cited_paper_id": 247411392
    },
    {
      "context_text": "Recently, various all-in-one methods [11], [12], [13], [14], [15] have emerged for multi-task IR.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to various all-in-one methods for multi-task image restoration. No dataset names are provided.",
      "processing_time": 57.95044183731079,
      "citing_paper_id": "275218066",
      "cited_paper_id": 250551851
    },
    {
      "context_text": "Recently, various all-in-one methods [11], [12], [13], [14], [15] have emerged for multi-task IR.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to various all-in-one methods for multi-task image restoration. No dataset names are provided.",
      "processing_time": 57.95044183731079,
      "citing_paper_id": "275218066",
      "cited_paper_id": 260085391
    },
    {
      "context_text": "…performance, several IR baselines, including task-specific methods [4], [5], [6], [7], [8], [9], [10] and all-in-one fashion methods [11], [12], [13], [14], [15], are chosen on seven challenging IR tasks, such as deraining, dehazing, denoising ( σ ∈ { 15 , 25 , 50 } ), deblurring, and…",
      "catation_intent": "reusable resource",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.0,
      "reasoning": "The citation span does not mention any specific datasets, only various image restoration tasks and methods. No clear, verifiable datasets are identified.",
      "processing_time": 57.59530520439148,
      "citing_paper_id": "275218066",
      "cited_paper_id": 244714491
    },
    {
      "context_text": "…performance, several IR baselines, including task-specific methods [4], [5], [6], [7], [8], [9], [10] and all-in-one fashion methods [11], [12], [13], [14], [15], are chosen on seven challenging IR tasks, such as deraining, dehazing, denoising ( σ ∈ { 15 , 25 , 50 } ), deblurring, and low-light…",
      "catation_intent": "reusable resource",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only tasks and methods. No clear identifiers for datasets are present.",
      "processing_time": 56.93929958343506,
      "citing_paper_id": "275218066",
      "cited_paper_id": 247411392
    },
    {
      "context_text": "Prompt learning [16] is a pivotal technique in artificial intelligence generated content (AIGC).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a technique called 'prompt learning'. No verifiable resources are identified.",
      "processing_time": 56.92941617965698,
      "citing_paper_id": "275218066",
      "cited_paper_id": 248562703
    },
    {
      "context_text": "Prompt learning [16] employs non-textual information to enhance the model input, making it essential in areas like large language models (LLMs).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (prompt learning) and its application in enhancing model input for large language models.",
      "processing_time": 57.58659100532532,
      "citing_paper_id": "275218066",
      "cited_paper_id": 248562703
    },
    {
      "context_text": "…several IR baselines, including task-specific methods [4], [5], [6], [7], [8], [9], [10] and all-in-one fashion methods [11], [12], [13], [14], [15], are chosen on seven challenging IR tasks, such as deraining, dehazing, denoising ( σ ∈ { 15 , 25 , 50 } ), deblurring, and low-light…",
      "catation_intent": "reusable resource",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only tasks and methods. The context focuses on comparing various image restoration methods across multiple tasks.",
      "processing_time": 57.598812103271484,
      "citing_paper_id": "275218066",
      "cited_paper_id": 250551851
    },
    {
      "context_text": "To mitigate the reliance on prior knowledge of degradation types and levels, AirNet [14] extracted degradation representations through contrastive learning, demonstrating strong flexibility and economy in real-world scenarios.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a method (AirNet) and its capabilities. No verifiable resources are identified.",
      "processing_time": 57.44060468673706,
      "citing_paper_id": "275218066",
      "cited_paper_id": 250551851
    },
    {
      "context_text": "Specifically, (i) when compared with classical all-in-one methods, the ERFM achieves ↑ 0.8 ∼↑ 1.3 dB PSNR over the previous best approach IDR [15] and ↑ 3.7 ∼↑ 4.2 dB PSNR over the second-best approach AirNet [14], verifying the effectiveness of our degradation-aware prompt learning scheme.",
      "catation_intent": "findings",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only comparisons with other methods. No verifiable resources are identified.",
      "processing_time": 56.481587409973145,
      "citing_paper_id": "275218066",
      "cited_paper_id": 250551851
    },
    {
      "context_text": "Specifically, (i) when compared with classical all-in-one methods, the ERFM achieves ↑ 0.8 ∼↑ 1.3 dB PSNR over the previous best approach IDR [15] and ↑ 3.7 ∼↑ 4.2 dB PSNR over the second-best approach AirNet [14], verifying the effectiveness of our degradation-aware prompt learning scheme.",
      "catation_intent": "findings",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only comparisons with other methods. No verifiable resources are identified.",
      "processing_time": 56.481587409973145,
      "citing_paper_id": "275218066",
      "cited_paper_id": 260085391
    },
    {
      "context_text": "Observing that DTQAtten [22] and DTATrans [24] utilize more computational resources compared to our design while consuming less area and power, the UIRA exhibits a smaller improvement in energy and area efficiency.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only comparisons between methods in terms of computational resources, area, and power consumption.",
      "processing_time": 57.25131964683533,
      "citing_paper_id": "275218066",
      "cited_paper_id": 256033931
    },
    {
      "context_text": "DTQAtten [22], FACT [23], and DTATrans [24] utilized low-precision and mixed-precision quantization to reduce overall memory footprint and computational density.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and models. The context focuses on quantization techniques and their application in reducing memory and computational requirements.",
      "processing_time": 58.55156135559082,
      "citing_paper_id": "275218066",
      "cited_paper_id": 256033931
    },
    {
      "context_text": "3) Comparison With Other Related Accelerators: We compare the proposed accelerator with previous ASIC-based designs [19], [20], [21], [22], [24], [28], [29], [35] to validate Authorized licensed use limited to the terms of the applicable license agreement with IEEE.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only comparisons with other accelerators. There are no clear identifiers for datasets.",
      "processing_time": 57.2527494430542,
      "citing_paper_id": "275218066",
      "cited_paper_id": 256033931
    },
    {
      "context_text": "Besides, considering different computing features between CNNs and Transformers, Li et al. [36] and Shao et al. [37] proposed reconfigurable architectures to support standard convolution and self-attention computations flexibly.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only reconfigurable architectures for CNNs and Transformers.",
      "processing_time": 55.532461166381836,
      "citing_paper_id": "275218066",
      "cited_paper_id": 260003105
    },
    {
      "context_text": "…several IR baselines, including task-specific methods [4], [5], [6], [7], [8], [9], [10] and all-in-one fashion methods [11], [12], [13], [14], [15], are chosen on seven challenging IR tasks, such as deraining, dehazing, denoising ( σ ∈ { 15 , 25 , 50 } ), deblurring, and low-light enhancement.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only tasks and methods. The context is focused on comparing various image restoration methods across multiple tasks.",
      "processing_time": 57.91665196418762,
      "citing_paper_id": "275218066",
      "cited_paper_id": 260085391
    },
    {
      "context_text": "Besides, Softermax [25], ELSA [26], and Huang et al. [27] focused on approximating the nonlinear operations, such as softmax and layer normalization (LN), to improve the processing speed.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and models. The context is about improving processing speed through approximation of nonlinear operations.",
      "processing_time": 57.56149649620056,
      "citing_paper_id": "275218066",
      "cited_paper_id": 264363267
    },
    {
      "context_text": "Moreover, for challenging nonlinear operations, the polynomial approximation and lookup table employed by current accelerators [25], [27] inevitably introduce additional hardware overhead and accuracy loss.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only discusses hardware and software methods.",
      "processing_time": 55.70560693740845,
      "citing_paper_id": "275218066",
      "cited_paper_id": 264363267
    },
    {
      "context_text": "For example, PromptIR [17] encoded degradation-specific information and guided the Transformer-based backbone without prior knowledge about the degradations.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a method called PromptIR. The citation is focused on describing the method rather than a dataset.",
      "processing_time": 58.54526734352112,
      "citing_paper_id": "275218066",
      "cited_paper_id": 268064460
    },
    {
      "context_text": "Due to its high flexibility and scalability, recent efforts [17], [18] have aimed to apply prompt learning to construct Transformer-based models for the all-in-one IR.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only the application of prompt learning to Transformer-based models for all-in-one image restoration.",
      "processing_time": 57.90558624267578,
      "citing_paper_id": "275218066",
      "cited_paper_id": 268064460
    },
    {
      "context_text": "PromptIR [17] applied prompt learning to guide the Transformer networks dynamically to restore corrupted images.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.3,
      "reasoning": "The context mentions 'corrupted images' which suggests the use of a dataset for image restoration, but no specific dataset name is provided.",
      "processing_time": 58.29157996177673,
      "citing_paper_id": "275218066",
      "cited_paper_id": 268064460
    },
    {
      "context_text": "The mean µ and L1-norm variance σ can be formulated as : When the inputs of the L1LN follow the Gaussian distribution, the L1-norm variance is linearly correlated with the L2-norm variance [39].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only discusses mathematical formulations and properties of norms.",
      "processing_time": 56.06231093406677,
      "citing_paper_id": "275218066",
      "cited_paper_id": null
    },
    {
      "context_text": "Most AIOR methods are trained using a single dataset for each restoration task such as RESIDE [23] for dehazing, Snow100k [29] for desnowing, Rain13K [49] for deraining and GoPro [33] dataset for motion deblurring.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "RESIDE",
        "Snow100k",
        "Rain13K",
        "GoPro"
      ],
      "dataset_descriptions": {
        "RESIDE": "Used for training dehazing models, focusing on removing atmospheric haze from images to improve clarity and visibility.",
        "Snow100k": "Used for training desnowing models, specifically targeting the removal of snow artifacts from images to enhance visual quality.",
        "Rain13K": "Used for training deraining models, aimed at removing rain streaks from images to restore clear visual content.",
        "GoPro": "Used for training motion deblurring models, focusing on restoring sharpness to images affected by camera or object motion."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions specific datasets used for training AIOR methods for various image restoration tasks. Each dataset is clearly identified and associated with a specific task.",
      "processing_time": 82.96689772605896,
      "citing_paper_id": "274281273",
      "cited_paper_id": 837707
    },
    {
      "context_text": "Most AIOR methods are trained using a single dataset for each restoration task such as RESIDE [23] for dehazing, Snow100k [29] for desnowing, Rain13K [49] for deraining and GoPro [33] dataset for motion deblurring.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "RESIDE",
        "Snow100k",
        "Rain13K",
        "GoPro"
      ],
      "dataset_descriptions": {
        "RESIDE": "Used for training dehazing models, focusing on removing atmospheric haze from images to improve clarity and visibility.",
        "Snow100k": "Used for training desnowing models, specifically targeting the removal of snow artifacts from images to enhance visual quality.",
        "Rain13K": "Used for training deraining models, aimed at removing rain streaks from images to restore clear visual content.",
        "GoPro": "Used for training motion deblurring models, focusing on restoring sharpness to images affected by camera or object motion."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions specific datasets used for training AIOR methods for various image restoration tasks. Each dataset is clearly identified and associated with a specific task.",
      "processing_time": 82.96689772605896,
      "citing_paper_id": "274281273",
      "cited_paper_id": 8671030
    },
    {
      "context_text": "REVIDE [53] O-Haze [1] RainDS [35] LHP [15] RSVD [8] GoPro [33] LOLv1 [44] SICE [6] RainDS [35] Degradation lightweight convolutional decoder.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "REVIDE",
        "O-Haze",
        "RainDS",
        "LHP",
        "RSVD",
        "GoPro",
        "LOLv1",
        "SICE"
      ],
      "dataset_descriptions": {
        "REVIDE": "Used for video deblurring, providing dynamic scene data to train and evaluate deblurring models.",
        "O-Haze": "Used for haze removal, offering real-world hazy images to train and test haze removal algorithms.",
        "RainDS": "Used for rain removal, providing synthetic and real rain images to train and evaluate rain removal methods.",
        "LHP": "Used for low-light image enhancement, offering low-light images to train and test enhancement models.",
        "RSVD": "Used for video deblurring, providing real-world blurred video data to train and evaluate deblurring models.",
        "GoPro": "Used for image deblurring, offering sharp and blurred image pairs to train and test deblurring algorithms.",
        "LOLv1": "Used for low-light image enhancement, providing low-light and normal-light image pairs to train and test enhancement models.",
        "SICE": "Used for single image contrast enhancement, offering multi-exposure images to train and test contrast enhancement methods."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions several datasets that are used for image restoration tasks, including deblurring, haze removal, and contrast enhancement. These datasets are likely used for training and evaluation.",
      "processing_time": 103.04650545120239,
      "citing_paper_id": "274281273",
      "cited_paper_id": 8671030
    },
    {
      "context_text": "REVIDE [53] O-Haze [1] RainDS [35] LHP [15] RSVD [8] GoPro [33] LOLv1 [44] SICE [6] RainDS [35] Degradation lightweight convolutional decoder.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "REVIDE",
        "O-Haze",
        "RainDS",
        "LHP",
        "RSVD",
        "GoPro",
        "LOLv1",
        "SICE"
      ],
      "dataset_descriptions": {
        "REVIDE": "Used for video deblurring, providing dynamic scene data to train and evaluate deblurring models.",
        "O-Haze": "Used for haze removal, offering real-world hazy images to train and test haze removal algorithms.",
        "RainDS": "Used for rain removal, providing synthetic and real rain images to train and evaluate rain removal methods.",
        "LHP": "Used for low-light image enhancement, offering low-light images to train and test enhancement models.",
        "RSVD": "Used for video deblurring, providing real-world blurred video data to train and evaluate deblurring models.",
        "GoPro": "Used for image deblurring, offering sharp and blurred image pairs to train and test deblurring algorithms.",
        "LOLv1": "Used for low-light image enhancement, providing low-light and normal-light image pairs to train and test enhancement models.",
        "SICE": "Used for single image contrast enhancement, offering multi-exposure images to train and test contrast enhancement methods."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions several datasets that are used for image restoration tasks, including deblurring, haze removal, and contrast enhancement. These datasets are likely used for training and evaluation.",
      "processing_time": 103.04650545120239,
      "citing_paper_id": "274281273",
      "cited_paper_id": 19494799
    },
    {
      "context_text": "REVIDE [53] O-Haze [1] RainDS [35] LHP [15] RSVD [8] GoPro [33] LOLv1 [44] SICE [6] RainDS [35] Degradation lightweight convolutional decoder.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "REVIDE",
        "O-Haze",
        "RainDS",
        "LHP",
        "RSVD",
        "GoPro",
        "LOLv1",
        "SICE"
      ],
      "dataset_descriptions": {
        "REVIDE": "Used for video deblurring, providing dynamic scene data to train and evaluate deblurring models.",
        "O-Haze": "Used for haze removal, offering real-world hazy images to train and test haze removal algorithms.",
        "RainDS": "Used for rain removal, providing synthetic and real rain images to train and evaluate rain removal methods.",
        "LHP": "Used for low-light image enhancement, offering low-light images to train and test enhancement models.",
        "RSVD": "Used for video deblurring, providing real-world blurred video data to train and evaluate deblurring models.",
        "GoPro": "Used for image deblurring, offering sharp and blurred image pairs to train and test deblurring algorithms.",
        "LOLv1": "Used for low-light image enhancement, providing low-light and normal-light image pairs to train and test enhancement models.",
        "SICE": "Used for single image contrast enhancement, offering multi-exposure images to train and test contrast enhancement methods."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions several datasets that are used for image restoration tasks, including deblurring, haze removal, and contrast enhancement. These datasets are likely used for training and evaluation.",
      "processing_time": 103.04650545120239,
      "citing_paper_id": "274281273",
      "cited_paper_id": 235719890
    },
    {
      "context_text": "REVIDE [53] O-Haze [1] RainDS [35] LHP [15] RSVD [8] GoPro [33] LOLv1 [44] SICE [6] RainDS [35] Degradation lightweight convolutional decoder.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "REVIDE",
        "O-Haze",
        "RainDS",
        "LHP",
        "RSVD",
        "GoPro",
        "LOLv1",
        "SICE"
      ],
      "dataset_descriptions": {
        "REVIDE": "Used for video deblurring, providing dynamic scene data to train and evaluate deblurring models.",
        "O-Haze": "Used for haze removal, offering real-world hazy images to train and test haze removal algorithms.",
        "RainDS": "Used for rain removal, providing synthetic and real rain images to train and evaluate rain removal methods.",
        "LHP": "Used for low-light image enhancement, offering low-light images to train and test enhancement models.",
        "RSVD": "Used for video deblurring, providing real-world blurred video data to train and evaluate deblurring models.",
        "GoPro": "Used for image deblurring, offering sharp and blurred image pairs to train and test deblurring algorithms.",
        "LOLv1": "Used for low-light image enhancement, providing low-light and normal-light image pairs to train and test enhancement models.",
        "SICE": "Used for single image contrast enhancement, offering multi-exposure images to train and test contrast enhancement methods."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions several datasets that are used for image restoration tasks, including deblurring, haze removal, and contrast enhancement. These datasets are likely used for training and evaluation.",
      "processing_time": 103.04650545120239,
      "citing_paper_id": "274281273",
      "cited_paper_id": 267026544
    },
    {
      "context_text": "Conversely, motion blur and rain have a de-cent number of real-world examples as they can be generated from existing videos [15, 25, 33, 38, 43].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only that motion blur and rain have a decent number of real-world examples. No clear, verifiable dataset names are provided.",
      "processing_time": 59.50782752037048,
      "citing_paper_id": "274281273",
      "cited_paper_id": 8671030
    },
    {
      "context_text": "Early approaches focused on handling specific degradations such as haze [16, 51], rain [19, 43], snow [11, 52], blur [33, 38] etc.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various image degradation types and related works. No verifiable resources are identified.",
      "processing_time": 58.086792945861816,
      "citing_paper_id": "274281273",
      "cited_paper_id": 8671030
    },
    {
      "context_text": "Early approaches focused on handling specific degradations such as haze [16, 51], rain [19, 43], snow [11, 52], blur [33, 38] etc.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various image degradation types and related works. No verifiable resources are identified.",
      "processing_time": 58.086792945861816,
      "citing_paper_id": "274281273",
      "cited_paper_id": 15197266
    },
    {
      "context_text": "Early approaches focused on handling specific degradations such as haze [16, 51], rain [19, 43], snow [11, 52], blur [33, 38] etc.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various image degradation types and related works. No verifiable resources are identified.",
      "processing_time": 58.086792945861816,
      "citing_paper_id": "274281273",
      "cited_paper_id": 26910155
    },
    {
      "context_text": "For instance, the images in the RESIDE [23] dataset are generated by the atmospheric haze model [7] with specific parameters.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "RESIDE"
      ],
      "dataset_descriptions": {
        "RESIDE": "Used to generate images with specific parameters for the atmospheric haze model, focusing on simulating realistic hazy conditions for image restoration research."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions the RESIDE dataset, which is used to generate images with specific parameters for the atmospheric haze model. This is a specific, verifiable dataset relevant to image restoration.",
      "processing_time": 66.27286171913147,
      "citing_paper_id": "274281273",
      "cited_paper_id": 20656092
    },
    {
      "context_text": "We hypothesize that trans-former encoders pre-trained on large datasets such as ImageNet [14] can serve as effective feature encoders for improving generalization in restoration tasks.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "ImageNet"
      ],
      "dataset_descriptions": {
        "ImageNet": "Used to pre-train transformer encoders, aiming to improve generalization in image restoration tasks by leveraging large-scale hierarchical image data."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions ImageNet as a pre-training dataset for transformer encoders, which is relevant to the research on improving generalization in image restoration tasks.",
      "processing_time": 65.19292688369751,
      "citing_paper_id": "274281273",
      "cited_paper_id": 57246310
    },
    {
      "context_text": "Significant improvements can be observed across all degradations.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general improvements across degradations. No dataset names are provided.",
      "processing_time": 14.85242247581482,
      "citing_paper_id": "274281273",
      "cited_paper_id": 195657934
    },
    {
      "context_text": "Significant improvements can be observed across all degradations.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general improvements across degradations. No dataset names are provided.",
      "processing_time": 14.85242247581482,
      "citing_paper_id": "274281273",
      "cited_paper_id": 202542259
    },
    {
      "context_text": "We specifically choose the Swin Transformer [30] over the standard Vision Transformer (ViT) as it provides hierarchical features at multiple resolutions, which is crucial for preserving fine details in restored images.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a method (Swin Transformer) and its advantages over another method (Vision Transformer).",
      "processing_time": 58.73047089576721,
      "citing_paper_id": "274281273",
      "cited_paper_id": 232352874
    },
    {
      "context_text": "However, classification tasks do not require preservation of intricate details in the generated images.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, models, or methods. It only discusses a general characteristic of classification tasks.",
      "processing_time": 57.86441159248352,
      "citing_paper_id": "274281273",
      "cited_paper_id": 234357913
    },
    {
      "context_text": "More recent methods such as Restormer [50], MPR-Net [49] and SwinIR [26] proposed architectures capable of addressing any single restoration task.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions several methods but does not refer to any specific datasets. The context is focused on describing recent methods for image restoration.",
      "processing_time": 58.24984884262085,
      "citing_paper_id": "274281273",
      "cited_paper_id": 244346144
    },
    {
      "context_text": "Airnet and [12] used contrastive loss to learn well-separated degradation representations.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (contrastive loss) and a concept (degradation representations).",
      "processing_time": 58.26906776428223,
      "citing_paper_id": "274281273",
      "cited_paper_id": 250581068
    },
    {
      "context_text": "Recent research has focused on leveraging the potential of latent diffusion models for generating synthetic data.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only a general research direction. No verifiable resources are identified.",
      "processing_time": 57.525432109832764,
      "citing_paper_id": "274281273",
      "cited_paper_id": 251800180
    },
    {
      "context_text": "[3, 4, 39, 40, 48] demonstrated that diffusion-generated images improve classification and zero-shot classification performance.",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only that diffusion-generated images improve classification performance. No clear, verifiable dataset names are provided.",
      "processing_time": 58.69854784011841,
      "citing_paper_id": "274281273",
      "cited_paper_id": 254926772
    },
    {
      "context_text": "[3, 4, 39, 40, 48] demonstrated that diffusion-generated images improve classification and zero-shot classification performance.",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only that diffusion-generated images improve classification performance. No clear, verifiable dataset names are provided.",
      "processing_time": 58.69854784011841,
      "citing_paper_id": "274281273",
      "cited_paper_id": 256616163
    },
    {
      "context_text": "[3, 4, 39, 40, 48] demonstrated that diffusion-generated images improve classification and zero-shot classification performance.",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only that diffusion-generated images improve classification performance. No clear, verifiable dataset names are provided.",
      "processing_time": 58.69854784011841,
      "citing_paper_id": "274281273",
      "cited_paper_id": 257757078
    },
    {
      "context_text": "[3, 4, 39, 40, 48] demonstrated that diffusion-generated images improve classification and zero-shot classification performance.",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only that diffusion-generated images improve classification performance. No clear, verifiable dataset names are provided.",
      "processing_time": 58.69854784011841,
      "citing_paper_id": "274281273",
      "cited_paper_id": 258179174
    },
    {
      "context_text": "[3, 4, 39, 40, 48] demonstrated that diffusion-generated images improve classification and zero-shot classification performance.",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only that diffusion-generated images improve classification performance. No clear, verifiable dataset names are provided.",
      "processing_time": 58.69854784011841,
      "citing_paper_id": "274281273",
      "cited_paper_id": 265506768
    },
    {
      "context_text": "To generate these text descriptions, we process the clean images through the BLIP-2 image captioning model to obtain scene descriptions.",
      "catation_intent": "none",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only the use of the BLIP-2 model for generating text descriptions from clean images.",
      "processing_time": 58.24734973907471,
      "citing_paper_id": "274281273",
      "cited_paper_id": 256390509
    },
    {
      "context_text": "Firstly, the figure shows that existing restoration datasets are significantly smaller than those used to train generalizable models for other low-level vision tasks, such as SAM [21] for segmentation and Depth-Anything [47] for depth estimation ( > 1 .",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions 'restoration datasets' but does not specify any particular dataset names. It compares the size of restoration datasets to those used for other tasks, mentioning SAM and Depth-Anything, which are methods, not datasets.",
      "processing_time": 61.99235272407532,
      "citing_paper_id": "274281273",
      "cited_paper_id": 257952310
    },
    {
      "context_text": "Additionally, we compared the performance of these models against state-of-the-art (SOTA) AIOR models, namely, DiffUIR [55], Diff-Plugin [28], In-structIR [13] and AutoDIR [18].",
      "catation_intent": "none",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only models. No dataset names are present in the context.",
      "processing_time": 57.36389994621277,
      "citing_paper_id": "274281273",
      "cited_paper_id": 267320695
    },
    {
      "context_text": "Additionally, we compared the performance of these models against state-of-the-art (SOTA) AIOR models, namely, DiffUIR [55], Diff-Plugin [28], In-structIR [13] and AutoDIR [18].",
      "catation_intent": "none",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only models. No dataset names are present in the context.",
      "processing_time": 57.36389994621277,
      "citing_paper_id": "274281273",
      "cited_paper_id": 268513542
    },
    {
      "context_text": "Recent approaches include PromptIR [34], DA-CLIP [31], DiffUIR [55], Diff-Plugin [28], InstructIR [13] and AutoDIR [18].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span mentions several methods/models but does not refer to any specific datasets. The cited paper titles also do not mention any datasets.",
      "processing_time": 58.70421123504639,
      "citing_paper_id": "274281273",
      "cited_paper_id": 267320695
    },
    {
      "context_text": "Recent approaches include PromptIR [34], DA-CLIP [31], DiffUIR [55], Diff-Plugin [28], InstructIR [13] and AutoDIR [18].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span mentions several methods/models but does not refer to any specific datasets. The cited paper titles also do not mention any datasets.",
      "processing_time": 58.70421123504639,
      "citing_paper_id": "274281273",
      "cited_paper_id": 268064460
    },
    {
      "context_text": "Recent approaches include PromptIR [34], DA-CLIP [31], DiffUIR [55], Diff-Plugin [28], InstructIR [13] and AutoDIR [18].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span mentions several methods/models but does not refer to any specific datasets. The cited paper titles also do not mention any datasets.",
      "processing_time": 58.70421123504639,
      "citing_paper_id": "274281273",
      "cited_paper_id": 268513542
    },
    {
      "context_text": "Additionally, InstructIR [13] utilized text guidance as instructions for AIOR.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions 'InstructIR' which is a method, not a dataset. No specific dataset is mentioned in the citation span.",
      "processing_time": 58.701690435409546,
      "citing_paper_id": "274281273",
      "cited_paper_id": 267320695
    },
    {
      "context_text": "Finally, we train three models on the GenDS dataset, namely NAFNet [10], PromptIR [34] and a Swin Transformer-based model that we propose.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "GenDS"
      ],
      "dataset_descriptions": {
        "GenDS": "Used to train three models (NAFNet, PromptIR, and a Swin Transformer-based model) for all-in-one image restoration, focusing on the performance and effectiveness of the proposed models."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions the GenDS dataset, which is used for training models for all-in-one image restoration. The dataset is clearly identified and used in the research.",
      "processing_time": 67.95835590362549,
      "citing_paper_id": "274281273",
      "cited_paper_id": 268064460
    },
    {
      "context_text": "Comparisons are shown for PromptIR [34], NAFNet [10] and Swin-transformer models trained with and without our GenDS dataset, along with SOTA models.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "GenDS"
      ],
      "dataset_descriptions": {
        "GenDS": "Used to train and evaluate image restoration models, specifically comparing performance with and without this dataset to establish state-of-the-art results."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions 'GenDS dataset' which is a specific dataset used for training models in the context of all-in-one image restoration.",
      "processing_time": 64.78606605529785,
      "citing_paper_id": "274281273",
      "cited_paper_id": 268064460
    },
    {
      "context_text": "To understand the impact of the GenDS dataset, we initially trained NAFNet [10], PromptIR [34] and the Swin model (Sec.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "GenDS"
      ],
      "dataset_descriptions": {
        "GenDS": "Used to train NAFNet, PromptIR, and the Swin model for all-in-one image restoration, focusing on the effectiveness of the dataset in improving model performance."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions the GenDS dataset, which is a specific dataset used for training models in the context of all-in-one image restoration.",
      "processing_time": 66.64660453796387,
      "citing_paper_id": "274281273",
      "cited_paper_id": 268064460
    },
    {
      "context_text": "PromptIR [34] utilized learnable prompt embed-dings to handle multiple degradations.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a method called PromptIR which uses learnable prompt embeddings for image restoration.",
      "processing_time": 58.46239495277405,
      "citing_paper_id": "274281273",
      "cited_paper_id": 268064460
    },
    {
      "context_text": "In addition to training the Swin Transformer-based architecture described above, we also train two other restoration networks: NAFNet [10] and PromptIR [34], on the combined dataset.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.3,
      "reasoning": "The context mentions 'combined dataset' but does not specify the name of the dataset. No other specific dataset names are mentioned.",
      "processing_time": 58.687028646469116,
      "citing_paper_id": "274281273",
      "cited_paper_id": 268064460
    },
    {
      "context_text": "Quantitative comparisons of NAFNet [10], PromptIR [34], and Swin-transformer models using LPIPS and FID metrics (lower is better), trained with and without our GenDS dataset.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "GenDS"
      ],
      "dataset_descriptions": {
        "GenDS": "Used to train and evaluate NAFNet, PromptIR, and Swin-transformer models for all-in-one image restoration, focusing on improving performance metrics like LPIPS and FID."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions 'GenDS dataset' which is a specific dataset used for training models in the context of image restoration. No other datasets are explicitly named.",
      "processing_time": 67.94397592544556,
      "citing_paper_id": "274281273",
      "cited_paper_id": 268064460
    },
    {
      "context_text": "DiffUIR [55] proposed selective hourglass mapping to create task-specific distributions with high image quality.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method or model. The context focuses on the technique 'selective hourglass mapping' rather than a dataset.",
      "processing_time": 60.3591742515564,
      "citing_paper_id": "274281273",
      "cited_paper_id": 268513542
    },
    {
      "context_text": "Further, [41, 54] showed that augmenting real data with diffusion-generated samples enhances aerial segmentation performance.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.3,
      "reasoning": "The citation mentions 'real data' and 'diffusion-generated samples', but does not specify a named dataset. The context suggests the use of synthetic data augmentation for aerial semantic segmentation.",
      "processing_time": 60.88628578186035,
      "citing_paper_id": "274281273",
      "cited_paper_id": 268680796
    },
    {
      "context_text": "Early approaches include All-in-one [24], which employed neural architecture search to select optimal encoders for weather tasks, and Transweather [42], which unified multiple encoders for efficient multi-weather restoration.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only models or methods. There are no clear identifiers for datasets.",
      "processing_time": 58.22421312332153,
      "citing_paper_id": "274281273",
      "cited_paper_id": null
    },
    {
      "context_text": "…deep learning have shown promising results in specific IR tasks such as denoising [8,16,78], dehazing [22,46,58], deraining [29,34,57], desnowing [12,13,48], motion deblurring [19,35,74], defocus deblurring [36,61,81], low-light enhancement [6,25,69], and JPEG artifact removal/correction [33,86].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various image restoration tasks and related papers. No verifiable resources are identified.",
      "processing_time": 59.12497544288635,
      "citing_paper_id": "270737774",
      "cited_paper_id": 837707
    },
    {
      "context_text": "…deep learning have shown promising results in specific IR tasks such as denoising [8,16,78], dehazing [22,46,58], deraining [29,34,57], desnowing [12,13,48], motion deblurring [19,35,74], defocus deblurring [36,61,81], low-light enhancement [6,25,69], and JPEG artifact removal/correction [33,86].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various image restoration tasks and related papers. No verifiable resources are identified.",
      "processing_time": 59.12497544288635,
      "citing_paper_id": "270737774",
      "cited_paper_id": 53011763
    },
    {
      "context_text": "…deep learning have shown promising results in specific IR tasks such as denoising [8,16,78], dehazing [22,46,58], deraining [29,34,57], desnowing [12,13,48], motion deblurring [19,35,74], defocus deblurring [36,61,81], low-light enhancement [6,25,69], and JPEG artifact removal/correction [33,86].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various image restoration tasks and related papers. No verifiable resources are identified.",
      "processing_time": 59.12497544288635,
      "citing_paper_id": "270737774",
      "cited_paper_id": 226308542
    },
    {
      "context_text": "…deep learning have shown promising results in specific IR tasks such as denoising [8,16,78], dehazing [22,46,58], deraining [29,34,57], desnowing [12,13,48], motion deblurring [19,35,74], defocus deblurring [36,61,81], low-light enhancement [6,25,69], and JPEG artifact removal/correction [33,86].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various image restoration tasks and related papers. No verifiable resources are identified.",
      "processing_time": 59.12497544288635,
      "citing_paper_id": "270737774",
      "cited_paper_id": 235703231
    },
    {
      "context_text": "…deep learning have shown promising results in specific IR tasks such as denoising [8,16,78], dehazing [22,46,58], deraining [29,34,57], desnowing [12,13,48], motion deblurring [19,35,74], defocus deblurring [36,61,81], low-light enhancement [6,25,69], and JPEG artifact removal/correction [33,86].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various image restoration tasks and related papers. No verifiable resources are identified.",
      "processing_time": 59.12497544288635,
      "citing_paper_id": "270737774",
      "cited_paper_id": 244461440
    },
    {
      "context_text": "…deep learning have shown promising results in specific IR tasks such as denoising [8,16,78], dehazing [22,46,58], deraining [29,34,57], desnowing [12,13,48], motion deblurring [19,35,74], defocus deblurring [36,61,81], low-light enhancement [6,25,69], and JPEG artifact removal/correction [33,86].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various image restoration tasks and related papers. No verifiable resources are identified.",
      "processing_time": 59.12497544288635,
      "citing_paper_id": "270737774",
      "cited_paper_id": 247922539
    },
    {
      "context_text": "…deep learning have shown promising results in specific IR tasks such as denoising [8,16,78], dehazing [22,46,58], deraining [29,34,57], desnowing [12,13,48], motion deblurring [19,35,74], defocus deblurring [36,61,81], low-light enhancement [6,25,69], and JPEG artifact removal/correction [33,86].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various image restoration tasks and related papers. No verifiable resources are identified.",
      "processing_time": 59.12497544288635,
      "citing_paper_id": "270737774",
      "cited_paper_id": 253761139
    },
    {
      "context_text": "…deep learning have shown promising results in specific IR tasks such as denoising [8,16,78], dehazing [22,46,58], deraining [29,34,57], desnowing [12,13,48], motion deblurring [19,35,74], defocus deblurring [36,61,81], low-light enhancement [6,25,69], and JPEG artifact removal/correction [33,86].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various image restoration tasks and related papers. No verifiable resources are identified.",
      "processing_time": 59.12497544288635,
      "citing_paper_id": "270737774",
      "cited_paper_id": 257496232
    },
    {
      "context_text": "…Test1200 [82], and Test2800 [27] are used for deraining, FiveK [5], LoL v1 [71], and LoL v2 [77] are used for low-light enhancement, CSD [13], Snow100K (S, M, and L) [48] are used for desnowing, CBSD68 [51] and urban100 [32] are used for denoising, and LIVE1 [62] is used for JPEG artifact removal.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Test1200",
        "Test2800",
        "FiveK",
        "LoL v1",
        "LoL v2",
        "CSD",
        "Snow100K S",
        "Snow100K M",
        "Snow100K L",
        "CBSD68",
        "urban100",
        "LIVE1"
      ],
      "dataset_descriptions": {
        "Test1200": "Used for deraining, evaluating the effectiveness of deraining algorithms in removing rain streaks from images.",
        "Test2800": "Used for deraining, assessing the performance of deraining methods on a larger set of images.",
        "FiveK": "Used for low-light enhancement, testing the ability to improve image quality in low-light conditions.",
        "LoL v1": "Used for low-light enhancement, evaluating the effectiveness of enhancing low-light images.",
        "LoL v2": "Used for low-light enhancement, providing a newer set of images for low-light enhancement evaluation.",
        "CSD": "Used for desnowing, assessing the performance of snow removal algorithms.",
        "Snow100K S": "Used for desnowing, evaluating the effectiveness of snow removal on small-scale images.",
        "Snow100K M": "Used for desnowing, assessing the performance of snow removal on medium-scale images.",
        "Snow100K L": "Used for desnowing, testing the effectiveness of snow removal on large-scale images.",
        "CBSD68": "Used for denoising, evaluating the performance of denoising algorithms on a benchmark set of images.",
        "urban100": "Used for denoising, assessing the effectiveness of denoising methods on urban scenes.",
        "LIVE1": "Used for JPEG artifact removal, testing the ability to reduce compression artifacts in JPEG images."
      },
      "confidence_score": 1.0,
      "reasoning": "The context lists several datasets used for various image restoration tasks, including deraining, low-light enhancement, desnowing, denoising, and JPEG artifact removal.",
      "processing_time": 130.34646821022034,
      "citing_paper_id": "270737774",
      "cited_paper_id": 837707
    },
    {
      "context_text": "…Test1200 [82], and Test2800 [27] are used for deraining, FiveK [5], LoL v1 [71], and LoL v2 [77] are used for low-light enhancement, CSD [13], Snow100K (S, M, and L) [48] are used for desnowing, CBSD68 [51] and urban100 [32] are used for denoising, and LIVE1 [62] is used for JPEG artifact removal.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Test1200",
        "Test2800",
        "FiveK",
        "LoL v1",
        "LoL v2",
        "CSD",
        "Snow100K S",
        "Snow100K M",
        "Snow100K L",
        "CBSD68",
        "urban100",
        "LIVE1"
      ],
      "dataset_descriptions": {
        "Test1200": "Used for deraining, evaluating the effectiveness of deraining algorithms in removing rain streaks from images.",
        "Test2800": "Used for deraining, assessing the performance of deraining methods on a larger set of images.",
        "FiveK": "Used for low-light enhancement, testing the ability to improve image quality in low-light conditions.",
        "LoL v1": "Used for low-light enhancement, evaluating the effectiveness of enhancing low-light images.",
        "LoL v2": "Used for low-light enhancement, providing a newer set of images for low-light enhancement evaluation.",
        "CSD": "Used for desnowing, assessing the performance of snow removal algorithms.",
        "Snow100K S": "Used for desnowing, evaluating the effectiveness of snow removal on small-scale images.",
        "Snow100K M": "Used for desnowing, assessing the performance of snow removal on medium-scale images.",
        "Snow100K L": "Used for desnowing, testing the effectiveness of snow removal on large-scale images.",
        "CBSD68": "Used for denoising, evaluating the performance of denoising algorithms on a benchmark set of images.",
        "urban100": "Used for denoising, assessing the effectiveness of denoising methods on urban scenes.",
        "LIVE1": "Used for JPEG artifact removal, testing the ability to reduce compression artifacts in JPEG images."
      },
      "confidence_score": 1.0,
      "reasoning": "The context lists several datasets used for various image restoration tasks, including deraining, low-light enhancement, desnowing, denoising, and JPEG artifact removal.",
      "processing_time": 130.34646821022034,
      "citing_paper_id": "270737774",
      "cited_paper_id": 8282555
    },
    {
      "context_text": "…Test1200 [82], and Test2800 [27] are used for deraining, FiveK [5], LoL v1 [71], and LoL v2 [77] are used for low-light enhancement, CSD [13], Snow100K (S, M, and L) [48] are used for desnowing, CBSD68 [51] and urban100 [32] are used for denoising, and LIVE1 [62] is used for JPEG artifact removal.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Test1200",
        "Test2800",
        "FiveK",
        "LoL v1",
        "LoL v2",
        "CSD",
        "Snow100K S",
        "Snow100K M",
        "Snow100K L",
        "CBSD68",
        "urban100",
        "LIVE1"
      ],
      "dataset_descriptions": {
        "Test1200": "Used for deraining, evaluating the effectiveness of deraining algorithms in removing rain streaks from images.",
        "Test2800": "Used for deraining, assessing the performance of deraining methods on a larger set of images.",
        "FiveK": "Used for low-light enhancement, testing the ability to improve image quality in low-light conditions.",
        "LoL v1": "Used for low-light enhancement, evaluating the effectiveness of enhancing low-light images.",
        "LoL v2": "Used for low-light enhancement, providing a newer set of images for low-light enhancement evaluation.",
        "CSD": "Used for desnowing, assessing the performance of snow removal algorithms.",
        "Snow100K S": "Used for desnowing, evaluating the effectiveness of snow removal on small-scale images.",
        "Snow100K M": "Used for desnowing, assessing the performance of snow removal on medium-scale images.",
        "Snow100K L": "Used for desnowing, testing the effectiveness of snow removal on large-scale images.",
        "CBSD68": "Used for denoising, evaluating the performance of denoising algorithms on a benchmark set of images.",
        "urban100": "Used for denoising, assessing the effectiveness of denoising methods on urban scenes.",
        "LIVE1": "Used for JPEG artifact removal, testing the ability to reduce compression artifacts in JPEG images."
      },
      "confidence_score": 1.0,
      "reasoning": "The context lists several datasets used for various image restoration tasks, including deraining, low-light enhancement, desnowing, denoising, and JPEG artifact removal.",
      "processing_time": 130.34646821022034,
      "citing_paper_id": "270737774",
      "cited_paper_id": null
    },
    {
      "context_text": "CBSD68 [51] Urban100(*) [32] Urban 100 [32] σ =15 σ =25 σ =50 σ =15 σ =25 σ =50 σ =15 σ =25 σ =50 σ =15 σ =25 σ =50",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "CBSD68",
        "Urban100"
      ],
      "dataset_descriptions": {
        "CBSD68": "Used to evaluate image denoising performance at various noise levels (σ = 15, 25, 50), focusing on the effectiveness of the proposed method in restoring images with different types of noise.",
        "Urban100": "Used to assess the quality of image restoration in urban scenes, evaluating the method's ability to handle complex textures and structures at multiple noise levels (σ = 15, 25, 50)."
      },
      "confidence_score": 1.0,
      "reasoning": "The citation mentions two specific datasets, CBSD68 and Urban100, which are commonly used in image restoration and super-resolution tasks.",
      "processing_time": 79.86117839813232,
      "citing_paper_id": "270737774",
      "cited_paper_id": 8282555
    },
    {
      "context_text": "In addition, the deraining dataset, which includes Rain14000 [26], Rain1800 [75], Rain800 [83], and Rain12 [43], initially contained 13,712 images, but two erroneous pictures are identified and removed.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Rain14000",
        "Rain1800",
        "Rain800",
        "Rain12"
      ],
      "dataset_descriptions": {
        "Rain14000": "Used to train and evaluate deraining models, containing 13,712 images with rain streaks for robustness testing.",
        "Rain1800": "Used to train and evaluate deraining models, providing a diverse set of rainy images for performance assessment.",
        "Rain800": "Used to train and evaluate deraining models, offering a smaller but high-quality dataset for fine-tuning and validation.",
        "Rain12": "Used to train and evaluate deraining models, consisting of a small set of challenging rainy images for benchmarking."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions specific datasets used for deraining, which are clearly named and relevant to the research topic of image restoration.",
      "processing_time": 86.18072938919067,
      "citing_paper_id": "270737774",
      "cited_paper_id": 9007541
    },
    {
      "context_text": "In addition, the deraining dataset, which includes Rain14000 [26], Rain1800 [75], Rain800 [83], and Rain12 [43], initially contained 13,712 images, but two erroneous pictures are identified and removed.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Rain14000",
        "Rain1800",
        "Rain800",
        "Rain12"
      ],
      "dataset_descriptions": {
        "Rain14000": "Used to train and evaluate deraining models, containing 13,712 images with rain streaks for robustness testing.",
        "Rain1800": "Used to train and evaluate deraining models, providing a diverse set of rainy images for performance assessment.",
        "Rain800": "Used to train and evaluate deraining models, offering a smaller but high-quality dataset for fine-tuning and validation.",
        "Rain12": "Used to train and evaluate deraining models, consisting of a small set of challenging rainy images for benchmarking."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions specific datasets used for deraining, which are clearly named and relevant to the research topic of image restoration.",
      "processing_time": 86.18072938919067,
      "citing_paper_id": "270737774",
      "cited_paper_id": 17115407
    },
    {
      "context_text": "The images with noise and JPEG artifacts are respectively generated using established methods same as [10,65,79] and [33,44,86].",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only methods for generating noisy and JPEG artifact images. No verifiable resources are identified.",
      "processing_time": 58.88435363769531,
      "citing_paper_id": "270737774",
      "cited_paper_id": 53011763
    },
    {
      "context_text": "The images with noise and JPEG artifacts are respectively generated using established methods same as [10,65,79] and [33,44,86].",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only methods for generating noisy and JPEG artifact images. No verifiable resources are identified.",
      "processing_time": 58.88435363769531,
      "citing_paper_id": "270737774",
      "cited_paper_id": 237266491
    },
    {
      "context_text": "The images with noise and JPEG artifacts are respectively generated using established methods same as [10,65,79] and [33,44,86].",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only methods for generating noisy and JPEG artifact images. No verifiable resources are identified.",
      "processing_time": 58.88435363769531,
      "citing_paper_id": "270737774",
      "cited_paper_id": 244346144
    },
    {
      "context_text": "…ConStyle v2 to be a powerful prompter that can produce a clean visual prompt based on the different degradations, we use the method in ImageNet-C [31] to generate motion blur, snow, and low contrast and the two-stage degradation method in Real-ESRGAN [68] to generate Gaussian blur, noise, and…",
      "catation_intent": "reusable resource",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions methods from ImageNet-C and Real-ESRGAN but does not refer to them as datasets. They are used to generate degradations for image restoration experiments.",
      "processing_time": 61.02533960342407,
      "citing_paper_id": "270737774",
      "cited_paper_id": 56657912
    },
    {
      "context_text": "For pre-training stage, we leverage the generation techniques of Real-ESRGAN [68] and ImageNet-C [31] in the Degradation Process (Fig.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "ImageNet-C"
      ],
      "dataset_descriptions": {
        "ImageNet-C": "Used to benchmark neural network robustness to common corruptions and perturbations during the degradation process in image restoration."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions ImageNet-C, which is a specific dataset used for benchmarking neural network robustness to common corruptions and perturbations.",
      "processing_time": 65.5982313156128,
      "citing_paper_id": "270737774",
      "cited_paper_id": 56657912
    },
    {
      "context_text": "In the process of generating degraded images, we directly use all configurations in Real-ESRGAN [68] and change the intensity of degradation in ImageNet-C [31].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "ImageNet-C"
      ],
      "dataset_descriptions": {
        "ImageNet-C": "Used to benchmark the robustness of image restoration methods by varying the intensity of degradation, focusing on common corruptions and perturbations."
      },
      "confidence_score": 0.9,
      "reasoning": "The context mentions the use of Real-ESRGAN and ImageNet-C, both of which are specific resources. Real-ESRGAN is a method, and ImageNet-C is a dataset used for benchmarking robustness to corruptions.",
      "processing_time": 69.75610208511353,
      "citing_paper_id": "270737774",
      "cited_paper_id": 56657912
    },
    {
      "context_text": "…advancements in deep learning have shown promising results in specific IR tasks such as denoising [8,16,78], dehazing [22,46,58], deraining [29,34,57], desnowing [12,13,48], motion deblurring [19,35,74], defocus deblurring [36,61,81], low-light enhancement [6,25,69], and JPEG artifact…",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various image restoration tasks. No dataset names are provided.",
      "processing_time": 58.41591548919678,
      "citing_paper_id": "270737774",
      "cited_paper_id": 59316941
    },
    {
      "context_text": "…advancements in deep learning have shown promising results in specific IR tasks such as denoising [8,16,78], dehazing [22,46,58], deraining [29,34,57], desnowing [12,13,48], motion deblurring [19,35,74], defocus deblurring [36,61,81], low-light enhancement [6,25,69], and JPEG artifact…",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various image restoration tasks. No dataset names are provided.",
      "processing_time": 58.41591548919678,
      "citing_paper_id": "270737774",
      "cited_paper_id": 214623217
    },
    {
      "context_text": "For evaluation, GoPro [52], HIDE [64], RealBlur-J [59], and RealBlur-R [59] are used for motion deblurring, DPDD [1] is used for defocus deblurring, SOTS outdoors [37] is used for dehazing, Rain100H [76], Rain100L [76], Test1200 [82], and Test2800 [27] are used for deraining, FiveK [5], LoL v1…",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "GoPro",
        "HIDE",
        "RealBlur-J",
        "RealBlur-R",
        "DPDD",
        "SOTS outdoors",
        "Rain100H",
        "Rain100L",
        "Test1200",
        "Test2800",
        "FiveK"
      ],
      "dataset_descriptions": {
        "GoPro": "Used for evaluating motion deblurring algorithms, providing real-world motion-blurred images and their corresponding sharp versions.",
        "HIDE": "Used for evaluating motion deblurring algorithms, containing high-resolution images with varying degrees of motion blur.",
        "RealBlur-J": "Used for evaluating motion deblurring algorithms, featuring real-world motion-blurred images captured under various conditions.",
        "RealBlur-R": "Used for evaluating motion deblurring algorithms, featuring real-world motion-blurred images captured under various conditions.",
        "DPDD": "Used for evaluating defocus deblurring algorithms, utilizing dual-pixel data to simulate defocus blur.",
        "SOTS outdoors": "Used for evaluating dehazing algorithms, providing outdoor scenes with synthetic haze and their clear counterparts.",
        "Rain100H": "Used for evaluating deraining algorithms, containing heavy rain images and their clean versions.",
        "Rain100L": "Used for evaluating deraining algorithms, containing light rain images and their clean versions.",
        "Test1200": "Used for evaluating deraining algorithms, providing a large set of rain images and their clean versions.",
        "Test2800": "Used for evaluating deraining algorithms, providing a large set of rain images and their clean versions.",
        "FiveK": "Used for evaluating image enhancement algorithms, containing a diverse set of images with professional retouching."
      },
      "confidence_score": 1.0,
      "reasoning": "The citation mentions multiple datasets used for evaluating different aspects of image restoration, including motion deblurring, defocus deblurring, dehazing, and deraining.",
      "processing_time": 122.38407921791077,
      "citing_paper_id": "270737774",
      "cited_paper_id": 218470249
    },
    {
      "context_text": "Thus, the more teacher networks there are, the more complex the training process becomes; Li et al. [41] (Fig.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only a reference to a figure and a discussion about the complexity of training with multiple teacher networks.",
      "processing_time": 60.00062870979309,
      "citing_paper_id": "270737774",
      "cited_paper_id": 219978541
    },
    {
      "context_text": "Recent advancements in deep learning have shown promising results in specific IR tasks such as denoising [8,16,78], dehazing [22,46,58], deraining [29,34,57], desnowing [12,13,48], motion deblurring [19,35,74], defocus deblurring [36,61,81], low-light enhancement [6,25,69], and JPEG artifact…",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various image restoration tasks. No clear, verifiable datasets are identified.",
      "processing_time": 59.100518465042114,
      "citing_paper_id": "270737774",
      "cited_paper_id": 229923112
    },
    {
      "context_text": "…is used for dehazing, Rain100H [76], Rain100L [76], Test1200 [82], and Test2800 [27] are used for deraining, FiveK [5], LoL v1 [71], and LoL v2 [77] are used for low-light enhancement, CSD [13], Snow100K (S, M, and L) [48] are used for desnowing, CBSD68 [51] and urban100 [32] are used for…",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Rain100H",
        "Rain100L",
        "Test1200",
        "Test2800",
        "FiveK",
        "LoL v1",
        "LoL v2",
        "CSD",
        "Snow100K S",
        "Snow100K M",
        "Snow100K L",
        "CBSD68",
        "urban100"
      ],
      "dataset_descriptions": {
        "Rain100H": "Used for deraining, focusing on high-resolution images to evaluate the effectiveness of deraining algorithms.",
        "Rain100L": "Used for deraining, focusing on low-resolution images to assess the performance of deraining methods.",
        "Test1200": "Used for deraining, providing a large test set to validate the robustness of deraining techniques.",
        "Test2800": "Used for deraining, offering a larger test set to further evaluate the performance of deraining algorithms.",
        "FiveK": "Used for low-light enhancement, providing a diverse set of images to improve low-light conditions.",
        "LoL v1": "Used for low-light enhancement, focusing on a variety of low-light scenarios to enhance image quality.",
        "LoL v2": "Used for low-light enhancement, offering an updated version with more challenging low-light images.",
        "CSD": "Used for desnowing, providing synthetic snow images to test the effectiveness of desnowing algorithms.",
        "Snow100K S": "Used for desnowing, focusing on small-scale synthetic snow images to evaluate desnowing performance.",
        "Snow100K M": "Used for desnowing, focusing on medium-scale synthetic snow images to assess desnowing methods.",
        "Snow100K L": "Used for desnowing, focusing on large-scale synthetic snow images to test the robustness of desnowing techniques.",
        "CBSD68": "Used for general image restoration, providing a benchmark set of images to evaluate denoising algorithms.",
        "urban100": "Used for general image restoration, offering a set of urban scenes to test super-resolution and other restoration techniques."
      },
      "confidence_score": 1.0,
      "reasoning": "The citation span mentions multiple datasets used for various image restoration tasks, including dehazing, deraining, low-light enhancement, desnowing, and general image restoration.",
      "processing_time": 142.1735758781433,
      "citing_paper_id": "270737774",
      "cited_paper_id": 231641545
    },
    {
      "context_text": "Inspired by BYOL [28], SimSam [15], and DINO [7], we take advantage of knowledge distillation.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models or methods. The context is about leveraging knowledge distillation inspired by other works.",
      "processing_time": 59.66472268104553,
      "citing_paper_id": "270737774",
      "cited_paper_id": 233444273
    },
    {
      "context_text": "To verify our method, we perform ConStyle v2 on three state-of-the-art IR models (Restormer [79], MAXIM-1S [65], NAFNet [10]) and a non-IR U-Net model consisting of vanilla convolutions.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only models and methods. The citation is focused on verifying the method using various models.",
      "processing_time": 59.65561866760254,
      "citing_paper_id": "270737774",
      "cited_paper_id": 244346144
    },
    {
      "context_text": "While numerous works [9,10,17,18,23,39,42,65,70,79,80] excel in various Image Restoration tasks, they are typically limited to addressing a single type of degradation with a specific set of weights.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general statements about image restoration tasks and limitations of existing works.",
      "processing_time": 59.08883833885193,
      "citing_paper_id": "270737774",
      "cited_paper_id": 244346144
    },
    {
      "context_text": "While numerous works [9,10,17,18,23,39,42,65,70,79,80] excel in various Image Restoration tasks, they are typically limited to addressing a single type of degradation with a specific set of weights.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general statements about image restoration tasks and limitations of existing works.",
      "processing_time": 59.08883833885193,
      "citing_paper_id": "270737774",
      "cited_paper_id": 253518543
    },
    {
      "context_text": "While numerous works [9,10,17,18,23,39,42,65,70,79,80] excel in various Image Restoration tasks, they are typically limited to addressing a single type of degradation with a specific set of weights.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general statements about image restoration tasks and limitations of existing works.",
      "processing_time": 59.08883833885193,
      "citing_paper_id": "270737774",
      "cited_paper_id": 260927679
    },
    {
      "context_text": "To evaluate whether ConStyle [24] can directly convert U-Net models to all-in-one models, we conduct experiments using Original models (Restormer [79], NAFNet [60], MAXIM-1S [65]) and ConStyle models (ConStyle Restormer, Con-Style NAFNet, ConStyle MAXIM-1S) on Mix Degradations datasets.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Mix Degradations datasets"
      ],
      "dataset_descriptions": {
        "Mix Degradations datasets": "Used to evaluate the performance of ConStyle models converted from U-Net models, focusing on their ability to handle various image degradations."
      },
      "confidence_score": 0.9,
      "reasoning": "The context mentions 'Mix Degradations datasets' which appears to be a specific dataset used for evaluating the conversion of U-Net models to all-in-one models. No other specific datasets are mentioned.",
      "processing_time": 69.5277304649353,
      "citing_paper_id": "270737774",
      "cited_paper_id": 244346144
    },
    {
      "context_text": "…[76], Test1200 [82], and Test2800 [27] are used for deraining, FiveK [5], LoL v1 [71], and LoL v2 [77] are used for low-light enhancement, CSD [13], Snow100K (S, M, and L) [48] are used for desnowing, CBSD68 [51] and urban100 [32] are used for denoising, and LIVE1 [62] is used for JPEG…",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Test1200",
        "Test2800",
        "FiveK",
        "LoL v1",
        "LoL v2",
        "CSD",
        "Snow100K",
        "CBSD68",
        "urban100",
        "LIVE1"
      ],
      "dataset_descriptions": {
        "Test1200": "Used for deraining, evaluating the effectiveness of deraining algorithms on real-world rain images.",
        "Test2800": "Used for deraining, assessing the performance of deraining methods on a larger set of rain images.",
        "FiveK": "Used for low-light enhancement, testing the ability to improve image quality in low-light conditions.",
        "LoL v1": "Used for low-light enhancement, evaluating the performance of low-light enhancement algorithms.",
        "LoL v2": "Used for low-light enhancement, providing an updated set of images for low-light enhancement evaluation.",
        "CSD": "Used for desnowing, assessing the effectiveness of desnowing algorithms on snow-covered images.",
        "Snow100K": "Used for desnowing, evaluating the performance of desnowing methods on a large-scale dataset.",
        "CBSD68": "Used for denoising, testing the ability to remove noise from images.",
        "urban100": "Used for denoising, evaluating the performance of denoising algorithms on urban scenes.",
        "LIVE1": "Used for JPEG artifact removal, assessing the effectiveness of algorithms in reducing JPEG compression artifacts."
      },
      "confidence_score": 1.0,
      "reasoning": "The citation mentions multiple datasets used for various image restoration tasks, including deraining, low-light enhancement, desnowing, denoising, and JPEG artifact removal.",
      "processing_time": 115.51328778266907,
      "citing_paper_id": "270737774",
      "cited_paper_id": 244461440
    },
    {
      "context_text": "…Convolution [20] to transform degradation priors obtained from the former into convolution kernels in the latter, enabling dynamic degradation removal; DA-CLIP [49] builds upon the architecture of CLIP [56], in which BLIP [40] is used to generate synthetic captions for high-quality images.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models and methods. The context focuses on the use of models and architectures rather than datasets.",
      "processing_time": 60.04464554786682,
      "citing_paper_id": "270737774",
      "cited_paper_id": 246411402
    },
    {
      "context_text": "Drawing inspiration from Prompting Learning in NLP, recently, there have been many excellent visual prompting works in the IR [3, 11, 45, 47, 50,55,63,67].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to visual prompting works. No dataset names are present in the context.",
      "processing_time": 59.64793562889099,
      "citing_paper_id": "270737774",
      "cited_paper_id": 251979350
    },
    {
      "context_text": "Drawing inspiration from Prompting Learning in NLP, recently, there have been many excellent visual prompting works in the IR [3, 11, 45, 47, 50,55,63,67].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to visual prompting works. No dataset names are present in the context.",
      "processing_time": 59.64793562889099,
      "citing_paper_id": "270737774",
      "cited_paper_id": 254246343
    },
    {
      "context_text": "Drawing inspiration from Prompting Learning in NLP, recently, there have been many excellent visual prompting works in the IR [3, 11, 45, 47, 50,55,63,67].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to visual prompting works. No dataset names are present in the context.",
      "processing_time": 59.64793562889099,
      "citing_paper_id": "270737774",
      "cited_paper_id": 265609653
    },
    {
      "context_text": "To solve this problem, all-in-one Image Restoration (IR) methods [14, 38, 47, 49, 50, 53–55, 66, 84, 85] have been developed.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to methods and approaches for image restoration.",
      "processing_time": 58.61861276626587,
      "citing_paper_id": "270737774",
      "cited_paper_id": 260085391
    },
    {
      "context_text": "To solve this problem, all-in-one Image Restoration (IR) methods [14, 38, 47, 49, 50, 53–55, 66, 84, 85] have been developed.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to methods and approaches for image restoration.",
      "processing_time": 58.61861276626587,
      "citing_paper_id": "270737774",
      "cited_paper_id": 260107992
    },
    {
      "context_text": "The all-in-one Image Restoration [14,53,66,84] is a kind of method that only uses a suit of weights to address multiple types of degradation.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only methods and approaches for image restoration. No verifiable resources are identified.",
      "processing_time": 59.3777232170105,
      "citing_paper_id": "270737774",
      "cited_paper_id": 260085391
    },
    {
      "context_text": "The all-in-one Image Restoration [14,53,66,84] is a kind of method that only uses a suit of weights to address multiple types of degradation.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only methods and approaches for image restoration. No verifiable resources are identified.",
      "processing_time": 59.3777232170105,
      "citing_paper_id": "270737774",
      "cited_paper_id": 260107992
    },
    {
      "context_text": "To tackle this issue, researchers have focused on developing models capable of handling multiple degradations [17,42,70,80].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to papers discussing models for image restoration.",
      "processing_time": 58.613807678222656,
      "citing_paper_id": "270737774",
      "cited_paper_id": 260927679
    },
    {
      "context_text": "…completed by combining all training sets of each restoration task, and then the trained model is evaluated on the test set of each restoration task, including: 1) Denoising uses the combination of BSD400 and WED dataset as training set and the testing is performed on BSD68 [63] and Urban100 [64].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "BSD400",
        "WED",
        "BSD68",
        "Urban100"
      ],
      "dataset_descriptions": {
        "BSD400": "Used for training the denoising model, providing a diverse set of natural images for robustness.",
        "WED": "Used for training the denoising model, complementing BSD400 with additional images.",
        "BSD68": "Used for testing the denoising model, evaluating performance on a standardized benchmark.",
        "Urban100": "Used for testing the denoising model, assessing performance on high-resolution urban scenes."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions specific datasets used for denoising tasks, including BSD400, WED, BSD68, and Urban100. These datasets are clearly identified and used for training and testing the model.",
      "processing_time": 85.11873459815979,
      "citing_paper_id": "274845190",
      "cited_paper_id": 64193
    },
    {
      "context_text": "…completed by combining all training sets of each restoration task, and then the trained model is evaluated on the test set of each restoration task, including: 1) Denoising uses the combination of BSD400 and WED dataset as training set and the testing is performed on BSD68 [63] and Urban100 [64].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "BSD400",
        "WED",
        "BSD68",
        "Urban100"
      ],
      "dataset_descriptions": {
        "BSD400": "Used for training the denoising model, providing a diverse set of natural images for robustness.",
        "WED": "Used for training the denoising model, complementing BSD400 with additional images.",
        "BSD68": "Used for testing the denoising model, evaluating performance on a standardized benchmark.",
        "Urban100": "Used for testing the denoising model, assessing performance on high-resolution urban scenes."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions specific datasets used for denoising tasks, including BSD400, WED, BSD68, and Urban100. These datasets are clearly identified and used for training and testing the model.",
      "processing_time": 85.11873459815979,
      "citing_paper_id": "274845190",
      "cited_paper_id": 8282555
    },
    {
      "context_text": "DID-MDN [34] proposed a multi-stream dense network to estimate rain density information to guide rain removal.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions a method (multi-stream dense network) rather than a dataset. No specific dataset is named or described.",
      "processing_time": 59.36320781707764,
      "citing_paper_id": "274845190",
      "cited_paper_id": 3406592
    },
    {
      "context_text": "3) Dehazing employs RESIDE dataset [67], which consists of five subsets of Indoor Training Set (ITS), Outdoor Training Set (OTS), Synthetic Objective Testing Set (SOTS), Real-world Task-driven Testing Set (RTTS), and Hybrid Subjective Testing Set (HSTS).",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "RESIDE",
        "Indoor Training Set (ITS)",
        "Outdoor Training Set (OTS)",
        "Synthetic Objective Testing Set (SOTS)",
        "Real-world Task-driven Testing Set (RTTS)",
        "Hybrid Subjective Testing Set (HSTS)"
      ],
      "dataset_descriptions": {
        "RESIDE": "Used for benchmarking single-image dehazing methods, consisting of multiple subsets designed for various testing and training scenarios.",
        "Indoor Training Set (ITS)": "Used for training dehazing models on indoor scenes, providing a diverse set of synthetic hazy images.",
        "Outdoor Training Set (OTS)": "Used for training dehazing models on outdoor scenes, offering a wide range of synthetic hazy images.",
        "Synthetic Objective Testing Set (SOTS)": "Used for objective evaluation of dehazing algorithms, containing synthetic hazy images with ground truth.",
        "Real-world Task-driven Testing Set (RTTS)": "Used for evaluating dehazing performance on real-world images, focusing on task-driven metrics.",
        "Hybrid Subjective Testing Set (HSTS)": "Used for subjective evaluation of dehazing results, combining both synthetic and real-world images."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions the RESIDE dataset, which is a specific, verifiable dataset used for dehazing research. It includes multiple subsets, all of which are named and described.",
      "processing_time": 102.72905158996582,
      "citing_paper_id": "274845190",
      "cited_paper_id": 39760169
    },
    {
      "context_text": "For example, the restored images of MIMO-UNet [28], FFANet [54], RESCAN [58], DRSformer [60] and AirNet [19] have heavy residuals of rain streaks, while there still exists light rain streak residuals in the restored images of PReNet [59], LMQFormer [35], Retixformer [61], Stoformer [14] and…",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only models and their performance on image restoration tasks. No verifiable resources are identified.",
      "processing_time": 60.02177715301514,
      "citing_paper_id": "274845190",
      "cited_paper_id": 49864080
    },
    {
      "context_text": "For example, the restored images of MIMO-UNet [28], FFANet [54], RESCAN [58], DRSformer [60] and AirNet [19] have heavy residuals of rain streaks, while there still exists light rain streak residuals in the restored images of PReNet [59], LMQFormer [35], Retixformer [61], Stoformer [14] and…",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only models and their performance on image restoration tasks. No verifiable resources are identified.",
      "processing_time": 60.02177715301514,
      "citing_paper_id": "274845190",
      "cited_paper_id": 236976210
    },
    {
      "context_text": "For example, the restored images of MIMO-UNet [28], FFANet [54], RESCAN [58], DRSformer [60] and AirNet [19] have heavy residuals of rain streaks, while there still exists light rain streak residuals in the restored images of PReNet [59], LMQFormer [35], Retixformer [61], Stoformer [14] and…",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only models and their performance on image restoration tasks. No verifiable resources are identified.",
      "processing_time": 60.02177715301514,
      "citing_paper_id": "274845190",
      "cited_paper_id": 250551851
    },
    {
      "context_text": "…and MSANet [53] for denoising, MIMO-UNet [28] for deblurring, FFANet [54], AECRNet [55], Dehazeformer [56] and MB-TaylorFormer [57] for dehazing, RESCAN [58], PReNet [59] and DRSformer [60] for deraining, LMQFormer [35] for desnowing, Retinexformer [61], LLFormer [62] for low-light enhancement.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions various methods and models for image restoration tasks but does not reference any specific datasets. The cited papers also do not provide additional context about datasets.",
      "processing_time": 60.97236466407776,
      "citing_paper_id": "274845190",
      "cited_paper_id": 49864080
    },
    {
      "context_text": "…and MSANet [53] for denoising, MIMO-UNet [28] for deblurring, FFANet [54], AECRNet [55], Dehazeformer [56] and MB-TaylorFormer [57] for dehazing, RESCAN [58], PReNet [59] and DRSformer [60] for deraining, LMQFormer [35] for desnowing, Retinexformer [61], LLFormer [62] for low-light enhancement.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions various methods and models for image restoration tasks but does not reference any specific datasets. The cited papers also do not provide additional context about datasets.",
      "processing_time": 60.97236466407776,
      "citing_paper_id": "274845190",
      "cited_paper_id": 59316941
    },
    {
      "context_text": "…and MSANet [53] for denoising, MIMO-UNet [28] for deblurring, FFANet [54], AECRNet [55], Dehazeformer [56] and MB-TaylorFormer [57] for dehazing, RESCAN [58], PReNet [59] and DRSformer [60] for deraining, LMQFormer [35] for desnowing, Retinexformer [61], LLFormer [62] for low-light enhancement.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions various methods and models for image restoration tasks but does not reference any specific datasets. The cited papers also do not provide additional context about datasets.",
      "processing_time": 60.97236466407776,
      "citing_paper_id": "274845190",
      "cited_paper_id": 257496232
    },
    {
      "context_text": "From the results, we can see that: (1) Some methods, such as AECR-Net [55], PReNet [59], RESCAN [58], TransWeather [21], embrace significantly fewer Flops or runtime than our method.",
      "catation_intent": "findings",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only models and their performance metrics. No verifiable resources are identified.",
      "processing_time": 59.95754933357239,
      "citing_paper_id": "274845190",
      "cited_paper_id": 49864080
    },
    {
      "context_text": "From the results, we can see that: (1) Some methods, such as AECR-Net [55], PReNet [59], RESCAN [58], TransWeather [21], embrace significantly fewer Flops or runtime than our method.",
      "catation_intent": "findings",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only models and their performance metrics. No verifiable resources are identified.",
      "processing_time": 59.95754933357239,
      "citing_paper_id": "274845190",
      "cited_paper_id": 59316941
    },
    {
      "context_text": "From the results, we can see that the visualization results in some compared baseline methods, such as MIMO-UNet [28], FFANet [54], RESCAN [58], have residual noises, while some compared baseline methods, such as Uformer [11], Stoformer [14] have over-smoothed and distorted results.",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only model names and their performance. No verifiable resources are identified.",
      "processing_time": 59.954455852508545,
      "citing_paper_id": "274845190",
      "cited_paper_id": 49864080
    },
    {
      "context_text": "From the results, we can see that the visualization results in some compared baseline methods, such as MIMO-UNet [28], FFANet [54], RESCAN [58], have residual noises, while some compared baseline methods, such as Uformer [11], Stoformer [14] have over-smoothed and distorted results.",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only model names and their performance. No verifiable resources are identified.",
      "processing_time": 59.954455852508545,
      "citing_paper_id": "274845190",
      "cited_paper_id": 236976210
    },
    {
      "context_text": "From the results, we can see that the visualization results in some compared baseline methods, such as MIMO-UNet [28], FFANet [54], RESCAN [58], have residual noises, while some compared baseline methods, such as Uformer [11], Stoformer [14] have over-smoothed and distorted results.",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only model names and their performance. No verifiable resources are identified.",
      "processing_time": 59.954455852508545,
      "citing_paper_id": "274845190",
      "cited_paper_id": 258509512
    },
    {
      "context_text": "6) Low-light Enhancement picks LOL [70] and LSRW [71] datasets, which have partition ratio between training and testing sets of 485:15 and 5600:50, respectively.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "LOL",
        "LSRW"
      ],
      "dataset_descriptions": {
        "LOL": "Used for low-light image enhancement, focusing on the training and testing partition ratio of 485:15 to evaluate model performance.",
        "LSRW": "Used for low-light image enhancement, focusing on the training and testing partition ratio of 5600:50 to evaluate model performance."
      },
      "confidence_score": 1.0,
      "reasoning": "The citation mentions two specific datasets, LOL and LSRW, used for low-light image enhancement. These datasets are directly relevant to the research topic of all-in-one image restoration.",
      "processing_time": 74.8777825832367,
      "citing_paper_id": "274845190",
      "cited_paper_id": 52008443
    },
    {
      "context_text": "…the restored images of MIMO-UNet [28], FFANet [54], RESCAN [58], DRSformer [60] and AirNet [19] have heavy residuals of rain streaks, while there still exists light rain streak residuals in the restored images of PReNet [59], LMQFormer [35], Retixformer [61], Stoformer [14] and TransWeather [21].",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only models and their performance on image restoration tasks.",
      "processing_time": 58.80925750732422,
      "citing_paper_id": "274845190",
      "cited_paper_id": 59316941
    },
    {
      "context_text": "…the restored images of MIMO-UNet [28], FFANet [54], RESCAN [58], DRSformer [60] and AirNet [19] have heavy residuals of rain streaks, while there still exists light rain streak residuals in the restored images of PReNet [59], LMQFormer [35], Retixformer [61], Stoformer [14] and TransWeather [21].",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only models and their performance on image restoration tasks.",
      "processing_time": 58.80925750732422,
      "citing_paper_id": "274845190",
      "cited_paper_id": 257496232
    },
    {
      "context_text": "…the restored images of MIMO-UNet [28], FFANet [54], RESCAN [58], DRSformer [60] and AirNet [19] have heavy residuals of rain streaks, while there still exists light rain streak residuals in the restored images of PReNet [59], LMQFormer [35], Retixformer [61], Stoformer [14] and TransWeather [21].",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only models and their performance on image restoration tasks.",
      "processing_time": 58.80925750732422,
      "citing_paper_id": "274845190",
      "cited_paper_id": 258509512
    },
    {
      "context_text": "A series of works [36], [37], [38] combined the Retinex decomposition with deep learning to resolve the low-light enhancement task.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a combination of a method (Retinex decomposition) with deep learning for low-light image enhancement.",
      "processing_time": 60.95819902420044,
      "citing_paper_id": "274845190",
      "cited_paper_id": 195510077
    },
    {
      "context_text": "2) Deblurring chooses the popular GoPro [65] and HIDE [66] datasets, where the former contains 2,103 pairs for training and 1,111 pairs for evaluation, the latter contains 6,397 pairs for training and 2,035 pairs for evaluation.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "GoPro",
        "HIDE"
      ],
      "dataset_descriptions": {
        "GoPro": "Used for training and evaluating deblurring models, containing 2,103 pairs for training and 1,111 pairs for evaluation.",
        "HIDE": "Used for training and evaluating deblurring models, containing 6,397 pairs for training and 2,035 pairs for evaluation."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions two specific datasets, GoPro and HIDE, which are used for training and evaluation in deblurring tasks.",
      "processing_time": 74.86918354034424,
      "citing_paper_id": "274845190",
      "cited_paper_id": 201624746
    },
    {
      "context_text": "…the domain shift property of the proposed all-in-one image restoration model, still lacking a large-scale evaluation with other important datasets, such as O-HAZE [75], I-HAZE [75] for dehazing, JRSRD [76] for deraining, SnowCityScapes [77] for desnowing, LOL-v2 [78] for low-light enhancement, etc.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "O-HAZE",
        "I-HAZE",
        "JRSRD",
        "SnowCityScapes",
        "LOL-v2"
      ],
      "dataset_descriptions": {
        "O-HAZE": "Used for evaluating dehazing performance, providing real hazy and haze-free outdoor images to assess the model's ability to restore clear images.",
        "I-HAZE": "Used for evaluating dehazing performance, providing real hazy and haze-free indoor images to assess the model's ability to restore clear images.",
        "JRSRD": "Used for evaluating deraining performance, providing images with rain streaks and raindrops to assess the model's ability to remove rain artifacts.",
        "SnowCityScapes": "Used for evaluating desnowing performance, providing images with snow to assess the model's ability to remove snow artifacts.",
        "LOL-v2": "Used for evaluating low-light enhancement performance, providing low-light images to assess the model's ability to improve image brightness and clarity."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions several datasets by name, which are used for evaluating different aspects of image restoration. Each dataset is associated with a specific task, such as dehazing, deraining, desnowing, and low-light enhancement.",
      "processing_time": 54.18152928352356,
      "citing_paper_id": "274845190",
      "cited_paper_id": 206598041
    },
    {
      "context_text": "…the domain shift property of the proposed all-in-one image restoration model, still lacking a large-scale evaluation with other important datasets, such as O-HAZE [75], I-HAZE [75] for dehazing, JRSRD [76] for deraining, SnowCityScapes [77] for desnowing, LOL-v2 [78] for low-light enhancement, etc.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "O-HAZE",
        "I-HAZE",
        "JRSRD",
        "SnowCityScapes",
        "LOL-v2"
      ],
      "dataset_descriptions": {
        "O-HAZE": "Used for evaluating dehazing performance, providing real hazy and haze-free outdoor images to assess the model's ability to restore clear images.",
        "I-HAZE": "Used for evaluating dehazing performance, providing real hazy and haze-free indoor images to assess the model's ability to restore clear images.",
        "JRSRD": "Used for evaluating deraining performance, providing images with rain streaks and raindrops to assess the model's ability to remove rain artifacts.",
        "SnowCityScapes": "Used for evaluating desnowing performance, providing images with snow to assess the model's ability to remove snow artifacts.",
        "LOL-v2": "Used for evaluating low-light enhancement performance, providing low-light images to assess the model's ability to improve image brightness and clarity."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions several datasets by name, which are used for evaluating different aspects of image restoration. Each dataset is associated with a specific task, such as dehazing, deraining, desnowing, and low-light enhancement.",
      "processing_time": 54.18152928352356,
      "citing_paper_id": "274845190",
      "cited_paper_id": 232222608
    },
    {
      "context_text": "…the domain shift property of the proposed all-in-one image restoration model, still lacking a large-scale evaluation with other important datasets, such as O-HAZE [75], I-HAZE [75] for dehazing, JRSRD [76] for deraining, SnowCityScapes [77] for desnowing, LOL-v2 [78] for low-light enhancement, etc.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "O-HAZE",
        "I-HAZE",
        "JRSRD",
        "SnowCityScapes",
        "LOL-v2"
      ],
      "dataset_descriptions": {
        "O-HAZE": "Used for evaluating dehazing performance, providing real hazy and haze-free outdoor images to assess the model's ability to restore clear images.",
        "I-HAZE": "Used for evaluating dehazing performance, providing real hazy and haze-free indoor images to assess the model's ability to restore clear images.",
        "JRSRD": "Used for evaluating deraining performance, providing images with rain streaks and raindrops to assess the model's ability to remove rain artifacts.",
        "SnowCityScapes": "Used for evaluating desnowing performance, providing images with snow to assess the model's ability to remove snow artifacts.",
        "LOL-v2": "Used for evaluating low-light enhancement performance, providing low-light images to assess the model's ability to improve image brightness and clarity."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions several datasets by name, which are used for evaluating different aspects of image restoration. Each dataset is associated with a specific task, such as dehazing, deraining, desnowing, and low-light enhancement.",
      "processing_time": 54.18152928352356,
      "citing_paper_id": "274845190",
      "cited_paper_id": 232306906
    },
    {
      "context_text": "Besides, several works [10], [11], [12], [13], [14], [15], [16], [17], [18] built efficient and flexible general-purpose backbones upon excellent vision transformers [41], [42], yet causing unacceptable computational cost for IR tasks.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets. It focuses on the use of vision transformers and their computational cost for image restoration tasks.",
      "processing_time": 60.56521439552307,
      "citing_paper_id": "274845190",
      "cited_paper_id": 218889832
    },
    {
      "context_text": "Besides, several works [10], [11], [12], [13], [14], [15], [16], [17], [18] built efficient and flexible general-purpose backbones upon excellent vision transformers [41], [42], yet causing unacceptable computational cost for IR tasks.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets. It focuses on the use of vision transformers and their computational cost for image restoration tasks.",
      "processing_time": 60.56521439552307,
      "citing_paper_id": "274845190",
      "cited_paper_id": 237266491
    },
    {
      "context_text": "Besides, several works [10], [11], [12], [13], [14], [15], [16], [17], [18] built efficient and flexible general-purpose backbones upon excellent vision transformers [41], [42], yet causing unacceptable computational cost for IR tasks.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets. It focuses on the use of vision transformers and their computational cost for image restoration tasks.",
      "processing_time": 60.56521439552307,
      "citing_paper_id": "274845190",
      "cited_paper_id": 244346144
    },
    {
      "context_text": "Besides, several works [10], [11], [12], [13], [14], [15], [16], [17], [18] built efficient and flexible general-purpose backbones upon excellent vision transformers [41], [42], yet causing unacceptable computational cost for IR tasks.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets. It focuses on the use of vision transformers and their computational cost for image restoration tasks.",
      "processing_time": 60.56521439552307,
      "citing_paper_id": "274845190",
      "cited_paper_id": 257255385
    },
    {
      "context_text": "Besides, several works [10], [11], [12], [13], [14], [15], [16], [17], [18] built efficient and flexible general-purpose backbones upon excellent vision transformers [41], [42], yet causing unacceptable computational cost for IR tasks.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets. It focuses on the use of vision transformers and their computational cost for image restoration tasks.",
      "processing_time": 60.56521439552307,
      "citing_paper_id": "274845190",
      "cited_paper_id": 258509512
    },
    {
      "context_text": "Besides, several works [10], [11], [12], [13], [14], [15], [16], [17], [18] built efficient and flexible general-purpose backbones upon excellent vision transformers [41], [42], yet causing unacceptable computational cost for IR tasks.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets. It focuses on the use of vision transformers and their computational cost for image restoration tasks.",
      "processing_time": 60.56521439552307,
      "citing_paper_id": "274845190",
      "cited_paper_id": 261081712
    },
    {
      "context_text": "PFDN [32] and C2PNet [3] respectively designed a feature dehazing unit and a physics-aware dual-branch unit to facilitate haze removal.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods or models for haze removal.",
      "processing_time": 58.78701305389404,
      "citing_paper_id": "274845190",
      "cited_paper_id": 221883930
    },
    {
      "context_text": "We choose MC-blur [72], RWBI [73], RainKITTI2012 [74] as unseen datasets and report the results in Table VI.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "MC-blur",
        "RWBI",
        "RainKITTI2012"
      ],
      "dataset_descriptions": {
        "MC-blur": "Used as an unseen dataset to evaluate deblurring algorithms, focusing on real-world blur scenarios and benchmarking performance.",
        "RWBI": "Used as an unseen dataset to evaluate deblurring algorithms, focusing on real-world blur scenarios and benchmarking performance.",
        "RainKITTI2012": "Used as an unseen dataset to evaluate deraining algorithms, focusing on stereo image deraining and semantic understanding."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions three datasets: MC-blur, RWBI, and RainKITTI2012. These are used as unseen datasets to evaluate the performance of the proposed method.",
      "processing_time": 81.53178024291992,
      "citing_paper_id": "274845190",
      "cited_paper_id": 222104551
    },
    {
      "context_text": "We choose MC-blur [72], RWBI [73], RainKITTI2012 [74] as unseen datasets and report the results in Table VI.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "MC-blur",
        "RWBI",
        "RainKITTI2012"
      ],
      "dataset_descriptions": {
        "MC-blur": "Used as an unseen dataset to evaluate deblurring algorithms, focusing on real-world blur scenarios and benchmarking performance.",
        "RWBI": "Used as an unseen dataset to evaluate deblurring algorithms, focusing on real-world blur scenarios and benchmarking performance.",
        "RainKITTI2012": "Used as an unseen dataset to evaluate deraining algorithms, focusing on stereo image deraining and semantic understanding."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions three datasets: MC-blur, RWBI, and RainKITTI2012. These are used as unseen datasets to evaluate the performance of the proposed method.",
      "processing_time": 81.53178024291992,
      "citing_paper_id": "274845190",
      "cited_paper_id": 227130078
    },
    {
      "context_text": "Following a similar architecture, IPT [44] enhanced the representation capability to cater to multiple tasks by means of the high-capacity model and pre-training paradigm.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (IPT) and its capabilities. There are no verifiable resources or datasets mentioned.",
      "processing_time": 61.1983757019043,
      "citing_paper_id": "274845190",
      "cited_paper_id": 227239228
    },
    {
      "context_text": "To this end, we employ the popular Window-based Multi-head Self-Attention (W-MSA) [51] to reduce the computation burden.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (W-MSA) used in the research.",
      "processing_time": 59.96684908866882,
      "citing_paper_id": "274845190",
      "cited_paper_id": 232352874
    },
    {
      "context_text": "By comparison, some compared baseline methods, such as MIMO-UNet [28], FFANet [54], DRSformer [60], suffer from heavy hazy effects.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only model names. There are no clear identifiers for datasets or other verifiable resources.",
      "processing_time": 60.9255690574646,
      "citing_paper_id": "274845190",
      "cited_paper_id": 236976210
    },
    {
      "context_text": "On the one hand, considering the effectiveness of multi-scale image information in many image restoration tasks [27], [28], [29], we propose a new Image Pyramid Transformer Network (IPT-Network) as the unified model, which can exploit multi-scale image information to establish the commonality of…",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only references to multi-scale image information and a proposed network. No verifiable datasets are named.",
      "processing_time": 61.189866065979004,
      "citing_paper_id": "274845190",
      "cited_paper_id": 236976210
    },
    {
      "context_text": "On the one hand, considering the effectiveness of multi-scale image information in many image restoration tasks [27], [28], [29], we propose a new Image Pyramid Transformer Network (IPT-Network) as the unified model, which can exploit multi-scale image information to establish the commonality of…",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only references to multi-scale image information and a proposed network. No verifiable datasets are named.",
      "processing_time": 61.189866065979004,
      "citing_paper_id": "274845190",
      "cited_paper_id": 247011083
    },
    {
      "context_text": "Specifically, the first group contains DRUNet [52] and MSANet [53] for denoising, MIMO-UNet [28] for deblurring, FFANet [54], AECRNet [55], Dehazeformer [56] and MB-TaylorFormer [57] for dehazing, RESCAN [58], PReNet [59] and DRSformer [60] for deraining, LMQFormer [35] for desnowing, Retinexformer…",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions various methods and networks for image restoration tasks but does not specify any datasets. The cited paper titles do not provide additional information about datasets.",
      "processing_time": 61.34201192855835,
      "citing_paper_id": "274845190",
      "cited_paper_id": 236976210
    },
    {
      "context_text": "Specifically, the first group contains DRUNet [52] and MSANet [53] for denoising, MIMO-UNet [28] for deblurring, FFANet [54], AECRNet [55], Dehazeformer [56] and MB-TaylorFormer [57] for dehazing, RESCAN [58], PReNet [59] and DRSformer [60] for deraining, LMQFormer [35] for desnowing, Retinexformer…",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions various methods and networks for image restoration tasks but does not specify any datasets. The cited paper titles do not provide additional information about datasets.",
      "processing_time": 61.34201192855835,
      "citing_paper_id": "274845190",
      "cited_paper_id": 247319040
    },
    {
      "context_text": "For instance, SwinIR [10] and Uformer [11] restricted the self-attention calculation into an overlapping 8 × 8 window.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models (SwinIR and Uformer). The context focuses on the methodology of restricting self-attention calculation within a window.",
      "processing_time": 62.217609167099,
      "citing_paper_id": "274845190",
      "cited_paper_id": 237266491
    },
    {
      "context_text": "The second group contains SwinIR [10], Uformer [11], Restormer [12], Stoformer [14], CAT [15], GRL [18], CODE [16], Shuf-fleFormer [17], and ART [13].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context lists various models and methods for image restoration, none of which are datasets. No specific datasets are mentioned or used.",
      "processing_time": 60.755972146987915,
      "citing_paper_id": "274845190",
      "cited_paper_id": 237266491
    },
    {
      "context_text": "The second group contains SwinIR [10], Uformer [11], Restormer [12], Stoformer [14], CAT [15], GRL [18], CODE [16], Shuf-fleFormer [17], and ART [13].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context lists various models and methods for image restoration, none of which are datasets. No specific datasets are mentioned or used.",
      "processing_time": 60.755972146987915,
      "citing_paper_id": "274845190",
      "cited_paper_id": 244346144
    },
    {
      "context_text": "The second group contains SwinIR [10], Uformer [11], Restormer [12], Stoformer [14], CAT [15], GRL [18], CODE [16], Shuf-fleFormer [17], and ART [13].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context lists various models and methods for image restoration, none of which are datasets. No specific datasets are mentioned or used.",
      "processing_time": 60.755972146987915,
      "citing_paper_id": "274845190",
      "cited_paper_id": 257255385
    },
    {
      "context_text": "The second group contains SwinIR [10], Uformer [11], Restormer [12], Stoformer [14], CAT [15], GRL [18], CODE [16], Shuf-fleFormer [17], and ART [13].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context lists various models and methods for image restoration, none of which are datasets. No specific datasets are mentioned or used.",
      "processing_time": 60.755972146987915,
      "citing_paper_id": "274845190",
      "cited_paper_id": 258509512
    },
    {
      "context_text": "The second group contains SwinIR [10], Uformer [11], Restormer [12], Stoformer [14], CAT [15], GRL [18], CODE [16], Shuf-fleFormer [17], and ART [13].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context lists various models and methods for image restoration, none of which are datasets. No specific datasets are mentioned or used.",
      "processing_time": 60.755972146987915,
      "citing_paper_id": "274845190",
      "cited_paper_id": 261081712
    },
    {
      "context_text": "Moreover, various general-purpose approaches [9], [10], [11], [12], [13], [14], [15], [16], [17], [18] have recently appeared to well address a wide range of image restoration tasks, demonstrating deep neural networks have a great potential to resolve the inverse problem with different forms under…",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general approaches and methods for image restoration using deep neural networks.",
      "processing_time": 60.3850417137146,
      "citing_paper_id": "274845190",
      "cited_paper_id": 237266491
    },
    {
      "context_text": "Moreover, various general-purpose approaches [9], [10], [11], [12], [13], [14], [15], [16], [17], [18] have recently appeared to well address a wide range of image restoration tasks, demonstrating deep neural networks have a great potential to resolve the inverse problem with different forms under…",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general approaches and methods for image restoration using deep neural networks.",
      "processing_time": 60.3850417137146,
      "citing_paper_id": "274845190",
      "cited_paper_id": 244346144
    },
    {
      "context_text": "Moreover, various general-purpose approaches [9], [10], [11], [12], [13], [14], [15], [16], [17], [18] have recently appeared to well address a wide range of image restoration tasks, demonstrating deep neural networks have a great potential to resolve the inverse problem with different forms under…",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general approaches and methods for image restoration using deep neural networks.",
      "processing_time": 60.3850417137146,
      "citing_paper_id": "274845190",
      "cited_paper_id": 245837508
    },
    {
      "context_text": "Moreover, various general-purpose approaches [9], [10], [11], [12], [13], [14], [15], [16], [17], [18] have recently appeared to well address a wide range of image restoration tasks, demonstrating deep neural networks have a great potential to resolve the inverse problem with different forms under…",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general approaches and methods for image restoration using deep neural networks.",
      "processing_time": 60.3850417137146,
      "citing_paper_id": "274845190",
      "cited_paper_id": 257255385
    },
    {
      "context_text": "Moreover, various general-purpose approaches [9], [10], [11], [12], [13], [14], [15], [16], [17], [18] have recently appeared to well address a wide range of image restoration tasks, demonstrating deep neural networks have a great potential to resolve the inverse problem with different forms under…",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general approaches and methods for image restoration using deep neural networks.",
      "processing_time": 60.3850417137146,
      "citing_paper_id": "274845190",
      "cited_paper_id": 258509512
    },
    {
      "context_text": "Moreover, various general-purpose approaches [9], [10], [11], [12], [13], [14], [15], [16], [17], [18] have recently appeared to well address a wide range of image restoration tasks, demonstrating deep neural networks have a great potential to resolve the inverse problem with different forms under…",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general approaches and methods for image restoration using deep neural networks.",
      "processing_time": 60.3850417137146,
      "citing_paper_id": "274845190",
      "cited_paper_id": 261081712
    },
    {
      "context_text": "From the results, we have the following findings: (1) On both GoPro and HIDE, our method ranks the second place in PSNR, only having inferior performance than the best-performing method of Restormer [12].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "GoPro",
        "HIDE"
      ],
      "dataset_descriptions": {
        "GoPro": "Used to evaluate image restoration methods, focusing on deblurring performance and comparing against state-of-the-art techniques.",
        "HIDE": "Used to assess image restoration quality, particularly in deblurring, and to benchmark performance against leading methods."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions 'GoPro' and 'HIDE' as datasets used for evaluating image restoration methods. These are specific datasets commonly used in the field.",
      "processing_time": 73.40677785873413,
      "citing_paper_id": "274845190",
      "cited_paper_id": 244346144
    },
    {
      "context_text": "Restormer [12] also ranks second place in the case of σ = 50, and our method exceeds it by 0.53 dB.",
      "catation_intent": "findings",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a comparison of performance metrics between methods.",
      "processing_time": 59.95176601409912,
      "citing_paper_id": "274845190",
      "cited_paper_id": 244346144
    },
    {
      "context_text": "Nevertheless, the performance gap between our method and Restormer [12] is marginal.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a comparison with another method called Restormer.",
      "processing_time": 60.37208271026611,
      "citing_paper_id": "274845190",
      "cited_paper_id": 244346144
    },
    {
      "context_text": "Specifically, our method is lower than Restormer [12] with only 0.1 dB and 0.2 dB on GoPro and HIDE, respectively.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "GoPro",
        "HIDE"
      ],
      "dataset_descriptions": {
        "GoPro": "Used to evaluate image restoration performance, focusing on deblurring high-resolution images with a specific emphasis on quantitative metrics.",
        "HIDE": "Used to assess image restoration quality, particularly in handling high-resolution images with complex distortions, emphasizing quantitative evaluation."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions 'GoPro' and 'HIDE' as datasets used for evaluating image restoration methods. These are specific datasets used in the field of image processing and computer vision.",
      "processing_time": 74.81232237815857,
      "citing_paper_id": "274845190",
      "cited_paper_id": 244346144
    },
    {
      "context_text": "Particularly, the second best score is achieved by Restormer [12], and our method obtains performance gains of 0.21 dB over it.",
      "catation_intent": "findings",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a comparison of performance scores between methods.",
      "processing_time": 60.209638595581055,
      "citing_paper_id": "274845190",
      "cited_paper_id": 244346144
    },
    {
      "context_text": "In the case of σ = 25, our method achieves comparable performance with the second best performing method of Restormer [12] in PSNR, while our method is higher than the second best performing method of promptIR [23] in SSIM.",
      "catation_intent": "findings",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only comparisons with other methods. No dataset names are present in the citation span.",
      "processing_time": 61.31696605682373,
      "citing_paper_id": "274845190",
      "cited_paper_id": 244346144
    },
    {
      "context_text": "Restormer [12] substituted the original spatial-wise attention with the channel-wise attention.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (Restormer) and a change in its architecture.",
      "processing_time": 40.54275131225586,
      "citing_paper_id": "274845190",
      "cited_paper_id": 244346144
    },
    {
      "context_text": "For example, MAXIM [9] employed the multi-axis gated MLP block and cross-gating block to simultaneously possess global and fully convolutional properties.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only model architectures and their properties.",
      "processing_time": 59.9330050945282,
      "citing_paper_id": "274845190",
      "cited_paper_id": 245837508
    },
    {
      "context_text": "UCL-Dehaze [33] employed the strengths of the patch-wise contrastive loss and pixel-wise contrastive loss to enhance the generalization ability.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions UCL-Dehaze but does not refer to a specific dataset. It describes a method or model, not a reusable dataset.",
      "processing_time": 62.4901328086853,
      "citing_paper_id": "274845190",
      "cited_paper_id": 248512492
    },
    {
      "context_text": "To address this challenging problem, the initial methods [19], [20], [21], [22], [23] focus on solving the feature entanglement to keep the specificity among different tasks.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to methods and tasks. There are no clear identifiers for datasets.",
      "processing_time": 61.575048208236694,
      "citing_paper_id": "274845190",
      "cited_paper_id": 250551851
    },
    {
      "context_text": "To address this challenging problem, the initial methods [19], [20], [21], [22], [23] focus on solving the feature entanglement to keep the specificity among different tasks.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to methods and tasks. There are no clear identifiers for datasets.",
      "processing_time": 61.575048208236694,
      "citing_paper_id": "274845190",
      "cited_paper_id": 265609786
    },
    {
      "context_text": "Otherwise, we note that some methods, such as AirNet [19], fail to recover well with distorted results.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method called AirNet. No dataset names are present in the citation span.",
      "processing_time": 61.797234773635864,
      "citing_paper_id": "274845190",
      "cited_paper_id": 250551851
    },
    {
      "context_text": "For example, AirNet [19] and ADMS [20] respectively employed the contrastive learning and degradation classifier for different degradation types to endow the encoder with feature discrimination ability.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods (contrastive learning and degradation classifier) used in the context of image restoration.",
      "processing_time": 62.05024170875549,
      "citing_paper_id": "274845190",
      "cited_paper_id": 250551851
    },
    {
      "context_text": "The third group contains AirNet [19], TransWeather [21] and promptIR [23].",
      "catation_intent": "none",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions AirNet, TransWeather, and promptIR, but these are likely methods or models rather than datasets, as indicated by the context and the cited paper title.",
      "processing_time": 63.27498722076416,
      "citing_paper_id": "274845190",
      "cited_paper_id": 250551851
    },
    {
      "context_text": "In addition, DeULLE [39] and LVE-S2D [40] respectively designed the luminance mask-guided low-light image generation module and siamese dynamic network to enhance the restoration performance under the video conditions.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions methods (DeULLE and LVE-S2D) but does not reference any specific datasets. The context focuses on the methodologies used for low-light image and video enhancement.",
      "processing_time": 63.957918882369995,
      "citing_paper_id": "274845190",
      "cited_paper_id": 250574953
    },
    {
      "context_text": "Moreover, the latest works [16], [18] are continually looking for new ways to model global context.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only a general reference to modeling global context in image restoration.",
      "processing_time": 61.55281186103821,
      "citing_paper_id": "274845190",
      "cited_paper_id": 257255385
    },
    {
      "context_text": "Moreover, the latest works [16], [18] are continually looking for new ways to model global context.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only a general reference to modeling global context in image restoration.",
      "processing_time": 61.55281186103821,
      "citing_paper_id": "274845190",
      "cited_paper_id": 261081712
    },
    {
      "context_text": "Nevertheless, Retix-former [61] does not demonstrate strong competitiveness on other image restoration sub-tasks in the all-one-in setting.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (Retix-former) and its performance on image restoration sub-tasks.",
      "processing_time": 62.7405104637146,
      "citing_paper_id": "274845190",
      "cited_paper_id": 257496232
    },
    {
      "context_text": "(2) The best performance in PSNR is achieved by Retixformer [61], which is a specific method for low-light enhancement using the Retinex principle.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions a method (Retixformer) and a metric (PSNR) but does not reference any specific dataset.",
      "processing_time": 62.46823573112488,
      "citing_paper_id": "274845190",
      "cited_paper_id": 257496232
    },
    {
      "context_text": "(3) Although our method is inferior to Retixformer [61], it still acquires the third-best performance on LOL dataset.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "LOL dataset"
      ],
      "dataset_descriptions": {
        "LOL dataset": "Used to evaluate low-light image enhancement methods, focusing on the performance of the proposed method compared to state-of-the-art techniques."
      },
      "confidence_score": 1.0,
      "reasoning": "The citation mentions the LOL dataset, which is used for evaluating low-light image enhancement methods. The dataset is relevant to the research topic of image restoration.",
      "processing_time": 70.04736876487732,
      "citing_paper_id": "274845190",
      "cited_paper_id": 257496232
    },
    {
      "context_text": "The head number of the multi-head attention is set to be [2, 2, 4, 8] from stage 1 to stage 4.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The provided context does not mention any specific datasets, only technical details about the multi-head attention mechanism. No verifiable resources are identified.",
      "processing_time": 62.737133741378784,
      "citing_paper_id": "274845190",
      "cited_paper_id": 257767233
    },
    {
      "context_text": "The head number of the multi-head attention is set to be [2, 2, 4, 8] from stage 1 to stage 4.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The provided context does not mention any specific datasets, only technical details about the multi-head attention mechanism. No verifiable resources are identified.",
      "processing_time": 62.737133741378784,
      "citing_paper_id": "274845190",
      "cited_paper_id": 258865387
    },
    {
      "context_text": "The head number of the multi-head attention is set to be [2, 2, 4, 8] from stage 1 to stage 4.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The provided context does not mention any specific datasets, only technical details about the multi-head attention mechanism. No verifiable resources are identified.",
      "processing_time": 62.737133741378784,
      "citing_paper_id": "274845190",
      "cited_paper_id": 267177503
    },
    {
      "context_text": "According to the diverse degradation factors, IR task is further specific to be image denoising [1], image deblurring [2], image dehazing [3], image deraining [4], [5], image desnowing [6], low-light enhancement [7], [8] and etc.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various image restoration tasks. No dataset names are provided.",
      "processing_time": 62.15584635734558,
      "citing_paper_id": "274845190",
      "cited_paper_id": 257767233
    },
    {
      "context_text": "According to the diverse degradation factors, IR task is further specific to be image denoising [1], image deblurring [2], image dehazing [3], image deraining [4], [5], image desnowing [6], low-light enhancement [7], [8] and etc.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various image restoration tasks. No dataset names are provided.",
      "processing_time": 62.15584635734558,
      "citing_paper_id": "274845190",
      "cited_paper_id": 258865387
    },
    {
      "context_text": "According to the diverse degradation factors, IR task is further specific to be image denoising [1], image deblurring [2], image dehazing [3], image deraining [4], [5], image desnowing [6], low-light enhancement [7], [8] and etc.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various image restoration tasks. No dataset names are provided.",
      "processing_time": 62.15584635734558,
      "citing_paper_id": "274845190",
      "cited_paper_id": 267177503
    },
    {
      "context_text": "According to the diverse degradation factors, IR task is further specific to be image denoising [1], image deblurring [2], image dehazing [3], image deraining [4], [5], image desnowing [6], low-light enhancement [7], [8] and etc.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various image restoration tasks. No dataset names are provided.",
      "processing_time": 62.15584635734558,
      "citing_paper_id": "274845190",
      "cited_paper_id": 268678353
    },
    {
      "context_text": "The number of blocks are set to be [2, 3, 3, 4] from stage 1 to stage 4.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The provided context does not mention any specific datasets, only the configuration of blocks in a network architecture. No verifiable resources are identified.",
      "processing_time": 62.91114139556885,
      "citing_paper_id": "274845190",
      "cited_paper_id": 257767233
    },
    {
      "context_text": "The number of blocks are set to be [2, 3, 3, 4] from stage 1 to stage 4.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The provided context does not mention any specific datasets, only the configuration of blocks in a network architecture. No verifiable resources are identified.",
      "processing_time": 62.91114139556885,
      "citing_paper_id": "274845190",
      "cited_paper_id": 258865387
    },
    {
      "context_text": "Besides, in order to enlarge the dependency range, Stoformer [14] and CAT [15] utilized the shifted window strategy to allow for inter-window connections.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods (Stoformer and CAT) and their approach to image restoration using the shifted window strategy.",
      "processing_time": 63.52120757102966,
      "citing_paper_id": "274845190",
      "cited_paper_id": 258509512
    },
    {
      "context_text": "AIRFormer [50] incorporated prior frequency information to restore multiple adverse weather images holistically.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (AIRFormer) and its application to image restoration. No verifiable dataset names are provided.",
      "processing_time": 63.64396548271179,
      "citing_paper_id": "274845190",
      "cited_paper_id": 260271048
    },
    {
      "context_text": "For example, CODE [16] combined the channel attention and spatial attention whilst designing an elegant fusion module, thereby activating more information.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method or model. The context focuses on the combination of channel and spatial attention in a fusion module.",
      "processing_time": 63.64694380760193,
      "citing_paper_id": "274845190",
      "cited_paper_id": 261081712
    },
    {
      "context_text": "Both PromptIR [23] and CAPTNet [45] inserted the learnable prompt interaction module into the decoder to implicitly learn the degradation knowledge.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models or methods. The context focuses on the use of learnable prompt interaction modules in image restoration models.",
      "processing_time": 18.5957453250885,
      "citing_paper_id": "274845190",
      "cited_paper_id": 261556765
    },
    {
      "context_text": "Recently, DA-CLIP [48] utilized a pre-trained vision-language model to obtain rich degradation-aware features, which were inserted into the restoration process.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (DA-CLIP) that uses a pre-trained vision-language model. No dataset names are provided.",
      "processing_time": 63.924094438552856,
      "citing_paper_id": "274845190",
      "cited_paper_id": 263605463
    },
    {
      "context_text": "Recently, LDR [22] leveraged a pre-trained vision-language model to strengthen the diversity among different weather removal tasks.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (LDR) and a pre-trained vision-language model. No verifiable datasets are referenced.",
      "processing_time": 63.91378164291382,
      "citing_paper_id": "274845190",
      "cited_paper_id": 265609786
    },
    {
      "context_text": "For example, DiffUIR [24] used a novel selective hourglass mapping method to integrate multiple IR tasks into a shared one.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method. The context focuses on the method 'DiffUIR' and its application in image restoration.",
      "processing_time": 63.918538093566895,
      "citing_paper_id": "274845190",
      "cited_paper_id": 268513542
    },
    {
      "context_text": "Differently, recent methods [24] begin to model the commonality of different tasks by mapping various distributions of multiple IR tasks into a shared one.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method or approach. There are no clear identifiers for datasets in the provided context.",
      "processing_time": 63.63154864311218,
      "citing_paper_id": "274845190",
      "cited_paper_id": 268513542
    },
    {
      "context_text": "OKM [49] modeled the common multi-scale representation ability of diverse image restoration tasks.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (OKM) for image restoration tasks.",
      "processing_time": 63.22247862815857,
      "citing_paper_id": "274845190",
      "cited_paper_id": 271274210
    },
    {
      "context_text": "Recently, works [19], [20], [34], [35] combining text prompts with pretrained visual-language models [36], [37] have been used to provide additional prior knowledge for the restoration network.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only the use of pretrained visual-language models. The cited papers do not provide additional context to identify specific datasets.",
      "processing_time": 65.4057297706604,
      "citing_paper_id": "274701967",
      "cited_paper_id": 52967399
    },
    {
      "context_text": "Recently, works [19], [20], [34], [35] combining text prompts with pretrained visual-language models [36], [37] have been used to provide additional prior knowledge for the restoration network.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only the use of pretrained visual-language models. The cited papers do not provide additional context to identify specific datasets.",
      "processing_time": 65.4057297706604,
      "citing_paper_id": "274701967",
      "cited_paper_id": 231591445
    },
    {
      "context_text": "Recently, works [19], [20], [34], [35] combining text prompts with pretrained visual-language models [36], [37] have been used to provide additional prior knowledge for the restoration network.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only the use of pretrained visual-language models. The cited papers do not provide additional context to identify specific datasets.",
      "processing_time": 65.4057297706604,
      "citing_paper_id": "274701967",
      "cited_paper_id": 259164829
    },
    {
      "context_text": "Recently, works [19], [20], [34], [35] combining text prompts with pretrained visual-language models [36], [37] have been used to provide additional prior knowledge for the restoration network.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only the use of pretrained visual-language models. The cited papers do not provide additional context to identify specific datasets.",
      "processing_time": 65.4057297706604,
      "citing_paper_id": "274701967",
      "cited_paper_id": 264145824
    },
    {
      "context_text": "Another chal-lengeisselectingtherightsetofparametersforoptimization,and classicalworksincludeoptimizingnormalizationlayers[53], [54]andfeatureextractors[51],[55].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to optimization techniques and feature extractors. No verifiable resources are identified.",
      "processing_time": 64.27304530143738,
      "citing_paper_id": "274701967",
      "cited_paper_id": 211205159
    },
    {
      "context_text": "This task entails a spectrum of objectives including but not limited to low-light enhancement[25],[26],[27],deraining[5],[7],deblurring[5],[28],[29], Authorized licensed use limited to the terms of the applicable license agreement with IEEE.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various image restoration tasks. The cited papers' titles do not provide additional dataset information.",
      "processing_time": 64.55044221878052,
      "citing_paper_id": "274701967",
      "cited_paper_id": 221068158
    },
    {
      "context_text": "This task entails a spectrum of objectives including but not limited to low-light enhancement[25],[26],[27],deraining[5],[7],deblurring[5],[28],[29], Authorized licensed use limited to the terms of the applicable license agreement with IEEE.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various image restoration tasks. The cited papers' titles do not provide additional dataset information.",
      "processing_time": 64.55044221878052,
      "citing_paper_id": "274701967",
      "cited_paper_id": 234482841
    },
    {
      "context_text": "This task entails a spectrum of objectives including but not limited to low-light enhancement[25],[26],[27],deraining[5],[7],deblurring[5],[28],[29], Authorized licensed use limited to the terms of the applicable license agreement with IEEE.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various image restoration tasks. The cited papers' titles do not provide additional dataset information.",
      "processing_time": 64.55044221878052,
      "citing_paper_id": "274701967",
      "cited_paper_id": 235358213
    },
    {
      "context_text": "This task entails a spectrum of objectives including but not limited to low-light enhancement[25],[26],[27],deraining[5],[7],deblurring[5],[28],[29], Authorized licensed use limited to the terms of the applicable license agreement with IEEE.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various image restoration tasks. The cited papers' titles do not provide additional dataset information.",
      "processing_time": 64.55044221878052,
      "citing_paper_id": "274701967",
      "cited_paper_id": 248085491
    },
    {
      "context_text": "Beneﬁting from knowledge obtained from large-scale image-text datasets, the CLIP [36] has exhibited exceptional performance on zero-shot image classiﬁcation tasks.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions 'large-scale image-text datasets' but does not specify any particular dataset. CLIP is a method, not a dataset, so it is excluded.",
      "processing_time": 65.85069727897644,
      "citing_paper_id": "274701967",
      "cited_paper_id": 231591445
    },
    {
      "context_text": "In contrast, our work employs a combination of CLIP [36] and learnable prompts to acquire more precise representations of degradation information, thereby enhancing the performance of image restoration.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models and methods. The context focuses on the use of CLIP and learnable prompts for image restoration.",
      "processing_time": 65.63207030296326,
      "citing_paper_id": "274701967",
      "cited_paper_id": 231591445
    },
    {
      "context_text": "With the advance-mentofdeeplearning,numerousIRmethodsbasedonCNNs[3], [4],[5]andTransforms[6],[7]havebeendevelopedforvarious tasks,suchasdenoising[6],[8],[8],deraining[5],[7],and dehazing[9],[10].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods and tasks. No dataset names are present in the text.",
      "processing_time": 64.27171969413757,
      "citing_paper_id": "274701967",
      "cited_paper_id": 235358213
    },
    {
      "context_text": "With the advance-mentofdeeplearning,numerousIRmethodsbasedonCNNs[3], [4],[5]andTransforms[6],[7]havebeendevelopedforvarious tasks,suchasdenoising[6],[8],[8],deraining[5],[7],and dehazing[9],[10].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods and tasks. No dataset names are present in the text.",
      "processing_time": 64.27171969413757,
      "citing_paper_id": "274701967",
      "cited_paper_id": 263784964
    },
    {
      "context_text": "With the advance-mentofdeeplearning,numerousIRmethodsbasedonCNNs[3], [4],[5]andTransforms[6],[7]havebeendevelopedforvarious tasks,suchasdenoising[6],[8],[8],deraining[5],[7],and dehazing[9],[10].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods and tasks. No dataset names are present in the text.",
      "processing_time": 64.27171969413757,
      "citing_paper_id": "274701967",
      "cited_paper_id": null
    },
    {
      "context_text": "With the advance-mentofdeeplearning,numerousIRmethodsbasedonCNNs[3], [4],[5]andTransforms[6],[7]havebeendevelopedforvarious tasks,suchasdenoising[6],[8],[8],deraining[5],[7],and dehazing[9],[10].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods and tasks. No dataset names are present in the text.",
      "processing_time": 64.27171969413757,
      "citing_paper_id": "274701967",
      "cited_paper_id": null
    },
    {
      "context_text": "Notably, extant prompt learning techniques [46], [47] predominantly concentrate on engineering better prompts for high-level computer vision tasks.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only prompt learning techniques for high-level computer vision tasks.",
      "processing_time": 63.760385036468506,
      "citing_paper_id": "274701967",
      "cited_paper_id": 237386023
    },
    {
      "context_text": "The CoOp model [46] applies prompt learning to adapt visual language models for downstream computer vision tasks.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a model (CoOp) and its application. No verifiable datasets are referenced.",
      "processing_time": 65.06382608413696,
      "citing_paper_id": "274701967",
      "cited_paper_id": 237386023
    },
    {
      "context_text": "TransWeather [33] leverages transformers to effectively manage diverse weather-induced deteriorations.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions 'TransWeather' but does not refer to it as a dataset. It is described as leveraging transformers for image restoration, which suggests it is a method or model.",
      "processing_time": 66.91482305526733,
      "citing_paper_id": "274701967",
      "cited_paper_id": 244714491
    },
    {
      "context_text": "A challenge in this ﬁeld is to design an effective test-time loss, and classical works include minimizing the entropy of the probability distribution [50] and adding self-supervised multitask branches [51], [52] to modify the training process.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and approaches. The context focuses on test-time loss and self-supervised multitask branches, which are not datasets.",
      "processing_time": 66.68552279472351,
      "citing_paper_id": "274701967",
      "cited_paper_id": 247410579
    },
    {
      "context_text": "Althoughtherehavebeen somesolutions,thesemethods[51],[52],[53]areoftendifﬁcult toapplytotheﬁeldofimagerestoration.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to methods. There are no clear identifiers for datasets or other verifiable resources.",
      "processing_time": 65.60758852958679,
      "citing_paper_id": "274701967",
      "cited_paper_id": 247410579
    },
    {
      "context_text": "Considering Restormer [8] and NAFNet [29] as robust generalized image restoration baselines, we additionally train these models under the same settings for comparison.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models and methods. There are no clear identifiers for datasets in the provided context.",
      "processing_time": 65.04316544532776,
      "citing_paper_id": "274701967",
      "cited_paper_id": 248085491
    },
    {
      "context_text": "Considering Restormer [8] and NAFNet [29] as robust generalized image restoration baselines, we additionally train these models under the same settings for comparison.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models and methods. There are no clear identifiers for datasets in the provided context.",
      "processing_time": 65.04316544532776,
      "citing_paper_id": "274701967",
      "cited_paper_id": null
    },
    {
      "context_text": "Within the framework, the backbone restoration network is based on the NAFNet [29] architecture, considering it is a reliable image restoration baseline.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a method (NAFNet) which is excluded according to the instructions.",
      "processing_time": 65.05103635787964,
      "citing_paper_id": "274701967",
      "cited_paper_id": 248085491
    },
    {
      "context_text": "Recently, several works [11], [12], [13], [14], [30] have been developed to investigate multitask restoration techniques aiming to tackle different forms of degradation within a set of pretraining weights.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to works that investigate multitask restoration techniques. No clear, verifiable datasets are identified.",
      "processing_time": 66.17093062400818,
      "citing_paper_id": "274701967",
      "cited_paper_id": 259144110
    },
    {
      "context_text": "There are also methods that train additional classiﬁers [14], [17] or manually provide degradation category information [13] to help the model distinguish between different tasks.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and approaches. No verifiable resources are identified.",
      "processing_time": 64.24748373031616,
      "citing_paper_id": "274701967",
      "cited_paper_id": 259144110
    },
    {
      "context_text": "Subsequent works have assisted models in discriminating between different tasks through strategies like comparative learning [17], classiﬁers [18], or manually assigned degradation labels [13].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods and strategies. No dataset names are present in the text.",
      "processing_time": 65.03279209136963,
      "citing_paper_id": "274701967",
      "cited_paper_id": 259144110
    },
    {
      "context_text": "Toaddressthisissue,recentstudies[11],[12],[13],[14] have begun to shift towards all-in-one IR, which aims to address multiple degradation types through a set of pretraining weights.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only a shift in research approach. No dataset names are present in the text.",
      "processing_time": 65.60043787956238,
      "citing_paper_id": "274701967",
      "cited_paper_id": 259144110
    },
    {
      "context_text": "Therefore, recent approaches [21], [22], [23] have delvedintotheutilizationoflearnablepromptstoenhancedegra-dationperceptionbylearningtherepresentationofdegradation fromtrainingdata.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.2,
      "reasoning": "The context does not mention any specific datasets, only the use of training data in a general sense. No multi-word proper nouns, acronyms, or hyphenated names with digits are present.",
      "processing_time": 68.20684027671814,
      "citing_paper_id": "274701967",
      "cited_paper_id": 259243623
    },
    {
      "context_text": "Therefore, recent approaches [21], [22], [23] have delvedintotheutilizationoflearnablepromptstoenhancedegra-dationperceptionbylearningtherepresentationofdegradation fromtrainingdata.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.2,
      "reasoning": "The context does not mention any specific datasets, only the use of training data in a general sense. No multi-word proper nouns, acronyms, or hyphenated names with digits are present.",
      "processing_time": 68.20684027671814,
      "citing_paper_id": "274701967",
      "cited_paper_id": 261556765
    },
    {
      "context_text": "In addition, existing methods [21], [22], [23] based on prompt learning lack a ﬂexible and effective prompt adjustment strategy to ensure accurate degradation perception in data with different distribution, resulting in suboptimal model generalization.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only methods and their limitations. No dataset names are present in the citation span.",
      "processing_time": 65.35756063461304,
      "citing_paper_id": "274701967",
      "cited_paper_id": 259243623
    },
    {
      "context_text": "In addition, existing methods [21], [22], [23] based on prompt learning lack a ﬂexible and effective prompt adjustment strategy to ensure accurate degradation perception in data with different distribution, resulting in suboptimal model generalization.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only methods and their limitations. No dataset names are present in the citation span.",
      "processing_time": 65.35756063461304,
      "citing_paper_id": "274701967",
      "cited_paper_id": 261556765
    },
    {
      "context_text": "To better represent degradation information, learnable prompts [21], [22], [23] are used to learn degradation representations from training data.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.2,
      "reasoning": "The context mentions 'training data' but does not specify a named dataset. The cited papers do not provide additional specific dataset names.",
      "processing_time": 65.93488931655884,
      "citing_paper_id": "274701967",
      "cited_paper_id": 259243623
    },
    {
      "context_text": "To better represent degradation information, learnable prompts [21], [22], [23] are used to learn degradation representations from training data.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.2,
      "reasoning": "The context mentions 'training data' but does not specify a named dataset. The cited papers do not provide additional specific dataset names.",
      "processing_time": 65.93488931655884,
      "citing_paper_id": "274701967",
      "cited_paper_id": 261556765
    },
    {
      "context_text": "Later, some methods [19], [20] also leverage text prompts to convey degradation information to the model, yet manually designing prompts can be laborious and difﬁcult to accurately represent degradation information.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and challenges in designing text prompts for image restoration.",
      "processing_time": 64.86132335662842,
      "citing_paper_id": "274701967",
      "cited_paper_id": 264145824
    },
    {
      "context_text": "PIP [39] learns both high-level degradation-aware prompts and low-level basic restoration prompts to improve restoration performance.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method called PIP. The context focuses on the technique and its application in image restoration.",
      "processing_time": 66.51308465003967,
      "citing_paper_id": "274701967",
      "cited_paper_id": 266149517
    },
    {
      "context_text": "Early methods [15], [16] lack the ability of degradation perception, thus requiring training specialized encoders and corresponding decoders, along with a manual selection of appropriate subnetworks to address various degradation issues.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general methods and approaches. There are no verifiable resources or specific datasets mentioned.",
      "processing_time": 66.14732003211975,
      "citing_paper_id": "274701967",
      "cited_paper_id": null
    },
    {
      "context_text": "The idea of adjusting the model at test time to enhance its generalization to changes in data distribution has been applied to various tasks [47], [49].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general idea applied to various tasks. No verifiable resources are identified.",
      "processing_time": 65.93039798736572,
      "citing_paper_id": "274701967",
      "cited_paper_id": null
    },
    {
      "context_text": "Restrictions apply. dehazing [9], [10], and denoising [6], [8], [8], among others.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general tasks such as dehazing and denoising. No verifiable resources are identified.",
      "processing_time": 67.04792594909668,
      "citing_paper_id": "274701967",
      "cited_paper_id": null
    },
    {
      "context_text": "Restrictions apply. dehazing [9], [10], and denoising [6], [8], [8], among others.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general tasks such as dehazing and denoising. No verifiable resources are identified.",
      "processing_time": 67.04792594909668,
      "citing_paper_id": "274701967",
      "cited_paper_id": null
    },
    {
      "context_text": "Evaluation is performed on the BSD68 [53] and Urban100 [54] datasets.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "BSD68",
        "Urban100"
      ],
      "dataset_descriptions": {
        "BSD68": "Used to evaluate image restoration methods, focusing on natural image denoising and restoration performance.",
        "Urban100": "Used to evaluate image restoration methods, focusing on urban scene restoration and enhancement quality."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions two specific datasets, BSD68 and Urban100, which are used for evaluation in the research.",
      "processing_time": 77.73775601387024,
      "citing_paper_id": "278528939",
      "cited_paper_id": 64193
    },
    {
      "context_text": "For SRResNet [69] and Uformer [36], integrating DPPD results in better overall performance.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only models (SRResNet and Uformer). The integration of DPPD (Dynamic Positional Pyramid Distillation) is discussed, but no dataset names are provided.",
      "processing_time": 70.1665313243866,
      "citing_paper_id": "278528939",
      "cited_paper_id": 211227
    },
    {
      "context_text": "For SRResNet [69] and Uformer [36], integrating DPPD results in better overall performance.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only models (SRResNet and Uformer). The integration of DPPD (Dynamic Positional Pyramid Distillation) is discussed, but no dataset names are provided.",
      "processing_time": 70.1665313243866,
      "citing_paper_id": "278528939",
      "cited_paper_id": 235358213
    },
    {
      "context_text": "3) Results on Setting 3: Table III presents a comprehensive comparison of our proposed method against various learning strategies integrated into two baseline architectures, SRResNet [69] and Uformer [36], across seven distinct degradation tasks.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and architectures. The context focuses on comparing the proposed method against other learning strategies using SRResNet and Uformer.",
      "processing_time": 68.34541606903076,
      "citing_paper_id": "278528939",
      "cited_paper_id": 211227
    },
    {
      "context_text": "3) Results on Setting 3: Table III presents a comprehensive comparison of our proposed method against various learning strategies integrated into two baseline architectures, SRResNet [69] and Uformer [36], across seven distinct degradation tasks.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and architectures. The context focuses on comparing the proposed method against other learning strategies using SRResNet and Uformer.",
      "processing_time": 68.34541606903076,
      "citing_paper_id": "278528939",
      "cited_paper_id": 235358213
    },
    {
      "context_text": "For image denoising, we combine the BSD400 [51] and WED [52] datasets for training, which include 400 and 4,744 high-quality images, respectively.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "BSD400",
        "WED"
      ],
      "dataset_descriptions": {
        "BSD400": "Used for training image denoising models, providing 400 high-quality images to enhance model performance.",
        "WED": "Used for training image denoising models, providing 4,744 high-quality images to enhance model performance."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions two specific datasets, BSD400 and WED, which are used for training in the context of image denoising.",
      "processing_time": 80.68126964569092,
      "citing_paper_id": "278528939",
      "cited_paper_id": 4840263
    },
    {
      "context_text": "For image deblurring, we utilize the GoPro [64] dataset, which consists of 2,103 pairs of blurred and sharp images for training and 1,111 pairs for testing.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "GoPro"
      ],
      "dataset_descriptions": {
        "GoPro": "Used for training and testing image deblurring models, consisting of 2,103 pairs of blurred and sharp images for training and 1,111 pairs for testing."
      },
      "confidence_score": 1.0,
      "reasoning": "The context clearly mentions the GoPro dataset, which is used for training and testing image deblurring models.",
      "processing_time": 75.36004304885864,
      "citing_paper_id": "278528939",
      "cited_paper_id": 8671030
    },
    {
      "context_text": "Dehazing is simulated using the atmospheric scattering model with varying visibility parameters [56].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific dataset, only a method or model for simulating dehazing.",
      "processing_time": 66.62936067581177,
      "citing_paper_id": "278528939",
      "cited_paper_id": 39760169
    },
    {
      "context_text": "For image dehazing, the RESIDE [56] dataset is employed, comprising the Outdoor Training Set (OTS) with 8,990 hazy-clean image pairs for training, and the Synthetic Outdoor Testing Set (SOTS) with 500 pairs for evaluation.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "RESIDE"
      ],
      "dataset_descriptions": {
        "RESIDE": "Used for training and evaluating image dehazing models, specifically employing the Outdoor Training Set (OTS) for training and the Synthetic Outdoor Testing Set (SOTS) for evaluation."
      },
      "confidence_score": 1.0,
      "reasoning": "The context clearly mentions the RESIDE dataset, which is used for training and evaluating image dehazing models. The dataset is described with specific subsets and their purposes.",
      "processing_time": 77.38477778434753,
      "citing_paper_id": "278528939",
      "cited_paper_id": 39760169
    },
    {
      "context_text": "5(a)) such as image denoising [7], [8], [27], and super-resolution [3], [28], [29], [30], [31], [32], image deraining [9], [10], [11], [33], and image dehazing [13], [14], [15],.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various image restoration tasks. No dataset names are provided.",
      "processing_time": 66.62459325790405,
      "citing_paper_id": "278528939",
      "cited_paper_id": 49657846
    },
    {
      "context_text": "5(a)) such as image denoising [7], [8], [27], and super-resolution [3], [28], [29], [30], [31], [32], image deraining [9], [10], [11], [33], and image dehazing [13], [14], [15],.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various image restoration tasks. No dataset names are provided.",
      "processing_time": 66.62459325790405,
      "citing_paper_id": "278528939",
      "cited_paper_id": 208138077
    },
    {
      "context_text": "5(a)) such as image denoising [7], [8], [27], and super-resolution [3], [28], [29], [30], [31], [32], image deraining [9], [10], [11], [33], and image dehazing [13], [14], [15],.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various image restoration tasks. No dataset names are provided.",
      "processing_time": 66.62459325790405,
      "citing_paper_id": "278528939",
      "cited_paper_id": 237198939
    },
    {
      "context_text": "5(a)) such as image denoising [7], [8], [27], and super-resolution [3], [28], [29], [30], [31], [32], image deraining [9], [10], [11], [33], and image dehazing [13], [14], [15],.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various image restoration tasks. No dataset names are provided.",
      "processing_time": 66.62459325790405,
      "citing_paper_id": "278528939",
      "cited_paper_id": 257636509
    },
    {
      "context_text": "5(a)) such as image denoising [7], [8], [27], and super-resolution [3], [28], [29], [30], [31], [32], image deraining [9], [10], [11], [33], and image dehazing [13], [14], [15],.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various image restoration tasks. No dataset names are provided.",
      "processing_time": 66.62459325790405,
      "citing_paper_id": "278528939",
      "cited_paper_id": 272552320
    },
    {
      "context_text": "Signiﬁcant progress has been made in various speciﬁc restoration tasks, including image super-resolution [3], [4], [5], [6], image denoising [7], [8], image deraining [9], [10], [11], [12], and image dehazing [13], [14], [15].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various image restoration tasks. The cited paper titles do not provide additional context about datasets.",
      "processing_time": 67.97646808624268,
      "citing_paper_id": "278528939",
      "cited_paper_id": 208138077
    },
    {
      "context_text": "Signiﬁcant progress has been made in various speciﬁc restoration tasks, including image super-resolution [3], [4], [5], [6], image denoising [7], [8], image deraining [9], [10], [11], [12], and image dehazing [13], [14], [15].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various image restoration tasks. The cited paper titles do not provide additional context about datasets.",
      "processing_time": 67.97646808624268,
      "citing_paper_id": "278528939",
      "cited_paper_id": 237198939
    },
    {
      "context_text": "Signiﬁcant progress has been made in various speciﬁc restoration tasks, including image super-resolution [3], [4], [5], [6], image denoising [7], [8], image deraining [9], [10], [11], [12], and image dehazing [13], [14], [15].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various image restoration tasks. The cited paper titles do not provide additional context about datasets.",
      "processing_time": 67.97646808624268,
      "citing_paper_id": "278528939",
      "cited_paper_id": 257636509
    },
    {
      "context_text": "One of the pioneering works in this direction is the Image Processing Transformer (IPT) [18], which adopts a multi-branch structure (see Fig.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (IPT). No dataset names are present in the citation span.",
      "processing_time": 67.61957859992981,
      "citing_paper_id": "278528939",
      "cited_paper_id": 227239228
    },
    {
      "context_text": "In recent years, there have been many works uniﬁed frameworks focusing on handling multiple degradations within a single model [18], [19], [20], [21], [38], [39], [40], [41], [42], [43], [44].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to other works. The cited paper titles do not provide additional context about datasets.",
      "processing_time": 67.97140789031982,
      "citing_paper_id": "278528939",
      "cited_paper_id": 227239228
    },
    {
      "context_text": "In recent years, there have been many works uniﬁed frameworks focusing on handling multiple degradations within a single model [18], [19], [20], [21], [38], [39], [40], [41], [42], [43], [44].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to other works. The cited paper titles do not provide additional context about datasets.",
      "processing_time": 67.97140789031982,
      "citing_paper_id": "278528939",
      "cited_paper_id": 250551851
    },
    {
      "context_text": "In recent years, there have been many works uniﬁed frameworks focusing on handling multiple degradations within a single model [18], [19], [20], [21], [38], [39], [40], [41], [42], [43], [44].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to other works. The cited paper titles do not provide additional context about datasets.",
      "processing_time": 67.97140789031982,
      "citing_paper_id": "278528939",
      "cited_paper_id": 251197000
    },
    {
      "context_text": "In recent years, there have been many works uniﬁed frameworks focusing on handling multiple degradations within a single model [18], [19], [20], [21], [38], [39], [40], [41], [42], [43], [44].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to other works. The cited paper titles do not provide additional context about datasets.",
      "processing_time": 67.97140789031982,
      "citing_paper_id": "278528939",
      "cited_paper_id": 260085391
    },
    {
      "context_text": "In recent years, there have been many works uniﬁed frameworks focusing on handling multiple degradations within a single model [18], [19], [20], [21], [38], [39], [40], [41], [42], [43], [44].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to other works. The cited paper titles do not provide additional context about datasets.",
      "processing_time": 67.97140789031982,
      "citing_paper_id": "278528939",
      "cited_paper_id": 268064460
    },
    {
      "context_text": "Early attempts [18] address multiple degradation tasks using a shared backbone network, leveraging shared data priors to improve performance across tasks.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general reference to 'shared data priors'. No clear, verifiable dataset names are provided.",
      "processing_time": 68.33001637458801,
      "citing_paper_id": "278528939",
      "cited_paper_id": 227239228
    },
    {
      "context_text": "We compare our method with several state-of-the-art models, including MPRNet [58], Restormer [12], NAFNet [8], FSNet [11], DRSformer [15], MambaIR [59], DL [60], AirNet [19], IDR [40], Gridformer [61], NDR [62], InstructIR [63], and Perceive-IR [57].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span mentions several models but does not refer to any specific datasets. The context is focused on comparing methods, not using datasets.",
      "processing_time": 67.80066108703613,
      "citing_paper_id": "278528939",
      "cited_paper_id": 231802205
    },
    {
      "context_text": "We compare our method with several state-of-the-art models, including MPRNet [58], Restormer [12], NAFNet [8], FSNet [11], DRSformer [15], MambaIR [59], DL [60], AirNet [19], IDR [40], Gridformer [61], NDR [62], InstructIR [63], and Perceive-IR [57].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span mentions several models but does not refer to any specific datasets. The context is focused on comparing methods, not using datasets.",
      "processing_time": 67.80066108703613,
      "citing_paper_id": "278528939",
      "cited_paper_id": 250551851
    },
    {
      "context_text": "We compare our method with several state-of-the-art models, including MPRNet [58], Restormer [12], NAFNet [8], FSNet [11], DRSformer [15], MambaIR [59], DL [60], AirNet [19], IDR [40], Gridformer [61], NDR [62], InstructIR [63], and Perceive-IR [57].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span mentions several models but does not refer to any specific datasets. The context is focused on comparing methods, not using datasets.",
      "processing_time": 67.80066108703613,
      "citing_paper_id": "278528939",
      "cited_paper_id": 257636509
    },
    {
      "context_text": "We compare our method with several state-of-the-art models, including MPRNet [58], Restormer [12], NAFNet [8], FSNet [11], DRSformer [15], MambaIR [59], DL [60], AirNet [19], IDR [40], Gridformer [61], NDR [62], InstructIR [63], and Perceive-IR [57].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span mentions several models but does not refer to any specific datasets. The context is focused on comparing methods, not using datasets.",
      "processing_time": 67.80066108703613,
      "citing_paper_id": "278528939",
      "cited_paper_id": 260085391
    },
    {
      "context_text": "We compare our method with several state-of-the-art models, including MPRNet [58], Restormer [12], NAFNet [8], FSNet [11], DRSformer [15], MambaIR [59], DL [60], AirNet [19], IDR [40], Gridformer [61], NDR [62], InstructIR [63], and Perceive-IR [57].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span mentions several models but does not refer to any specific datasets. The context is focused on comparing methods, not using datasets.",
      "processing_time": 67.80066108703613,
      "citing_paper_id": "278528939",
      "cited_paper_id": 267320695
    },
    {
      "context_text": "We compare our method with several state-of-the-art models, including MPRNet [58], Restormer [12], NAFNet [8], FSNet [11], DRSformer [15], MambaIR [59], DL [60], AirNet [19], IDR [40], Gridformer [61], NDR [62], InstructIR [63], and Perceive-IR [57].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span mentions several models but does not refer to any specific datasets. The context is focused on comparing methods, not using datasets.",
      "processing_time": 67.80066108703613,
      "citing_paper_id": "278528939",
      "cited_paper_id": 267938238
    },
    {
      "context_text": "More recently, transformer-based architectures have further enhanced performance by capturing long-range dependencies and complex structures [4], [12], [34], [35], [36], [37].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only transformer-based architectures and their performance improvements. No verifiable resources are identified.",
      "processing_time": 67.95558094978333,
      "citing_paper_id": "278528939",
      "cited_paper_id": 235358213
    },
    {
      "context_text": "More recently, transformer-based architectures have further enhanced performance by capturing long-range dependencies and complex structures [4], [12], [34], [35], [36], [37].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only transformer-based architectures and their performance improvements. No verifiable resources are identified.",
      "processing_time": 67.95558094978333,
      "citing_paper_id": "278528939",
      "cited_paper_id": 270736284
    },
    {
      "context_text": "Prompt learning has gained signiﬁcant attention in natural language processing (NLP) as a method to adapt large language models (LLMs) to downstream tasks [45].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (prompt learning) and its application in NLP. The cited paper title confirms the focus on methods rather than datasets.",
      "processing_time": 69.90598511695862,
      "citing_paper_id": "278528939",
      "cited_paper_id": 236493269
    },
    {
      "context_text": "Building upon this, CoCoOp [47] introduced instance-speciﬁc prompts conditioned on input image features, employing a small neural network to generate dynamic prompts.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (CoCoOp) that uses instance-specific prompts. No dataset names are provided in the context.",
      "processing_time": 69.06981253623962,
      "citing_paper_id": "278528939",
      "cited_paper_id": 237386023
    },
    {
      "context_text": "For instance, Zhou et al. [46] proposed Context Optimization (CoOp), which learns continuous prompts to adapt the CLIP model for downstream classiﬁcation tasks by optimizing a set of learnable prompt tokens while keeping the pretrained model frozen.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method called Context Optimization (CoOp).",
      "processing_time": 67.1271550655365,
      "citing_paper_id": "278528939",
      "cited_paper_id": 247363011
    },
    {
      "context_text": "To achieve discriminative degradation information, following prototype learning [49], the prototypes are designed to satisfy: where K is the number of prototypes.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method or approach related to prototype learning.",
      "processing_time": 66.83006691932678,
      "citing_paper_id": "278528939",
      "cited_paper_id": 249953544
    },
    {
      "context_text": "For example, AirNet [19] employs self-supervised pre-training to learn degradation representations.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (AirNet) and its approach to self-supervised pre-training.",
      "processing_time": 68.12985110282898,
      "citing_paper_id": "278528939",
      "cited_paper_id": 250551851
    },
    {
      "context_text": "Task prompts are essential for guiding the restoration model to perform the appropriate inverse mapping for each degradation type [19], [20], [21].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only the importance of task prompts for image restoration. No verifiable resources are identified.",
      "processing_time": 68.30872797966003,
      "citing_paper_id": "278528939",
      "cited_paper_id": 250551851
    },
    {
      "context_text": "Task prompts are essential for guiding the restoration model to perform the appropriate inverse mapping for each degradation type [19], [20], [21].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only the importance of task prompts for image restoration. No verifiable resources are identified.",
      "processing_time": 68.30872797966003,
      "citing_paper_id": "278528939",
      "cited_paper_id": 268064460
    },
    {
      "context_text": "Methods like AirNet [19] employ unsupervised learning techniques to obtain degradation features without labeled data (see Fig.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a method (AirNet) and a general approach (unsupervised learning).",
      "processing_time": 68.51286673545837,
      "citing_paper_id": "278528939",
      "cited_paper_id": 250551851
    },
    {
      "context_text": "Recognizing that DPPD serves as a plug-and-play enhancement for task prompts, we integrated it into several representative methods, speciﬁcally PromptIR [20] and AirNet [19].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only methods (PromptIR and AirNet). The cited papers' titles do not provide additional dataset information.",
      "processing_time": 69.05962896347046,
      "citing_paper_id": "278528939",
      "cited_paper_id": 250551851
    },
    {
      "context_text": "Recognizing that DPPD serves as a plug-and-play enhancement for task prompts, we integrated it into several representative methods, speciﬁcally PromptIR [20] and AirNet [19].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only methods (PromptIR and AirNet). The cited papers' titles do not provide additional dataset information.",
      "processing_time": 69.05962896347046,
      "citing_paper_id": "278528939",
      "cited_paper_id": 268064460
    },
    {
      "context_text": "From left to right: Input hazy images, results from AirNet [19], original PromptIR [20], our enhanced PromptIR with DPPD, and ground truth (GT) images.",
      "catation_intent": "reusable resource",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.2,
      "reasoning": "The context mentions 'ground truth (GT) images' which could imply a dataset, but there are no specific dataset names mentioned. The context focuses on comparing different methods on input hazy images.",
      "processing_time": 71.245858669281,
      "citing_paper_id": "278528939",
      "cited_paper_id": 250551851
    },
    {
      "context_text": "From left to right: Input hazy images, results from AirNet [19], original PromptIR [20], our enhanced PromptIR with DPPD, and ground truth (GT) images.",
      "catation_intent": "reusable resource",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.2,
      "reasoning": "The context mentions 'ground truth (GT) images' which could imply a dataset, but there are no specific dataset names mentioned. The context focuses on comparing different methods on input hazy images.",
      "processing_time": 71.245858669281,
      "citing_paper_id": "278528939",
      "cited_paper_id": 268064460
    },
    {
      "context_text": "From left to right: Input rainy images, results from AirNet [19], original PromptIR [20], our enhanced PromptIR with DPPD, and ground truth (GT) images.",
      "catation_intent": "reusable resource",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.2,
      "reasoning": "The context mentions 'ground truth (GT) images' which could imply a dataset, but there is no specific named dataset mentioned. The other items mentioned are methods or models, not datasets.",
      "processing_time": 71.00642800331116,
      "citing_paper_id": "278528939",
      "cited_paper_id": 250551851
    },
    {
      "context_text": "From left to right: Input rainy images, results from AirNet [19], original PromptIR [20], our enhanced PromptIR with DPPD, and ground truth (GT) images.",
      "catation_intent": "reusable resource",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.2,
      "reasoning": "The context mentions 'ground truth (GT) images' which could imply a dataset, but there is no specific named dataset mentioned. The other items mentioned are methods or models, not datasets.",
      "processing_time": 71.00642800331116,
      "citing_paper_id": "278528939",
      "cited_paper_id": 268064460
    },
    {
      "context_text": "1) Setting 1: Three Distinct Degradation Tasks: Following the setups in AirNet [19] and PromptIR [20], we conduct experiments across a range of datasets encompassing ﬁve di ﬀ erent degradation types, focusing on dehazing, deraining, and denoising tasks with multiple noise levels.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.3,
      "reasoning": "The context mentions 'experiments across a range of datasets' but does not specify any particular dataset names. The degradation types (dehazing, deraining, denoising) are mentioned, but no specific datasets are named.",
      "processing_time": 50.03728723526001,
      "citing_paper_id": "278528939",
      "cited_paper_id": 250551851
    },
    {
      "context_text": "1) Setting 1: Three Distinct Degradation Tasks: Following the setups in AirNet [19] and PromptIR [20], we conduct experiments across a range of datasets encompassing ﬁve di ﬀ erent degradation types, focusing on dehazing, deraining, and denoising tasks with multiple noise levels.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.3,
      "reasoning": "The context mentions 'experiments across a range of datasets' but does not specify any particular dataset names. The degradation types (dehazing, deraining, denoising) are mentioned, but no specific datasets are named.",
      "processing_time": 50.03728723526001,
      "citing_paper_id": "278528939",
      "cited_paper_id": 268064460
    },
    {
      "context_text": "AirNet [19], IDR [40], InstructIR [63]), and Perceive-IR [57].",
      "catation_intent": "none",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only models or methods. The cited paper titles do not provide additional context to identify datasets.",
      "processing_time": 68.49751138687134,
      "citing_paper_id": "278528939",
      "cited_paper_id": 250551851
    },
    {
      "context_text": "AirNet [19], IDR [40], InstructIR [63]), and Perceive-IR [57].",
      "catation_intent": "none",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only models or methods. The cited paper titles do not provide additional context to identify datasets.",
      "processing_time": 68.49751138687134,
      "citing_paper_id": "278528939",
      "cited_paper_id": 260085391
    },
    {
      "context_text": "AirNet [19], IDR [40], InstructIR [63]), and Perceive-IR [57].",
      "catation_intent": "none",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only models or methods. The cited paper titles do not provide additional context to identify datasets.",
      "processing_time": 68.49751138687134,
      "citing_paper_id": "278528939",
      "cited_paper_id": 267320695
    },
    {
      "context_text": "* A RE A DOPTED F ORM E XISTING P APERS [20], [57], [40] synthetic rain streaks following the method in [55].",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only a method for generating synthetic rain streaks. No dataset names are present in the context.",
      "processing_time": 68.67617011070251,
      "citing_paper_id": "278528939",
      "cited_paper_id": 260085391
    },
    {
      "context_text": "* A RE A DOPTED F ORM E XISTING P APERS [20], [57], [40] synthetic rain streaks following the method in [55].",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only a method for generating synthetic rain streaks. No dataset names are present in the context.",
      "processing_time": 68.67617011070251,
      "citing_paper_id": "278528939",
      "cited_paper_id": 268064460
    },
    {
      "context_text": "Following the protocol in [40], denoising results are reported for noise level σ = 25.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a noise level for denoising results.",
      "processing_time": 67.34575819969177,
      "citing_paper_id": "278528939",
      "cited_paper_id": 260085391
    },
    {
      "context_text": "2) Setting 2: Five Degradation Types: Expanding upon Setting 2, we introduce Setting 3 to evaluate our method on ﬁve degradation tasks, following the experimental setup in [40].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a reference to an experimental setup in another paper.",
      "processing_time": 67.5738513469696,
      "citing_paper_id": "278528939",
      "cited_paper_id": 260085391
    },
    {
      "context_text": "6, our method builds upon existing architectures like Restormer [12] and PromptIR [20], integrating a novel prompt module that is the focus of our approach.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models and methods. The context focuses on architectural components and the novel prompt module.",
      "processing_time": 68.67642068862915,
      "citing_paper_id": "278528939",
      "cited_paper_id": 268064460
    },
    {
      "context_text": "PromptIR [20] introduces an adaptive prompt framework that extracts degradation representations end-to-end (see Fig.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method or framework called PromptIR. No verifiable datasets are referenced.",
      "processing_time": 68.48424959182739,
      "citing_paper_id": "278528939",
      "cited_paper_id": 268064460
    },
    {
      "context_text": "PromptIR [20] introduces an adaptive prompt framework where the prompts are directly parameterized and jointly optimized with the restoration model.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a method (PromptIR) and its application in image restoration.",
      "processing_time": 68.10035681724548,
      "citing_paper_id": "278528939",
      "cited_paper_id": 268064460
    },
    {
      "context_text": "PromptIR has achieved significant performance improvements and has inspired numerous subsequent studies [22], [23].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only references to other papers. No verifiable resources are identified.",
      "processing_time": 67.90944409370422,
      "citing_paper_id": "278528939",
      "cited_paper_id": 268553835
    },
    {
      "context_text": "Furthermore, some research [24], [25], [26] incorporates pretrained text models and texture representations to guide the restoration process, integrating rich semantic information but introducing additional complexity.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only the use of pretrained text models and texture representations. No clear, verifiable datasets are identified.",
      "processing_time": 69.38559341430664,
      "citing_paper_id": "278528939",
      "cited_paper_id": 272724277
    },
    {
      "context_text": "Meanwhile, recent methods based on texture prompts introduce text information and leverage pretrained multimodal models [24], [25], [26].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and models. The context focuses on the use of texture prompts and pretrained multimodal models for image restoration.",
      "processing_time": 70.08005809783936,
      "citing_paper_id": "278528939",
      "cited_paper_id": 272724277
    },
    {
      "context_text": "3) Seven-Task Setting: We follow the protocol described in [12], where images are synthesized with seven different degradation types (e.g., super-resolution, blur, noise, ) [80], the Rain1200 dataset [86], and the Snow100k dataset [85].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Rain1200",
        "Snow100k"
      ],
      "dataset_descriptions": {
        "Rain1200": "Used to synthesize images with rain degradation for evaluating image deraining techniques, focusing on the effectiveness of the restoration methods.",
        "Snow100k": "Used to synthesize images with snow degradation for evaluating image desnowing techniques, focusing on the effectiveness of the restoration methods."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions the Rain1200 and Snow100k datasets, which are specific and relevant to the image restoration tasks involving rain and snow removal.",
      "processing_time": 85.58727145195007,
      "citing_paper_id": "277780904",
      "cited_paper_id": 3406592
    },
    {
      "context_text": "3) Seven-Task Setting: We follow the protocol described in [12], where images are synthesized with seven different degradation types (e.g., super-resolution, blur, noise, ) [80], the Rain1200 dataset [86], and the Snow100k dataset [85].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Rain1200",
        "Snow100k"
      ],
      "dataset_descriptions": {
        "Rain1200": "Used to synthesize images with rain degradation for evaluating image deraining techniques, focusing on the effectiveness of the restoration methods.",
        "Snow100k": "Used to synthesize images with snow degradation for evaluating image desnowing techniques, focusing on the effectiveness of the restoration methods."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions the Rain1200 and Snow100k datasets, which are specific and relevant to the image restoration tasks involving rain and snow removal.",
      "processing_time": 85.58727145195007,
      "citing_paper_id": "277780904",
      "cited_paper_id": 39760169
    },
    {
      "context_text": "3) Seven-Task Setting: We follow the protocol described in [12], where images are synthesized with seven different degradation types (e.g., super-resolution, blur, noise, ) [80], the Rain1200 dataset [86], and the Snow100k dataset [85].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Rain1200",
        "Snow100k"
      ],
      "dataset_descriptions": {
        "Rain1200": "Used to synthesize images with rain degradation for evaluating image deraining techniques, focusing on the effectiveness of the restoration methods.",
        "Snow100k": "Used to synthesize images with snow degradation for evaluating image desnowing techniques, focusing on the effectiveness of the restoration methods."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions the Rain1200 and Snow100k datasets, which are specific and relevant to the image restoration tasks involving rain and snow removal.",
      "processing_time": 85.58727145195007,
      "citing_paper_id": "277780904",
      "cited_paper_id": 266844752
    },
    {
      "context_text": "For denoising, we use BSD400 [77] and WED [78] for training, and test on BSD68 with noise levels of 15, 25, and 50.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "BSD400",
        "WED",
        "BSD68"
      ],
      "dataset_descriptions": {
        "BSD400": "Used for training denoising models, providing a diverse set of images to improve generalization.",
        "WED": "Used for training denoising models, offering additional challenges for image quality assessment.",
        "BSD68": "Used for testing denoising performance at specific noise levels (15, 25, 50) to evaluate model effectiveness."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions specific datasets used for training and testing in the image restoration task, particularly for denoising.",
      "processing_time": 86.93124008178711,
      "citing_paper_id": "277780904",
      "cited_paper_id": 4840263
    },
    {
      "context_text": "Image restoration (IR) has long been a core challenge in low-level computer vision, focusing on recovering high-quality images from degraded observations [1], [2].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general reference to image restoration as a challenge in computer vision.",
      "processing_time": 68.25825428962708,
      "citing_paper_id": "277780904",
      "cited_paper_id": 8235201
    },
    {
      "context_text": "For deraining, we employ the Rain100L [79] dataset, and for dehazing, we use the outdoor subset of RESIDE [80].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Rain100L",
        "RESIDE"
      ],
      "dataset_descriptions": {
        "Rain100L": "Used for deraining tasks, focusing on synthetic rain images to evaluate the performance of image restoration models.",
        "RESIDE": "Used for dehazing tasks, specifically employing the outdoor subset to assess the effectiveness of dehazing algorithms."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions two specific datasets, Rain100L and RESIDE, which are used for deraining and dehazing tasks, respectively.",
      "processing_time": 82.488596200943,
      "citing_paper_id": "277780904",
      "cited_paper_id": 39760169
    },
    {
      "context_text": "We tested on four challenging weather degradation datasets: Snow100K-S and Snow100K-L [85] for snow removal, Outdoor-Rain [84] for rain removal, and RainDrop [83] for raindrop removal.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Snow100K-S",
        "Snow100K-L",
        "Outdoor-Rain",
        "RainDrop"
      ],
      "dataset_descriptions": {
        "Snow100K-S": "Used for testing snow removal algorithms, focusing on synthetic snow images with varying intensities and complexities.",
        "Snow100K-L": "Used for testing snow removal algorithms, focusing on synthetic snow images with higher resolution and complexity compared to Snow100K-S.",
        "Outdoor-Rain": "Used for testing rain removal algorithms, focusing on real-world outdoor scenes affected by rain, evaluating the effectiveness of restoration methods.",
        "RainDrop": "Used for testing raindrop removal algorithms, focusing on images with raindrops on camera lenses, assessing the ability to restore clear visibility."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions four specific datasets used for testing image restoration under various weather degradations. These datasets are clearly named and relevant to the research topic.",
      "processing_time": 96.45440077781677,
      "citing_paper_id": "277780904",
      "cited_paper_id": 131773964
    },
    {
      "context_text": "For instance, dehazing and low-light enhancement share global illumination patterns, while rain removal and denoising involve similar texture statistics [16], [17].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and their applications. The context focuses on the similarities in texture statistics and global illumination patterns between different image restoration tasks.",
      "processing_time": 70.61409854888916,
      "citing_paper_id": "277780904",
      "cited_paper_id": 201134031
    },
    {
      "context_text": "For example, Li et al. introduce AirNet, which integrates self-supervised pretraining [9] to obtain a degradation encoder, while Potlapalli et al. propose PromptIR, utilizing learnable parameters as task prompts to enable adaptive multiple degradation handling [10].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only methods and models. The cited papers do not provide additional context to identify datasets.",
      "processing_time": 69.17758464813232,
      "citing_paper_id": "277780904",
      "cited_paper_id": 207930212
    },
    {
      "context_text": "For example, Li et al. introduce AirNet, which integrates self-supervised pretraining [9] to obtain a degradation encoder, while Potlapalli et al. propose PromptIR, utilizing learnable parameters as task prompts to enable adaptive multiple degradation handling [10].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only methods and models. The cited papers do not provide additional context to identify datasets.",
      "processing_time": 69.17758464813232,
      "citing_paper_id": "277780904",
      "cited_paper_id": 268064460
    },
    {
      "context_text": "…progress has been achieved in addressing single degradation tasks, where specialized algorithms are developed for specific degradation types, such as image denoising [20], [21], deblurring [4], [22]–[25], dehazing [26]–[30], deraining [5], [31]–[33], and low-light enhancement [7], [34]–[37].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context discusses various image restoration tasks but does not mention any specific datasets. The cited papers also do not provide clear dataset names within the given context.",
      "processing_time": 19.642943143844604,
      "citing_paper_id": "277780904",
      "cited_paper_id": 214623217
    },
    {
      "context_text": "…progress has been achieved in addressing single degradation tasks, where specialized algorithms are developed for specific degradation types, such as image denoising [20], [21], deblurring [4], [22]–[25], dehazing [26]–[30], deraining [5], [31]–[33], and low-light enhancement [7], [34]–[37].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context discusses various image restoration tasks but does not mention any specific datasets. The cited papers also do not provide clear dataset names within the given context.",
      "processing_time": 19.642943143844604,
      "citing_paper_id": "277780904",
      "cited_paper_id": 247748724
    },
    {
      "context_text": "…progress has been achieved in addressing single degradation tasks, where specialized algorithms are developed for specific degradation types, such as image denoising [20], [21], deblurring [4], [22]–[25], dehazing [26]–[30], deraining [5], [31]–[33], and low-light enhancement [7], [34]–[37].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context discusses various image restoration tasks but does not mention any specific datasets. The cited papers also do not provide clear dataset names within the given context.",
      "processing_time": 19.642943143844604,
      "citing_paper_id": "277780904",
      "cited_paper_id": 273320871
    },
    {
      "context_text": "…progress has been achieved in addressing single degradation tasks, where specialized algorithms are developed for specific degradation types, such as image denoising [20], [21], deblurring [4], [22]–[25], dehazing [26]–[30], deraining [5], [31]–[33], and low-light enhancement [7], [34]–[37].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context discusses various image restoration tasks but does not mention any specific datasets. The cited papers also do not provide clear dataset names within the given context.",
      "processing_time": 19.642943143844604,
      "citing_paper_id": "277780904",
      "cited_paper_id": 273659089
    },
    {
      "context_text": "We conduct comprehensive experiments across a diverse range of all-in-one image restoration scenarios, including 3-task [15], 5-task [11], and 7-task [12] configurations, as well as challenging weather condition restoration [18] and composite degradation tasks [19].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.2,
      "reasoning": "The context mentions various task configurations and challenging scenarios for all-in-one image restoration, but does not explicitly name any datasets. The cited papers' titles do not provide additional dataset names.",
      "processing_time": 71.55719566345215,
      "citing_paper_id": "277780904",
      "cited_paper_id": 244714491
    },
    {
      "context_text": "We conduct comprehensive experiments across a diverse range of all-in-one image restoration scenarios, including 3-task [15], 5-task [11], and 7-task [12] configurations, as well as challenging weather condition restoration [18] and composite degradation tasks [19].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.2,
      "reasoning": "The context mentions various task configurations and challenging scenarios for all-in-one image restoration, but does not explicitly name any datasets. The cited papers' titles do not provide additional dataset names.",
      "processing_time": 71.55719566345215,
      "citing_paper_id": "277780904",
      "cited_paper_id": 266844752
    },
    {
      "context_text": "We conduct comprehensive experiments across a diverse range of all-in-one image restoration scenarios, including 3-task [15], 5-task [11], and 7-task [12] configurations, as well as challenging weather condition restoration [18] and composite degradation tasks [19].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.2,
      "reasoning": "The context mentions various task configurations and challenging scenarios for all-in-one image restoration, but does not explicitly name any datasets. The cited papers' titles do not provide additional dataset names.",
      "processing_time": 71.55719566345215,
      "citing_paper_id": "277780904",
      "cited_paper_id": 271039782
    },
    {
      "context_text": ": To evaluate our method’s effectiveness on real-world weather degradations, we conducted comprehensive experiments following the protocol established in [18].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.2,
      "reasoning": "The citation does not mention any specific dataset names, only a general reference to 'real-world weather degradations'. No clear, verifiable dataset names are provided.",
      "processing_time": 70.94461703300476,
      "citing_paper_id": "277780904",
      "cited_paper_id": 244714491
    },
    {
      "context_text": "Beyond these task-specific approaches, several general restoration architectures have emerged that provide strong baseline performance across multiple tasks [24], [38]–[44], and some recent research with diffusion model-based architectures [45]–[48].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only general restoration architectures and diffusion models. No verifiable resources are identified.",
      "processing_time": 69.17380619049072,
      "citing_paper_id": "277780904",
      "cited_paper_id": 248085491
    },
    {
      "context_text": "Beyond these task-specific approaches, several general restoration architectures have emerged that provide strong baseline performance across multiple tasks [24], [38]–[44], and some recent research with diffusion model-based architectures [45]–[48].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only general restoration architectures and diffusion models. No verifiable resources are identified.",
      "processing_time": 69.17380619049072,
      "citing_paper_id": "277780904",
      "cited_paper_id": 268364340
    },
    {
      "context_text": "Beyond these task-specific approaches, several general restoration architectures have emerged that provide strong baseline performance across multiple tasks [24], [38]–[44], and some recent research with diffusion model-based architectures [45]–[48].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only general restoration architectures and diffusion models. No verifiable resources are identified.",
      "processing_time": 69.17380619049072,
      "citing_paper_id": "277780904",
      "cited_paper_id": 275133634
    },
    {
      "context_text": "The first focuses on improving learnable task prompts through additional prior regularization techniques, such as frequency analysis in AdaIR [11] and principal component analysis in [49].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and techniques. The context is focused on improving task prompts using regularization techniques.",
      "processing_time": 69.36425042152405,
      "citing_paper_id": "277780904",
      "cited_paper_id": 260085391
    },
    {
      "context_text": "This setting follows [49] and tests the model’s ability to handle a diverse set of degradation types in real-world scenarios.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general reference to handling diverse degradation types in real-world scenarios.",
      "processing_time": 68.82383704185486,
      "citing_paper_id": "277780904",
      "cited_paper_id": 260085391
    },
    {
      "context_text": "Contrastive learning has emerged as a powerful paradigm in self-supervised representation learning, enabling models to learn discriminative features by contrasting positive and negative sample pairs [55].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general method (contrastive learning).",
      "processing_time": 68.22677516937256,
      "citing_paper_id": "277780904",
      "cited_paper_id": 261046875
    },
    {
      "context_text": "However, recent studies [13] reveal that highly discriminative features optimized for classification do not always translate to improved restoration quality, indicating a misalignment between obtaining discriminative prompts and the requirements of the restoration model.",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general observation about the misalignment between discriminative features and restoration quality.",
      "processing_time": 69.35754585266113,
      "citing_paper_id": "277780904",
      "cited_paper_id": 263605463
    },
    {
      "context_text": "Further extending this concept, several studies have incorporated pre-trained vision-language models (VLMs) like CLIP [50] to inject semantic information into the restoration process [13], [51].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only pre-trained models and their application in image restoration.",
      "processing_time": 68.61865830421448,
      "citing_paper_id": "277780904",
      "cited_paper_id": 263605463
    },
    {
      "context_text": "In recent years, researchers have increasingly adapted contrastive learning principles to address the unique challenges of low-level image restoration tasks [56]–[62].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only a general reference to contrastive learning principles in image restoration.",
      "processing_time": 69.15774488449097,
      "citing_paper_id": "277780904",
      "cited_paper_id": 265999804
    },
    {
      "context_text": "Moving toward more general image restoration frameworks, Wu et al. [62] introduced a task-agnostic model contrastive learning paradigm that simplifies negative sample construction while maintaining effectiveness across different degradation types.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method or approach. The context focuses on a task-agnostic model contrastive learning paradigm for image restoration.",
      "processing_time": 70.9243700504303,
      "citing_paper_id": "277780904",
      "cited_paper_id": 265999804
    },
    {
      "context_text": "Based on existing research [10], [12]–[15], task prompts are crucial for all-in-one image restoration and are typically obtained through either adaptive or explicit learning.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only the importance of task prompts for all-in-one image restoration. No verifiable resources are identified.",
      "processing_time": 70.38992071151733,
      "citing_paper_id": "277780904",
      "cited_paper_id": 266844752
    },
    {
      "context_text": "Based on existing research [10], [12]–[15], task prompts are crucial for all-in-one image restoration and are typically obtained through either adaptive or explicit learning.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only the importance of task prompts for all-in-one image restoration. No verifiable resources are identified.",
      "processing_time": 70.38992071151733,
      "citing_paper_id": "277780904",
      "cited_paper_id": 268064460
    },
    {
      "context_text": ": To thoroughly evaluate our method’s versatility across diverse image restoration tasks, we conducted experiments on a comprehensive seven-task benchmark [12] derived from the DF2K dataset.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "DF2K"
      ],
      "dataset_descriptions": {
        "DF2K": "Used to evaluate the method's versatility across diverse image restoration tasks, specifically through a comprehensive seven-task benchmark."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions the DF2K dataset, which is a specific and verifiable dataset used for image restoration tasks.",
      "processing_time": 75.94856834411621,
      "citing_paper_id": "277780904",
      "cited_paper_id": 266844752
    },
    {
      "context_text": "MioIR [12] introduced a sequential learning strategy, effectively capturing task-specific characteristics through a pretrained degradation classifier.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method or strategy. The context focuses on a pretrained degradation classifier, which is not a dataset.",
      "processing_time": 70.58731508255005,
      "citing_paper_id": "277780904",
      "cited_paper_id": 266844752
    },
    {
      "context_text": "…of degradation-aware and task-oriented prompts, such as AdaIR [11], which uses frequency-aware prompts to provide discriminative degradation representation, and two-stage frameworks [12]–[14] where pretrained classifiers help guide the degradation prior, resulting in improved performance.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only methods and approaches. No dataset names are present in the text.",
      "processing_time": 69.33864617347717,
      "citing_paper_id": "277780904",
      "cited_paper_id": 266844752
    },
    {
      "context_text": "…of degradation-aware and task-oriented prompts, such as AdaIR [11], which uses frequency-aware prompts to provide discriminative degradation representation, and two-stage frameworks [12]–[14] where pretrained classifiers help guide the degradation prior, resulting in improved performance.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only methods and approaches. No dataset names are present in the text.",
      "processing_time": 69.33864617347717,
      "citing_paper_id": "277780904",
      "cited_paper_id": 275921949
    },
    {
      "context_text": "Alternatively, explicit prompt learning approaches employ a two-stage strategy with pretrained degradation classification [12]–[14] to yield more discriminative representations.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only methods and strategies for image restoration. No verifiable resources are identified.",
      "processing_time": 69.33577752113342,
      "citing_paper_id": "277780904",
      "cited_paper_id": 266844752
    },
    {
      "context_text": "Alternatively, explicit prompt learning approaches employ a two-stage strategy with pretrained degradation classification [12]–[14] to yield more discriminative representations.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only methods and strategies for image restoration. No verifiable resources are identified.",
      "processing_time": 69.33577752113342,
      "citing_paper_id": "277780904",
      "cited_paper_id": 275921949
    },
    {
      "context_text": "The plug-and-play nature of our approach allows for seamless integration with existing architectures [10] without requiring fundamental redesigns or complex training protocols.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only architectural integration and training protocols.",
      "processing_time": 68.02582502365112,
      "citing_paper_id": "277780904",
      "cited_paper_id": 268064460
    },
    {
      "context_text": "Building on this foundation, PromptIR [10] integrated learnable prompt components during the decoding stage to inject task-related information, significantly enhancing the model’s flexibility and performance.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method called PromptIR. The context focuses on the integration of learnable prompt components in the decoding stage.",
      "processing_time": 71.29955863952637,
      "citing_paper_id": "277780904",
      "cited_paper_id": 268064460
    },
    {
      "context_text": "PromptIR [10] cannot fully exploit task-specific guidance, as the prompts themselves lack clear task boundaries.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (PromptIR). The context focuses on the limitations of the method rather than the use of a dataset.",
      "processing_time": 18.735785484313965,
      "citing_paper_id": "277780904",
      "cited_paper_id": 268064460
    },
    {
      "context_text": "Comparison of prompt selection probability distributions between baseline PromptIR [10] and our proposed method across different degradation tasks.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a comparison between methods. No dataset names are present in the citation span.",
      "processing_time": 69.78477573394775,
      "citing_paper_id": "277780904",
      "cited_paper_id": 268064460
    },
    {
      "context_text": "This redundancy is quantitatively evident in PromptIR [10], which uses a softmax function to ensemble multiple task-oriented prompts and the probability distribution is not so sparse.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a method called PromptIR. The citation is used to reference a method, not a dataset.",
      "processing_time": 70.56701874732971,
      "citing_paper_id": "277780904",
      "cited_paper_id": 268064460
    },
    {
      "context_text": "3) Prompt-Feature Fusion: After generating task-specific prompts, we perform a simple yet effective prompt-feature fusion by concatenating the selected sparse prompts p with input features x along the channel dimension, followed by a Transformer block for feature interaction, as in [10].",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for prompt-feature fusion in the context of image restoration.",
      "processing_time": 69.31895589828491,
      "citing_paper_id": "277780904",
      "cited_paper_id": 268064460
    },
    {
      "context_text": "These shared characteristics make it difficult for adaptive learning approaches, where prompts are learned alongside the restoration model [10], [11], [15], to develop non-overlapping task representations.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only adaptive learning approaches and prompts. No verifiable resources are identified.",
      "processing_time": 69.53449892997742,
      "citing_paper_id": "277780904",
      "cited_paper_id": 268064460
    },
    {
      "context_text": "However, existing prompt-based approaches frequently encounter prompt-task misalignment [10], where the intended prompt p t fails to provide distinct task-specific guidance.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a methodological issue in prompt-based approaches.",
      "processing_time": 68.77970218658447,
      "citing_paper_id": "277780904",
      "cited_paper_id": 268064460
    },
    {
      "context_text": "Yue et al. [48] proposed an efficient diffusion model for image restoration, termed Residual Shifting, which dramatically accelerates inference by reducing sampling steps through a novel residual-based Markov chain.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for image restoration.",
      "processing_time": 68.3845636844635,
      "citing_paper_id": "277780904",
      "cited_paper_id": 268364340
    },
    {
      "context_text": "As shown in Table V, our PromptIR+CPL achieves the best performance in all degradation settings, with an impressive average PSNR of 29.07 dB, surpassing the previous state-of-the-art OneRestore [19] by 0.60 dB and significantly out-performing the original PromptIR by 3.17 dB.",
      "catation_intent": "findings",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only performance comparisons with other methods.",
      "processing_time": 68.37353920936584,
      "citing_paper_id": "277780904",
      "cited_paper_id": 271039782
    },
    {
      "context_text": ": To thoroughly evaluate our method’s capability in handling real-world scenarios where multiple degradations often coexist, we conducted extensive experiments on composite degradation settings [19].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.0,
      "reasoning": "The citation does not mention any specific datasets, only a general evaluation context. No clear, verifiable resource names are provided.",
      "processing_time": 70.3423056602478,
      "citing_paper_id": "277780904",
      "cited_paper_id": 271039782
    },
    {
      "context_text": "Besides these, some researcher pay attention to the optimization of all-in-one image restoration and introduce multi-task learning approaches [52], [53] and masked image pretraining [54].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only methods and approaches. The cited paper title does not provide additional dataset information.",
      "processing_time": 70.16062760353088,
      "citing_paper_id": "277780904",
      "cited_paper_id": 272987034
    },
    {
      "context_text": "To overcome these limitations, all-in-one image restoration strategies have gained increasing attention [8].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general trend in all-in-one image restoration strategies.",
      "processing_time": 69.5275354385376,
      "citing_paper_id": "277780904",
      "cited_paper_id": 273502246
    },
    {
      "context_text": "To address this limitation, recent research has shifted towards developing all-in-one image restoration frameworks capable of handling multiple degradation types within a unified model architecture [8].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a shift in research towards all-in-one image restoration frameworks.",
      "processing_time": 69.75885486602783,
      "citing_paper_id": "277780904",
      "cited_paper_id": 273502246
    },
    {
      "context_text": "…the existing works [15], [18], we assess the effectiveness of the proposed approaches in multi-degradation restoration using seven datasets: BSD400 [41], BSD68 [41], WED [42], and Urban100 [43] for image denoising, Rain100L [44] for image deraining, RESIDE [45] for image dehazing, and GoPro [46]…",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "BSD400",
        "BSD68",
        "WED",
        "Urban100",
        "Rain100L",
        "RESIDE",
        "GoPro"
      ],
      "dataset_descriptions": {
        "BSD400": "Used for image denoising, assessing the effectiveness of the proposed approaches in restoring images with noise.",
        "BSD68": "Used for image denoising, evaluating the performance of the proposed methods on a smaller, curated set of images.",
        "WED": "Used for image denoising, testing the robustness of the proposed approaches on a diverse set of images.",
        "Urban100": "Used for image denoising, focusing on urban scenes to evaluate the restoration quality in complex environments.",
        "Rain100L": "Used for image deraining, assessing the ability of the proposed methods to remove rain streaks from images.",
        "RESIDE": "Used for image dehazing, evaluating the effectiveness of the proposed approaches in enhancing visibility in hazy images.",
        "GoPro": "Used for image deblurring, testing the performance of the proposed methods on motion-blurred images."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions seven specific datasets used for evaluating the effectiveness of the proposed approaches in multi-degradation restoration. Each dataset is associated with a specific task in image restoration.",
      "processing_time": 113.09600496292114,
      "citing_paper_id": "280011566",
      "cited_paper_id": 64193
    },
    {
      "context_text": "…works [15], [18], we assess the effectiveness of the proposed approaches in multi-degradation restoration using seven datasets: BSD400 [41], BSD68 [41], WED [42], and Urban100 [43] for image denoising, Rain100L [44] for image deraining, RESIDE [45] for image dehazing, and GoPro [46] for image…",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "BSD400",
        "BSD68",
        "WED",
        "Urban100",
        "Rain100L",
        "RESIDE",
        "GoPro"
      ],
      "dataset_descriptions": {
        "BSD400": "Used for image denoising, assessing the effectiveness of the proposed approaches in removing noise from images.",
        "BSD68": "Used for image denoising, evaluating the performance of the proposed methods in reducing noise.",
        "WED": "Used for image denoising, testing the robustness of the proposed techniques against various noise levels.",
        "Urban100": "Used for image denoising, focusing on urban scenes to evaluate the quality of restored images.",
        "Rain100L": "Used for image deraining, assessing the ability of the proposed methods to remove rain streaks from images.",
        "RESIDE": "Used for image dehazing, evaluating the effectiveness of the proposed approaches in enhancing visibility in hazy images.",
        "GoPro": "Used for image deblurring, testing the performance of the proposed methods in restoring sharpness to blurred images."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions multiple datasets used for evaluating the effectiveness of the proposed approaches in multi-degradation restoration. Each dataset is associated with a specific task in image restoration.",
      "processing_time": 112.0643515586853,
      "citing_paper_id": "280011566",
      "cited_paper_id": 64193
    },
    {
      "context_text": "Following the experimental setup of AirNet [15], we partition each clean image of the BSD68 [41] dataset into four regions.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "BSD68"
      ],
      "dataset_descriptions": {
        "BSD68": "Used to partition clean images into four regions for evaluating image restoration methods, focusing on segmentation accuracy and ecological statistics."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions the BSD68 dataset, which is a specific, verifiable dataset used for image restoration experiments.",
      "processing_time": 76.80782723426819,
      "citing_paper_id": "280011566",
      "cited_paper_id": 64193
    },
    {
      "context_text": "Prevailing image restoration efforts center on the meticulous design of task-specific approaches and have demonstrated promising results in tasks such as denoising [1]–[4], deraining [5]–[8], and deblurring [9]–[12].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general tasks in image restoration. No dataset names are provided in the context.",
      "processing_time": 70.86286234855652,
      "citing_paper_id": "280011566",
      "cited_paper_id": 996788
    },
    {
      "context_text": "Prevailing image restoration efforts center on the meticulous design of task-specific approaches and have demonstrated promising results in tasks such as denoising [1]–[4], deraining [5]–[8], and deblurring [9]–[12].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general tasks in image restoration. No dataset names are provided in the context.",
      "processing_time": 70.86286234855652,
      "citing_paper_id": "280011566",
      "cited_paper_id": 17115407
    },
    {
      "context_text": "Prevailing image restoration efforts center on the meticulous design of task-specific approaches and have demonstrated promising results in tasks such as denoising [1]–[4], deraining [5]–[8], and deblurring [9]–[12].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general tasks in image restoration. No dataset names are provided in the context.",
      "processing_time": 70.86286234855652,
      "citing_paper_id": "280011566",
      "cited_paper_id": 209323842
    },
    {
      "context_text": "Prevailing image restoration efforts center on the meticulous design of task-specific approaches and have demonstrated promising results in tasks such as denoising [1]–[4], deraining [5]–[8], and deblurring [9]–[12].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general tasks in image restoration. No dataset names are provided in the context.",
      "processing_time": 70.86286234855652,
      "citing_paper_id": "280011566",
      "cited_paper_id": 209376714
    },
    {
      "context_text": "Prevailing image restoration efforts center on the meticulous design of task-specific approaches and have demonstrated promising results in tasks such as denoising [1]–[4], deraining [5]–[8], and deblurring [9]–[12].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general tasks in image restoration. No dataset names are provided in the context.",
      "processing_time": 70.86286234855652,
      "citing_paper_id": "280011566",
      "cited_paper_id": 244398686
    },
    {
      "context_text": "Prevailing image restoration efforts center on the meticulous design of task-specific approaches and have demonstrated promising results in tasks such as denoising [1]–[4], deraining [5]–[8], and deblurring [9]–[12].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general tasks in image restoration. No dataset names are provided in the context.",
      "processing_time": 70.86286234855652,
      "citing_paper_id": "280011566",
      "cited_paper_id": 244908890
    },
    {
      "context_text": "Prevailing image restoration efforts center on the meticulous design of task-specific approaches and have demonstrated promising results in tasks such as denoising [1]–[4], deraining [5]–[8], and deblurring [9]–[12].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general tasks in image restoration. No dataset names are provided in the context.",
      "processing_time": 70.86286234855652,
      "citing_paper_id": "270878215",
      "cited_paper_id": 996788
    },
    {
      "context_text": "Prevailing image restoration efforts center on the meticulous design of task-specific approaches and have demonstrated promising results in tasks such as denoising [1]–[4], deraining [5]–[8], and deblurring [9]–[12].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general tasks in image restoration. No dataset names are provided in the context.",
      "processing_time": 70.86286234855652,
      "citing_paper_id": "270878215",
      "cited_paper_id": 209323842
    },
    {
      "context_text": "Prevailing image restoration efforts center on the meticulous design of task-specific approaches and have demonstrated promising results in tasks such as denoising [1]–[4], deraining [5]–[8], and deblurring [9]–[12].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general tasks in image restoration. No dataset names are provided in the context.",
      "processing_time": 70.86286234855652,
      "citing_paper_id": "270878215",
      "cited_paper_id": 244398686
    },
    {
      "context_text": "Prevailing image restoration efforts center on the meticulous design of task-specific approaches and have demonstrated promising results in tasks such as denoising [1]–[4], deraining [5]–[8], and deblurring [9]–[12].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general tasks in image restoration. No dataset names are provided in the context.",
      "processing_time": 70.86286234855652,
      "citing_paper_id": "270878215",
      "cited_paper_id": 247221335
    },
    {
      "context_text": "Numerous restoration methods have been developed for specific tasks, utilizing convolutional neural networks [1], [2], [5], [6], [9], [10], [19] or vision transformers [20]–[26].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various methods and models used for image restoration tasks.",
      "processing_time": 69.74599313735962,
      "citing_paper_id": "280011566",
      "cited_paper_id": 996788
    },
    {
      "context_text": "Numerous restoration methods have been developed for specific tasks, utilizing convolutional neural networks [1], [2], [5], [6], [9], [10], [19] or vision transformers [20]–[26].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various methods and models used for image restoration tasks.",
      "processing_time": 69.74599313735962,
      "citing_paper_id": "280011566",
      "cited_paper_id": 3406592
    },
    {
      "context_text": "Numerous restoration methods have been developed for specific tasks, utilizing convolutional neural networks [1], [2], [5], [6], [9], [10], [19] or vision transformers [20]–[26].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various methods and models used for image restoration tasks.",
      "processing_time": 69.74599313735962,
      "citing_paper_id": "280011566",
      "cited_paper_id": 4552226
    },
    {
      "context_text": "Numerous restoration methods have been developed for specific tasks, utilizing convolutional neural networks [1], [2], [5], [6], [9], [10], [19] or vision transformers [20]–[26].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various methods and models used for image restoration tasks.",
      "processing_time": 69.74599313735962,
      "citing_paper_id": "280011566",
      "cited_paper_id": 6593498
    },
    {
      "context_text": "Numerous restoration methods have been developed for specific tasks, utilizing convolutional neural networks [1], [2], [5], [6], [9], [10], [19] or vision transformers [20]–[26].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various methods and models used for image restoration tasks.",
      "processing_time": 69.74599313735962,
      "citing_paper_id": "280011566",
      "cited_paper_id": 10514149
    },
    {
      "context_text": "Numerous restoration methods have been developed for specific tasks, utilizing convolutional neural networks [1], [2], [5], [6], [9], [10], [19] or vision transformers [20]–[26].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various methods and models used for image restoration tasks.",
      "processing_time": 69.74599313735962,
      "citing_paper_id": "280011566",
      "cited_paper_id": 17115407
    },
    {
      "context_text": "Numerous restoration methods have been developed for specific tasks, utilizing convolutional neural networks [1], [2], [5], [6], [9], [10], [19] or vision transformers [20]–[26].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various methods and models used for image restoration tasks.",
      "processing_time": 69.74599313735962,
      "citing_paper_id": "280011566",
      "cited_paper_id": 244398686
    },
    {
      "context_text": "Numerous restoration methods have been developed for specific tasks, utilizing convolutional neural networks [1], [2], [5], [6], [9], [10], [19] or vision transformers [20]–[26].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various methods and models used for image restoration tasks.",
      "processing_time": 69.74599313735962,
      "citing_paper_id": "280011566",
      "cited_paper_id": 248085101
    },
    {
      "context_text": "Numerous restoration methods have been developed for specific tasks, utilizing convolutional neural networks [1], [2], [5], [6], [9], [10], [19] or vision transformers [20]–[26].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various methods and models used for image restoration tasks.",
      "processing_time": 69.74599313735962,
      "citing_paper_id": "280011566",
      "cited_paper_id": 252693111
    },
    {
      "context_text": "Traditional degradation estimation methods [1], [16] often assume a predefined degradation type and estimate degradation level, which makes them less effective in scenarios with multiple unknown degradations.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only general methods and their limitations. No dataset names are present in the text.",
      "processing_time": 70.85727262496948,
      "citing_paper_id": "280011566",
      "cited_paper_id": 996788
    },
    {
      "context_text": "Traditional degradation estimation methods [1], [16] often assume a predefined degradation type and estimate degradation level, which makes them less effective in scenarios with multiple unknown degradations.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only general methods and their limitations. No dataset names are present in the text.",
      "processing_time": 70.85727262496948,
      "citing_paper_id": "280011566",
      "cited_paper_id": 1974825
    },
    {
      "context_text": "…[15], [18], we assess the effectiveness of the proposed approaches in multi-degradation restoration using seven datasets: BSD400 [41], BSD68 [41], WED [42], and Urban100 [43] for image denoising, Rain100L [44] for image deraining, RESIDE [45] for image dehazing, and GoPro [46] for image deblurring.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "BSD400",
        "BSD68",
        "WED",
        "Urban100",
        "Rain100L",
        "RESIDE",
        "GoPro"
      ],
      "dataset_descriptions": {
        "BSD400": "Used for image denoising, assessing the effectiveness of proposed approaches in multi-degradation restoration.",
        "BSD68": "Used for image denoising, assessing the effectiveness of proposed approaches in multi-degradation restoration.",
        "WED": "Used for image denoising, assessing the effectiveness of proposed approaches in multi-degradation restoration.",
        "Urban100": "Used for image denoising, assessing the effectiveness of proposed approaches in multi-degradation restoration.",
        "Rain100L": "Used for image deraining, assessing the effectiveness of proposed approaches in multi-degradation restoration.",
        "RESIDE": "Used for image dehazing, assessing the effectiveness of proposed approaches in multi-degradation restoration.",
        "GoPro": "Used for image deblurring, assessing the effectiveness of proposed approaches in multi-degradation restoration."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions seven specific datasets used for evaluating the effectiveness of proposed approaches in multi-degradation restoration tasks.",
      "processing_time": 52.847362995147705,
      "citing_paper_id": "280011566",
      "cited_paper_id": 4840263
    },
    {
      "context_text": "…[15], [18], we assess the effectiveness of the proposed approaches in multi-degradation restoration using seven datasets: BSD400 [41], BSD68 [41], WED [42], and Urban100 [43] for image denoising, Rain100L [44] for image deraining, RESIDE [45] for image dehazing, and GoPro [46] for image deblurring.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "BSD400",
        "BSD68",
        "WED",
        "Urban100",
        "Rain100L",
        "RESIDE",
        "GoPro"
      ],
      "dataset_descriptions": {
        "BSD400": "Used for image denoising, assessing the effectiveness of proposed approaches in multi-degradation restoration.",
        "BSD68": "Used for image denoising, assessing the effectiveness of proposed approaches in multi-degradation restoration.",
        "WED": "Used for image denoising, assessing the effectiveness of proposed approaches in multi-degradation restoration.",
        "Urban100": "Used for image denoising, assessing the effectiveness of proposed approaches in multi-degradation restoration.",
        "Rain100L": "Used for image deraining, assessing the effectiveness of proposed approaches in multi-degradation restoration.",
        "RESIDE": "Used for image dehazing, assessing the effectiveness of proposed approaches in multi-degradation restoration.",
        "GoPro": "Used for image deblurring, assessing the effectiveness of proposed approaches in multi-degradation restoration."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions seven specific datasets used for evaluating the effectiveness of proposed approaches in multi-degradation restoration tasks.",
      "processing_time": 52.847362995147705,
      "citing_paper_id": "280011566",
      "cited_paper_id": 8282555
    },
    {
      "context_text": "…[15], [18], we assess the effectiveness of the proposed approaches in multi-degradation restoration using seven datasets: BSD400 [41], BSD68 [41], WED [42], and Urban100 [43] for image denoising, Rain100L [44] for image deraining, RESIDE [45] for image dehazing, and GoPro [46] for image deblurring.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "BSD400",
        "BSD68",
        "WED",
        "Urban100",
        "Rain100L",
        "RESIDE",
        "GoPro"
      ],
      "dataset_descriptions": {
        "BSD400": "Used for image denoising, assessing the effectiveness of proposed approaches in multi-degradation restoration.",
        "BSD68": "Used for image denoising, assessing the effectiveness of proposed approaches in multi-degradation restoration.",
        "WED": "Used for image denoising, assessing the effectiveness of proposed approaches in multi-degradation restoration.",
        "Urban100": "Used for image denoising, assessing the effectiveness of proposed approaches in multi-degradation restoration.",
        "Rain100L": "Used for image deraining, assessing the effectiveness of proposed approaches in multi-degradation restoration.",
        "RESIDE": "Used for image dehazing, assessing the effectiveness of proposed approaches in multi-degradation restoration.",
        "GoPro": "Used for image deblurring, assessing the effectiveness of proposed approaches in multi-degradation restoration."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions seven specific datasets used for evaluating the effectiveness of proposed approaches in multi-degradation restoration tasks.",
      "processing_time": 52.847362995147705,
      "citing_paper_id": "280011566",
      "cited_paper_id": 8671030
    },
    {
      "context_text": "…[15], [18], we assess the effectiveness of the proposed approaches in multi-degradation restoration using seven datasets: BSD400 [41], BSD68 [41], WED [42], and Urban100 [43] for image denoising, Rain100L [44] for image deraining, RESIDE [45] for image dehazing, and GoPro [46] for image deblurring.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "BSD400",
        "BSD68",
        "WED",
        "Urban100",
        "Rain100L",
        "RESIDE",
        "GoPro"
      ],
      "dataset_descriptions": {
        "BSD400": "Used for image denoising, assessing the effectiveness of proposed approaches in multi-degradation restoration.",
        "BSD68": "Used for image denoising, assessing the effectiveness of proposed approaches in multi-degradation restoration.",
        "WED": "Used for image denoising, assessing the effectiveness of proposed approaches in multi-degradation restoration.",
        "Urban100": "Used for image denoising, assessing the effectiveness of proposed approaches in multi-degradation restoration.",
        "Rain100L": "Used for image deraining, assessing the effectiveness of proposed approaches in multi-degradation restoration.",
        "RESIDE": "Used for image dehazing, assessing the effectiveness of proposed approaches in multi-degradation restoration.",
        "GoPro": "Used for image deblurring, assessing the effectiveness of proposed approaches in multi-degradation restoration."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions seven specific datasets used for evaluating the effectiveness of proposed approaches in multi-degradation restoration tasks.",
      "processing_time": 52.847362995147705,
      "citing_paper_id": "280011566",
      "cited_paper_id": 39760169
    },
    {
      "context_text": "…[15], [18], we assess the effectiveness of the proposed approaches in multi-degradation restoration using seven datasets: BSD400 [41], BSD68 [41], WED [42], and Urban100 [43] for image denoising, Rain100L [44] for image deraining, RESIDE [45] for image dehazing, and GoPro [46] for image deblurring.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "BSD400",
        "BSD68",
        "WED",
        "Urban100",
        "Rain100L",
        "RESIDE",
        "GoPro"
      ],
      "dataset_descriptions": {
        "BSD400": "Used for image denoising, assessing the effectiveness of proposed approaches in multi-degradation restoration.",
        "BSD68": "Used for image denoising, assessing the effectiveness of proposed approaches in multi-degradation restoration.",
        "WED": "Used for image denoising, assessing the effectiveness of proposed approaches in multi-degradation restoration.",
        "Urban100": "Used for image denoising, assessing the effectiveness of proposed approaches in multi-degradation restoration.",
        "Rain100L": "Used for image deraining, assessing the effectiveness of proposed approaches in multi-degradation restoration.",
        "RESIDE": "Used for image dehazing, assessing the effectiveness of proposed approaches in multi-degradation restoration.",
        "GoPro": "Used for image deblurring, assessing the effectiveness of proposed approaches in multi-degradation restoration."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions seven specific datasets used for evaluating the effectiveness of proposed approaches in multi-degradation restoration tasks.",
      "processing_time": 52.847362995147705,
      "citing_paper_id": "280011566",
      "cited_paper_id": 73439498
    },
    {
      "context_text": "Our comparison encompasses four single-degradation image restoration techniques, namely BDRNet [47], LP-Net [48], FDGAN [49], and MPRNet [50], alongside the multi-task method for multiple degradation image : The performance of various methods on denoising of σ = 25 (first row), deraining (second…",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions several image restoration techniques but does not explicitly reference any specific datasets. The focus is on comparing different methods rather than using a particular dataset.",
      "processing_time": 71.64967060089111,
      "citing_paper_id": "280011566",
      "cited_paper_id": 21712570
    },
    {
      "context_text": "Our comparison encompasses four single-degradation image restoration techniques, namely BDRNet [47], LP-Net [48], FDGAN [49], and MPRNet [50], alongside the multi-task method for multiple degradation image : The performance of various methods on denoising of σ = 25 (first row), deraining (second…",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions several image restoration techniques but does not explicitly reference any specific datasets. The focus is on comparing different methods rather than using a particular dataset.",
      "processing_time": 71.64967060089111,
      "citing_paper_id": "280011566",
      "cited_paper_id": 203042751
    },
    {
      "context_text": "Our comparison encompasses four single-degradation image restoration techniques, namely BDRNet [47], LP-Net [48], FDGAN [49], and MPRNet [50], alongside the multi-task method for multiple degradation image : The performance of various methods on denoising of σ = 25 (first row), deraining (second…",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions several image restoration techniques but does not explicitly reference any specific datasets. The focus is on comparing different methods rather than using a particular dataset.",
      "processing_time": 71.64967060089111,
      "citing_paper_id": "280011566",
      "cited_paper_id": 210838848
    },
    {
      "context_text": "Our comparison encompasses four single-degradation image restoration techniques, namely BDRNet [47], LP-Net [48], FDGAN [49], and MPRNet [50], alongside the multi-task method for multiple degradation image : The performance of various methods on denoising of σ = 25 (first row), deraining (second…",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions several image restoration techniques but does not explicitly reference any specific datasets. The focus is on comparing different methods rather than using a particular dataset.",
      "processing_time": 71.64967060089111,
      "citing_paper_id": "280011566",
      "cited_paper_id": 231802205
    },
    {
      "context_text": "Frequency domain frameworks [31]–[36] aim to bridge frequency gaps between sharp and degraded images.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only frequency domain frameworks. The cited papers do not provide additional context to identify datasets.",
      "processing_time": 71.24633717536926,
      "citing_paper_id": "280011566",
      "cited_paper_id": 43925777
    },
    {
      "context_text": "Frequency domain frameworks [31]–[36] aim to bridge frequency gaps between sharp and degraded images.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only frequency domain frameworks. The cited papers do not provide additional context to identify datasets.",
      "processing_time": 71.24633717536926,
      "citing_paper_id": "280011566",
      "cited_paper_id": 211572645
    },
    {
      "context_text": "Additionally, it achieves better color fidelity in the dehazing compared to other methods. restoration, DL [51].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a comparison of methods for dehazing and color fidelity.",
      "processing_time": 70.13636088371277,
      "citing_paper_id": "280011566",
      "cited_paper_id": 195787503
    },
    {
      "context_text": "For instance, Yang et al. [32] use discrete wavelet transforms to facilitate edge feature extraction.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (discrete wavelet transforms) used for edge feature extraction.",
      "processing_time": 70.51332330703735,
      "citing_paper_id": "280011566",
      "cited_paper_id": 202781591
    },
    {
      "context_text": "Wei et al. [30] and Li et al. [15] pioneered this approach by introducing a new method that utilizes contrastive learning to extract degradation representations, thereby guiding the restoration process.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and approaches. The context focuses on the introduction of a new method using contrastive learning for image restoration.",
      "processing_time": 72.37435984611511,
      "citing_paper_id": "280011566",
      "cited_paper_id": 214775210
    },
    {
      "context_text": "Wei et al. [30] and Li et al. [15] pioneered this approach by introducing a new method that utilizes contrastive learning to extract degradation representations, thereby guiding the restoration process.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and approaches. The context focuses on the introduction of a new method using contrastive learning for image restoration.",
      "processing_time": 72.37435984611511,
      "citing_paper_id": "280011566",
      "cited_paper_id": 250551851
    },
    {
      "context_text": "Multi-task methods [13], [14] focus on training a single model to address multiple image restoration tasks simultaneously by incorporating separate modules for each task in parallel at the input and output layers.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods and approaches. No dataset names are present in the text.",
      "processing_time": 70.83936500549316,
      "citing_paper_id": "280011566",
      "cited_paper_id": 219978541
    },
    {
      "context_text": "Multi-task methods [13], [14] focus on training a single model to address multiple image restoration tasks simultaneously by incorporating separate modules for each task in parallel at the input and output layers.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods and approaches. No dataset names are present in the text.",
      "processing_time": 70.83936500549316,
      "citing_paper_id": "280011566",
      "cited_paper_id": 227239228
    },
    {
      "context_text": "Recent studies, e.g. , [13], [14], have tried to handle multiple degradations with a multitask learning framework.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only a general reference to multitask learning frameworks. No dataset names are present in the context.",
      "processing_time": 71.89003252983093,
      "citing_paper_id": "280011566",
      "cited_paper_id": 219978541
    },
    {
      "context_text": "Recent studies, e.g. , [13], [14], have tried to handle multiple degradations with a multitask learning framework.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only a general reference to multitask learning frameworks. No dataset names are present in the context.",
      "processing_time": 71.89003252983093,
      "citing_paper_id": "280011566",
      "cited_paper_id": 227239228
    },
    {
      "context_text": "Recent studies, e.g. , [13], [14], have tried to handle multiple degradations with a multitask learning framework.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only a general reference to multitask learning frameworks. No dataset names are present in the context.",
      "processing_time": 71.89003252983093,
      "citing_paper_id": "270878215",
      "cited_paper_id": 227239228
    },
    {
      "context_text": "Li et al. [14] introduced a task-specific feature extractor to extract common clean features for different adverse weather conditions.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for feature extraction in adverse weather conditions.",
      "processing_time": 69.25768804550171,
      "citing_paper_id": "280011566",
      "cited_paper_id": 219978541
    },
    {
      "context_text": "For example, Chen et al. [13] developed distinct heads and tails for various tasks, with only the backbone being shared among them.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method involving distinct heads and tails for various tasks. No verifiable resources are identified.",
      "processing_time": 71.86990070343018,
      "citing_paper_id": "280011566",
      "cited_paper_id": 227239228
    },
    {
      "context_text": "For example, Chen et al. [13] developed distinct heads and tails for various tasks, with only the backbone being shared among them.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method involving distinct heads and tails for various tasks. No verifiable resources are identified.",
      "processing_time": 71.86990070343018,
      "citing_paper_id": "270878215",
      "cited_paper_id": 227239228
    },
    {
      "context_text": "Dformer constructs a hierarchical encoder network following the architecture of the Swin Transformer [40], as illustrated in Fig.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (Swin Transformer).",
      "processing_time": 69.47128248214722,
      "citing_paper_id": "280011566",
      "cited_paper_id": 232352874
    },
    {
      "context_text": "The architecture of Rformer follows Uformer [23], but employs a new degradation-adaptive self-attention mechanism to adaptively focus on the most affected frequency bands, guided by the acquired degradation representations.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (Uformer) and a modification to it (Rformer).",
      "processing_time": 71.43529534339905,
      "citing_paper_id": "280011566",
      "cited_paper_id": 235358213
    },
    {
      "context_text": "Recent studies [37]–[39] have explored biases in frequency domain modules.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only a general reference to biases in frequency domain modules. No verifiable resources are identified.",
      "processing_time": 71.85329484939575,
      "citing_paper_id": "280011566",
      "cited_paper_id": 246823327
    },
    {
      "context_text": "Recent studies [37]–[39] have explored biases in frequency domain modules.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only a general reference to biases in frequency domain modules. No verifiable resources are identified.",
      "processing_time": 71.85329484939575,
      "citing_paper_id": "280011566",
      "cited_paper_id": 247411040
    },
    {
      "context_text": "In line with Li et al. [15], we employ two widely used metrics for quantitative comparisons: Peak Signal-to-Noise Ratio (PSNR) and Structural Similarity (SSIM).",
      "catation_intent": "none",
      "resource_type": "metric",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions metrics (PSNR and SSIM) but does not reference any specific datasets. The context is focused on evaluation metrics rather than data sources.",
      "processing_time": 72.35434865951538,
      "citing_paper_id": "280011566",
      "cited_paper_id": 250551851
    },
    {
      "context_text": "We are also specifically evaluating two specialized all-in-one methods, AirNet [15] and PromptIR [18].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only methods (AirNet and PromptIR). The context is focused on evaluating these methods for all-in-one image restoration.",
      "processing_time": 72.99928641319275,
      "citing_paper_id": "280011566",
      "cited_paper_id": 250551851
    },
    {
      "context_text": "We are also specifically evaluating two specialized all-in-one methods, AirNet [15] and PromptIR [18].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only methods (AirNet and PromptIR). The context is focused on evaluating these methods for all-in-one image restoration.",
      "processing_time": 72.99928641319275,
      "citing_paper_id": "280011566",
      "cited_paper_id": 268064460
    },
    {
      "context_text": "Li et al. [15] argue that different degradation tasks necessitate different receptive fields within the restoration network.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a discussion about degradation tasks and receptive fields in restoration networks.",
      "processing_time": 70.80000352859497,
      "citing_paper_id": "280011566",
      "cited_paper_id": 250551851
    },
    {
      "context_text": "To avoid these drawbacks, all-in-one image restoration has been studied recently, pioneered by Li et al. [15].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.0,
      "reasoning": "The citation does not mention any specific datasets, only a method or approach. The context is too vague to identify a dataset.",
      "processing_time": 71.41698861122131,
      "citing_paper_id": "280011566",
      "cited_paper_id": 250551851
    },
    {
      "context_text": "All-in-one methods [15], [18], [27]–[29] aim to address a broad spectrum of image restoration tasks with a single, unified model.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only all-in-one methods for image restoration. No verifiable resources are identified.",
      "processing_time": 71.60406231880188,
      "citing_paper_id": "280011566",
      "cited_paper_id": 250551851
    },
    {
      "context_text": "All-in-one methods [15], [18], [27]–[29] aim to address a broad spectrum of image restoration tasks with a single, unified model.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only all-in-one methods for image restoration. No verifiable resources are identified.",
      "processing_time": 71.60406231880188,
      "citing_paper_id": "280011566",
      "cited_paper_id": 268064460
    },
    {
      "context_text": "The training settings are outlined following AirNet [15].",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method called AirNet. No verifiable datasets are referenced.",
      "processing_time": 70.80930137634277,
      "citing_paper_id": "280011566",
      "cited_paper_id": 250551851
    },
    {
      "context_text": "Li et al. [15] suggest obtaining degradation representation using a contrastive learning framework, while Park et al. [17] propose learning a degradation classifier to estimate the type of degradation.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only methods and approaches for image restoration and degradation estimation.",
      "processing_time": 18.2793972492218,
      "citing_paper_id": "280011566",
      "cited_paper_id": 250551851
    },
    {
      "context_text": "Li et al. [15] suggest obtaining degradation representation using a contrastive learning framework, while Park et al. [17] propose learning a degradation classifier to estimate the type of degradation.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only methods and approaches for image restoration and degradation estimation.",
      "processing_time": 18.2793972492218,
      "citing_paper_id": "280011566",
      "cited_paper_id": 260107992
    },
    {
      "context_text": "Following the existing works [15], [18], we assess the effectiveness of the proposed approaches in multi-degradation restoration using seven datasets: BSD400 [41], BSD68 [41], WED [42], and Urban100 [43] for image denoising, Rain100L [44] for image deraining, RESIDE [45] for image dehazing, and…",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "BSD400",
        "BSD68",
        "WED",
        "Urban100",
        "Rain100L",
        "RESIDE"
      ],
      "dataset_descriptions": {
        "BSD400": "Used for image denoising, assessing the effectiveness of the proposed approaches in multi-degradation restoration.",
        "BSD68": "Used for image denoising, assessing the effectiveness of the proposed approaches in multi-degradation restoration.",
        "WED": "Used for image denoising, assessing the effectiveness of the proposed approaches in multi-degradation restoration.",
        "Urban100": "Used for image denoising, assessing the effectiveness of the proposed approaches in multi-degradation restoration.",
        "Rain100L": "Used for image deraining, assessing the effectiveness of the proposed approaches in multi-degradation restoration.",
        "RESIDE": "Used for image dehazing, assessing the effectiveness of the proposed approaches in multi-degradation restoration."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions seven specific datasets used for evaluating image restoration methods across different degradation types. These datasets are clearly identified and used for benchmarking.",
      "processing_time": 105.51136565208435,
      "citing_paper_id": "280011566",
      "cited_paper_id": 250551851
    },
    {
      "context_text": "Following the existing works [15], [18], we assess the effectiveness of the proposed approaches in multi-degradation restoration using seven datasets: BSD400 [41], BSD68 [41], WED [42], and Urban100 [43] for image denoising, Rain100L [44] for image deraining, RESIDE [45] for image dehazing, and…",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "BSD400",
        "BSD68",
        "WED",
        "Urban100",
        "Rain100L",
        "RESIDE"
      ],
      "dataset_descriptions": {
        "BSD400": "Used for image denoising, assessing the effectiveness of the proposed approaches in multi-degradation restoration.",
        "BSD68": "Used for image denoising, assessing the effectiveness of the proposed approaches in multi-degradation restoration.",
        "WED": "Used for image denoising, assessing the effectiveness of the proposed approaches in multi-degradation restoration.",
        "Urban100": "Used for image denoising, assessing the effectiveness of the proposed approaches in multi-degradation restoration.",
        "Rain100L": "Used for image deraining, assessing the effectiveness of the proposed approaches in multi-degradation restoration.",
        "RESIDE": "Used for image dehazing, assessing the effectiveness of the proposed approaches in multi-degradation restoration."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions seven specific datasets used for evaluating image restoration methods across different degradation types. These datasets are clearly identified and used for benchmarking.",
      "processing_time": 105.51136565208435,
      "citing_paper_id": "280011566",
      "cited_paper_id": 268064460
    },
    {
      "context_text": "Specifically, we surpass AirNet [15] across all tasks, achieving an average performance improvement of 1 .",
      "catation_intent": "findings",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a comparison with another method (AirNet).",
      "processing_time": 70.47077775001526,
      "citing_paper_id": "280011566",
      "cited_paper_id": 250551851
    },
    {
      "context_text": "Mao et al. [33] distinguish between blurry and sharp images by processing low-and high-frequency components separately using Fast Fourier Transform.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a method for distinguishing between blurry and sharp images using FFT.",
      "processing_time": 70.79779171943665,
      "citing_paper_id": "280011566",
      "cited_paper_id": 254069870
    },
    {
      "context_text": "Cui et al. [34] propose a selective frequency module that dynamically separates feature maps into distinct frequency components with learnable filters.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for image restoration.",
      "processing_time": 69.44655084609985,
      "citing_paper_id": "280011566",
      "cited_paper_id": 259298517
    },
    {
      "context_text": "Park et al. [17] introduced an adaptive discriminative filter-based model to explicitly disentangle the restoration network for multiple degradations.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a model and its application to image restoration.",
      "processing_time": 70.25327253341675,
      "citing_paper_id": "280011566",
      "cited_paper_id": 260107992
    },
    {
      "context_text": "Compared to AirNet [15] and PromptIR [18], our approach better preserves edge details when performing denoising and deraining, and achieves better color fidelity during dehazing.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only comparisons to other methods (AirNet and PromptIR).",
      "processing_time": 71.03168225288391,
      "citing_paper_id": "280011566",
      "cited_paper_id": 268064460
    },
    {
      "context_text": "Potlapalli et al. [18] proposed a prompt interaction module to enable dynamic interaction between input features and degradation prompts for guided restoration.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method or model called 'PromptIR'. No verifiable resources are identified.",
      "processing_time": 71.8269944190979,
      "citing_paper_id": "280011566",
      "cited_paper_id": 268064460
    },
    {
      "context_text": "Potlapalli et al. [18] utilize prompts to encode degradation-specific information.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (PromptIR) for image restoration.",
      "processing_time": 70.4567482471466,
      "citing_paper_id": "280011566",
      "cited_paper_id": 268064460
    },
    {
      "context_text": "Potlapalli et al. [18] proposed a universal and efficient plugin module that employs adjustable prompts to encode degradation-specific information without prior information on the degradations.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (PromptIR) for image restoration. No dataset names are provided in the context.",
      "processing_time": 72.50623345375061,
      "citing_paper_id": "280011566",
      "cited_paper_id": 268064460
    },
    {
      "context_text": "In this experiment, we conduct a comparative analysis between the proposed method, AirNet [15], and PromptIR [18] across different numbers of degradations to assess the stability of our approach.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods (AirNet and PromptIR). The context focuses on comparing these methods across different numbers of degradations.",
      "processing_time": 73.11516118049622,
      "citing_paper_id": "280011566",
      "cited_paper_id": 268064460
    },
    {
      "context_text": "Furthermore, we outperform PromptIR [18] in denoising and deraining tasks, with an average performance improvement of 0 .",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a comparison with a method called PromptIR. No verifiable resources are identified.",
      "processing_time": 71.81801223754883,
      "citing_paper_id": "280011566",
      "cited_paper_id": 268064460
    },
    {
      "context_text": "These methods include statistical models [9], total variation [10], least squares [11], nonlinear methods [12] and dictionary-based approaches [13].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various methods and approaches for image restoration. No verifiable resources are identified.",
      "processing_time": 72.09664916992188,
      "citing_paper_id": "271379298",
      "cited_paper_id": 968094
    },
    {
      "context_text": "These methods include statistical models [9], total variation [10], least squares [11], nonlinear methods [12] and dictionary-based approaches [13].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various methods and approaches for image restoration. No verifiable resources are identified.",
      "processing_time": 72.09664916992188,
      "citing_paper_id": "271379298",
      "cited_paper_id": 15799108
    },
    {
      "context_text": "These useless noises not only degrade the image quality but also mislead deep learning models [5–8], leading to incorrect decisions and analysis.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only general issues with image degradation and deep learning models. No verifiable resources are identified.",
      "processing_time": 72.08965826034546,
      "citing_paper_id": "271379298",
      "cited_paper_id": 1779661
    },
    {
      "context_text": "These useless noises not only degrade the image quality but also mislead deep learning models [5–8], leading to incorrect decisions and analysis.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only general issues with image degradation and deep learning models. No verifiable resources are identified.",
      "processing_time": 72.08965826034546,
      "citing_paper_id": "271379298",
      "cited_paper_id": 263895657
    },
    {
      "context_text": "The comparative methods on the Rain200L, Rain200H and Rain800 datasets include DSC [33], DiG-CoM [34], DerainCycleGAN [35], SPD-Net [36], NLCL [37], Syn2Real [38], SIRR [39], JRGB [40], DDN [41], Air-Net [42], DID-MDN [43], RESCAN [44], RainDiffusion [45], PreNet [46] and MSPFN [47].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Rain200L",
        "Rain200H",
        "Rain800"
      ],
      "dataset_descriptions": {
        "Rain200L": "Used to evaluate deraining methods, focusing on low-resolution synthetic rain images to assess performance in removing light rain.",
        "Rain200H": "Used to evaluate deraining methods, focusing on high-resolution synthetic rain images to assess performance in removing heavy rain.",
        "Rain800": "Used to evaluate deraining methods, focusing on a larger set of synthetic rain images to assess generalization and robustness."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions specific datasets used for comparing deraining methods. These datasets are relevant to the topic of image restoration, particularly for rain removal.",
      "processing_time": 92.08882212638855,
      "citing_paper_id": "271379298",
      "cited_paper_id": 3406592
    },
    {
      "context_text": "The comparative methods on the Rain200L, Rain200H and Rain800 datasets include DSC [33], DiG-CoM [34], DerainCycleGAN [35], SPD-Net [36], NLCL [37], Syn2Real [38], SIRR [39], JRGB [40], DDN [41], Air-Net [42], DID-MDN [43], RESCAN [44], RainDiffusion [45], PreNet [46] and MSPFN [47].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Rain200L",
        "Rain200H",
        "Rain800"
      ],
      "dataset_descriptions": {
        "Rain200L": "Used to evaluate deraining methods, focusing on low-resolution synthetic rain images to assess performance in removing light rain.",
        "Rain200H": "Used to evaluate deraining methods, focusing on high-resolution synthetic rain images to assess performance in removing heavy rain.",
        "Rain800": "Used to evaluate deraining methods, focusing on a larger set of synthetic rain images to assess generalization and robustness."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions specific datasets used for comparing deraining methods. These datasets are relevant to the topic of image restoration, particularly for rain removal.",
      "processing_time": 92.08882212638855,
      "citing_paper_id": "271379298",
      "cited_paper_id": 17115407
    },
    {
      "context_text": "The comparative methods on the Rain200L, Rain200H and Rain800 datasets include DSC [33], DiG-CoM [34], DerainCycleGAN [35], SPD-Net [36], NLCL [37], Syn2Real [38], SIRR [39], JRGB [40], DDN [41], Air-Net [42], DID-MDN [43], RESCAN [44], RainDiffusion [45], PreNet [46] and MSPFN [47].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Rain200L",
        "Rain200H",
        "Rain800"
      ],
      "dataset_descriptions": {
        "Rain200L": "Used to evaluate deraining methods, focusing on low-resolution synthetic rain images to assess performance in removing light rain.",
        "Rain200H": "Used to evaluate deraining methods, focusing on high-resolution synthetic rain images to assess performance in removing heavy rain.",
        "Rain800": "Used to evaluate deraining methods, focusing on a larger set of synthetic rain images to assess generalization and robustness."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions specific datasets used for comparing deraining methods. These datasets are relevant to the topic of image restoration, particularly for rain removal.",
      "processing_time": 92.08882212638855,
      "citing_paper_id": "271379298",
      "cited_paper_id": 49864080
    },
    {
      "context_text": "The comparative methods on the Rain200L, Rain200H and Rain800 datasets include DSC [33], DiG-CoM [34], DerainCycleGAN [35], SPD-Net [36], NLCL [37], Syn2Real [38], SIRR [39], JRGB [40], DDN [41], Air-Net [42], DID-MDN [43], RESCAN [44], RainDiffusion [45], PreNet [46] and MSPFN [47].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Rain200L",
        "Rain200H",
        "Rain800"
      ],
      "dataset_descriptions": {
        "Rain200L": "Used to evaluate deraining methods, focusing on low-resolution synthetic rain images to assess performance in removing light rain.",
        "Rain200H": "Used to evaluate deraining methods, focusing on high-resolution synthetic rain images to assess performance in removing heavy rain.",
        "Rain800": "Used to evaluate deraining methods, focusing on a larger set of synthetic rain images to assess generalization and robustness."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions specific datasets used for comparing deraining methods. These datasets are relevant to the topic of image restoration, particularly for rain removal.",
      "processing_time": 92.08882212638855,
      "citing_paper_id": "271379298",
      "cited_paper_id": 221119346
    },
    {
      "context_text": "The comparative methods on the Rain200L, Rain200H and Rain800 datasets include DSC [33], DiG-CoM [34], DerainCycleGAN [35], SPD-Net [36], NLCL [37], Syn2Real [38], SIRR [39], JRGB [40], DDN [41], Air-Net [42], DID-MDN [43], RESCAN [44], RainDiffusion [45], PreNet [46] and MSPFN [47].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Rain200L",
        "Rain200H",
        "Rain800"
      ],
      "dataset_descriptions": {
        "Rain200L": "Used to evaluate deraining methods, focusing on low-resolution synthetic rain images to assess performance in removing light rain.",
        "Rain200H": "Used to evaluate deraining methods, focusing on high-resolution synthetic rain images to assess performance in removing heavy rain.",
        "Rain800": "Used to evaluate deraining methods, focusing on a larger set of synthetic rain images to assess generalization and robustness."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions specific datasets used for comparing deraining methods. These datasets are relevant to the topic of image restoration, particularly for rain removal.",
      "processing_time": 92.08882212638855,
      "citing_paper_id": "271379298",
      "cited_paper_id": 245339944
    },
    {
      "context_text": "The comparative methods on the Rain200L, Rain200H and Rain800 datasets include DSC [33], DiG-CoM [34], DerainCycleGAN [35], SPD-Net [36], NLCL [37], Syn2Real [38], SIRR [39], JRGB [40], DDN [41], Air-Net [42], DID-MDN [43], RESCAN [44], RainDiffusion [45], PreNet [46] and MSPFN [47].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Rain200L",
        "Rain200H",
        "Rain800"
      ],
      "dataset_descriptions": {
        "Rain200L": "Used to evaluate deraining methods, focusing on low-resolution synthetic rain images to assess performance in removing light rain.",
        "Rain200H": "Used to evaluate deraining methods, focusing on high-resolution synthetic rain images to assess performance in removing heavy rain.",
        "Rain800": "Used to evaluate deraining methods, focusing on a larger set of synthetic rain images to assess generalization and robustness."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions specific datasets used for comparing deraining methods. These datasets are relevant to the topic of image restoration, particularly for rain removal.",
      "processing_time": 92.08882212638855,
      "citing_paper_id": "271379298",
      "cited_paper_id": 247597149
    },
    {
      "context_text": "The comparative methods on the Rain200L, Rain200H and Rain800 datasets include DSC [33], DiG-CoM [34], DerainCycleGAN [35], SPD-Net [36], NLCL [37], Syn2Real [38], SIRR [39], JRGB [40], DDN [41], Air-Net [42], DID-MDN [43], RESCAN [44], RainDiffusion [45], PreNet [46] and MSPFN [47].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Rain200L",
        "Rain200H",
        "Rain800"
      ],
      "dataset_descriptions": {
        "Rain200L": "Used to evaluate deraining methods, focusing on low-resolution synthetic rain images to assess performance in removing light rain.",
        "Rain200H": "Used to evaluate deraining methods, focusing on high-resolution synthetic rain images to assess performance in removing heavy rain.",
        "Rain800": "Used to evaluate deraining methods, focusing on a larger set of synthetic rain images to assess generalization and robustness."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions specific datasets used for comparing deraining methods. These datasets are relevant to the topic of image restoration, particularly for rain removal.",
      "processing_time": 92.08882212638855,
      "citing_paper_id": "271379298",
      "cited_paper_id": null
    },
    {
      "context_text": "The compared methods include DSC [33], DiG-CoM [34], DerainCycleGAN [35], SPD-Net [36], NLCL [37], Syn2Real [38], SIRR [39], JRGB [40], DDN [41], Air-Net [42], DID-MDN [43], RESCAN [44], RainDiffusion [45], PreNet [46] and MSPFN [47].",
      "catation_intent": "none",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various methods and models used for image restoration tasks. No verifiable resources are identified.",
      "processing_time": 72.48467373847961,
      "citing_paper_id": "271379298",
      "cited_paper_id": 3406592
    },
    {
      "context_text": "The compared methods include DSC [33], DiG-CoM [34], DerainCycleGAN [35], SPD-Net [36], NLCL [37], Syn2Real [38], SIRR [39], JRGB [40], DDN [41], Air-Net [42], DID-MDN [43], RESCAN [44], RainDiffusion [45], PreNet [46] and MSPFN [47].",
      "catation_intent": "none",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various methods and models used for image restoration tasks. No verifiable resources are identified.",
      "processing_time": 72.48467373847961,
      "citing_paper_id": "271379298",
      "cited_paper_id": 17115407
    },
    {
      "context_text": "The compared methods include DSC [33], DiG-CoM [34], DerainCycleGAN [35], SPD-Net [36], NLCL [37], Syn2Real [38], SIRR [39], JRGB [40], DDN [41], Air-Net [42], DID-MDN [43], RESCAN [44], RainDiffusion [45], PreNet [46] and MSPFN [47].",
      "catation_intent": "none",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various methods and models used for image restoration tasks. No verifiable resources are identified.",
      "processing_time": 72.48467373847961,
      "citing_paper_id": "271379298",
      "cited_paper_id": 49864080
    },
    {
      "context_text": "The compared methods include DSC [33], DiG-CoM [34], DerainCycleGAN [35], SPD-Net [36], NLCL [37], Syn2Real [38], SIRR [39], JRGB [40], DDN [41], Air-Net [42], DID-MDN [43], RESCAN [44], RainDiffusion [45], PreNet [46] and MSPFN [47].",
      "catation_intent": "none",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various methods and models used for image restoration tasks. No verifiable resources are identified.",
      "processing_time": 72.48467373847961,
      "citing_paper_id": "271379298",
      "cited_paper_id": 221119346
    },
    {
      "context_text": "The compared methods include DSC [33], DiG-CoM [34], DerainCycleGAN [35], SPD-Net [36], NLCL [37], Syn2Real [38], SIRR [39], JRGB [40], DDN [41], Air-Net [42], DID-MDN [43], RESCAN [44], RainDiffusion [45], PreNet [46] and MSPFN [47].",
      "catation_intent": "none",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various methods and models used for image restoration tasks. No verifiable resources are identified.",
      "processing_time": 72.48467373847961,
      "citing_paper_id": "271379298",
      "cited_paper_id": 245339944
    },
    {
      "context_text": "The compared methods include DSC [33], DiG-CoM [34], DerainCycleGAN [35], SPD-Net [36], NLCL [37], Syn2Real [38], SIRR [39], JRGB [40], DDN [41], Air-Net [42], DID-MDN [43], RESCAN [44], RainDiffusion [45], PreNet [46] and MSPFN [47].",
      "catation_intent": "none",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various methods and models used for image restoration tasks. No verifiable resources are identified.",
      "processing_time": 72.48467373847961,
      "citing_paper_id": "271379298",
      "cited_paper_id": 247597149
    },
    {
      "context_text": "The compared methods include DSC [33], DiG-CoM [34], DerainCycleGAN [35], SPD-Net [36], NLCL [37], Syn2Real [38], SIRR [39], JRGB [40], DDN [41], Air-Net [42], DID-MDN [43], RESCAN [44], RainDiffusion [45], PreNet [46] and MSPFN [47].",
      "catation_intent": "none",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various methods and models used for image restoration tasks. No verifiable resources are identified.",
      "processing_time": 72.48467373847961,
      "citing_paper_id": "271379298",
      "cited_paper_id": null
    },
    {
      "context_text": "Zheng et al. [21] proposed a joint framework capable of simultaneous image denoising and restoration, enabling multi-task learning through a shared encoder-decoder structure.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for image denoising and restoration.",
      "processing_time": 70.05715680122375,
      "citing_paper_id": "271379298",
      "cited_paper_id": 4072789
    },
    {
      "context_text": "The Generative Image Inpainting with Contextual Attention model proposed by Yu et al. [21] uses the self-attention mechanism to generate more natural restoration results by learning the contextual relationship of images.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a model and its methodology.",
      "processing_time": 70.04609799385071,
      "citing_paper_id": "271379298",
      "cited_paper_id": 4072789
    },
    {
      "context_text": "Fu et al. [17] proposed a network with fewer parameters, shallow depth and simple structure by combining convolutional neural networks with classic pyramid techniques.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method or network architecture.",
      "processing_time": 70.43313956260681,
      "citing_paper_id": "271379298",
      "cited_paper_id": 21712570
    },
    {
      "context_text": "The comparative methods on the RSID dataset include Cycle-SNSPGAN [63], ZID [64], FCTF-Net [65], FFA-Net [66], TCN [67], EVPM [68], IDeRs [69], GRS-HTM [70], SDCP [71], UHD [72], DeHHamer [73], Dehaze-cGAN [74], STD [75], Zero-restore [76] and ROP [77].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "RSID"
      ],
      "dataset_descriptions": {
        "RSID": "Used to compare various image restoration methods, specifically focusing on dehazing and restoration techniques for remote sensing images."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions the RSID dataset, which is used for comparing various image restoration methods. The dataset is relevant to the topic of all-in-one image restoration.",
      "processing_time": 79.2849748134613,
      "citing_paper_id": "271379298",
      "cited_paper_id": 40504781
    },
    {
      "context_text": "The comparative methods on the RSID dataset include Cycle-SNSPGAN [63], ZID [64], FCTF-Net [65], FFA-Net [66], TCN [67], EVPM [68], IDeRs [69], GRS-HTM [70], SDCP [71], UHD [72], DeHHamer [73], Dehaze-cGAN [74], STD [75], Zero-restore [76] and ROP [77].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "RSID"
      ],
      "dataset_descriptions": {
        "RSID": "Used to compare various image restoration methods, specifically focusing on dehazing and restoration techniques for remote sensing images."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions the RSID dataset, which is used for comparing various image restoration methods. The dataset is relevant to the topic of all-in-one image restoration.",
      "processing_time": 79.2849748134613,
      "citing_paper_id": "271379298",
      "cited_paper_id": 115196811
    },
    {
      "context_text": "The comparative methods on the RSID dataset include Cycle-SNSPGAN [63], ZID [64], FCTF-Net [65], FFA-Net [66], TCN [67], EVPM [68], IDeRs [69], GRS-HTM [70], SDCP [71], UHD [72], DeHHamer [73], Dehaze-cGAN [74], STD [75], Zero-restore [76] and ROP [77].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "RSID"
      ],
      "dataset_descriptions": {
        "RSID": "Used to compare various image restoration methods, specifically focusing on dehazing and restoration techniques for remote sensing images."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions the RSID dataset, which is used for comparing various image restoration methods. The dataset is relevant to the topic of all-in-one image restoration.",
      "processing_time": 79.2849748134613,
      "citing_paper_id": "271379298",
      "cited_paper_id": 235691711
    },
    {
      "context_text": "The comparative methods on the RSID dataset include Cycle-SNSPGAN [63], ZID [64], FCTF-Net [65], FFA-Net [66], TCN [67], EVPM [68], IDeRs [69], GRS-HTM [70], SDCP [71], UHD [72], DeHHamer [73], Dehaze-cGAN [74], STD [75], Zero-restore [76] and ROP [77].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "RSID"
      ],
      "dataset_descriptions": {
        "RSID": "Used to compare various image restoration methods, specifically focusing on dehazing and restoration techniques for remote sensing images."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions the RSID dataset, which is used for comparing various image restoration methods. The dataset is relevant to the topic of all-in-one image restoration.",
      "processing_time": 79.2849748134613,
      "citing_paper_id": "271379298",
      "cited_paper_id": 248634766
    },
    {
      "context_text": "The comparative methods on the RSID dataset include Cycle-SNSPGAN [63], ZID [64], FCTF-Net [65], FFA-Net [66], TCN [67], EVPM [68], IDeRs [69], GRS-HTM [70], SDCP [71], UHD [72], DeHHamer [73], Dehaze-cGAN [74], STD [75], Zero-restore [76] and ROP [77].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "RSID"
      ],
      "dataset_descriptions": {
        "RSID": "Used to compare various image restoration methods, specifically focusing on dehazing and restoration techniques for remote sensing images."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions the RSID dataset, which is used for comparing various image restoration methods. The dataset is relevant to the topic of all-in-one image restoration.",
      "processing_time": 79.2849748134613,
      "citing_paper_id": "271379298",
      "cited_paper_id": 254179587
    },
    {
      "context_text": "The compared methods include Cycle-SNSPGAN [63], ZID [64], FCTF-Net [65], FFA-Net [66], TCN [67], EVPM [68], IDeRs [69], GRS-HTM [70], SDCP [71], UHD [72], DeHHamer [73], Dehaze-cGAN [74], STD [75], Zero-restore [76] and ROP [77].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various methods and models used for image restoration. No verifiable resources are identified.",
      "processing_time": 16.17776894569397,
      "citing_paper_id": "271379298",
      "cited_paper_id": 40504781
    },
    {
      "context_text": "The compared methods include Cycle-SNSPGAN [63], ZID [64], FCTF-Net [65], FFA-Net [66], TCN [67], EVPM [68], IDeRs [69], GRS-HTM [70], SDCP [71], UHD [72], DeHHamer [73], Dehaze-cGAN [74], STD [75], Zero-restore [76] and ROP [77].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various methods and models used for image restoration. No verifiable resources are identified.",
      "processing_time": 16.17776894569397,
      "citing_paper_id": "271379298",
      "cited_paper_id": 115196811
    },
    {
      "context_text": "The compared methods include Cycle-SNSPGAN [63], ZID [64], FCTF-Net [65], FFA-Net [66], TCN [67], EVPM [68], IDeRs [69], GRS-HTM [70], SDCP [71], UHD [72], DeHHamer [73], Dehaze-cGAN [74], STD [75], Zero-restore [76] and ROP [77].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various methods and models used for image restoration. No verifiable resources are identified.",
      "processing_time": 16.17776894569397,
      "citing_paper_id": "271379298",
      "cited_paper_id": 235691711
    },
    {
      "context_text": "The compared methods include Cycle-SNSPGAN [63], ZID [64], FCTF-Net [65], FFA-Net [66], TCN [67], EVPM [68], IDeRs [69], GRS-HTM [70], SDCP [71], UHD [72], DeHHamer [73], Dehaze-cGAN [74], STD [75], Zero-restore [76] and ROP [77].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various methods and models used for image restoration. No verifiable resources are identified.",
      "processing_time": 16.17776894569397,
      "citing_paper_id": "271379298",
      "cited_paper_id": 248634766
    },
    {
      "context_text": "The compared methods include Cycle-SNSPGAN [63], ZID [64], FCTF-Net [65], FFA-Net [66], TCN [67], EVPM [68], IDeRs [69], GRS-HTM [70], SDCP [71], UHD [72], DeHHamer [73], Dehaze-cGAN [74], STD [75], Zero-restore [76] and ROP [77].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various methods and models used for image restoration. No verifiable resources are identified.",
      "processing_time": 16.17776894569397,
      "citing_paper_id": "271379298",
      "cited_paper_id": 254179587
    },
    {
      "context_text": "…[64] 18.992 0.727 FCTF-Net [65] 19.306 0.856 FFA-Net [66] 24.052 0.899 TCN [67] 14.208 0.606 EVPM [68] 15.579 0.689 IDeRs [69] 13.604 0.644 GRS-HTM [70] 14.800 0.519 SDCP [71] 16.055 0.691 UHD [72] 26.659 0.923 DeHamer [73] 23.752 0.899 Dehaze-cGAN [74] 18.703 0.743 STD [75] 16.258 0.559…",
      "catation_intent": "findings",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various methods and their performance metrics. No dataset names are present in the text.",
      "processing_time": 17.817153215408325,
      "citing_paper_id": "271379298",
      "cited_paper_id": 40504781
    },
    {
      "context_text": "The comparative methods on the Snow100K and CSD datasets include CycleGAN [48], RESCAN [44], DesnowNet [29], ALL in one [49], JSTASR [50], HDCW-Net [30], DDMSNet [51], MPRNet [52], TransWeather [53], SMGARN [54], TKL [55], Results.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Snow100K",
        "CSD"
      ],
      "dataset_descriptions": {
        "Snow100K": "Used to compare various image desnowing methods, focusing on performance metrics and visual quality improvements.",
        "CSD": "Used to evaluate the effectiveness of different image restoration techniques, particularly in removing snow from images."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions two specific datasets, Snow100K and CSD, which are used for comparing various image restoration methods. These datasets are relevant to the topic of all-in-one image restoration.",
      "processing_time": 30.586849451065063,
      "citing_paper_id": "271379298",
      "cited_paper_id": 49864080
    },
    {
      "context_text": "The comparative methods on the Snow100K and CSD datasets include CycleGAN [48], RESCAN [44], DesnowNet [29], ALL in one [49], JSTASR [50], HDCW-Net [30], DDMSNet [51], MPRNet [52], TransWeather [53], SMGARN [54], TKL [55], Results.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Snow100K",
        "CSD"
      ],
      "dataset_descriptions": {
        "Snow100K": "Used to compare various image desnowing methods, focusing on performance metrics and visual quality improvements.",
        "CSD": "Used to evaluate the effectiveness of different image restoration techniques, particularly in removing snow from images."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions two specific datasets, Snow100K and CSD, which are used for comparing various image restoration methods. These datasets are relevant to the topic of all-in-one image restoration.",
      "processing_time": 30.586849451065063,
      "citing_paper_id": "271379298",
      "cited_paper_id": 226308542
    },
    {
      "context_text": "The comparative methods on the Snow100K and CSD datasets include CycleGAN [48], RESCAN [44], DesnowNet [29], ALL in one [49], JSTASR [50], HDCW-Net [30], DDMSNet [51], MPRNet [52], TransWeather [53], SMGARN [54], TKL [55], Results.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Snow100K",
        "CSD"
      ],
      "dataset_descriptions": {
        "Snow100K": "Used to compare various image desnowing methods, focusing on performance metrics and visual quality improvements.",
        "CSD": "Used to evaluate the effectiveness of different image restoration techniques, particularly in removing snow from images."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions two specific datasets, Snow100K and CSD, which are used for comparing various image restoration methods. These datasets are relevant to the topic of all-in-one image restoration.",
      "processing_time": 30.586849451065063,
      "citing_paper_id": "271379298",
      "cited_paper_id": 231802205
    },
    {
      "context_text": "The comparative methods on the Snow100K and CSD datasets include CycleGAN [48], RESCAN [44], DesnowNet [29], ALL in one [49], JSTASR [50], HDCW-Net [30], DDMSNet [51], MPRNet [52], TransWeather [53], SMGARN [54], TKL [55], Results.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Snow100K",
        "CSD"
      ],
      "dataset_descriptions": {
        "Snow100K": "Used to compare various image desnowing methods, focusing on performance metrics and visual quality improvements.",
        "CSD": "Used to evaluate the effectiveness of different image restoration techniques, particularly in removing snow from images."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions two specific datasets, Snow100K and CSD, which are used for comparing various image restoration methods. These datasets are relevant to the topic of all-in-one image restoration.",
      "processing_time": 30.586849451065063,
      "citing_paper_id": "271379298",
      "cited_paper_id": 232306906
    },
    {
      "context_text": "The comparative methods on the Snow100K and CSD datasets include CycleGAN [48], RESCAN [44], DesnowNet [29], ALL in one [49], JSTASR [50], HDCW-Net [30], DDMSNet [51], MPRNet [52], TransWeather [53], SMGARN [54], TKL [55], Results.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Snow100K",
        "CSD"
      ],
      "dataset_descriptions": {
        "Snow100K": "Used to compare various image desnowing methods, focusing on performance metrics and visual quality improvements.",
        "CSD": "Used to evaluate the effectiveness of different image restoration techniques, particularly in removing snow from images."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions two specific datasets, Snow100K and CSD, which are used for comparing various image restoration methods. These datasets are relevant to the topic of all-in-one image restoration.",
      "processing_time": 30.586849451065063,
      "citing_paper_id": "271379298",
      "cited_paper_id": 244461440
    },
    {
      "context_text": "The comparative methods on the Snow100K and CSD datasets include CycleGAN [48], RESCAN [44], DesnowNet [29], ALL in one [49], JSTASR [50], HDCW-Net [30], DDMSNet [51], MPRNet [52], TransWeather [53], SMGARN [54], TKL [55], Results.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Snow100K",
        "CSD"
      ],
      "dataset_descriptions": {
        "Snow100K": "Used to compare various image desnowing methods, focusing on performance metrics and visual quality improvements.",
        "CSD": "Used to evaluate the effectiveness of different image restoration techniques, particularly in removing snow from images."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions two specific datasets, Snow100K and CSD, which are used for comparing various image restoration methods. These datasets are relevant to the topic of all-in-one image restoration.",
      "processing_time": 30.586849451065063,
      "citing_paper_id": "271379298",
      "cited_paper_id": 250426326
    },
    {
      "context_text": "The compared methods include CycleGAN [48], RESCAN [44], DesnowNet [29], ALL in one [49], JSTASR [50], HDCW-Net [30], DDMSNet [51], MPRNet [52], TransWeather [53], SMGARN [54], TKL [55], WeatherDiff128 [56], MSP-Former [57], Uformer [58], Weath-erDiff64 [56], Restormer [59], SnowDiff128 [56],…",
      "catation_intent": "none",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various methods and models used for image restoration tasks. No verifiable datasets are referenced.",
      "processing_time": 18.865947246551514,
      "citing_paper_id": "271379298",
      "cited_paper_id": 49864080
    },
    {
      "context_text": "The compared methods include CycleGAN [48], RESCAN [44], DesnowNet [29], ALL in one [49], JSTASR [50], HDCW-Net [30], DDMSNet [51], MPRNet [52], TransWeather [53], SMGARN [54], TKL [55], WeatherDiff128 [56], MSP-Former [57], Uformer [58], Weath-erDiff64 [56], Restormer [59], SnowDiff128 [56],…",
      "catation_intent": "none",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various methods and models used for image restoration tasks. No verifiable datasets are referenced.",
      "processing_time": 18.865947246551514,
      "citing_paper_id": "271379298",
      "cited_paper_id": 226308542
    },
    {
      "context_text": "The compared methods include CycleGAN [48], RESCAN [44], DesnowNet [29], ALL in one [49], JSTASR [50], HDCW-Net [30], DDMSNet [51], MPRNet [52], TransWeather [53], SMGARN [54], TKL [55], WeatherDiff128 [56], MSP-Former [57], Uformer [58], Weath-erDiff64 [56], Restormer [59], SnowDiff128 [56],…",
      "catation_intent": "none",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various methods and models used for image restoration tasks. No verifiable datasets are referenced.",
      "processing_time": 18.865947246551514,
      "citing_paper_id": "271379298",
      "cited_paper_id": 231802205
    },
    {
      "context_text": "The compared methods include CycleGAN [48], RESCAN [44], DesnowNet [29], ALL in one [49], JSTASR [50], HDCW-Net [30], DDMSNet [51], MPRNet [52], TransWeather [53], SMGARN [54], TKL [55], WeatherDiff128 [56], MSP-Former [57], Uformer [58], Weath-erDiff64 [56], Restormer [59], SnowDiff128 [56],…",
      "catation_intent": "none",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various methods and models used for image restoration tasks. No verifiable datasets are referenced.",
      "processing_time": 18.865947246551514,
      "citing_paper_id": "271379298",
      "cited_paper_id": 232306906
    },
    {
      "context_text": "The compared methods include CycleGAN [48], RESCAN [44], DesnowNet [29], ALL in one [49], JSTASR [50], HDCW-Net [30], DDMSNet [51], MPRNet [52], TransWeather [53], SMGARN [54], TKL [55], WeatherDiff128 [56], MSP-Former [57], Uformer [58], Weath-erDiff64 [56], Restormer [59], SnowDiff128 [56],…",
      "catation_intent": "none",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various methods and models used for image restoration tasks. No verifiable datasets are referenced.",
      "processing_time": 18.865947246551514,
      "citing_paper_id": "271379298",
      "cited_paper_id": 244461440
    },
    {
      "context_text": "To explore the advantages of ELAU, we separately added the ELAU, Convolutional Block Attention Module (CBAM) [88] and Squeeze-and-Excitation (SE) [89] module to our network and then compared their combination with the Spare Transformer (ST) [90].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models and methods. The context focuses on comparing different modules within a network.",
      "processing_time": 18.219112634658813,
      "citing_paper_id": "271379298",
      "cited_paper_id": 49867180
    },
    {
      "context_text": "On this basis, Wang et al. [22] introduced a multi-scale and cross-attention mechanism to further improve the quality and detail fidelity of image-restoration.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for improving image restoration.",
      "processing_time": 28.652498722076416,
      "citing_paper_id": "271379298",
      "cited_paper_id": 53047249
    },
    {
      "context_text": "The comparative methods on the EUVP dataset include PRWNet [78], ShallowUW [79], UWCNN [80], FUnIE-GAN Shal-Electronics lowUW [32], UT-UIE [81], WaterNet [82], RAUNE-Net [83], CPDM [84], SyreaNet [85], SGUIE-Net [86] and Cycle-GAN [87].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "EUVP"
      ],
      "dataset_descriptions": {
        "EUVP": "Used to compare various underwater image enhancement methods, focusing on visual perception improvement and evaluating the performance of different models."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions the EUVP dataset, which is used for comparing various underwater image enhancement methods. The dataset is specific and relevant to the research topic of image restoration.",
      "processing_time": 25.140972137451172,
      "citing_paper_id": "271379298",
      "cited_paper_id": 85498726
    },
    {
      "context_text": "The comparative methods on the EUVP dataset include PRWNet [78], ShallowUW [79], UWCNN [80], FUnIE-GAN Shal-Electronics lowUW [32], UT-UIE [81], WaterNet [82], RAUNE-Net [83], CPDM [84], SyreaNet [85], SGUIE-Net [86] and Cycle-GAN [87].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "EUVP"
      ],
      "dataset_descriptions": {
        "EUVP": "Used to compare various underwater image enhancement methods, focusing on visual perception improvement and evaluating the performance of different models."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions the EUVP dataset, which is used for comparing various underwater image enhancement methods. The dataset is specific and relevant to the research topic of image restoration.",
      "processing_time": 25.140972137451172,
      "citing_paper_id": "271379298",
      "cited_paper_id": 230770098
    },
    {
      "context_text": "The comparative methods on the EUVP dataset include PRWNet [78], ShallowUW [79], UWCNN [80], FUnIE-GAN Shal-Electronics lowUW [32], UT-UIE [81], WaterNet [82], RAUNE-Net [83], CPDM [84], SyreaNet [85], SGUIE-Net [86] and Cycle-GAN [87].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "EUVP"
      ],
      "dataset_descriptions": {
        "EUVP": "Used to compare various underwater image enhancement methods, focusing on visual perception improvement and evaluating the performance of different models."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions the EUVP dataset, which is used for comparing various underwater image enhancement methods. The dataset is specific and relevant to the research topic of image restoration.",
      "processing_time": 25.140972137451172,
      "citing_paper_id": "271379298",
      "cited_paper_id": 244488578
    },
    {
      "context_text": "The comparative methods on the EUVP dataset include PRWNet [78], ShallowUW [79], UWCNN [80], FUnIE-GAN Shal-Electronics lowUW [32], UT-UIE [81], WaterNet [82], RAUNE-Net [83], CPDM [84], SyreaNet [85], SGUIE-Net [86] and Cycle-GAN [87].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "EUVP"
      ],
      "dataset_descriptions": {
        "EUVP": "Used to compare various underwater image enhancement methods, focusing on visual perception improvement and evaluating the performance of different models."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions the EUVP dataset, which is used for comparing various underwater image enhancement methods. The dataset is specific and relevant to the research topic of image restoration.",
      "processing_time": 25.140972137451172,
      "citing_paper_id": "271379298",
      "cited_paper_id": 256900743
    },
    {
      "context_text": "The comparative methods on the EUVP dataset include PRWNet [78], ShallowUW [79], UWCNN [80], FUnIE-GAN Shal-Electronics lowUW [32], UT-UIE [81], WaterNet [82], RAUNE-Net [83], CPDM [84], SyreaNet [85], SGUIE-Net [86] and Cycle-GAN [87].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "EUVP"
      ],
      "dataset_descriptions": {
        "EUVP": "Used to compare various underwater image enhancement methods, focusing on visual perception improvement and evaluating the performance of different models."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions the EUVP dataset, which is used for comparing various underwater image enhancement methods. The dataset is specific and relevant to the research topic of image restoration.",
      "processing_time": 25.140972137451172,
      "citing_paper_id": "271379298",
      "cited_paper_id": 264833084
    },
    {
      "context_text": "The comparative methods on the EUVP dataset include PRWNet [78], ShallowUW [79], UWCNN [80], FUnIE-GAN Shal-Electronics lowUW [32], UT-UIE [81], WaterNet [82], RAUNE-Net [83], CPDM [84], SyreaNet [85], SGUIE-Net [86] and Cycle-GAN [87].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "EUVP"
      ],
      "dataset_descriptions": {
        "EUVP": "Used to compare various underwater image enhancement methods, focusing on visual perception improvement and evaluating the performance of different models."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions the EUVP dataset, which is used for comparing various underwater image enhancement methods. The dataset is specific and relevant to the research topic of image restoration.",
      "processing_time": 25.140972137451172,
      "citing_paper_id": "271379298",
      "cited_paper_id": 267786640
    },
    {
      "context_text": "For the underwater enhancement task, our testing dataset comprised 515 pairs of test samples from the EUVP [32] dataset.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "EUVP"
      ],
      "dataset_descriptions": {
        "EUVP": "Used for testing underwater image enhancement, specifically evaluating visual perception improvements on 515 pairs of test samples."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions a specific dataset, EUVP, which is used for testing underwater image enhancement. The dataset is clearly identified and its purpose is explicitly stated.",
      "processing_time": 25.138408660888672,
      "citing_paper_id": "271379298",
      "cited_paper_id": 85498726
    },
    {
      "context_text": "For the underwater enhancement task, we conducted experiments using the EUVP [32] public dataset.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "EUVP"
      ],
      "dataset_descriptions": {
        "EUVP": "Used for conducting experiments on underwater image enhancement, focusing on improving visual perception through specific image processing techniques."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions the use of the EUVP dataset for underwater image enhancement experiments, which is directly relevant to the research topic of all-in-one image restoration.",
      "processing_time": 24.5562584400177,
      "citing_paper_id": "271379298",
      "cited_paper_id": 85498726
    },
    {
      "context_text": "The comparison methods included PRWNet [78], ShallowUW [79], UWCNN [80], FUnIE-GAN [32], UT-UIE [81], WaterNet [82], RAUNE-Net [83], CPDM [84], SyreaNet [85], SGUIE-Net [86] and Cycle-GAN [87].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span mentions several models and methods but does not reference any specific datasets. The context is focused on comparing different underwater image enhancement techniques.",
      "processing_time": 20.430161952972412,
      "citing_paper_id": "271379298",
      "cited_paper_id": 85498726
    },
    {
      "context_text": "The comparison methods included PRWNet [78], ShallowUW [79], UWCNN [80], FUnIE-GAN [32], UT-UIE [81], WaterNet [82], RAUNE-Net [83], CPDM [84], SyreaNet [85], SGUIE-Net [86] and Cycle-GAN [87].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span mentions several models and methods but does not reference any specific datasets. The context is focused on comparing different underwater image enhancement techniques.",
      "processing_time": 20.430161952972412,
      "citing_paper_id": "271379298",
      "cited_paper_id": 230770098
    },
    {
      "context_text": "The comparison methods included PRWNet [78], ShallowUW [79], UWCNN [80], FUnIE-GAN [32], UT-UIE [81], WaterNet [82], RAUNE-Net [83], CPDM [84], SyreaNet [85], SGUIE-Net [86] and Cycle-GAN [87].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span mentions several models and methods but does not reference any specific datasets. The context is focused on comparing different underwater image enhancement techniques.",
      "processing_time": 20.430161952972412,
      "citing_paper_id": "271379298",
      "cited_paper_id": 244488578
    },
    {
      "context_text": "The comparison methods included PRWNet [78], ShallowUW [79], UWCNN [80], FUnIE-GAN [32], UT-UIE [81], WaterNet [82], RAUNE-Net [83], CPDM [84], SyreaNet [85], SGUIE-Net [86] and Cycle-GAN [87].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span mentions several models and methods but does not reference any specific datasets. The context is focused on comparing different underwater image enhancement techniques.",
      "processing_time": 20.430161952972412,
      "citing_paper_id": "271379298",
      "cited_paper_id": 256900743
    },
    {
      "context_text": "The comparison methods included PRWNet [78], ShallowUW [79], UWCNN [80], FUnIE-GAN [32], UT-UIE [81], WaterNet [82], RAUNE-Net [83], CPDM [84], SyreaNet [85], SGUIE-Net [86] and Cycle-GAN [87].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span mentions several models and methods but does not reference any specific datasets. The context is focused on comparing different underwater image enhancement techniques.",
      "processing_time": 20.430161952972412,
      "citing_paper_id": "271379298",
      "cited_paper_id": 264833084
    },
    {
      "context_text": "The comparison methods included PRWNet [78], ShallowUW [79], UWCNN [80], FUnIE-GAN [32], UT-UIE [81], WaterNet [82], RAUNE-Net [83], CPDM [84], SyreaNet [85], SGUIE-Net [86] and Cycle-GAN [87].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span mentions several models and methods but does not reference any specific datasets. The context is focused on comparing different underwater image enhancement techniques.",
      "processing_time": 20.430161952972412,
      "citing_paper_id": "271379298",
      "cited_paper_id": 267786640
    },
    {
      "context_text": "…[74] 18.703 0.743 STD [75] 16.258 0.559 Zero-restore [76] 16.648 0.717 ROP [77] 15.575 0.750 24.551 0.852 UWCNN [80] 17.725 0.704 FunIE-GAN [32] 24.077 0.794 UT-UIE [81] 25.214 0.813 Water-Net [82] 25.285 0.833 RAUNE-Net [83] 26.331 0.845 CPDM [84] 23.243 0.901 SyreaNet [85] 17.721 0.743…",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only model names and performance metrics. No dataset names are present as required by the rules.",
      "processing_time": 19.119234800338745,
      "citing_paper_id": "271379298",
      "cited_paper_id": 85498726
    },
    {
      "context_text": "[27] 1800 200 Rain800 [28] 700 100 Snow100K [29] 50,000 50,000 CSD [30] 7000 1000 RSID [31] 900 100 EUVP [32] 11435 515 For the Rain200H dataset, the first 1800 pairs of a total of 2000 pairs (degraded and clean) were used as the training dataset, and the remaining 200 pairs of images were used as…",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Rain800",
        "Snow100K",
        "CSD",
        "RSID",
        "EUVP",
        "Rain200H"
      ],
      "dataset_descriptions": {
        "Rain800": "Used for training and evaluating rain removal algorithms, providing pairs of degraded and clean images.",
        "Snow100K": "Used for training and evaluating snow removal algorithms, providing pairs of degraded and clean images.",
        "CSD": "Used for training and evaluating image restoration algorithms, providing a large dataset of degraded and clean images.",
        "RSID": "Used for training and evaluating image restoration algorithms, providing pairs of degraded and clean images.",
        "EUVP": "Used for training and evaluating underwater image enhancement algorithms, providing pairs of degraded and clean images.",
        "Rain200H": "Used for training and evaluating rain removal algorithms, with 1800 pairs for training and 200 pairs for testing."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions several datasets used for training and evaluation in image restoration tasks, particularly for rain and snow removal.",
      "processing_time": 51.07739591598511,
      "citing_paper_id": "271379298",
      "cited_paper_id": 85498726
    },
    {
      "context_text": "[27] 1800 200 Rain800 [28] 700 100 Snow100K [29] 50,000 50,000 CSD [30] 7000 1000 RSID [31] 900 100 EUVP [32] 11435 515 For the Rain200H dataset, the first 1800 pairs of a total of 2000 pairs (degraded and clean) were used as the training dataset, and the remaining 200 pairs of images were used as…",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Rain800",
        "Snow100K",
        "CSD",
        "RSID",
        "EUVP",
        "Rain200H"
      ],
      "dataset_descriptions": {
        "Rain800": "Used for training and evaluating rain removal algorithms, providing pairs of degraded and clean images.",
        "Snow100K": "Used for training and evaluating snow removal algorithms, providing pairs of degraded and clean images.",
        "CSD": "Used for training and evaluating image restoration algorithms, providing a large dataset of degraded and clean images.",
        "RSID": "Used for training and evaluating image restoration algorithms, providing pairs of degraded and clean images.",
        "EUVP": "Used for training and evaluating underwater image enhancement algorithms, providing pairs of degraded and clean images.",
        "Rain200H": "Used for training and evaluating rain removal algorithms, with 1800 pairs for training and 200 pairs for testing."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions several datasets used for training and evaluation in image restoration tasks, particularly for rain and snow removal.",
      "processing_time": 51.07739591598511,
      "citing_paper_id": "271379298",
      "cited_paper_id": 244461440
    },
    {
      "context_text": "…[63] 18.344 0.729 ZID [64] 18.992 0.727 FCTF-Net [65] 19.306 0.856 FFA-Net [66] 24.052 0.899 TCN [67] 14.208 0.606 EVPM [68] 15.579 0.689 IDeRs [69] 13.604 0.644 GRS-HTM [70] 14.800 0.519 SDCP [71] 16.055 0.691 UHD [72] 26.659 0.923 DeHamer [73] 23.752 0.899 Dehaze-cGAN [74] 18.703 0.743 STD…",
      "catation_intent": "findings",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various methods and their performance metrics. No dataset names are present in the text.",
      "processing_time": 19.99835705757141,
      "citing_paper_id": "271379298",
      "cited_paper_id": 115196811
    },
    {
      "context_text": "These noises can significantly reduce the clarity and quality of the images, thereby affecting the accuracy of downstream tasks such as object detection [1], image classification [2], stereo matching [3] and image recognition [4].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only general tasks affected by image noise. No dataset names are present in the text.",
      "processing_time": 20.23344087600708,
      "citing_paper_id": "271379298",
      "cited_paper_id": 131776850
    },
    {
      "context_text": "These noises can significantly reduce the clarity and quality of the images, thereby affecting the accuracy of downstream tasks such as object detection [1], image classification [2], stereo matching [3] and image recognition [4].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only general tasks affected by image noise. No dataset names are present in the text.",
      "processing_time": 20.23344087600708,
      "citing_paper_id": "271379298",
      "cited_paper_id": 266244375
    },
    {
      "context_text": "Mou et al. [19] proposed a novel network architecture that utilizes windowed attention to mimic the selective focusing mechanism of the human eye.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (network architecture).",
      "processing_time": 14.452442646026611,
      "citing_paper_id": "271379298",
      "cited_paper_id": 232170566
    },
    {
      "context_text": "…0.519 SDCP [71] 16.055 0.691 UHD [72] 26.659 0.923 DeHamer [73] 23.752 0.899 Dehaze-cGAN [74] 18.703 0.743 STD [75] 16.258 0.559 Zero-restore [76] 16.648 0.717 ROP [77] 15.575 0.750 24.551 0.852 UWCNN [80] 17.725 0.704 FunIE-GAN [32] 24.077 0.794 UT-UIE [81] 25.214 0.813 Water-Net [82]…",
      "catation_intent": "findings",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various methods and their performance metrics. No dataset names are present in the text.",
      "processing_time": 19.758848905563354,
      "citing_paper_id": "271379298",
      "cited_paper_id": 235691711
    },
    {
      "context_text": "…[29], ALL in one [49], JSTASR [50], HDCW-Net [30], DDMSNet [51], MPRNet [52], TransWeather [53], SMGARN [54], TKL [55], WeatherDiff128 [56], MSP-Former [57], Uformer [58], Weath-erDiff64 [56], Restormer [59], SnowDiff128 [56], NAFNet [60], DGUNet [61], SnowD-iff64 [56] and GridFormer-S [62].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions several models and methods, but does not explicitly refer to any specific datasets. The cited paper titles do not provide additional context to identify datasets.",
      "processing_time": 28.323421955108643,
      "citing_paper_id": "271379298",
      "cited_paper_id": 244346144
    },
    {
      "context_text": "…[29], ALL in one [49], JSTASR [50], HDCW-Net [30], DDMSNet [51], MPRNet [52], TransWeather [53], SMGARN [54], TKL [55], WeatherDiff128 [56], MSP-Former [57], Uformer [58], Weath-erDiff64 [56], Restormer [59], SnowDiff128 [56], NAFNet [60], DGUNet [61], SnowD-iff64 [56] and GridFormer-S [62].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions several models and methods, but does not explicitly refer to any specific datasets. The cited paper titles do not provide additional context to identify datasets.",
      "processing_time": 28.323421955108643,
      "citing_paper_id": "271379298",
      "cited_paper_id": 251197000
    },
    {
      "context_text": "In the snow-restoral task phase, we used three public datasets: Snow100K-S [29], Snow100K-L [29] and CSD [30].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Snow100K-S",
        "Snow100K-L",
        "CSD"
      ],
      "dataset_descriptions": {
        "Snow100K-S": "Used for evaluating single image desnowing algorithms, focusing on smaller-scale images to test algorithm performance under various snow conditions.",
        "Snow100K-L": "Used for evaluating single image desnowing algorithms, focusing on larger-scale images to test algorithm robustness and efficiency.",
        "CSD": "Used for comprehensive evaluation of single image desnowing algorithms, providing a diverse set of images with varying snow intensities and scenes."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions three specific datasets used for the snow-restoral task phase, which are relevant to the topic of image restoration.",
      "processing_time": 37.69109344482422,
      "citing_paper_id": "271379298",
      "cited_paper_id": 244461440
    },
    {
      "context_text": "For the desnowing task, we conducted experiments using two public datasets: Snow100K-L [29] and CSD [30].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Snow100K-L",
        "CSD"
      ],
      "dataset_descriptions": {
        "Snow100K-L": "Used for evaluating desnowing algorithms, focusing on synthetic snow removal from images using hierarchical dual-tree complex wavelet representation.",
        "CSD": "Used for testing desnowing performance, specifically assessing the effectiveness of the contradict channel loss in single image desnowing."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions two specific datasets used for the desnowing task, which are relevant to the topic of image restoration.",
      "processing_time": 27.78166913986206,
      "citing_paper_id": "271379298",
      "cited_paper_id": 244461440
    },
    {
      "context_text": "…STD [75] 16.258 0.559 Zero-restore [76] 16.648 0.717 ROP [77] 15.575 0.750 24.551 0.852 UWCNN [80] 17.725 0.704 FunIE-GAN [32] 24.077 0.794 UT-UIE [81] 25.214 0.813 Water-Net [82] 25.285 0.833 RAUNE-Net [83] 26.331 0.845 CPDM [84] 23.243 0.901 SyreaNet [85] 17.721 0.743 SGUIE-Net [86] 19.187…",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only model names and performance metrics. There are no clear identifiers for datasets.",
      "processing_time": 44.40470051765442,
      "citing_paper_id": "271379298",
      "cited_paper_id": 244488578
    },
    {
      "context_text": "Cycle-SNSPGAN [63] 18.344 0.729 ZID [64] 18.992 0.727 FCTF-Net [65] 19.306 0.856 FFA-Net [66] 24.052 0.899 TCN [67] 14.208 0.606 EVPM [68] 15.579 0.689 IDeRs [69] 13.604 0.644 GRS-HTM [70] 14.800 0.519 SDCP [71] 16.055 0.691 UHD [72] 26.659 0.923 DeHamer [73] 23.752 0.899 Dehaze-cGAN [74] 18.703…",
      "catation_intent": "findings",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only model names and their performance metrics. No verifiable resources are identified.",
      "processing_time": 14.437256336212158,
      "citing_paper_id": "271379298",
      "cited_paper_id": 248634766
    },
    {
      "context_text": "…CycleGAN [48], RESCAN [44], DesnowNet [29], ALL in one [49], JSTASR [50], HDCW-Net [30], DDMSNet [51], MPRNet [52], TransWeather [53], SMGARN [54], TKL [55], WeatherDiff128 [56], MSP-Former [57], Uformer [58], Weath-erDiff64 [56], Restormer [59], SnowDiff128 [56], NAFNet [60], DGUNet [61],…",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span mentions several models and methods but does not reference any specific datasets. No dataset names are present in the context.",
      "processing_time": 19.374199628829956,
      "citing_paper_id": "271379298",
      "cited_paper_id": 250426326
    },
    {
      "context_text": "…DesnowNet [29], ALL in one [49], JSTASR [50], HDCW-Net [30], DDMSNet [51], MPRNet [52], TransWeather [53], SMGARN [54], TKL [55], WeatherDiff128 [56], MSP-Former [57], Uformer [58], Weath-erDiff64 [56], Restormer [59], SnowDiff128 [56], NAFNet [60], DGUNet [61], SnowD-iff64 [56] and…",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only models and methods. There are no clear identifiers for datasets within the text.",
      "processing_time": 30.745757341384888,
      "citing_paper_id": "271379298",
      "cited_paper_id": 251197000
    },
    {
      "context_text": "…16.055 0.691 UHD [72] 26.659 0.923 DeHamer [73] 23.752 0.899 Dehaze-cGAN [74] 18.703 0.743 STD [75] 16.258 0.559 Zero-restore [76] 16.648 0.717 ROP [77] 15.575 0.750 24.551 0.852 UWCNN [80] 17.725 0.704 FunIE-GAN [32] 24.077 0.794 UT-UIE [81] 25.214 0.813 Water-Net [82] 25.285 0.833 RAUNE-Net [83]…",
      "catation_intent": "findings",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various methods and their performance metrics. No dataset names are present in the text.",
      "processing_time": 30.736417055130005,
      "citing_paper_id": "271379298",
      "cited_paper_id": 254179587
    },
    {
      "context_text": "…0.717 ROP [77] 15.575 0.750 24.551 0.852 UWCNN [80] 17.725 0.704 FunIE-GAN [32] 24.077 0.794 UT-UIE [81] 25.214 0.813 Water-Net [82] 25.285 0.833 RAUNE-Net [83] 26.331 0.845 CPDM [84] 23.243 0.901 SyreaNet [85] 17.721 0.743 SGUIE-Net [86] 19.187 0.760 Cycle-GAN [87] 17.963 0.709 PerNet 25.592 0.913",
      "catation_intent": "findings",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only model names and their performance metrics. No verifiable resources are identified.",
      "processing_time": 18.818355798721313,
      "citing_paper_id": "271379298",
      "cited_paper_id": 256900743
    },
    {
      "context_text": "…0.717 ROP [77] 15.575 0.750 24.551 0.852 UWCNN [80] 17.725 0.704 FunIE-GAN [32] 24.077 0.794 UT-UIE [81] 25.214 0.813 Water-Net [82] 25.285 0.833 RAUNE-Net [83] 26.331 0.845 CPDM [84] 23.243 0.901 SyreaNet [85] 17.721 0.743 SGUIE-Net [86] 19.187 0.760 Cycle-GAN [87] 17.963 0.709 PerNet 25.592 0.913",
      "catation_intent": "findings",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only model names and their performance metrics. No verifiable resources are identified.",
      "processing_time": 18.818355798721313,
      "citing_paper_id": "271379298",
      "cited_paper_id": 264833084
    },
    {
      "context_text": "Sen et al. [20] introduced two sub-networks with integrated loss functions for the first time.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only the introduction of sub-networks and loss functions.",
      "processing_time": 15.076255083084106,
      "citing_paper_id": "271379298",
      "cited_paper_id": 260436809
    },
    {
      "context_text": "Mei et al. [24] proposed a reference-based task-adaptive degradation modeling method.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for image restoration.",
      "processing_time": 15.3714919090271,
      "citing_paper_id": "271379298",
      "cited_paper_id": 260748050
    },
    {
      "context_text": "…[73] 23.752 0.899 Dehaze-cGAN [74] 18.703 0.743 STD [75] 16.258 0.559 Zero-restore [76] 16.648 0.717 ROP [77] 15.575 0.750 24.551 0.852 UWCNN [80] 17.725 0.704 FunIE-GAN [32] 24.077 0.794 UT-UIE [81] 25.214 0.813 Water-Net [82] 25.285 0.833 RAUNE-Net [83] 26.331 0.845 CPDM [84] 23.243 0.901…",
      "catation_intent": "findings",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various methods and their performance metrics. No dataset names are present in the text.",
      "processing_time": 17.308358669281006,
      "citing_paper_id": "271379298",
      "cited_paper_id": 267786640
    },
    {
      "context_text": "Chen et al. [25] introduced Neural Degradation Representation to represent the latent features and statistical characteristics of various degradations.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method called Neural Degradation Representation. No verifiable resources are identified.",
      "processing_time": 15.070557355880737,
      "citing_paper_id": "271379298",
      "cited_paper_id": 268838239
    },
    {
      "context_text": "The inherently ill-posed nature of IR presents a significant challenge for conventional approaches limiting their effectiveness [12, 15, 18, 21, 22, 34, 38, 47].",
      "catation_intent": "research work",
      "resource_type": "limitation",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only challenges and limitations of conventional approaches in image restoration.",
      "processing_time": 27.056377172470093,
      "citing_paper_id": "274306415",
      "cited_paper_id": 715896
    },
    {
      "context_text": "The inherently ill-posed nature of IR presents a significant challenge for conventional approaches limiting their effectiveness [12, 15, 18, 21, 22, 34, 38, 47].",
      "catation_intent": "research work",
      "resource_type": "limitation",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only challenges and limitations of conventional approaches in image restoration.",
      "processing_time": 27.056377172470093,
      "citing_paper_id": "274306415",
      "cited_paper_id": 7044126
    },
    {
      "context_text": "The inherently ill-posed nature of IR presents a significant challenge for conventional approaches limiting their effectiveness [12, 15, 18, 21, 22, 34, 38, 47].",
      "catation_intent": "research work",
      "resource_type": "limitation",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only challenges and limitations of conventional approaches in image restoration.",
      "processing_time": 27.056377172470093,
      "citing_paper_id": "274306415",
      "cited_paper_id": 14925285
    },
    {
      "context_text": "The inherently ill-posed nature of IR presents a significant challenge for conventional approaches limiting their effectiveness [12, 15, 18, 21, 22, 34, 38, 47].",
      "catation_intent": "research work",
      "resource_type": "limitation",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only challenges and limitations of conventional approaches in image restoration.",
      "processing_time": 27.056377172470093,
      "citing_paper_id": "274306415",
      "cited_paper_id": 49333383
    },
    {
      "context_text": "The inherently ill-posed nature of IR presents a significant challenge for conventional approaches limiting their effectiveness [12, 15, 18, 21, 22, 34, 38, 47].",
      "catation_intent": "research work",
      "resource_type": "limitation",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only challenges and limitations of conventional approaches in image restoration.",
      "processing_time": 27.056377172470093,
      "citing_paper_id": "274306415",
      "cited_paper_id": 206769988
    },
    {
      "context_text": "Specifically, we use Rain200L[56] for deraining, RESIDE [25] for dehazing, BSD400 [33] and WED [32] for denoising with σ = 25 , GoPro [35] for deblurring, and LOL [52] for low-light enhancement.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Rain200L",
        "RESIDE",
        "BSD400",
        "WED",
        "GoPro",
        "LOL"
      ],
      "dataset_descriptions": {
        "Rain200L": "Used for deraining, focusing on removing rain streaks from images using a deep learning approach.",
        "RESIDE": "Used for dehazing, evaluating the effectiveness of dehazing algorithms on synthetic and real-world hazy images.",
        "BSD400": "Used for denoising, specifically with a noise level of σ = 25, to assess the performance of denoising techniques.",
        "WED": "Used for denoising, complementing BSD400 with a different set of images and noise characteristics.",
        "GoPro": "Used for deblurring, testing the ability to restore sharpness in motion-blurred images.",
        "LOL": "Used for low-light enhancement, enhancing the visibility of images captured in low-light conditions."
      },
      "confidence_score": 1.0,
      "reasoning": "The citation span mentions several datasets used for various image restoration tasks, which align with the research topic of all-in-one image restoration.",
      "processing_time": 64.31217885017395,
      "citing_paper_id": "274306415",
      "cited_paper_id": 4840263
    },
    {
      "context_text": "Specifically, we use Rain200L[56] for deraining, RESIDE [25] for dehazing, BSD400 [33] and WED [32] for denoising with σ = 25 , GoPro [35] for deblurring, and LOL [52] for low-light enhancement.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Rain200L",
        "RESIDE",
        "BSD400",
        "WED",
        "GoPro",
        "LOL"
      ],
      "dataset_descriptions": {
        "Rain200L": "Used for deraining, focusing on removing rain streaks from images using a deep learning approach.",
        "RESIDE": "Used for dehazing, evaluating the effectiveness of dehazing algorithms on synthetic and real-world hazy images.",
        "BSD400": "Used for denoising, specifically with a noise level of σ = 25, to assess the performance of denoising techniques.",
        "WED": "Used for denoising, complementing BSD400 with a different set of images and noise characteristics.",
        "GoPro": "Used for deblurring, testing the ability to restore sharpness in motion-blurred images.",
        "LOL": "Used for low-light enhancement, enhancing the visibility of images captured in low-light conditions."
      },
      "confidence_score": 1.0,
      "reasoning": "The citation span mentions several datasets used for various image restoration tasks, which align with the research topic of all-in-one image restoration.",
      "processing_time": 64.31217885017395,
      "citing_paper_id": "274306415",
      "cited_paper_id": 8671030
    },
    {
      "context_text": "Specifically, we use Rain200L[56] for deraining, RESIDE [25] for dehazing, BSD400 [33] and WED [32] for denoising with σ = 25 , GoPro [35] for deblurring, and LOL [52] for low-light enhancement.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Rain200L",
        "RESIDE",
        "BSD400",
        "WED",
        "GoPro",
        "LOL"
      ],
      "dataset_descriptions": {
        "Rain200L": "Used for deraining, focusing on removing rain streaks from images using a deep learning approach.",
        "RESIDE": "Used for dehazing, evaluating the effectiveness of dehazing algorithms on synthetic and real-world hazy images.",
        "BSD400": "Used for denoising, specifically with a noise level of σ = 25, to assess the performance of denoising techniques.",
        "WED": "Used for denoising, complementing BSD400 with a different set of images and noise characteristics.",
        "GoPro": "Used for deblurring, testing the ability to restore sharpness in motion-blurred images.",
        "LOL": "Used for low-light enhancement, enhancing the visibility of images captured in low-light conditions."
      },
      "confidence_score": 1.0,
      "reasoning": "The citation span mentions several datasets used for various image restoration tasks, which align with the research topic of all-in-one image restoration.",
      "processing_time": 64.31217885017395,
      "citing_paper_id": "274306415",
      "cited_paper_id": 39760169
    },
    {
      "context_text": "Specifically, we use Rain200L[56] for deraining, RESIDE [25] for dehazing, BSD400 [33] and WED [32] for denoising with σ = 25 , GoPro [35] for deblurring, and LOL [52] for low-light enhancement.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Rain200L",
        "RESIDE",
        "BSD400",
        "WED",
        "GoPro",
        "LOL"
      ],
      "dataset_descriptions": {
        "Rain200L": "Used for deraining, focusing on removing rain streaks from images using a deep learning approach.",
        "RESIDE": "Used for dehazing, evaluating the effectiveness of dehazing algorithms on synthetic and real-world hazy images.",
        "BSD400": "Used for denoising, specifically with a noise level of σ = 25, to assess the performance of denoising techniques.",
        "WED": "Used for denoising, complementing BSD400 with a different set of images and noise characteristics.",
        "GoPro": "Used for deblurring, testing the ability to restore sharpness in motion-blurred images.",
        "LOL": "Used for low-light enhancement, enhancing the visibility of images captured in low-light conditions."
      },
      "confidence_score": 1.0,
      "reasoning": "The citation span mentions several datasets used for various image restoration tasks, which align with the research topic of all-in-one image restoration.",
      "processing_time": 64.31217885017395,
      "citing_paper_id": "274306415",
      "cited_paper_id": 52008443
    },
    {
      "context_text": "These features are then processed through a four-level encoder-decoder structure composed of transformer blocks, with pixel unshuffling and shuffling [44] used for downsampling and upsampling, respectively.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for image and video super-resolution.",
      "processing_time": 26.25985836982727,
      "citing_paper_id": "274306415",
      "cited_paper_id": 7037846
    },
    {
      "context_text": "R a i n [ 56 ] H aze [ 26 ] N o i s e [ 35 ] B l u r [ 35 ] L o w -",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions 'Rain', 'Haze', 'Noise', and 'Blur' as types of image degradations, but does not specify any named datasets. The context suggests these are categories of degradation rather than specific datasets.",
      "processing_time": 22.35053062438965,
      "citing_paper_id": "274306415",
      "cited_paper_id": 8671030
    },
    {
      "context_text": "For evaluation, we employ Rain100L [56], SOTS-Outdoor [25], BSD68 [33], Go-Pro [35], and LOL [52].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Rain100L",
        "SOTS-Outdoor",
        "BSD68",
        "Go-Pro",
        "LOL"
      ],
      "dataset_descriptions": {
        "Rain100L": "Used to evaluate image deraining methods, focusing on synthetic rain streaks and real-world rain images.",
        "SOTS-Outdoor": "Used to assess single-image dehazing performance, emphasizing outdoor scenes with varying atmospheric conditions.",
        "BSD68": "Used to test image denoising algorithms, featuring a diverse set of natural images with added Gaussian noise.",
        "Go-Pro": "Used to evaluate dynamic scene deblurring techniques, containing high-speed video sequences and corresponding sharp reference frames.",
        "LOL": "Used to assess low-light image enhancement methods, including pairs of underexposed and well-exposed images."
      },
      "confidence_score": 1.0,
      "reasoning": "The citation mentions specific datasets used for evaluation in the context of image restoration tasks, which align with the research topic.",
      "processing_time": 46.46461820602417,
      "citing_paper_id": "274306415",
      "cited_paper_id": 8671030
    },
    {
      "context_text": "For evaluation, we employ Rain100L [56], SOTS-Outdoor [25], BSD68 [33], Go-Pro [35], and LOL [52].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Rain100L",
        "SOTS-Outdoor",
        "BSD68",
        "Go-Pro",
        "LOL"
      ],
      "dataset_descriptions": {
        "Rain100L": "Used to evaluate image deraining methods, focusing on synthetic rain streaks and real-world rain images.",
        "SOTS-Outdoor": "Used to assess single-image dehazing performance, emphasizing outdoor scenes with varying atmospheric conditions.",
        "BSD68": "Used to test image denoising algorithms, featuring a diverse set of natural images with added Gaussian noise.",
        "Go-Pro": "Used to evaluate dynamic scene deblurring techniques, containing high-speed video sequences and corresponding sharp reference frames.",
        "LOL": "Used to assess low-light image enhancement methods, including pairs of underexposed and well-exposed images."
      },
      "confidence_score": 1.0,
      "reasoning": "The citation mentions specific datasets used for evaluation in the context of image restoration tasks, which align with the research topic.",
      "processing_time": 46.46461820602417,
      "citing_paper_id": "274306415",
      "cited_paper_id": 39760169
    },
    {
      "context_text": "For evaluation, we employ Rain100L [56], SOTS-Outdoor [25], BSD68 [33], Go-Pro [35], and LOL [52].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Rain100L",
        "SOTS-Outdoor",
        "BSD68",
        "Go-Pro",
        "LOL"
      ],
      "dataset_descriptions": {
        "Rain100L": "Used to evaluate image deraining methods, focusing on synthetic rain streaks and real-world rain images.",
        "SOTS-Outdoor": "Used to assess single-image dehazing performance, emphasizing outdoor scenes with varying atmospheric conditions.",
        "BSD68": "Used to test image denoising algorithms, featuring a diverse set of natural images with added Gaussian noise.",
        "Go-Pro": "Used to evaluate dynamic scene deblurring techniques, containing high-speed video sequences and corresponding sharp reference frames.",
        "LOL": "Used to assess low-light image enhancement methods, including pairs of underexposed and well-exposed images."
      },
      "confidence_score": 1.0,
      "reasoning": "The citation mentions specific datasets used for evaluation in the context of image restoration tasks, which align with the research topic.",
      "processing_time": 46.46461820602417,
      "citing_paper_id": "274306415",
      "cited_paper_id": 52008443
    },
    {
      "context_text": "To evaluate our approach, we evaluate it on three mixed-degradation scenarios: blur combined with noise σ =25 using the Go-Pro dataset[35], blur with JPEG artifacts using the REDS dataset [36], and haze combined with snow using the SRRS dataset [6].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Go-Pro dataset",
        "REDS dataset",
        "SRRS dataset"
      ],
      "dataset_descriptions": {
        "Go-Pro dataset": "Used to evaluate the approach on blur combined with noise σ=25, focusing on dynamic scene deblurring.",
        "REDS dataset": "Used to evaluate the approach on blur with JPEG artifacts, assessing performance in video restoration.",
        "SRRS dataset": "Used to evaluate the approach on haze combined with snow, testing robustness in complex weather conditions."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions three specific datasets used for evaluating the approach in different mixed-degradation scenarios.",
      "processing_time": 33.88553333282471,
      "citing_paper_id": "274306415",
      "cited_paper_id": 8671030
    },
    {
      "context_text": "The first row depicts an image with blur and noise from the GoPRO dataset [35], while the second row shows an image with haze and snow from the SRRS dataset [6].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "GoPRO",
        "SRRS"
      ],
      "dataset_descriptions": {
        "GoPRO": "Used to depict images with blur and noise, focusing on dynamic scene deblurring using a multi-scale convolutional neural network.",
        "SRRS": "Used to show images with haze and snow, demonstrating the effectiveness of the restoration method on various image degradations."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions two specific datasets, GoPRO and SRRS, which are used to depict images with specific degradations for the purpose of image restoration.",
      "processing_time": 32.57228875160217,
      "citing_paper_id": "274306415",
      "cited_paper_id": 8671030
    },
    {
      "context_text": "02] , rain length l : [25 , 35] , rain angle θ : Haze: As with rainy images, capturing paired degraded and clean images under hazy conditions is nearly impossible due to the variability of environmental factors.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only general challenges in capturing paired degraded and clean images under hazy conditions.",
      "processing_time": 17.269469022750854,
      "citing_paper_id": "274306415",
      "cited_paper_id": 8671030
    },
    {
      "context_text": "02] , rain length l : [25 , 35] , rain angle θ : Haze: As with rainy images, capturing paired degraded and clean images under hazy conditions is nearly impossible due to the variability of environmental factors.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only general challenges in capturing paired degraded and clean images under hazy conditions.",
      "processing_time": 17.269469022750854,
      "citing_paper_id": "274306415",
      "cited_paper_id": 39760169
    },
    {
      "context_text": "Qualitative results for single degradation removal, including deblurring on the GoPro [35] dataset, denoising on the LoLv1 [52] dataset, and deraining on the Rain100H [56] dataset.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "GoPro",
        "LoLv1",
        "Rain100H"
      ],
      "dataset_descriptions": {
        "GoPro": "Used for deblurring evaluation, focusing on dynamic scene deblurring with a multi-scale convolutional neural network.",
        "LoLv1": "Used for denoising evaluation, focusing on low-light enhancement using deep retinex decomposition.",
        "Rain100H": "Used for deraining evaluation, focusing on removing rain streaks from images."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions three specific datasets used for evaluating image restoration tasks: GoPro for deblurring, LoLv1 for denoising, and Rain100H for deraining.",
      "processing_time": 35.97673678398132,
      "citing_paper_id": "274306415",
      "cited_paper_id": 8671030
    },
    {
      "context_text": "Qualitative results for single degradation removal, including deblurring on the GoPro [35] dataset, denoising on the LoLv1 [52] dataset, and deraining on the Rain100H [56] dataset.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "GoPro",
        "LoLv1",
        "Rain100H"
      ],
      "dataset_descriptions": {
        "GoPro": "Used for deblurring evaluation, focusing on dynamic scene deblurring with a multi-scale convolutional neural network.",
        "LoLv1": "Used for denoising evaluation, focusing on low-light enhancement using deep retinex decomposition.",
        "Rain100H": "Used for deraining evaluation, focusing on removing rain streaks from images."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions three specific datasets used for evaluating image restoration tasks: GoPro for deblurring, LoLv1 for denoising, and Rain100H for deraining.",
      "processing_time": 35.97673678398132,
      "citing_paper_id": "274306415",
      "cited_paper_id": 52008443
    },
    {
      "context_text": "Mathematically, The kernel size ( d ) is selected from odd values within the range [9 , 35] , while the angle ( Θ ) specifies the orientation of the blur in degrees, ranging from [0 , 360] .",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only parameters for a deblurring method.",
      "processing_time": 12.99072527885437,
      "citing_paper_id": "274306415",
      "cited_paper_id": 8671030
    },
    {
      "context_text": "G o P r o [ 35 ] L o L v1 [ 52 ] R a i n100 H [ 56 ] Input Restormer [63] PromptIR [40] Ours-OH Ground truth Figure 4.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Rain100H"
      ],
      "dataset_descriptions": {
        "Rain100H": "Used to evaluate rain removal performance, focusing on synthetic rain images with varying intensities and complexities."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions 'Rain100H' which is a known dataset for image restoration, particularly for rain removal. The other items mentioned are methods or models, not datasets.",
      "processing_time": 24.755314588546753,
      "citing_paper_id": "274306415",
      "cited_paper_id": 8671030
    },
    {
      "context_text": "G o P r o [ 35 ] L o L v1 [ 52 ] R a i n100 H [ 56 ] Input Restormer [63] PromptIR [40] Ours-OH Ground truth Figure 4.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Rain100H"
      ],
      "dataset_descriptions": {
        "Rain100H": "Used to evaluate rain removal performance, focusing on synthetic rain images with varying intensities and complexities."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions 'Rain100H' which is a known dataset for image restoration, particularly for rain removal. The other items mentioned are methods or models, not datasets.",
      "processing_time": 24.755314588546753,
      "citing_paper_id": "274306415",
      "cited_paper_id": 52008443
    },
    {
      "context_text": "G o P r o [ 35 ] L o L v1 [ 52 ] R a i n100 H [ 56 ] Input Restormer [63] PromptIR [40] Ours-OH Ground truth Figure 4.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Rain100H"
      ],
      "dataset_descriptions": {
        "Rain100H": "Used to evaluate rain removal performance, focusing on synthetic rain images with varying intensities and complexities."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions 'Rain100H' which is a known dataset for image restoration, particularly for rain removal. The other items mentioned are methods or models, not datasets.",
      "processing_time": 24.755314588546753,
      "citing_paper_id": "274306415",
      "cited_paper_id": 268064460
    },
    {
      "context_text": "Inspired by prior works [3, 16, 26], we model haze degradation us-ing the Kochsmieder model [17], which describes how the visibility of distant objects diminishes, vanishing into the horizon as a function of their distance from the observer.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only models and methods. The cited papers' titles do not provide additional context to identify a dataset.",
      "processing_time": 18.49806261062622,
      "citing_paper_id": "274306415",
      "cited_paper_id": 9335112
    },
    {
      "context_text": "Inspired by prior works [3, 16, 26], we model haze degradation us-ing the Kochsmieder model [17], which describes how the visibility of distant objects diminishes, vanishing into the horizon as a function of their distance from the observer.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only models and methods. The cited papers' titles do not provide additional context to identify a dataset.",
      "processing_time": 18.49806261062622,
      "citing_paper_id": "274306415",
      "cited_paper_id": 43854597
    },
    {
      "context_text": "Inspired by prior works [3, 16, 26], we model haze degradation us-ing the Kochsmieder model [17], which describes how the visibility of distant objects diminishes, vanishing into the horizon as a function of their distance from the observer.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only models and methods. The cited papers' titles do not provide additional context to identify a dataset.",
      "processing_time": 18.49806261062622,
      "citing_paper_id": "274306415",
      "cited_paper_id": null
    },
    {
      "context_text": "To ensure that the inputs to our pipeline are high quality, we filter out the images with a resolution smaller than 400 pixels on any side and exclude images with a NIMA score [46] below the 4.90 threshold, retaining only high-quality natural images.",
      "catation_intent": "reusable resource",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions filtering images based on resolution and NIMA score, but does not specify a dataset name. NIMA is a method, not a dataset.",
      "processing_time": 18.781254529953003,
      "citing_paper_id": "274306415",
      "cited_paper_id": 13911460
    },
    {
      "context_text": "L i gh t [ 52 ] R a i n100 H [ 56 ] L o L v2 -",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Rain100H",
        "LoLv2"
      ],
      "dataset_descriptions": {
        "Rain100H": "Used to evaluate rain removal algorithms, focusing on synthetic rain images with varying intensities and complexities.",
        "LoLv2": "Used to assess low-light image enhancement methods, containing pairs of low-light and normal-light images for training and evaluation."
      },
      "confidence_score": 0.9,
      "reasoning": "The citation mentions 'Rain100H' and 'LoLv2', which are known datasets for image restoration tasks. However, 'Light' is not a recognized dataset and seems to be part of the citation format or a typo.",
      "processing_time": 46.204769134521484,
      "citing_paper_id": "274306415",
      "cited_paper_id": 52008443
    },
    {
      "context_text": "In phase III, we apply cross-entropy loss between the regressor’s output and the input image’s degradation type.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only a methodological detail about applying cross-entropy loss. The cited papers' titles do not provide additional context to identify specific datasets.",
      "processing_time": 20.57942223548889,
      "citing_paper_id": "274306415",
      "cited_paper_id": 195350771
    },
    {
      "context_text": "In phase III, we apply cross-entropy loss between the regressor’s output and the input image’s degradation type.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only a methodological detail about applying cross-entropy loss. The cited papers' titles do not provide additional context to identify specific datasets.",
      "processing_time": 20.57942223548889,
      "citing_paper_id": "274306415",
      "cited_paper_id": 264289165
    },
    {
      "context_text": "Specifically, we utilize 450K images from the Google Landmarks dataset [53] — those with NIMA score higher than 4.90 and with short-edge resolution larger than 400, providing a diverse set of real scenes.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Google Landmarks dataset"
      ],
      "dataset_descriptions": {
        "Google Landmarks dataset": "Used to train and evaluate image restoration models, focusing on high-quality images with NIMA scores > 4.90 and short-edge resolution > 400 pixels, providing a diverse set of real scenes."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions the Google Landmarks dataset, which is a specific, verifiable dataset used for image restoration tasks. The dataset is filtered based on NIMA scores and resolution.",
      "processing_time": 28.04453682899475,
      "citing_paper_id": "274306415",
      "cited_paper_id": 195350771
    },
    {
      "context_text": "Specifically, we utilize 450K images from the Google Landmarks dataset [53] — those with NIMA score higher than 4.90 and with short-edge resolution larger than 400, providing a diverse set of real scenes.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Google Landmarks dataset"
      ],
      "dataset_descriptions": {
        "Google Landmarks dataset": "Used to train and evaluate image restoration models, focusing on high-quality images with NIMA scores > 4.90 and short-edge resolution > 400 pixels, providing a diverse set of real scenes."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions the Google Landmarks dataset, which is a specific, verifiable dataset used for image restoration tasks. The dataset is filtered based on NIMA scores and resolution.",
      "processing_time": 28.04453682899475,
      "citing_paper_id": "274306415",
      "cited_paper_id": 214802288
    },
    {
      "context_text": "In what follows, we detail the generation process for each type of synthetic degradation.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a general process for generating synthetic degradations.",
      "processing_time": 15.312162160873413,
      "citing_paper_id": "274306415",
      "cited_paper_id": 195350771
    },
    {
      "context_text": "Image restoration (IR) is a fundamental task in computer vision, essential for enhancing visual quality and optimizing the performance of downstream tasks [37, 45].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only the general task of image restoration. No dataset names are provided in the context.",
      "processing_time": 19.03766393661499,
      "citing_paper_id": "274306415",
      "cited_paper_id": 204187430
    },
    {
      "context_text": "Image restoration (IR) is a fundamental task in computer vision, essential for enhancing visual quality and optimizing the performance of downstream tasks [37, 45].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only the general task of image restoration. No dataset names are provided in the context.",
      "processing_time": 19.03766393661499,
      "citing_paper_id": "274306415",
      "cited_paper_id": 258509666
    },
    {
      "context_text": "We use the Google Landmarks dataset (GLD) [53], a large-scale collection of five million images of diverse landmarks worldwide.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Google Landmarks dataset (GLD)"
      ],
      "dataset_descriptions": {
        "Google Landmarks dataset (GLD)": "Used as a large-scale benchmark for image restoration, focusing on diverse landmarks worldwide. The dataset contains five million images, providing extensive variability for training and evaluation."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions the Google Landmarks dataset (GLD) as a large-scale collection of images used for the research. The dataset is clearly identified and relevant to the topic of image restoration.",
      "processing_time": 27.66178297996521,
      "citing_paper_id": "274306415",
      "cited_paper_id": 214802288
    },
    {
      "context_text": "We use the Google Landmarks dataset (GLD) [53], a large-scale collection of five million images of diverse landmarks worldwide.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Google Landmarks dataset (GLD)"
      ],
      "dataset_descriptions": {
        "Google Landmarks dataset (GLD)": "Used as a large-scale benchmark for image restoration, focusing on diverse landmarks worldwide. The dataset contains five million images, providing extensive variability for training and evaluation."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions the Google Landmarks dataset (GLD) as a large-scale collection of images used for the research. The dataset is clearly identified and relevant to the topic of image restoration.",
      "processing_time": 27.66178297996521,
      "citing_paper_id": "274306415",
      "cited_paper_id": 270440448
    },
    {
      "context_text": "These degradations often emerge through a complex interplay of external environmental conditions and camera limitations during the image acquisition process such as adverse weather conditions [28, 48, 49], noise [13], blur [41] and low-light environments [4, 68].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions various degradations but does not specify any datasets. The cited papers' titles suggest methods for image restoration under specific conditions but do not mention datasets.",
      "processing_time": 17.187974452972412,
      "citing_paper_id": "274306415",
      "cited_paper_id": 244714491
    },
    {
      "context_text": "These degradations often emerge through a complex interplay of external environmental conditions and camera limitations during the image acquisition process such as adverse weather conditions [28, 48, 49], noise [13], blur [41] and low-light environments [4, 68].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions various degradations but does not specify any datasets. The cited papers' titles suggest methods for image restoration under specific conditions but do not mention datasets.",
      "processing_time": 17.187974452972412,
      "citing_paper_id": "274306415",
      "cited_paper_id": 258741296
    },
    {
      "context_text": "Multi-degradation meth-ods [5, 39, 60, 62, 63] proposed unified model architectures that are effective on multiple IR tasks.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods and approaches. No dataset names are present in the text.",
      "processing_time": 15.571877241134644,
      "citing_paper_id": "274306415",
      "cited_paper_id": 248085491
    },
    {
      "context_text": "Multi-degradation meth-ods [5, 39, 60, 62, 63] proposed unified model architectures that are effective on multiple IR tasks.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods and approaches. No dataset names are present in the text.",
      "processing_time": 15.571877241134644,
      "citing_paper_id": "274306415",
      "cited_paper_id": 265658932
    },
    {
      "context_text": "Multi-degradation meth-ods [5, 39, 60, 62, 63] proposed unified model architectures that are effective on multiple IR tasks.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods and approaches. No dataset names are present in the text.",
      "processing_time": 15.571877241134644,
      "citing_paper_id": "274306415",
      "cited_paper_id": 267320695
    },
    {
      "context_text": "Multi-degradation meth-ods [5, 39, 60, 62, 63] proposed unified model architectures that are effective on multiple IR tasks.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods and approaches. No dataset names are present in the text.",
      "processing_time": 15.571877241134644,
      "citing_paper_id": "274306415",
      "cited_paper_id": 268364340
    },
    {
      "context_text": "Multi-degradation meth-ods [5, 39, 60, 62, 63] proposed unified model architectures that are effective on multiple IR tasks.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods and approaches. No dataset names are present in the text.",
      "processing_time": 15.571877241134644,
      "citing_paper_id": "274306415",
      "cited_paper_id": 269605449
    },
    {
      "context_text": "Typical single degradation IR tasks include denoising [13], deblurring [41], deraining [7], dehazing [59], low-light enhancement [4, 68], etc .",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various image restoration tasks. No dataset names are provided.",
      "processing_time": 13.828447341918945,
      "citing_paper_id": "274306415",
      "cited_paper_id": 257636509
    },
    {
      "context_text": "Typical single degradation IR tasks include denoising [13], deblurring [41], deraining [7], dehazing [59], low-light enhancement [4, 68], etc .",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various image restoration tasks. No dataset names are provided.",
      "processing_time": 13.828447341918945,
      "citing_paper_id": "274306415",
      "cited_paper_id": 258741296
    },
    {
      "context_text": "Additionally, existing all-in-one approaches require all degradation types to be predefined during training, limiting the addition of new distortion to already trained models.",
      "catation_intent": "limitation",
      "resource_type": "limitation",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a limitation of existing all-in-one approaches in image restoration.",
      "processing_time": 29.341237545013428,
      "citing_paper_id": "274306415",
      "cited_paper_id": 259144110
    },
    {
      "context_text": "PromptIR [40], for instance, integrates a dedicated prompt block to capture degradation-specific features from input images, while DA-CLIP [30], MPerceiver [2], ProRes [31] and Painter [50] leverage large pre-trained models as prompt generators.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only models and methods. No dataset names are present in the text.",
      "processing_time": 16.747870445251465,
      "citing_paper_id": "274306415",
      "cited_paper_id": 264145824
    },
    {
      "context_text": "PromptIR [40], for instance, integrates a dedicated prompt block to capture degradation-specific features from input images, while DA-CLIP [30], MPerceiver [2], ProRes [31] and Painter [50] leverage large pre-trained models as prompt generators.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only models and methods. No dataset names are present in the text.",
      "processing_time": 16.747870445251465,
      "citing_paper_id": "274306415",
      "cited_paper_id": 268064460
    },
    {
      "context_text": "PromptIR [40], for instance, integrates a dedicated prompt block to capture degradation-specific features from input images, while DA-CLIP [30], MPerceiver [2], ProRes [31] and Painter [50] leverage large pre-trained models as prompt generators.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only models and methods. No dataset names are present in the text.",
      "processing_time": 16.747870445251465,
      "citing_paper_id": "274306415",
      "cited_paper_id": 272724853
    },
    {
      "context_text": "Notably, some current meth-ods are either evaluated under different setups or lack available code and models, which complicates direct comparisons.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only issues with method evaluation and availability of code and models.",
      "processing_time": 15.238461256027222,
      "citing_paper_id": "274306415",
      "cited_paper_id": 264145824
    },
    {
      "context_text": "Therefore, we train three state-of-the-art methods, Restormer [63], PromptIR [40], and X-Restormer [8], on the five-degradation IR setup to provide a fair comparison.",
      "catation_intent": "reusable resource",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.2,
      "reasoning": "The context mentions training methods on a 'five-degradation IR setup' but does not specify a named dataset. The cited papers do not provide additional context to identify a specific dataset.",
      "processing_time": 19.246742010116577,
      "citing_paper_id": "274306415",
      "cited_paper_id": 264289165
    },
    {
      "context_text": "Therefore, we train three state-of-the-art methods, Restormer [63], PromptIR [40], and X-Restormer [8], on the five-degradation IR setup to provide a fair comparison.",
      "catation_intent": "reusable resource",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.2,
      "reasoning": "The context mentions training methods on a 'five-degradation IR setup' but does not specify a named dataset. The cited papers do not provide additional context to identify a specific dataset.",
      "processing_time": 19.246742010116577,
      "citing_paper_id": "274306415",
      "cited_paper_id": 268064460
    },
    {
      "context_text": "This combination enables the base-line model to effectively capture the inherent information of various degradations, establishing a strong foundation for generalized IR tasks.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general discussion about the model's ability to handle various degradations.",
      "processing_time": 17.19279909133911,
      "citing_paper_id": "274306415",
      "cited_paper_id": 264289165
    },
    {
      "context_text": "For pre-training in Phase I, we use the same setup as the five-task configuration, i.e. training the LoRAs and the estimator using task-specific IR datasets and the obtained pre-trained weights.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.3,
      "reasoning": "The context mentions 'task-specific IR datasets' but does not provide specific names. The term 'IR datasets' is too generic and lacks a clear identifier.",
      "processing_time": 19.262816190719604,
      "citing_paper_id": "274306415",
      "cited_paper_id": 264289165
    },
    {
      "context_text": "Our model significantly outperforms state-of-the-art all-in-one image restoration (IR) methods, Restormer [63], Promp-tIR [40], and DiffUIR [66], across five known IR tasks, three un-seen tasks, and three mixed degradation scenarios.",
      "catation_intent": "findings",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions several methods/models (Restormer, PromptIR, DiffUIR) but does not refer to any specific datasets. The citation is focused on comparing performance across various tasks and scenarios.",
      "processing_time": 20.2824923992157,
      "citing_paper_id": "274306415",
      "cited_paper_id": 264289165
    },
    {
      "context_text": "Our model significantly outperforms state-of-the-art all-in-one image restoration (IR) methods, Restormer [63], Promp-tIR [40], and DiffUIR [66], across five known IR tasks, three un-seen tasks, and three mixed degradation scenarios.",
      "catation_intent": "findings",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions several methods/models (Restormer, PromptIR, DiffUIR) but does not refer to any specific datasets. The citation is focused on comparing performance across various tasks and scenarios.",
      "processing_time": 20.2824923992157,
      "citing_paper_id": "274306415",
      "cited_paper_id": 268064460
    },
    {
      "context_text": "Our model significantly outperforms state-of-the-art all-in-one image restoration (IR) methods, Restormer [63], Promp-tIR [40], and DiffUIR [66], across five known IR tasks, three un-seen tasks, and three mixed degradation scenarios.",
      "catation_intent": "findings",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions several methods/models (Restormer, PromptIR, DiffUIR) but does not refer to any specific datasets. The citation is focused on comparing performance across various tasks and scenarios.",
      "processing_time": 20.2824923992157,
      "citing_paper_id": "274306415",
      "cited_paper_id": 268513542
    },
    {
      "context_text": "For instance, AirNet [27] uses a contrastive-based encoder to extract a latent degradation representation from the input images.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (AirNet) and its approach to image restoration.",
      "processing_time": 18.01172709465027,
      "citing_paper_id": "274306415",
      "cited_paper_id": 264289165
    },
    {
      "context_text": "Our model largely outperforms the state-of-the-art on five-and three-task IR setups, and shows improved generalization to unseen datasets and IR tasks.",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only generic references to 'unseen datasets' and 'IR tasks'. No clear, verifiable resource names are provided.",
      "processing_time": 18.684807538986206,
      "citing_paper_id": "274306415",
      "cited_paper_id": 267320695
    },
    {
      "context_text": "Our model largely outperforms the state-of-the-art on five-and three-task IR setups, and shows improved generalization to unseen datasets and IR tasks.",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only generic references to 'unseen datasets' and 'IR tasks'. No clear, verifiable resource names are provided.",
      "processing_time": 18.684807538986206,
      "citing_paper_id": "274306415",
      "cited_paper_id": 270440448
    },
    {
      "context_text": "Building upon LoRA, alternative decomposition methods have been proposed, such as Vector-based Random Matrix Adaptation [24] and Conv-LoRA[67].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and models.",
      "processing_time": 12.4175124168396,
      "citing_paper_id": "274306415",
      "cited_paper_id": 267334929
    },
    {
      "context_text": "Specifically, we evaluate VeRA [24] and Conv-LoRA [67].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only models (VeRA and Conv-LoRA). The context is focused on evaluating these models, not on using datasets.",
      "processing_time": 20.258360385894775,
      "citing_paper_id": "274306415",
      "cited_paper_id": 267334929
    },
    {
      "context_text": "On the other hand, Conv-LoRA [67] performs worse despite having more parameters, owing to its Mixture-of-Experts approach with convolution layers in the decomposed space.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (Conv-LoRA) and its performance. No verifiable resources are identified.",
      "processing_time": 30.60248875617981,
      "citing_paper_id": "274306415",
      "cited_paper_id": 267334929
    },
    {
      "context_text": "Restormer [63] PromptIR [40] Ours-SW Ground truth Figure 6.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only model names and a figure reference. There is no information about the usage of datasets.",
      "processing_time": 19.829121828079224,
      "citing_paper_id": "274306415",
      "cited_paper_id": 268064460
    },
    {
      "context_text": "In particular, our method surpasses IDR [64] by 2.16 dB on image deraining and provides a 2.40 dB improvement over PromptIR [40] on low-light image enhancement.",
      "catation_intent": "findings",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only performance improvements over other methods.",
      "processing_time": 12.877968072891235,
      "citing_paper_id": "274306415",
      "cited_paper_id": 268064460
    },
    {
      "context_text": "Particularly, we use the Restormer architecture [63] with spatial attention mechanisms [8] and incorporate a modified version of Promp-tIR [40] prompt blocks.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models and methods. The context focuses on the architecture and modifications used for image restoration.",
      "processing_time": 17.617682695388794,
      "citing_paper_id": "274306415",
      "cited_paper_id": 268064460
    },
    {
      "context_text": "PromptIR [40] and Ours are not trained for this task, while Ours retrained has a specified LoRA in an 8-degradation setup.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models and training setups. There are no clear identifiers for datasets in the provided context.",
      "processing_time": 15.953384637832642,
      "citing_paper_id": "274306415",
      "cited_paper_id": 268064460
    },
    {
      "context_text": "Recent advances in deep learning techniques have led to remarkable progress in IR [11, 27, 40, 41, 63], achieving substantial improvements in reconstruction accuracy.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to papers that discuss advancements in image restoration using deep learning techniques.",
      "processing_time": 17.167109727859497,
      "citing_paper_id": "274306415",
      "cited_paper_id": 268064460
    },
    {
      "context_text": "J P E G [ 42 ] 4 - t o - 8 b it s [ 42 ] Input PromptIR [40] Ours-SW Ours-SW retrained Ground truth Figure 5.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only model names and methods. There are no clear identifiers for datasets.",
      "processing_time": 14.927005052566528,
      "citing_paper_id": "274306415",
      "cited_paper_id": 268064460
    },
    {
      "context_text": "IDR [64] learns degradation-specific priors and incorporates them into restoration, while DiffUIR [66] equips a diffusion model with a condition guidance.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models and methods. No verifiable resources are identified.",
      "processing_time": 13.323582649230957,
      "citing_paper_id": "274306415",
      "cited_paper_id": 268513542
    },
    {
      "context_text": "This setup helps the model distinguish and manage multiple degradation types within the same image.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, models, or methods. It only describes a setup for managing multiple degradation types in images.",
      "processing_time": 17.591827392578125,
      "citing_paper_id": "274306415",
      "cited_paper_id": 270440448
    },
    {
      "context_text": "Moreover, pre-training with synthetic data has also shown effectiveness when the domain gap is properly addressed [55].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific dataset names, only a general reference to synthetic data. No clear, verifiable dataset is identified.",
      "processing_time": 18.64630889892578,
      "citing_paper_id": "274306415",
      "cited_paper_id": 270440448
    },
    {
      "context_text": "Our approach outperforms all the other state-of-the-art methods on average and stays consistent in all the degradation types.",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a comparison of performance against other methods.",
      "processing_time": 14.26891303062439,
      "citing_paper_id": "274306415",
      "cited_paper_id": 272987034
    },
    {
      "context_text": "Our approach outperforms all the other state-of-the-art methods on average and stays consistent in all the degradation types.",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a comparison of performance against other methods.",
      "processing_time": 14.26891303062439,
      "citing_paper_id": "274306415",
      "cited_paper_id": 273643057
    },
    {
      "context_text": "General schema of our proposed method.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only a general schema of the proposed method. No verifiable resources are identified.",
      "processing_time": 15.195814371109009,
      "citing_paper_id": "274306415",
      "cited_paper_id": 272987034
    },
    {
      "context_text": "General schema of our proposed method.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only a general schema of the proposed method. No verifiable resources are identified.",
      "processing_time": 15.195814371109009,
      "citing_paper_id": "274306415",
      "cited_paper_id": 273643057
    },
    {
      "context_text": "In contrast, aiming at versatile image restoration, our 2.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only a general reference to image restoration. No verifiable resources are identified.",
      "processing_time": 16.282166242599487,
      "citing_paper_id": "274306415",
      "cited_paper_id": 276318260
    },
    {
      "context_text": "Dehazing Deraining Denoising on BSD68 dataset [30] on SOTS [21] on Rain100L [ highlighting the synergistic effect of these techniques in enhancing the model’s ability to selectively forget and restore information as intended.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "BSD68",
        "SOTS",
        "Rain100L"
      ],
      "dataset_descriptions": {
        "BSD68": "Used to evaluate dehazing and denoising techniques, focusing on the model's ability to selectively restore information in natural images.",
        "SOTS": "Used to benchmark single-image dehazing methods, assessing the effectiveness of the model in enhancing image clarity under various conditions.",
        "Rain100L": "Used to evaluate deraining techniques, highlighting the model's capability to remove rain streaks and restore image quality."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions specific datasets used for dehazing, deraining, and denoising experiments. The datasets are clearly identified and used for evaluating the performance of image restoration techniques.",
      "processing_time": 37.5120165348053,
      "citing_paper_id": "272368450",
      "cited_paper_id": 64193
    },
    {
      "context_text": "Dehazing Deraining Denoising on BSD68 dataset [30] on SOTS [21] on Rain100L [ highlighting the synergistic effect of these techniques in enhancing the model’s ability to selectively forget and restore information as intended.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "BSD68",
        "SOTS",
        "Rain100L"
      ],
      "dataset_descriptions": {
        "BSD68": "Used to evaluate dehazing and denoising techniques, focusing on the model's ability to selectively restore information in natural images.",
        "SOTS": "Used to benchmark single-image dehazing methods, assessing the effectiveness of the model in enhancing image clarity under various conditions.",
        "Rain100L": "Used to evaluate deraining techniques, highlighting the model's capability to remove rain streaks and restore image quality."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions specific datasets used for dehazing, deraining, and denoising experiments. The datasets are clearly identified and used for evaluating the performance of image restoration techniques.",
      "processing_time": 37.5120165348053,
      "citing_paper_id": "272368450",
      "cited_paper_id": 39760169
    },
    {
      "context_text": "Adversarial examples were initially proposed by [35], and the classic method for generating them, called the Fast Gradient Sign Method (FGSM), proved to be a simple and effective approach.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (FGSM) for generating adversarial examples.",
      "processing_time": 16.65503191947937,
      "citing_paper_id": "272368450",
      "cited_paper_id": 604334
    },
    {
      "context_text": "Since then, various methods [2, 11, 27] have been proposed to generate adversarial examples.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods for generating adversarial examples.",
      "processing_time": 13.970017671585083,
      "citing_paper_id": "272368450",
      "cited_paper_id": 3488815
    },
    {
      "context_text": "Since then, various methods [2, 11, 27] have been proposed to generate adversarial examples.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods for generating adversarial examples.",
      "processing_time": 13.970017671585083,
      "citing_paper_id": "272368450",
      "cited_paper_id": 195791557
    },
    {
      "context_text": "Machine unlearning is proposed by [5], which aims at protecting machine learning models from extraction attacks, involves the process of removing specific data from a model in a manner that ensures the data appears as though it were never part of the training set.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method or concept called 'machine unlearning'.",
      "processing_time": 16.31618905067444,
      "citing_paper_id": "272368450",
      "cited_paper_id": 5945696
    },
    {
      "context_text": "Following [24], we adopt commonly-used IQA PyTorch Toolbox 1 to compute the PSNR [17] and SSIM [39] scores of all compared methods.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only tools and metrics. The cited papers are about PSNR and SSIM, which are metrics, not datasets.",
      "processing_time": 20.985756158828735,
      "citing_paper_id": "272368450",
      "cited_paper_id": 62732555
    },
    {
      "context_text": "Following [24], we adopt commonly-used IQA PyTorch Toolbox 1 to compute the PSNR [17] and SSIM [39] scores of all compared methods.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only tools and metrics. The cited papers are about PSNR and SSIM, which are metrics, not datasets.",
      "processing_time": 20.985756158828735,
      "citing_paper_id": "272368450",
      "cited_paper_id": 207761262
    },
    {
      "context_text": "The concept of adversarial examples, inspired by the seminal work [18] that introduces perturbations for classification tasks, is adapted in our approach for image restoration.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a concept from a cited paper. No dataset names are present in the citation span.",
      "processing_time": 17.137253046035767,
      "citing_paper_id": "272368450",
      "cited_paper_id": 146121358
    },
    {
      "context_text": "Existing works [4, 14, 28] focus on approximating unlearning linear/logistic regression, k-means clustering, and random forests.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and models. There are no verifiable resources that meet the criteria for inclusion.",
      "processing_time": 14.883876323699951,
      "citing_paper_id": "272368450",
      "cited_paper_id": 195886255
    },
    {
      "context_text": "According to the level of forgetting, existing machine unlearning methods can be categorized into: (1) Exact unlearning [3, 6]; (2) Approximate unlearning [8, 31].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only categories of machine unlearning methods. No verifiable resources are identified.",
      "processing_time": 15.907670021057129,
      "citing_paper_id": "272368450",
      "cited_paper_id": 208909851
    },
    {
      "context_text": "According to the level of forgetting, existing machine unlearning methods can be categorized into: (1) Exact unlearning [3, 6]; (2) Approximate unlearning [8, 31].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only categories of machine unlearning methods. No verifiable resources are identified.",
      "processing_time": 15.907670021057129,
      "citing_paper_id": "272368450",
      "cited_paper_id": 249890116
    },
    {
      "context_text": "To facilitate the design of our method, we chose the all-in-one task in the field of image restoration [22, 29, 41, 42] as a case.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general reference to the field of image restoration.",
      "processing_time": 27.569289207458496,
      "citing_paper_id": "272368450",
      "cited_paper_id": 241040811
    },
    {
      "context_text": "…have introduced various innovative techniques, such as those leveraging the Fisher Information Matrix [15], NTK theory [16], and gradient update storage [43], as well as error-maximizing noise [10, 36], teacher-student frameworks [9, 20, 37], and parameter attenuation during inference [10].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various techniques and methods. There are no clear identifiers for datasets within the text.",
      "processing_time": 18.63168454170227,
      "citing_paper_id": "272368450",
      "cited_paper_id": 244270535
    },
    {
      "context_text": "…have introduced various innovative techniques, such as those leveraging the Fisher Information Matrix [15], NTK theory [16], and gradient update storage [43], as well as error-maximizing noise [10, 36], teacher-student frameworks [9, 20, 37], and parameter attenuation during inference [10].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various techniques and methods. There are no clear identifiers for datasets within the text.",
      "processing_time": 18.63168454170227,
      "citing_paper_id": "272368450",
      "cited_paper_id": 246015506
    },
    {
      "context_text": "…have introduced various innovative techniques, such as those leveraging the Fisher Information Matrix [15], NTK theory [16], and gradient update storage [43], as well as error-maximizing noise [10, 36], teacher-student frameworks [9, 20, 37], and parameter attenuation during inference [10].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various techniques and methods. There are no clear identifiers for datasets within the text.",
      "processing_time": 18.63168454170227,
      "citing_paper_id": "272368450",
      "cited_paper_id": 248834527
    },
    {
      "context_text": "Similar to Air-Net, [12, 26, 32, 33, 40] both utilize uniquely designed prompts to guide their networks.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only methods or models using prompts. No verifiable resources are identified.",
      "processing_time": 15.494423627853394,
      "citing_paper_id": "272368450",
      "cited_paper_id": 260107992
    },
    {
      "context_text": "Similar to Air-Net, [12, 26, 32, 33, 40] both utilize uniquely designed prompts to guide their networks.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only methods or models using prompts. No verifiable resources are identified.",
      "processing_time": 15.494423627853394,
      "citing_paper_id": "272368450",
      "cited_paper_id": 268064460
    },
    {
      "context_text": "Similar to Air-Net, [12, 26, 32, 33, 40] both utilize uniquely designed prompts to guide their networks.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only methods or models using prompts. No verifiable resources are identified.",
      "processing_time": 15.494423627853394,
      "citing_paper_id": "272368450",
      "cited_paper_id": 268553835
    },
    {
      "context_text": "Similar to Air-Net, [12, 26, 32, 33, 40] both utilize uniquely designed prompts to guide their networks.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only methods or models using prompts. No verifiable resources are identified.",
      "processing_time": 15.494423627853394,
      "citing_paper_id": "272368450",
      "cited_paper_id": null
    },
    {
      "context_text": "We use PromptIR [33] and AdaIR [12] as the base pre-trained all-in-one model.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions PromptIR and AdaIR as pre-trained models, not datasets. These are methods or tools, not reusable data resources.",
      "processing_time": 18.343329429626465,
      "citing_paper_id": "272368450",
      "cited_paper_id": 268064460
    },
    {
      "context_text": "We use PromptIR [33] and AdaIR [12] as the base pre-trained all-in-one model.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions PromptIR and AdaIR as pre-trained models, not datasets. These are methods or tools, not reusable data resources.",
      "processing_time": 18.343329429626465,
      "citing_paper_id": "272368450",
      "cited_paper_id": 268553835
    },
    {
      "context_text": "For image deraining, we train our model on 13,712 clean-rain image pairs gathered from multiple datasets [27], [51], [52], [53], and perform evaluation on Rain100L [27], Rain100H [27], Test100 [53] and Test1200 [54].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Rain100L",
        "Rain100H",
        "Test100",
        "Test1200"
      ],
      "dataset_descriptions": {
        "Rain100L": "Used for evaluating the model's performance on light rain conditions, focusing on the quality of derained images.",
        "Rain100H": "Used for evaluating the model's performance on heavy rain conditions, focusing on the quality of derained images.",
        "Test100": "Used for evaluating the model's performance on a test set of 100 images, focusing on generalization and robustness.",
        "Test1200": "Used for evaluating the model's performance on a larger test set of 1200 images, focusing on scalability and consistency."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions specific datasets used for training and evaluation in the context of image deraining. These datasets are clearly identified and used for evaluating the performance of the model.",
      "processing_time": 45.77790284156799,
      "citing_paper_id": "261556765",
      "cited_paper_id": 3406592
    },
    {
      "context_text": "For image deraining, we train our model on 13,712 clean-rain image pairs gathered from multiple datasets [27], [51], [52], [53], and perform evaluation on Rain100L [27], Rain100H [27], Test100 [53] and Test1200 [54].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Rain100L",
        "Rain100H",
        "Test100",
        "Test1200"
      ],
      "dataset_descriptions": {
        "Rain100L": "Used for evaluating the model's performance on light rain conditions, focusing on the quality of derained images.",
        "Rain100H": "Used for evaluating the model's performance on heavy rain conditions, focusing on the quality of derained images.",
        "Test100": "Used for evaluating the model's performance on a test set of 100 images, focusing on generalization and robustness.",
        "Test1200": "Used for evaluating the model's performance on a larger test set of 1200 images, focusing on scalability and consistency."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions specific datasets used for training and evaluation in the context of image deraining. These datasets are clearly identified and used for evaluating the performance of the model.",
      "processing_time": 45.77790284156799,
      "citing_paper_id": "261556765",
      "cited_paper_id": 11922819
    },
    {
      "context_text": "I , including Rain14000 [51], Rain1800 [27], Rain12 [52] and Rain800 [53] for deraining, RESIDE [29] for dehazing, BSD300 [28] and WED [57] for denoising and GoPro [26] for deblurring.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Rain14000",
        "Rain1800",
        "Rain12",
        "Rain800",
        "RESIDE",
        "BSD300",
        "WED",
        "GoPro"
      ],
      "dataset_descriptions": {
        "Rain14000": "Used for deraining, focusing on removing rain streaks from images using a conditional generative adversarial network.",
        "Rain1800": "Used for deraining, focusing on removing rain streaks from images using a conditional generative adversarial network.",
        "Rain12": "Used for deraining, focusing on removing rain streaks from images using a conditional generative adversarial network.",
        "Rain800": "Used for deraining, focusing on removing rain streaks from images using a conditional generative adversarial network.",
        "RESIDE": "Used for dehazing, focusing on improving visibility in hazy images using a conditional generative adversarial network.",
        "BSD300": "Used for denoising, focusing on reducing noise in images using a conditional generative adversarial network.",
        "WED": "Used for denoising, focusing on reducing noise in images using a conditional generative adversarial network.",
        "GoPro": "Used for deblurring, focusing on sharpening blurred images using a conditional generative adversarial network."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions several datasets used for various image restoration tasks, which align with the research topic of all-in-one image restoration.",
      "processing_time": 75.81859421730042,
      "citing_paper_id": "261556765",
      "cited_paper_id": 11922819
    },
    {
      "context_text": "999) and PSNR loss for 5 × 10 5 iterations with the initial learning rate 5 × 10 4 gradually reduced to 1 × 10 7 with the cosine annealing [59].",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for adjusting the learning rate during training.",
      "processing_time": 16.64445447921753,
      "citing_paper_id": "261556765",
      "cited_paper_id": 14337532
    },
    {
      "context_text": "Although many methods have achieved excellent performance, such as denoising [1], [2], [3], debluring [4], [5], [6], [7], deraining [8], [9], [10], [11], [12], dehazing [13], [14], [15], [16], etc., these methods either focus solely on the task at hand or fine-tune the model individually for each…",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various image restoration tasks and methods. No verifiable resources are identified.",
      "processing_time": 18.334857940673828,
      "citing_paper_id": "261556765",
      "cited_paper_id": 49672261
    },
    {
      "context_text": "Although many methods have achieved excellent performance, such as denoising [1], [2], [3], debluring [4], [5], [6], [7], deraining [8], [9], [10], [11], [12], dehazing [13], [14], [15], [16], etc., these methods either focus solely on the task at hand or fine-tune the model individually for each…",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various image restoration tasks and methods. No verifiable resources are identified.",
      "processing_time": 18.334857940673828,
      "citing_paper_id": "261556765",
      "cited_paper_id": 247596985
    },
    {
      "context_text": "Although many methods have achieved excellent performance, such as denoising [1], [2], [3], debluring [4], [5], [6], [7], deraining [8], [9], [10], [11], [12], dehazing [13], [14], [15], [16], etc., these methods either focus solely on the task at hand or fine-tune the model individually for each…",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various image restoration tasks and methods. No verifiable resources are identified.",
      "processing_time": 18.334857940673828,
      "citing_paper_id": "261556765",
      "cited_paper_id": 259459919
    },
    {
      "context_text": "Although many methods have achieved excellent performance, such as denoising [1], [2], [3], debluring [4], [5], [6], [7], deraining [8], [9], [10], [11], [12], dehazing [13], [14], [15], [16], etc., these methods either focus solely on the task at hand or fine-tune the model individually for each…",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various image restoration tasks and methods. No verifiable resources are identified.",
      "processing_time": 18.334857940673828,
      "citing_paper_id": "261556765",
      "cited_paper_id": 259634520
    },
    {
      "context_text": "Although many methods have achieved excellent performance, such as denoising [1], [2], [3], debluring [4], [5], [6], [7], deraining [8], [9], [10], [11], [12], dehazing [13], [14], [15], [16], etc., these methods either focus solely on the task at hand or fine-tune the model individually for each…",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various image restoration tasks and methods. No verifiable resources are identified.",
      "processing_time": 18.334857940673828,
      "citing_paper_id": "261556765",
      "cited_paper_id": 260006244
    },
    {
      "context_text": "Although many methods have achieved excellent performance, such as denoising [1], [2], [3], debluring [4], [5], [6], [7], deraining [8], [9], [10], [11], [12], dehazing [13], [14], [15], [16], etc., these methods either focus solely on the task at hand or fine-tune the model individually for each…",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various image restoration tasks and methods. No verifiable resources are identified.",
      "processing_time": 18.334857940673828,
      "citing_paper_id": "261556765",
      "cited_paper_id": null
    },
    {
      "context_text": "…been a significant paradigm shift from traditional restoration methods to learning-based approaches, driven by their impressive performance in a wide range of image restoration tasks, such as denoising [1], [2], [3], dehazing [13], [14], [16], debluring [4], [5], [6], deraining [8], [9], [10], etc.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various image restoration tasks. The cited papers' titles do not provide additional dataset information.",
      "processing_time": 30.064353942871094,
      "citing_paper_id": "261556765",
      "cited_paper_id": 49672261
    },
    {
      "context_text": "…been a significant paradigm shift from traditional restoration methods to learning-based approaches, driven by their impressive performance in a wide range of image restoration tasks, such as denoising [1], [2], [3], dehazing [13], [14], [16], debluring [4], [5], [6], deraining [8], [9], [10], etc.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various image restoration tasks. The cited papers' titles do not provide additional dataset information.",
      "processing_time": 30.064353942871094,
      "citing_paper_id": "261556765",
      "cited_paper_id": 247596985
    },
    {
      "context_text": "…been a significant paradigm shift from traditional restoration methods to learning-based approaches, driven by their impressive performance in a wide range of image restoration tasks, such as denoising [1], [2], [3], dehazing [13], [14], [16], debluring [4], [5], [6], deraining [8], [9], [10], etc.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various image restoration tasks. The cited papers' titles do not provide additional dataset information.",
      "processing_time": 30.064353942871094,
      "citing_paper_id": "261556765",
      "cited_paper_id": 259634520
    },
    {
      "context_text": "…been a significant paradigm shift from traditional restoration methods to learning-based approaches, driven by their impressive performance in a wide range of image restoration tasks, such as denoising [1], [2], [3], dehazing [13], [14], [16], debluring [4], [5], [6], deraining [8], [9], [10], etc.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various image restoration tasks. The cited papers' titles do not provide additional dataset information.",
      "processing_time": 30.064353942871094,
      "citing_paper_id": "261556765",
      "cited_paper_id": 260006244
    },
    {
      "context_text": "…been a significant paradigm shift from traditional restoration methods to learning-based approaches, driven by their impressive performance in a wide range of image restoration tasks, such as denoising [1], [2], [3], dehazing [13], [14], [16], debluring [4], [5], [6], deraining [8], [9], [10], etc.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various image restoration tasks. The cited papers' titles do not provide additional dataset information.",
      "processing_time": 30.064353942871094,
      "citing_paper_id": "261556765",
      "cited_paper_id": null
    },
    {
      "context_text": "Reference [18] propose a multi-encoder single-encoder to handle multiple bad weather degradations.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for handling multiple bad weather degradations.",
      "processing_time": 28.761780261993408,
      "citing_paper_id": "261556765",
      "cited_paper_id": 219978541
    },
    {
      "context_text": "The former [17], [18] address various forms of degradation using individual subnetworks.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only references to papers addressing degradation using subnetworks.",
      "processing_time": 17.939249753952026,
      "citing_paper_id": "261556765",
      "cited_paper_id": 219978541
    },
    {
      "context_text": "References [40] and [41] introduce pre-training model based on transformers IPT and EDT respectively for image restoration tasks.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions pre-training models IPT and EDT for image restoration tasks but does not refer to any specific datasets.",
      "processing_time": 15.914335250854492,
      "citing_paper_id": "261556765",
      "cited_paper_id": 220686623
    },
    {
      "context_text": "2) Simplified Prompt-based Transformer Block: There are two main challenges to apply standard Transformer [48], [49] for image restoration.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only challenges related to applying standard Transformers for image restoration.",
      "processing_time": 13.063856363296509,
      "citing_paper_id": "261556765",
      "cited_paper_id": 225039882
    },
    {
      "context_text": "1) Multiple Degradations All-in-One Results: We compare our CAPTNet with six general image restoration methods [20], [31], [32], [37], [38], [59] and five all-in-one fashion methods [18], [19], [41], [43], [60] on four challenging image restoration tasks including deblurring, deraining, denoising and dehazing.",
      "catation_intent": "reusable resource",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and tasks. The context focuses on comparing different image restoration methods across various tasks.",
      "processing_time": 15.471452951431274,
      "citing_paper_id": "261556765",
      "cited_paper_id": 231802205
    },
    {
      "context_text": "[32], [34] pay attention to balance the competinng goals of spatial details and high-level contextualized information.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only discusses balancing competing goals in image restoration.",
      "processing_time": 13.264568090438843,
      "citing_paper_id": "261556765",
      "cited_paper_id": 231802205
    },
    {
      "context_text": "Reference [46] introduce Visual Prompt Tuning (VPT) as an efficient and effective alternative to full fine-tuning for large-scale Transformer models.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method called Visual Prompt Tuning (VPT).",
      "processing_time": 17.548461437225342,
      "citing_paper_id": "261556765",
      "cited_paper_id": 247618727
    },
    {
      "context_text": "While our approach is no match for the best approach FFTformer [4] on the GoPro dataset, it outperforms Restormer [43], NAFNet [25], MSFS-Net [7], HINet [61] and Uformer [44].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "GoPro"
      ],
      "dataset_descriptions": {
        "GoPro": "Used to evaluate image restoration methods, specifically comparing performance against state-of-the-art models like FFTformer, Restormer, NAFNet, MSFS-Net, HINet, and Uformer."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions the GoPro dataset, which is a specific dataset used for evaluating image restoration methods. No other datasets are mentioned.",
      "processing_time": 26.574870347976685,
      "citing_paper_id": "261556765",
      "cited_paper_id": 248085491
    },
    {
      "context_text": "1) Multiple Degradations All-in-One Results: We compare our CAPTNet with six general image restoration meth-ods [25], [36], [37], [42], [43], [61] and five all-in-one fashion methods [19], [20], [21], [22], [23], [24], [62] on four challenging image restoration tasks including deblurring,…",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and tasks. The context focuses on comparing different image restoration methods across various tasks.",
      "processing_time": 17.097288846969604,
      "citing_paper_id": "261556765",
      "cited_paper_id": 248085491
    },
    {
      "context_text": "Reference [25] propose a streamlined baseline network for image restoration, involving the removal or replacement of nonlinear activation functions.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for image restoration.",
      "processing_time": 27.31462574005127,
      "citing_paper_id": "261556765",
      "cited_paper_id": 248085491
    },
    {
      "context_text": "In the CNNs-based block, we did not make any innovation in the CNN-based block, but just used the nonlinear activation free block (NAFBlock) [25].",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (NAFBlock) used in the CNN-based block for image restoration.",
      "processing_time": 17.920150756835938,
      "citing_paper_id": "261556765",
      "cited_paper_id": 248085491
    },
    {
      "context_text": "1) CNNs Based Method: Image restoration methods based on CNNs have made significant strides in recent years, with a plethora of emerging architectures [25], [35], [36], [37], [38], [39].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to CNN-based methods and architectures. No verifiable resources are identified.",
      "processing_time": 14.530898332595825,
      "citing_paper_id": "261556765",
      "cited_paper_id": 248085491
    },
    {
      "context_text": "1) CNNs Based Method: Image restoration methods based on CNNs have made significant strides in recent years, with a plethora of emerging architectures [25], [35], [36], [37], [38], [39].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to CNN-based methods and architectures. No verifiable resources are identified.",
      "processing_time": 14.530898332595825,
      "citing_paper_id": "261556765",
      "cited_paper_id": 248426764
    },
    {
      "context_text": "Restrictions apply. block, but just used the nonlinear activation free block (NAF-Block) [25].",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (NAF-Block).",
      "processing_time": 15.894136428833008,
      "citing_paper_id": "261556765",
      "cited_paper_id": 248085491
    },
    {
      "context_text": "6(a)) [25].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not provide any specific dataset names or details about the usage of datasets. It only references a figure in another paper.",
      "processing_time": 17.53540062904358,
      "citing_paper_id": "261556765",
      "cited_paper_id": 248085491
    },
    {
      "context_text": "Reference [38] employ an expansion strategy to delve deeper into the principles of image restoration.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method or approach for image restoration.",
      "processing_time": 13.041543960571289,
      "citing_paper_id": "261556765",
      "cited_paper_id": 248426764
    },
    {
      "context_text": "We adopt TLC [60] to solve the issue of performance degradation caused by training on patched images and testing on the full image.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (TLC) for addressing performance degradation in image restoration.",
      "processing_time": 15.109909772872925,
      "citing_paper_id": "261556765",
      "cited_paper_id": 250451699
    },
    {
      "context_text": "Reference [48] propose an implicit vision prompt tuning model for various complex tasks with stable memory costs.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method or model. The context is about a vision prompt tuning model, which is not a dataset.",
      "processing_time": 17.52840232849121,
      "citing_paper_id": "261556765",
      "cited_paper_id": 257622589
    },
    {
      "context_text": "Reference [23] introduce a novel perspective that delved into degradation through an ingredient-oriented approach, ultimately improving the model’s scalability.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method or approach. The context focuses on the model's scalability and the ingredient-oriented approach to degradation.",
      "processing_time": 18.283488035202026,
      "citing_paper_id": "261556765",
      "cited_paper_id": 260085391
    },
    {
      "context_text": "Our CAPTNet demonstrates the favorable generalization ability compared to IDR [23], yielding 2.27 dB PSNR difference.",
      "catation_intent": "findings",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a comparison of performance metrics between two methods.",
      "processing_time": 12.79165768623352,
      "citing_paper_id": "261556765",
      "cited_paper_id": 260085391
    },
    {
      "context_text": "To solve the above problem, [23] proposed a pioneering perspective to investigate degradation through the utilization of ingredient-oriented multi-degradation learning.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method or approach. The context focuses on the method 'Ingredient-oriented Multi-Degradation Learning' rather than a dataset.",
      "processing_time": 21.572386026382446,
      "citing_paper_id": "261556765",
      "cited_paper_id": 260085391
    },
    {
      "context_text": "…image restoration tasks including deblurring, deraining, denoising and dehazing. restoration tasks, our algorithm yields 0.3 dB performance gain over the previous best method PromptIR [24], and we improve 0.32 dB, 0.43 dB and 0.42 dB when compare to IDR [23], AIRFormer [22] and AirNet [21].",
      "catation_intent": "findings",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only performance comparisons with other methods. No verifiable resources are identified.",
      "processing_time": 16.22181272506714,
      "citing_paper_id": "261556765",
      "cited_paper_id": 260085391
    },
    {
      "context_text": "…image restoration tasks including deblurring, deraining, denoising and dehazing. restoration tasks, our algorithm yields 0.3 dB performance gain over the previous best method PromptIR [24], and we improve 0.32 dB, 0.43 dB and 0.42 dB when compare to IDR [23], AIRFormer [22] and AirNet [21].",
      "catation_intent": "findings",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only performance comparisons with other methods. No verifiable resources are identified.",
      "processing_time": 16.22181272506714,
      "citing_paper_id": "261556765",
      "cited_paper_id": 260271048
    },
    {
      "context_text": "…CAPTNet with six general image restoration meth-ods [25], [36], [37], [42], [43], [61] and five all-in-one fashion methods [19], [20], [21], [22], [23], [24], [62] on four challenging image restoration tasks including deblurring, deraining, denoising and dehazing. restoration tasks, our algorithm…",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods and tasks. There are no clear identifiers for datasets.",
      "processing_time": 16.220903158187866,
      "citing_paper_id": "261556765",
      "cited_paper_id": 260085391
    },
    {
      "context_text": "While the latter [19], [20], [21], [22] overcome the constraints associated with the earlier corruption types, enhancing the model’s practical flexibility.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to papers. There are no clear identifiers for datasets or other verifiable resources.",
      "processing_time": 19.49878454208374,
      "citing_paper_id": "261556765",
      "cited_paper_id": 260271048
    },
    {
      "context_text": "…our CAPTNet with six general image restoration meth-ods [25], [36], [37], [42], [43], [61] and five all-in-one fashion methods [19], [20], [21], [22], [23], [24], [62] on four challenging image restoration tasks including deblurring, deraining, denoising and dehazing. restoration tasks, our…",
      "catation_intent": "reusable resource",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods and tasks. There are no clear identifiers for datasets.",
      "processing_time": 17.0675847530365,
      "citing_paper_id": "261556765",
      "cited_paper_id": 260271048
    },
    {
      "context_text": "Pixel-space AR predicts raw pixels one by one in raster order, as in PixelRNN [41] and PixelCNN++ [52], but is prohibitively slow at high resolutions.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only models and methods. The citation is used to reference prior work on pixel-space autoregressive models.",
      "processing_time": 20.13468360900879,
      "citing_paper_id": "278886852",
      "cited_paper_id": 8142135
    },
    {
      "context_text": "Recent works [41, 20] have extended AR models to vision and can be categorized as pixel-space AR [41, 42, 43, 44], token-based AR [45, 46, 47, 48] and scale-space AR [20, 49, 50, 51].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only categories of autoregressive models. No verifiable resources are identified.",
      "processing_time": 15.41073489189148,
      "citing_paper_id": "278886852",
      "cited_paper_id": 8142135
    },
    {
      "context_text": "Recent works [41, 20] have extended AR models to vision and can be categorized as pixel-space AR [41, 42, 43, 44], token-based AR [45, 46, 47, 48] and scale-space AR [20, 49, 50, 51].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only categories of autoregressive models. No verifiable resources are identified.",
      "processing_time": 15.41073489189148,
      "citing_paper_id": "278886852",
      "cited_paper_id": 8525940
    },
    {
      "context_text": "Recent works [41, 20] have extended AR models to vision and can be categorized as pixel-space AR [41, 42, 43, 44], token-based AR [45, 46, 47, 48] and scale-space AR [20, 49, 50, 51].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only categories of autoregressive models. No verifiable resources are identified.",
      "processing_time": 15.41073489189148,
      "citing_paper_id": "278886852",
      "cited_paper_id": 20282961
    },
    {
      "context_text": "Recent works [41, 20] have extended AR models to vision and can be categorized as pixel-space AR [41, 42, 43, 44], token-based AR [45, 46, 47, 48] and scale-space AR [20, 49, 50, 51].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only categories of autoregressive models. No verifiable resources are identified.",
      "processing_time": 15.41073489189148,
      "citing_paper_id": "278886852",
      "cited_paper_id": 232135095
    },
    {
      "context_text": "Recent works [41, 20] have extended AR models to vision and can be categorized as pixel-space AR [41, 42, 43, 44], token-based AR [45, 46, 47, 48] and scale-space AR [20, 49, 50, 51].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only categories of autoregressive models. No verifiable resources are identified.",
      "processing_time": 15.41073489189148,
      "citing_paper_id": "278886852",
      "cited_paper_id": 256868405
    },
    {
      "context_text": "Recent works [41, 20] have extended AR models to vision and can be categorized as pixel-space AR [41, 42, 43, 44], token-based AR [45, 46, 47, 48] and scale-space AR [20, 49, 50, 51].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only categories of autoregressive models. No verifiable resources are identified.",
      "processing_time": 15.41073489189148,
      "citing_paper_id": "278886852",
      "cited_paper_id": 268876071
    },
    {
      "context_text": "Recent works [41, 20] have extended AR models to vision and can be categorized as pixel-space AR [41, 42, 43, 44], token-based AR [45, 46, 47, 48] and scale-space AR [20, 49, 50, 51].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only categories of autoregressive models. No verifiable resources are identified.",
      "processing_time": 15.41073489189148,
      "citing_paper_id": "278886852",
      "cited_paper_id": 274116624
    },
    {
      "context_text": "Recent works [41, 20] have extended AR models to vision and can be categorized as pixel-space AR [41, 42, 43, 44], token-based AR [45, 46, 47, 48] and scale-space AR [20, 49, 50, 51].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only categories of autoregressive models. No verifiable resources are identified.",
      "processing_time": 15.41073489189148,
      "citing_paper_id": "278886852",
      "cited_paper_id": 277451671
    },
    {
      "context_text": "Early restoration models primarily addressed specific degradations [24, 25, 26, 27, 28, 29, 30, 31, 32, 33].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to papers addressing specific degradations. No dataset names are explicitly mentioned.",
      "processing_time": 30.280143976211548,
      "citing_paper_id": "278886852",
      "cited_paper_id": 8671030
    },
    {
      "context_text": "Early restoration models primarily addressed specific degradations [24, 25, 26, 27, 28, 29, 30, 31, 32, 33].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to papers addressing specific degradations. No dataset names are explicitly mentioned.",
      "processing_time": 30.280143976211548,
      "citing_paper_id": "278886852",
      "cited_paper_id": 49333383
    },
    {
      "context_text": "Early restoration models primarily addressed specific degradations [24, 25, 26, 27, 28, 29, 30, 31, 32, 33].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to papers addressing specific degradations. No dataset names are explicitly mentioned.",
      "processing_time": 30.280143976211548,
      "citing_paper_id": "278886852",
      "cited_paper_id": 91184545
    },
    {
      "context_text": "Early restoration models primarily addressed specific degradations [24, 25, 26, 27, 28, 29, 30, 31, 32, 33].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to papers addressing specific degradations. No dataset names are explicitly mentioned.",
      "processing_time": 30.280143976211548,
      "citing_paper_id": "278886852",
      "cited_paper_id": 195657934
    },
    {
      "context_text": "Early restoration models primarily addressed specific degradations [24, 25, 26, 27, 28, 29, 30, 31, 32, 33].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to papers addressing specific degradations. No dataset names are explicitly mentioned.",
      "processing_time": 30.280143976211548,
      "citing_paper_id": "278886852",
      "cited_paper_id": 202542259
    },
    {
      "context_text": "Early restoration models primarily addressed specific degradations [24, 25, 26, 27, 28, 29, 30, 31, 32, 33].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to papers addressing specific degradations. No dataset names are explicitly mentioned.",
      "processing_time": 30.280143976211548,
      "citing_paper_id": "278886852",
      "cited_paper_id": 226308542
    },
    {
      "context_text": "For deblurring, we used the GoPro [32] dataset comprising 2,103 training images and 1,111 test images.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "GoPro"
      ],
      "dataset_descriptions": {
        "GoPro": "Used for deblurring tasks, comprising 2,103 training images and 1,111 test images, focusing on dynamic scene deblurring."
      },
      "confidence_score": 1.0,
      "reasoning": "The context clearly mentions the GoPro dataset, which is a specific, verifiable dataset used for deblurring tasks.",
      "processing_time": 23.680901765823364,
      "citing_paper_id": "278886852",
      "cited_paper_id": 8671030
    },
    {
      "context_text": "Token-based AR compresses images into discrete latent codes via vector quantization (e.g., VQ-VAE [45], VQ-VAE-2 [53], VQGAN [54]) and then models code sequences with transformers (e.g., Image Transformer [55], ImageGPT [56]).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only models and methods. No verifiable resources are identified.",
      "processing_time": 13.232317447662354,
      "citing_paper_id": "278886852",
      "cited_paper_id": 20282961
    },
    {
      "context_text": "Token-based AR compresses images into discrete latent codes via vector quantization (e.g., VQ-VAE [45], VQ-VAE-2 [53], VQGAN [54]) and then models code sequences with transformers (e.g., Image Transformer [55], ImageGPT [56]).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only models and methods. No verifiable resources are identified.",
      "processing_time": 13.232317447662354,
      "citing_paper_id": "278886852",
      "cited_paper_id": 219781060
    },
    {
      "context_text": "Token-based AR compresses images into discrete latent codes via vector quantization (e.g., VQ-VAE [45], VQ-VAE-2 [53], VQGAN [54]) and then models code sequences with transformers (e.g., Image Transformer [55], ImageGPT [56]).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only models and methods. No verifiable resources are identified.",
      "processing_time": 13.232317447662354,
      "citing_paper_id": "278886852",
      "cited_paper_id": 229297973
    },
    {
      "context_text": "For dehazing, we used the RESIDE [62] dataset comprising 72135 training and 500 test images.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "RESIDE"
      ],
      "dataset_descriptions": {
        "RESIDE": "Used for dehazing experiments, comprising 72135 training and 500 test images to evaluate single-image dehazing methods."
      },
      "confidence_score": 1.0,
      "reasoning": "The context clearly mentions the use of the RESIDE dataset for dehazing experiments, providing specific details about the number of training and test images.",
      "processing_time": 37.95008826255798,
      "citing_paper_id": "278886852",
      "cited_paper_id": 39760169
    },
    {
      "context_text": "The LOLv1 [64] dataset was used for low-light enhancement, consisting of 485 training images and 15 test images.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "LOLv1"
      ],
      "dataset_descriptions": {
        "LOLv1": "Used for low-light image enhancement, specifically to train and test models on 485 training images and 15 test images."
      },
      "confidence_score": 1.0,
      "reasoning": "The context clearly mentions the use of the LOLv1 dataset for low-light image enhancement, providing specific details about the dataset's composition.",
      "processing_time": 23.944732189178467,
      "citing_paper_id": "278886852",
      "cited_paper_id": 52008443
    },
    {
      "context_text": "This paradigm has revolutionized natural language processing (NLP) and forms the foundation of large language models (LLMs) such as GPT-3 [18] and LLaMA [19].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only models and methods. No verifiable resources are identified.",
      "processing_time": 13.877326250076294,
      "citing_paper_id": "278886852",
      "cited_paper_id": 160025533
    },
    {
      "context_text": "This paradigm has revolutionized natural language processing (NLP) and forms the foundation of large language models (LLMs) such as GPT-3 [18] and LLaMA [19].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only models and methods. No verifiable resources are identified.",
      "processing_time": 13.877326250076294,
      "citing_paper_id": "278886852",
      "cited_paper_id": 257219404
    },
    {
      "context_text": "An alternative class of generative models are autoregressive models, which have driven rapid advances in natural language processing through large language models (LLMs) such as GPT-3 [18] and LLaMA [19].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only models and methods. No verifiable resources are identified.",
      "processing_time": 14.489202737808228,
      "citing_paper_id": "278886852",
      "cited_paper_id": 160025533
    },
    {
      "context_text": "An alternative class of generative models are autoregressive models, which have driven rapid advances in natural language processing through large language models (LLMs) such as GPT-3 [18] and LLaMA [19].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only models and methods. No verifiable resources are identified.",
      "processing_time": 14.489202737808228,
      "citing_paper_id": "278886852",
      "cited_paper_id": 257219404
    },
    {
      "context_text": "( 2) LDMs rely on variational autoencoders (VAEs) [17] which are primarily trained for generative diversity, rather than accurate pixel-level reconstruction.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a method (VAEs) and its training objective. There are no verifiable resources or datasets mentioned.",
      "processing_time": 17.495869159698486,
      "citing_paper_id": "278886852",
      "cited_paper_id": 211146177
    },
    {
      "context_text": "We additionally test the generalization capability of our model on real-world datasets, namely, LHP [65] ( 1000 images), REVIDE [66] ( 284 images), SICE [67] ( 565 images), TOLED [68] ( 30 images) and POLED [68] ( 30 images).",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "LHP",
        "REVIDE",
        "SICE",
        "TOLED",
        "POLED"
      ],
      "dataset_descriptions": {
        "LHP": "Used to test the generalization capability of the model on real-world hazy images, containing 1000 images.",
        "REVIDE": "Used to test the generalization capability of the model on real-world dehazing, containing 284 images.",
        "SICE": "Used to test the generalization capability of the model on real-world image enhancement, containing 565 images.",
        "TOLED": "Used to test the generalization capability of the model on real-world under-display camera images, containing 30 images.",
        "POLED": "Used to test the generalization capability of the model on real-world under-display camera images, containing 30 images."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions specific datasets used to test the generalization capability of a model in image restoration. These datasets are clearly named and have specific characteristics such as the number of images.",
      "processing_time": 48.0703980922699,
      "citing_paper_id": "278886852",
      "cited_paper_id": 212647851
    },
    {
      "context_text": "We additionally test the generalization capability of our model on real-world datasets, namely, LHP [65] ( 1000 images), REVIDE [66] ( 284 images), SICE [67] ( 565 images), TOLED [68] ( 30 images) and POLED [68] ( 30 images).",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "LHP",
        "REVIDE",
        "SICE",
        "TOLED",
        "POLED"
      ],
      "dataset_descriptions": {
        "LHP": "Used to test the generalization capability of the model on real-world hazy images, containing 1000 images.",
        "REVIDE": "Used to test the generalization capability of the model on real-world dehazing, containing 284 images.",
        "SICE": "Used to test the generalization capability of the model on real-world image enhancement, containing 565 images.",
        "TOLED": "Used to test the generalization capability of the model on real-world under-display camera images, containing 30 images.",
        "POLED": "Used to test the generalization capability of the model on real-world under-display camera images, containing 30 images."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions specific datasets used to test the generalization capability of a model in image restoration. These datasets are clearly named and have specific characteristics such as the number of images.",
      "processing_time": 48.0703980922699,
      "citing_paper_id": "278886852",
      "cited_paper_id": 235719890
    },
    {
      "context_text": "We additionally test the generalization capability of our model on real-world datasets, namely, LHP [65] ( 1000 images), REVIDE [66] ( 284 images), SICE [67] ( 565 images), TOLED [68] ( 30 images) and POLED [68] ( 30 images).",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "LHP",
        "REVIDE",
        "SICE",
        "TOLED",
        "POLED"
      ],
      "dataset_descriptions": {
        "LHP": "Used to test the generalization capability of the model on real-world hazy images, containing 1000 images.",
        "REVIDE": "Used to test the generalization capability of the model on real-world dehazing, containing 284 images.",
        "SICE": "Used to test the generalization capability of the model on real-world image enhancement, containing 565 images.",
        "TOLED": "Used to test the generalization capability of the model on real-world under-display camera images, containing 30 images.",
        "POLED": "Used to test the generalization capability of the model on real-world under-display camera images, containing 30 images."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions specific datasets used to test the generalization capability of a model in image restoration. These datasets are clearly named and have specific characteristics such as the number of images.",
      "processing_time": 48.0703980922699,
      "citing_paper_id": "278886852",
      "cited_paper_id": 260704481
    },
    {
      "context_text": "To improve fidelity and training stability, methods such as DiffUIR [10] and DA-CLIP [11] employed pixel-space diffusion models [12].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only methods and models. The cited papers' titles do not provide additional dataset information.",
      "processing_time": 15.408338069915771,
      "citing_paper_id": "278886852",
      "cited_paper_id": 219955663
    },
    {
      "context_text": "To improve fidelity and training stability, methods such as DiffUIR [10] and DA-CLIP [11] employed pixel-space diffusion models [12].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only methods and models. The cited papers' titles do not provide additional dataset information.",
      "processing_time": 15.408338069915771,
      "citing_paper_id": "278886852",
      "cited_paper_id": 268513542
    },
    {
      "context_text": "To improve fidelity and training stability, methods such as DiffUIR [10] and DA-CLIP [11] employed pixel-space diffusion models [12].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only methods and models. The cited papers' titles do not provide additional dataset information.",
      "processing_time": 15.408338069915771,
      "citing_paper_id": "278886852",
      "cited_paper_id": null
    },
    {
      "context_text": "For deraining, we used Rain13K [35] consisting of 13,711 training images and 4,298 test images.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Rain13K"
      ],
      "dataset_descriptions": {
        "Rain13K": "Used for deraining experiments, specifically evaluating image restoration models on 13,711 training and 4,298 test images."
      },
      "confidence_score": 1.0,
      "reasoning": "Rain13K is explicitly mentioned as a dataset used for deraining, with specific details about the number of training and test images.",
      "processing_time": 24.24406623840332,
      "citing_paper_id": "278886852",
      "cited_paper_id": 231802205
    },
    {
      "context_text": "Later methods such as Restormer [34], MPRNet [35] and SwinIR [36] introduced baseline architectures for any single restoration task.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only models and methods. There are no verifiable resources that meet the criteria for inclusion.",
      "processing_time": 20.118031978607178,
      "citing_paper_id": "278886852",
      "cited_paper_id": 231802205
    },
    {
      "context_text": "For testing generalization, we report MUSIQ [69] and CLIPIQA [70] scores in Input PromptIR [2] InstructIR [3] AWRaCLe [4] DCPT [39] RestoreVAR GT (on average), indicating better robustness under real degradations.",
      "catation_intent": "findings",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.0,
      "reasoning": "The context mentions several methods and models but does not explicitly refer to any specific datasets. The cited papers do not provide additional context to identify datasets.",
      "processing_time": 19.46669363975525,
      "citing_paper_id": "278886852",
      "cited_paper_id": 251040466
    },
    {
      "context_text": "For testing generalization, we report MUSIQ [69] and CLIPIQA [70] scores in Input PromptIR [2] InstructIR [3] AWRaCLe [4] DCPT [39] RestoreVAR GT (on average), indicating better robustness under real degradations.",
      "catation_intent": "findings",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.0,
      "reasoning": "The context mentions several methods and models but does not explicitly refer to any specific datasets. The cited papers do not provide additional context to identify datasets.",
      "processing_time": 19.46669363975525,
      "citing_paper_id": "278886852",
      "cited_paper_id": 267320695
    },
    {
      "context_text": "For testing generalization, we report MUSIQ [69] and CLIPIQA [70] scores in Input PromptIR [2] InstructIR [3] AWRaCLe [4] DCPT [39] RestoreVAR GT (on average), indicating better robustness under real degradations.",
      "catation_intent": "findings",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.0,
      "reasoning": "The context mentions several methods and models but does not explicitly refer to any specific datasets. The cited papers do not provide additional context to identify datasets.",
      "processing_time": 19.46669363975525,
      "citing_paper_id": "278886852",
      "cited_paper_id": 275921949
    },
    {
      "context_text": "AutoDIR [14] developed an automatic approach for degradation detection and restoration using an LDM. PixWizard [15] is a multi-task SD-XL [40] based model capable of performing AiOR among other tasks.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions AutoDIR and PixWizard, which are methods/models, and SD-XL, which is also a model. No specific datasets are mentioned.",
      "processing_time": 19.087578296661377,
      "citing_paper_id": "278886852",
      "cited_paper_id": 259341735
    },
    {
      "context_text": "AutoDIR [14] developed an automatic approach for degradation detection and restoration using an LDM. PixWizard [15] is a multi-task SD-XL [40] based model capable of performing AiOR among other tasks.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions AutoDIR and PixWizard, which are methods/models, and SD-XL, which is also a model. No specific datasets are mentioned.",
      "processing_time": 19.087578296661377,
      "citing_paper_id": "278886852",
      "cited_paper_id": 264145824
    },
    {
      "context_text": "AutoDIR [14] developed an automatic approach for degradation detection and restoration using an LDM. PixWizard [15] is a multi-task SD-XL [40] based model capable of performing AiOR among other tasks.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions AutoDIR and PixWizard, which are methods/models, and SD-XL, which is also a model. No specific datasets are mentioned.",
      "processing_time": 19.087578296661377,
      "citing_paper_id": "278886852",
      "cited_paper_id": 272826725
    },
    {
      "context_text": "In contrast, recent methods such as Diff-Plugin[13], AutoDIR[14], and PixWizard [15] leverage latent diffusion models (LDMs), such as Stable Diffusion [16].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context mentions several methods and models but does not refer to any specific datasets. The context is focused on describing the methods and their use of latent diffusion models.",
      "processing_time": 20.476963758468628,
      "citing_paper_id": "278886852",
      "cited_paper_id": 264145824
    },
    {
      "context_text": "In contrast, recent methods such as Diff-Plugin[13], AutoDIR[14], and PixWizard [15] leverage latent diffusion models (LDMs), such as Stable Diffusion [16].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context mentions several methods and models but does not refer to any specific datasets. The context is focused on describing the methods and their use of latent diffusion models.",
      "processing_time": 20.476963758468628,
      "citing_paper_id": "278886852",
      "cited_paper_id": 268230646
    },
    {
      "context_text": "In contrast, recent methods such as Diff-Plugin[13], AutoDIR[14], and PixWizard [15] leverage latent diffusion models (LDMs), such as Stable Diffusion [16].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context mentions several methods and models but does not refer to any specific datasets. The context is focused on describing the methods and their use of latent diffusion models.",
      "processing_time": 20.476963758468628,
      "citing_paper_id": "278886852",
      "cited_paper_id": 272826725
    },
    {
      "context_text": "To demonstrate these strengths, we evaluate generalization using no-reference image quality metrics (following prior works [13, 14, 4]), and assess perceptual realism through a user study.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only evaluation methods and a user study. No verifiable resources are identified.",
      "processing_time": 16.55737853050232,
      "citing_paper_id": "278886852",
      "cited_paper_id": 264145824
    },
    {
      "context_text": "To demonstrate these strengths, we evaluate generalization using no-reference image quality metrics (following prior works [13, 14, 4]), and assess perceptual realism through a user study.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only evaluation methods and a user study. No verifiable resources are identified.",
      "processing_time": 16.55737853050232,
      "citing_paper_id": "278886852",
      "cited_paper_id": 268230646
    },
    {
      "context_text": "Among generative methods, we compare with the latent diffusion model (LDM) based approaches Diff-Plugin [13], AutoDIR [14] and PixWizard [15].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions generative methods and compares them with latent diffusion model-based approaches, but does not specify any datasets. The cited papers' titles do not indicate the presence of datasets either.",
      "processing_time": 20.108681440353394,
      "citing_paper_id": "278886852",
      "cited_paper_id": 264145824
    },
    {
      "context_text": "Among generative methods, we compare with the latent diffusion model (LDM) based approaches Diff-Plugin [13], AutoDIR [14] and PixWizard [15].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions generative methods and compares them with latent diffusion model-based approaches, but does not specify any datasets. The cited papers' titles do not indicate the presence of datasets either.",
      "processing_time": 20.108681440353394,
      "citing_paper_id": "278886852",
      "cited_paper_id": 268230646
    },
    {
      "context_text": "Among generative methods, we compare with the latent diffusion model (LDM) based approaches Diff-Plugin [13], AutoDIR [14] and PixWizard [15].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions generative methods and compares them with latent diffusion model-based approaches, but does not specify any datasets. The cited papers' titles do not indicate the presence of datasets either.",
      "processing_time": 20.108681440353394,
      "citing_paper_id": "278886852",
      "cited_paper_id": 272826725
    },
    {
      "context_text": "Other approaches such as InstructIR [3] adopted textual guidance, and DCPT [39] proposed a novel pre-training strategy for AiOR.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only methods and models. The cited papers' titles do not provide additional dataset information.",
      "processing_time": 17.884411334991455,
      "citing_paper_id": "278886852",
      "cited_paper_id": 267320695
    },
    {
      "context_text": "Other approaches such as InstructIR [3] adopted textual guidance, and DCPT [39] proposed a novel pre-training strategy for AiOR.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only methods and models. The cited papers' titles do not provide additional dataset information.",
      "processing_time": 17.884411334991455,
      "citing_paper_id": "278886852",
      "cited_paper_id": 275921949
    },
    {
      "context_text": "For non-generative approaches, we include PromptIR [2], InstructIR [3], AWRaCLe [4] and DCPT [39].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context mentions several methods/models (PromptIR, InstructIR, AWRaCLe, DCPT) but does not reference any specific datasets. The cited paper titles do not provide additional context to identify datasets.",
      "processing_time": 21.686781406402588,
      "citing_paper_id": "278886852",
      "cited_paper_id": 267320695
    },
    {
      "context_text": "For non-generative approaches, we include PromptIR [2], InstructIR [3], AWRaCLe [4] and DCPT [39].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context mentions several methods/models (PromptIR, InstructIR, AWRaCLe, DCPT) but does not reference any specific datasets. The cited paper titles do not provide additional context to identify datasets.",
      "processing_time": 21.686781406402588,
      "citing_paper_id": "278886852",
      "cited_paper_id": 275921949
    },
    {
      "context_text": "Non-generative models such as AirNet [1], PromptIR [2], InstructIR [3], AWRaCLe [4], and AdaIR [5], deterministically map degraded images to their clean counterparts.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only models and methods. No dataset names are present in the text.",
      "processing_time": 28.930392503738403,
      "citing_paper_id": "278886852",
      "cited_paper_id": 267320695
    },
    {
      "context_text": "Diff-Plugin [13] adopts task plugins to guide an LDM for AiOR.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (Diff-Plugin) and a task (AiOR).",
      "processing_time": 19.095782041549683,
      "citing_paper_id": "278886852",
      "cited_paper_id": 268230646
    },
    {
      "context_text": "Pixel-space diffusion models (PSDMs) such as DA-CLIP [11] and DiffUIR [10] demonstrated improved AiOR performance.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models and methods. The context focuses on the performance of PSDMs in AiOR, which is not sufficient to identify a dataset.",
      "processing_time": 19.09305739402771,
      "citing_paper_id": "278886852",
      "cited_paper_id": 268513542
    },
    {
      "context_text": "Pixel-space diffusion models (PSDMs) such as DA-CLIP [11] and DiffUIR [10] demonstrated improved AiOR performance.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models and methods. The context focuses on the performance of PSDMs in AiOR, which is not sufficient to identify a dataset.",
      "processing_time": 19.09305739402771,
      "citing_paper_id": "278886852",
      "cited_paper_id": null
    },
    {
      "context_text": "However, LDM-based approaches are slow at inference time—a limitation we aim to overcome using visual autoregressive modeling (VAR) [20].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (LDM) and a proposed alternative (VAR).",
      "processing_time": 15.382468938827515,
      "citing_paper_id": "278886852",
      "cited_paper_id": 268876071
    },
    {
      "context_text": "Scale-space AR, as introduced in VAR [20], generates latents hierarchically from coarse to fine scales and matches the quality of Diffusion Transformers [21] at a fraction of the inference cost[20].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models and methods. The context focuses on comparing the performance and efficiency of different models.",
      "processing_time": 18.503572463989258,
      "citing_paper_id": "278886852",
      "cited_paper_id": 268876071
    },
    {
      "context_text": "Recently, Visual AutoRegressive (VAR) Modeling [20] introduced a scale-space autoregressive framework for image generation, performing next-scale prediction in the latent space of a multi-scale vector-quantized VAE (VQVAE).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a method (Visual AutoRegressive Modeling) and a model (multi-scale vector-quantized VAE).",
      "processing_time": 19.086786031723022,
      "citing_paper_id": "278886852",
      "cited_paper_id": 268876071
    },
    {
      "context_text": "To analyze its impact, we compare four RestoreVAR variants: (i) No refiner, (ii) HART’s diffusion refiner [57], (iii) LRT without final block outputs, and (iv) our proposed LRT.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only model variants and methods. There are no clear identifiers for datasets in the provided context.",
      "processing_time": 14.760074615478516,
      "citing_paper_id": "278886852",
      "cited_paper_id": 273346791
    },
    {
      "context_text": "HART [57] addressed VAE-induced distortions by fine-tuning the VAE decoder on both discrete and continuous latents.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (VAE) and a technique (fine-tuning).",
      "processing_time": 17.009538888931274,
      "citing_paper_id": "278886852",
      "cited_paper_id": 273346791
    },
    {
      "context_text": "HART [57] scales VAR to 1024 × 1024 synthesis by using a lightweight MLP-based diffusion refiner to convert discrete VAR latents into continuous representations.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (HART) and its application in scaling VAR synthesis. No verifiable datasets are referenced.",
      "processing_time": 19.8920156955719,
      "citing_paper_id": "278886852",
      "cited_paper_id": 273346791
    },
    {
      "context_text": "To the best of our knowledge, only two prior works—VarSR [22] and Varformer [23]—have attempted to use VAR for image restoration.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only prior works that have attempted to use VAR for image restoration.",
      "processing_time": 15.790496349334717,
      "citing_paper_id": "278886852",
      "cited_paper_id": 276079427
    },
    {
      "context_text": "Only two prior works-VarSR [22] and Varformer [23]-have exploited VAR’s generative priors for image restoration.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models (VarSR and Varformer). No verifiable resources are identified.",
      "processing_time": 15.045651197433472,
      "citing_paper_id": "278886852",
      "cited_paper_id": 276079427
    },
    {
      "context_text": "CNN-based methods [5, 16, 21, 23, 24, 57, 66–68] utilize convolutions for fast, efficient processing but struggle to capture global dependencies.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only CNN-based methods and their limitations. No dataset names are present in the text.",
      "processing_time": 17.828134775161743,
      "citing_paper_id": "277271537",
      "cited_paper_id": 996788
    },
    {
      "context_text": "CNN-based methods [5, 16, 21, 23, 24, 57, 66–68] utilize convolutions for fast, efficient processing but struggle to capture global dependencies.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only CNN-based methods and their limitations. No dataset names are present in the text.",
      "processing_time": 17.828134775161743,
      "citing_paper_id": "277271537",
      "cited_paper_id": 195657934
    },
    {
      "context_text": "CNN-based methods [5, 16, 21, 23, 24, 57, 66–68] utilize convolutions for fast, efficient processing but struggle to capture global dependencies.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only CNN-based methods and their limitations. No dataset names are present in the text.",
      "processing_time": 17.828134775161743,
      "citing_paper_id": "277271537",
      "cited_paper_id": 199543931
    },
    {
      "context_text": "CNN-based methods [5, 16, 21, 23, 24, 57, 66–68] utilize convolutions for fast, efficient processing but struggle to capture global dependencies.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only CNN-based methods and their limitations. No dataset names are present in the text.",
      "processing_time": 17.828134775161743,
      "citing_paper_id": "277271537",
      "cited_paper_id": null
    },
    {
      "context_text": "Deep learning has significantly advanced performance across various restoration tasks, including denoising [5, 6, 29, 34, 60, 63, 66–68], deraining [16, 21, 57], dehazing [4, 12, 31], deblurring [23, 24, 35, 52, 64], and low light enhancement [16, 44, 51, 59, 69].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various image restoration tasks and related papers. No verifiable resources are identified.",
      "processing_time": 60.04781484603882,
      "citing_paper_id": "277271537",
      "cited_paper_id": 996788
    },
    {
      "context_text": "Deep learning has significantly advanced performance across various restoration tasks, including denoising [5, 6, 29, 34, 60, 63, 66–68], deraining [16, 21, 57], dehazing [4, 12, 31], deblurring [23, 24, 35, 52, 64], and low light enhancement [16, 44, 51, 59, 69].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various image restoration tasks and related papers. No verifiable resources are identified.",
      "processing_time": 60.04781484603882,
      "citing_paper_id": "277271537",
      "cited_paper_id": 8671030
    },
    {
      "context_text": "Deep learning has significantly advanced performance across various restoration tasks, including denoising [5, 6, 29, 34, 60, 63, 66–68], deraining [16, 21, 57], dehazing [4, 12, 31], deblurring [23, 24, 35, 52, 64], and low light enhancement [16, 44, 51, 59, 69].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various image restoration tasks and related papers. No verifiable resources are identified.",
      "processing_time": 60.04781484603882,
      "citing_paper_id": "277271537",
      "cited_paper_id": 14092238
    },
    {
      "context_text": "Deep learning has significantly advanced performance across various restoration tasks, including denoising [5, 6, 29, 34, 60, 63, 66–68], deraining [16, 21, 57], dehazing [4, 12, 31], deblurring [23, 24, 35, 52, 64], and low light enhancement [16, 44, 51, 59, 69].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various image restoration tasks and related papers. No verifiable resources are identified.",
      "processing_time": 60.04781484603882,
      "citing_paper_id": "277271537",
      "cited_paper_id": 84843405
    },
    {
      "context_text": "Deep learning has significantly advanced performance across various restoration tasks, including denoising [5, 6, 29, 34, 60, 63, 66–68], deraining [16, 21, 57], dehazing [4, 12, 31], deblurring [23, 24, 35, 52, 64], and low light enhancement [16, 44, 51, 59, 69].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various image restoration tasks and related papers. No verifiable resources are identified.",
      "processing_time": 60.04781484603882,
      "citing_paper_id": "277271537",
      "cited_paper_id": 150373946
    },
    {
      "context_text": "Deep learning has significantly advanced performance across various restoration tasks, including denoising [5, 6, 29, 34, 60, 63, 66–68], deraining [16, 21, 57], dehazing [4, 12, 31], deblurring [23, 24, 35, 52, 64], and low light enhancement [16, 44, 51, 59, 69].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various image restoration tasks and related papers. No verifiable resources are identified.",
      "processing_time": 60.04781484603882,
      "citing_paper_id": "277271537",
      "cited_paper_id": 195657934
    },
    {
      "context_text": "Deep learning has significantly advanced performance across various restoration tasks, including denoising [5, 6, 29, 34, 60, 63, 66–68], deraining [16, 21, 57], dehazing [4, 12, 31], deblurring [23, 24, 35, 52, 64], and low light enhancement [16, 44, 51, 59, 69].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various image restoration tasks and related papers. No verifiable resources are identified.",
      "processing_time": 60.04781484603882,
      "citing_paper_id": "277271537",
      "cited_paper_id": 199543931
    },
    {
      "context_text": "Deep learning has significantly advanced performance across various restoration tasks, including denoising [5, 6, 29, 34, 60, 63, 66–68], deraining [16, 21, 57], dehazing [4, 12, 31], deblurring [23, 24, 35, 52, 64], and low light enhancement [16, 44, 51, 59, 69].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various image restoration tasks and related papers. No verifiable resources are identified.",
      "processing_time": 60.04781484603882,
      "citing_paper_id": "277271537",
      "cited_paper_id": 206596913
    },
    {
      "context_text": "Deep learning has significantly advanced performance across various restoration tasks, including denoising [5, 6, 29, 34, 60, 63, 66–68], deraining [16, 21, 57], dehazing [4, 12, 31], deblurring [23, 24, 35, 52, 64], and low light enhancement [16, 44, 51, 59, 69].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various image restoration tasks and related papers. No verifiable resources are identified.",
      "processing_time": 60.04781484603882,
      "citing_paper_id": "277271537",
      "cited_paper_id": 216562731
    },
    {
      "context_text": "Deep learning has significantly advanced performance across various restoration tasks, including denoising [5, 6, 29, 34, 60, 63, 66–68], deraining [16, 21, 57], dehazing [4, 12, 31], deblurring [23, 24, 35, 52, 64], and low light enhancement [16, 44, 51, 59, 69].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various image restoration tasks and related papers. No verifiable resources are identified.",
      "processing_time": 60.04781484603882,
      "citing_paper_id": "277271537",
      "cited_paper_id": 237266491
    },
    {
      "context_text": "Deep learning has significantly advanced performance across various restoration tasks, including denoising [5, 6, 29, 34, 60, 63, 66–68], deraining [16, 21, 57], dehazing [4, 12, 31], deblurring [23, 24, 35, 52, 64], and low light enhancement [16, 44, 51, 59, 69].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various image restoration tasks and related papers. No verifiable resources are identified.",
      "processing_time": 60.04781484603882,
      "citing_paper_id": "277271537",
      "cited_paper_id": 248085491
    },
    {
      "context_text": "Deep learning has significantly advanced performance across various restoration tasks, including denoising [5, 6, 29, 34, 60, 63, 66–68], deraining [16, 21, 57], dehazing [4, 12, 31], deblurring [23, 24, 35, 52, 64], and low light enhancement [16, 44, 51, 59, 69].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various image restoration tasks and related papers. No verifiable resources are identified.",
      "processing_time": 60.04781484603882,
      "citing_paper_id": "277271537",
      "cited_paper_id": 260926685
    },
    {
      "context_text": "Deep learning has significantly advanced performance across various restoration tasks, including denoising [5, 6, 29, 34, 60, 63, 66–68], deraining [16, 21, 57], dehazing [4, 12, 31], deblurring [23, 24, 35, 52, 64], and low light enhancement [16, 44, 51, 59, 69].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various image restoration tasks and related papers. No verifiable resources are identified.",
      "processing_time": 60.04781484603882,
      "citing_paper_id": "277271537",
      "cited_paper_id": null
    },
    {
      "context_text": "The final mask M is predicted by applying convolutions over the concatenation of these features: During training, we employ gumbel softmax [20] on M to sample hard and easy patches.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (gumbel softmax) used during training.",
      "processing_time": 58.93440079689026,
      "citing_paper_id": "277271537",
      "cited_paper_id": 2428314
    },
    {
      "context_text": "Our framework builds upon a U-shaped architecture [39, 47, 62] with an asymmetric transformer-based encoder-decoder design, as illustrated in Fig.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only architectural designs and models. No verifiable datasets are referenced.",
      "processing_time": 59.418044328689575,
      "citing_paper_id": "277271537",
      "cited_paper_id": 3719281
    },
    {
      "context_text": "Our framework builds upon a U-shaped architecture [39, 47, 62] with an asymmetric transformer-based encoder-decoder design, as illustrated in Fig.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only architectural designs and models. No verifiable datasets are referenced.",
      "processing_time": 59.418044328689575,
      "citing_paper_id": "277271537",
      "cited_paper_id": 244346144
    },
    {
      "context_text": "• Dehazing : The SOTS dataset [26] provides 72,135 training images and 500 testing images.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "SOTS"
      ],
      "dataset_descriptions": {
        "SOTS": "Used to train and test dehazing algorithms, providing 72,135 training images and 500 testing images for benchmarking single-image dehazing methods."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions the SOTS dataset, which is a specific dataset used for dehazing research. It provides a clear number of training and testing images.",
      "processing_time": 69.22591257095337,
      "citing_paper_id": "277271537",
      "cited_paper_id": 39760169
    },
    {
      "context_text": "NAFNet [5] TAPE [26] Restormer [59] AirNet [24] IDR [61] Cat-AIR (ours) GT D e no i s e D e r a i n D e h aze D e b l u r L o w - li gh t Figure 5.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only models and methods. No dataset names are present in the citation span.",
      "processing_time": 59.58196783065796,
      "citing_paper_id": "277271537",
      "cited_paper_id": 39760169
    },
    {
      "context_text": "NAFNet [5] TAPE [26] Restormer [59] AirNet [24] IDR [61] Cat-AIR (ours) GT D e no i s e D e r a i n D e h aze D e b l u r L o w - li gh t Figure 5.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only models and methods. No dataset names are present in the citation span.",
      "processing_time": 59.58196783065796,
      "citing_paper_id": "277271537",
      "cited_paper_id": 199543931
    },
    {
      "context_text": "…features with limited global context, we propose an adaptive channel attention scheme for (1) that balances efficiency and expressiveness: using lightweight squeeze-and-excitation (SE) [6, 17] in shallow layers while deploying sophisticated self-attention mechanisms [63] in the bottleneck layers.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only methods and architectures. The cited papers do not introduce new datasets either.",
      "processing_time": 59.39788508415222,
      "citing_paper_id": "277271537",
      "cited_paper_id": 140309863
    },
    {
      "context_text": "…features with limited global context, we propose an adaptive channel attention scheme for (1) that balances efficiency and expressiveness: using lightweight squeeze-and-excitation (SE) [6, 17] in shallow layers while deploying sophisticated self-attention mechanisms [63] in the bottleneck layers.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only methods and architectures. The cited papers do not introduce new datasets either.",
      "processing_time": 59.39788508415222,
      "citing_paper_id": "277271537",
      "cited_paper_id": 248085491
    },
    {
      "context_text": "Evaluation uses BSD68 [3], Urban100 [18], and Kodak24 [15].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "BSD68",
        "Urban100",
        "Kodak24"
      ],
      "dataset_descriptions": {
        "BSD68": "Used to evaluate image denoising performance, focusing on natural image restoration and enhancement techniques.",
        "Urban100": "Used to assess image super-resolution quality, specifically targeting urban scene images with complex structures.",
        "Kodak24": "Used to evaluate image restoration algorithms, providing a benchmark for color image quality and detail preservation."
      },
      "confidence_score": 1.0,
      "reasoning": "The citation clearly mentions three datasets used for evaluation: BSD68, Urban100, and Kodak24. These are specific datasets commonly used in image restoration and enhancement tasks.",
      "processing_time": 78.2681450843811,
      "citing_paper_id": "277271537",
      "cited_paper_id": 206764694
    },
    {
      "context_text": "This specialization limits their practical utility in critical applications such as autonomous driving [9, 42] and night-time surveillance [25], where multiple, unpredictable degradations often co-exist [2, 58, 59].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only general applications and challenges in image restoration. No dataset names are present in the citation span.",
      "processing_time": 60.32986664772034,
      "citing_paper_id": "277271537",
      "cited_paper_id": 212737191
    },
    {
      "context_text": "This specialization limits their practical utility in critical applications such as autonomous driving [9, 42] and night-time surveillance [25], where multiple, unpredictable degradations often co-exist [2, 58, 59].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only general applications and challenges in image restoration. No dataset names are present in the citation span.",
      "processing_time": 60.32986664772034,
      "citing_paper_id": "277271537",
      "cited_paper_id": 222103869
    },
    {
      "context_text": "This specialization limits their practical utility in critical applications such as autonomous driving [9, 42] and night-time surveillance [25], where multiple, unpredictable degradations often co-exist [2, 58, 59].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only general applications and challenges in image restoration. No dataset names are present in the citation span.",
      "processing_time": 60.32986664772034,
      "citing_paper_id": "277271537",
      "cited_paper_id": 235640808
    },
    {
      "context_text": "This specialization limits their practical utility in critical applications such as autonomous driving [9, 42] and night-time surveillance [25], where multiple, unpredictable degradations often co-exist [2, 58, 59].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only general applications and challenges in image restoration. No dataset names are present in the citation span.",
      "processing_time": 60.32986664772034,
      "citing_paper_id": "277271537",
      "cited_paper_id": 244714491
    },
    {
      "context_text": "Our network features a 4-level encoder-decoder structure with alternating channel and spatial attention Transformer blocks, distributed as [2 , 4 , 4 , 4] across levels.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only the architecture details of the network.",
      "processing_time": 58.22576951980591,
      "citing_paper_id": "277271537",
      "cited_paper_id": 222103869
    },
    {
      "context_text": "While Transformer-based architectures [6, 29, 46, 63] excel at modeling global dependencies through self-attention, their quadratic complexity presents challenges in resource-limited settings.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only models and their limitations. No dataset names are present in the text.",
      "processing_time": 59.61103296279907,
      "citing_paper_id": "277271537",
      "cited_paper_id": 235358213
    },
    {
      "context_text": "While Transformer-based architectures [6, 29, 46, 63] excel at modeling global dependencies through self-attention, their quadratic complexity presents challenges in resource-limited settings.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only models and their limitations. No dataset names are present in the text.",
      "processing_time": 59.61103296279907,
      "citing_paper_id": "277271537",
      "cited_paper_id": 237266491
    },
    {
      "context_text": "While Transformer-based architectures [6, 29, 46, 63] excel at modeling global dependencies through self-attention, their quadratic complexity presents challenges in resource-limited settings.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only models and their limitations. No dataset names are present in the text.",
      "processing_time": 59.61103296279907,
      "citing_paper_id": "277271537",
      "cited_paper_id": 248085491
    },
    {
      "context_text": "While deep learning has revolutionized this field with remarkable results [6, 10, 29, 47, 62, 70], most existing meth-ods are designed for specific degradation types, requiring prior knowledge of the corruption source.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general statements about deep learning methods and their limitations in image restoration.",
      "processing_time": 59.477508544921875,
      "citing_paper_id": "277271537",
      "cited_paper_id": 237266491
    },
    {
      "context_text": "While deep learning has revolutionized this field with remarkable results [6, 10, 29, 47, 62, 70], most existing meth-ods are designed for specific degradation types, requiring prior knowledge of the corruption source.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general statements about deep learning methods and their limitations in image restoration.",
      "processing_time": 59.477508544921875,
      "citing_paper_id": "277271537",
      "cited_paper_id": 244346144
    },
    {
      "context_text": "While deep learning has revolutionized this field with remarkable results [6, 10, 29, 47, 62, 70], most existing meth-ods are designed for specific degradation types, requiring prior knowledge of the corruption source.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general statements about deep learning methods and their limitations in image restoration.",
      "processing_time": 59.477508544921875,
      "citing_paper_id": "277271537",
      "cited_paper_id": 248085491
    },
    {
      "context_text": "While deep learning has revolutionized this field with remarkable results [6, 10, 29, 47, 62, 70], most existing meth-ods are designed for specific degradation types, requiring prior knowledge of the corruption source.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general statements about deep learning methods and their limitations in image restoration.",
      "processing_time": 59.477508544921875,
      "citing_paper_id": "277271537",
      "cited_paper_id": 267320695
    },
    {
      "context_text": "Following [6, 48, 62], we use ℓ 1 loss for high-quality image restoration.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only the use of ℓ1 loss for image restoration. No verifiable resources are identified.",
      "processing_time": 59.87135076522827,
      "citing_paper_id": "277271537",
      "cited_paper_id": 244346144
    },
    {
      "context_text": "Following [6, 48, 62], we use ℓ 1 loss for high-quality image restoration.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only the use of ℓ1 loss for image restoration. No verifiable resources are identified.",
      "processing_time": 59.87135076522827,
      "citing_paper_id": "277271537",
      "cited_paper_id": 248085491
    },
    {
      "context_text": "We compare against both specialized models (Restormer [63] and NAFNet [6], which use separate models for each degradation type) and all-in-one methods (TAPE [30], AirNet [28], and IDR [65]).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only models and methods. No verifiable resources are identified.",
      "processing_time": 58.74661993980408,
      "citing_paper_id": "277271537",
      "cited_paper_id": 248085491
    },
    {
      "context_text": "We compare against both specialized models (Restormer [63] and NAFNet [6], which use separate models for each degradation type) and all-in-one methods (TAPE [30], AirNet [28], and IDR [65]).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only models and methods. No verifiable resources are identified.",
      "processing_time": 58.74661993980408,
      "citing_paper_id": "277271537",
      "cited_paper_id": 260085391
    },
    {
      "context_text": "Notably, our model achieves the best PSNR on Rain100L, SOTS, GoPro, and LOL datasets, with improvements of 2.58 dB, 5.64 dB, 1.04 dB, and 2.12 dB over the previous best model, IDR [65].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Rain100L",
        "SOTS",
        "GoPro",
        "LOL"
      ],
      "dataset_descriptions": {
        "Rain100L": "Used to evaluate the model's performance in rain removal, achieving a 2.58 dB improvement in PSNR over the previous best model.",
        "SOTS": "Used to assess the model's effectiveness in dehazing, showing a 5.64 dB improvement in PSNR compared to the previous best model.",
        "GoPro": "Used to test the model's capability in motion deblurring, with a 1.04 dB improvement in PSNR over the previous best model.",
        "LOL": "Used to evaluate low-light image enhancement, demonstrating a 2.12 dB improvement in PSNR over the previous best model."
      },
      "confidence_score": 1.0,
      "reasoning": "The citation mentions specific datasets used for evaluating the performance of the model in image restoration tasks.",
      "processing_time": 87.05577540397644,
      "citing_paper_id": "277271537",
      "cited_paper_id": 260085391
    },
    {
      "context_text": "Among blind all-in-one methods, AirNet [27] uses contrastive learning for degradation detection, while IDR [65] employs meta-learning to decompose degradations.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods (AirNet, IDR). No dataset names are present in the citation span.",
      "processing_time": 59.86851406097412,
      "citing_paper_id": "277271537",
      "cited_paper_id": 260085391
    },
    {
      "context_text": "Recent research has explored all-in-one image restoration approaches [27, 37, 43, 65] for handling multiple restoration tasks within a unified framework.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general research approaches. No dataset names are present in the text.",
      "processing_time": 59.543553829193115,
      "citing_paper_id": "277271537",
      "cited_paper_id": 260085391
    },
    {
      "context_text": "Recent research has explored all-in-one image restoration approaches [27, 37, 43, 65] for handling multiple restoration tasks within a unified framework.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general research approaches. No dataset names are present in the text.",
      "processing_time": 59.543553829193115,
      "citing_paper_id": "277271537",
      "cited_paper_id": 268030812
    },
    {
      "context_text": "Further extending to light enhancement, we surpass previous methods [65] trained on all five tasks by more than 1dB PSNR on average, demonstrating Cat-AIR’s exceptional scalability and robustness.",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a comparison of performance metrics. No dataset names are provided in the context.",
      "processing_time": 59.704522132873535,
      "citing_paper_id": "277271537",
      "cited_paper_id": 260085391
    },
    {
      "context_text": "X-Restormer X-Restormer [7] employs alternating spatial and channel attention, but it incurs high computational costs.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (X-Restormer) and its computational costs.",
      "processing_time": 59.702338218688965,
      "citing_paper_id": "277271537",
      "cited_paper_id": 264289165
    },
    {
      "context_text": "Unlike InstructIR [10] and In-structIPT [41], which require additional text instructions as prior information, we focus on blind restoration without explicit degradation information.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only references to methods or models. The focus is on comparing approaches to image restoration.",
      "processing_time": 60.00499653816223,
      "citing_paper_id": "277271537",
      "cited_paper_id": 267320695
    },
    {
      "context_text": "Unlike InstructIR [10] and In-structIPT [41], which require additional text instructions as prior information, we focus on blind restoration without explicit degradation information.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only references to methods or models. The focus is on comparing approaches to image restoration.",
      "processing_time": 60.00499653816223,
      "citing_paper_id": "277271537",
      "cited_paper_id": 270869793
    },
    {
      "context_text": "Unlike previous approaches [10, 37] that often require training new models from scratch on combined datasets, we pro-pose a more efficient method leveraging pre-trained models.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general reference to 'combined datasets' without naming them.",
      "processing_time": 59.25425863265991,
      "citing_paper_id": "277271537",
      "cited_paper_id": 267320695
    },
    {
      "context_text": "Observing that different image regions require varying levels of computational resources for effective restoration, and inspired by [45], we develop a content-aware spatial attention mechanism that dynamically allocates computational effort based on region complexity.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method or approach. The context focuses on the development of a content-aware spatial attention mechanism.",
      "processing_time": 60.64058589935303,
      "citing_paper_id": "277271537",
      "cited_paper_id": 268063754
    },
    {
      "context_text": "Inspired by [38, 45, 53], we add a regularization term to align the mask ratio γ j with γ 0 .",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a regularization term in a method. No verifiable resources are identified.",
      "processing_time": 59.58722734451294,
      "citing_paper_id": "277271537",
      "cited_paper_id": 268063754
    },
    {
      "context_text": "Recently, CAMixerSR [45] introduces a similar mixer but is limited to the task of super-resolution.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (CAMixerSR) and its limitation to super-resolution tasks.",
      "processing_time": 59.651957750320435,
      "citing_paper_id": "277271537",
      "cited_paper_id": 268063754
    },
    {
      "context_text": "State Space Model (SSM), originally introduced in the 1960s for control systems[33], has recently gained significant attention in computer vision due to its ability to model long-range dependencies with linear computational complexity.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a method (State Space Model) and its applications. No verifiable resources are identified.",
      "processing_time": 60.17162895202637,
      "citing_paper_id": "278033660",
      "cited_paper_id": 1242324
    },
    {
      "context_text": "(3 ) For deblurring, we compare against DeepDeblur [54], DeblurGAN [36], DeblurGANv2 [37], MT-RNN [59], and IR-SDE [49].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models and methods used for deblurring. No verifiable resources are identified.",
      "processing_time": 60.16749978065491,
      "citing_paper_id": "278033660",
      "cited_paper_id": 4552226
    },
    {
      "context_text": "For de-blurring on GoPro [53], it attains 31.42dB PSNR and 0.948 SSIM, exceeding MT-RNN by 0.27dB and 0.003.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "GoPro"
      ],
      "dataset_descriptions": {
        "GoPro": "Used to evaluate de-blurring performance, comparing PSNR and SSIM metrics against MT-RNN, focusing on dynamic scene de-blurring."
      },
      "confidence_score": 0.9,
      "reasoning": "The context mentions 'GoPro' which is likely referring to the GoPro dataset used for de-blurring experiments. The dataset is used to evaluate the performance of the proposed method against another model (MT-RNN).",
      "processing_time": 70.83943390846252,
      "citing_paper_id": "278033660",
      "cited_paper_id": 8671030
    },
    {
      "context_text": "Additionally, we evaluate performance on Degradation-specific tasks, using the LOL dataset[87] for low-light image enhancement, the GoPro dataset[53] for image deblurring, the Rain100H dataset[98] for image deraining, and the RESIDE6K dataset[71] for image dehazing.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "LOL dataset",
        "GoPro dataset",
        "Rain100H dataset",
        "RESIDE6K dataset"
      ],
      "dataset_descriptions": {
        "LOL dataset": "Used for evaluating low-light image enhancement, focusing on improving visibility in dark images through a multi-scale CNN approach.",
        "GoPro dataset": "Used for evaluating image deblurring, specifically addressing motion blur in dynamic scenes using a deep multi-scale convolutional neural network.",
        "Rain100H dataset": "Used for evaluating image deraining, focusing on removing rain streaks from images to improve clarity and detail.",
        "RESIDE6K dataset": "Used for evaluating image dehazing, specifically addressing the removal of haze to enhance image quality and visibility."
      },
      "confidence_score": 1.0,
      "reasoning": "The citation mentions specific datasets used for evaluating performance on various image restoration tasks, which are directly relevant to the research topic.",
      "processing_time": 86.05244135856628,
      "citing_paper_id": "278033660",
      "cited_paper_id": 8671030
    },
    {
      "context_text": "Additionally, we evaluate performance on Degradation-specific tasks, using the LOL dataset[87] for low-light image enhancement, the GoPro dataset[53] for image deblurring, the Rain100H dataset[98] for image deraining, and the RESIDE6K dataset[71] for image dehazing.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "LOL dataset",
        "GoPro dataset",
        "Rain100H dataset",
        "RESIDE6K dataset"
      ],
      "dataset_descriptions": {
        "LOL dataset": "Used for evaluating low-light image enhancement, focusing on improving visibility in dark images through a multi-scale CNN approach.",
        "GoPro dataset": "Used for evaluating image deblurring, specifically addressing motion blur in dynamic scenes using a deep multi-scale convolutional neural network.",
        "Rain100H dataset": "Used for evaluating image deraining, focusing on removing rain streaks from images to improve clarity and detail.",
        "RESIDE6K dataset": "Used for evaluating image dehazing, specifically addressing the removal of haze to enhance image quality and visibility."
      },
      "confidence_score": 1.0,
      "reasoning": "The citation mentions specific datasets used for evaluating performance on various image restoration tasks, which are directly relevant to the research topic.",
      "processing_time": 86.05244135856628,
      "citing_paper_id": "278033660",
      "cited_paper_id": 208138077
    },
    {
      "context_text": "Additionally, we evaluate performance on Degradation-specific tasks, using the LOL dataset[87] for low-light image enhancement, the GoPro dataset[53] for image deblurring, the Rain100H dataset[98] for image deraining, and the RESIDE6K dataset[71] for image dehazing.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "LOL dataset",
        "GoPro dataset",
        "Rain100H dataset",
        "RESIDE6K dataset"
      ],
      "dataset_descriptions": {
        "LOL dataset": "Used for evaluating low-light image enhancement, focusing on improving visibility in dark images through a multi-scale CNN approach.",
        "GoPro dataset": "Used for evaluating image deblurring, specifically addressing motion blur in dynamic scenes using a deep multi-scale convolutional neural network.",
        "Rain100H dataset": "Used for evaluating image deraining, focusing on removing rain streaks from images to improve clarity and detail.",
        "RESIDE6K dataset": "Used for evaluating image dehazing, specifically addressing the removal of haze to enhance image quality and visibility."
      },
      "confidence_score": 1.0,
      "reasoning": "The citation mentions specific datasets used for evaluating performance on various image restoration tasks, which are directly relevant to the research topic.",
      "processing_time": 86.05244135856628,
      "citing_paper_id": "278033660",
      "cited_paper_id": null
    },
    {
      "context_text": "Traditional methods, such as Dark Channel Prior [23] and Color Line Prior [16], rely on handcrafted priors to model degradation but struggle with complex scenarios due to predefined assumptions.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods. The context focuses on describing traditional methods and their limitations.",
      "processing_time": 59.53496170043945,
      "citing_paper_id": "278033660",
      "cited_paper_id": 49333383
    },
    {
      "context_text": "(4 ) For dehazing, we compare against GCANet [3], GridDehazeNet [46], DeHaze-Former [76], MAXIM [79], and DA-CLIP [48].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only models and methods used for dehazing. No verifiable resources are identified.",
      "processing_time": 60.57230043411255,
      "citing_paper_id": "278033660",
      "cited_paper_id": 53755281
    },
    {
      "context_text": "(4 ) For dehazing, we compare against GCANet [3], GridDehazeNet [46], DeHaze-Former [76], MAXIM [79], and DA-CLIP [48].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only models and methods used for dehazing. No verifiable resources are identified.",
      "processing_time": 60.57230043411255,
      "citing_paper_id": "278033660",
      "cited_paper_id": 245837508
    },
    {
      "context_text": "(4 ) For dehazing, we compare against GCANet [3], GridDehazeNet [46], DeHaze-Former [76], MAXIM [79], and DA-CLIP [48].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only models and methods used for dehazing. No verifiable resources are identified.",
      "processing_time": 60.57230043411255,
      "citing_paper_id": "278033660",
      "cited_paper_id": null
    },
    {
      "context_text": "The details are as follows: (1) For deraining, we compare against JORDER [97], PReNet [73], MPRNet [102], MAXIM [79], and Restormer [100].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only models and methods used for deraining. No verifiable resources are identified.",
      "processing_time": 60.33884525299072,
      "citing_paper_id": "278033660",
      "cited_paper_id": 59316941
    },
    {
      "context_text": "The details are as follows: (1) For deraining, we compare against JORDER [97], PReNet [73], MPRNet [102], MAXIM [79], and Restormer [100].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only models and methods used for deraining. No verifiable resources are identified.",
      "processing_time": 60.33884525299072,
      "citing_paper_id": "278033660",
      "cited_paper_id": 73439498
    },
    {
      "context_text": "The details are as follows: (1) For deraining, we compare against JORDER [97], PReNet [73], MPRNet [102], MAXIM [79], and Restormer [100].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only models and methods used for deraining. No verifiable resources are identified.",
      "processing_time": 60.33884525299072,
      "citing_paper_id": "278033660",
      "cited_paper_id": 245837508
    },
    {
      "context_text": "(2) For low-light image enhancement, we compare against EnlightenGAN [32], MIR-Net [101], Uretinex-Net [92], MAXIM [79], and IAT [12].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only models and methods used for comparison in low-light image enhancement.",
      "processing_time": 59.649555921554565,
      "citing_paper_id": "278033660",
      "cited_paper_id": 189928152
    },
    {
      "context_text": "(2) For low-light image enhancement, we compare against EnlightenGAN [32], MIR-Net [101], Uretinex-Net [92], MAXIM [79], and IAT [12].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only models and methods used for comparison in low-light image enhancement.",
      "processing_time": 59.649555921554565,
      "citing_paper_id": "278033660",
      "cited_paper_id": 245837508
    },
    {
      "context_text": "(2) For low-light image enhancement, we compare against EnlightenGAN [32], MIR-Net [101], Uretinex-Net [92], MAXIM [79], and IAT [12].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only models and methods used for comparison in low-light image enhancement.",
      "processing_time": 59.649555921554565,
      "citing_paper_id": "278033660",
      "cited_paper_id": null
    },
    {
      "context_text": "(2) For low-light image enhancement, we compare against EnlightenGAN [32], MIR-Net [101], Uretinex-Net [92], MAXIM [79], and IAT [12].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only models and methods used for comparison in low-light image enhancement.",
      "processing_time": 59.649555921554565,
      "citing_paper_id": "278033660",
      "cited_paper_id": null
    },
    {
      "context_text": "On RESIDE6K [71], it achieves 30.89dB PSNR and 0.973 SSIM, outperforming Dehaze-Former by 0.60dB and 0.009.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "RESIDE6K"
      ],
      "dataset_descriptions": {
        "RESIDE6K": "Used to evaluate image dehazing performance, specifically comparing PSNR and SSIM metrics against Dehaze-Former."
      },
      "confidence_score": 1.0,
      "reasoning": "The citation mentions RESIDE6K, which is a specific dataset used for evaluating image dehazing performance.",
      "processing_time": 66.53801250457764,
      "citing_paper_id": "278033660",
      "cited_paper_id": 208138077
    },
    {
      "context_text": "Recent advances in deep learning have significantly improved performance in Degradation-specific restoration problems, such as image denoising [34, 74, 103], deblurring [5, 52, 90], deraining[1, 7, 8, 25, 63, 64], dehazing[86, 91, 112, 113] and low-light enhancement[2, 22, 50, 81, 85].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various image restoration tasks. No dataset names are provided as exact substrings.",
      "processing_time": 60.745662212371826,
      "citing_paper_id": "278033660",
      "cited_paper_id": 213183248
    },
    {
      "context_text": "Recent advances in deep learning have significantly improved performance in Degradation-specific restoration problems, such as image denoising [34, 74, 103], deblurring [5, 52, 90], deraining[1, 7, 8, 25, 63, 64], dehazing[86, 91, 112, 113] and low-light enhancement[2, 22, 50, 81, 85].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various image restoration tasks. No dataset names are provided as exact substrings.",
      "processing_time": 60.745662212371826,
      "citing_paper_id": "278033660",
      "cited_paper_id": 258048853
    },
    {
      "context_text": "Recent advances in deep learning have significantly improved performance in Degradation-specific restoration problems, such as image denoising [34, 74, 103], deblurring [5, 52, 90], deraining[1, 7, 8, 25, 63, 64], dehazing[86, 91, 112, 113] and low-light enhancement[2, 22, 50, 81, 85].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various image restoration tasks. No dataset names are provided as exact substrings.",
      "processing_time": 60.745662212371826,
      "citing_paper_id": "278033660",
      "cited_paper_id": 268856944
    },
    {
      "context_text": "Recent advances in deep learning have significantly improved performance in Degradation-specific restoration problems, such as image denoising [34, 74, 103], deblurring [5, 52, 90], deraining[1, 7, 8, 25, 63, 64], dehazing[86, 91, 112, 113] and low-light enhancement[2, 22, 50, 81, 85].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various image restoration tasks. No dataset names are provided as exact substrings.",
      "processing_time": 60.745662212371826,
      "citing_paper_id": "278033660",
      "cited_paper_id": 270440599
    },
    {
      "context_text": "Recent advances in deep learning have significantly improved performance in Degradation-specific restoration problems, such as image denoising [34, 74, 103], deblurring [5, 52, 90], deraining[1, 7, 8, 25, 63, 64], dehazing[86, 91, 112, 113] and low-light enhancement[2, 22, 50, 81, 85].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various image restoration tasks. No dataset names are provided as exact substrings.",
      "processing_time": 60.745662212371826,
      "citing_paper_id": "278033660",
      "cited_paper_id": 271489742
    },
    {
      "context_text": "Recent advances in deep learning have significantly improved performance in Degradation-specific restoration problems, such as image denoising [34, 74, 103], deblurring [5, 52, 90], deraining[1, 7, 8, 25, 63, 64], dehazing[86, 91, 112, 113] and low-light enhancement[2, 22, 50, 81, 85].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various image restoration tasks. No dataset names are provided as exact substrings.",
      "processing_time": 60.745662212371826,
      "citing_paper_id": "278033660",
      "cited_paper_id": 272722348
    },
    {
      "context_text": "Recent advances in deep learning have significantly improved performance in Degradation-specific restoration problems, such as image denoising [34, 74, 103], deblurring [5, 52, 90], deraining[1, 7, 8, 25, 63, 64], dehazing[86, 91, 112, 113] and low-light enhancement[2, 22, 50, 81, 85].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various image restoration tasks. No dataset names are provided as exact substrings.",
      "processing_time": 60.745662212371826,
      "citing_paper_id": "278033660",
      "cited_paper_id": null
    },
    {
      "context_text": "Recent advances in deep learning have significantly improved performance in Degradation-specific restoration problems, such as image denoising [34, 74, 103], deblurring [5, 52, 90], deraining[1, 7, 8, 25, 63, 64], dehazing[86, 91, 112, 113] and low-light enhancement[2, 22, 50, 81, 85].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various image restoration tasks. No dataset names are provided as exact substrings.",
      "processing_time": 60.745662212371826,
      "citing_paper_id": "278033660",
      "cited_paper_id": null
    },
    {
      "context_text": "…learning [38] has significantly advanced tasks like such as image super-resolution[17, 65, 66, 68, 75, 83], image de-noising [34, 74, 103], deblurring [5, 52, 90], deraining[1, 7, 8, 25, 58, 63, 64], dehazing[86, 91, 112, 113], desnowing[6, 20, 109] and low-light enhancement[2, 22, 50, 51, 81, 85].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various image restoration tasks. No dataset names are present in the text.",
      "processing_time": 60.32819175720215,
      "citing_paper_id": "278033660",
      "cited_paper_id": 213183248
    },
    {
      "context_text": "…learning [38] has significantly advanced tasks like such as image super-resolution[17, 65, 66, 68, 75, 83], image de-noising [34, 74, 103], deblurring [5, 52, 90], deraining[1, 7, 8, 25, 58, 63, 64], dehazing[86, 91, 112, 113], desnowing[6, 20, 109] and low-light enhancement[2, 22, 50, 51, 81, 85].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various image restoration tasks. No dataset names are present in the text.",
      "processing_time": 60.32819175720215,
      "citing_paper_id": "278033660",
      "cited_paper_id": 258048853
    },
    {
      "context_text": "…learning [38] has significantly advanced tasks like such as image super-resolution[17, 65, 66, 68, 75, 83], image de-noising [34, 74, 103], deblurring [5, 52, 90], deraining[1, 7, 8, 25, 58, 63, 64], dehazing[86, 91, 112, 113], desnowing[6, 20, 109] and low-light enhancement[2, 22, 50, 51, 81, 85].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various image restoration tasks. No dataset names are present in the text.",
      "processing_time": 60.32819175720215,
      "citing_paper_id": "278033660",
      "cited_paper_id": 268856944
    },
    {
      "context_text": "…learning [38] has significantly advanced tasks like such as image super-resolution[17, 65, 66, 68, 75, 83], image de-noising [34, 74, 103], deblurring [5, 52, 90], deraining[1, 7, 8, 25, 58, 63, 64], dehazing[86, 91, 112, 113], desnowing[6, 20, 109] and low-light enhancement[2, 22, 50, 51, 81, 85].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various image restoration tasks. No dataset names are present in the text.",
      "processing_time": 60.32819175720215,
      "citing_paper_id": "278033660",
      "cited_paper_id": 270440599
    },
    {
      "context_text": "…learning [38] has significantly advanced tasks like such as image super-resolution[17, 65, 66, 68, 75, 83], image de-noising [34, 74, 103], deblurring [5, 52, 90], deraining[1, 7, 8, 25, 58, 63, 64], dehazing[86, 91, 112, 113], desnowing[6, 20, 109] and low-light enhancement[2, 22, 50, 51, 81, 85].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various image restoration tasks. No dataset names are present in the text.",
      "processing_time": 60.32819175720215,
      "citing_paper_id": "278033660",
      "cited_paper_id": null
    },
    {
      "context_text": "…learning [38] has significantly advanced tasks like such as image super-resolution[17, 65, 66, 68, 75, 83], image de-noising [34, 74, 103], deblurring [5, 52, 90], deraining[1, 7, 8, 25, 58, 63, 64], dehazing[86, 91, 112, 113], desnowing[6, 20, 109] and low-light enhancement[2, 22, 50, 51, 81, 85].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various image restoration tasks. No dataset names are present in the text.",
      "processing_time": 60.32819175720215,
      "citing_paper_id": "278033660",
      "cited_paper_id": null
    },
    {
      "context_text": "…learning [38] has significantly advanced tasks like such as image super-resolution[17, 65, 66, 68, 75, 83], image de-noising [34, 74, 103], deblurring [5, 52, 90], deraining[1, 7, 8, 25, 58, 63, 64], dehazing[86, 91, 112, 113], desnowing[6, 20, 109] and low-light enhancement[2, 22, 50, 51, 81, 85].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various image restoration tasks. No dataset names are present in the text.",
      "processing_time": 60.32819175720215,
      "citing_paper_id": "278033660",
      "cited_paper_id": null
    },
    {
      "context_text": "Vision Mamba demonstrated superior performance compared to Vision Transformers (ViTs)[14], while maintaining lower model complexity, thereby laying a solid foundation for subsequent advancements in the field of computer vision.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a comparison between Vision Mamba and Vision Transformers. No datasets are referenced for training or evaluation.",
      "processing_time": 61.102707862854004,
      "citing_paper_id": "278033660",
      "cited_paper_id": 225039882
    },
    {
      "context_text": "The rise of deep learning [38] has significantly advanced tasks like such as image super-resolution[17, 65, 66, 68, 75, 83], image de-noising [34, 74, 103], deblurring [5, 52, 90], deraining[1, 7, 8, 25, 58, 63, 64], dehazing[86, 91, 112, 113], desnowing[6, 20, 109] and low-light enhancement[2, 22,…",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various image restoration tasks. No dataset names are provided in the context.",
      "processing_time": 60.61396813392639,
      "citing_paper_id": "278033660",
      "cited_paper_id": 257804739
    },
    {
      "context_text": "The rise of deep learning [38] has significantly advanced tasks like such as image super-resolution[17, 65, 66, 68, 75, 83], image de-noising [34, 74, 103], deblurring [5, 52, 90], deraining[1, 7, 8, 25, 58, 63, 64], dehazing[86, 91, 112, 113], desnowing[6, 20, 109] and low-light enhancement[2, 22,…",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various image restoration tasks. No dataset names are provided in the context.",
      "processing_time": 60.61396813392639,
      "citing_paper_id": "278033660",
      "cited_paper_id": 270379537
    },
    {
      "context_text": "The rise of deep learning [38] has significantly advanced tasks like such as image super-resolution[17, 65, 66, 68, 75, 83], image de-noising [34, 74, 103], deblurring [5, 52, 90], deraining[1, 7, 8, 25, 58, 63, 64], dehazing[86, 91, 112, 113], desnowing[6, 20, 109] and low-light enhancement[2, 22,…",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various image restoration tasks. No dataset names are provided in the context.",
      "processing_time": 60.61396813392639,
      "citing_paper_id": "278033660",
      "cited_paper_id": 271489742
    },
    {
      "context_text": "The rise of deep learning [38] has significantly advanced tasks like such as image super-resolution[17, 65, 66, 68, 75, 83], image de-noising [34, 74, 103], deblurring [5, 52, 90], deraining[1, 7, 8, 25, 58, 63, 64], dehazing[86, 91, 112, 113], desnowing[6, 20, 109] and low-light enhancement[2, 22,…",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various image restoration tasks. No dataset names are provided in the context.",
      "processing_time": 60.61396813392639,
      "citing_paper_id": "278033660",
      "cited_paper_id": 272722348
    },
    {
      "context_text": "The rise of deep learning [38] has significantly advanced tasks like such as image super-resolution[17, 65, 66, 68, 75, 83], image de-noising [34, 74, 103], deblurring [5, 52, 90], deraining[1, 7, 8, 25, 58, 63, 64], dehazing[86, 91, 112, 113], desnowing[6, 20, 109] and low-light enhancement[2, 22,…",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various image restoration tasks. No dataset names are provided in the context.",
      "processing_time": 60.61396813392639,
      "citing_paper_id": "278033660",
      "cited_paper_id": null
    },
    {
      "context_text": "The success of VSSM has inspired extensive research into the application of SSM across various vision tasks, such as object detection[43, 47, 96, 105, 106, 111], semantic segmentation[30, 94, 104], and image classification[44, 99, 107 ?",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The provided context does not mention any specific datasets, only various vision tasks. The cited paper titles do not provide additional information about datasets.",
      "processing_time": 61.5364294052124,
      "citing_paper_id": "278033660",
      "cited_paper_id": 259251801
    },
    {
      "context_text": "The success of VSSM has inspired extensive research into the application of SSM across various vision tasks, such as object detection[43, 47, 96, 105, 106, 111], semantic segmentation[30, 94, 104], and image classification[44, 99, 107 ?",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The provided context does not mention any specific datasets, only various vision tasks. The cited paper titles do not provide additional information about datasets.",
      "processing_time": 61.5364294052124,
      "citing_paper_id": "278033660",
      "cited_paper_id": 262216939
    },
    {
      "context_text": "The success of VSSM has inspired extensive research into the application of SSM across various vision tasks, such as object detection[43, 47, 96, 105, 106, 111], semantic segmentation[30, 94, 104], and image classification[44, 99, 107 ?",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The provided context does not mention any specific datasets, only various vision tasks. The cited paper titles do not provide additional information about datasets.",
      "processing_time": 61.5364294052124,
      "citing_paper_id": "278033660",
      "cited_paper_id": 273351043
    },
    {
      "context_text": "The success of VSSM has inspired extensive research into the application of SSM across various vision tasks, such as object detection[43, 47, 96, 105, 106, 111], semantic segmentation[30, 94, 104], and image classification[44, 99, 107 ?",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The provided context does not mention any specific datasets, only various vision tasks. The cited paper titles do not provide additional information about datasets.",
      "processing_time": 61.5364294052124,
      "citing_paper_id": "278033660",
      "cited_paper_id": null
    },
    {
      "context_text": "The success of VSSM has inspired extensive research into the application of SSM across various vision tasks, such as object detection[43, 47, 96, 105, 106, 111], semantic segmentation[30, 94, 104], and image classification[44, 99, 107 ?",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The provided context does not mention any specific datasets, only various vision tasks. The cited paper titles do not provide additional information about datasets.",
      "processing_time": 61.5364294052124,
      "citing_paper_id": "278033660",
      "cited_paper_id": null
    },
    {
      "context_text": "For All-in-One image restoration, we compare our method against eight approaches: MIRNet [101], NAFNet [4], MPRNet [102], Restormer [100], Promp-tIR [69], IDR [108], OneRestore [21], and AdaIR [11].",
      "catation_intent": "reusable resource",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods and models used for comparison in the research.",
      "processing_time": 60.004865646362305,
      "citing_paper_id": "278033660",
      "cited_paper_id": 260085391
    },
    {
      "context_text": "For All-in-One image restoration, we compare our method against eight approaches: MIRNet [101], NAFNet [4], MPRNet [102], Restormer [100], Promp-tIR [69], IDR [108], OneRestore [21], and AdaIR [11].",
      "catation_intent": "reusable resource",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods and models used for comparison in the research.",
      "processing_time": 60.004865646362305,
      "citing_paper_id": "278033660",
      "cited_paper_id": null
    },
    {
      "context_text": "For All-in-One image restoration, we compare our method against eight approaches: MIRNet [101], NAFNet [4], MPRNet [102], Restormer [100], Promp-tIR [69], IDR [108], OneRestore [21], and AdaIR [11].",
      "catation_intent": "reusable resource",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods and models used for comparison in the research.",
      "processing_time": 60.004865646362305,
      "citing_paper_id": "278033660",
      "cited_paper_id": null
    },
    {
      "context_text": "IDR[108] uses meta-learning to decompose degradations, enabling generalization across diverse scenarios.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for image restoration. The context focuses on the technique and its application rather than a particular dataset.",
      "processing_time": 61.88592863082886,
      "citing_paper_id": "278033660",
      "cited_paper_id": 260085391
    },
    {
      "context_text": "Specifically, we introduce central difference convolution(CDC)[60, 61, 84] to extract edge and texture degradation cues related to illumination and blur.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only methods and techniques. The cited papers' titles do not provide additional information about datasets.",
      "processing_time": 61.521573305130005,
      "citing_paper_id": "278033660",
      "cited_paper_id": 266741509
    },
    {
      "context_text": "Specifically, we introduce central difference convolution(CDC)[60, 61, 84] to extract edge and texture degradation cues related to illumination and blur.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only methods and techniques. The cited papers' titles do not provide additional information about datasets.",
      "processing_time": 61.521573305130005,
      "citing_paper_id": "278033660",
      "cited_paper_id": 269757927
    },
    {
      "context_text": "Specifically, we introduce central difference convolution(CDC)[60, 61, 84] to extract edge and texture degradation cues related to illumination and blur.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only methods and techniques. The cited papers' titles do not provide additional information about datasets.",
      "processing_time": 61.521573305130005,
      "citing_paper_id": "278033660",
      "cited_paper_id": null
    },
    {
      "context_text": "These approaches, leveraging convolutional neural networks (CNN)-based [39], transformer-based [55, 57, 80], or diffusion-based [28] architectures, enable powerful feature learning for image restoration.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various architectures used for image restoration. No verifiable resources are identified.",
      "processing_time": 61.24910306930542,
      "citing_paper_id": "278033660",
      "cited_paper_id": 271903072
    },
    {
      "context_text": "These approaches, leveraging convolutional neural networks (CNN)-based [39], transformer-based [55, 57, 80], or diffusion-based [28] architectures, enable powerful feature learning for image restoration.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various architectures used for image restoration. No verifiable resources are identified.",
      "processing_time": 61.24910306930542,
      "citing_paper_id": "278033660",
      "cited_paper_id": null
    },
    {
      "context_text": "Furthermore, other SSM-based approaches have also emerged for various image restoration tasks[9, 13, 15, 18, 24, 26, 27, 29, 31, 41, 42, 45, 56, 62, 67, 70, 72, 77, 78, 82, 88, 89, 93, 95], showcasing the strong potential and promising opportunities of SSM in this domain.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general references to image restoration tasks using SSM-based approaches.",
      "processing_time": 60.60078048706055,
      "citing_paper_id": "278033660",
      "cited_paper_id": 274437167
    },
    {
      "context_text": "Furthermore, other SSM-based approaches have also emerged for various image restoration tasks[9, 13, 15, 18, 24, 26, 27, 29, 31, 41, 42, 45, 56, 62, 67, 70, 72, 77, 78, 82, 88, 89, 93, 95], showcasing the strong potential and promising opportunities of SSM in this domain.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general references to image restoration tasks using SSM-based approaches.",
      "processing_time": 60.60078048706055,
      "citing_paper_id": "278033660",
      "cited_paper_id": null
    },
    {
      "context_text": "Furthermore, other SSM-based approaches have also emerged for various image restoration tasks[9, 13, 15, 18, 24, 26, 27, 29, 31, 41, 42, 45, 56, 62, 67, 70, 72, 77, 78, 82, 88, 89, 93, 95], showcasing the strong potential and promising opportunities of SSM in this domain.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general references to image restoration tasks using SSM-based approaches.",
      "processing_time": 60.60078048706055,
      "citing_paper_id": "278033660",
      "cited_paper_id": null
    },
    {
      "context_text": "Furthermore, other SSM-based approaches have also emerged for various image restoration tasks[9, 13, 15, 18, 24, 26, 27, 29, 31, 41, 42, 45, 56, 62, 67, 70, 72, 77, 78, 82, 88, 89, 93, 95], showcasing the strong potential and promising opportunities of SSM in this domain.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general references to image restoration tasks using SSM-based approaches.",
      "processing_time": 60.60078048706055,
      "citing_paper_id": "278033660",
      "cited_paper_id": null
    },
    {
      "context_text": "Furthermore, other SSM-based approaches have also emerged for various image restoration tasks[9, 13, 15, 18, 24, 26, 27, 29, 31, 41, 42, 45, 56, 62, 67, 70, 72, 77, 78, 82, 88, 89, 93, 95], showcasing the strong potential and promising opportunities of SSM in this domain.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general references to image restoration tasks using SSM-based approaches.",
      "processing_time": 60.60078048706055,
      "citing_paper_id": "278033660",
      "cited_paper_id": null
    },
    {
      "context_text": "Furthermore, other SSM-based approaches have also emerged for various image restoration tasks[9, 13, 15, 18, 24, 26, 27, 29, 31, 41, 42, 45, 56, 62, 67, 70, 72, 77, 78, 82, 88, 89, 93, 95], showcasing the strong potential and promising opportunities of SSM in this domain.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general references to image restoration tasks using SSM-based approaches.",
      "processing_time": 60.60078048706055,
      "citing_paper_id": "278033660",
      "cited_paper_id": null
    },
    {
      "context_text": "Furthermore, other SSM-based approaches have also emerged for various image restoration tasks[9, 13, 15, 18, 24, 26, 27, 29, 31, 41, 42, 45, 56, 62, 67, 70, 72, 77, 78, 82, 88, 89, 93, 95], showcasing the strong potential and promising opportunities of SSM in this domain.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general references to image restoration tasks using SSM-based approaches.",
      "processing_time": 60.60078048706055,
      "citing_paper_id": "278033660",
      "cited_paper_id": null
    },
    {
      "context_text": "Additionally, methods like DA-CLIP[48] leverage the pre-trained vision-language model (VLM) for prompt extraction, but the significant computational overhead introduced by VLM limits their practical efficiency.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (DA-CLIP) and a general reference to vision-language models (VLM).",
      "processing_time": 62.48229622840881,
      "citing_paper_id": "278033660",
      "cited_paper_id": null
    },
    {
      "context_text": "On the LOL dataset [87], it achieves 24.20dB PSNR and 0.852 SSIM, outperforming MIRNet by 0.06dB and 0.022.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "LOL"
      ],
      "dataset_descriptions": {
        "LOL": "Used to evaluate the performance of an image restoration method, focusing on PSNR and SSIM metrics to compare against MIRNet."
      },
      "confidence_score": 1.0,
      "reasoning": "The citation mentions the LOL dataset, which is used to evaluate the performance of an image restoration method. The dataset is specific and relevant to the research topic.",
      "processing_time": 69.73323655128479,
      "citing_paper_id": "278033660",
      "cited_paper_id": null
    },
    {
      "context_text": "Among these, MIRNet, NAFNet, MPRNet, and Restormer are general-purpose image restoration models, while the latter four methods are specifically designed for All-in-One image restoration.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only models and methods. There are no verifiable resources that meet the criteria for inclusion.",
      "processing_time": 62.16676902770996,
      "citing_paper_id": "278033660",
      "cited_paper_id": null
    },
    {
      "context_text": "PromptIR[69] integrates end-to-end prompt generation and interaction to adaptively handle different degradation types, while OneRestore[21] aligns text-based degradation categories with visual features as prompts.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods or models. There are no clear identifiers for datasets.",
      "processing_time": 61.9585816860199,
      "citing_paper_id": "278033660",
      "cited_paper_id": null
    },
    {
      "context_text": "As shown in Figure 1, DPMambaIR achieves state-of-the-art performance in terms of PSNR and SSIM, outperforming existing All-in-One methods such as AdaIR and OneRestore.",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only performance metrics and competing methods.",
      "processing_time": 60.491735219955444,
      "citing_paper_id": "278033660",
      "cited_paper_id": null
    },
    {
      "context_text": "Table 3 presents the results of our ablation study on DPMambaIR for the All-in-One image restoration task, illustrating the impact For degradation extraction, we compare three approaches: explicit category labels, a pre-trained degradation extractor from OneRestore [21], and our fine-grained degradation representation via image reconstruction.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific, verifiable datasets. It discusses methods and approaches for degradation extraction in image restoration but does not reference any named datasets.",
      "processing_time": 62.90082144737244,
      "citing_paper_id": "278033660",
      "cited_paper_id": null
    },
    {
      "context_text": "Prompt-based methods, like OneRestore [21], guide restoration using degradation prompts, integrating scene descriptors via cross-attention mechanisms for flexible and adaptive strategies.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method called OneRestore. No verifiable resources are identified.",
      "processing_time": 61.78674364089966,
      "citing_paper_id": "278033660",
      "cited_paper_id": null
    },
    {
      "context_text": "On the other hand, OneRestore excels in dehazing but delivers suboptimal results in tasks such as deraining and deblurring.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only performance comparisons of a method called OneRestore.",
      "processing_time": 61.19432091712952,
      "citing_paper_id": "278033660",
      "cited_paper_id": null
    },
    {
      "context_text": "…on DPMambaIR for the All-in-One image restoration task, illustrating the impact For degradation extraction, we compare three approaches: explicit category labels, a pre-trained degradation extractor from OneRestore [21], and our fine-grained degradation representation via image reconstruction.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions 'DPMambaIR' and 'OneRestore', but these are likely methods or models rather than datasets. No specific datasets are mentioned.",
      "processing_time": 62.95911908149719,
      "citing_paper_id": "278033660",
      "cited_paper_id": null
    },
    {
      "context_text": "The OneRestore extractor, designed for classification tasks, performs worse than explicit category labels.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only a comparison between an extractor and explicit category labels.",
      "processing_time": 61.294660806655884,
      "citing_paper_id": "278033660",
      "cited_paper_id": null
    },
    {
      "context_text": "Specifically, PromptIR employs visual prompts to guide restoration, OneRestore leverages a pre-trained degradation category encoder with cross-attention mechanisms, and AdaIR utilizes frequency domain information for adaptive restoration.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods and approaches for image restoration.",
      "processing_time": 61.52065181732178,
      "citing_paper_id": "278033660",
      "cited_paper_id": null
    },
    {
      "context_text": "On Rain100H [98], our method achieves a PSNR of 32.30dB and SSIM of 0.967, surpassing Restormer by 0.84dB and 0.063, respectively.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Rain100H"
      ],
      "dataset_descriptions": {
        "Rain100H": "Used to evaluate the performance of the proposed method in rain removal, comparing PSNR and SSIM metrics against Restormer."
      },
      "confidence_score": 1.0,
      "reasoning": "The citation mentions 'Rain100H' as a dataset used to evaluate the performance of the method in image restoration, specifically for rain removal.",
      "processing_time": 70.34735989570618,
      "citing_paper_id": "278033660",
      "cited_paper_id": null
    },
    {
      "context_text": "The metrics are reported as PSNR( ↑ )/SSIM( ↑ )/LPIPS( ↓ )/FID( ↓ ). datasets Lai [18] (blur and noise), SPANet [48] (rain and haze), using no-reference metrics NIQE [34] and PIQE [46] for evaluation.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Lai [18]",
        "SPANet [48]"
      ],
      "dataset_descriptions": {
        "Lai [18]": "Used to evaluate image restoration performance on blur and noise, providing a benchmark for these specific degradations.",
        "SPANet [48]": "Used to evaluate image restoration performance on rain and haze, providing a benchmark for these specific degradations."
      },
      "confidence_score": 0.9,
      "reasoning": "The context mentions specific datasets used for evaluating image restoration methods, including Lai [18] and SPANet [48]. These datasets are used to evaluate performance on specific degradation types.",
      "processing_time": 77.86987519264221,
      "citing_paper_id": "278959180",
      "cited_paper_id": 6917137
    },
    {
      "context_text": "The metrics are reported as PSNR( ↑ )/SSIM( ↑ )/LPIPS( ↓ )/FID( ↓ ). datasets Lai [18] (blur and noise), SPANet [48] (rain and haze), using no-reference metrics NIQE [34] and PIQE [46] for evaluation.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Lai [18]",
        "SPANet [48]"
      ],
      "dataset_descriptions": {
        "Lai [18]": "Used to evaluate image restoration performance on blur and noise, providing a benchmark for these specific degradations.",
        "SPANet [48]": "Used to evaluate image restoration performance on rain and haze, providing a benchmark for these specific degradations."
      },
      "confidence_score": 0.9,
      "reasoning": "The context mentions specific datasets used for evaluating image restoration methods, including Lai [18] and SPANet [48]. These datasets are used to evaluate performance on specific degradation types.",
      "processing_time": 77.86987519264221,
      "citing_paper_id": "278959180",
      "cited_paper_id": 91184545
    },
    {
      "context_text": "Recent advances of deep neural networks (NNs) [12, 14, 29, 45] have triggered remarkable successes in image restoration, in which most works [6, 19, 20, 25, 31, 41, 54, 55, 57, 59] develop task-specific restoration networks to handle single known degradations Figure 1.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to deep neural networks and image restoration works. No verifiable resources are identified.",
      "processing_time": 62.93692922592163,
      "citing_paper_id": "278959180",
      "cited_paper_id": 10514149
    },
    {
      "context_text": "Recent advances of deep neural networks (NNs) [12, 14, 29, 45] have triggered remarkable successes in image restoration, in which most works [6, 19, 20, 25, 31, 41, 54, 55, 57, 59] develop task-specific restoration networks to handle single known degradations Figure 1.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to deep neural networks and image restoration works. No verifiable resources are identified.",
      "processing_time": 62.93692922592163,
      "citing_paper_id": "278959180",
      "cited_paper_id": 26229170
    },
    {
      "context_text": "Recent advances of deep neural networks (NNs) [12, 14, 29, 45] have triggered remarkable successes in image restoration, in which most works [6, 19, 20, 25, 31, 41, 54, 55, 57, 59] develop task-specific restoration networks to handle single known degradations Figure 1.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to deep neural networks and image restoration works. No verifiable resources are identified.",
      "processing_time": 62.93692922592163,
      "citing_paper_id": "278959180",
      "cited_paper_id": 232352874
    },
    {
      "context_text": "Recent advances of deep neural networks (NNs) [12, 14, 29, 45] have triggered remarkable successes in image restoration, in which most works [6, 19, 20, 25, 31, 41, 54, 55, 57, 59] develop task-specific restoration networks to handle single known degradations Figure 1.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to deep neural networks and image restoration works. No verifiable resources are identified.",
      "processing_time": 62.93692922592163,
      "citing_paper_id": "278959180",
      "cited_paper_id": 244346144
    },
    {
      "context_text": "Recent advances of deep neural networks (NNs) [12, 14, 29, 45] have triggered remarkable successes in image restoration, in which most works [6, 19, 20, 25, 31, 41, 54, 55, 57, 59] develop task-specific restoration networks to handle single known degradations Figure 1.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to deep neural networks and image restoration works. No verifiable resources are identified.",
      "processing_time": 62.93692922592163,
      "citing_paper_id": "278959180",
      "cited_paper_id": 253264930
    },
    {
      "context_text": "However, this specificity hinders their applicability in real-world scenarios such as autonomous navigation [21, 36] and surveillance systems [26], where varied and unexpected degradations frequently occur.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only general applications and challenges in autonomous navigation and surveillance systems.",
      "processing_time": 62.00480675697327,
      "citing_paper_id": "278959180",
      "cited_paper_id": 10519397
    },
    {
      "context_text": "6 we can see that BaryIR achieves superior quantitative advantages over other methods when generalizing to unseen degradation levels, e.g., 2.80 dB PSNR gain for deraining on Rain100L [51], and 3.30 dB gain for denoising with severe unseen noise level σ = 75 over InstructIR [10].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Rain100L"
      ],
      "dataset_descriptions": {
        "Rain100L": "Used to evaluate deraining performance, specifically measuring PSNR gains for unseen degradation levels. The dataset provides synthetic rain images for benchmarking image restoration algorithms."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions 'Rain100L' as a dataset used for evaluating deraining performance. No other datasets are explicitly named.",
      "processing_time": 71.94454288482666,
      "citing_paper_id": "278959180",
      "cited_paper_id": 15443600
    },
    {
      "context_text": "6 we can see that BaryIR achieves superior quantitative advantages over other methods when generalizing to unseen degradation levels, e.g., 2.80 dB PSNR gain for deraining on Rain100L [51], and 3.30 dB gain for denoising with severe unseen noise level σ = 75 over InstructIR [10].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Rain100L"
      ],
      "dataset_descriptions": {
        "Rain100L": "Used to evaluate deraining performance, specifically measuring PSNR gains for unseen degradation levels. The dataset provides synthetic rain images for benchmarking image restoration algorithms."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions 'Rain100L' as a dataset used for evaluating deraining performance. No other datasets are explicitly named.",
      "processing_time": 71.94454288482666,
      "citing_paper_id": "278959180",
      "cited_paper_id": 267320695
    },
    {
      "context_text": "Specifically, we train three-degradation models for dehazing (SOTS [22]), deraining (Rain100H [51]), and denoising with noise levels σ ∈ { 15 , 25 , 50 } (BSD400 [5] and WED [33]).",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "SOTS",
        "Rain100H",
        "BSD400",
        "WED"
      ],
      "dataset_descriptions": {
        "SOTS": "Used for training a dehazing model, focusing on synthetic and real-world hazy images to improve visibility.",
        "Rain100H": "Used for training a deraining model, focusing on high-resolution rain streaks to enhance image clarity.",
        "BSD400": "Used for training a denoising model, focusing on natural images with varying noise levels to improve image quality.",
        "WED": "Used for training a denoising model, focusing on web images with synthetic noise to enhance visual fidelity."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions specific datasets used for training models for image restoration tasks such as dehazing, deraining, and denoising.",
      "processing_time": 86.55220651626587,
      "citing_paper_id": "278959180",
      "cited_paper_id": 15443600
    },
    {
      "context_text": "Specifically, we train three-degradation models for dehazing (SOTS [22]), deraining (Rain100H [51]), and denoising with noise levels σ ∈ { 15 , 25 , 50 } (BSD400 [5] and WED [33]).",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "SOTS",
        "Rain100H",
        "BSD400",
        "WED"
      ],
      "dataset_descriptions": {
        "SOTS": "Used for training a dehazing model, focusing on synthetic and real-world hazy images to improve visibility.",
        "Rain100H": "Used for training a deraining model, focusing on high-resolution rain streaks to enhance image clarity.",
        "BSD400": "Used for training a denoising model, focusing on natural images with varying noise levels to improve image quality.",
        "WED": "Used for training a denoising model, focusing on web images with synthetic noise to enhance visual fidelity."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions specific datasets used for training models for image restoration tasks such as dehazing, deraining, and denoising.",
      "processing_time": 86.55220651626587,
      "citing_paper_id": "278959180",
      "cited_paper_id": 39760169
    },
    {
      "context_text": "Additionally, we evaluate on 49 mixed-degradation images collected from real-world [2] and SPANet [48].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "SPANet"
      ],
      "dataset_descriptions": {
        "SPANet": "Used to evaluate image restoration methods on mixed-degradation images, focusing on real-world scenarios and synthetic degradations."
      },
      "confidence_score": 0.85,
      "reasoning": "The context mentions 'SPANet' which is likely a dataset based on the cited paper title. 'Real-world' is too generic, but SPANet is specific and relevant.",
      "processing_time": 71.33910298347473,
      "citing_paper_id": "278959180",
      "cited_paper_id": 91184545
    },
    {
      "context_text": "Additionally, we evaluate on 49 mixed-degradation images collected from real-world [2] and SPANet [48].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "SPANet"
      ],
      "dataset_descriptions": {
        "SPANet": "Used to evaluate image restoration methods on mixed-degradation images, focusing on real-world scenarios and synthetic degradations."
      },
      "confidence_score": 0.85,
      "reasoning": "The context mentions 'SPANet' which is likely a dataset based on the cited paper title. 'Real-world' is too generic, but SPANet is specific and relevant.",
      "processing_time": 71.33910298347473,
      "citing_paper_id": "278959180",
      "cited_paper_id": 206598041
    },
    {
      "context_text": "3 reports the quantitative results and shows that BaryIR yields PSNR gains of 3.81 dB on O-HAZE [2] and 2.73 dB on SPANet [48] over the second-best methods.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "O-HAZE",
        "SPANet"
      ],
      "dataset_descriptions": {
        "O-HAZE": "Used to evaluate BaryIR's dehazing performance, yielding a PSNR gain of 3.81 dB over the second-best methods.",
        "SPANet": "Used to evaluate BaryIR's deraining performance, yielding a PSNR gain of 2.73 dB over the second-best methods."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions two datasets, O-HAZE and SPANet, which are used to evaluate the performance of BaryIR in image restoration tasks.",
      "processing_time": 77.39789915084839,
      "citing_paper_id": "278959180",
      "cited_paper_id": 91184545
    },
    {
      "context_text": "3 reports the quantitative results and shows that BaryIR yields PSNR gains of 3.81 dB on O-HAZE [2] and 2.73 dB on SPANet [48] over the second-best methods.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "O-HAZE",
        "SPANet"
      ],
      "dataset_descriptions": {
        "O-HAZE": "Used to evaluate BaryIR's dehazing performance, yielding a PSNR gain of 3.81 dB over the second-best methods.",
        "SPANet": "Used to evaluate BaryIR's deraining performance, yielding a PSNR gain of 2.73 dB over the second-best methods."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions two datasets, O-HAZE and SPANet, which are used to evaluate the performance of BaryIR in image restoration tasks.",
      "processing_time": 77.39789915084839,
      "citing_paper_id": "278959180",
      "cited_paper_id": 206598041
    },
    {
      "context_text": "We compare BaryIR with SOTA methods on unseen real-world haze O-HAZE [2] and rain SPANet [48] datasets using the five-degradation models.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "O-HAZE",
        "SPANet"
      ],
      "dataset_descriptions": {
        "O-HAZE": "Used to evaluate BaryIR on real-world haze degradation, comparing performance with state-of-the-art methods using a benchmark with real hazy and haze-free outdoor images.",
        "SPANet": "Used to evaluate BaryIR on real-world rain degradation, comparing performance with state-of-the-art methods using a high-quality real rain dataset."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions two specific datasets, O-HAZE and SPANet, which are used to compare BaryIR with state-of-the-art methods on real-world haze and rain degradations.",
      "processing_time": 79.4691994190216,
      "citing_paper_id": "278959180",
      "cited_paper_id": 91184545
    },
    {
      "context_text": "We compare BaryIR with SOTA methods on unseen real-world haze O-HAZE [2] and rain SPANet [48] datasets using the five-degradation models.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "O-HAZE",
        "SPANet"
      ],
      "dataset_descriptions": {
        "O-HAZE": "Used to evaluate BaryIR on real-world haze degradation, comparing performance with state-of-the-art methods using a benchmark with real hazy and haze-free outdoor images.",
        "SPANet": "Used to evaluate BaryIR on real-world rain degradation, comparing performance with state-of-the-art methods using a high-quality real rain dataset."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions two specific datasets, O-HAZE and SPANet, which are used to compare BaryIR with state-of-the-art methods on real-world haze and rain degradations.",
      "processing_time": 79.4691994190216,
      "citing_paper_id": "278959180",
      "cited_paper_id": 206598041
    },
    {
      "context_text": "Generalization to unseen real-world O-HAZE [2] and SPANet [48] datasets with the five-degradation models.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "O-HAZE",
        "SPANet"
      ],
      "dataset_descriptions": {
        "O-HAZE": "Used to evaluate generalization of five-degradation models in dehazing real-world images, focusing on performance on unseen data.",
        "SPANet": "Used to evaluate generalization of five-degradation models in deraining real-world images, focusing on performance on unseen data."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions two specific datasets, O-HAZE and SPANet, which are used for evaluating generalization of degradation models in image restoration.",
      "processing_time": 75.98340034484863,
      "citing_paper_id": "278959180",
      "cited_paper_id": 91184545
    },
    {
      "context_text": "Generalization to unseen real-world O-HAZE [2] and SPANet [48] datasets with the five-degradation models.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "O-HAZE",
        "SPANet"
      ],
      "dataset_descriptions": {
        "O-HAZE": "Used to evaluate generalization of five-degradation models in dehazing real-world images, focusing on performance on unseen data.",
        "SPANet": "Used to evaluate generalization of five-degradation models in deraining real-world images, focusing on performance on unseen data."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions two specific datasets, O-HAZE and SPANet, which are used for evaluating generalization of degradation models in image restoration.",
      "processing_time": 75.98340034484863,
      "citing_paper_id": "278959180",
      "cited_paper_id": 206598041
    },
    {
      "context_text": "The problem (1) admits the following dual form [47]: where",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, models, or methods. It only refers to a mathematical formulation.",
      "processing_time": 62.08587622642517,
      "citing_paper_id": "278959180",
      "cited_paper_id": 118347220
    },
    {
      "context_text": "By substituting the optimization over target z Bk ∈ Z B with an equivalent optimization over the barycenter map of interest T (guaranteed by Rockafellar interchange theorem [38], Theorem 3A), we can reformulate Eq.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a mathematical theorem. There are no verifiable resources or datasets mentioned.",
      "processing_time": 62.45448851585388,
      "citing_paper_id": "278959180",
      "cited_paper_id": 118734031
    },
    {
      "context_text": "Given two distributions P ∈ P ( Y ) and Q ∈ P ( X ) with a transport cost function c : X × Y → R + , the Kantorovich formulation [16] of the OT problem is defined as: where π ∈ Π( P , Q ) is a transport plan.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a mathematical formulation of the optimal transport problem.",
      "processing_time": 61.68478178977966,
      "citing_paper_id": "278959180",
      "cited_paper_id": 122853046
    },
    {
      "context_text": "Different from prior works [9, 17, 24] that model individual maps for each source and test on simple domains, we seek the unified representation of high-dimensional multi-source data by learning an NN-based unified barycenter map.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a methodological approach. The context focuses on the difference in modeling approaches rather than the use of specific datasets.",
      "processing_time": 63.94460368156433,
      "citing_paper_id": "278959180",
      "cited_paper_id": 221370709
    },
    {
      "context_text": "The other line of works explores how to express the shared content from different domains with explicit unified representations, e.g., codebooks [4, 27, 30] or prototypes [13, 52].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only methods or approaches such as codebooks and prototypes. No verifiable resources are identified.",
      "processing_time": 63.29582667350769,
      "citing_paper_id": "278959180",
      "cited_paper_id": 235390890
    },
    {
      "context_text": "The other line of works explores how to express the shared content from different domains with explicit unified representations, e.g., codebooks [4, 27, 30] or prototypes [13, 52].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only methods or approaches such as codebooks and prototypes. No verifiable resources are identified.",
      "processing_time": 63.29582667350769,
      "citing_paper_id": "278959180",
      "cited_paper_id": 249848272
    },
    {
      "context_text": "The other line of works explores how to express the shared content from different domains with explicit unified representations, e.g., codebooks [4, 27, 30] or prototypes [13, 52].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only methods or approaches such as codebooks and prototypes. No verifiable resources are identified.",
      "processing_time": 63.29582667350769,
      "citing_paper_id": "278959180",
      "cited_paper_id": 259674925
    },
    {
      "context_text": "Compared to PromptIR [35] which adopts the same backbone (Restormer [55]), BaryIR obtains an average PSNR gain of 0.8 dB. BaryIR also surpasses the recent InstructIR [10] with an average PSNR gain of 0.42 dB and a 13.36 FID decline.",
      "catation_intent": "findings",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only comparisons between methods. No dataset names are present in the context.",
      "processing_time": 62.612024784088135,
      "citing_paper_id": "278959180",
      "cited_paper_id": 244346144
    },
    {
      "context_text": "Compared to PromptIR [35] which adopts the same backbone (Restormer [55]), BaryIR obtains an average PSNR gain of 0.8 dB. BaryIR also surpasses the recent InstructIR [10] with an average PSNR gain of 0.42 dB and a 13.36 FID decline.",
      "catation_intent": "findings",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only comparisons between methods. No dataset names are present in the context.",
      "processing_time": 62.612024784088135,
      "citing_paper_id": "278959180",
      "cited_paper_id": 259224666
    },
    {
      "context_text": "Compared to PromptIR [35] which adopts the same backbone (Restormer [55]), BaryIR obtains an average PSNR gain of 0.8 dB. BaryIR also surpasses the recent InstructIR [10] with an average PSNR gain of 0.42 dB and a 13.36 FID decline.",
      "catation_intent": "findings",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only comparisons between methods. No dataset names are present in the context.",
      "processing_time": 62.612024784088135,
      "citing_paper_id": "278959180",
      "cited_paper_id": 267320695
    },
    {
      "context_text": "For the All-in-One configuration, we compare BaryIR with SOTA methods including three general restorers, i.e., MPRNet [54], Restormer [55], IR-SDE [31]; and five recent All-in-One models, i.e., PromptIR [35], DA-CLIP [32], RCOT [42], DiffUIR [58], and InstructIR [10].",
      "catation_intent": "reusable resource",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only models and methods. No dataset names are present in the text.",
      "processing_time": 62.40132737159729,
      "citing_paper_id": "278959180",
      "cited_paper_id": 244346144
    },
    {
      "context_text": "For the All-in-One configuration, we compare BaryIR with SOTA methods including three general restorers, i.e., MPRNet [54], Restormer [55], IR-SDE [31]; and five recent All-in-One models, i.e., PromptIR [35], DA-CLIP [32], RCOT [42], DiffUIR [58], and InstructIR [10].",
      "catation_intent": "reusable resource",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only models and methods. No dataset names are present in the text.",
      "processing_time": 62.40132737159729,
      "citing_paper_id": "278959180",
      "cited_paper_id": 259224666
    },
    {
      "context_text": "For the All-in-One configuration, we compare BaryIR with SOTA methods including three general restorers, i.e., MPRNet [54], Restormer [55], IR-SDE [31]; and five recent All-in-One models, i.e., PromptIR [35], DA-CLIP [32], RCOT [42], DiffUIR [58], and InstructIR [10].",
      "catation_intent": "reusable resource",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only models and methods. No dataset names are present in the text.",
      "processing_time": 62.40132737159729,
      "citing_paper_id": "278959180",
      "cited_paper_id": 267320695
    },
    {
      "context_text": "For the All-in-One configuration, we compare BaryIR with SOTA methods including three general restorers, i.e., MPRNet [54], Restormer [55], IR-SDE [31]; and five recent All-in-One models, i.e., PromptIR [35], DA-CLIP [32], RCOT [42], DiffUIR [58], and InstructIR [10].",
      "catation_intent": "reusable resource",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only models and methods. No dataset names are present in the text.",
      "processing_time": 62.40132737159729,
      "citing_paper_id": "278959180",
      "cited_paper_id": 268513542
    },
    {
      "context_text": "For the All-in-One configuration, we compare BaryIR with SOTA methods including three general restorers, i.e., MPRNet [54], Restormer [55], IR-SDE [31]; and five recent All-in-One models, i.e., PromptIR [35], DA-CLIP [32], RCOT [42], DiffUIR [58], and InstructIR [10].",
      "catation_intent": "reusable resource",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only models and methods. No dataset names are present in the text.",
      "processing_time": 62.40132737159729,
      "citing_paper_id": "278959180",
      "cited_paper_id": 269605449
    },
    {
      "context_text": "In response to the AIR problem, most existing works [11, 23, 32, 35, 42, 44, 56] leverage degradation-specific information to guide the unified restoration networks by encoding extra degradation-specific signals, e.g., learnable prompts [28, 32, 35, 44], residual embeddings [42, 43], and frequency…",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods and approaches. The cited papers' titles do not provide additional dataset names.",
      "processing_time": 62.88450479507446,
      "citing_paper_id": "278959180",
      "cited_paper_id": 244714491
    },
    {
      "context_text": "In response to the AIR problem, most existing works [11, 23, 32, 35, 42, 44, 56] leverage degradation-specific information to guide the unified restoration networks by encoding extra degradation-specific signals, e.g., learnable prompts [28, 32, 35, 44], residual embeddings [42, 43], and frequency…",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods and approaches. The cited papers' titles do not provide additional dataset names.",
      "processing_time": 62.88450479507446,
      "citing_paper_id": "278959180",
      "cited_paper_id": 250551851
    },
    {
      "context_text": "In response to the AIR problem, most existing works [11, 23, 32, 35, 42, 44, 56] leverage degradation-specific information to guide the unified restoration networks by encoding extra degradation-specific signals, e.g., learnable prompts [28, 32, 35, 44], residual embeddings [42, 43], and frequency…",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods and approaches. The cited papers' titles do not provide additional dataset names.",
      "processing_time": 62.88450479507446,
      "citing_paper_id": "278959180",
      "cited_paper_id": 259224666
    },
    {
      "context_text": "In response to the AIR problem, most existing works [11, 23, 32, 35, 42, 44, 56] leverage degradation-specific information to guide the unified restoration networks by encoding extra degradation-specific signals, e.g., learnable prompts [28, 32, 35, 44], residual embeddings [42, 43], and frequency…",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods and approaches. The cited papers' titles do not provide additional dataset names.",
      "processing_time": 62.88450479507446,
      "citing_paper_id": "278959180",
      "cited_paper_id": 268553835
    },
    {
      "context_text": "In response to the AIR problem, most existing works [11, 23, 32, 35, 42, 44, 56] leverage degradation-specific information to guide the unified restoration networks by encoding extra degradation-specific signals, e.g., learnable prompts [28, 32, 35, 44], residual embeddings [42, 43], and frequency…",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods and approaches. The cited papers' titles do not provide additional dataset names.",
      "processing_time": 62.88450479507446,
      "citing_paper_id": "278959180",
      "cited_paper_id": 269605449
    },
    {
      "context_text": "…to the AIR problem, most existing works [11, 23, 32, 35, 42, 44, 56] leverage degradation-specific information to guide the unified restoration networks by encoding extra degradation-specific signals, e.g., learnable prompts [28, 32, 35, 44], residual embeddings [42, 43], and frequency bands [11].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods and approaches. The cited paper titles do not provide additional dataset names.",
      "processing_time": 62.56671762466431,
      "citing_paper_id": "278959180",
      "cited_paper_id": 244714491
    },
    {
      "context_text": "…to the AIR problem, most existing works [11, 23, 32, 35, 42, 44, 56] leverage degradation-specific information to guide the unified restoration networks by encoding extra degradation-specific signals, e.g., learnable prompts [28, 32, 35, 44], residual embeddings [42, 43], and frequency bands [11].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods and approaches. The cited paper titles do not provide additional dataset names.",
      "processing_time": 62.56671762466431,
      "citing_paper_id": "278959180",
      "cited_paper_id": 259224666
    },
    {
      "context_text": "…to the AIR problem, most existing works [11, 23, 32, 35, 42, 44, 56] leverage degradation-specific information to guide the unified restoration networks by encoding extra degradation-specific signals, e.g., learnable prompts [28, 32, 35, 44], residual embeddings [42, 43], and frequency bands [11].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods and approaches. The cited paper titles do not provide additional dataset names.",
      "processing_time": 62.56671762466431,
      "citing_paper_id": "278959180",
      "cited_paper_id": 268553835
    },
    {
      "context_text": "…to the AIR problem, most existing works [11, 23, 32, 35, 42, 44, 56] leverage degradation-specific information to guide the unified restoration networks by encoding extra degradation-specific signals, e.g., learnable prompts [28, 32, 35, 44], residual embeddings [42, 43], and frequency bands [11].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods and approaches. The cited paper titles do not provide additional dataset names.",
      "processing_time": 62.56671762466431,
      "citing_paper_id": "278959180",
      "cited_paper_id": 269605449
    },
    {
      "context_text": "The majority of existing works aim to align diverse sources/modalities ( e.g., text and images) within a shared latent space [3, 37, 39, 50] or train a source-agnostic encoder to extract information across heterogeneous sources [8, 49].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only general methods and approaches. No dataset names are present in the text.",
      "processing_time": 62.56389117240906,
      "citing_paper_id": "278959180",
      "cited_paper_id": 248085000
    },
    {
      "context_text": "The majority of existing works aim to align diverse sources/modalities ( e.g., text and images) within a shared latent space [3, 37, 39, 50] or train a source-agnostic encoder to extract information across heterogeneous sources [8, 49].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only general methods and approaches. No dataset names are present in the text.",
      "processing_time": 62.56389117240906,
      "citing_paper_id": "278959180",
      "cited_paper_id": 249848207
    },
    {
      "context_text": "The majority of existing works aim to align diverse sources/modalities ( e.g., text and images) within a shared latent space [3, 37, 39, 50] or train a source-agnostic encoder to extract information across heterogeneous sources [8, 49].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only general methods and approaches. No dataset names are present in the text.",
      "processing_time": 62.56389117240906,
      "citing_paper_id": "278959180",
      "cited_paper_id": 254564501
    },
    {
      "context_text": "Pioneer AIR meth-ods typically utilize informative degradation embeddings [7, 11, 23, 32, 35, 42, 53] to guide the restoration.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods and approaches. The cited papers' titles do not provide additional context to identify datasets.",
      "processing_time": 63.51646137237549,
      "citing_paper_id": "278959180",
      "cited_paper_id": 250551851
    },
    {
      "context_text": "Pioneer AIR meth-ods typically utilize informative degradation embeddings [7, 11, 23, 32, 35, 42, 53] to guide the restoration.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods and approaches. The cited papers' titles do not provide additional context to identify datasets.",
      "processing_time": 63.51646137237549,
      "citing_paper_id": "278959180",
      "cited_paper_id": 259224666
    },
    {
      "context_text": "Pioneer AIR meth-ods typically utilize informative degradation embeddings [7, 11, 23, 32, 35, 42, 53] to guide the restoration.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods and approaches. The cited papers' titles do not provide additional context to identify datasets.",
      "processing_time": 63.51646137237549,
      "citing_paper_id": "278959180",
      "cited_paper_id": 268553835
    },
    {
      "context_text": "Pioneer AIR meth-ods typically utilize informative degradation embeddings [7, 11, 23, 32, 35, 42, 53] to guide the restoration.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods and approaches. The cited papers' titles do not provide additional context to identify datasets.",
      "processing_time": 63.51646137237549,
      "citing_paper_id": "278959180",
      "cited_paper_id": 269605449
    },
    {
      "context_text": "Pioneer AIR meth-ods typically utilize informative degradation embeddings [7, 11, 23, 32, 35, 42, 53] to guide the restoration.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods and approaches. The cited papers' titles do not provide additional context to identify datasets.",
      "processing_time": 63.51646137237549,
      "citing_paper_id": "278959180",
      "cited_paper_id": 271891992
    },
    {
      "context_text": "For instance, AirNet [23] trains an extra encoder using contrastive learning to extract degradation embeddings from degraded images.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (AirNet) and a technique (contrastive learning).",
      "processing_time": 62.55204963684082,
      "citing_paper_id": "278959180",
      "cited_paper_id": 250551851
    },
    {
      "context_text": "PromptIR [35] and DA-CLIP [32] employ learnable visual prompts to encode the information of degradation type.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods (PromptIR and DA-CLIP). The context focuses on the use of learnable visual prompts for encoding degradation types, which is a methodological detail rather than a dataset.",
      "processing_time": 66.93118357658386,
      "citing_paper_id": "278959180",
      "cited_paper_id": 259224666
    },
    {
      "context_text": "Following the standard setting of prior works [10], [35], we evaluate on the three-degradation and five-degradation benchmarks.",
      "catation_intent": "reusable resource",
      "resource_type": "benchmark",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.3,
      "reasoning": "The citation mentions 'three-degradation and five-degradation benchmarks' but does not specify them as datasets. These are likely part of a broader evaluation framework rather than specific, downloadable datasets.",
      "processing_time": 65.0789086818695,
      "citing_paper_id": "278959180",
      "cited_paper_id": 259224666
    },
    {
      "context_text": "Following the standard setting of prior works [10], [35], we evaluate on the three-degradation and five-degradation benchmarks.",
      "catation_intent": "reusable resource",
      "resource_type": "benchmark",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.3,
      "reasoning": "The citation mentions 'three-degradation and five-degradation benchmarks' but does not specify them as datasets. These are likely part of a broader evaluation framework rather than specific, downloadable datasets.",
      "processing_time": 65.0789086818695,
      "citing_paper_id": "278959180",
      "cited_paper_id": 267320695
    },
    {
      "context_text": "Besides, BaryIR yields 1.11 dB and 0.97 dB gain on the dehazing and deraining tasks compared to InstructIR [10].",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only performance gains on dehazing and deraining tasks. No verifiable resources are identified.",
      "processing_time": 63.30985999107361,
      "citing_paper_id": "278959180",
      "cited_paper_id": 267320695
    },
    {
      "context_text": "Some other works [1, 10, 53] adopt mixture-of-experts or adaptation modules to process images with different degradations, thereby leading to degradation-aware restoration.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and approaches. There are no clear identifiers for datasets in the provided context.",
      "processing_time": 63.087321043014526,
      "citing_paper_id": "278959180",
      "cited_paper_id": 267320695
    },
    {
      "context_text": "Notably, BaryIR also proceeds InstructIR [10] with 4.11 dB PSNR gain on the dehazing task, demonstrating its robustness to diverse degradations.",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a performance metric (PSNR) and a task (dehazing).",
      "processing_time": 62.96963357925415,
      "citing_paper_id": "278959180",
      "cited_paper_id": 267320695
    },
    {
      "context_text": "2, BaryIR excels Instruc-tIR [10] with an average PSNR gain of 1.11 dB and a 15.53 FID reduction.",
      "catation_intent": "findings",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only performance metrics and a comparison between two methods.",
      "processing_time": 61.68649458885193,
      "citing_paper_id": "278959180",
      "cited_paper_id": 267320695
    },
    {
      "context_text": "Another line of works, e.g., InstructIR [10], DaAIR [53], Histoformer [40], route samples with different degradation patterns to specific experts or architectures for dynamic restoration.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models or methods. The context focuses on routing samples with different degradation patterns to specific experts or architectures.",
      "processing_time": 63.85257577896118,
      "citing_paper_id": "278959180",
      "cited_paper_id": 267320695
    },
    {
      "context_text": "In aerial navigation, the drones [20], [21] are affected by fog [22], [23], [24], rain [25], [26], snow [27], [28], and the cloud reduces the scene visibility while the satellite [29] is remotely sensing the ground level information.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only general environmental conditions affecting aerial navigation and satellite imaging. No verifiable resources are identified.",
      "processing_time": 62.96535778045654,
      "citing_paper_id": "259726182",
      "cited_paper_id": 1372266
    },
    {
      "context_text": "In aerial navigation, the drones [20], [21] are affected by fog [22], [23], [24], rain [25], [26], snow [27], [28], and the cloud reduces the scene visibility while the satellite [29] is remotely sensing the ground level information.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only general environmental conditions affecting aerial navigation and satellite imaging. No verifiable resources are identified.",
      "processing_time": 62.96535778045654,
      "citing_paper_id": "259726182",
      "cited_paper_id": 129948333
    },
    {
      "context_text": "In aerial navigation, the drones [20], [21] are affected by fog [22], [23], [24], rain [25], [26], snow [27], [28], and the cloud reduces the scene visibility while the satellite [29] is remotely sensing the ground level information.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only general environmental conditions affecting aerial navigation and satellite imaging. No verifiable resources are identified.",
      "processing_time": 62.96535778045654,
      "citing_paper_id": "259726182",
      "cited_paper_id": 202143808
    },
    {
      "context_text": "In aerial navigation, the drones [20], [21] are affected by fog [22], [23], [24], rain [25], [26], snow [27], [28], and the cloud reduces the scene visibility while the satellite [29] is remotely sensing the ground level information.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only general environmental conditions affecting aerial navigation and satellite imaging. No verifiable resources are identified.",
      "processing_time": 62.96535778045654,
      "citing_paper_id": "259726182",
      "cited_paper_id": 230511190
    },
    {
      "context_text": "In aerial navigation, the drones [20], [21] are affected by fog [22], [23], [24], rain [25], [26], snow [27], [28], and the cloud reduces the scene visibility while the satellite [29] is remotely sensing the ground level information.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only general environmental conditions affecting aerial navigation and satellite imaging. No verifiable resources are identified.",
      "processing_time": 62.96535778045654,
      "citing_paper_id": "259726182",
      "cited_paper_id": 255626371
    },
    {
      "context_text": "Detailed experiments are carried out for each input domain with five state-of-the-art image-to-image translation models: UNIT [82], pix2pix [83], cycle transformer [84], cycle-GAN [85], CUT [86]; three all-in-one adverse weather removal models: UMWTransformer [79], Wei-Ting et al. [81],…",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only models and methods. There are no clear identifiers for datasets within the text.",
      "processing_time": 63.5560245513916,
      "citing_paper_id": "259726182",
      "cited_paper_id": 6200260
    },
    {
      "context_text": "UNIT [82], pix2pix [83], cycle transformer [84], cycle-GAN [85], and CUT [86] models are trained until they reach minimum error convergence, yet they still struggle to restore images due to their lack of context encoding and effective feature learning ability.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions several models but does not refer to any specific datasets. The focus is on the limitations of these models in image restoration.",
      "processing_time": 63.36903762817383,
      "citing_paper_id": "259726182",
      "cited_paper_id": 6200260
    },
    {
      "context_text": "UNIT [82], pix2pix [83], cycle transformer [84], cycle-GAN [85], and CUT [86] models are trained until they reach minimum error convergence, yet they still struggle to restore images due to their lack of context encoding and effective feature learning ability.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions several models but does not refer to any specific datasets. The focus is on the limitations of these models in image restoration.",
      "processing_time": 63.36903762817383,
      "citing_paper_id": "259726182",
      "cited_paper_id": 220871180
    },
    {
      "context_text": "The attention blocks play a vital role in MACGAN’s optimal performance, while models like pix2pix [83], lacking an attention mechanism, fail to improve visibility across different input domains.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only models and methods. The cited paper title 'Image-to-Image Translation with Conditional Adversarial Networks' confirms that pix2pix is a method, not a dataset.",
      "processing_time": 67.28895807266235,
      "citing_paper_id": "259726182",
      "cited_paper_id": 6200260
    },
    {
      "context_text": "Pix2pix [83] is a supervised image-to-image translation model based on conditional GAN.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions Pix2pix, which is a method, not a dataset. No specific dataset is referenced.",
      "processing_time": 62.350542306900024,
      "citing_paper_id": "259726182",
      "cited_paper_id": 6200260
    },
    {
      "context_text": "When the vehicle is navigating on ground level, haze [10], [11], fog [12], [13], [14], [15], rain [16], [17], and snow [18], [19] are the main causes of its poor performance.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only environmental conditions affecting vehicle navigation. The cited papers' titles suggest methods for image restoration but do not explicitly name datasets.",
      "processing_time": 64.32980012893677,
      "citing_paper_id": "259726182",
      "cited_paper_id": 53039974
    },
    {
      "context_text": "When the vehicle is navigating on ground level, haze [10], [11], fog [12], [13], [14], [15], rain [16], [17], and snow [18], [19] are the main causes of its poor performance.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only environmental conditions affecting vehicle navigation. The cited papers' titles suggest methods for image restoration but do not explicitly name datasets.",
      "processing_time": 64.32980012893677,
      "citing_paper_id": "259726182",
      "cited_paper_id": 216562731
    },
    {
      "context_text": "When the vehicle is navigating on ground level, haze [10], [11], fog [12], [13], [14], [15], rain [16], [17], and snow [18], [19] are the main causes of its poor performance.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only environmental conditions affecting vehicle navigation. The cited papers' titles suggest methods for image restoration but do not explicitly name datasets.",
      "processing_time": 64.32980012893677,
      "citing_paper_id": "259726182",
      "cited_paper_id": 254439006
    },
    {
      "context_text": "Various high-level vision-based tasks like object detection [1], [2], recognition, tracking [3], classification, localization, segmentation [4], scene understanding, analysis, manipulation, and many others have a high dependence on clear images for its optimal performance, due to which intensive…",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only general tasks and their dependencies on clear images. No verifiable resources are identified.",
      "processing_time": 63.15096092224121,
      "citing_paper_id": "259726182",
      "cited_paper_id": 109612799
    },
    {
      "context_text": "Various high-level vision-based tasks like object detection [1], [2], recognition, tracking [3], classification, localization, segmentation [4], scene understanding, analysis, manipulation, and many others have a high dependence on clear images for its optimal performance, due to which intensive…",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only general tasks and their dependencies on clear images. No verifiable resources are identified.",
      "processing_time": 63.15096092224121,
      "citing_paper_id": "259726182",
      "cited_paper_id": 252089325
    },
    {
      "context_text": "Analyzing Table 4, it is evident that FFA-Net [87] performs significantly worse than MACGAN in the haze domain.",
      "catation_intent": "findings",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a comparison between two methods (FFA-Net and MACGAN) in the context of image dehazing.",
      "processing_time": 64.48516345024109,
      "citing_paper_id": "259726182",
      "cited_paper_id": 208138077
    },
    {
      "context_text": "…[83], cycle transformer [84], cycle-GAN [85], CUT [86]; three all-in-one adverse weather removal models: UMWTransformer [79], Wei-Ting et al. [81], Transweather [78]; and six single-effect removal models: FFA-Net [87], Yeying et al. [88], MPRNet [89], DDMSNet [27], GLF-CR [90], and DGD-cGAN [91].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span mentions several models and methods but does not reference any specific datasets. The context is focused on describing various models used for image restoration and dehazing.",
      "processing_time": 64.879141330719,
      "citing_paper_id": "259726182",
      "cited_paper_id": 208138077
    },
    {
      "context_text": "…[83], cycle transformer [84], cycle-GAN [85], CUT [86]; three all-in-one adverse weather removal models: UMWTransformer [79], Wei-Ting et al. [81], Transweather [78]; and six single-effect removal models: FFA-Net [87], Yeying et al. [88], MPRNet [89], DDMSNet [27], GLF-CR [90], and DGD-cGAN [91].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span mentions several models and methods but does not reference any specific datasets. The context is focused on describing various models used for image restoration and dehazing.",
      "processing_time": 64.879141330719,
      "citing_paper_id": "259726182",
      "cited_paper_id": 244714491
    },
    {
      "context_text": "…[83], cycle transformer [84], cycle-GAN [85], CUT [86]; three all-in-one adverse weather removal models: UMWTransformer [79], Wei-Ting et al. [81], Transweather [78]; and six single-effect removal models: FFA-Net [87], Yeying et al. [88], MPRNet [89], DDMSNet [27], GLF-CR [90], and DGD-cGAN [91].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span mentions several models and methods but does not reference any specific datasets. The context is focused on describing various models used for image restoration and dehazing.",
      "processing_time": 64.879141330719,
      "citing_paper_id": "259726182",
      "cited_paper_id": 253708090
    },
    {
      "context_text": "On the other hand, the outputs of FFA-Net [87], Yeying et al. [88], MPRNet [89], DDMSNet [27], GLF-CR [90], and DGD-cGAN [91] are not optimal, most of them failed to restore the degradation properly, whereas, MACGAN outputs showed up-to-the-mark restoration performance.",
      "catation_intent": "findings",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only models and their performance in image restoration tasks.",
      "processing_time": 61.99244570732117,
      "citing_paper_id": "259726182",
      "cited_paper_id": 208138077
    },
    {
      "context_text": "On the other hand, the outputs of FFA-Net [87], Yeying et al. [88], MPRNet [89], DDMSNet [27], GLF-CR [90], and DGD-cGAN [91] are not optimal, most of them failed to restore the degradation properly, whereas, MACGAN outputs showed up-to-the-mark restoration performance.",
      "catation_intent": "findings",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only models and their performance in image restoration tasks.",
      "processing_time": 61.99244570732117,
      "citing_paper_id": "259726182",
      "cited_paper_id": 253708090
    },
    {
      "context_text": "FFA-Net [87] is a feature fusion attention network designed for haze removal, which combines channel and pixel features.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions FFA-Net as a method for haze removal, not as a dataset. No datasets are explicitly mentioned or used in the given context.",
      "processing_time": 64.12995076179504,
      "citing_paper_id": "259726182",
      "cited_paper_id": 208138077
    },
    {
      "context_text": "Linjian et al. [69] proposed a two-stage generative network for cloud removal, the first stage uses an attention block to get a cloud mask, and the second stage uses an autoencoder conditioned by partial convolution to denoise and in-paint the occluded image patches.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation describes a method for cloud removal using a two-stage generative network but does not mention any specific datasets.",
      "processing_time": 62.14698624610901,
      "citing_paper_id": "259726182",
      "cited_paper_id": 209940843
    },
    {
      "context_text": "Similarly, Kui et al. [53] explored the feature similarity across multiple scales of rainy images and used the attention mechanisms for feature fusion according to their correlation and weighted attention to get rain-free images.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for deraining images using attention mechanisms.",
      "processing_time": 62.37734508514404,
      "citing_paper_id": "259726182",
      "cited_paper_id": 214623217
    },
    {
      "context_text": "For defogging, Yan et al. [45] introduced a model that first extracts features, then an attention layer is used to keep significant features by assigning more weight to it and rejecting the rest, the atmospheric light and transmission map is computed using an estimating module with unified…",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a method for image defogging. No dataset names are provided.",
      "processing_time": 63.17909264564514,
      "citing_paper_id": "259726182",
      "cited_paper_id": 219423517
    },
    {
      "context_text": "Xiaoqin et al. [39] restored the hazy images by progressively applying attention to different channel scales keeping the trade-off between the low and high-level features to preserve the structure and color.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific dataset names, only a method for image dehazing. The context focuses on the technique used rather than a particular dataset.",
      "processing_time": 64.71502923965454,
      "citing_paper_id": "259726182",
      "cited_paper_id": 219919444
    },
    {
      "context_text": "Ruoteng et al. [77] proposed an all-in-one model for fog, rain, and snow removal using a neural architectural search technique, they designed domain-specific encoders to handle each type of degradation which increased the number of parameters and computational cost.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a model and its components. The focus is on the method and architecture design.",
      "processing_time": 63.16516351699829,
      "citing_paper_id": "259726182",
      "cited_paper_id": 219978541
    },
    {
      "context_text": "[71] used class-conditioned GAN having channel and spatial attention block.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (class-conditioned GAN with attention blocks).",
      "processing_time": 63.65510034561157,
      "citing_paper_id": "259726182",
      "cited_paper_id": 220835853
    },
    {
      "context_text": "Rain streaks are visible in the outputs of cycleGAN [85] and CUT [86] models, indicating their failure in effectively eliminating rain.",
      "catation_intent": "findings",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models (cycleGAN and CUT). The context focuses on the performance of these models in image restoration tasks, particularly in removing rain streaks.",
      "processing_time": 66.44782638549805,
      "citing_paper_id": "259726182",
      "cited_paper_id": 220871180
    },
    {
      "context_text": "CUT [86] is an unsupervised contrastive learning-based image-to-image translation model.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a model called CUT. The title confirms that CUT is a method, not a dataset.",
      "processing_time": 64.96462631225586,
      "citing_paper_id": "259726182",
      "cited_paper_id": 220871180
    },
    {
      "context_text": "…input domain with five state-of-the-art image-to-image translation models: UNIT [82], pix2pix [83], cycle transformer [84], cycle-GAN [85], CUT [86]; three all-in-one adverse weather removal models: UMWTransformer [79], Wei-Ting et al. [81], Transweather [78]; and six single-effect removal…",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only models and methods. There are no clear identifiers for datasets within the text.",
      "processing_time": 64.50064396858215,
      "citing_paper_id": "259726182",
      "cited_paper_id": 220871180
    },
    {
      "context_text": "Chih-Yang et al. [56] presented an effective deraining model consisting of three modules connected sequentially namely the residual dense block which extracts the features, the sequential dual attention block which retains the most significant spatial and channel-wise features, and the multiscale…",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for deraining images.",
      "processing_time": 62.53081393241882,
      "citing_paper_id": "259726182",
      "cited_paper_id": 221939192
    },
    {
      "context_text": "The robots designed for marine navigation function inefficiently when the scene is affected by the mud and the blue-green color distortion [30], [31], [32], [33].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only issues with marine navigation robots. The cited papers' titles do not provide additional dataset information.",
      "processing_time": 64.26901483535767,
      "citing_paper_id": "259726182",
      "cited_paper_id": 225026402
    },
    {
      "context_text": "The robots designed for marine navigation function inefficiently when the scene is affected by the mud and the blue-green color distortion [30], [31], [32], [33].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only issues with marine navigation robots. The cited papers' titles do not provide additional dataset information.",
      "processing_time": 64.26901483535767,
      "citing_paper_id": "259726182",
      "cited_paper_id": 249314049
    },
    {
      "context_text": "[76] introduced a residual two-fold attention network with non-local and channel attention combined to enhance the features required for proper denoising and color correction.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for image restoration and enhancement.",
      "processing_time": 62.70367622375488,
      "citing_paper_id": "259726182",
      "cited_paper_id": 228987244
    },
    {
      "context_text": "Aiwen et al. [59] removed snow from images by embedding an attention module in GAN which shortlists and encodes significant features required for decoding images without degradation.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for removing snow from images using an attention mechanism in a GAN.",
      "processing_time": 63.488322734832764,
      "citing_paper_id": "259726182",
      "cited_paper_id": 231715795
    },
    {
      "context_text": "Yang et al. [72] presented a multi-scale grid CNN that aggregates the multiple kernels with different scales to boost the receptive field for attentive maps, it adaptively focuses on the feature maps of degraded regions to enhance the underwater image patches consistently.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (multi-scale grid CNN) and its application to underwater image enhancement.",
      "processing_time": 64.50636506080627,
      "citing_paper_id": "259726182",
      "cited_paper_id": 234011980
    },
    {
      "context_text": "Qingyi et al. [46] used an attention network in GAN, first the foggy image is passed to the attention network consisting of residual blocks, LSTM, and convolution layers to generate an attention map with the most discriminating features.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a method (attention network in GAN) used for image defogging.",
      "processing_time": 64.49311780929565,
      "citing_paper_id": "259726182",
      "cited_paper_id": 234107267
    },
    {
      "context_text": "[41] using spatial and channel attention blocks in parallel to highlight significant feature positions and the correlation of features within channels, respectively.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for single-image dehazing.",
      "processing_time": 63.18556761741638,
      "citing_paper_id": "259726182",
      "cited_paper_id": 235689508
    },
    {
      "context_text": "segmentation [4], scene understanding, analysis, manipulation, and many others have a high dependence on clear images for its optimal performance, due to which intensive",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general areas of application. The cited paper title suggests a focus on underwater image segmentation but does not introduce a specific dataset.",
      "processing_time": 66.45007371902466,
      "citing_paper_id": "259726182",
      "cited_paper_id": 238262738
    },
    {
      "context_text": "…a high dependence on clear images for its optimal performance, due to which intensive research in the field of image restoration [5], [6], [7], [8], [9] The associate editor coordinating the review of this manuscript and approving it for publication was Mostafa M. Fouda . can be found in the…",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only a general reference to the field of image restoration. No clear, verifiable datasets are identified.",
      "processing_time": 65.29793429374695,
      "citing_paper_id": "259726182",
      "cited_paper_id": 238803304
    },
    {
      "context_text": "UMWTransformer [79] and Transweather [78] are computationally intensive transformer-based models.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions models but does not refer to any specific datasets. The context is about model computational intensity, not dataset usage.",
      "processing_time": 64.4908435344696,
      "citing_paper_id": "259726182",
      "cited_paper_id": 244714491
    },
    {
      "context_text": "Wei-Ting et al. [81] and Transweather [78] models struggle to completely remove snowflakes.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions models but does not reference any specific datasets. The context is about the performance of models in removing snowflakes from images.",
      "processing_time": 65.14871907234192,
      "citing_paper_id": "259726182",
      "cited_paper_id": 244714491
    },
    {
      "context_text": "Transweather [78] also exhibits visible rain streaks in its output.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions 'Transweather' but does not refer to it as a dataset. It appears to be a method or model, not a dataset.",
      "processing_time": 65.52166604995728,
      "citing_paper_id": "259726182",
      "cited_paper_id": 244714491
    },
    {
      "context_text": "In the case of fog, UMWTransformer [79] effectively restores the fog, while the results of Wei-Ting et al. [81] and Transweather [78] appear distorted.",
      "catation_intent": "findings",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models and their performance. No dataset names are provided in the context.",
      "processing_time": 64.05906677246094,
      "citing_paper_id": "259726182",
      "cited_paper_id": 244714491
    },
    {
      "context_text": "9, it is evident that UMWTransformer [79] and Wei-Ting et al. [81] successfully remove the hazy effect from the images, while Transweather [78] still retains some patches of haze in certain regions.",
      "catation_intent": "findings",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models and their performance. The context focuses on the effectiveness of different models in removing haze from images.",
      "processing_time": 65.27433609962463,
      "citing_paper_id": "259726182",
      "cited_paper_id": 244714491
    },
    {
      "context_text": "Thick clouds remain in the images generated by Transweather [78], and even UMWTransformer [79] and Wei-Ting et al. [81] show regions where restoration is not entirely successful.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions Transweather, UMWTransformer, and Wei-Ting et al. as methods/models, not datasets. No specific datasets are mentioned.",
      "processing_time": 65.28317189216614,
      "citing_paper_id": "259726182",
      "cited_paper_id": 244714491
    },
    {
      "context_text": "Transweather [78] fails to deliver satisfactory in underwater image restoration, as struggles to enhance contrast and remove the muddy effect.",
      "catation_intent": "findings",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions 'Transweather' but does not refer to it as a dataset. It is described as a method or model that fails to perform well in underwater image restoration.",
      "processing_time": 66.43701124191284,
      "citing_paper_id": "259726182",
      "cited_paper_id": 244714491
    },
    {
      "context_text": "In terms of rain scores, UMWTransformer [79] and Wei-Ting et al. [81] achieve higher scores than Transweather [78].",
      "catation_intent": "findings",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only comparisons between different methods. No dataset names are provided in the context.",
      "processing_time": 64.25225138664246,
      "citing_paper_id": "259726182",
      "cited_paper_id": 244714491
    },
    {
      "context_text": "Similarly, Jeya et al. [78] introduced a transformer-based single encoder and decoder model to remove fog, rain, and snow from the images.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for image restoration.",
      "processing_time": 62.728013038635254,
      "citing_paper_id": "259726182",
      "cited_paper_id": 244714491
    },
    {
      "context_text": "are affected by fog [22], [23], [24], rain [25], [26],",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only environmental conditions affecting image quality. No dataset names are present.",
      "processing_time": 63.77901339530945,
      "citing_paper_id": "259726182",
      "cited_paper_id": 246233298
    },
    {
      "context_text": "Xue et al. [65] combined channel attention with residual learning constructing an encoder-decoder-based residual channel attention network for cloud removal.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for cloud removal.",
      "processing_time": 62.72254514694214,
      "citing_paper_id": "259726182",
      "cited_paper_id": 247605912
    },
    {
      "context_text": "al [55] used a UNet architecture with multi-level features of encoder retrieved via channel-wise attention connected to every decoding block recursively and divided the images into wide horizontal patches to explore",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a method (UNet architecture) and a high-level description of the approach. No verifiable resources are identified.",
      "processing_time": 65.99629664421082,
      "citing_paper_id": "259726182",
      "cited_paper_id": 247940251
    },
    {
      "context_text": "Bodong et al. [60] proposed an efficient snow removal model consisting of three modules namely mask-net which has self-pixel and cross-pixel attention to capture significant features and their accurate location to predict the snow mask, guidance-fusion network which adaptively guides the model for…",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for snow removal. No dataset names are provided in the context.",
      "processing_time": 64.43552780151367,
      "citing_paper_id": "259726182",
      "cited_paper_id": 250426326
    },
    {
      "context_text": "Sixiang et al. [62] designed a desnowing model which understands various snow degradation features in a multi-path manner, a local capture module is connected in parallel to consider local details, and using self-attention the scene context information is integrated for generating a clean image.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation describes a method for desnowing images, focusing on the model architecture and its components. No specific dataset is mentioned.",
      "processing_time": 64.4825291633606,
      "citing_paper_id": "259726182",
      "cited_paper_id": 250450940
    },
    {
      "context_text": "Yun et al. [34], [35], [36] developed a nighttime haze removal model by first adding various degradations within the hazy images, divided them into reflectance, illumination, and noise components, then improved the illumination using the prior-based method, improved reflectance contrast in the…",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context describes a method for nighttime haze removal, but does not mention any specific datasets. The focus is on the methodology and components of the model.",
      "processing_time": 65.42976403236389,
      "citing_paper_id": "259726182",
      "cited_paper_id": 251035417
    },
    {
      "context_text": "Yun et al. [34], [35], [36] developed a nighttime haze removal model by first adding various degradations within the hazy images, divided them into reflectance, illumination, and noise components, then improved the illumination using the prior-based method, improved reflectance contrast in the…",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context describes a method for nighttime haze removal, but does not mention any specific datasets. The focus is on the methodology and components of the model.",
      "processing_time": 65.42976403236389,
      "citing_paper_id": "259726182",
      "cited_paper_id": 252477950
    },
    {
      "context_text": "Yitong et al. [42] proposed an unsupervised cycleGAN model embedded with attention-guided modules and total variation loss to limit the noise caused by sea waves for dehazing remote sensing images.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation describes a method (Dehaze-AGGAN) rather than a dataset. No specific dataset is mentioned or used in the context provided.",
      "processing_time": 65.6258852481842,
      "citing_paper_id": "259726182",
      "cited_paper_id": 252148753
    },
    {
      "context_text": "DGD-cGAN [91] manages to correct the blue-green tone, but it generates blurry images, resulting in a lower SSIM value.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a method (DGD-cGAN) and its performance metrics (SSIM).",
      "processing_time": 65.42424750328064,
      "citing_paper_id": "259726182",
      "cited_paper_id": 253708090
    },
    {
      "context_text": "DGD-cGAN [91] consists of a dual generator-based conditional GAN model for improving degraded underwater images.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions a model (DGD-cGAN) rather than a dataset. No specific dataset is named or described.",
      "processing_time": 64.0783679485321,
      "citing_paper_id": "259726182",
      "cited_paper_id": 253708090
    },
    {
      "context_text": "In aerial navigation, the drones [20], [21]",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only references to papers. No clear identifiers for datasets are present.",
      "processing_time": 63.40440034866333,
      "citing_paper_id": "259726182",
      "cited_paper_id": 255825808
    },
    {
      "context_text": "Hao et al. [64] integrated spatial details in a sentinel-2 clean image with spectral patterns in the sentinel-3 image as spatiotemporal guidance to generate missing regions in the cloudy sentinel-2 image by introducing a spatiotemporal attention network consisting of self-attention mechanism,…",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions Sentinel-2 and Sentinel-3 imagery but does not specify them as datasets. They are described as sources of spatial and spectral patterns used in a method for cloud removal.",
      "processing_time": 67.15699577331543,
      "citing_paper_id": "259726182",
      "cited_paper_id": 255915952
    },
    {
      "context_text": "[58] modified the UNet architecture by combining the self-attention transformer block with residue spatial attention along with residue channel loss to preserve global and local semantic infor-",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only modifications to a neural network architecture.",
      "processing_time": 62.80724263191223,
      "citing_paper_id": "259726182",
      "cited_paper_id": 255995928
    },
    {
      "context_text": "…have a high dependence on clear images for its optimal performance, due to which intensive research in the field of image restoration [5], [6], [7], [8], [9] The associate editor coordinating the review of this manuscript and approving it for publication was Mostafa M. Fouda . can be found in…",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only a general reference to the field of image restoration. No verifiable resources are identified.",
      "processing_time": 64.87931251525879,
      "citing_paper_id": "259726182",
      "cited_paper_id": 257122519
    },
    {
      "context_text": "rain [16], [17], and snow [18], [19] are the main causes of its poor performance.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only environmental factors affecting performance. No dataset names are present in the text.",
      "processing_time": 64.68873929977417,
      "citing_paper_id": "259726182",
      "cited_paper_id": 258383425
    },
    {
      "context_text": "…high dependence on clear images for its optimal performance, due to which intensive research in the field of image restoration [5], [6], [7], [8], [9] The associate editor coordinating the review of this manuscript and approving it for publication was Mostafa M. Fouda . can be found in the…",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only a general reference to the field of image restoration. No clear, verifiable resources are identified.",
      "processing_time": 65.86969947814941,
      "citing_paper_id": "259726182",
      "cited_paper_id": null
    },
    {
      "context_text": "…many others have a high dependence on clear images for its optimal performance, due to which intensive research in the field of image restoration [5], [6], [7], [8], [9] The associate editor coordinating the review of this manuscript and approving it for publication was Mostafa M. Fouda . can be…",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only a general reference to the field of image restoration.",
      "processing_time": 63.85428285598755,
      "citing_paper_id": "259726182",
      "cited_paper_id": null
    },
    {
      "context_text": "I MAGE restoration (IR) aims to recover clean and high-quality images from degraded images with adverse degradations, which is critical for subsequent high-level vision tasks such as object detection [1]–[3] and image segmentation [4]–[6].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only general concepts and tasks related to image restoration. No verifiable resources are identified.",
      "processing_time": 64.85646414756775,
      "citing_paper_id": "259937812",
      "cited_paper_id": 1629541
    },
    {
      "context_text": "I MAGE restoration (IR) aims to recover clean and high-quality images from degraded images with adverse degradations, which is critical for subsequent high-level vision tasks such as object detection [1]–[3] and image segmentation [4]–[6].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only general concepts and tasks related to image restoration. No verifiable resources are identified.",
      "processing_time": 64.85646414756775,
      "citing_paper_id": "259937812",
      "cited_paper_id": 218889832
    },
    {
      "context_text": ", haze concentration is related to the scene depth Laina et al. [2016], Ranftl et al.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a relationship between haze concentration and scene depth.",
      "processing_time": 63.83917188644409,
      "citing_paper_id": "259937812",
      "cited_paper_id": 11091110
    },
    {
      "context_text": ", haze concentration is related to the scene depth Laina et al. [2016], Ranftl et al. [2020], Yang et al.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only references to papers discussing haze concentration and scene depth.",
      "processing_time": 64.034903049469,
      "citing_paper_id": "259937812",
      "cited_paper_id": 11091110
    },
    {
      "context_text": ", haze concentration is related to the scene depth Laina et al. [2016], Ranftl et al. [2020], Yang et al. [2022]), degradation matrices obtained from the reference image pair inevitably contains instance-specific statistics (top row of Fig.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only references to papers discussing haze concentration and scene depth. No verifiable resources are identified.",
      "processing_time": 65.25267720222473,
      "citing_paper_id": "259937812",
      "cited_paper_id": 11091110
    },
    {
      "context_text": "The supervising loss function L sup is defined as the combination of SSIM loss [38] and Charbonnier loss [39]: where ˆ y is the predicted value and y is the ground-truth value.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only loss functions used in the training process.",
      "processing_time": 63.03461742401123,
      "citing_paper_id": "259937812",
      "cited_paper_id": 38030033
    },
    {
      "context_text": "Following the previous works Zamir et al. [2022], Wu et al.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to previous works. No dataset names are provided.",
      "processing_time": 64.09600472450256,
      "citing_paper_id": "259937812",
      "cited_paper_id": 52008443
    },
    {
      "context_text": "2(b), we use an encoder with CrossAttention [3] to extract instance-independent information and further produce the target-relevant degradation matrices that are semantically matched with the target degraded image.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (CrossAttention) used in the research.",
      "processing_time": 63.67579960823059,
      "citing_paper_id": "259937812",
      "cited_paper_id": 218889832
    },
    {
      "context_text": "Following the previous works [8], [45], [46], we evaluate the IR performance of different methods in terms of the Peak Signal-to-Noise Ratio (PSNR) and the structural similarity (SSIM) [38].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only evaluation metrics (PSNR and SSIM).",
      "processing_time": 63.494943141937256,
      "citing_paper_id": "259937812",
      "cited_paper_id": 248299814
    },
    {
      "context_text": "Franzen [1999] and BSD68 Martin et al. [2001] datasets.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "BSD68"
      ],
      "dataset_descriptions": {
        "BSD68": "Used to evaluate segmentation algorithms and measure ecological statistics in natural images, focusing on human-segmented data."
      },
      "confidence_score": 0.8,
      "reasoning": "The context mentions 'BSD68' which is a known dataset in image processing, likely referring to the Berkeley Segmentation Dataset. The other reference, Franzen [1999], does not provide enough information to identify a specific dataset.",
      "processing_time": 74.39312362670898,
      "citing_paper_id": "279070747",
      "cited_paper_id": 64193
    },
    {
      "context_text": "Image restoration, a fundamental task within computer vision, seeks to reconstruct high-quality (HQ) images from degraded observations Banham and Katsaggelos [1997], Su et al. [2022-05].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general reference to image restoration. No clear, verifiable datasets are identified.",
      "processing_time": 65.02831792831421,
      "citing_paper_id": "279070747",
      "cited_paper_id": 8235201
    },
    {
      "context_text": "Privilege Learning Vapnik and Vashist [2009], Vapnik et al. [2015] offers an elegant solution by leveraging additional information during training that enhances the learning process.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method or approach called 'Privileged Learning'. No verifiable resources are identified.",
      "processing_time": 64.96983361244202,
      "citing_paper_id": "279070747",
      "cited_paper_id": 12874183
    },
    {
      "context_text": "Privilege Learning (PL), formally introduced as Learning Using Privileged Information (LUPI) Vapnik and Vashist [2009], Vapnik et al. [2015], is a machine learning paradigm where auxiliary information, inaccessible during inference, is provided to the model during training.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a machine learning paradigm called Privilege Learning (PL) or Learning Using Privileged Information (LUPI).",
      "processing_time": 66.1777400970459,
      "citing_paper_id": "279070747",
      "cited_paper_id": 12874183
    },
    {
      "context_text": "To address this fundamental limitation, we draw inspiration from Privilege Learning (PL) Vapnik and Vashist [2009], Vapnik et al. [2015].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method called Privilege Learning. No datasets are referenced for extraction.",
      "processing_time": 64.48967289924622,
      "citing_paper_id": "279070747",
      "cited_paper_id": 12874183
    },
    {
      "context_text": "2 Related Work 2.1 All-in-One Image Restoration Traditional image restoration typically targets specific degradations, such as noise or blur, using specialized models Zamir et al. [2022], Huang et al. [2023], Cai et al. [2023].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to methods and models. No verifiable resources are identified.",
      "processing_time": 64.76189470291138,
      "citing_paper_id": "279070747",
      "cited_paper_id": 244346144
    },
    {
      "context_text": "2 Related Work 2.1 All-in-One Image Restoration Traditional image restoration typically targets specific degradations, such as noise or blur, using specialized models Zamir et al. [2022], Huang et al. [2023], Cai et al. [2023].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to methods and models. No verifiable resources are identified.",
      "processing_time": 64.76189470291138,
      "citing_paper_id": "279070747",
      "cited_paper_id": 249432930
    },
    {
      "context_text": "Early efforts toward unified models utilized powerful backbones Chen et al. [2022, 2021], Wang et al. [2022], Zamir et al. [2022], Wang et al. [2024], Guo et al. [2024a] or generative approaches like diffusion models Belhasin et al. [2024], Yue and Loy [2024], Li et al. [2025], Yue et al. [2025].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only models and methods. The cited papers' titles also do not indicate the use of specific datasets.",
      "processing_time": 65.70137739181519,
      "citing_paper_id": "279070747",
      "cited_paper_id": 244346144
    },
    {
      "context_text": "Early efforts toward unified models utilized powerful backbones Chen et al. [2022, 2021], Wang et al. [2022], Zamir et al. [2022], Wang et al. [2024], Guo et al. [2024a] or generative approaches like diffusion models Belhasin et al. [2024], Yue and Loy [2024], Li et al. [2025], Yue et al. [2025].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only models and methods. The cited papers' titles also do not indicate the use of specific datasets.",
      "processing_time": 65.70137739181519,
      "citing_paper_id": "279070747",
      "cited_paper_id": 254591838
    },
    {
      "context_text": "Early efforts toward unified models utilized powerful backbones Chen et al. [2022, 2021], Wang et al. [2022], Zamir et al. [2022], Wang et al. [2024], Guo et al. [2024a] or generative approaches like diffusion models Belhasin et al. [2024], Yue and Loy [2024], Li et al. [2025], Yue et al. [2025].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only models and methods. The cited papers' titles also do not indicate the use of specific datasets.",
      "processing_time": 65.70137739181519,
      "citing_paper_id": "279070747",
      "cited_paper_id": 267938238
    },
    {
      "context_text": "Early efforts toward unified models utilized powerful backbones Chen et al. [2022, 2021], Wang et al. [2022], Zamir et al. [2022], Wang et al. [2024], Guo et al. [2024a] or generative approaches like diffusion models Belhasin et al. [2024], Yue and Loy [2024], Li et al. [2025], Yue et al. [2025].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only models and methods. The cited papers' titles also do not indicate the use of specific datasets.",
      "processing_time": 65.70137739181519,
      "citing_paper_id": "279070747",
      "cited_paper_id": 273230367
    },
    {
      "context_text": "Other methods use degradation priors from classifiers Kong et al. [2024] or vision-language models like CLIP with textual prompts Luo et al. [2024], Lin et al. [2024].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and models. The context focuses on using degradation priors from classifiers and vision-language models, which are not datasets.",
      "processing_time": 66.70322370529175,
      "citing_paper_id": "279070747",
      "cited_paper_id": 266690738
    },
    {
      "context_text": "…to handle diverse restoration tasks, leading to a focus on conditioning mechanisms, from early degradation encoders Li et al. [2022] to advanced learnable prompts Potlapalli et al. [2023], sometimes refined with frequency priors or dimensionality reduction Cui et al. [2025], Zhang et al. [2023].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods and approaches. No verifiable resources are identified.",
      "processing_time": 64.22356820106506,
      "citing_paper_id": "279070747",
      "cited_paper_id": 268553835
    },
    {
      "context_text": "However, most existing image restoration technologies are designed for single tasks[1–3], with limited applicability and a lack of generality.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only a general limitation of existing image restoration technologies.",
      "processing_time": 63.55102038383484,
      "citing_paper_id": "273119576",
      "cited_paper_id": 996788
    },
    {
      "context_text": "Derain The Rain100L[16] dataset is a commonly used dataset for image deraining tasks, containing 300 pairs of images, with each pair consisting of a synthetic rain-streaked image and a corresponding clear image without rain.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Rain100L"
      ],
      "dataset_descriptions": {
        "Rain100L": "Used for image deraining tasks, containing 300 pairs of synthetic rain-streaked images and corresponding clear images. This dataset supports the evaluation of deraining algorithms."
      },
      "confidence_score": 1.0,
      "reasoning": "The context clearly mentions a specific dataset, 'Rain100L', which is used for image deraining tasks. The dataset is described as containing 300 pairs of images, each pair including a synthetic rain-streaked image and a clear image.",
      "processing_time": 77.63663148880005,
      "citing_paper_id": "273119576",
      "cited_paper_id": 15443600
    },
    {
      "context_text": "The application of the Mamba model in this field is still in its infancy, and its future prospects are still promising: (1) Incorporating prior UMR[22] MSPFN[23] LPNet[19] AirNet[10] Restormer[3] PromptIR[4]",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only models and methods. The cited papers' titles do not provide additional dataset information.",
      "processing_time": 65.28096413612366,
      "citing_paper_id": "273119576",
      "cited_paper_id": 195657934
    },
    {
      "context_text": "The application of the Mamba model in this field is still in its infancy, and its future prospects are still promising: (1) Incorporating prior UMR[22] MSPFN[23] LPNet[19] AirNet[10] Restormer[3] PromptIR[4]",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only models and methods. The cited papers' titles do not provide additional dataset information.",
      "processing_time": 65.28096413612366,
      "citing_paper_id": "273119576",
      "cited_paper_id": 250551851
    },
    {
      "context_text": "The application of the Mamba model in this field is still in its infancy, and its future prospects are still promising: (1) Incorporating prior UMR[22] MSPFN[23] LPNet[19] AirNet[10] Restormer[3] PromptIR[4]",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only models and methods. The cited papers' titles do not provide additional dataset information.",
      "processing_time": 65.28096413612366,
      "citing_paper_id": "273119576",
      "cited_paper_id": 259224666
    },
    {
      "context_text": "The AirNet[10] proposed by Li Boyun and colleagues consists of two neural modules: a contrast-based degradation encoder (CBDE) and a degradation-guided restoration network.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions AirNet, which is a method, not a dataset. No specific datasets are mentioned or used according to the given context.",
      "processing_time": 65.44084000587463,
      "citing_paper_id": "273119576",
      "cited_paper_id": 250551851
    },
    {
      "context_text": "Some current All-in-One image restoration methods[4] replace the original convolutional layers or pooling layers of the U-Net[5] network structure with Transformer-based blocks, which experience a quadratic increase in computational load with the size of the image, making it impossible to process…",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only discusses methodological changes in network structures.",
      "processing_time": 63.5263192653656,
      "citing_paper_id": "273119576",
      "cited_paper_id": 259224666
    },
    {
      "context_text": "Vaishnav Potlapalli, in collaboration with Syed Waqas Zamir and others, proposed Promp-tIR[4] based on Restormer, which uses a prompt plug-in module to encode specific degradation information and dynamically guides the restoration network using this information, thereby achieving effective…",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method called PromptIR. No verifiable resources are identified.",
      "processing_time": 64.40420317649841,
      "citing_paper_id": "273119576",
      "cited_paper_id": 259224666
    },
    {
      "context_text": "Vaishnav Potlapalli, in collaboration with Syed Waqas Zamir and others, proposed Promp-tIR[4] based on Restormer, which uses a prompt plug-in module to encode specific degradation information and dynamically guides the restoration network using this information, thereby achieving effective generalization for different types and levels of degradation without any prior degradation information.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a method (PromptIR) and its capabilities. No verifiable resources are identified.",
      "processing_time": 65.15760064125061,
      "citing_paper_id": "273119576",
      "cited_paper_id": 259224666
    },
    {
      "context_text": "The core idea of implicit prompting is to generate a series of learnable parameter matrices that will gradually learn the degradation information during the training process[4].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for image restoration.",
      "processing_time": 62.84631609916687,
      "citing_paper_id": "273119576",
      "cited_paper_id": 259224666
    },
    {
      "context_text": "As depicted in Figure 4, the core operations of the explicit prompting block involve using a CNN network to extract features from the degraded image and appending a small module after each block of the reconstruction module[15].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method or model architecture. The context focuses on the operations within a neural network block.",
      "processing_time": 65.60365033149719,
      "citing_paper_id": "273119576",
      "cited_paper_id": 266844752
    },
    {
      "context_text": "Traditional degradation estimation methods [1], [23] often assume a predefined degradation type and estimate degradation level, which makes them less effective in scenarios with multiple unknown degradations.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only general methods and their limitations.",
      "processing_time": 63.10567665100098,
      "citing_paper_id": "270878215",
      "cited_paper_id": 996788
    },
    {
      "context_text": "Particularly, convolutional neural networks (CNNs) have demonstrated remarkable efficacy in tasks like image denoising [1], [2], [25], [26], dehazing [27]–[30], deraining [31]–[34], and deblurring [35]–[38].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only tasks and methods. The cited papers' titles also do not indicate the use of specific datasets.",
      "processing_time": 66.14369893074036,
      "citing_paper_id": "270878215",
      "cited_paper_id": 996788
    },
    {
      "context_text": "Particularly, convolutional neural networks (CNNs) have demonstrated remarkable efficacy in tasks like image denoising [1], [2], [25], [26], dehazing [27]–[30], deraining [31]–[34], and deblurring [35]–[38].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only tasks and methods. The cited papers' titles also do not indicate the use of specific datasets.",
      "processing_time": 66.14369893074036,
      "citing_paper_id": "270878215",
      "cited_paper_id": 1887989
    },
    {
      "context_text": "Particularly, convolutional neural networks (CNNs) have demonstrated remarkable efficacy in tasks like image denoising [1], [2], [25], [26], dehazing [27]–[30], deraining [31]–[34], and deblurring [35]–[38].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only tasks and methods. The cited papers' titles also do not indicate the use of specific datasets.",
      "processing_time": 66.14369893074036,
      "citing_paper_id": "270878215",
      "cited_paper_id": 14092238
    },
    {
      "context_text": "Particularly, convolutional neural networks (CNNs) have demonstrated remarkable efficacy in tasks like image denoising [1], [2], [25], [26], dehazing [27]–[30], deraining [31]–[34], and deblurring [35]–[38].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only tasks and methods. The cited papers' titles also do not indicate the use of specific datasets.",
      "processing_time": 66.14369893074036,
      "citing_paper_id": "270878215",
      "cited_paper_id": 17115407
    },
    {
      "context_text": "Particularly, convolutional neural networks (CNNs) have demonstrated remarkable efficacy in tasks like image denoising [1], [2], [25], [26], dehazing [27]–[30], deraining [31]–[34], and deblurring [35]–[38].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only tasks and methods. The cited papers' titles also do not indicate the use of specific datasets.",
      "processing_time": 66.14369893074036,
      "citing_paper_id": "270878215",
      "cited_paper_id": 119188226
    },
    {
      "context_text": "Particularly, convolutional neural networks (CNNs) have demonstrated remarkable efficacy in tasks like image denoising [1], [2], [25], [26], dehazing [27]–[30], deraining [31]–[34], and deblurring [35]–[38].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only tasks and methods. The cited papers' titles also do not indicate the use of specific datasets.",
      "processing_time": 66.14369893074036,
      "citing_paper_id": "270878215",
      "cited_paper_id": 203042751
    },
    {
      "context_text": "Particularly, convolutional neural networks (CNNs) have demonstrated remarkable efficacy in tasks like image denoising [1], [2], [25], [26], dehazing [27]–[30], deraining [31]–[34], and deblurring [35]–[38].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only tasks and methods. The cited papers' titles also do not indicate the use of specific datasets.",
      "processing_time": 66.14369893074036,
      "citing_paper_id": "270878215",
      "cited_paper_id": 257767233
    },
    {
      "context_text": "…approaches in multi-degradation restoration, we mainly employ seven datasets: BSD400 [68], BSD68 [68], WED [69], and Urban100 [70] for image denoising, Rain100L [71] for image deraining, RESIDE [72] for image dehazing, GoPro [73] for image deblurring, and LOL dataset [74] for low-light enhancement.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "BSD400",
        "BSD68",
        "WED",
        "Urban100",
        "Rain100L",
        "RESIDE",
        "GoPro",
        "LOL dataset"
      ],
      "dataset_descriptions": {
        "BSD400": "Used for image denoising, providing a benchmark for evaluating denoising algorithms in the context of multi-degradation restoration.",
        "BSD68": "Used for image denoising, serving as a smaller benchmark set for evaluating denoising performance.",
        "WED": "Used for image denoising, offering a diverse set of images to test the robustness of denoising methods.",
        "Urban100": "Used for image denoising, focusing on urban scenes to assess the effectiveness of denoising techniques in complex environments.",
        "Rain100L": "Used for image deraining, providing a large set of rainy images to evaluate deraining algorithms.",
        "RESIDE": "Used for image dehazing, offering a comprehensive dataset to test and compare dehazing methods.",
        "GoPro": "Used for image deblurring, providing high-quality video frames to evaluate deblurring performance.",
        "LOL dataset": "Used for low-light enhancement, offering a variety of low-light images to assess the effectiveness of enhancement techniques."
      },
      "confidence_score": 1.0,
      "reasoning": "The context explicitly lists several datasets used for various image restoration tasks, which are directly relevant to the research topic of all-in-one image restoration.",
      "processing_time": 112.47237420082092,
      "citing_paper_id": "270878215",
      "cited_paper_id": 39760169
    },
    {
      "context_text": "…approaches in multi-degradation restoration, we mainly employ seven datasets: BSD400 [68], BSD68 [68], WED [69], and Urban100 [70] for image denoising, Rain100L [71] for image deraining, RESIDE [72] for image dehazing, GoPro [73] for image deblurring, and LOL dataset [74] for low-light enhancement.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "BSD400",
        "BSD68",
        "WED",
        "Urban100",
        "Rain100L",
        "RESIDE",
        "GoPro",
        "LOL dataset"
      ],
      "dataset_descriptions": {
        "BSD400": "Used for image denoising, providing a benchmark for evaluating denoising algorithms in the context of multi-degradation restoration.",
        "BSD68": "Used for image denoising, serving as a smaller benchmark set for evaluating denoising performance.",
        "WED": "Used for image denoising, offering a diverse set of images to test the robustness of denoising methods.",
        "Urban100": "Used for image denoising, focusing on urban scenes to assess the effectiveness of denoising techniques in complex environments.",
        "Rain100L": "Used for image deraining, providing a large set of rainy images to evaluate deraining algorithms.",
        "RESIDE": "Used for image dehazing, offering a comprehensive dataset to test and compare dehazing methods.",
        "GoPro": "Used for image deblurring, providing high-quality video frames to evaluate deblurring performance.",
        "LOL dataset": "Used for low-light enhancement, offering a variety of low-light images to assess the effectiveness of enhancement techniques."
      },
      "confidence_score": 1.0,
      "reasoning": "The context explicitly lists several datasets used for various image restoration tasks, which are directly relevant to the research topic of all-in-one image restoration.",
      "processing_time": 112.47237420082092,
      "citing_paper_id": "270878215",
      "cited_paper_id": 52008443
    },
    {
      "context_text": "…approaches in multi-degradation restoration, we mainly employ seven datasets: BSD400 [68], BSD68 [68], WED [69], and Urban100 [70] for image denoising, Rain100L [71] for image deraining, RESIDE [72] for image dehazing, GoPro [73] for image deblurring, and LOL dataset [74] for low-light enhancement.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "BSD400",
        "BSD68",
        "WED",
        "Urban100",
        "Rain100L",
        "RESIDE",
        "GoPro",
        "LOL dataset"
      ],
      "dataset_descriptions": {
        "BSD400": "Used for image denoising, providing a benchmark for evaluating denoising algorithms in the context of multi-degradation restoration.",
        "BSD68": "Used for image denoising, serving as a smaller benchmark set for evaluating denoising performance.",
        "WED": "Used for image denoising, offering a diverse set of images to test the robustness of denoising methods.",
        "Urban100": "Used for image denoising, focusing on urban scenes to assess the effectiveness of denoising techniques in complex environments.",
        "Rain100L": "Used for image deraining, providing a large set of rainy images to evaluate deraining algorithms.",
        "RESIDE": "Used for image dehazing, offering a comprehensive dataset to test and compare dehazing methods.",
        "GoPro": "Used for image deblurring, providing high-quality video frames to evaluate deblurring performance.",
        "LOL dataset": "Used for low-light enhancement, offering a variety of low-light images to assess the effectiveness of enhancement techniques."
      },
      "confidence_score": 1.0,
      "reasoning": "The context explicitly lists several datasets used for various image restoration tasks, which are directly relevant to the research topic of all-in-one image restoration.",
      "processing_time": 112.47237420082092,
      "citing_paper_id": "270878215",
      "cited_paper_id": 73439498
    },
    {
      "context_text": "Informed by recent studies, including Gridformer [82] and InstructIR [19], we extended the existing four degradation types by incorporating the LOL dataset [74] for low-light degradation to further validate the efficacy of the proposed method.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "LOL dataset"
      ],
      "dataset_descriptions": {
        "LOL dataset": "Used to validate the efficacy of the proposed method for low-light image degradation, focusing on enhancing images under low-light conditions."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions the LOL dataset, which is a specific dataset used for low-light image degradation. The dataset is used to validate the efficacy of the proposed method.",
      "processing_time": 72.07039475440979,
      "citing_paper_id": "270878215",
      "cited_paper_id": 52008443
    },
    {
      "context_text": "Informed by recent studies, including Gridformer [82] and InstructIR [19], we extended the existing four degradation types by incorporating the LOL dataset [74] for low-light degradation to further validate the efficacy of the proposed method.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "LOL dataset"
      ],
      "dataset_descriptions": {
        "LOL dataset": "Used to validate the efficacy of the proposed method for low-light image degradation, focusing on enhancing images under low-light conditions."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions the LOL dataset, which is a specific dataset used for low-light image degradation. The dataset is used to validate the efficacy of the proposed method.",
      "processing_time": 72.07039475440979,
      "citing_paper_id": "270878215",
      "cited_paper_id": 258959359
    },
    {
      "context_text": "Informed by recent studies, including Gridformer [82] and InstructIR [19], we extended the existing four degradation types by incorporating the LOL dataset [74] for low-light degradation to further validate the efficacy of the proposed method.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "LOL dataset"
      ],
      "dataset_descriptions": {
        "LOL dataset": "Used to validate the efficacy of the proposed method for low-light image degradation, focusing on enhancing images under low-light conditions."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions the LOL dataset, which is a specific dataset used for low-light image degradation. The dataset is used to validate the efficacy of the proposed method.",
      "processing_time": 72.07039475440979,
      "citing_paper_id": "270878215",
      "cited_paper_id": 267320695
    },
    {
      "context_text": "Specifically, we injected Gaussian noise with a standard deviation of σ =25 to the degraded images from the rain removal dataset Rain100L [71] and the blur removal dataset GoPro [73], constructing a test set to assess the models trained under the 4 tasks All-in-one setting.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Rain100L",
        "GoPro"
      ],
      "dataset_descriptions": {
        "Rain100L": "Used to inject Gaussian noise into degraded images for testing models trained under the All-in-One setting, focusing on rain removal.",
        "GoPro": "Used to inject Gaussian noise into degraded images for testing models trained under the All-in-One setting, focusing on blur removal."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions two specific datasets, Rain100L and GoPro, which are used to construct a test set for assessing models trained under the All-in-One setting.",
      "processing_time": 79.80320429801941,
      "citing_paper_id": "270878215",
      "cited_paper_id": 73439498
    },
    {
      "context_text": "Our comparison encompasses four single-degradation image restoration techniques, namely BDRNet [75], LP-Net [76], FDGAN [28], and MPRNet [52], alongside the multi-task method for multiple degradation image restoration, DL [77].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only models and methods. No verifiable resources are identified.",
      "processing_time": 64.65012168884277,
      "citing_paper_id": "270878215",
      "cited_paper_id": 195787503
    },
    {
      "context_text": "Our comparison encompasses four single-degradation image restoration techniques, namely BDRNet [75], LP-Net [76], FDGAN [28], and MPRNet [52], alongside the multi-task method for multiple degradation image restoration, DL [77].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only models and methods. No verifiable resources are identified.",
      "processing_time": 64.65012168884277,
      "citing_paper_id": "270878215",
      "cited_paper_id": 231802205
    },
    {
      "context_text": "For instance, Yang et al. [60] utilize discrete wavelet transform and inverse discrete wavelet transform to replace down-sampling and up-sampling, facilitating the extraction of edge features.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and transformations used for image dehazing.",
      "processing_time": 63.90766882896423,
      "citing_paper_id": "270878215",
      "cited_paper_id": 202781591
    },
    {
      "context_text": "Wei et al. [54] and Li et al. [15] pioneered this approach by introducing a novel method that utilizes contrastive learning to extract degradation representations without prior knowledge of the types and levels of degradations, thereby guiding the restoration process.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and approaches. There are no clear identifiers for datasets in the provided context.",
      "processing_time": 65.06475710868835,
      "citing_paper_id": "270878215",
      "cited_paper_id": 214775210
    },
    {
      "context_text": "Recent research [13], [14] has focused on training a single model to address multiple image restoration tasks simultaneously by incorporating separate modules for each task in parallel at the input and output layers.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a methodological approach to training models for multiple image restoration tasks.",
      "processing_time": 64.46885681152344,
      "citing_paper_id": "270878215",
      "cited_paper_id": 227239228
    },
    {
      "context_text": "Building on the success of vision-language models [55], recent studies [56]–[58] have integrated language instructions into unified image restoration approaches.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to vision-language models and image restoration approaches. No verifiable resources are identified.",
      "processing_time": 65.41056394577026,
      "citing_paper_id": "270878215",
      "cited_paper_id": 231591445
    },
    {
      "context_text": "Building on the success of vision-language models [55], recent studies [56]–[58] have integrated language instructions into unified image restoration approaches.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to vision-language models and image restoration approaches. No verifiable resources are identified.",
      "processing_time": 65.41056394577026,
      "citing_paper_id": "270878215",
      "cited_paper_id": 264146360
    },
    {
      "context_text": "In this work, we adopt the Uformer [43] as a specific instantiation of Rformer.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions Uformer but does not indicate it is a dataset. It is described as a specific instantiation of Rformer, which suggests it is a method or model.",
      "processing_time": 66.68056774139404,
      "citing_paper_id": "270878215",
      "cited_paper_id": 235358213
    },
    {
      "context_text": "Without frequency modulation, Uformer [43] fails to provide a discriminative capability superior to PromptIR [17].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models (Uformer and PromptIR). The context focuses on comparing the performance of these models without referring to any datasets.",
      "processing_time": 66.45800614356995,
      "citing_paper_id": "270878215",
      "cited_paper_id": 235358213
    },
    {
      "context_text": "…degradations but also effectively discriminates between different degradations, we perform t-SNE visualization on standard “noise-rain-haze” setting on the latent degradation representations obtained by PromptIR [17], Uformer [43] (i.e., the Rformer without frequency modulation), and our method.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific, verifiable datasets. It refers to methods and visualizations but does not cite any dataset names.",
      "processing_time": 65.18741583824158,
      "citing_paper_id": "270878215",
      "cited_paper_id": 235358213
    },
    {
      "context_text": "Similarly, Wang et al. [43] enhance the decoder layers by replacing the Feed-Forward Network (FFN) with a Locally-enhanced FFN and introducing a learnable modulator before the input of Multi-head Self-attention, allowing for flexible adjustment of feature maps.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only model architecture enhancements. No dataset names are present in the citation span.",
      "processing_time": 64.56804251670837,
      "citing_paper_id": "270878215",
      "cited_paper_id": 235358213
    },
    {
      "context_text": "In experiments, We leverage Uformer [43] as the Rformer, due to its extensively validated performance and computational efficiency across various restoration tasks.",
      "catation_intent": "reusable resource",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions Uformer but does not indicate it is a dataset. It is described as a method or model used for image restoration tasks.",
      "processing_time": 65.5607738494873,
      "citing_paper_id": "270878215",
      "cited_paper_id": 235358213
    },
    {
      "context_text": "Moreover, several recent studies [66], [67] have investigated the biases of different modules in the frequency domain.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general domain of investigation.",
      "processing_time": 62.87730669975281,
      "citing_paper_id": "270878215",
      "cited_paper_id": 247411040
    },
    {
      "context_text": "However, with their potent feature representation capabilities, transformer architectures [39] have emerged as a superior alternative to CNNs in image restoration tasks [40]–[46].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only the use of transformer architectures in image restoration tasks.",
      "processing_time": 64.05826115608215,
      "citing_paper_id": "270878215",
      "cited_paper_id": 248085101
    },
    {
      "context_text": "However, with their potent feature representation capabilities, transformer architectures [39] have emerged as a superior alternative to CNNs in image restoration tasks [40]–[46].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only the use of transformer architectures in image restoration tasks.",
      "processing_time": 64.05826115608215,
      "citing_paper_id": "270878215",
      "cited_paper_id": 252693111
    },
    {
      "context_text": "Consequently, Swin Transformer has become a popular choice as the backbone for low-level image restoration models [41]–[44].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (Swin Transformer) used for image restoration.",
      "processing_time": 64.01019191741943,
      "citing_paper_id": "270878215",
      "cited_paper_id": 249062606
    },
    {
      "context_text": "Mao et al. [61] focus on discerning the differences between blurry and sharp images based on low-frequency and high-frequency components, explicitly segregating the processing of these components using Fast Fourier Transform, and ultimately integrating the residual information from both high and…",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a method for image deblurring. The citation is focused on the technique rather than a dataset.",
      "processing_time": 65.50206637382507,
      "citing_paper_id": "270878215",
      "cited_paper_id": 254069870
    },
    {
      "context_text": "Park et al. [20] introduced an adaptive discriminant filter-based degradation classifier to explicitly disentangle the network for multiple degradations.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for image restoration. The context focuses on the introduction of a degradation classifier and does not reference any particular dataset.",
      "processing_time": 66.44377374649048,
      "citing_paper_id": "270878215",
      "cited_paper_id": 260107992
    },
    {
      "context_text": "GoPro [44] Rain100L [45] BSD68 [46] SOTS [47] Average Methods PSNR ↑ SSIM ↑ PSNR ↑ SSIM ↑ PSNR ↑ SSIM ↑ PSNR ↑ SSIM ↑ PSNR ↑ SSIM ↑ HINet [59] 30.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "GoPro",
        "Rain100L",
        "BSD68",
        "SOTS"
      ],
      "dataset_descriptions": {
        "GoPro": "Used to evaluate image restoration methods, focusing on deblurring performance using natural images captured with GoPro cameras.",
        "Rain100L": "Used to assess rain removal techniques, specifically measuring the effectiveness of methods in restoring images degraded by rain.",
        "BSD68": "Used to evaluate denoising performance, focusing on the quality of restored images from a benchmark set of 68 images.",
        "SOTS": "Used to test dehazing algorithms, specifically measuring the ability to restore images degraded by atmospheric conditions."
      },
      "confidence_score": 1.0,
      "reasoning": "The citation mentions several datasets by name, which are likely used for evaluating image restoration methods. The context suggests these datasets are used for benchmarking performance.",
      "processing_time": 88.40549302101135,
      "citing_paper_id": "272680196",
      "cited_paper_id": 64193
    },
    {
      "context_text": "I, including Rain14000 [50], Rain1800 [45], Rain12 [51] and Rain800 [52] for deraining, RESIDE [47] for dehazing, BSD300 [46] and WED [55] for denoising and GoPro [44] for deblurring.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Rain14000",
        "Rain1800",
        "Rain12",
        "Rain800",
        "RESIDE",
        "BSD300",
        "WED",
        "GoPro"
      ],
      "dataset_descriptions": {
        "Rain14000": "Used for deraining, providing a large set of images with rain streaks for training and evaluation.",
        "Rain1800": "Used for deraining, offering a moderate-sized dataset with diverse rain conditions.",
        "Rain12": "Used for deraining, containing a small but challenging set of images with heavy rain.",
        "Rain800": "Used for deraining, providing a balanced dataset with varied rain intensities.",
        "RESIDE": "Used for dehazing, containing synthetic and real-world hazy images for training and testing.",
        "BSD300": "Used for denoising, providing a well-known dataset of natural images with noise.",
        "WED": "Used for denoising, offering a diverse set of images with varying noise levels.",
        "GoPro": "Used for deblurring, containing high-resolution video frames with motion blur."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions several datasets used for various image restoration tasks, such as deraining, dehazing, denoising, and deblurring. These datasets are specific and relevant to the topic of all-in-one image restoration.",
      "processing_time": 109.8855664730072,
      "citing_paper_id": "272680196",
      "cited_paper_id": 64193
    },
    {
      "context_text": "I, including Rain14000 [50], Rain1800 [45], Rain12 [51] and Rain800 [52] for deraining, RESIDE [47] for dehazing, BSD300 [46] and WED [55] for denoising and GoPro [44] for deblurring.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Rain14000",
        "Rain1800",
        "Rain12",
        "Rain800",
        "RESIDE",
        "BSD300",
        "WED",
        "GoPro"
      ],
      "dataset_descriptions": {
        "Rain14000": "Used for deraining, providing a large set of images with rain streaks for training and evaluation.",
        "Rain1800": "Used for deraining, offering a moderate-sized dataset with diverse rain conditions.",
        "Rain12": "Used for deraining, containing a small but challenging set of images with heavy rain.",
        "Rain800": "Used for deraining, providing a balanced dataset with varied rain intensities.",
        "RESIDE": "Used for dehazing, containing synthetic and real-world hazy images for training and testing.",
        "BSD300": "Used for denoising, providing a well-known dataset of natural images with noise.",
        "WED": "Used for denoising, offering a diverse set of images with varying noise levels.",
        "GoPro": "Used for deblurring, containing high-resolution video frames with motion blur."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions several datasets used for various image restoration tasks, such as deraining, dehazing, denoising, and deblurring. These datasets are specific and relevant to the topic of all-in-one image restoration.",
      "processing_time": 109.8855664730072,
      "citing_paper_id": "272680196",
      "cited_paper_id": 4840263
    },
    {
      "context_text": "I, including Rain14000 [50], Rain1800 [45], Rain12 [51] and Rain800 [52] for deraining, RESIDE [47] for dehazing, BSD300 [46] and WED [55] for denoising and GoPro [44] for deblurring.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Rain14000",
        "Rain1800",
        "Rain12",
        "Rain800",
        "RESIDE",
        "BSD300",
        "WED",
        "GoPro"
      ],
      "dataset_descriptions": {
        "Rain14000": "Used for deraining, providing a large set of images with rain streaks for training and evaluation.",
        "Rain1800": "Used for deraining, offering a moderate-sized dataset with diverse rain conditions.",
        "Rain12": "Used for deraining, containing a small but challenging set of images with heavy rain.",
        "Rain800": "Used for deraining, providing a balanced dataset with varied rain intensities.",
        "RESIDE": "Used for dehazing, containing synthetic and real-world hazy images for training and testing.",
        "BSD300": "Used for denoising, providing a well-known dataset of natural images with noise.",
        "WED": "Used for denoising, offering a diverse set of images with varying noise levels.",
        "GoPro": "Used for deblurring, containing high-resolution video frames with motion blur."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions several datasets used for various image restoration tasks, such as deraining, dehazing, denoising, and deblurring. These datasets are specific and relevant to the topic of all-in-one image restoration.",
      "processing_time": 109.8855664730072,
      "citing_paper_id": "272680196",
      "cited_paper_id": 9007541
    },
    {
      "context_text": "I, including Rain14000 [50], Rain1800 [45], Rain12 [51] and Rain800 [52] for deraining, RESIDE [47] for dehazing, BSD300 [46] and WED [55] for denoising and GoPro [44] for deblurring.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Rain14000",
        "Rain1800",
        "Rain12",
        "Rain800",
        "RESIDE",
        "BSD300",
        "WED",
        "GoPro"
      ],
      "dataset_descriptions": {
        "Rain14000": "Used for deraining, providing a large set of images with rain streaks for training and evaluation.",
        "Rain1800": "Used for deraining, offering a moderate-sized dataset with diverse rain conditions.",
        "Rain12": "Used for deraining, containing a small but challenging set of images with heavy rain.",
        "Rain800": "Used for deraining, providing a balanced dataset with varied rain intensities.",
        "RESIDE": "Used for dehazing, containing synthetic and real-world hazy images for training and testing.",
        "BSD300": "Used for denoising, providing a well-known dataset of natural images with noise.",
        "WED": "Used for denoising, offering a diverse set of images with varying noise levels.",
        "GoPro": "Used for deblurring, containing high-resolution video frames with motion blur."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions several datasets used for various image restoration tasks, such as deraining, dehazing, denoising, and deblurring. These datasets are specific and relevant to the topic of all-in-one image restoration.",
      "processing_time": 109.8855664730072,
      "citing_paper_id": "272680196",
      "cited_paper_id": 11922819
    },
    {
      "context_text": "BSD68 [46] Urban100 [?] Methods PSNR ↑ PSNR ↑ Transweather [37] 13.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "BSD68",
        "Urban100"
      ],
      "dataset_descriptions": {
        "BSD68": "Used to evaluate image restoration methods, focusing on PSNR improvements in natural images with human-segmented ground truth.",
        "Urban100": "Used to assess image restoration performance, particularly in urban scenes, emphasizing PSNR gains in complex, real-world environments."
      },
      "confidence_score": 0.9,
      "reasoning": "The context mentions BSD68 and Urban100, which are known datasets in image processing and computer vision. Transweather is also mentioned but appears to be a method or benchmark rather than a dataset.",
      "processing_time": 80.14362812042236,
      "citing_paper_id": "272680196",
      "cited_paper_id": 64193
    },
    {
      "context_text": "For evaluation, GoPro [44], Rain100L [45], BSD68 [46] and SOTS [47] are utilized as the test sets.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "GoPro",
        "Rain100L",
        "BSD68",
        "SOTS"
      ],
      "dataset_descriptions": {
        "GoPro": "Used to evaluate image restoration methods, focusing on deblurring performance on real-world video frames.",
        "Rain100L": "Utilized to assess rain removal techniques, specifically evaluating the ability to restore images degraded by rain.",
        "BSD68": "Employed to test denoising algorithms, emphasizing the restoration of images corrupted by noise.",
        "SOTS": "Applied to evaluate dehazing methods, specifically assessing the restoration of images affected by atmospheric conditions."
      },
      "confidence_score": 1.0,
      "reasoning": "The citation mentions specific datasets used for evaluation in the context of image restoration. These datasets are clearly named and relevant to the research topic.",
      "processing_time": 85.97803497314453,
      "citing_paper_id": "272680196",
      "cited_paper_id": 64193
    },
    {
      "context_text": "Denoising WED [55] 4744 4944 0 BSD300 [46] 200 0 BSD68 [46] 0 68 BSD68 Dehazing RESIDE [47] 13990 13990 0 SOTS-Outdoor [47] 0 492 SOTS",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "BSD300",
        "BSD68",
        "RESIDE",
        "SOTS-Outdoor"
      ],
      "dataset_descriptions": {
        "BSD300": "Used for denoising experiments, providing a benchmark for evaluating image restoration algorithms on natural images.",
        "BSD68": "Used for denoising experiments, specifically focusing on a smaller subset of images for evaluation.",
        "RESIDE": "Used for dehazing experiments, providing a comprehensive dataset for training and evaluating dehazing algorithms.",
        "SOTS-Outdoor": "Used for dehazing experiments, focusing on outdoor scenes to test the robustness of dehazing methods."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions several datasets used for denoising and dehazing experiments. These datasets are specific and have clear identifiers.",
      "processing_time": 86.37360167503357,
      "citing_paper_id": "272680196",
      "cited_paper_id": 64193
    },
    {
      "context_text": "Denoising WED [55] 4744 4944 0 BSD300 [46] 200 0 BSD68 [46] 0 68 BSD68 Dehazing RESIDE [47] 13990 13990 0 SOTS-Outdoor [47] 0 492 SOTS",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "BSD300",
        "BSD68",
        "RESIDE",
        "SOTS-Outdoor"
      ],
      "dataset_descriptions": {
        "BSD300": "Used for denoising experiments, providing a benchmark for evaluating image restoration algorithms on natural images.",
        "BSD68": "Used for denoising experiments, specifically focusing on a smaller subset of images for evaluation.",
        "RESIDE": "Used for dehazing experiments, providing a comprehensive dataset for training and evaluating dehazing algorithms.",
        "SOTS-Outdoor": "Used for dehazing experiments, focusing on outdoor scenes to test the robustness of dehazing methods."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions several datasets used for denoising and dehazing experiments. These datasets are specific and have clear identifiers.",
      "processing_time": 86.37360167503357,
      "citing_paper_id": "272680196",
      "cited_paper_id": 4840263
    },
    {
      "context_text": "(a) represents the GoPro [44] dataset for image deblurring, (b) denotes the Rain100H [45] dataset for image deraining, (c) is the BSD300 [46] for image denoising, and (d) shows the SOTS [47] dataset for image dehazing.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "GoPro",
        "Rain100H",
        "BSD300",
        "SOTS"
      ],
      "dataset_descriptions": {
        "GoPro": "Used for image deblurring, specifically to evaluate the effectiveness of the proposed method in restoring sharp images from blurred ones.",
        "Rain100H": "Used for image deraining, focusing on removing rain streaks from images to improve visual quality and clarity.",
        "BSD300": "Used for image denoising, specifically to assess the performance of the method in reducing noise while preserving image details.",
        "SOTS": "Used for image dehazing, aimed at enhancing visibility by removing haze effects from images."
      },
      "confidence_score": 1.0,
      "reasoning": "The citation clearly mentions four datasets used for different image restoration tasks, which align with the research topic of all-in-one image restoration.",
      "processing_time": 88.06048130989075,
      "citing_paper_id": "272680196",
      "cited_paper_id": 64193
    },
    {
      "context_text": "Test100 [52] Test1200 [53] Rain100H [45] Rain100L [45] Average Methods PSNR ↑ SSIM ↑ PSNR ↑ SSIM ↑ PSNR ↑ SSIM ↑ PSNR ↑ SSIM ↑ PSNR SSIM",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Rain100H",
        "Rain100L"
      ],
      "dataset_descriptions": {
        "Rain100H": "Used to evaluate image de-raining methods, focusing on high-resolution images with synthetic rain. The dataset assesses the performance of models in removing rain streaks while preserving image details.",
        "Rain100L": "Used to evaluate image de-raining methods, focusing on low-resolution images with synthetic rain. The dataset assesses the performance of models in removing rain streaks while preserving image details."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions specific datasets used for image de-raining experiments, which are relevant to the topic of all-in-one image restoration.",
      "processing_time": 82.52170538902283,
      "citing_paper_id": "272680196",
      "cited_paper_id": 3406592
    },
    {
      "context_text": "Test100 [52] Test1200 [53] Rain100H [45] Rain100L [45] Average Methods PSNR ↑ SSIM ↑ PSNR ↑ SSIM ↑ PSNR ↑ SSIM ↑ PSNR ↑ SSIM ↑ PSNR SSIM",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Rain100H",
        "Rain100L"
      ],
      "dataset_descriptions": {
        "Rain100H": "Used to evaluate image de-raining methods, focusing on high-resolution images with synthetic rain. The dataset assesses the performance of models in removing rain streaks while preserving image details.",
        "Rain100L": "Used to evaluate image de-raining methods, focusing on low-resolution images with synthetic rain. The dataset assesses the performance of models in removing rain streaks while preserving image details."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions specific datasets used for image de-raining experiments, which are relevant to the topic of all-in-one image restoration.",
      "processing_time": 82.52170538902283,
      "citing_paper_id": "272680196",
      "cited_paper_id": 11922819
    },
    {
      "context_text": "13712 0 Rain1800 [45] 1800 0 Rain12 [51] 12 0 Rain800 [52] 700 98 Test100 Rain1200 [53] 0 1200 Test1200 Rain100H [45] 0 100 Rain100H Rain100L [45] 0 100 Rain100L Deblurring GoPro [44] 2130 2130 1111 GoPro HIDE [54] 0 2025 HIDE",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Rain1800",
        "Rain12",
        "Rain800",
        "Test100",
        "Rain1200",
        "Rain100H",
        "Rain100L",
        "GoPro",
        "HIDE"
      ],
      "dataset_descriptions": {
        "Rain1800": "Used for training and evaluating de-raining models, providing a large set of synthetic rain images.",
        "Rain12": "Used for evaluating de-raining models, containing a smaller set of synthetic rain images.",
        "Rain800": "Used for training and evaluating de-raining models, offering a moderate-sized set of synthetic rain images.",
        "Test100": "Used for evaluating de-raining models, consisting of 100 test images with synthetic rain.",
        "Rain1200": "Used for training and evaluating de-raining models, providing a large set of synthetic rain images.",
        "Rain100H": "Used for evaluating de-raining models, containing 100 high-resolution synthetic rain images.",
        "Rain100L": "Used for evaluating de-raining models, containing 100 low-resolution synthetic rain images.",
        "GoPro": "Used for training and evaluating deblurring models, providing a large set of real-world blurry images.",
        "HIDE": "Used for training and evaluating deblurring models, offering a diverse set of real-world blurry images."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions several datasets used for training and testing image restoration models, particularly for de-raining and deblurring tasks.",
      "processing_time": 119.13407826423645,
      "citing_paper_id": "272680196",
      "cited_paper_id": 3406592
    },
    {
      "context_text": "13712 0 Rain1800 [45] 1800 0 Rain12 [51] 12 0 Rain800 [52] 700 98 Test100 Rain1200 [53] 0 1200 Test1200 Rain100H [45] 0 100 Rain100H Rain100L [45] 0 100 Rain100L Deblurring GoPro [44] 2130 2130 1111 GoPro HIDE [54] 0 2025 HIDE",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Rain1800",
        "Rain12",
        "Rain800",
        "Test100",
        "Rain1200",
        "Rain100H",
        "Rain100L",
        "GoPro",
        "HIDE"
      ],
      "dataset_descriptions": {
        "Rain1800": "Used for training and evaluating de-raining models, providing a large set of synthetic rain images.",
        "Rain12": "Used for evaluating de-raining models, containing a smaller set of synthetic rain images.",
        "Rain800": "Used for training and evaluating de-raining models, offering a moderate-sized set of synthetic rain images.",
        "Test100": "Used for evaluating de-raining models, consisting of 100 test images with synthetic rain.",
        "Rain1200": "Used for training and evaluating de-raining models, providing a large set of synthetic rain images.",
        "Rain100H": "Used for evaluating de-raining models, containing 100 high-resolution synthetic rain images.",
        "Rain100L": "Used for evaluating de-raining models, containing 100 low-resolution synthetic rain images.",
        "GoPro": "Used for training and evaluating deblurring models, providing a large set of real-world blurry images.",
        "HIDE": "Used for training and evaluating deblurring models, offering a diverse set of real-world blurry images."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions several datasets used for training and testing image restoration models, particularly for de-raining and deblurring tasks.",
      "processing_time": 119.13407826423645,
      "citing_paper_id": "272680196",
      "cited_paper_id": 9007541
    },
    {
      "context_text": "13712 0 Rain1800 [45] 1800 0 Rain12 [51] 12 0 Rain800 [52] 700 98 Test100 Rain1200 [53] 0 1200 Test1200 Rain100H [45] 0 100 Rain100H Rain100L [45] 0 100 Rain100L Deblurring GoPro [44] 2130 2130 1111 GoPro HIDE [54] 0 2025 HIDE",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Rain1800",
        "Rain12",
        "Rain800",
        "Test100",
        "Rain1200",
        "Rain100H",
        "Rain100L",
        "GoPro",
        "HIDE"
      ],
      "dataset_descriptions": {
        "Rain1800": "Used for training and evaluating de-raining models, providing a large set of synthetic rain images.",
        "Rain12": "Used for evaluating de-raining models, containing a smaller set of synthetic rain images.",
        "Rain800": "Used for training and evaluating de-raining models, offering a moderate-sized set of synthetic rain images.",
        "Test100": "Used for evaluating de-raining models, consisting of 100 test images with synthetic rain.",
        "Rain1200": "Used for training and evaluating de-raining models, providing a large set of synthetic rain images.",
        "Rain100H": "Used for evaluating de-raining models, containing 100 high-resolution synthetic rain images.",
        "Rain100L": "Used for evaluating de-raining models, containing 100 low-resolution synthetic rain images.",
        "GoPro": "Used for training and evaluating deblurring models, providing a large set of real-world blurry images.",
        "HIDE": "Used for training and evaluating deblurring models, offering a diverse set of real-world blurry images."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions several datasets used for training and testing image restoration models, particularly for de-raining and deblurring tasks.",
      "processing_time": 119.13407826423645,
      "citing_paper_id": "272680196",
      "cited_paper_id": 11922819
    },
    {
      "context_text": "For image deraining, we train our model on 13,712 clean-rain image pairs gathered from multiple datasets [45], [50]–[52], and perform evaluation on Rain100L [45], Rain100H [45], Test100 [52] and Test1200 [53].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Rain100L",
        "Rain100H",
        "Test100",
        "Test1200"
      ],
      "dataset_descriptions": {
        "Rain100L": "Used for evaluating the model's performance on light rain conditions, focusing on image quality and restoration accuracy.",
        "Rain100H": "Used for evaluating the model's performance on heavy rain conditions, focusing on image quality and restoration accuracy.",
        "Test100": "Used for evaluating the model's performance on a diverse set of test images, focusing on generalization and robustness.",
        "Test1200": "Used for evaluating the model's performance on a larger set of test images, focusing on scalability and consistency."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions specific datasets used for training and evaluation in the context of image deraining. These datasets are clearly identified and used for evaluating the performance of the model.",
      "processing_time": 91.6531035900116,
      "citing_paper_id": "272680196",
      "cited_paper_id": 3406592
    },
    {
      "context_text": "For image deraining, we train our model on 13,712 clean-rain image pairs gathered from multiple datasets [45], [50]–[52], and perform evaluation on Rain100L [45], Rain100H [45], Test100 [52] and Test1200 [53].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Rain100L",
        "Rain100H",
        "Test100",
        "Test1200"
      ],
      "dataset_descriptions": {
        "Rain100L": "Used for evaluating the model's performance on light rain conditions, focusing on image quality and restoration accuracy.",
        "Rain100H": "Used for evaluating the model's performance on heavy rain conditions, focusing on image quality and restoration accuracy.",
        "Test100": "Used for evaluating the model's performance on a diverse set of test images, focusing on generalization and robustness.",
        "Test1200": "Used for evaluating the model's performance on a larger set of test images, focusing on scalability and consistency."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions specific datasets used for training and evaluation in the context of image deraining. These datasets are clearly identified and used for evaluating the performance of the model.",
      "processing_time": 91.6531035900116,
      "citing_paper_id": "272680196",
      "cited_paper_id": 11922819
    },
    {
      "context_text": "However, on all four datasets, our results are competitive with the best methods, e.g., we obtain a performance gain of 1.15db over DRSformer [8] and 1.58 db over Restormer [34] (on Test1200).",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.2,
      "reasoning": "The citation mentions 'four datasets' but does not specify their names. The context focuses on performance comparisons with other methods rather than describing the datasets.",
      "processing_time": 65.80405950546265,
      "citing_paper_id": "272680196",
      "cited_paper_id": 3406592
    },
    {
      "context_text": "However, on all four datasets, our results are competitive with the best methods, e.g., we obtain a performance gain of 1.15db over DRSformer [8] and 1.58 db over Restormer [34] (on Test1200).",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.2,
      "reasoning": "The citation mentions 'four datasets' but does not specify their names. The context focuses on performance comparisons with other methods rather than describing the datasets.",
      "processing_time": 65.80405950546265,
      "citing_paper_id": "272680196",
      "cited_paper_id": 244346144
    },
    {
      "context_text": "For task-special, we train models with Adam [56] optimizer(β1 = 0.9, β2 = 0.999) and PSNR loss for 5× 105 iterations with the initial learning rate 5× 10−4 gradually reduced to 1× 10−7 with the cosine annealing [57].",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only the optimizer and loss function used during training.",
      "processing_time": 63.61726236343384,
      "citing_paper_id": "272680196",
      "cited_paper_id": 6628106
    },
    {
      "context_text": "models with Adam [56] optimizer(β1 = 0.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only an optimization method.",
      "processing_time": 62.55300998687744,
      "citing_paper_id": "272680196",
      "cited_paper_id": 6628106
    },
    {
      "context_text": "Early restoration approaches often relied on manually crafted priors to mitigate the ill-posed nature of the problems [21]–[25].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only general approaches and methods. The cited papers do not provide additional context to identify datasets.",
      "processing_time": 65.49383807182312,
      "citing_paper_id": "272680196",
      "cited_paper_id": 13133466
    },
    {
      "context_text": "Early restoration approaches often relied on manually crafted priors to mitigate the ill-posed nature of the problems [21]–[25].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only general approaches and methods. The cited papers do not provide additional context to identify datasets.",
      "processing_time": 65.49383807182312,
      "citing_paper_id": "272680196",
      "cited_paper_id": 49333383
    },
    {
      "context_text": "In recent years, there has been a significant paradigm shift from traditional restoration methods to learning-based approaches, driven by their impressive performance in a wide range of image restoration tasks, such as denoising [1]–[3], dehazing [10], [11], [13], debluring [4]–[6], deraining [7]–[9], etc.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various image restoration tasks. The cited papers' titles do not provide additional dataset names.",
      "processing_time": 65.48299813270569,
      "citing_paper_id": "272680196",
      "cited_paper_id": 30151664
    },
    {
      "context_text": "In recent years, there has been a significant paradigm shift from traditional restoration methods to learning-based approaches, driven by their impressive performance in a wide range of image restoration tasks, such as denoising [1]–[3], dehazing [10], [11], [13], debluring [4]–[6], deraining [7]–[9], etc.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various image restoration tasks. The cited papers' titles do not provide additional dataset names.",
      "processing_time": 65.48299813270569,
      "citing_paper_id": "272680196",
      "cited_paper_id": 49672261
    },
    {
      "context_text": "In recent years, there has been a significant paradigm shift from traditional restoration methods to learning-based approaches, driven by their impressive performance in a wide range of image restoration tasks, such as denoising [1]–[3], dehazing [10], [11], [13], debluring [4]–[6], deraining [7]–[9], etc.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various image restoration tasks. The cited papers' titles do not provide additional dataset names.",
      "processing_time": 65.48299813270569,
      "citing_paper_id": "272680196",
      "cited_paper_id": 253761139
    },
    {
      "context_text": "In recent years, there has been a significant paradigm shift from traditional restoration methods to learning-based approaches, driven by their impressive performance in a wide range of image restoration tasks, such as denoising [1]–[3], dehazing [10], [11], [13], debluring [4]–[6], deraining [7]–[9], etc.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various image restoration tasks. The cited papers' titles do not provide additional dataset names.",
      "processing_time": 65.48299813270569,
      "citing_paper_id": "272680196",
      "cited_paper_id": 260006244
    },
    {
      "context_text": "mance, such as denoising [1]–[3], debluring [4]–[6], deraining [7]–[9], dehazing [10]–[13], etc.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context mentions various image restoration tasks but does not specify any datasets. The cited papers' titles do not provide additional dataset information.",
      "processing_time": 65.0560531616211,
      "citing_paper_id": "272680196",
      "cited_paper_id": 30151664
    },
    {
      "context_text": "mance, such as denoising [1]–[3], debluring [4]–[6], deraining [7]–[9], dehazing [10]–[13], etc.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context mentions various image restoration tasks but does not specify any datasets. The cited papers' titles do not provide additional dataset information.",
      "processing_time": 65.0560531616211,
      "citing_paper_id": "272680196",
      "cited_paper_id": 49672261
    },
    {
      "context_text": "mance, such as denoising [1]–[3], debluring [4]–[6], deraining [7]–[9], dehazing [10]–[13], etc.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context mentions various image restoration tasks but does not specify any datasets. The cited papers' titles do not provide additional dataset information.",
      "processing_time": 65.0560531616211,
      "citing_paper_id": "272680196",
      "cited_paper_id": 253761139
    },
    {
      "context_text": "mance, such as denoising [1]–[3], debluring [4]–[6], deraining [7]–[9], dehazing [10]–[13], etc.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context mentions various image restoration tasks but does not specify any datasets. The cited papers' titles do not provide additional dataset information.",
      "processing_time": 65.0560531616211,
      "citing_paper_id": "272680196",
      "cited_paper_id": 260006244
    },
    {
      "context_text": "We utilize the following block configurations in our network for each level: [1, 1, 1, 28] blocks for the encoder, [1, 1, 1, 1] blocks for the decoder.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only network configurations. No dataset names are present in the citation span.",
      "processing_time": 64.15931630134583,
      "citing_paper_id": "272680196",
      "cited_paper_id": 49672261
    },
    {
      "context_text": "We utilize the following block configurations in our network for each level: [1, 1, 1, 28] blocks for the encoder, [1, 1, 1, 1] blocks for the decoder.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only network configurations. No dataset names are present in the citation span.",
      "processing_time": 64.15931630134583,
      "citing_paper_id": "272680196",
      "cited_paper_id": 231802205
    },
    {
      "context_text": "1) Multiple Degradations All-in-One Results: We compare our CAPTNet with six general image restoration methods [20], [27], [28], [33], [34], [59] and five all-in-one fashion methods [18], [19], [37], [39], [60] on four challenging image restoration tasks including deblurring, deraining, denoising and dehazing.",
      "catation_intent": "reusable resource",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods and models. No dataset names are present in the text.",
      "processing_time": 64.78817248344421,
      "citing_paper_id": "272680196",
      "cited_paper_id": 195787503
    },
    {
      "context_text": "1) Multiple Degradations All-in-One Results: We compare our CAPTNet with six general image restoration methods [20], [27], [28], [33], [34], [59] and five all-in-one fashion methods [18], [19], [37], [39], [60] on four challenging image restoration tasks including deblurring, deraining, denoising and dehazing.",
      "catation_intent": "reusable resource",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods and models. No dataset names are present in the text.",
      "processing_time": 64.78817248344421,
      "citing_paper_id": "272680196",
      "cited_paper_id": 212725053
    },
    {
      "context_text": "1) Multiple Degradations All-in-One Results: We compare our CAPTNet with six general image restoration methods [20], [27], [28], [33], [34], [59] and five all-in-one fashion methods [18], [19], [37], [39], [60] on four challenging image restoration tasks including deblurring, deraining, denoising and dehazing.",
      "catation_intent": "reusable resource",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods and models. No dataset names are present in the text.",
      "processing_time": 64.78817248344421,
      "citing_paper_id": "272680196",
      "cited_paper_id": 231802205
    },
    {
      "context_text": "1) Multiple Degradations All-in-One Results: We compare our CAPTNet with six general image restoration methods [20], [27], [28], [33], [34], [59] and five all-in-one fashion methods [18], [19], [37], [39], [60] on four challenging image restoration tasks including deblurring, deraining, denoising and dehazing.",
      "catation_intent": "reusable resource",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods and models. No dataset names are present in the text.",
      "processing_time": 64.78817248344421,
      "citing_paper_id": "272680196",
      "cited_paper_id": 237266491
    },
    {
      "context_text": "1) Multiple Degradations All-in-One Results: We compare our CAPTNet with six general image restoration methods [20], [27], [28], [33], [34], [59] and five all-in-one fashion methods [18], [19], [37], [39], [60] on four challenging image restoration tasks including deblurring, deraining, denoising and dehazing.",
      "catation_intent": "reusable resource",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods and models. No dataset names are present in the text.",
      "processing_time": 64.78817248344421,
      "citing_paper_id": "272680196",
      "cited_paper_id": 244346144
    },
    {
      "context_text": "[27] introduced a multi-scale architecture that effectively integrates contextual information while concurrently preserving details.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method or architecture. The context is focused on the introduction of a multi-scale architecture for image restoration and enhancement.",
      "processing_time": 65.9757993221283,
      "citing_paper_id": "272680196",
      "cited_paper_id": 212725053
    },
    {
      "context_text": "[36] proposed a multi-encoder single-encoder to handle multiple bad weather degradations.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for handling multiple bad weather degradations.",
      "processing_time": 63.97250008583069,
      "citing_paper_id": "272680196",
      "cited_paper_id": 219978541
    },
    {
      "context_text": "[31], [32] introduced pre-training model based on transformers IPT and EDT respectively for image",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only models (IPT and EDT) which are excluded according to the rules.",
      "processing_time": 64.57583975791931,
      "citing_paper_id": "272680196",
      "cited_paper_id": 220686623
    },
    {
      "context_text": "[31], [32] introduced pre-training model based on transformers IPT and EDT respectively for image",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only models (IPT and EDT) which are excluded according to the rules.",
      "processing_time": 64.57583975791931,
      "citing_paper_id": "272680196",
      "cited_paper_id": 227239228
    },
    {
      "context_text": "[31] leveraged a multi-head, multi-tail architecture based on transformer to handle a variety of degradation.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method or architecture. There are no clear identifiers for datasets in the provided context.",
      "processing_time": 64.62336468696594,
      "citing_paper_id": "272680196",
      "cited_paper_id": 227239228
    },
    {
      "context_text": "To assist the revovery process, we add skip-connections to bridge across continuous intermediate features [28], [34], [35].",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only architectural components (skip-connections).",
      "processing_time": 62.9868266582489,
      "citing_paper_id": "272680196",
      "cited_paper_id": 231802205
    },
    {
      "context_text": "To assist the revovery process, we add skip-connections to bridge across continuous intermediate features [28], [34], [35].",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only architectural components (skip-connections).",
      "processing_time": 62.9868266582489,
      "citing_paper_id": "272680196",
      "cited_paper_id": 235358213
    },
    {
      "context_text": "To assist the revovery process, we add skip-connections to bridge across continuous intermediate features [28], [34], [35].",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only architectural components (skip-connections).",
      "processing_time": 62.9868266582489,
      "citing_paper_id": "272680196",
      "cited_paper_id": 244346144
    },
    {
      "context_text": "[28], [30] pay attention to balance the competinng goals of spatial details and high-level contextualized information.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only a general discussion about balancing competing goals in image restoration.",
      "processing_time": 63.74526405334473,
      "citing_paper_id": "272680196",
      "cited_paper_id": 231802205
    },
    {
      "context_text": "Previous studies [34], [35] usually add depth-wise convolutions to improve the capa-",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only a methodological approach involving depth-wise convolutions. No verifiable resources are identified.",
      "processing_time": 64.90985202789307,
      "citing_paper_id": "272680196",
      "cited_paper_id": 235358213
    },
    {
      "context_text": "Previous studies [34], [35] usually add depth-wise convolutions to improve the capa-",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only a methodological approach involving depth-wise convolutions. No verifiable resources are identified.",
      "processing_time": 64.90985202789307,
      "citing_paper_id": "272680196",
      "cited_paper_id": 244346144
    },
    {
      "context_text": "While our approach is no match for the best approach FFTformer [4] on the GoPro dataset, it\n10\noutperforms Restormer [34], NAFNet [20], HINet [59] and Uformer [35].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "GoPro dataset"
      ],
      "dataset_descriptions": {
        "GoPro dataset": "Used to evaluate image restoration models, specifically comparing performance against state-of-the-art methods like FFTformer, Restormer, NAFNet, HINet, and Uformer."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions the GoPro dataset, which is a specific dataset used for image restoration tasks. No other datasets are mentioned.",
      "processing_time": 71.84228277206421,
      "citing_paper_id": "272680196",
      "cited_paper_id": 235358213
    },
    {
      "context_text": "While our approach is no match for the best approach FFTformer [4] on the GoPro dataset, it\n10\noutperforms Restormer [34], NAFNet [20], HINet [59] and Uformer [35].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "GoPro dataset"
      ],
      "dataset_descriptions": {
        "GoPro dataset": "Used to evaluate image restoration models, specifically comparing performance against state-of-the-art methods like FFTformer, Restormer, NAFNet, HINet, and Uformer."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions the GoPro dataset, which is a specific dataset used for image restoration tasks. No other datasets are mentioned.",
      "processing_time": 71.84228277206421,
      "citing_paper_id": "272680196",
      "cited_paper_id": 244346144
    },
    {
      "context_text": "While our approach is no match for the best approach FFTformer [4] on the GoPro dataset, it\n10\noutperforms Restormer [34], NAFNet [20], HINet [59] and Uformer [35].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "GoPro dataset"
      ],
      "dataset_descriptions": {
        "GoPro dataset": "Used to evaluate image restoration models, specifically comparing performance against state-of-the-art methods like FFTformer, Restormer, NAFNet, HINet, and Uformer."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions the GoPro dataset, which is a specific dataset used for image restoration tasks. No other datasets are mentioned.",
      "processing_time": 71.84228277206421,
      "citing_paper_id": "272680196",
      "cited_paper_id": 253761139
    },
    {
      "context_text": "[35] proposed a U-Net based transformer architecture which uses local-enhanced window Transformer block.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (U-Net based transformer architecture).",
      "processing_time": 63.48904848098755,
      "citing_paper_id": "272680196",
      "cited_paper_id": 235358213
    },
    {
      "context_text": "outperforms Restormer [34], NAFNet [20], HINet [59] and Uformer [35].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only models and methods. No verifiable resources are identified.",
      "processing_time": 63.49471354484558,
      "citing_paper_id": "272680196",
      "cited_paper_id": 235358213
    },
    {
      "context_text": "outperforms Restormer [34], NAFNet [20], HINet [59] and Uformer [35].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only models and methods. No verifiable resources are identified.",
      "processing_time": 63.49471354484558,
      "citing_paper_id": "272680196",
      "cited_paper_id": 244346144
    },
    {
      "context_text": "1) CNNs Based Method: Image restoration methods based on CNNs have made significant strides in recent years, with a plethora of emerging architectures [20], [26]–[30].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only references to papers discussing CNN-based image restoration methods.",
      "processing_time": 63.34274125099182,
      "citing_paper_id": "272680196",
      "cited_paper_id": 237213661
    },
    {
      "context_text": "[33] proposed SwinIR for image restoration based on Swin Transformer model.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions SwinIR, which is a method for image restoration using the Swin Transformer model. No specific dataset is mentioned in the citation context.",
      "processing_time": 65.04392862319946,
      "citing_paper_id": "272680196",
      "cited_paper_id": 237266491
    },
    {
      "context_text": "58 db over Restormer [34] (on Test1200).",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Test1200"
      ],
      "dataset_descriptions": {
        "Test1200": "Used to evaluate the performance of Restormer on high-resolution image restoration, focusing on quantitative metrics and visual quality."
      },
      "confidence_score": 0.7,
      "reasoning": "The citation mentions 'Test1200' which appears to be a dataset used for testing image restoration methods. However, there is no additional context provided about the dataset's origin or specific characteristics.",
      "processing_time": 72.67307686805725,
      "citing_paper_id": "272680196",
      "cited_paper_id": 244346144
    },
    {
      "context_text": "Inspired by [34], we apply SA across channels rather than spatial dimensions to reduce the computation complexity.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (SA) applied in a certain way. The context is about computational efficiency, not data.",
      "processing_time": 63.935269594192505,
      "citing_paper_id": "272680196",
      "cited_paper_id": 244346144
    },
    {
      "context_text": "[34] devised a transposed attention mechanism and a feedforward neural network, culminating in the creation of an efficient transformer model.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a model and its components. The context is about the method and not about data.",
      "processing_time": 63.20681071281433,
      "citing_paper_id": "272680196",
      "cited_paper_id": 244346144
    },
    {
      "context_text": "Compare to the previous best methods, we obtain the best on HIDE datasets, our CAPTNet yields 0.26 dB performance gain over the previous best method FFTformer [4], and 0.37 dB over the second best approach Restormer [34].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "HIDE datasets"
      ],
      "dataset_descriptions": {
        "HIDE datasets": "Used to evaluate the performance of CAPTNet in image restoration, comparing it against FFTformer and Restormer, focusing on high-resolution image deblurring."
      },
      "confidence_score": 0.9,
      "reasoning": "The context mentions 'HIDE datasets' as a specific dataset used for evaluating the performance of CAPTNet compared to other methods. No other specific datasets are mentioned.",
      "processing_time": 71.38525629043579,
      "citing_paper_id": "272680196",
      "cited_paper_id": 244346144
    },
    {
      "context_text": "Compare to the previous best methods, we obtain the best on HIDE datasets, our CAPTNet yields 0.26 dB performance gain over the previous best method FFTformer [4], and 0.37 dB over the second best approach Restormer [34].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "HIDE datasets"
      ],
      "dataset_descriptions": {
        "HIDE datasets": "Used to evaluate the performance of CAPTNet in image restoration, comparing it against FFTformer and Restormer, focusing on high-resolution image deblurring."
      },
      "confidence_score": 0.9,
      "reasoning": "The context mentions 'HIDE datasets' as a specific dataset used for evaluating the performance of CAPTNet compared to other methods. No other specific datasets are mentioned.",
      "processing_time": 71.38525629043579,
      "citing_paper_id": "272680196",
      "cited_paper_id": 253761139
    },
    {
      "context_text": "26 dB performance gain over the previous best method FFTformer [4], and 0.",
      "catation_intent": "findings",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a comparison with a method called FFTformer.",
      "processing_time": 61.56975197792053,
      "citing_paper_id": "272680196",
      "cited_paper_id": 253761139
    },
    {
      "context_text": "While our approach is no match for the best approach FFTformer [4] on the GoPro dataset, it",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "GoPro dataset"
      ],
      "dataset_descriptions": {
        "GoPro dataset": "Used to evaluate the performance of the proposed image restoration approach, specifically comparing it to the FFTformer method in the context of high-quality image deblurring."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions the GoPro dataset, which is a specific, verifiable dataset used for image deblurring. The dataset is used to evaluate the performance of the proposed approach compared to FFTformer.",
      "processing_time": 72.39949250221252,
      "citing_paper_id": "272680196",
      "cited_paper_id": 253761139
    },
    {
      "context_text": "We also compare our method with previous SOTAs on perceptual metrics, namely FID [35] and LPIPS [141], which are usually more aligned with human visual preference than reference-based metrics such as PSNR and SSIM [119].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only metrics and methods. No datasets are identified that meet the criteria.",
      "processing_time": 61.86716341972351,
      "citing_paper_id": "279464818",
      "cited_paper_id": 326772
    },
    {
      "context_text": "We implement s θ with U-Net [98] and introduce our conditioning mechanisms in this section.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions U-Net but does not refer to it as a dataset. U-Net is a method/model, not a dataset.",
      "processing_time": 62.34127640724182,
      "citing_paper_id": "279464818",
      "cited_paper_id": 3719281
    },
    {
      "context_text": "(6) via denoising score matching [115]: Degradation Diffusion.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method related to denoising score matching.",
      "processing_time": 61.15150237083435,
      "citing_paper_id": "279464818",
      "cited_paper_id": 5560643
    },
    {
      "context_text": "Details of these datasets are given below: • Motion Deblur: collected from GoPro [81]",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Motion Deblur"
      ],
      "dataset_descriptions": {
        "Motion Deblur": "Used to collect motion deblurring data from GoPro cameras, specifically for training and evaluating deblurring algorithms in dynamic scenes."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions 'Motion Deblur' as a dataset collected from GoPro, which is a specific, identifiable dataset.",
      "processing_time": 67.81116342544556,
      "citing_paper_id": "279464818",
      "cited_paper_id": 8671030
    },
    {
      "context_text": "During the evaluation, we used Dense-Haze [6] for de-hazing, Rain1400 [30] for deraining, GoPro [81] for motion deblurring, RainDrop [87] for raindrop removal, DPDD [2] for defocus deblurring, Snow100K [67] for desnowing, SIDD [1] for real denoising, and LIVE1 [104] for JPEG artifact removal (with…",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Dense-Haze",
        "Rain1400",
        "GoPro",
        "RainDrop",
        "DPDD",
        "Snow100K",
        "SIDD",
        "LIVE1"
      ],
      "dataset_descriptions": {
        "Dense-Haze": "Used for de-hazing, evaluating the effectiveness of the proposed method in removing haze from images.",
        "Rain1400": "Used for deraining, assessing the performance in removing rain streaks from images.",
        "GoPro": "Used for motion deblurring, testing the ability to restore sharpness in blurred images.",
        "RainDrop": "Used for raindrop removal, evaluating the method's capability to remove raindrops from images.",
        "DPDD": "Used for defocus deblurring, assessing the performance in restoring focus in defocused images.",
        "Snow100K": "Used for desnowing, evaluating the method's effectiveness in removing snow from images.",
        "SIDD": "Used for real denoising, testing the performance in reducing noise in real-world images.",
        "LIVE1": "Used for JPEG artifact removal, assessing the method's ability to reduce compression artifacts."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions several datasets used for evaluating different image restoration tasks, which are specific and relevant to the topic of all-in-one image restoration.",
      "processing_time": 103.4104733467102,
      "citing_paper_id": "279464818",
      "cited_paper_id": 8671030
    },
    {
      "context_text": "During the evaluation, we used Dense-Haze [6] for de-hazing, Rain1400 [30] for deraining, GoPro [81] for motion deblurring, RainDrop [87] for raindrop removal, DPDD [2] for defocus deblurring, Snow100K [67] for desnowing, SIDD [1] for real denoising, and LIVE1 [104] for JPEG artifact removal (with…",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Dense-Haze",
        "Rain1400",
        "GoPro",
        "RainDrop",
        "DPDD",
        "Snow100K",
        "SIDD",
        "LIVE1"
      ],
      "dataset_descriptions": {
        "Dense-Haze": "Used for de-hazing, evaluating the effectiveness of the proposed method in removing haze from images.",
        "Rain1400": "Used for deraining, assessing the performance in removing rain streaks from images.",
        "GoPro": "Used for motion deblurring, testing the ability to restore sharpness in blurred images.",
        "RainDrop": "Used for raindrop removal, evaluating the method's capability to remove raindrops from images.",
        "DPDD": "Used for defocus deblurring, assessing the performance in restoring focus in defocused images.",
        "Snow100K": "Used for desnowing, evaluating the method's effectiveness in removing snow from images.",
        "SIDD": "Used for real denoising, testing the performance in reducing noise in real-world images.",
        "LIVE1": "Used for JPEG artifact removal, assessing the method's ability to reduce compression artifacts."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions several datasets used for evaluating different image restoration tasks, which are specific and relevant to the topic of all-in-one image restoration.",
      "processing_time": 103.4104733467102,
      "citing_paper_id": "279464818",
      "cited_paper_id": 17115407
    },
    {
      "context_text": "…dehazing datasets like RESIDE [49], and Dense-Haze [6]; desnowing datasets such as Snow100K [67] and RealSnow [149]; raindrop removal datasets like RainDrop [87] and RainDS [90]; real denoising with SIDD [1]; motion deblurring with GoPro [81] and RealBlur [96]; defocus deblurring with DPDD [2].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "RESIDE",
        "Dense-Haze",
        "Snow100K",
        "RealSnow",
        "RainDrop",
        "RainDS",
        "SIDD",
        "GoPro",
        "RealBlur",
        "DPDD"
      ],
      "dataset_descriptions": {
        "RESIDE": "Used for dehazing, providing synthetic and real-world hazy images for training and evaluation.",
        "Dense-Haze": "Used for dehazing, focusing on dense haze conditions to test model robustness.",
        "Snow100K": "Used for desnowing, containing synthetic snow images to train and evaluate snow removal algorithms.",
        "RealSnow": "Used for desnowing, providing real-world snow images to validate desnowing performance.",
        "RainDrop": "Used for raindrop removal, offering synthetic raindrop images to train and test raindrop removal models.",
        "RainDS": "Used for raindrop removal, providing a diverse set of raindrop images for comprehensive evaluation.",
        "SIDD": "Used for real denoising, containing real noisy images and their clean counterparts for denoising tasks.",
        "GoPro": "Used for motion deblurring, providing sharp and blurred image pairs for training and testing.",
        "RealBlur": "Used for motion deblurring, offering real-world blurred images to assess deblurring performance.",
        "DPDD": "Used for defocus deblurring, leveraging dual-pixel data to improve defocus deblurring accuracy."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions several datasets used for various image restoration tasks, including dehazing, desnowing, raindrop removal, denoising, and deblurring. These datasets are clearly identified and used for training and evaluation.",
      "processing_time": 117.12080407142639,
      "citing_paper_id": "279464818",
      "cited_paper_id": 8671030
    },
    {
      "context_text": "…dehazing datasets like RESIDE [49], and Dense-Haze [6]; desnowing datasets such as Snow100K [67] and RealSnow [149]; raindrop removal datasets like RainDrop [87] and RainDS [90]; real denoising with SIDD [1]; motion deblurring with GoPro [81] and RealBlur [96]; defocus deblurring with DPDD [2].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "RESIDE",
        "Dense-Haze",
        "Snow100K",
        "RealSnow",
        "RainDrop",
        "RainDS",
        "SIDD",
        "GoPro",
        "RealBlur",
        "DPDD"
      ],
      "dataset_descriptions": {
        "RESIDE": "Used for dehazing, providing synthetic and real-world hazy images for training and evaluation.",
        "Dense-Haze": "Used for dehazing, focusing on dense haze conditions to test model robustness.",
        "Snow100K": "Used for desnowing, containing synthetic snow images to train and evaluate snow removal algorithms.",
        "RealSnow": "Used for desnowing, providing real-world snow images to validate desnowing performance.",
        "RainDrop": "Used for raindrop removal, offering synthetic raindrop images to train and test raindrop removal models.",
        "RainDS": "Used for raindrop removal, providing a diverse set of raindrop images for comprehensive evaluation.",
        "SIDD": "Used for real denoising, containing real noisy images and their clean counterparts for denoising tasks.",
        "GoPro": "Used for motion deblurring, providing sharp and blurred image pairs for training and testing.",
        "RealBlur": "Used for motion deblurring, offering real-world blurred images to assess deblurring performance.",
        "DPDD": "Used for defocus deblurring, leveraging dual-pixel data to improve defocus deblurring accuracy."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions several datasets used for various image restoration tasks, including dehazing, desnowing, raindrop removal, denoising, and deblurring. These datasets are clearly identified and used for training and evaluation.",
      "processing_time": 117.12080407142639,
      "citing_paper_id": "279464818",
      "cited_paper_id": 218470249
    },
    {
      "context_text": "999 , starting with an initial learning rate of 1e-4 and decaying to 1e-6 via cosine annealing [69].",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for adjusting the learning rate during training.",
      "processing_time": 60.736812353134155,
      "citing_paper_id": "279464818",
      "cited_paper_id": 14337532
    },
    {
      "context_text": "This includes datasets for deraining such as Rain1400 [30], Outdoor-Rain [54], and LHP [33]; dehazing datasets like RESIDE [49], and Dense-Haze [6]; desnowing datasets such as Snow100K [67] and RealSnow [149]; raindrop removal datasets like RainDrop [87] and RainDS [90]; real denoising with SIDD…",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Rain1400",
        "Outdoor-Rain",
        "LHP",
        "RESIDE",
        "Dense-Haze",
        "Snow100K",
        "RealSnow",
        "RainDrop",
        "RainDS",
        "SIDD"
      ],
      "dataset_descriptions": {
        "Rain1400": "Used for deraining single images, evaluating the effectiveness of the deep detail network in removing rain streaks.",
        "Outdoor-Rain": "Used for deraining outdoor scenes, assessing the performance of deraining algorithms in diverse environmental conditions.",
        "LHP": "Used for deraining, focusing on the removal of rain streaks from images captured in various lighting conditions.",
        "RESIDE": "Used for dehazing, benchmarking the performance of dehazing algorithms on synthetic and real-world hazy images.",
        "Dense-Haze": "Used for dehazing, evaluating the ability of algorithms to handle dense haze conditions in images.",
        "Snow100K": "Used for desnowing, testing the effectiveness of desnowing methods on synthetic and real snow-covered images.",
        "RealSnow": "Used for desnowing, assessing the performance of desnowing algorithms on real-world snow-covered images.",
        "RainDrop": "Used for raindrop removal, evaluating the ability of algorithms to remove raindrops from images.",
        "RainDS": "Used for raindrop removal, testing the effectiveness of raindrop removal methods on both synthetic and real images.",
        "SIDD": "Used for real denoising, benchmarking the performance of denoising algorithms on real-world noisy images."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions several datasets used for various image restoration tasks, including deraining, dehazing, desnowing, and raindrop removal. These datasets are clearly identified and used for training and evaluation.",
      "processing_time": 121.81008434295654,
      "citing_paper_id": "279464818",
      "cited_paper_id": 17115407
    },
    {
      "context_text": "This includes datasets for deraining such as Rain1400 [30], Outdoor-Rain [54], and LHP [33]; dehazing datasets like RESIDE [49], and Dense-Haze [6]; desnowing datasets such as Snow100K [67] and RealSnow [149]; raindrop removal datasets like RainDrop [87] and RainDS [90]; real denoising with SIDD…",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Rain1400",
        "Outdoor-Rain",
        "LHP",
        "RESIDE",
        "Dense-Haze",
        "Snow100K",
        "RealSnow",
        "RainDrop",
        "RainDS",
        "SIDD"
      ],
      "dataset_descriptions": {
        "Rain1400": "Used for deraining single images, evaluating the effectiveness of the deep detail network in removing rain streaks.",
        "Outdoor-Rain": "Used for deraining outdoor scenes, assessing the performance of deraining algorithms in diverse environmental conditions.",
        "LHP": "Used for deraining, focusing on the removal of rain streaks from images captured in various lighting conditions.",
        "RESIDE": "Used for dehazing, benchmarking the performance of dehazing algorithms on synthetic and real-world hazy images.",
        "Dense-Haze": "Used for dehazing, evaluating the ability of algorithms to handle dense haze conditions in images.",
        "Snow100K": "Used for desnowing, testing the effectiveness of desnowing methods on synthetic and real snow-covered images.",
        "RealSnow": "Used for desnowing, assessing the performance of desnowing algorithms on real-world snow-covered images.",
        "RainDrop": "Used for raindrop removal, evaluating the ability of algorithms to remove raindrops from images.",
        "RainDS": "Used for raindrop removal, testing the effectiveness of raindrop removal methods on both synthetic and real images.",
        "SIDD": "Used for real denoising, benchmarking the performance of denoising algorithms on real-world noisy images."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions several datasets used for various image restoration tasks, including deraining, dehazing, desnowing, and raindrop removal. These datasets are clearly identified and used for training and evaluation.",
      "processing_time": 121.81008434295654,
      "citing_paper_id": "279464818",
      "cited_paper_id": 39760169
    },
    {
      "context_text": "We also introduced underwater datasets EUVP [39] and TURBID [24] to examine the Table 1.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "EUVP",
        "TURBID"
      ],
      "dataset_descriptions": {
        "EUVP": "Used to evaluate underwater image enhancement techniques, focusing on visual perception improvements in Table 1.",
        "TURBID": "Used to evaluate underwater image enhancement techniques, focusing on visual perception improvements in Table 1."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions two specific datasets, EUVP and TURBID, which are used to examine a table. The datasets are relevant to the topic of image restoration, particularly in the underwater domain.",
      "processing_time": 74.59362602233887,
      "citing_paper_id": "279464818",
      "cited_paper_id": 85498726
    },
    {
      "context_text": "Many works [4, 43, 59, 65, 73, 125, 131, 144] adopt the latent diffusion architecture following Stable Diffusion [97], where a separate autoencoder (VAE) [47] extracts latent codes from the LQ images to denoise.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions the use of a VAE to extract latent codes from LQ images for denoising, which is a method rather than a specific dataset. No specific datasets are named.",
      "processing_time": 63.31888127326965,
      "citing_paper_id": "279464818",
      "cited_paper_id": 211146177
    },
    {
      "context_text": "Many works [4, 43, 59, 65, 73, 125, 131, 144] adopt the latent diffusion architecture following Stable Diffusion [97], where a separate autoencoder (VAE) [47] extracts latent codes from the LQ images to denoise.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions the use of a VAE to extract latent codes from LQ images for denoising, which is a method rather than a specific dataset. No specific datasets are named.",
      "processing_time": 63.31888127326965,
      "citing_paper_id": "279464818",
      "cited_paper_id": 258179052
    },
    {
      "context_text": "Many works [4, 43, 59, 65, 73, 125, 131, 144] adopt the latent diffusion architecture following Stable Diffusion [97], where a separate autoencoder (VAE) [47] extracts latent codes from the LQ images to denoise.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions the use of a VAE to extract latent codes from LQ images for denoising, which is a method rather than a specific dataset. No specific datasets are named.",
      "processing_time": 63.31888127326965,
      "citing_paper_id": "279464818",
      "cited_paper_id": 261276317
    },
    {
      "context_text": "Many works [4, 43, 59, 65, 73, 125, 131, 144] adopt the latent diffusion architecture following Stable Diffusion [97], where a separate autoencoder (VAE) [47] extracts latent codes from the LQ images to denoise.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions the use of a VAE to extract latent codes from LQ images for denoising, which is a method rather than a specific dataset. No specific datasets are named.",
      "processing_time": 63.31888127326965,
      "citing_paper_id": "279464818",
      "cited_paper_id": 267199774
    },
    {
      "context_text": "Many works [4, 43, 59, 65, 73, 125, 131, 144] adopt the latent diffusion architecture following Stable Diffusion [97], where a separate autoencoder (VAE) [47] extracts latent codes from the LQ images to denoise.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions the use of a VAE to extract latent codes from LQ images for denoising, which is a method rather than a specific dataset. No specific datasets are named.",
      "processing_time": 63.31888127326965,
      "citing_paper_id": "279464818",
      "cited_paper_id": 272724853
    },
    {
      "context_text": "…we used Dense-Haze [6] for de-hazing, Rain1400 [30] for deraining, GoPro [81] for motion deblurring, RainDrop [87] for raindrop removal, DPDD [2] for defocus deblurring, Snow100K [67] for desnowing, SIDD [1] for real denoising, and LIVE1 [104] for JPEG artifact removal (with quality factor q…",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Dense-Haze",
        "Rain1400",
        "GoPro",
        "RainDrop",
        "DPDD",
        "Snow100K",
        "SIDD",
        "LIVE1"
      ],
      "dataset_descriptions": {
        "Dense-Haze": "Used for de-hazing images, focusing on improving visibility and color fidelity in hazy conditions.",
        "Rain1400": "Used for deraining images, specifically addressing the removal of rain streaks and drops to enhance clarity.",
        "GoPro": "Used for motion deblurring, focusing on restoring sharpness in images affected by camera or object motion.",
        "RainDrop": "Used for raindrop removal, specifically targeting the elimination of water droplets from images.",
        "DPDD": "Used for defocus deblurring, focusing on sharpening out-of-focus regions using dual-pixel data.",
        "Snow100K": "Used for desnowing images, specifically addressing the removal of snowflakes and snow accumulation.",
        "SIDD": "Used for real denoising, focusing on reducing noise in images while preserving details.",
        "LIVE1": "Used for JPEG artifact removal, specifically addressing compression artifacts at different quality factors."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions multiple datasets used for various image restoration tasks, which align with the research topic of all-in-one image restoration.",
      "processing_time": 104.49896717071533,
      "citing_paper_id": "279464818",
      "cited_paper_id": 218470249
    },
    {
      "context_text": "For real-world degradation datasets, we evaluated our model on the NH-HAZE [7] dataset, LHP [33] dataset, RealBlur [96] dataset, and the RealSnow [149] dataset.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "NH-HAZE",
        "LHP",
        "RealBlur",
        "RealSnow"
      ],
      "dataset_descriptions": {
        "NH-HAZE": "Used to evaluate image dehazing performance, focusing on non-homogeneous hazy and haze-free images.",
        "LHP": "Used to assess the model's effectiveness in handling low-light and high-dynamic-range images.",
        "RealBlur": "Used to test the model's ability to restore images with realistic motion blur.",
        "RealSnow": "Used to evaluate the model's performance in removing snow from images under various weather conditions."
      },
      "confidence_score": 1.0,
      "reasoning": "The context explicitly mentions four datasets used for evaluating the model on real-world degradations, which are NH-HAZE, LHP, RealBlur, and RealSnow.",
      "processing_time": 82.59084343910217,
      "citing_paper_id": "279464818",
      "cited_paper_id": 218538083
    },
    {
      "context_text": "For real-world degradation datasets, we evaluated our model on the NH-HAZE [7] dataset, LHP [33] dataset, RealBlur [96] dataset, and the RealSnow [149] dataset.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "NH-HAZE",
        "LHP",
        "RealBlur",
        "RealSnow"
      ],
      "dataset_descriptions": {
        "NH-HAZE": "Used to evaluate image dehazing performance, focusing on non-homogeneous hazy and haze-free images.",
        "LHP": "Used to assess the model's effectiveness in handling low-light and high-dynamic-range images.",
        "RealBlur": "Used to test the model's ability to restore images with realistic motion blur.",
        "RealSnow": "Used to evaluate the model's performance in removing snow from images under various weather conditions."
      },
      "confidence_score": 1.0,
      "reasoning": "The context explicitly mentions four datasets used for evaluating the model on real-world degradations, which are NH-HAZE, LHP, RealBlur, and RealSnow.",
      "processing_time": 82.59084343910217,
      "citing_paper_id": "279464818",
      "cited_paper_id": 259144110
    },
    {
      "context_text": "Dehazing (NH-HAZE [7]) Deraining (LHP [33]) Desnowing (RealSnow [149]) Motion Deblur (RealBlur-J [96]) hazing.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "NH-HAZE",
        "LHP",
        "RealSnow",
        "RealBlur-J"
      ],
      "dataset_descriptions": {
        "NH-HAZE": "Used for dehazing images, providing non-homogeneous hazy and haze-free images for training and evaluation.",
        "LHP": "Used for deraining images, focusing on learning weather-general and weather-specific features for image restoration.",
        "RealSnow": "Used for desnowing images, providing real-world snow conditions for training and evaluation.",
        "RealBlur-J": "Used for motion deblurring, offering real-world motion blur scenarios for training and evaluation."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions specific datasets used for various image restoration tasks, including dehazing, deraining, desnowing, and motion deblurring. Each dataset is associated with a specific task and is likely used for training and evaluation.",
      "processing_time": 85.95477747917175,
      "citing_paper_id": "279464818",
      "cited_paper_id": 218538083
    },
    {
      "context_text": "Dehazing (NH-HAZE [7]) Deraining (LHP [33]) Desnowing (RealSnow [149]) Motion Deblur (RealBlur-J [96]) hazing.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "NH-HAZE",
        "LHP",
        "RealSnow",
        "RealBlur-J"
      ],
      "dataset_descriptions": {
        "NH-HAZE": "Used for dehazing images, providing non-homogeneous hazy and haze-free images for training and evaluation.",
        "LHP": "Used for deraining images, focusing on learning weather-general and weather-specific features for image restoration.",
        "RealSnow": "Used for desnowing images, providing real-world snow conditions for training and evaluation.",
        "RealBlur-J": "Used for motion deblurring, offering real-world motion blur scenarios for training and evaluation."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions specific datasets used for various image restoration tasks, including dehazing, deraining, desnowing, and motion deblurring. Each dataset is associated with a specific task and is likely used for training and evaluation.",
      "processing_time": 85.95477747917175,
      "citing_paper_id": "279464818",
      "cited_paper_id": 259144110
    },
    {
      "context_text": "The model used the U-Net [22] architecture, DDIM [107] as the sampling strategy during inference, and 4 timesteps for all tasks.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models and methods. The context focuses on the architecture and sampling strategy used.",
      "processing_time": 61.29996204376221,
      "citing_paper_id": "279464818",
      "cited_paper_id": 222140788
    },
    {
      "context_text": "Diffusion models [36, 46] generate data x 0 ∼ p 0 ( · ) by reversing the following forward process that “diffuses” x 0 into standard Gaussian noise ϵ ∼ N ( 0 , I ) : where α t , σ t define the noise schedule.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only diffusion models and their processes. No verifiable resources are identified.",
      "processing_time": 61.04701900482178,
      "citing_paper_id": "279464818",
      "cited_paper_id": 235694314
    },
    {
      "context_text": "This encompasses various challenges, including denoising [8, 95, 121], deblurring [110, 111, 122], de-hazing [64, 109], deraining [16, 17, 42], JPEG artifact removal [25, 41], etc. Traditional deep learning-based approaches have achieved remarkable progress by developing task-specific models that…",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various image restoration tasks. The cited papers' titles do not provide additional dataset information.",
      "processing_time": 61.374738931655884,
      "citing_paper_id": "279464818",
      "cited_paper_id": 235702920
    },
    {
      "context_text": "This encompasses various challenges, including denoising [8, 95, 121], deblurring [110, 111, 122], de-hazing [64, 109], deraining [16, 17, 42], JPEG artifact removal [25, 41], etc. Traditional deep learning-based approaches have achieved remarkable progress by developing task-specific models that…",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various image restoration tasks. The cited papers' titles do not provide additional dataset information.",
      "processing_time": 61.374738931655884,
      "citing_paper_id": "279464818",
      "cited_paper_id": 244908890
    },
    {
      "context_text": "This encompasses various challenges, including denoising [8, 95, 121], deblurring [110, 111, 122], de-hazing [64, 109], deraining [16, 17, 42], JPEG artifact removal [25, 41], etc. Traditional deep learning-based approaches have achieved remarkable progress by developing task-specific models that…",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various image restoration tasks. The cited papers' titles do not provide additional dataset information.",
      "processing_time": 61.374738931655884,
      "citing_paper_id": "279464818",
      "cited_paper_id": 248069347
    },
    {
      "context_text": "This encompasses various challenges, including denoising [8, 95, 121], deblurring [110, 111, 122], de-hazing [64, 109], deraining [16, 17, 42], JPEG artifact removal [25, 41], etc. Traditional deep learning-based approaches have achieved remarkable progress by developing task-specific models that…",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various image restoration tasks. The cited papers' titles do not provide additional dataset information.",
      "processing_time": 61.374738931655884,
      "citing_paper_id": "279464818",
      "cited_paper_id": 248085101
    },
    {
      "context_text": "This encompasses various challenges, including denoising [8, 95, 121], deblurring [110, 111, 122], de-hazing [64, 109], deraining [16, 17, 42], JPEG artifact removal [25, 41], etc. Traditional deep learning-based approaches have achieved remarkable progress by developing task-specific models that…",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various image restoration tasks. The cited papers' titles do not provide additional dataset information.",
      "processing_time": 61.374738931655884,
      "citing_paper_id": "279464818",
      "cited_paper_id": 261245351
    },
    {
      "context_text": "…challenges, including denoising [8, 95, 121], deblurring [110, 111, 122], de-hazing [64, 109], deraining [16, 17, 42], JPEG artifact removal [25, 41], etc. Traditional deep learning-based approaches have achieved remarkable progress by developing task-specific models that excel in individual…",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various image restoration tasks. The cited paper title 'Towards Flexible Blind JPEG Artifacts Removal' does not introduce a dataset either.",
      "processing_time": 62.52242398262024,
      "citing_paper_id": "279464818",
      "cited_paper_id": 238215243
    },
    {
      "context_text": "We draw inspiration from image quality assessment [77, 78, 114] and select TE42 [26], a family of charts commonly used for camera testing and visual analysis, to construct a pool of visual grounds.",
      "catation_intent": "reusable resource",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions 'TE42' as a family of charts used for camera testing and visual analysis. However, it does not specify a dataset, and 'TE42' is more likely a method or tool rather than a dataset.",
      "processing_time": 64.45064926147461,
      "citing_paper_id": "279464818",
      "cited_paper_id": 242630136
    },
    {
      "context_text": "We draw inspiration from image quality assessment [77, 78, 114] and select TE42 [26], a family of charts commonly used for camera testing and visual analysis, to construct a pool of visual grounds.",
      "catation_intent": "reusable resource",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions 'TE42' as a family of charts used for camera testing and visual analysis. However, it does not specify a dataset, and 'TE42' is more likely a method or tool rather than a dataset.",
      "processing_time": 64.45064926147461,
      "citing_paper_id": "279464818",
      "cited_paper_id": null
    },
    {
      "context_text": "Image restoration methods mainly target predefined image degradation [15, 28, 31, 44, 52, 53, 57, 60, 134, 148] such as motion blurring or rainy images.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general types of image degradation. No dataset names are provided.",
      "processing_time": 60.743977785110474,
      "citing_paper_id": "279464818",
      "cited_paper_id": 248085491
    },
    {
      "context_text": "Image restoration methods mainly target predefined image degradation [15, 28, 31, 44, 52, 53, 57, 60, 134, 148] such as motion blurring or rainy images.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general types of image degradation. No dataset names are provided.",
      "processing_time": 60.743977785110474,
      "citing_paper_id": "279464818",
      "cited_paper_id": 250085592
    },
    {
      "context_text": "Image restoration methods mainly target predefined image degradation [15, 28, 31, 44, 52, 53, 57, 60, 134, 148] such as motion blurring or rainy images.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general types of image degradation. No dataset names are provided.",
      "processing_time": 60.743977785110474,
      "citing_paper_id": "279464818",
      "cited_paper_id": 251020317
    },
    {
      "context_text": "Image restoration methods mainly target predefined image degradation [15, 28, 31, 44, 52, 53, 57, 60, 134, 148] such as motion blurring or rainy images.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general types of image degradation. No dataset names are provided.",
      "processing_time": 60.743977785110474,
      "citing_paper_id": "279464818",
      "cited_paper_id": 260887508
    },
    {
      "context_text": "…with various types of instructions, including both texts [29, 56, 94, 97, 101] and visual instructions such as depth maps [140], segmentation maps [20, 117], layouts [97], in-painting masks [38, 127], sketches [116], skeletons [80], keypoints [55], example images [10, 82, 118], to name a few.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.0,
      "reasoning": "The context mentions various types of visual instructions but does not specify any particular dataset. The cited papers do not provide additional context to identify specific datasets.",
      "processing_time": 61.34977674484253,
      "citing_paper_id": "279464818",
      "cited_paper_id": 253018768
    },
    {
      "context_text": "…with various types of instructions, including both texts [29, 56, 94, 97, 101] and visual instructions such as depth maps [140], segmentation maps [20, 117], layouts [97], in-painting masks [38, 127], sketches [116], skeletons [80], keypoints [55], example images [10, 82, 118], to name a few.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.0,
      "reasoning": "The context mentions various types of visual instructions but does not specify any particular dataset. The cited papers do not provide additional context to identify specific datasets.",
      "processing_time": 61.34977674484253,
      "citing_paper_id": "279464818",
      "cited_paper_id": 254018130
    },
    {
      "context_text": "…with various types of instructions, including both texts [29, 56, 94, 97, 101] and visual instructions such as depth maps [140], segmentation maps [20, 117], layouts [97], in-painting masks [38, 127], sketches [116], skeletons [80], keypoints [55], example images [10, 82, 118], to name a few.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.0,
      "reasoning": "The context mentions various types of visual instructions but does not specify any particular dataset. The cited papers do not provide additional context to identify specific datasets.",
      "processing_time": 61.34977674484253,
      "citing_paper_id": "279464818",
      "cited_paper_id": 254535802
    },
    {
      "context_text": "…with various types of instructions, including both texts [29, 56, 94, 97, 101] and visual instructions such as depth maps [140], segmentation maps [20, 117], layouts [97], in-painting masks [38, 127], sketches [116], skeletons [80], keypoints [55], example images [10, 82, 118], to name a few.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.0,
      "reasoning": "The context mentions various types of visual instructions but does not specify any particular dataset. The cited papers do not provide additional context to identify specific datasets.",
      "processing_time": 61.34977674484253,
      "citing_paper_id": "279464818",
      "cited_paper_id": 257038979
    },
    {
      "context_text": "Among them, some works propose dedicated diffusion processes to model image degradations, where the HQ images are corrupted given the LQ images [45] or shifts between HQ and LQ images [62, 63, 72, 105, 132].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only references to methods or models for image restoration. No verifiable resources are identified.",
      "processing_time": 61.03262114524841,
      "citing_paper_id": "279464818",
      "cited_paper_id": 256358842
    },
    {
      "context_text": "Among them, some works propose dedicated diffusion processes to model image degradations, where the HQ images are corrupted given the LQ images [45] or shifts between HQ and LQ images [62, 63, 72, 105, 132].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only references to methods or models for image restoration. No verifiable resources are identified.",
      "processing_time": 61.03262114524841,
      "citing_paper_id": "279464818",
      "cited_paper_id": 260125321
    },
    {
      "context_text": "Among them, some works propose dedicated diffusion processes to model image degradations, where the HQ images are corrupted given the LQ images [45] or shifts between HQ and LQ images [62, 63, 72, 105, 132].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only references to methods or models for image restoration. No verifiable resources are identified.",
      "processing_time": 61.03262114524841,
      "citing_paper_id": "279464818",
      "cited_paper_id": 261244079
    },
    {
      "context_text": "Other works operate on pixels [45, 62, 72] or frequency domain [146].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only general approaches in image restoration. No dataset names are present in the citation span.",
      "processing_time": 61.18663573265076,
      "citing_paper_id": "279464818",
      "cited_paper_id": 256358842
    },
    {
      "context_text": "Other works operate on pixels [45, 62, 72] or frequency domain [146].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only general approaches in image restoration. No dataset names are present in the citation span.",
      "processing_time": 61.18663573265076,
      "citing_paper_id": "279464818",
      "cited_paper_id": 265466804
    },
    {
      "context_text": "…models to generate images with various types of instructions, including both texts [29, 56, 94, 97, 101] and visual instructions such as depth maps [140], segmentation maps [20, 117], layouts [97], in-painting masks [38, 127], sketches [116], skeletons [80], keypoints [55], example images [10, 82,…",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various types of visual instructions used in image generation. No clear, verifiable datasets are identified.",
      "processing_time": 61.3609893321991,
      "citing_paper_id": "279464818",
      "cited_paper_id": 256827727
    },
    {
      "context_text": "Unlike previous works [4, 59, 131], the performance deteriorates if we inject the features into the U-Net decoder with ControlNet [140] or a similar adapter.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models and methods. There are no clear identifiers for datasets in the provided context.",
      "processing_time": 60.81398844718933,
      "citing_paper_id": "279464818",
      "cited_paper_id": 256827727
    },
    {
      "context_text": "Unlike previous works [4, 59, 131], the performance deteriorates if we inject the features into the U-Net decoder with ControlNet [140] or a similar adapter.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models and methods. There are no clear identifiers for datasets in the provided context.",
      "processing_time": 60.81398844718933,
      "citing_paper_id": "279464818",
      "cited_paper_id": 261276317
    },
    {
      "context_text": "Unlike previous works [4, 59, 131], the performance deteriorates if we inject the features into the U-Net decoder with ControlNet [140] or a similar adapter.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models and methods. There are no clear identifiers for datasets in the provided context.",
      "processing_time": 60.81398844718933,
      "citing_paper_id": "279464818",
      "cited_paper_id": 267199774
    },
    {
      "context_text": "Unlike previous works [4, 59, 131], the performance deteriorates if we inject the features into the U-Net decoder with ControlNet [140] or a similar adapter.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models and methods. There are no clear identifiers for datasets in the provided context.",
      "processing_time": 60.81398844718933,
      "citing_paper_id": "279464818",
      "cited_paper_id": 272724853
    },
    {
      "context_text": "Most all-in-one approaches [4, 34, 51, 59, 65, 74, 84, 86, 88, 130, 143, 145, 147] adopt auxiliary modules or learnable prompts to detect degradation types for the restoration model.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to methods or approaches for image restoration. No dataset names are present in the text.",
      "processing_time": 61.487568855285645,
      "citing_paper_id": "279464818",
      "cited_paper_id": 257804691
    },
    {
      "context_text": "Most all-in-one approaches [4, 34, 51, 59, 65, 74, 84, 86, 88, 130, 143, 145, 147] adopt auxiliary modules or learnable prompts to detect degradation types for the restoration model.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to methods or approaches for image restoration. No dataset names are present in the text.",
      "processing_time": 61.487568855285645,
      "citing_paper_id": "279464818",
      "cited_paper_id": 259243623
    },
    {
      "context_text": "Most all-in-one approaches [4, 34, 51, 59, 65, 74, 84, 86, 88, 130, 143, 145, 147] adopt auxiliary modules or learnable prompts to detect degradation types for the restoration model.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to methods or approaches for image restoration. No dataset names are present in the text.",
      "processing_time": 61.487568855285645,
      "citing_paper_id": "279464818",
      "cited_paper_id": 260107992
    },
    {
      "context_text": "Most all-in-one approaches [4, 34, 51, 59, 65, 74, 84, 86, 88, 130, 143, 145, 147] adopt auxiliary modules or learnable prompts to detect degradation types for the restoration model.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to methods or approaches for image restoration. No dataset names are present in the text.",
      "processing_time": 61.487568855285645,
      "citing_paper_id": "279464818",
      "cited_paper_id": 261276317
    },
    {
      "context_text": "Most all-in-one approaches [4, 34, 51, 59, 65, 74, 84, 86, 88, 130, 143, 145, 147] adopt auxiliary modules or learnable prompts to detect degradation types for the restoration model.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to methods or approaches for image restoration. No dataset names are present in the text.",
      "processing_time": 61.487568855285645,
      "citing_paper_id": "279464818",
      "cited_paper_id": 268064460
    },
    {
      "context_text": "Most all-in-one approaches [4, 34, 51, 59, 65, 74, 84, 86, 88, 130, 143, 145, 147] adopt auxiliary modules or learnable prompts to detect degradation types for the restoration model.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to methods or approaches for image restoration. No dataset names are present in the text.",
      "processing_time": 61.487568855285645,
      "citing_paper_id": "279464818",
      "cited_paper_id": 268513542
    },
    {
      "context_text": "Most all-in-one approaches [4, 34, 51, 59, 65, 74, 84, 86, 88, 130, 143, 145, 147] adopt auxiliary modules or learnable prompts to detect degradation types for the restoration model.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to methods or approaches for image restoration. No dataset names are present in the text.",
      "processing_time": 61.487568855285645,
      "citing_paper_id": "279464818",
      "cited_paper_id": 269430366
    },
    {
      "context_text": "Most all-in-one approaches [4, 34, 51, 59, 65, 74, 84, 86, 88, 130, 143, 145, 147] adopt auxiliary modules or learnable prompts to detect degradation types for the restoration model.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to methods or approaches for image restoration. No dataset names are present in the text.",
      "processing_time": 61.487568855285645,
      "citing_paper_id": "279464818",
      "cited_paper_id": 271039782
    },
    {
      "context_text": "Most all-in-one approaches [4, 34, 51, 59, 65, 74, 84, 86, 88, 130, 143, 145, 147] adopt auxiliary modules or learnable prompts to detect degradation types for the restoration model.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to methods or approaches for image restoration. No dataset names are present in the text.",
      "processing_time": 61.487568855285645,
      "citing_paper_id": "279464818",
      "cited_paper_id": 272724853
    },
    {
      "context_text": "Most all-in-one approaches [4, 34, 51, 59, 65, 74, 84, 86, 88, 130, 143, 145, 147] adopt auxiliary modules or learnable prompts to detect degradation types for the restoration model.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to methods or approaches for image restoration. No dataset names are present in the text.",
      "processing_time": 61.487568855285645,
      "citing_paper_id": "279464818",
      "cited_paper_id": 272987034
    },
    {
      "context_text": "Recently, several all-in-one approaches [4, 19, 34, 43, 51, 59, 65, 74, 84, 86, 88, 130, 131, 143, 145, 147] have been proposed to tackle multiple degradations with one unified model.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to various papers proposing all-in-one image restoration approaches.",
      "processing_time": 60.67425346374512,
      "citing_paper_id": "279464818",
      "cited_paper_id": 257804691
    },
    {
      "context_text": "Recently, several all-in-one approaches [4, 19, 34, 43, 51, 59, 65, 74, 84, 86, 88, 130, 131, 143, 145, 147] have been proposed to tackle multiple degradations with one unified model.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to various papers proposing all-in-one image restoration approaches.",
      "processing_time": 60.67425346374512,
      "citing_paper_id": "279464818",
      "cited_paper_id": 259243623
    },
    {
      "context_text": "Recently, several all-in-one approaches [4, 19, 34, 43, 51, 59, 65, 74, 84, 86, 88, 130, 131, 143, 145, 147] have been proposed to tackle multiple degradations with one unified model.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to various papers proposing all-in-one image restoration approaches.",
      "processing_time": 60.67425346374512,
      "citing_paper_id": "279464818",
      "cited_paper_id": 260107992
    },
    {
      "context_text": "Recently, several all-in-one approaches [4, 19, 34, 43, 51, 59, 65, 74, 84, 86, 88, 130, 131, 143, 145, 147] have been proposed to tackle multiple degradations with one unified model.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to various papers proposing all-in-one image restoration approaches.",
      "processing_time": 60.67425346374512,
      "citing_paper_id": "279464818",
      "cited_paper_id": 261276317
    },
    {
      "context_text": "Recently, several all-in-one approaches [4, 19, 34, 43, 51, 59, 65, 74, 84, 86, 88, 130, 131, 143, 145, 147] have been proposed to tackle multiple degradations with one unified model.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to various papers proposing all-in-one image restoration approaches.",
      "processing_time": 60.67425346374512,
      "citing_paper_id": "279464818",
      "cited_paper_id": 267199774
    },
    {
      "context_text": "Recently, several all-in-one approaches [4, 19, 34, 43, 51, 59, 65, 74, 84, 86, 88, 130, 131, 143, 145, 147] have been proposed to tackle multiple degradations with one unified model.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to various papers proposing all-in-one image restoration approaches.",
      "processing_time": 60.67425346374512,
      "citing_paper_id": "279464818",
      "cited_paper_id": 267320695
    },
    {
      "context_text": "Recently, several all-in-one approaches [4, 19, 34, 43, 51, 59, 65, 74, 84, 86, 88, 130, 131, 143, 145, 147] have been proposed to tackle multiple degradations with one unified model.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to various papers proposing all-in-one image restoration approaches.",
      "processing_time": 60.67425346374512,
      "citing_paper_id": "279464818",
      "cited_paper_id": 268064460
    },
    {
      "context_text": "Recently, several all-in-one approaches [4, 19, 34, 43, 51, 59, 65, 74, 84, 86, 88, 130, 131, 143, 145, 147] have been proposed to tackle multiple degradations with one unified model.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to various papers proposing all-in-one image restoration approaches.",
      "processing_time": 60.67425346374512,
      "citing_paper_id": "279464818",
      "cited_paper_id": 268513542
    },
    {
      "context_text": "Recently, several all-in-one approaches [4, 19, 34, 43, 51, 59, 65, 74, 84, 86, 88, 130, 131, 143, 145, 147] have been proposed to tackle multiple degradations with one unified model.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to various papers proposing all-in-one image restoration approaches.",
      "processing_time": 60.67425346374512,
      "citing_paper_id": "279464818",
      "cited_paper_id": 269430366
    },
    {
      "context_text": "Recently, several all-in-one approaches [4, 19, 34, 43, 51, 59, 65, 74, 84, 86, 88, 130, 131, 143, 145, 147] have been proposed to tackle multiple degradations with one unified model.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to various papers proposing all-in-one image restoration approaches.",
      "processing_time": 60.67425346374512,
      "citing_paper_id": "279464818",
      "cited_paper_id": 271039782
    },
    {
      "context_text": "Recently, several all-in-one approaches [4, 19, 34, 43, 51, 59, 65, 74, 84, 86, 88, 130, 131, 143, 145, 147] have been proposed to tackle multiple degradations with one unified model.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to various papers proposing all-in-one image restoration approaches.",
      "processing_time": 60.67425346374512,
      "citing_paper_id": "279464818",
      "cited_paper_id": 272724853
    },
    {
      "context_text": "Recently, several all-in-one approaches [4, 19, 34, 43, 51, 59, 65, 74, 84, 86, 88, 130, 131, 143, 145, 147] have been proposed to tackle multiple degradations with one unified model.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to various papers proposing all-in-one image restoration approaches.",
      "processing_time": 60.67425346374512,
      "citing_paper_id": "279464818",
      "cited_paper_id": 272987034
    },
    {
      "context_text": "Diffusion-based generative models (DMs) [36, 106, 108] have risen as attractive solutions due to their remarkable ability to produce natural and visually appealing HQ images [22, 23, 45, 73, 97].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to diffusion-based generative models and their capabilities.",
      "processing_time": 60.543798208236694,
      "citing_paper_id": "279464818",
      "cited_paper_id": 258179052
    },
    {
      "context_text": "…Outdoor-Rain [54], and LHP [33]; dehazing datasets like RESIDE [49], and Dense-Haze [6]; desnowing datasets such as Snow100K [67] and RealSnow [149]; raindrop removal datasets like RainDrop [87] and RainDS [90]; real denoising with SIDD [1]; motion deblurring with GoPro [81] and RealBlur…",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Outdoor-Rain",
        "LHP",
        "RESIDE",
        "Dense-Haze",
        "Snow100K",
        "RealSnow",
        "RainDrop",
        "RainDS",
        "SIDD",
        "GoPro",
        "RealBlur"
      ],
      "dataset_descriptions": {
        "Outdoor-Rain": "Used for training and evaluating rain removal algorithms, focusing on synthetic and real rain images to improve image clarity.",
        "LHP": "Used for evaluating rain removal performance, providing a diverse set of images with varying rain intensities and backgrounds.",
        "RESIDE": "Used for dehazing, containing synthetic and real hazy images to train and test haze removal algorithms.",
        "Dense-Haze": "Used for dehazing, focusing on dense haze conditions to evaluate the effectiveness of haze removal techniques.",
        "Snow100K": "Used for desnowing, providing a large dataset of synthetic snow images to train and test snow removal algorithms.",
        "RealSnow": "Used for desnowing, containing real-world snow images to evaluate the performance of snow removal methods.",
        "RainDrop": "Used for raindrop removal, focusing on images with raindrops on camera lenses to improve image quality.",
        "RainDS": "Used for raindrop removal, providing a diverse set of images with raindrops to evaluate the robustness of raindrop removal algorithms.",
        "SIDD": "Used for real denoising, containing real noisy images to train and test denoising algorithms.",
        "GoPro": "Used for motion deblurring, providing sharp and blurred image pairs to train and evaluate deblurring algorithms.",
        "RealBlur": "Used for motion deblurring, containing real-world blurred images to assess the performance of deblurring methods."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions several datasets used for various image restoration tasks, which are directly relevant to the research topic of all-in-one image restoration.",
      "processing_time": 126.38897585868835,
      "citing_paper_id": "279464818",
      "cited_paper_id": 259144110
    },
    {
      "context_text": "Explicit priors, typically text descriptions processed by language models [19, 131], suffer from weak alignment with low-level visual structures [66, 124].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only a general reference to 'text descriptions' which is too generic. No dataset names are present in the context.",
      "processing_time": 62.41963076591492,
      "citing_paper_id": "279464818",
      "cited_paper_id": 259837088
    },
    {
      "context_text": "Explicit priors, typically text descriptions processed by language models [19, 131], suffer from weak alignment with low-level visual structures [66, 124].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only a general reference to 'text descriptions' which is too generic. No dataset names are present in the context.",
      "processing_time": 62.41963076591492,
      "citing_paper_id": "279464818",
      "cited_paper_id": 267199774
    },
    {
      "context_text": "Explicit priors, typically text descriptions processed by language models [19, 131], suffer from weak alignment with low-level visual structures [66, 124].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only a general reference to 'text descriptions' which is too generic. No dataset names are present in the context.",
      "processing_time": 62.41963076591492,
      "citing_paper_id": "279464818",
      "cited_paper_id": 267320695
    },
    {
      "context_text": "We combined various image restoration datasets to form the training set, following [137].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.3,
      "reasoning": "The context mentions 'image restoration datasets' but does not specify any particular dataset names. The cited paper title suggests a focus on image restoration but does not help disambiguate specific datasets.",
      "processing_time": 63.21158528327942,
      "citing_paper_id": "279464818",
      "cited_paper_id": 260085391
    },
    {
      "context_text": "Most all-in-one image restoration estimate degradation types given the input LQ images to guide restoration [4, 59, 84, 86], where the estimation becomes inaccurate for unseen degradation types.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only general concepts about all-in-one image restoration and degradation types.",
      "processing_time": 60.810850620269775,
      "citing_paper_id": "279464818",
      "cited_paper_id": 260107992
    },
    {
      "context_text": "Most all-in-one image restoration estimate degradation types given the input LQ images to guide restoration [4, 59, 84, 86], where the estimation becomes inaccurate for unseen degradation types.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only general concepts about all-in-one image restoration and degradation types.",
      "processing_time": 60.810850620269775,
      "citing_paper_id": "279464818",
      "cited_paper_id": 261276317
    },
    {
      "context_text": "Most all-in-one image restoration estimate degradation types given the input LQ images to guide restoration [4, 59, 84, 86], where the estimation becomes inaccurate for unseen degradation types.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only general concepts about all-in-one image restoration and degradation types.",
      "processing_time": 60.810850620269775,
      "citing_paper_id": "279464818",
      "cited_paper_id": 268064460
    },
    {
      "context_text": "Most all-in-one image restoration estimate degradation types given the input LQ images to guide restoration [4, 59, 84, 86], where the estimation becomes inaccurate for unseen degradation types.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only general concepts about all-in-one image restoration and degradation types.",
      "processing_time": 60.810850620269775,
      "citing_paper_id": "279464818",
      "cited_paper_id": 272724853
    },
    {
      "context_text": "Moreover, following the method in [84], we constructed a mixed distortion dataset on the WED [75] dataset using rain, snow, and noise distortions.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "WED"
      ],
      "dataset_descriptions": {
        "WED": "Used to construct a mixed distortion dataset incorporating rain, snow, and noise distortions for evaluating all-in-one image restoration methods."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions the construction of a mixed distortion dataset using the WED dataset, which is relevant to the topic of all-in-one image restoration.",
      "processing_time": 67.48851561546326,
      "citing_paper_id": "279464818",
      "cited_paper_id": 260107992
    },
    {
      "context_text": "Implicit priors are extracted from input low-quality (LQ) images by task-specific sub-networks [14, 84] or learnable prompts [4, 59, 86, 143]; they are essentially equivalent to increasing the parameter counts, limiting their generalizability to complex degradations [43].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only methods and models. The cited papers' titles also do not provide clear dataset names.",
      "processing_time": 61.92705416679382,
      "citing_paper_id": "279464818",
      "cited_paper_id": 260107992
    },
    {
      "context_text": "Implicit priors are extracted from input low-quality (LQ) images by task-specific sub-networks [14, 84] or learnable prompts [4, 59, 86, 143]; they are essentially equivalent to increasing the parameter counts, limiting their generalizability to complex degradations [43].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only methods and models. The cited papers' titles also do not provide clear dataset names.",
      "processing_time": 61.92705416679382,
      "citing_paper_id": "279464818",
      "cited_paper_id": 261276317
    },
    {
      "context_text": "Implicit priors are extracted from input low-quality (LQ) images by task-specific sub-networks [14, 84] or learnable prompts [4, 59, 86, 143]; they are essentially equivalent to increasing the parameter counts, limiting their generalizability to complex degradations [43].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only methods and models. The cited papers' titles also do not provide clear dataset names.",
      "processing_time": 61.92705416679382,
      "citing_paper_id": "279464818",
      "cited_paper_id": 268064460
    },
    {
      "context_text": "Implicit priors are extracted from input low-quality (LQ) images by task-specific sub-networks [14, 84] or learnable prompts [4, 59, 86, 143]; they are essentially equivalent to increasing the parameter counts, limiting their generalizability to complex degradations [43].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only methods and models. The cited papers' titles also do not provide clear dataset names.",
      "processing_time": 61.92705416679382,
      "citing_paper_id": "279464818",
      "cited_paper_id": 272724853
    },
    {
      "context_text": "To address this limitation, all-in-one image restoration methods handling various degradations in a single model have gained significant attention [4, 59, 65, 84, 86, 143].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to methods and approaches for all-in-one image restoration.",
      "processing_time": 61.27067446708679,
      "citing_paper_id": "279464818",
      "cited_paper_id": 260107992
    },
    {
      "context_text": "To address this limitation, all-in-one image restoration methods handling various degradations in a single model have gained significant attention [4, 59, 65, 84, 86, 143].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to methods and approaches for all-in-one image restoration.",
      "processing_time": 61.27067446708679,
      "citing_paper_id": "279464818",
      "cited_paper_id": 261276317
    },
    {
      "context_text": "To address this limitation, all-in-one image restoration methods handling various degradations in a single model have gained significant attention [4, 59, 65, 84, 86, 143].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to methods and approaches for all-in-one image restoration.",
      "processing_time": 61.27067446708679,
      "citing_paper_id": "279464818",
      "cited_paper_id": 268064460
    },
    {
      "context_text": "To address this limitation, all-in-one image restoration methods handling various degradations in a single model have gained significant attention [4, 59, 65, 84, 86, 143].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to methods and approaches for all-in-one image restoration.",
      "processing_time": 61.27067446708679,
      "citing_paper_id": "279464818",
      "cited_paper_id": 272724853
    },
    {
      "context_text": "Besides, unlike [63, 105, 132], our formulation has no special requirement for α t , σ t ; thus, we can adapt any pretrained diffusion model into our formulation with minimal efforts.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only references to other works and their formulations. No verifiable resources are identified.",
      "processing_time": 61.79690408706665,
      "citing_paper_id": "279464818",
      "cited_paper_id": 260125321
    },
    {
      "context_text": "Besides, unlike [63, 105, 132], our formulation has no special requirement for α t , σ t ; thus, we can adapt any pretrained diffusion model into our formulation with minimal efforts.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only references to other works and their formulations. No verifiable resources are identified.",
      "processing_time": 61.79690408706665,
      "citing_paper_id": "279464818",
      "cited_paper_id": 261244079
    },
    {
      "context_text": "Previous works [4, 43, 59, 131] model the distribution of HQ images x HQ with diffusion models guided by LQ images x LQ , so that x 0 = x HQ in Eq.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only a general approach using HQ and LQ images. No verifiable resource names are provided.",
      "processing_time": 62.07736539840698,
      "citing_paper_id": "279464818",
      "cited_paper_id": 261276317
    },
    {
      "context_text": "Previous works [4, 43, 59, 131] model the distribution of HQ images x HQ with diffusion models guided by LQ images x LQ , so that x 0 = x HQ in Eq.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only a general approach using HQ and LQ images. No verifiable resource names are provided.",
      "processing_time": 62.07736539840698,
      "citing_paper_id": "279464818",
      "cited_paper_id": 267199774
    },
    {
      "context_text": "Previous works [4, 43, 59, 131] model the distribution of HQ images x HQ with diffusion models guided by LQ images x LQ , so that x 0 = x HQ in Eq.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only a general approach using HQ and LQ images. No verifiable resource names are provided.",
      "processing_time": 62.07736539840698,
      "citing_paper_id": "279464818",
      "cited_paper_id": 272724853
    },
    {
      "context_text": "Our diffusion formulation in degradation space has several advantages over previous diffusion in image space [4, 43, 59, 131]: 1) it achieves better restoration because y t is highly correlated with degradations; 2) training is more stable since the distribution of y t is more consistent than…",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only discusses methodological advantages of a diffusion formulation in degradation space.",
      "processing_time": 61.2839629650116,
      "citing_paper_id": "279464818",
      "cited_paper_id": 261276317
    },
    {
      "context_text": "Our diffusion formulation in degradation space has several advantages over previous diffusion in image space [4, 43, 59, 131]: 1) it achieves better restoration because y t is highly correlated with degradations; 2) training is more stable since the distribution of y t is more consistent than…",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only discusses methodological advantages of a diffusion formulation in degradation space.",
      "processing_time": 61.2839629650116,
      "citing_paper_id": "279464818",
      "cited_paper_id": 267199774
    },
    {
      "context_text": "Our diffusion formulation in degradation space has several advantages over previous diffusion in image space [4, 43, 59, 131]: 1) it achieves better restoration because y t is highly correlated with degradations; 2) training is more stable since the distribution of y t is more consistent than…",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only discusses methodological advantages of a diffusion formulation in degradation space.",
      "processing_time": 61.2839629650116,
      "citing_paper_id": "279464818",
      "cited_paper_id": 272724853
    },
    {
      "context_text": "While the high-level semantics can be aligned by visual-language pretraining [5, 61, 93], language cannot accurately convey visual details [79, 103] such as how the degradations rearrange and combine pixels.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only general concepts about visual-language pretraining and the limitations of language in conveying visual details.",
      "processing_time": 62.05151867866516,
      "citing_paper_id": "279464818",
      "cited_paper_id": 263671990
    },
    {
      "context_text": "While the high-level semantics can be aligned by visual-language pretraining [5, 61, 93], language cannot accurately convey visual details [79, 103] such as how the degradations rearrange and combine pixels.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only general concepts about visual-language pretraining and the limitations of language in conveying visual details.",
      "processing_time": 62.05151867866516,
      "citing_paper_id": "279464818",
      "cited_paper_id": 272593061
    },
    {
      "context_text": "SUPIR [131], InstructIR [19], and AutoDIR [43] guide image restoration with text-based instructions and utilize the concepts the pretrained language models learned to generalize to multiple degradations.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions SUPIR, InstructIR, and AutoDIR as methods for guiding image restoration with text-based instructions, but does not explicitly mention any datasets.",
      "processing_time": 62.82442331314087,
      "citing_paper_id": "279464818",
      "cited_paper_id": 267199774
    },
    {
      "context_text": "SUPIR [131], InstructIR [19], and AutoDIR [43] guide image restoration with text-based instructions and utilize the concepts the pretrained language models learned to generalize to multiple degradations.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions SUPIR, InstructIR, and AutoDIR as methods for guiding image restoration with text-based instructions, but does not explicitly mention any datasets.",
      "processing_time": 62.82442331314087,
      "citing_paper_id": "279464818",
      "cited_paper_id": 267320695
    },
    {
      "context_text": "Inspired by Instruct-Pix2Pix [12], some work [19, 43] control the restoration process with user-provided text instructions such as “make this blurry images sharper”.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (Instruct-Pix2Pix) and a general concept (user-provided text instructions).",
      "processing_time": 62.66465735435486,
      "citing_paper_id": "279464818",
      "cited_paper_id": 267320695
    },
    {
      "context_text": "Our method also outperforms learned task embeddings [4, 86], sometimes by large margins.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only references to learned task embeddings which are methods, not datasets.",
      "processing_time": 61.43692636489868,
      "citing_paper_id": "279464818",
      "cited_paper_id": 268064460
    },
    {
      "context_text": "Our method also outperforms learned task embeddings [4, 86], sometimes by large margins.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only references to learned task embeddings which are methods, not datasets.",
      "processing_time": 61.43692636489868,
      "citing_paper_id": "279464818",
      "cited_paper_id": 272724853
    },
    {
      "context_text": "…de-hazing, Rain1400 [30] for deraining, GoPro [81] for motion deblurring, RainDrop [87] for raindrop removal, DPDD [2] for defocus deblurring, Snow100K [67] for desnowing, SIDD [1] for real denoising, and LIVE1 [104] for JPEG artifact removal (with quality factor q = 10 ) on the synthetic datasets.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Rain1400",
        "GoPro",
        "RainDrop",
        "DPDD",
        "Snow100K",
        "SIDD",
        "LIVE1"
      ],
      "dataset_descriptions": {
        "Rain1400": "Used for deraining, focusing on removing rain streaks from images using synthetic data.",
        "GoPro": "Used for motion deblurring, addressing blur caused by camera or object motion in synthetic images.",
        "RainDrop": "Used for raindrop removal, targeting the elimination of raindrops from images using synthetic data.",
        "DPDD": "Used for defocus deblurring, focusing on restoring sharpness in out-of-focus regions of synthetic images.",
        "Snow100K": "Used for desnowing, aiming to remove snowflakes from images using synthetic data.",
        "SIDD": "Used for real denoising, addressing noise reduction in real-world images.",
        "LIVE1": "Used for JPEG artifact removal, focusing on reducing compression artifacts with a quality factor of q = 10."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions several datasets used for various image restoration tasks, which are directly relevant to the research topic of all-in-one image restoration.",
      "processing_time": 99.6999762058258,
      "citing_paper_id": "279464818",
      "cited_paper_id": null
    },
    {
      "context_text": "…de-hazing, Rain1400 [30] for deraining, GoPro [81] for motion deblurring, RainDrop [87] for raindrop removal, DPDD [2] for defocus deblurring, Snow100K [67] for desnowing, SIDD [1] for real denoising, and LIVE1 [104] for JPEG artifact removal (with quality factor q = 10 ) on the synthetic datasets.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Rain1400",
        "GoPro",
        "RainDrop",
        "DPDD",
        "Snow100K",
        "SIDD",
        "LIVE1"
      ],
      "dataset_descriptions": {
        "Rain1400": "Used for deraining, focusing on removing rain streaks from images using synthetic data.",
        "GoPro": "Used for motion deblurring, addressing blur caused by camera or object motion in synthetic images.",
        "RainDrop": "Used for raindrop removal, targeting the elimination of raindrops from images using synthetic data.",
        "DPDD": "Used for defocus deblurring, focusing on restoring sharpness in out-of-focus regions of synthetic images.",
        "Snow100K": "Used for desnowing, aiming to remove snowflakes from images using synthetic data.",
        "SIDD": "Used for real denoising, addressing noise reduction in real-world images.",
        "LIVE1": "Used for JPEG artifact removal, focusing on reducing compression artifacts with a quality factor of q = 10."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions several datasets used for various image restoration tasks, which are directly relevant to the research topic of all-in-one image restoration.",
      "processing_time": 99.6999762058258,
      "citing_paper_id": "279464818",
      "cited_paper_id": null
    },
    {
      "context_text": "For setting (II), we train and evaluate our method on five degradation-specific datasets: the merged dataset [19, 64] for deraining, LOL [65] for low-light enhancement, Snow100K [35] for desnowing, RESIDE [25] for de-hazing, and GoPro [46] for deblurring.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "merged dataset",
        "LOL",
        "Snow100K",
        "RESIDE",
        "GoPro"
      ],
      "dataset_descriptions": {
        "merged dataset": "Used for deraining, focusing on removing rain streaks from images using a combined dataset from multiple sources.",
        "LOL": "Used for low-light enhancement, focusing on improving visibility in low-light conditions through image brightness and contrast adjustments.",
        "Snow100K": "Used for desnowing, focusing on removing snowflakes from images to restore clear visibility.",
        "RESIDE": "Used for de-hazing, focusing on reducing atmospheric haze to improve image clarity and color fidelity.",
        "GoPro": "Used for deblurring, focusing on restoring sharpness to images affected by motion blur."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions five specific datasets used for training and evaluating methods for image restoration under various degradations.",
      "processing_time": 87.83836030960083,
      "citing_paper_id": "277781355",
      "cited_paper_id": 837707
    },
    {
      "context_text": "For setting (II), we train and evaluate our method on five degradation-specific datasets: the merged dataset [19, 64] for deraining, LOL [65] for low-light enhancement, Snow100K [35] for desnowing, RESIDE [25] for de-hazing, and GoPro [46] for deblurring.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "merged dataset",
        "LOL",
        "Snow100K",
        "RESIDE",
        "GoPro"
      ],
      "dataset_descriptions": {
        "merged dataset": "Used for deraining, focusing on removing rain streaks from images using a combined dataset from multiple sources.",
        "LOL": "Used for low-light enhancement, focusing on improving visibility in low-light conditions through image brightness and contrast adjustments.",
        "Snow100K": "Used for desnowing, focusing on removing snowflakes from images to restore clear visibility.",
        "RESIDE": "Used for de-hazing, focusing on reducing atmospheric haze to improve image clarity and color fidelity.",
        "GoPro": "Used for deblurring, focusing on restoring sharpness to images affected by motion blur."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions five specific datasets used for training and evaluating methods for image restoration under various degradations.",
      "processing_time": 87.83836030960083,
      "citing_paper_id": "277781355",
      "cited_paper_id": 8671030
    },
    {
      "context_text": "For setting (II), we train and evaluate our method on five degradation-specific datasets: the merged dataset [19, 64] for deraining, LOL [65] for low-light enhancement, Snow100K [35] for desnowing, RESIDE [25] for de-hazing, and GoPro [46] for deblurring.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "merged dataset",
        "LOL",
        "Snow100K",
        "RESIDE",
        "GoPro"
      ],
      "dataset_descriptions": {
        "merged dataset": "Used for deraining, focusing on removing rain streaks from images using a combined dataset from multiple sources.",
        "LOL": "Used for low-light enhancement, focusing on improving visibility in low-light conditions through image brightness and contrast adjustments.",
        "Snow100K": "Used for desnowing, focusing on removing snowflakes from images to restore clear visibility.",
        "RESIDE": "Used for de-hazing, focusing on reducing atmospheric haze to improve image clarity and color fidelity.",
        "GoPro": "Used for deblurring, focusing on restoring sharpness to images affected by motion blur."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions five specific datasets used for training and evaluating methods for image restoration under various degradations.",
      "processing_time": 87.83836030960083,
      "citing_paper_id": "277781355",
      "cited_paper_id": null
    },
    {
      "context_text": "For setting (II), we train and evaluate our method on five degradation-specific datasets: the merged dataset [19, 64] for deraining, LOL [65] for low-light enhancement, Snow100K [35] for desnowing, RESIDE [25] for de-hazing, and GoPro [46] for deblurring.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "merged dataset",
        "LOL",
        "Snow100K",
        "RESIDE",
        "GoPro"
      ],
      "dataset_descriptions": {
        "merged dataset": "Used for deraining, focusing on removing rain streaks from images using a combined dataset from multiple sources.",
        "LOL": "Used for low-light enhancement, focusing on improving visibility in low-light conditions through image brightness and contrast adjustments.",
        "Snow100K": "Used for desnowing, focusing on removing snowflakes from images to restore clear visibility.",
        "RESIDE": "Used for de-hazing, focusing on reducing atmospheric haze to improve image clarity and color fidelity.",
        "GoPro": "Used for deblurring, focusing on restoring sharpness to images affected by motion blur."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions five specific datasets used for training and evaluating methods for image restoration under various degradations.",
      "processing_time": 87.83836030960083,
      "citing_paper_id": "277781355",
      "cited_paper_id": null
    },
    {
      "context_text": "For evaluation metrics, we employ PSNR and SSIM for distortion measurement, and LPIPS [76] for perceptual quality assessment.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions PSNR, SSIM, and LPIPS, but these are metrics, not datasets. No specific datasets are mentioned in the context.",
      "processing_time": 62.37371110916138,
      "citing_paper_id": "277781355",
      "cited_paper_id": 4766599
    },
    {
      "context_text": "To further evaluate generalization capability, we evaluate HOGformer on real-world datasets: Practical [67] for deraining, MEF [42], NPE [59], and DICM [23] for low-light enhancement, HIDE [53] and RealBlur-J/R [52] for deblurring, and T-OLED [80] for under-display camera restoration.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Practical",
        "MEF",
        "NPE",
        "DICM",
        "HIDE",
        "RealBlur-J",
        "RealBlur-R",
        "T-OLED"
      ],
      "dataset_descriptions": {
        "Practical": "Used for deraining evaluation, assessing the model's ability to remove rain streaks from real-world images.",
        "MEF": "Used for low-light enhancement, evaluating the model's performance in fusing multi-exposure images.",
        "NPE": "Used for low-light enhancement, testing the model's effectiveness in improving image quality in low-light conditions.",
        "DICM": "Used for low-light enhancement, assessing the model's capability to enhance images captured in dark environments.",
        "HIDE": "Used for deblurring, evaluating the model's ability to restore sharpness in motion-blurred images.",
        "RealBlur-J": "Used for deblurring, assessing the model's performance on real-world motion-blurred images.",
        "RealBlur-R": "Used for deblurring, testing the model's effectiveness in restoring sharpness in real-world motion-blurred images.",
        "T-OLED": "Used for under-display camera restoration, evaluating the model's ability to improve image quality from under-display cameras."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions several datasets used for evaluating the HOGformer model on various image restoration tasks. These datasets are specific and relevant to the research topic.",
      "processing_time": 106.9018726348877,
      "citing_paper_id": "277781355",
      "cited_paper_id": 4828378
    },
    {
      "context_text": "To further evaluate generalization capability, we evaluate HOGformer on real-world datasets: Practical [67] for deraining, MEF [42], NPE [59], and DICM [23] for low-light enhancement, HIDE [53] and RealBlur-J/R [52] for deblurring, and T-OLED [80] for under-display camera restoration.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Practical",
        "MEF",
        "NPE",
        "DICM",
        "HIDE",
        "RealBlur-J",
        "RealBlur-R",
        "T-OLED"
      ],
      "dataset_descriptions": {
        "Practical": "Used for deraining evaluation, assessing the model's ability to remove rain streaks from real-world images.",
        "MEF": "Used for low-light enhancement, evaluating the model's performance in fusing multi-exposure images.",
        "NPE": "Used for low-light enhancement, testing the model's effectiveness in improving image quality in low-light conditions.",
        "DICM": "Used for low-light enhancement, assessing the model's capability to enhance images captured in dark environments.",
        "HIDE": "Used for deblurring, evaluating the model's ability to restore sharpness in motion-blurred images.",
        "RealBlur-J": "Used for deblurring, assessing the model's performance on real-world motion-blurred images.",
        "RealBlur-R": "Used for deblurring, testing the model's effectiveness in restoring sharpness in real-world motion-blurred images.",
        "T-OLED": "Used for under-display camera restoration, evaluating the model's ability to improve image quality from under-display cameras."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions several datasets used for evaluating the HOGformer model on various image restoration tasks. These datasets are specific and relevant to the research topic.",
      "processing_time": 106.9018726348877,
      "citing_paper_id": "277781355",
      "cited_paper_id": 15443600
    },
    {
      "context_text": "To further evaluate generalization capability, we evaluate HOGformer on real-world datasets: Practical [67] for deraining, MEF [42], NPE [59], and DICM [23] for low-light enhancement, HIDE [53] and RealBlur-J/R [52] for deblurring, and T-OLED [80] for under-display camera restoration.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Practical",
        "MEF",
        "NPE",
        "DICM",
        "HIDE",
        "RealBlur-J",
        "RealBlur-R",
        "T-OLED"
      ],
      "dataset_descriptions": {
        "Practical": "Used for deraining evaluation, assessing the model's ability to remove rain streaks from real-world images.",
        "MEF": "Used for low-light enhancement, evaluating the model's performance in fusing multi-exposure images.",
        "NPE": "Used for low-light enhancement, testing the model's effectiveness in improving image quality in low-light conditions.",
        "DICM": "Used for low-light enhancement, assessing the model's capability to enhance images captured in dark environments.",
        "HIDE": "Used for deblurring, evaluating the model's ability to restore sharpness in motion-blurred images.",
        "RealBlur-J": "Used for deblurring, assessing the model's performance on real-world motion-blurred images.",
        "RealBlur-R": "Used for deblurring, testing the model's effectiveness in restoring sharpness in real-world motion-blurred images.",
        "T-OLED": "Used for under-display camera restoration, evaluating the model's ability to improve image quality from under-display cameras."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions several datasets used for evaluating the HOGformer model on various image restoration tasks. These datasets are specific and relevant to the research topic.",
      "processing_time": 106.9018726348877,
      "citing_paper_id": "277781355",
      "cited_paper_id": 201624746
    },
    {
      "context_text": "In this section, we first conduct an ablation study to evaluate the contribution of the proposed core components on Outdoor-Rain [26].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Outdoor-Rain"
      ],
      "dataset_descriptions": {
        "Outdoor-Rain": "Used to evaluate the proposed method for heavy rain image restoration, focusing on the effectiveness of integrating physics models and conditional adversarial learning."
      },
      "confidence_score": 0.9,
      "reasoning": "The context mentions 'Outdoor-Rain' which appears to be a specific dataset used for evaluating the proposed method in the context of heavy rain image restoration.",
      "processing_time": 69.88994717597961,
      "citing_paper_id": "277781355",
      "cited_paper_id": 131773964
    },
    {
      "context_text": "In this work, we observe that the Histogram of Oriented Gra-dients (HOG) feature descriptor [12] inherently characterizes and distinguishes different degradations.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a feature descriptor (HOG). No verifiable resources are identified.",
      "processing_time": 61.41642737388611,
      "citing_paper_id": "277781355",
      "cited_paper_id": 206590483
    },
    {
      "context_text": "𝑏𝑖𝑛 is set to 9 according to original HOG setting [12].",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific dataset, only a method (HOG) and its setting. No dataset names are present in the citation span.",
      "processing_time": 62.67338418960571,
      "citing_paper_id": "277781355",
      "cited_paper_id": 206590483
    },
    {
      "context_text": "…depth-wise convolution with dilation rate 2, ChannelShuffle represents the channel shuffling operation with 2 groups, Mish denotes the Mish activation [43], GELU is the Gaussian Error Linear Unit activation, 𝛾 𝑐 represents the learnable element-wise scaling operation, and 𝐹 𝑙 + 1 is the…",
      "catation_intent": "none",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various operations and functions used in a neural network architecture.",
      "processing_time": 61.24424624443054,
      "citing_paper_id": "277781355",
      "cited_paper_id": 221113156
    },
    {
      "context_text": "For a fair comparison, we incorporate leading degradation removal methods (e.g., MPRNet [71], MAXIM [56], and Restormer [70]) as baselines.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions methods/models (MPRNet, MAXIM, Restormer) but does not refer to any specific datasets. The context is about comparing methods, not using datasets.",
      "processing_time": 64.26698970794678,
      "citing_paper_id": "277781355",
      "cited_paper_id": 231802205
    },
    {
      "context_text": "For a fair comparison, we incorporate leading degradation removal methods (e.g., MPRNet [71], MAXIM [56], and Restormer [70]) as baselines.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions methods/models (MPRNet, MAXIM, Restormer) but does not refer to any specific datasets. The context is about comparing methods, not using datasets.",
      "processing_time": 64.26698970794678,
      "citing_paper_id": "277781355",
      "cited_paper_id": 245837508
    },
    {
      "context_text": "The introduction of the Vision Transformer has notably enhanced performance by modeling long-range dependencies [31, 31, 63, 70].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only references to models or methods. The cited papers' titles do not provide additional context to identify datasets.",
      "processing_time": 63.47629904747009,
      "citing_paper_id": "277781355",
      "cited_paper_id": 237266491
    },
    {
      "context_text": "The introduction of the Vision Transformer has notably enhanced performance by modeling long-range dependencies [31, 31, 63, 70].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only references to models or methods. The cited papers' titles do not provide additional context to identify datasets.",
      "processing_time": 63.47629904747009,
      "citing_paper_id": "277781355",
      "cited_paper_id": 261242635
    },
    {
      "context_text": "For example, Swin-IR [31] employs the Swin Transformer [36] for image restoration, using its shifted window mechanism to enhance local and global contextual understanding.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions Swin-IR and Swin Transformer but does not refer to any specific dataset. The focus is on the method and its application.",
      "processing_time": 63.21191453933716,
      "citing_paper_id": "277781355",
      "cited_paper_id": 237266491
    },
    {
      "context_text": "The computer vision community has witnessed growing interest in all-in-one image restoration [2, 21, 24, 48, 55, 66, 69].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only a general trend in the field. No verifiable resources are identified.",
      "processing_time": 62.4939284324646,
      "citing_paper_id": "277781355",
      "cited_paper_id": 250551851
    },
    {
      "context_text": "The computer vision community has witnessed growing interest in all-in-one image restoration [2, 21, 24, 48, 55, 66, 69].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only a general trend in the field. No verifiable resources are identified.",
      "processing_time": 62.4939284324646,
      "citing_paper_id": "277781355",
      "cited_paper_id": 264145824
    },
    {
      "context_text": "The computer vision community has witnessed growing interest in all-in-one image restoration [2, 21, 24, 48, 55, 66, 69].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only a general trend in the field. No verifiable resources are identified.",
      "processing_time": 62.4939284324646,
      "citing_paper_id": "277781355",
      "cited_paper_id": null
    },
    {
      "context_text": "Recent advances [2, 21, 24, 48] improve upon these limitations through degradation-related conditioning mechanisms, where degradation features are extracted and injected into a shared network backbone to enable dynamic adaptation.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general advancements in image restoration techniques.",
      "processing_time": 61.546868085861206,
      "citing_paper_id": "277781355",
      "cited_paper_id": 250551851
    },
    {
      "context_text": "Recent advances [2, 21, 24, 48] improve upon these limitations through degradation-related conditioning mechanisms, where degradation features are extracted and injected into a shared network backbone to enable dynamic adaptation.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general advancements in image restoration techniques.",
      "processing_time": 61.546868085861206,
      "citing_paper_id": "277781355",
      "cited_paper_id": 264145824
    },
    {
      "context_text": "Recent advances [2, 21, 24, 48] improve upon these limitations through degradation-related conditioning mechanisms, where degradation features are extracted and injected into a shared network backbone to enable dynamic adaptation.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general advancements in image restoration techniques.",
      "processing_time": 61.546868085861206,
      "citing_paper_id": "277781355",
      "cited_paper_id": null
    },
    {
      "context_text": "All-in-one image restoration [18, 24, 55, 79] uses a single model to address various degradation tasks without task-specific retraining, which significantly improves efficiency and usage.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only the concept of all-in-one image restoration. No verifiable resources are identified.",
      "processing_time": 62.69171714782715,
      "citing_paper_id": "277781355",
      "cited_paper_id": 250551851
    },
    {
      "context_text": "The fused features then undergo channel aggregation [28] to preserve crucial global information.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for channel aggregation.",
      "processing_time": 60.96782875061035,
      "citing_paper_id": "277781355",
      "cited_paper_id": 253384197
    },
    {
      "context_text": "Transformer-based methods have recently been developed for tasks such as deraining [10], desnowing [7], dehazing [54, 78], deblurring [22, 32], and low-light enhancement [4, 77].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span mentions various image restoration tasks but does not specify any datasets. The cited papers' titles do not provide additional dataset information.",
      "processing_time": 62.79338002204895,
      "citing_paper_id": "277781355",
      "cited_paper_id": 253761139
    },
    {
      "context_text": "Transformer-based methods have recently been developed for tasks such as deraining [10], desnowing [7], dehazing [54, 78], deblurring [22, 32], and low-light enhancement [4, 77].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span mentions various image restoration tasks but does not specify any datasets. The cited papers' titles do not provide additional dataset information.",
      "processing_time": 62.79338002204895,
      "citing_paper_id": "277781355",
      "cited_paper_id": 257767233
    },
    {
      "context_text": "Transformer-based methods have recently been developed for tasks such as deraining [10], desnowing [7], dehazing [54, 78], deblurring [22, 32], and low-light enhancement [4, 77].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span mentions various image restoration tasks but does not specify any datasets. The cited papers' titles do not provide additional dataset information.",
      "processing_time": 62.79338002204895,
      "citing_paper_id": "277781355",
      "cited_paper_id": null
    },
    {
      "context_text": "Transformer-based methods have recently been developed for tasks such as deraining [10], desnowing [7], dehazing [54, 78], deblurring [22, 32], and low-light enhancement [4, 77].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span mentions various image restoration tasks but does not specify any datasets. The cited papers' titles do not provide additional dataset information.",
      "processing_time": 62.79338002204895,
      "citing_paper_id": "277781355",
      "cited_paper_id": null
    },
    {
      "context_text": "Transformer-based methods have recently been developed for tasks such as deraining [10], desnowing [7], dehazing [54, 78], deblurring [22, 32], and low-light enhancement [4, 77].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span mentions various image restoration tasks but does not specify any datasets. The cited papers' titles do not provide additional dataset information.",
      "processing_time": 62.79338002204895,
      "citing_paper_id": "277781355",
      "cited_paper_id": null
    },
    {
      "context_text": "This capability is particularly crucial for real-world applications like autonomous driving [6], where input conditions are highly dynamic and unpredictable.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, models, or methods. It only refers to the challenges and frontiers of autonomous driving.",
      "processing_time": 62.962854623794556,
      "citing_paper_id": "277781355",
      "cited_paper_id": 259287283
    },
    {
      "context_text": "In recent years, with the development of deep learning, many methods [15, 16, 50, 58, 70] have been proposed and achieved outstanding performance in image restoration tasks.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to methods and their performance in image restoration tasks.",
      "processing_time": 62.066171169281006,
      "citing_paper_id": "277781355",
      "cited_paper_id": 261242635
    },
    {
      "context_text": "While recent deep learning advances have significantly improved image restoration for specific degradations [15, 16, 50, 58, 70], these approaches typically require separate models for each degradation, increasing computational costs and deployment complexity.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only discusses the limitations of existing deep learning approaches for image restoration.",
      "processing_time": 62.221752882003784,
      "citing_paper_id": "277781355",
      "cited_paper_id": 261242635
    },
    {
      "context_text": "Existing Vision Transformers [7, 50, 57, 70] typically adopt fixed spatial attention windows or restrict attention to channel-wise operations to reduce computational and memory cost.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and models. The context is about the architectural design of Vision Transformers and their computational efficiency.",
      "processing_time": 63.348581314086914,
      "citing_paper_id": "277781355",
      "cited_paper_id": 261242635
    },
    {
      "context_text": "In benchmark tests without reference images, we utilize NIQE [44], LOE [59], and IL-NIQE [75] as no-reference metrics.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions no-reference metrics but does not refer to any specific datasets. Metrics like NIQE, LOE, and IL-NIQE are excluded as they are not datasets.",
      "processing_time": 64.2141981124878,
      "citing_paper_id": "277781355",
      "cited_paper_id": 262546756
    },
    {
      "context_text": "In-structIR [11] specifies restoration targets through natural language instructions, but requires high data preparation costs.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a method (In-structIR) and a limitation regarding data preparation costs.",
      "processing_time": 62.976927757263184,
      "citing_paper_id": "277781355",
      "cited_paper_id": 267320695
    },
    {
      "context_text": "In past studies, image restoration approaches predominantly focused on individual degradation type in a closed and controlled scenarios, and required different models or weights for different cases (denoising [1], dehazing [2], low-light [3], etc.).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only general categories of image restoration tasks such as denoising, dehazing, and low-light enhancement. No verifiable resources are named.",
      "processing_time": 65.34389281272888,
      "citing_paper_id": "278959539",
      "cited_paper_id": null
    },
    {
      "context_text": "In past studies, image restoration approaches predominantly focused on individual degradation type in a closed and controlled scenarios, and required different models or weights for different cases (denoising [1], dehazing [2], low-light [3], etc.).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only general categories of image restoration tasks such as denoising, dehazing, and low-light enhancement. No verifiable resources are named.",
      "processing_time": 65.34389281272888,
      "citing_paper_id": "278959539",
      "cited_paper_id": 250126736
    },
    {
      "context_text": "In past studies, image restoration approaches predominantly focused on individual degradation type in a closed and controlled scenarios, and required different models or weights for different cases (denoising [1], dehazing [2], low-light [3], etc.).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only general categories of image restoration tasks such as denoising, dehazing, and low-light enhancement. No verifiable resources are named.",
      "processing_time": 65.34389281272888,
      "citing_paper_id": "278959539",
      "cited_paper_id": 252046269
    },
    {
      "context_text": "In addition, the BRISQUE [47] is also frequently used to evaluate perceptual distortions perceptible to the human visual sys-tem.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "BRISQUE is a method for no-reference image quality assessment, not a dataset. The context does not mention any specific dataset.",
      "processing_time": 62.701667070388794,
      "citing_paper_id": "278959539",
      "cited_paper_id": 2927709
    },
    {
      "context_text": "Target domain datasets: Polyu [38] (Denoising), URHI [34] (Dehazing), LHP-Rain [39] (Deraining), LIME [40] (LLIE), UFO-120 [41] (UIE).",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Polyu",
        "URHI",
        "LHP-Rain",
        "LIME",
        "UFO-120"
      ],
      "dataset_descriptions": {
        "Polyu": "Used for denoising real-world noisy images, providing a benchmark for evaluating denoising algorithms.",
        "URHI": "Used for dehazing single images, serving as a benchmark for dehazing performance evaluation.",
        "LHP-Rain": "Used for rain removal from images, offering a dataset for training and testing deraining algorithms.",
        "LIME": "Used for low-light image enhancement, providing a benchmark for evaluating low-light image enhancement methods.",
        "UFO-120": "Used for underwater image enhancement, offering a large-scale benchmark for improving visual perception in underwater imagery."
      },
      "confidence_score": 1.0,
      "reasoning": "The context lists specific datasets used for various image restoration tasks, which are directly relevant to the research topic of all-in-one image restoration.",
      "processing_time": 90.8565022945404,
      "citing_paper_id": "278959539",
      "cited_paper_id": 4715123
    },
    {
      "context_text": "Target domain datasets: Polyu [38] (Denoising), URHI [34] (Dehazing), LHP-Rain [39] (Deraining), LIME [40] (LLIE), UFO-120 [41] (UIE).",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Polyu",
        "URHI",
        "LHP-Rain",
        "LIME",
        "UFO-120"
      ],
      "dataset_descriptions": {
        "Polyu": "Used for denoising real-world noisy images, providing a benchmark for evaluating denoising algorithms.",
        "URHI": "Used for dehazing single images, serving as a benchmark for dehazing performance evaluation.",
        "LHP-Rain": "Used for rain removal from images, offering a dataset for training and testing deraining algorithms.",
        "LIME": "Used for low-light image enhancement, providing a benchmark for evaluating low-light image enhancement methods.",
        "UFO-120": "Used for underwater image enhancement, offering a large-scale benchmark for improving visual perception in underwater imagery."
      },
      "confidence_score": 1.0,
      "reasoning": "The context lists specific datasets used for various image restoration tasks, which are directly relevant to the research topic of all-in-one image restoration.",
      "processing_time": 90.8565022945404,
      "citing_paper_id": "278959539",
      "cited_paper_id": 39760169
    },
    {
      "context_text": "Target domain datasets: Polyu [38] (Denoising), URHI [34] (Dehazing), LHP-Rain [39] (Deraining), LIME [40] (LLIE), UFO-120 [41] (UIE).",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Polyu",
        "URHI",
        "LHP-Rain",
        "LIME",
        "UFO-120"
      ],
      "dataset_descriptions": {
        "Polyu": "Used for denoising real-world noisy images, providing a benchmark for evaluating denoising algorithms.",
        "URHI": "Used for dehazing single images, serving as a benchmark for dehazing performance evaluation.",
        "LHP-Rain": "Used for rain removal from images, offering a dataset for training and testing deraining algorithms.",
        "LIME": "Used for low-light image enhancement, providing a benchmark for evaluating low-light image enhancement methods.",
        "UFO-120": "Used for underwater image enhancement, offering a large-scale benchmark for improving visual perception in underwater imagery."
      },
      "confidence_score": 1.0,
      "reasoning": "The context lists specific datasets used for various image restoration tasks, which are directly relevant to the research topic of all-in-one image restoration.",
      "processing_time": 90.8565022945404,
      "citing_paper_id": "278959539",
      "cited_paper_id": 211020567
    },
    {
      "context_text": "Target domain datasets: Polyu [38] (Denoising), URHI [34] (Dehazing), LHP-Rain [39] (Deraining), LIME [40] (LLIE), UFO-120 [41] (UIE).",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Polyu",
        "URHI",
        "LHP-Rain",
        "LIME",
        "UFO-120"
      ],
      "dataset_descriptions": {
        "Polyu": "Used for denoising real-world noisy images, providing a benchmark for evaluating denoising algorithms.",
        "URHI": "Used for dehazing single images, serving as a benchmark for dehazing performance evaluation.",
        "LHP-Rain": "Used for rain removal from images, offering a dataset for training and testing deraining algorithms.",
        "LIME": "Used for low-light image enhancement, providing a benchmark for evaluating low-light image enhancement methods.",
        "UFO-120": "Used for underwater image enhancement, offering a large-scale benchmark for improving visual perception in underwater imagery."
      },
      "confidence_score": 1.0,
      "reasoning": "The context lists specific datasets used for various image restoration tasks, which are directly relevant to the research topic of all-in-one image restoration.",
      "processing_time": 90.8565022945404,
      "citing_paper_id": "278959539",
      "cited_paper_id": 260704481
    },
    {
      "context_text": "…datasets (including URHI, LIME, and UFO-120), the task-related evaluation metrics are selected to evaluate the IR performance, as summarized as: • For the dehazing task, the FADE [43] is widely used to estimate haze density in an image by leveraging statistical features related to fog.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "URHI",
        "LIME",
        "UFO-120"
      ],
      "dataset_descriptions": {
        "URHI": "Used to evaluate image restoration performance, specifically for the dehazing task, leveraging statistical features related to fog.",
        "LIME": "Used to evaluate image restoration performance, specifically for the dehazing task, leveraging statistical features related to fog.",
        "UFO-120": "Used to evaluate image restoration performance, specifically for the dehazing task, leveraging statistical features related to fog."
      },
      "confidence_score": 0.9,
      "reasoning": "The context mentions specific datasets (URHI, LIME, and UFO-120) used for evaluating image restoration performance, particularly for the dehazing task.",
      "processing_time": 83.08279490470886,
      "citing_paper_id": "278959539",
      "cited_paper_id": 10225720
    },
    {
      "context_text": "• For the UIE task, UIQM [48] and UCIQE [49] are standard evaluation metrics commonly adopted to assess the overall visual quality of enhanced underwater images [50].",
      "catation_intent": "none",
      "resource_type": "metric",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions UIQM and UCIQE as evaluation metrics, not datasets. No specific datasets are mentioned or used in the context.",
      "processing_time": 63.1662175655365,
      "citing_paper_id": "278959539",
      "cited_paper_id": 16568282
    },
    {
      "context_text": "• For the UIE task, UIQM [48] and UCIQE [49] are standard evaluation metrics commonly adopted to assess the overall visual quality of enhanced underwater images [50].",
      "catation_intent": "none",
      "resource_type": "metric",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions UIQM and UCIQE as evaluation metrics, not datasets. No specific datasets are mentioned or used in the context.",
      "processing_time": 63.1662175655365,
      "citing_paper_id": "278959539",
      "cited_paper_id": 29860648
    },
    {
      "context_text": "• For the LLIE task, the NIQE [45] is a popular non-reference metric that quantifies image quality based on deviations from statistical regularities observed in natural scenes.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions NIQE as a metric, which is excluded according to the instructions. No datasets are mentioned.",
      "processing_time": 61.86847376823425,
      "citing_paper_id": "278959539",
      "cited_paper_id": 16892725
    },
    {
      "context_text": "In this context, a total of 10 open-source datasets are determined to conduct the comparison experiments, as shown below: Source domain datasets: SIDD [33] (Denoising), OTS [34] (Dehazing), RealRain-1k [35] (Deraining), LOL [36] (LLIE), UIEB [37] (UIE).",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "SIDD",
        "OTS",
        "RealRain-1k",
        "LOL",
        "UIEB"
      ],
      "dataset_descriptions": {
        "SIDD": "Used for denoising experiments, providing real-world noisy images and their ground truth clean versions.",
        "OTS": "Used for dehazing experiments, offering synthetic hazy images and corresponding clear images.",
        "RealRain-1k": "Used for deraining experiments, containing real rain images and their clean counterparts.",
        "LOL": "Used for low-light image enhancement experiments, featuring low-light images and their well-exposed versions.",
        "UIEB": "Used for underwater image enhancement experiments, providing underwater images and their enhanced versions."
      },
      "confidence_score": 1.0,
      "reasoning": "The citation mentions specific datasets used for comparison experiments in image restoration tasks, including denoising, dehazing, deraining, low-light image enhancement, and underwater image enhancement.",
      "processing_time": 89.08528113365173,
      "citing_paper_id": "278959539",
      "cited_paper_id": 39760169
    },
    {
      "context_text": "In the backbone, the Vision Transformer [31] is primarily used as the core structure for both the encoder and de-coder, with each layer consisting of L Transformer blocks.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only the use of a Vision Transformer model. No dataset names are present in the citation span.",
      "processing_time": 63.0734748840332,
      "citing_paper_id": "278959539",
      "cited_paper_id": 225039882
    },
    {
      "context_text": "The MDIR methods, such as [15, 16, 17, 18], attempt to develop a unified framework to flexibly cope with various degradation with degradation-related model weights.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods and frameworks. No dataset names are present in the text.",
      "processing_time": 62.43954038619995,
      "citing_paper_id": "278959539",
      "cited_paper_id": 237266491
    },
    {
      "context_text": "Existing work [29] typically fine-tunes the IR model depending only on the target domain data via TTA, with insufficient attention on the learned prior of degradation types in the source domain, which leads to catastrophic forgetting and further reduce so-lution performance [30].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only general concepts and methods. No clear, verifiable resource names are provided.",
      "processing_time": 62.64865207672119,
      "citing_paper_id": "278959539",
      "cited_paper_id": 247748613
    },
    {
      "context_text": "Existing work [29] typically fine-tunes the IR model depending only on the target domain data via TTA, with insufficient attention on the learned prior of degradation types in the source domain, which leads to catastrophic forgetting and further reduce so-lution performance [30].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only general concepts and methods. No clear, verifiable resource names are provided.",
      "processing_time": 62.64865207672119,
      "citing_paper_id": "278959539",
      "cited_paper_id": 265658932
    },
    {
      "context_text": "…distribution discrepancies of images between source and target domain, in this work, a correlation alignment Test-Time Adaptation (TTA) [27, 28] strategy is proposed to perform the online re-adjustment via progressive alignment based on the sample-wise features, which benefits to mitigate…",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (Test-Time Adaptation) and a general problem (domain distribution discrepancies).",
      "processing_time": 62.93142247200012,
      "citing_paper_id": "278959539",
      "cited_paper_id": 249097890
    },
    {
      "context_text": "Therefore, an All-in-One Image Restoration (AiOIR) framework has emerged as a promising solution to tackle the images with multiple degradation patterns using only a single model with unified parameters [5, 6].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only the concept of an All-in-One Image Restoration framework. No verifiable resources are identified.",
      "processing_time": 63.5041229724884,
      "citing_paper_id": "278959539",
      "cited_paper_id": 257321888
    },
    {
      "context_text": "Therefore, an All-in-One Image Restoration (AiOIR) framework has emerged as a promising solution to tackle the images with multiple degradation patterns using only a single model with unified parameters [5, 6].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only the concept of an All-in-One Image Restoration framework. No verifiable resources are identified.",
      "processing_time": 63.5041229724884,
      "citing_paper_id": "278959539",
      "cited_paper_id": 273502246
    },
    {
      "context_text": "…of different domains inevitably deteriorate the performance of existing methods with degradation prior assumptions from close scenarios datasets [5, 6], indicating the dual challenges bias in identifying degradation patterns and domain distribution shift [23], as shown below: • In recent works,…",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not provide specific dataset names, only mentions 'close scenarios datasets' without naming them. The discussion is about the challenges and biases in using such datasets.",
      "processing_time": 63.9630868434906,
      "citing_paper_id": "278959539",
      "cited_paper_id": 257321888
    },
    {
      "context_text": "…of different domains inevitably deteriorate the performance of existing methods with degradation prior assumptions from close scenarios datasets [5, 6], indicating the dual challenges bias in identifying degradation patterns and domain distribution shift [23], as shown below: • In recent works,…",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not provide specific dataset names, only mentions 'close scenarios datasets' without naming them. The discussion is about the challenges and biases in using such datasets.",
      "processing_time": 63.9630868434906,
      "citing_paper_id": "278959539",
      "cited_paper_id": 273502246
    },
    {
      "context_text": "The AiOIR is expected to provide a general solution for degraded image restoration in complex environments, such as autonomous driving [7], unmanned aerial vehicle [8], underwater robotics [9], and other applications [10, 11].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only application areas for image restoration. The cited papers do not provide additional dataset information.",
      "processing_time": 62.7968909740448,
      "citing_paper_id": "278959539",
      "cited_paper_id": 257423144
    },
    {
      "context_text": "The AiOIR is expected to provide a general solution for degraded image restoration in complex environments, such as autonomous driving [7], unmanned aerial vehicle [8], underwater robotics [9], and other applications [10, 11].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only application areas for image restoration. The cited papers do not provide additional dataset information.",
      "processing_time": 62.7968909740448,
      "citing_paper_id": "278959539",
      "cited_paper_id": 269865211
    },
    {
      "context_text": "The AiOIR is expected to provide a general solution for degraded image restoration in complex environments, such as autonomous driving [7], unmanned aerial vehicle [8], underwater robotics [9], and other applications [10, 11].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only application areas for image restoration. The cited papers do not provide additional dataset information.",
      "processing_time": 62.7968909740448,
      "citing_paper_id": "278959539",
      "cited_paper_id": 278770160
    },
    {
      "context_text": "The FADE has been frequently adopted in dehazing studies, such as in [44], to provide a non-reference assessment of haze severity.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "FADE"
      ],
      "dataset_descriptions": {
        "FADE": "Used to provide a non-reference assessment of haze severity in dehazing studies, focusing on evaluating the effectiveness of dehazing algorithms."
      },
      "confidence_score": 0.9,
      "reasoning": "FADE is mentioned as a tool for assessing haze severity, which is relevant to image restoration, particularly in dehazing studies.",
      "processing_time": 70.18133759498596,
      "citing_paper_id": "278959539",
      "cited_paper_id": 257521024
    },
    {
      "context_text": "The SDIR methods, such as [12, 13, 14], are typically designed for specific tasks and are particularly effective at handling known degradation types.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods (SDIR methods) and their effectiveness for specific tasks and degradation types.",
      "processing_time": 62.96603536605835,
      "citing_paper_id": "278959539",
      "cited_paper_id": 258258066
    },
    {
      "context_text": "The SDIR methods, such as [12, 13, 14], are typically designed for specific tasks and are particularly effective at handling known degradation types.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods (SDIR methods) and their effectiveness for specific tasks and degradation types.",
      "processing_time": 62.96603536605835,
      "citing_paper_id": "278959539",
      "cited_paper_id": 269430992
    },
    {
      "context_text": "Recently, AiOIR methods, such as [19, 20, 21, 22], are proposed to simultaneously address multiple unknown degradation tasks using a single framework with unified parameter weights, thereby providing both generality and practicality in real-world tasks.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to methods and approaches. There are no clear identifiers for datasets within the text.",
      "processing_time": 63.24895763397217,
      "citing_paper_id": "278959539",
      "cited_paper_id": 266844752
    },
    {
      "context_text": "…from close scenarios datasets [5, 6], indicating the dual challenges bias in identifying degradation patterns and domain distribution shift [23], as shown below: • In recent works, prompt-based learning [24], contrastive learning [25], and multi-modal [26] were proposed and improved to…",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific, verifiable datasets. It discusses methods and approaches but does not reference any named datasets.",
      "processing_time": 63.24644160270691,
      "citing_paper_id": "278959539",
      "cited_paper_id": 272444309
    },
    {
      "context_text": "For denoising, we use BSD400 (Arbelaez et al. 2010) and WED (Ma et al. 2016) with synthetic Gaussian noise ( σ = 15 , 25 , 50 ) for training, and BSD68 (Martin et al. 2001) for testing.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "BSD400",
        "WED",
        "BSD68"
      ],
      "dataset_descriptions": {
        "BSD400": "Used for training denoising models with synthetic Gaussian noise (σ = 15, 25, 50) to evaluate performance under controlled conditions.",
        "WED": "Used for training denoising models with synthetic Gaussian noise (σ = 15, 25, 50) to enhance robustness and generalization.",
        "BSD68": "Used for testing denoising models to evaluate their performance on a standardized benchmark dataset."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions specific datasets used for training and testing in the context of image denoising. BSD400 and WED are used for training with synthetic Gaussian noise, while BSD68 is used for testing.",
      "processing_time": 87.614013671875,
      "citing_paper_id": "280401292",
      "cited_paper_id": 64193
    },
    {
      "context_text": "For denoising, we use BSD400 (Arbelaez et al. 2010) and WED (Ma et al. 2016) with synthetic Gaussian noise ( σ = 15 , 25 , 50 ) for training, and BSD68 (Martin et al. 2001) for testing.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "BSD400",
        "WED",
        "BSD68"
      ],
      "dataset_descriptions": {
        "BSD400": "Used for training denoising models with synthetic Gaussian noise (σ = 15, 25, 50) to evaluate performance under controlled conditions.",
        "WED": "Used for training denoising models with synthetic Gaussian noise (σ = 15, 25, 50) to enhance robustness and generalization.",
        "BSD68": "Used for testing denoising models to evaluate their performance on a standardized benchmark dataset."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions specific datasets used for training and testing in the context of image denoising. BSD400 and WED are used for training with synthetic Gaussian noise, while BSD68 is used for testing.",
      "processing_time": 87.614013671875,
      "citing_paper_id": "280401292",
      "cited_paper_id": 4840263
    },
    {
      "context_text": "For denoising, we use BSD400 (Arbelaez et al. 2010) and WED (Ma et al. 2016) with synthetic Gaussian noise ( σ = 15 , 25 , 50 ) for training, and BSD68 (Martin et al. 2001) for testing.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "BSD400",
        "WED",
        "BSD68"
      ],
      "dataset_descriptions": {
        "BSD400": "Used for training denoising models with synthetic Gaussian noise (σ = 15, 25, 50) to evaluate performance under controlled conditions.",
        "WED": "Used for training denoising models with synthetic Gaussian noise (σ = 15, 25, 50) to enhance robustness and generalization.",
        "BSD68": "Used for testing denoising models to evaluate their performance on a standardized benchmark dataset."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions specific datasets used for training and testing in the context of image denoising. BSD400 and WED are used for training with synthetic Gaussian noise, while BSD68 is used for testing.",
      "processing_time": 87.614013671875,
      "citing_paper_id": "280401292",
      "cited_paper_id": 206764694
    },
    {
      "context_text": "To better align with human perception, we employ LPIPS (Zhang et al. 2018) and DISTS (Ding et al. 2020).",
      "catation_intent": "none",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions LPIPS and DISTS, which are methods or metrics, not datasets. No specific datasets are mentioned or used in the context.",
      "processing_time": 63.39680194854736,
      "citing_paper_id": "280401292",
      "cited_paper_id": 4766599
    },
    {
      "context_text": "To better align with human perception, we employ LPIPS (Zhang et al. 2018) and DISTS (Ding et al. 2020).",
      "catation_intent": "none",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions LPIPS and DISTS, which are methods or metrics, not datasets. No specific datasets are mentioned or used in the context.",
      "processing_time": 63.39680194854736,
      "citing_paper_id": "280401292",
      "cited_paper_id": 215785896
    },
    {
      "context_text": "Rain100L (Yang et al. 2017), RESIDE-SOTS (Li et al. 2018), GoPro (Nah, Hyun Kim, and Mu Lee 2017), and LOL-v1 (Wei et al. 2018) are used for deraining, dehazing, deblurring, and low-light enhancement, respectively.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Rain100L",
        "RESIDE-SOTS",
        "GoPro",
        "LOL-v1"
      ],
      "dataset_descriptions": {
        "Rain100L": "Used for deraining, focusing on removing rain streaks from images using a deep multi-scale convolutional neural network.",
        "RESIDE-SOTS": "Used for dehazing, evaluating single-image dehazing methods and benchmarking performance on synthetic and real-world hazy images.",
        "GoPro": "Used for deblurring, specifically addressing motion blur in dynamic scenes with a deep learning approach.",
        "LOL-v1": "Used for low-light enhancement, enhancing visibility in low-light conditions through deep retinex decomposition."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions specific datasets used for various image restoration tasks, which align with the research topic of all-in-one image restoration.",
      "processing_time": 88.23932719230652,
      "citing_paper_id": "280401292",
      "cited_paper_id": 8671030
    },
    {
      "context_text": "Rain100L (Yang et al. 2017), RESIDE-SOTS (Li et al. 2018), GoPro (Nah, Hyun Kim, and Mu Lee 2017), and LOL-v1 (Wei et al. 2018) are used for deraining, dehazing, deblurring, and low-light enhancement, respectively.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Rain100L",
        "RESIDE-SOTS",
        "GoPro",
        "LOL-v1"
      ],
      "dataset_descriptions": {
        "Rain100L": "Used for deraining, focusing on removing rain streaks from images using a deep multi-scale convolutional neural network.",
        "RESIDE-SOTS": "Used for dehazing, evaluating single-image dehazing methods and benchmarking performance on synthetic and real-world hazy images.",
        "GoPro": "Used for deblurring, specifically addressing motion blur in dynamic scenes with a deep learning approach.",
        "LOL-v1": "Used for low-light enhancement, enhancing visibility in low-light conditions through deep retinex decomposition."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions specific datasets used for various image restoration tasks, which align with the research topic of all-in-one image restoration.",
      "processing_time": 88.23932719230652,
      "citing_paper_id": "280401292",
      "cited_paper_id": 15443600
    },
    {
      "context_text": "Rain100L (Yang et al. 2017), RESIDE-SOTS (Li et al. 2018), GoPro (Nah, Hyun Kim, and Mu Lee 2017), and LOL-v1 (Wei et al. 2018) are used for deraining, dehazing, deblurring, and low-light enhancement, respectively.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Rain100L",
        "RESIDE-SOTS",
        "GoPro",
        "LOL-v1"
      ],
      "dataset_descriptions": {
        "Rain100L": "Used for deraining, focusing on removing rain streaks from images using a deep multi-scale convolutional neural network.",
        "RESIDE-SOTS": "Used for dehazing, evaluating single-image dehazing methods and benchmarking performance on synthetic and real-world hazy images.",
        "GoPro": "Used for deblurring, specifically addressing motion blur in dynamic scenes with a deep learning approach.",
        "LOL-v1": "Used for low-light enhancement, enhancing visibility in low-light conditions through deep retinex decomposition."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions specific datasets used for various image restoration tasks, which align with the research topic of all-in-one image restoration.",
      "processing_time": 88.23932719230652,
      "citing_paper_id": "280401292",
      "cited_paper_id": 39760169
    },
    {
      "context_text": "Rain100L (Yang et al. 2017), RESIDE-SOTS (Li et al. 2018), GoPro (Nah, Hyun Kim, and Mu Lee 2017), and LOL-v1 (Wei et al. 2018) are used for deraining, dehazing, deblurring, and low-light enhancement, respectively.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Rain100L",
        "RESIDE-SOTS",
        "GoPro",
        "LOL-v1"
      ],
      "dataset_descriptions": {
        "Rain100L": "Used for deraining, focusing on removing rain streaks from images using a deep multi-scale convolutional neural network.",
        "RESIDE-SOTS": "Used for dehazing, evaluating single-image dehazing methods and benchmarking performance on synthetic and real-world hazy images.",
        "GoPro": "Used for deblurring, specifically addressing motion blur in dynamic scenes with a deep learning approach.",
        "LOL-v1": "Used for low-light enhancement, enhancing visibility in low-light conditions through deep retinex decomposition."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions specific datasets used for various image restoration tasks, which align with the research topic of all-in-one image restoration.",
      "processing_time": 88.23932719230652,
      "citing_paper_id": "280401292",
      "cited_paper_id": 52008443
    },
    {
      "context_text": "We often rank first or second in PSNR and SSIM, achieving state-of-the-art results on two-fold composite degradations such as L+H, L+S, and H+S.",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only performance metrics (PSNR, SSIM) and types of degradations (L+H, L+S, H+S).",
      "processing_time": 65.72847390174866,
      "citing_paper_id": "280401292",
      "cited_paper_id": 207761262
    },
    {
      "context_text": "Even under challenging three-fold degradations (L+H+R and L+H+S), our model maintains leading performance with PSNR/SSIM of 25.80/0.794 and 25.50/0.790, respectively—demonstrating its robust cross-degradation modeling capability.",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only performance metrics (PSNR/SSIM) and types of degradations. No verifiable resources are identified.",
      "processing_time": 64.3015365600586,
      "citing_paper_id": "280401292",
      "cited_paper_id": 207761262
    },
    {
      "context_text": "On average, our approach achieves 29.35 dB PSNR and 0.886 SSIM across all 11 tasks, outperforming the representative MoCE-IR baseline by 0.30 dB and 0.005.",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only performance metrics and a comparison to a baseline method.",
      "processing_time": 61.89957904815674,
      "citing_paper_id": "280401292",
      "cited_paper_id": 207761262
    },
    {
      "context_text": "For full-reference evaluation, PSNR and SSIM (Wang et al. 2004) are used to assess pixel-level and structural similarity.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions PSNR and SSIM but does not refer to them as datasets. They are metrics, which are excluded according to the instructions.",
      "processing_time": 63.46295642852783,
      "citing_paper_id": "280401292",
      "cited_paper_id": 207761262
    },
    {
      "context_text": "As shown in Table 1, although our PSNR and SSIM are slightly lower than some state-of-the-art non-diffusion methods (e.g., DF-PIR, AdaIR), our method achieves the best scores on percep-Method We further extend the evaluation to a five-task setup by including deblurring and low-light enhancement, where task conflict becomes more pronounced.",
      "catation_intent": "findings",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only performance metrics and methods. There are no clear identifiers for datasets in the text.",
      "processing_time": 63.424914836883545,
      "citing_paper_id": "280401292",
      "cited_paper_id": 207761262
    },
    {
      "context_text": "In addition, we compare our method with a representative residual self-attention (RSA) fusion strategy (Radford et al. 2021), which replaces DAFF with a standard attention block.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (RSA) for comparison. No dataset names are present in the citation span.",
      "processing_time": 63.436821937561035,
      "citing_paper_id": "280401292",
      "cited_paper_id": 231591445
    },
    {
      "context_text": "To fully leverage the MoE architecture and prevent expert collapse, we introduce an auxiliary load-balancing loss L aux (Riquelme et al. 2021), which encourages uniform expert utilization across batches.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (auxiliary load-balancing loss) used in the MoE architecture.",
      "processing_time": 63.4196400642395,
      "citing_paper_id": "280401292",
      "cited_paper_id": 235417196
    },
    {
      "context_text": "Additionally, to support reference-free evaluation in real-world scenarios, we include no-reference metrics such as NIQE (Zhang, Zhang, and Bovik 2015), MANIQA (Yang et al. 2022), MUSIQ (Ke et al. 2021), and CLIPIQA (Wang, Chan, and Loy 2023).",
      "catation_intent": "none",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions no-reference metrics but does not specify any datasets. The cited papers are methods or metrics, not datasets.",
      "processing_time": 63.129908323287964,
      "citing_paper_id": "280401292",
      "cited_paper_id": 237048383
    },
    {
      "context_text": "Additionally, to support reference-free evaluation in real-world scenarios, we include no-reference metrics such as NIQE (Zhang, Zhang, and Bovik 2015), MANIQA (Yang et al. 2022), MUSIQ (Ke et al. 2021), and CLIPIQA (Wang, Chan, and Loy 2023).",
      "catation_intent": "none",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions no-reference metrics but does not specify any datasets. The cited papers are methods or metrics, not datasets.",
      "processing_time": 63.129908323287964,
      "citing_paper_id": "280401292",
      "cited_paper_id": 251040466
    },
    {
      "context_text": "Additionally, to support reference-free evaluation in real-world scenarios, we include no-reference metrics such as NIQE (Zhang, Zhang, and Bovik 2015), MANIQA (Yang et al. 2022), MUSIQ (Ke et al. 2021), and CLIPIQA (Wang, Chan, and Loy 2023).",
      "catation_intent": "none",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions no-reference metrics but does not specify any datasets. The cited papers are methods or metrics, not datasets.",
      "processing_time": 63.129908323287964,
      "citing_paper_id": "280401292",
      "cited_paper_id": 262546756
    },
    {
      "context_text": "Additionally, to support reference-free evaluation in real-world scenarios, we include no-reference metrics such as NIQE (Zhang, Zhang, and Bovik 2015), MANIQA (Yang et al. 2022), MUSIQ (Ke et al. 2021), and CLIPIQA (Wang, Chan, and Loy 2023).",
      "catation_intent": "none",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions no-reference metrics but does not specify any datasets. The cited papers are methods or metrics, not datasets.",
      "processing_time": 63.129908323287964,
      "citing_paper_id": "280401292",
      "cited_paper_id": null
    },
    {
      "context_text": "As shown in Table 2, our method achieves the top MUSIQ score in most tasks and ranks among the best in MANIQA, confirming its robustness and adaptability under complex degradation scenarios.",
      "catation_intent": "findings",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions MUSIQ and MANIQA, but these are not datasets. MUSIQ is a method, and MANIQA is likely a benchmark or challenge. No specific datasets are mentioned.",
      "processing_time": 66.64447355270386,
      "citing_paper_id": "280401292",
      "cited_paper_id": 237048383
    },
    {
      "context_text": "As shown in Table 2, our method achieves the top MUSIQ score in most tasks and ranks among the best in MANIQA, confirming its robustness and adaptability under complex degradation scenarios.",
      "catation_intent": "findings",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions MUSIQ and MANIQA, but these are not datasets. MUSIQ is a method, and MANIQA is likely a benchmark or challenge. No specific datasets are mentioned.",
      "processing_time": 66.64447355270386,
      "citing_paper_id": "280401292",
      "cited_paper_id": null
    },
    {
      "context_text": "To ensure global semantic coherence, a shared expert branch S ( · ) with transposed self-attention (Zamir et al. 2022) is added.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method or model component.",
      "processing_time": 61.70709204673767,
      "citing_paper_id": "280401292",
      "cited_paper_id": 244346144
    },
    {
      "context_text": "In recent years, various strategies have been proposed to enhance model generalization, including task-specific architectural designs (Li et al. 2022), explicit prior modeling (Valanarasu, Yasarla, and Patel 2022), frequency-aware modulation (Cui et al. 2024), contrastive learning (Chen et al.…",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods and approaches. There are no clear identifiers for datasets within the text.",
      "processing_time": 63.57476210594177,
      "citing_paper_id": "280401292",
      "cited_paper_id": 250551851
    },
    {
      "context_text": "Recent AiOIR approaches have explored a variety of strategies to achieve generality, including task-specific architectures (Li et al. 2022), task-specific priors (Valanarasu, Yasarla, and Patel 2022), frequency-aware modulation (Cui et al. 2024), contrastive learning (Chen et al. 2022b), and…",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various strategies and methods used in All-in-One Image Restoration. No verifiable resources are identified.",
      "processing_time": 64.3928439617157,
      "citing_paper_id": "280401292",
      "cited_paper_id": 250551851
    },
    {
      "context_text": "…to enhance model generalization, including task-specific architectural designs (Li et al. 2022), explicit prior modeling (Valanarasu, Yasarla, and Patel 2022), frequency-aware modulation (Cui et al. 2024), contrastive learning (Chen et al. 2022b), and MOE-based frameworks (Zamfir et al. 2025).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods and approaches. The cited paper titles do not provide additional context to identify datasets.",
      "processing_time": 64.05575370788574,
      "citing_paper_id": "280401292",
      "cited_paper_id": 250581068
    },
    {
      "context_text": "…to enhance model generalization, including task-specific architectural designs (Li et al. 2022), explicit prior modeling (Valanarasu, Yasarla, and Patel 2022), frequency-aware modulation (Cui et al. 2024), contrastive learning (Chen et al. 2022b), and MOE-based frameworks (Zamfir et al. 2025).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods and approaches. The cited paper titles do not provide additional context to identify datasets.",
      "processing_time": 64.05575370788574,
      "citing_paper_id": "280401292",
      "cited_paper_id": 268553835
    },
    {
      "context_text": "…to enhance model generalization, including task-specific architectural designs (Li et al. 2022), explicit prior modeling (Valanarasu, Yasarla, and Patel 2022), frequency-aware modulation (Cui et al. 2024), contrastive learning (Chen et al. 2022b), and MOE-based frameworks (Zamfir et al. 2025).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods and approaches. The cited paper titles do not provide additional context to identify datasets.",
      "processing_time": 64.05575370788574,
      "citing_paper_id": "280401292",
      "cited_paper_id": 274305801
    },
    {
      "context_text": "…a variety of strategies to achieve generality, including task-specific architectures (Li et al. 2022), task-specific priors (Valanarasu, Yasarla, and Patel 2022), frequency-aware modulation (Cui et al. 2024), contrastive learning (Chen et al. 2022b), and prompt-driven guidance (Zeng et al. 2025).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various strategies and methods for achieving generality in image restoration. No dataset names are present.",
      "processing_time": 64.40879821777344,
      "citing_paper_id": "280401292",
      "cited_paper_id": 250581068
    },
    {
      "context_text": "…a variety of strategies to achieve generality, including task-specific architectures (Li et al. 2022), task-specific priors (Valanarasu, Yasarla, and Patel 2022), frequency-aware modulation (Cui et al. 2024), contrastive learning (Chen et al. 2022b), and prompt-driven guidance (Zeng et al. 2025).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various strategies and methods for achieving generality in image restoration. No dataset names are present.",
      "processing_time": 64.40879821777344,
      "citing_paper_id": "280401292",
      "cited_paper_id": 268553835
    },
    {
      "context_text": "…a variety of strategies to achieve generality, including task-specific architectures (Li et al. 2022), task-specific priors (Valanarasu, Yasarla, and Patel 2022), frequency-aware modulation (Cui et al. 2024), contrastive learning (Chen et al. 2022b), and prompt-driven guidance (Zeng et al. 2025).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various strategies and methods for achieving generality in image restoration. No dataset names are present.",
      "processing_time": 64.40879821777344,
      "citing_paper_id": "280401292",
      "cited_paper_id": 277244267
    },
    {
      "context_text": "In this work, we adopt Stable Diffusion XL (Podell et al. 2023) as our underlying latent diffusion model, leveraging its pre-trained components to initialize the encoder, decoder, and denoising UNet.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions a model (Stable Diffusion XL) but does not refer to any specific dataset. The focus is on using the model's pre-trained components.",
      "processing_time": 65.42492961883545,
      "citing_paper_id": "280401292",
      "cited_paper_id": 259341735
    },
    {
      "context_text": "These limitations arise from the high compression ratio of the VAE encoder, which discards high-frequency signals, and the progressive sampling process, which introduces texture hallucinations (Dai et al. 2023; Zhu et al. 2023).",
      "catation_intent": "research work",
      "resource_type": "limitation",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only discusses limitations of VAE encoders and progressive sampling processes.",
      "processing_time": 63.04010057449341,
      "citing_paper_id": "280401292",
      "cited_paper_id": 263151865
    },
    {
      "context_text": "Additionally, LDMs often suffer from detail loss due to the high compression of VAE encoders (Kingma, Welling et al. 2013) and the progressive nature of iterative sampling (Dai et al. 2023), resulting in blurred textures and incomplete structures.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only discusses issues with LDMs and VAE encoders.",
      "processing_time": 63.036686182022095,
      "citing_paper_id": "280401292",
      "cited_paper_id": 263151865
    },
    {
      "context_text": "Au-toDIR (Jiang et al. 2024b) pioneered text-guided diffusion by using natural language prompts for restoration.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions Au-toDIR but does not refer to it as a dataset. It is described as a method or tool for text-guided diffusion in image restoration.",
      "processing_time": 65.41574048995972,
      "citing_paper_id": "280401292",
      "cited_paper_id": 264145824
    },
    {
      "context_text": "…on global textual or visual prompts, each with intrinsic limitations: textual prompts offer only global semantic cues and lack spatial specificity (Jiang et al. 2024b), while visual prompts depend on pre-trained degradation encoders or modality-specific tokens, assuming known and spatially…",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only discusses limitations of textual and visual prompts in image restoration.",
      "processing_time": 63.030137062072754,
      "citing_paper_id": "280401292",
      "cited_paper_id": 264145824
    },
    {
      "context_text": "It enables task-conditioned modeling through visual prompts (Zeng et al. 2025; Tian et al. 2025; Cui et al. 2024), textual prompts (Yan et al. 2025), or multimodal prompts (Duan et al. 2024).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods and models. The cited paper titles do not provide additional context about datasets.",
      "processing_time": 64.19959092140198,
      "citing_paper_id": "280401292",
      "cited_paper_id": 268553835
    },
    {
      "context_text": "It enables task-conditioned modeling through visual prompts (Zeng et al. 2025; Tian et al. 2025; Cui et al. 2024), textual prompts (Yan et al. 2025), or multimodal prompts (Duan et al. 2024).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods and models. The cited paper titles do not provide additional context about datasets.",
      "processing_time": 64.19959092140198,
      "citing_paper_id": "280401292",
      "cited_paper_id": 271543778
    },
    {
      "context_text": "It enables task-conditioned modeling through visual prompts (Zeng et al. 2025; Tian et al. 2025; Cui et al. 2024), textual prompts (Yan et al. 2025), or multimodal prompts (Duan et al. 2024).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods and models. The cited paper titles do not provide additional context about datasets.",
      "processing_time": 64.19959092140198,
      "citing_paper_id": "280401292",
      "cited_paper_id": 277244267
    },
    {
      "context_text": "It enables task-conditioned modeling through visual prompts (Zeng et al. 2025; Tian et al. 2025; Cui et al. 2024), textual prompts (Yan et al. 2025), or multimodal prompts (Duan et al. 2024).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods and models. The cited paper titles do not provide additional context about datasets.",
      "processing_time": 64.19959092140198,
      "citing_paper_id": "280401292",
      "cited_paper_id": 278740510
    },
    {
      "context_text": "For compositional degradations, we additionally evaluate on CDD11 (Guo et al. 2024).",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "CDD11"
      ],
      "dataset_descriptions": {
        "CDD11": "Used to evaluate compositional degradations in image restoration, focusing on the performance of the OneRestore framework under various degradation scenarios."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions 'CDD11' as a dataset used for evaluating compositional degradations in image restoration. The dataset name is specific and plausible.",
      "processing_time": 71.86857509613037,
      "citing_paper_id": "280401292",
      "cited_paper_id": 271039782
    },
    {
      "context_text": "To better simulate real-world restoration demands, we adopt a unified setting based on the CDD11 dataset, which includes 11 degradation types spanning haze, rain, snow, low-light, and their various combinations.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "CDD11"
      ],
      "dataset_descriptions": {
        "CDD11": "Used to simulate real-world restoration demands, including 11 degradation types such as haze, rain, snow, low-light, and their combinations."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions the CDD11 dataset, which is a specific dataset used for image restoration tasks involving multiple degradation types.",
      "processing_time": 70.91078639030457,
      "citing_paper_id": "280401292",
      "cited_paper_id": 271039782
    },
    {
      "context_text": "Inspired by FLUX (Labs et al. 2025), we adopt a cascaded fusion de-sign combining double-stream and single-stream pathways (Fig.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (FLUX) and a design approach. The context is focused on the architectural design of the model rather than the use of a dataset.",
      "processing_time": 66.72407865524292,
      "citing_paper_id": "280401292",
      "cited_paper_id": 279464475
    },
    {
      "context_text": "…have explored a variety of strategies to achieve generality, including task-specific architectures (Li et al. 2022), task-specific priors (Valanarasu, Yasarla, and Patel 2022), frequency-aware modulation (Cui et al. 2024), contrastive learning (Chen et al. 2022b), and prompt-driven…",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various strategies and methods. There are no clear identifiers for datasets or other verifiable resources.",
      "processing_time": 64.54591941833496,
      "citing_paper_id": "280401292",
      "cited_paper_id": null
    },
    {
      "context_text": "…have been proposed to enhance model generalization, including task-specific architectural designs (Li et al. 2022), explicit prior modeling (Valanarasu, Yasarla, and Patel 2022), frequency-aware modulation (Cui et al. 2024), contrastive learning (Chen et al. 2022b), and MOE-based…",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods and approaches. There are no clear identifiers for datasets.",
      "processing_time": 63.35611915588379,
      "citing_paper_id": "280401292",
      "cited_paper_id": null
    },
    {
      "context_text": "DiffUIR (Zheng et al. 2024) further introduced a selective hourglass mapping strategy that combines strong condition guidance with shared distribution modeling for unified multi-task restoration.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method or model called DiffUIR. There are no clear identifiers for datasets in the provided context.",
      "processing_time": 65.10779094696045,
      "citing_paper_id": "280401292",
      "cited_paper_id": null
    },
    {
      "context_text": "Each expert E i is built using lightweight NAFBlocks (Chen et al. 2022a) with varied receptive fields, allowing for multi-scale perception of both fine and coarse structures.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (NAFBlocks).",
      "processing_time": 62.134697914123535,
      "citing_paper_id": "280401292",
      "cited_paper_id": null
    },
    {
      "context_text": "…each with intrinsic limitations: textual prompts offer only global semantic cues and lack spatial specificity (Jiang et al. 2024b), while visual prompts depend on pre-trained degradation encoders or modality-specific tokens, assuming known and spatially uniform degradation types (Luo et al. 2023).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only discusses limitations of textual and visual prompts in image restoration.",
      "processing_time": 63.346540212631226,
      "citing_paper_id": "280401292",
      "cited_paper_id": null
    },
    {
      "context_text": "DaCLIP (Luo et al. 2023) leverages contrastive language-image pretraining to align degradation-aware prompts with visual features, enabling adaptive restoration under diverse degradation types.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a method (DaCLIP) and its application in image restoration.",
      "processing_time": 62.72453498840332,
      "citing_paper_id": "280401292",
      "cited_paper_id": null
    },
    {
      "context_text": "…this context, diffusion models, particularly Latent Diffusion Models (LDMs), have emerged as promising candidates for unified image restoration, owing to their strong generative priors and success in high-fidelity image synthesis and cross-modal tasks (Chen, Pan, and Dong 2025; Jiang et al. 2024a).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only models and methods. There are no clear identifiers for datasets within the text.",
      "processing_time": 63.851688623428345,
      "citing_paper_id": "280401292",
      "cited_paper_id": null
    },
    {
      "context_text": "Testing is performed on BSD68 [41] and Urban100 [22] datasets.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "BSD68",
        "Urban100"
      ],
      "dataset_descriptions": {
        "BSD68": "Used to test image restoration methods, focusing on natural images with human-segmented annotations.",
        "Urban100": "Used to evaluate image restoration techniques, specifically targeting high-resolution urban scenes."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions two specific datasets, BSD68 and Urban100, which are used for testing image restoration methods.",
      "processing_time": 73.51817202568054,
      "citing_paper_id": "259224666",
      "cited_paper_id": 64193
    },
    {
      "context_text": "BSD68 [41] Urban100 [22] Method σ = 15 σ = 25 σ = 50 σ = 15 σ = 25 σ = 50 CBM3D [11] 33.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "BSD68",
        "Urban100"
      ],
      "dataset_descriptions": {
        "BSD68": "Used to evaluate image denoising performance at various noise levels (σ = 15, 25, 50), focusing on color image restoration using sparse 3D collaborative filtering.",
        "Urban100": "Used to assess image restoration quality in urban scenes, evaluating denoising effectiveness at different noise levels (σ = 15, 25, 50) with a grouping constraint in luminance-chrominance space."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions BSD68 and Urban100, which are known image datasets used for image restoration tasks. The citation intent is to reference reusable resources, and the resource type is a dataset.",
      "processing_time": 86.03947687149048,
      "citing_paper_id": "259224666",
      "cited_paper_id": 1887989
    },
    {
      "context_text": "Method DIDMDN[72] UMR[66] SIRR[63] MSPFN[24] LPNet[17] AirNet[29] Restormer[68] In this section, we evaluate the performance of our PromptIR under the single-task setting, i.e., a separate model is trained for different restoration tasks.",
      "catation_intent": "reusable resource",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only various methods and models used for image restoration tasks.",
      "processing_time": 63.09740495681763,
      "citing_paper_id": "259224666",
      "cited_paper_id": 3406592
    },
    {
      "context_text": "Method DIDMDN[72] UMR[66] SIRR[63] MSPFN[24] LPNet[17] AirNet[29] Restormer[68] In this section, we evaluate the performance of our PromptIR under the single-task setting, i.e., a separate model is trained for different restoration tasks.",
      "catation_intent": "reusable resource",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only various methods and models used for image restoration tasks.",
      "processing_time": 63.09740495681763,
      "citing_paper_id": "259224666",
      "cited_paper_id": 214623217
    },
    {
      "context_text": "For image denoising in the single-task setting, we use a combined set of BSD400 [1] and WED [40] datasets for training.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "BSD400",
        "WED"
      ],
      "dataset_descriptions": {
        "BSD400": "Used for training in the single-task setting of image denoising, providing a diverse set of images to improve model robustness.",
        "WED": "Used for training in the single-task setting of image denoising, offering additional challenges and variations to enhance model performance."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions two specific datasets, BSD400 and WED, which are used for training in the image denoising task. Both names are specific and plausible.",
      "processing_time": 78.94027662277222,
      "citing_paper_id": "259224666",
      "cited_paper_id": 4840263
    },
    {
      "context_text": "The BSD400 dataset contains 400 training images and the WED dataset has 4,744 images.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "BSD400",
        "WED"
      ],
      "dataset_descriptions": {
        "BSD400": "Used for training image restoration models, providing a set of 400 high-quality images for algorithm development and evaluation.",
        "WED": "Used for evaluating image quality assessment models, containing 4,744 diverse images to test the robustness and accuracy of restoration algorithms."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions two specific datasets, BSD400 and WED, which are clearly identified and used for image restoration research.",
      "processing_time": 77.66103744506836,
      "citing_paper_id": "259224666",
      "cited_paper_id": 4840263
    },
    {
      "context_text": "Transformer-based restoration: Transformer [58] architectures have found great success across various computer vision tasks [25] such as image recognition [15, 54, 67], object detection [5, 80, 38] and semantic segmentation [64, 59, 78].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only models and tasks. The cited papers do not introduce new dataset names either.",
      "processing_time": 63.81799244880676,
      "citing_paper_id": "259224666",
      "cited_paper_id": 13756489
    },
    {
      "context_text": "Transformer-based restoration: Transformer [58] architectures have found great success across various computer vision tasks [25] such as image recognition [15, 54, 67], object detection [5, 80, 38] and semantic segmentation [64, 59, 78].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only models and tasks. The cited papers do not introduce new dataset names either.",
      "processing_time": 63.81799244880676,
      "citing_paper_id": "259224666",
      "cited_paper_id": 229924195
    },
    {
      "context_text": "Transformer-based restoration: Transformer [58] architectures have found great success across various computer vision tasks [25] such as image recognition [15, 54, 67], object detection [5, 80, 38] and semantic segmentation [64, 59, 78].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only models and tasks. The cited papers do not introduce new dataset names either.",
      "processing_time": 63.81799244880676,
      "citing_paper_id": "259224666",
      "cited_paper_id": 230435805
    },
    {
      "context_text": "In NLP [3, 48, 21, 34] and vision tasks [23, 26, 18, 50], prompting-based techniques have been explored for parameter-efficient finetuning of large frozen models trained on a source task S onto a target task T .",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general tasks and techniques. No dataset names are present in the text.",
      "processing_time": 63.160025119781494,
      "citing_paper_id": "259224666",
      "cited_paper_id": 59599816
    },
    {
      "context_text": "In NLP [3, 48, 21, 34] and vision tasks [23, 26, 18, 50], prompting-based techniques have been explored for parameter-efficient finetuning of large frozen models trained on a source task S onto a target task T .",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general tasks and techniques. No dataset names are present in the text.",
      "processing_time": 63.160025119781494,
      "citing_paper_id": "259224666",
      "cited_paper_id": 252735181
    },
    {
      "context_text": "In NLP [3, 48, 21, 34] and vision tasks [23, 26, 18, 50], prompting-based techniques have been explored for parameter-efficient finetuning of large frozen models trained on a source task S onto a target task T .",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general tasks and techniques. No dataset names are present in the text.",
      "processing_time": 63.160025119781494,
      "citing_paper_id": "259224666",
      "cited_paper_id": 252780157
    },
    {
      "context_text": "Method DehazeNet[4] MSCNN[46] AODNet[30] EPDN[44] FDGAN[14] AirNet[29] Restormer[68] Table 3: Deraining results in the single-task setting on Rain100L [16].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Rain100L"
      ],
      "dataset_descriptions": {
        "Rain100L": "Used to evaluate deraining performance in the single-task setting, focusing on image restoration quality and effectiveness of the proposed method."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions 'Rain100L' as a dataset used for deraining results. It is a specific dataset with a clear name and version tag.",
      "processing_time": 71.31339406967163,
      "citing_paper_id": "259224666",
      "cited_paper_id": 195489762
    },
    {
      "context_text": "Recently, deep learning based restoration approaches [47, 12, 68, 45, 75, 54, 43, 73] have emerged as more effective choice in comparison to conventional methods [19, 36, 13, 52, 27, 42, 28].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to deep learning and conventional methods for image restoration.",
      "processing_time": 62.96135234832764,
      "citing_paper_id": "259224666",
      "cited_paper_id": 206769988
    },
    {
      "context_text": "This kind of problem setting has been tackled in the context of image super-resolution [72, 39, 10].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only references to papers on image super-resolution.",
      "processing_time": 62.13004755973816,
      "citing_paper_id": "259224666",
      "cited_paper_id": 207998010
    },
    {
      "context_text": "We carry out this ablation experiment on Rain100L [65] for deraining task.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Rain100L"
      ],
      "dataset_descriptions": {
        "Rain100L": "Used for the deraining task in an ablation experiment, focusing on evaluating the effectiveness of the proposed method in removing rain artifacts from images."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions 'Rain100L' as a dataset used for the deraining task in an ablation experiment.",
      "processing_time": 71.05213499069214,
      "citing_paper_id": "259224666",
      "cited_paper_id": 219530930
    },
    {
      "context_text": "For single-task image deraining, we use the Rain100L [65] dataset, which consists of 200 clean-rainy image pairs for training, and 100 pairs for testing.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Rain100L"
      ],
      "dataset_descriptions": {
        "Rain100L": "Used for training and testing single-task image deraining models, consisting of 200 clean-rainy image pairs for training and 100 pairs for testing."
      },
      "confidence_score": 1.0,
      "reasoning": "The context clearly mentions the use of the Rain100L dataset for training and testing in the context of single-task image deraining.",
      "processing_time": 71.85428595542908,
      "citing_paper_id": "259224666",
      "cited_paper_id": 219530930
    },
    {
      "context_text": "Transformer-based restoration: Transformer [57] architectures have found great success across various computer vision tasks [25] such as image recognition [15, 53, 66], object detection [5, 79, 38] and semantic segmentation [63, 58, 77].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only models and tasks. No dataset names are present in the citation span.",
      "processing_time": 63.39551591873169,
      "citing_paper_id": "259224666",
      "cited_paper_id": 225039882
    },
    {
      "context_text": "Transformer-based restoration: Transformer [57] architectures have found great success across various computer vision tasks [25] such as image recognition [15, 53, 66], object detection [5, 79, 38] and semantic segmentation [63, 58, 77].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only models and tasks. No dataset names are present in the citation span.",
      "processing_time": 63.39551591873169,
      "citing_paper_id": "259224666",
      "cited_paper_id": 232352874
    },
    {
      "context_text": "Prompt learning-based techniques have also been applied in the case of multitask learning [20, 60], where choosing the right prompt for each task remains critical.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only the application of prompt learning-based techniques in multitask learning. No verifiable resources are identified.",
      "processing_time": 64.31373262405396,
      "citing_paper_id": "259224666",
      "cited_paper_id": 247218062
    },
    {
      "context_text": "Prompt learning-based techniques have also been applied in the case of multitask learning [20, 60], where choosing the right prompt for each task remains critical.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only the application of prompt learning-based techniques in multitask learning. No verifiable resources are identified.",
      "processing_time": 64.31373262405396,
      "citing_paper_id": "259224666",
      "cited_paper_id": 257365136
    },
    {
      "context_text": "Prompting is an efficient[23] and suitable[20] method for supplementing the model with relevant knowledge of the degradation type while recovering the clean image.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and approaches. There are no clear identifiers for datasets in the provided context.",
      "processing_time": 63.37978196144104,
      "citing_paper_id": "259224666",
      "cited_paper_id": 247218062
    },
    {
      "context_text": "This kind of problem setting has been tackled in the context of image super-resolution [73, 39, 10].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only references to papers tackling image super-resolution.",
      "processing_time": 61.91310358047485,
      "citing_paper_id": "259224666",
      "cited_paper_id": 247362538
    },
    {
      "context_text": "Recently, deep learning based restoration approaches [47, 12, 69, 45, 76, 55, 43, 74] have emerged as more effective choice in comparison to conventional methods [19, 36, 13, 53, 27, 42, 28].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to deep learning and conventional methods for image restoration.",
      "processing_time": 62.93100452423096,
      "citing_paper_id": "259224666",
      "cited_paper_id": 247939726
    },
    {
      "context_text": "Some works incorporate explicit task-specific knowledge in the network to deal with the corresponding restoration task, such as denoising [45, 76], deblurring [43, 74], and dehazing [47, 12, 36].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to papers that deal with various image restoration tasks. No verifiable resources are identified.",
      "processing_time": 64.11297106742859,
      "citing_paper_id": "259224666",
      "cited_paper_id": 247939726
    },
    {
      "context_text": "Multi-degradation Image Restoration: While single degradation image restoration methods [68, 47, 12, 69, 45, 76, 55, 43, 74, 51] have received significant interest, multi-degradation image restoration is relatively under-explored in the literature.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to other papers. There are no clear identifiers for datasets, benchmarks, or other verifiable resources.",
      "processing_time": 64.7750780582428,
      "citing_paper_id": "259224666",
      "cited_paper_id": 247939726
    },
    {
      "context_text": "Owing to their strong feature representation capability, they are extended to image restoration tasks [7, 58, 55, 9].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only references to papers that extend methods to image restoration tasks.",
      "processing_time": 62.734899282455444,
      "citing_paper_id": "259224666",
      "cited_paper_id": 248572065
    },
    {
      "context_text": "Prompt learning techniques can effectively model task-specific context hence they have been used for finetuning to vision tasks [23, 34, 26] and incremental learning [62, 49, 61].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and techniques. There are no clear identifiers for datasets in the provided context.",
      "processing_time": 63.76388120651245,
      "citing_paper_id": "259224666",
      "cited_paper_id": 252735181
    },
    {
      "context_text": "The research in [29] reveals that discrete wavelet transform (DWT) [83] improves the representation ability of vision Transformer.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (DWT) and its application to improve vision Transformers.",
      "processing_time": 63.17478036880493,
      "citing_paper_id": "260271048",
      "cited_paper_id": 2356353
    },
    {
      "context_text": "We collect an all-in-one image restoration dataset named AIR40K, including Rain14000 [26], Rain800 [79], Rain100H [80], Rain100L [80], Rain1200 [33], Rain12 [81], Raindrop [43], Snow100K [27], CSD [10], ITS [34], OTS [34], HSTS [34].",
      "catation_intent": [],
      "resource_type": [],
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.3,
      "reasoning": "JSON decode error: Expecting ',' delimiter: line 33 column 9 (char 2201), response: ```json\n{\n    \"reasoning\": \"The context mentions multiple datasets used for image restoration, which",
      "processing_time": 129.02841424942017,
      "citing_paper_id": "260271048",
      "cited_paper_id": 3406592
    },
    {
      "context_text": "We collect an all-in-one image restoration dataset named AIR40K, including Rain14000 [26], Rain800 [79], Rain100H [80], Rain100L [80], Rain1200 [33], Rain12 [81], Raindrop [43], Snow100K [27], CSD [10], ITS [34], OTS [34], HSTS [34].",
      "catation_intent": [],
      "resource_type": [],
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.3,
      "reasoning": "JSON decode error: Expecting ',' delimiter: line 33 column 9 (char 2201), response: ```json\n{\n    \"reasoning\": \"The context mentions multiple datasets used for image restoration, which",
      "processing_time": 129.02841424942017,
      "citing_paper_id": "260271048",
      "cited_paper_id": 11922819
    },
    {
      "context_text": "We collect an all-in-one image restoration dataset named AIR40K, including Rain14000 [26], Rain800 [79], Rain100H [80], Rain100L [80], Rain1200 [33], Rain12 [81], Raindrop [43], Snow100K [27], CSD [10], ITS [34], OTS [34], HSTS [34].",
      "catation_intent": [],
      "resource_type": [],
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.3,
      "reasoning": "JSON decode error: Expecting ',' delimiter: line 33 column 9 (char 2201), response: ```json\n{\n    \"reasoning\": \"The context mentions multiple datasets used for image restoration, which",
      "processing_time": 129.02841424942017,
      "citing_paper_id": "260271048",
      "cited_paper_id": 15443600
    },
    {
      "context_text": "We collect an all-in-one image restoration dataset named AIR40K, including Rain14000 [26], Rain800 [79], Rain100H [80], Rain100L [80], Rain1200 [33], Rain12 [81], Raindrop [43], Snow100K [27], CSD [10], ITS [34], OTS [34], HSTS [34].",
      "catation_intent": [],
      "resource_type": [],
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.3,
      "reasoning": "JSON decode error: Expecting ',' delimiter: line 33 column 9 (char 2201), response: ```json\n{\n    \"reasoning\": \"The context mentions multiple datasets used for image restoration, which",
      "processing_time": 129.02841424942017,
      "citing_paper_id": "260271048",
      "cited_paper_id": 244461440
    },
    {
      "context_text": "…to single-degradation, and achieve great successes over traditional models in single-degradation image restoration tasks, like deraining [26], [33], [37], [38], [39], [40], desnowing [10], [27], [31], [41], [42], raindrop removal [12], [43], [44], [45], [46], [47] and dehazing [48], [49],…",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span mentions various image restoration tasks but does not specify any datasets. The cited paper titles do not provide additional context to identify specific datasets.",
      "processing_time": 64.3060314655304,
      "citing_paper_id": "260271048",
      "cited_paper_id": 3406592
    },
    {
      "context_text": "For instance, DID-MDN [33] estimates rain-density and removes rain streaks using a multi-stream densely connected network.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific dataset names, only a method (DID-MDN) and its application to de-raining images.",
      "processing_time": 64.09475207328796,
      "citing_paper_id": "260271048",
      "cited_paper_id": 3406592
    },
    {
      "context_text": "1) Image Deraining: We choose the datasets of Test1 [88], Rain800 [79] and DID-Data [33] to evaluate our proposed AIRFormer.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Test1",
        "Rain800",
        "DID-Data"
      ],
      "dataset_descriptions": {
        "Test1": "Used to evaluate the proposed AIRFormer in image deraining, focusing on performance metrics and visual quality.",
        "Rain800": "Used to evaluate the proposed AIRFormer in image deraining, focusing on performance metrics and visual quality.",
        "DID-Data": "Used to evaluate the proposed AIRFormer in image deraining, focusing on performance metrics and visual quality."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions specific datasets used for evaluating the proposed method in image deraining. These datasets are clearly named and relevant to the research topic.",
      "processing_time": 81.40293073654175,
      "citing_paper_id": "260271048",
      "cited_paper_id": 3406592
    },
    {
      "context_text": "1) Image Deraining: We choose the datasets of Test1 [88], Rain800 [79] and DID-Data [33] to evaluate our proposed AIRFormer.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Test1",
        "Rain800",
        "DID-Data"
      ],
      "dataset_descriptions": {
        "Test1": "Used to evaluate the proposed AIRFormer in image deraining, focusing on performance metrics and visual quality.",
        "Rain800": "Used to evaluate the proposed AIRFormer in image deraining, focusing on performance metrics and visual quality.",
        "DID-Data": "Used to evaluate the proposed AIRFormer in image deraining, focusing on performance metrics and visual quality."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions specific datasets used for evaluating the proposed method in image deraining. These datasets are clearly named and relevant to the research topic.",
      "processing_time": 81.40293073654175,
      "citing_paper_id": "260271048",
      "cited_paper_id": 11922819
    },
    {
      "context_text": "1) Image Deraining: We choose the datasets of Test1 [88], Rain800 [79] and DID-Data [33] to evaluate our proposed AIRFormer.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Test1",
        "Rain800",
        "DID-Data"
      ],
      "dataset_descriptions": {
        "Test1": "Used to evaluate the proposed AIRFormer in image deraining, focusing on performance metrics and visual quality.",
        "Rain800": "Used to evaluate the proposed AIRFormer in image deraining, focusing on performance metrics and visual quality.",
        "DID-Data": "Used to evaluate the proposed AIRFormer in image deraining, focusing on performance metrics and visual quality."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions specific datasets used for evaluating the proposed method in image deraining. These datasets are clearly named and relevant to the research topic.",
      "processing_time": 81.40293073654175,
      "citing_paper_id": "260271048",
      "cited_paper_id": 131773964
    },
    {
      "context_text": "These weather degradations reduce image visibility and drastically affect the performance of certain computer vision algorithms, such as recognition [1], [2], [3], segmentation [4] and tracking [5], [6].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only general impacts on computer vision algorithms.",
      "processing_time": 61.996150970458984,
      "citing_paper_id": "260271048",
      "cited_paper_id": 3638670
    },
    {
      "context_text": "These weather degradations reduce image visibility and drastically affect the performance of certain computer vision algorithms, such as recognition [1], [2], [3], segmentation [4] and tracking [5], [6].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only general impacts on computer vision algorithms.",
      "processing_time": 61.996150970458984,
      "citing_paper_id": "260271048",
      "cited_paper_id": 232428140
    },
    {
      "context_text": "We choose two unsupervised metrics, naturalness image quality evaluator (NIQE) [105] and spatial-spectral entropy-based quality (SSEQ) [106], to objectively measure the performance of our AIRFormer and other methods on real-world degraded images.",
      "catation_intent": "reusable resource",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions two metrics, NIQE and SSEQ, but does not refer to any specific datasets. The metrics are used to evaluate the performance of image restoration methods on real-world degraded images.",
      "processing_time": 66.19814038276672,
      "citing_paper_id": "260271048",
      "cited_paper_id": 16892725
    },
    {
      "context_text": "We choose two unsupervised metrics, naturalness image quality evaluator (NIQE) [105] and spatial-spectral entropy-based quality (SSEQ) [106], to objectively measure the performance of our AIRFormer and other methods on real-world degraded images.",
      "catation_intent": "reusable resource",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions two metrics, NIQE and SSEQ, but does not refer to any specific datasets. The metrics are used to evaluate the performance of image restoration methods on real-world degraded images.",
      "processing_time": 66.19814038276672,
      "citing_paper_id": "260271048",
      "cited_paper_id": 27755051
    },
    {
      "context_text": "…and achieve great successes over traditional models in single-degradation image restoration tasks, like deraining [26], [33], [37], [38], [39], [40], desnowing [10], [27], [31], [41], [42], raindrop removal [12], [43], [44], [45], [46], [47] and dehazing [48], [49], [50], [51], [52], [53].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various image restoration tasks and related works. No verifiable resources are identified.",
      "processing_time": 63.58176565170288,
      "citing_paper_id": "260271048",
      "cited_paper_id": 26910155
    },
    {
      "context_text": "…and achieve great successes over traditional models in single-degradation image restoration tasks, like deraining [26], [33], [37], [38], [39], [40], desnowing [10], [27], [31], [41], [42], raindrop removal [12], [43], [44], [45], [46], [47] and dehazing [48], [49], [50], [51], [52], [53].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various image restoration tasks and related works. No verifiable resources are identified.",
      "processing_time": 63.58176565170288,
      "citing_paper_id": "260271048",
      "cited_paper_id": 199528450
    },
    {
      "context_text": "…and achieve great successes over traditional models in single-degradation image restoration tasks, like deraining [26], [33], [37], [38], [39], [40], desnowing [10], [27], [31], [41], [42], raindrop removal [12], [43], [44], [45], [46], [47] and dehazing [48], [49], [50], [51], [52], [53].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various image restoration tasks and related works. No verifiable resources are identified.",
      "processing_time": 63.58176565170288,
      "citing_paper_id": "260271048",
      "cited_paper_id": 207991474
    },
    {
      "context_text": "…and achieve great successes over traditional models in single-degradation image restoration tasks, like deraining [26], [33], [37], [38], [39], [40], desnowing [10], [27], [31], [41], [42], raindrop removal [12], [43], [44], [45], [46], [47] and dehazing [48], [49], [50], [51], [52], [53].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various image restoration tasks and related works. No verifiable resources are identified.",
      "processing_time": 63.58176565170288,
      "citing_paper_id": "260271048",
      "cited_paper_id": 216562731
    },
    {
      "context_text": "…and achieve great successes over traditional models in single-degradation image restoration tasks, like deraining [26], [33], [37], [38], [39], [40], desnowing [10], [27], [31], [41], [42], raindrop removal [12], [43], [44], [45], [46], [47] and dehazing [48], [49], [50], [51], [52], [53].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various image restoration tasks and related works. No verifiable resources are identified.",
      "processing_time": 63.58176565170288,
      "citing_paper_id": "260271048",
      "cited_paper_id": 227311248
    },
    {
      "context_text": "…and achieve great successes over traditional models in single-degradation image restoration tasks, like deraining [26], [33], [37], [38], [39], [40], desnowing [10], [27], [31], [41], [42], raindrop removal [12], [43], [44], [45], [46], [47] and dehazing [48], [49], [50], [51], [52], [53].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various image restoration tasks and related works. No verifiable resources are identified.",
      "processing_time": 63.58176565170288,
      "citing_paper_id": "260271048",
      "cited_paper_id": 232306906
    },
    {
      "context_text": "…and achieve great successes over traditional models in single-degradation image restoration tasks, like deraining [26], [33], [37], [38], [39], [40], desnowing [10], [27], [31], [41], [42], raindrop removal [12], [43], [44], [45], [46], [47] and dehazing [48], [49], [50], [51], [52], [53].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various image restoration tasks and related works. No verifiable resources are identified.",
      "processing_time": 63.58176565170288,
      "citing_paper_id": "260271048",
      "cited_paper_id": 244461440
    },
    {
      "context_text": "Among these meth-ods as shown in Figure 3 (b), [56] presents a multi-level wavelet CNN for enlarging receptive field with better trade-off between efficiency and effectiveness.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for image restoration.",
      "processing_time": 61.31531095504761,
      "citing_paper_id": "260271048",
      "cited_paper_id": 29151865
    },
    {
      "context_text": "In related research, [61] proposes a decoupled learning framework to adapt several image restoration tasks, which is similar to late [21], [62].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method or framework. There are no clear identifiers for datasets in the provided context.",
      "processing_time": 63.692553997039795,
      "citing_paper_id": "260271048",
      "cited_paper_id": 195787503
    },
    {
      "context_text": "We compare the PSNR/SSIM [87] scores of the proposed AIRFormer with the state-of-the-art task-aligned meth-ods (MPRNet [14], Restormer [17]) and all-in-one meth-ods (AirNet [21], Transweather [22]).",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only metrics (PSNR/SSIM) and methods/models (AIRFormer, MPRNet, Restormer, AirNet, Transweather).",
      "processing_time": 65.94892978668213,
      "citing_paper_id": "260271048",
      "cited_paper_id": 207761262
    },
    {
      "context_text": "…principles, the powerful representational capability of deep learning leads to task-aligned single-degradation image restoration methods [14], [16], [17], [54], [55], which can be expressed as where f θ ρ denotes the same solution with parameters θ ρ corresponding to the degraded image I ρ .",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to other papers. There are no clear identifiers for datasets, benchmarks, or other verifiable resources.",
      "processing_time": 64.72899222373962,
      "citing_paper_id": "260271048",
      "cited_paper_id": 209389923
    },
    {
      "context_text": "Previous methods either establish corresponding models for specific task [12], [26], [27], [48], or learn unified solutions but trained separately [14], [16], [17], [18], without exploring the commonality among different degradations to achieve AIR.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to previous methods and their approaches. There are no clear identifiers for datasets.",
      "processing_time": 63.673354148864746,
      "citing_paper_id": "260271048",
      "cited_paper_id": 216562731
    },
    {
      "context_text": "For example, PoolFormer [24] applies a simple operator named “ pooling ”, which outperforms the mainstream vision Transformer-based methods [64], [74], [76], [77].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models and methods. The context focuses on the performance of PoolFormer compared to other vision Transformer-based methods.",
      "processing_time": 64.72428059577942,
      "citing_paper_id": "260271048",
      "cited_paper_id": 229363322
    },
    {
      "context_text": "Transformer has been widely adopted in up-/low-stream computer vision tasks [16], [17], [30], [63], [64], [65], and shown considerable competitiveness.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only references to papers that discuss the use of Transformers in various computer vision tasks.",
      "processing_time": 63.46681189537048,
      "citing_paper_id": "260271048",
      "cited_paper_id": 229924195
    },
    {
      "context_text": "However, single-degraded image restoration plays the leading role currently, and it can be divided into two categories, 1) task-specific ones [7], [8], [9], [10], [11], [12], [13] and 2) task-aligned ones [14], [15], [16], [17], [18].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only categories of image restoration tasks. No dataset names are provided.",
      "processing_time": 62.93796515464783,
      "citing_paper_id": "260271048",
      "cited_paper_id": 232222608
    },
    {
      "context_text": "However, single-degraded image restoration plays the leading role currently, and it can be divided into two categories, 1) task-specific ones [7], [8], [9], [10], [11], [12], [13] and 2) task-aligned ones [14], [15], [16], [17], [18].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only categories of image restoration tasks. No dataset names are provided.",
      "processing_time": 62.93796515464783,
      "citing_paper_id": "260271048",
      "cited_paper_id": 234335673
    },
    {
      "context_text": "However, single-degraded image restoration plays the leading role currently, and it can be divided into two categories, 1) task-specific ones [7], [8], [9], [10], [11], [12], [13] and 2) task-aligned ones [14], [15], [16], [17], [18].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only categories of image restoration tasks. No dataset names are provided.",
      "processing_time": 62.93796515464783,
      "citing_paper_id": "260271048",
      "cited_paper_id": 235702840
    },
    {
      "context_text": "However, single-degraded image restoration plays the leading role currently, and it can be divided into two categories, 1) task-specific ones [7], [8], [9], [10], [11], [12], [13] and 2) task-aligned ones [14], [15], [16], [17], [18].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only categories of image restoration tasks. No dataset names are provided.",
      "processing_time": 62.93796515464783,
      "citing_paper_id": "260271048",
      "cited_paper_id": 235703017
    },
    {
      "context_text": "However, single-degraded image restoration plays the leading role currently, and it can be divided into two categories, 1) task-specific ones [7], [8], [9], [10], [11], [12], [13] and 2) task-aligned ones [14], [15], [16], [17], [18].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only categories of image restoration tasks. No dataset names are provided.",
      "processing_time": 62.93796515464783,
      "citing_paper_id": "260271048",
      "cited_paper_id": 244461440
    },
    {
      "context_text": "Beyond above, some other works [73], [74], [75] simply replace SA with MLP-like modules and still achieve comparable effects, which questions the dominance of attention-based architectures and inspires the work on what plays the core role in Transformer.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only discusses architectural choices in neural networks.",
      "processing_time": 62.08842706680298,
      "citing_paper_id": "260271048",
      "cited_paper_id": 233714958
    },
    {
      "context_text": "Beyond above, some other works [73], [74], [75] simply replace SA with MLP-like modules and still achieve comparable effects, which questions the dominance of attention-based architectures and inspires the work on what plays the core role in Transformer.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only discusses architectural choices in neural networks.",
      "processing_time": 62.08842706680298,
      "citing_paper_id": "260271048",
      "cited_paper_id": 235606034
    },
    {
      "context_text": "…on Transformer believe that spatial SA can effectively learn long-range dependencies and adapt to the input, therefore focus on improving the token mixers, like window partitioning [64], [66], relative position coding [67], convolution combining [68], [69], [70] and formula optimizing [71], [72].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and improvements to the Transformer architecture.",
      "processing_time": 62.08418869972229,
      "citing_paper_id": "260271048",
      "cited_paper_id": 236493453
    },
    {
      "context_text": "3) Desnowing: We evaluate our AIRFormer on the test datasets of Snow100K [27] and CSD [10].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Snow100K",
        "CSD"
      ],
      "dataset_descriptions": {
        "Snow100K": "Used to evaluate the desnowing performance of the AIRFormer model, focusing on single image desnowing using hierarchical dual-tree complex wavelet representation and contradict channel loss.",
        "CSD": "Used to evaluate the desnowing performance of the AIRFormer model, focusing on single image desnowing using hierarchical dual-tree complex wavelet representation and contradict channel loss."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions two specific datasets, Snow100K and CSD, which are used for evaluating the performance of the AIRFormer model on desnowing tasks.",
      "processing_time": 81.52704000473022,
      "citing_paper_id": "260271048",
      "cited_paper_id": 244461440
    },
    {
      "context_text": "Meanwhile, most methods [14], [17], [21], [22] primarily focus on the error in the spatial domain while disregarding the error in the frequency domain.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods and their focus on spatial domain errors.",
      "processing_time": 62.51371669769287,
      "citing_paper_id": "260271048",
      "cited_paper_id": 244714491
    },
    {
      "context_text": "Our proposed AIRFormer outperforms all other methods [17], [19], [21], [22] and achieves the best trade-off between inference time and image quality. reconstruct high-quality images that have been degraded by various weather conditions, including rain, raindrop, snow, and haze.",
      "catation_intent": "reusable resource",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only methods and performance comparisons. The cited paper title suggests a focus on weather-degraded image restoration but does not name a dataset.",
      "processing_time": 65.4649384021759,
      "citing_paper_id": "260271048",
      "cited_paper_id": 244714491
    },
    {
      "context_text": "Besides, the method named TransWeather [22] is designed for multi-degraded image restoration with only one single encoder and decoder.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method called TransWeather. The context focuses on the method's design and capabilities rather than on any particular dataset.",
      "processing_time": 64.90004396438599,
      "citing_paper_id": "260271048",
      "cited_paper_id": 244714491
    },
    {
      "context_text": "Different with [22], it constrains the attention by both a set of learnable vectors and input features.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a methodological difference with another paper.",
      "processing_time": 62.04545712471008,
      "citing_paper_id": "260271048",
      "cited_paper_id": 244714491
    },
    {
      "context_text": "According to the research in [30] and [22], we introduce special task-biased queries into the frequency refinement decoder, where the Q performs as prior knowledge related to the consistency of different weather conditions.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method or approach. The context focuses on the introduction of task-biased queries in a frequency refinement decoder, which is related to the consistency of different weather conditions.",
      "processing_time": 66.99338746070862,
      "citing_paper_id": "260271048",
      "cited_paper_id": 244714491
    },
    {
      "context_text": "However, the composition and distribution of pollution in reality are different, yet specific modeling limits the solution space and leads to inability to adapt to the diversity [22], [27], [36], [37], Authorized licensed use limited to the terms of the applicable license agreement with IEEE.",
      "catation_intent": "limitation",
      "resource_type": "limitation",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to limitations in modeling pollution. No clear, verifiable datasets are identified.",
      "processing_time": 64.04106450080872,
      "citing_paper_id": "260271048",
      "cited_paper_id": 244714491
    },
    {
      "context_text": "In contrast, AirNet [21] and TransWeather [22] adopt an end-to-end scheme to remove unwanted factors during the restoration process.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions TransWeather but does not indicate it is a dataset. It appears to be a method or model for image restoration.",
      "processing_time": 63.66715621948242,
      "citing_paper_id": "260271048",
      "cited_paper_id": 244714491
    },
    {
      "context_text": "According to [30] and [22], we construct task-biased queries separately for frequency-refined decoder.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for constructing task-biased queries. No verifiable resources are identified.",
      "processing_time": 63.688060998916626,
      "citing_paper_id": "260271048",
      "cited_paper_id": 244714491
    },
    {
      "context_text": "However, most Transformer-based methods [15], [16], [17], [22], [23] primarily focus on improving self-attention (SA) based on spatial features, without adequately considering the knowledge of frequency variation.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to Transformer-based methods. No verifiable resources are identified.",
      "processing_time": 63.21389985084534,
      "citing_paper_id": "260271048",
      "cited_paper_id": 244714491
    },
    {
      "context_text": "Figure 3 shows our more detailed classification on different methods for weather-degraded image restoration than [21] and [22].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only references to other papers for method comparisons.",
      "processing_time": 62.09987211227417,
      "citing_paper_id": "260271048",
      "cited_paper_id": 244714491
    },
    {
      "context_text": "The physical model of an image occluded by fog [34], [35] is where T is the transmission map produced by scattering effect, A is the scene atmospheric light value.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a physical model of image occlusion by fog. No dataset names are present in the citation span.",
      "processing_time": 64.36476230621338,
      "citing_paper_id": "260271048",
      "cited_paper_id": 250569514
    },
    {
      "context_text": "Baselines for MRI super-resolution include SRCNN [23], VDSR [24], SwinIR [25], and Restormer [2]; for CT denoising, CNN [3], REDCNN [4], Eformer [5], and CTformer [6]; and for PET synthesis, Xiang’s method [7], DCNN [8], ARGAN [9], and Spach Trans-former [10].",
      "catation_intent": "none",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span mentions various methods and models for MRI super-resolution, CT denoising, and PET synthesis, but does not mention any specific datasets. The cited paper titles also do not indicate the use of specific datasets.",
      "processing_time": 66.63791918754578,
      "citing_paper_id": "270123740",
      "cited_paper_id": 9971732
    },
    {
      "context_text": "Baselines for MRI super-resolution include SRCNN [23], VDSR [24], SwinIR [25], and Restormer [2]; for CT denoising, CNN [3], REDCNN [4], Eformer [5], and CTformer [6]; and for PET synthesis, Xiang’s method [7], DCNN [8], ARGAN [9], and Spach Trans-former [10].",
      "catation_intent": "none",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span mentions various methods and models for MRI super-resolution, CT denoising, and PET synthesis, but does not mention any specific datasets. The cited paper titles also do not indicate the use of specific datasets.",
      "processing_time": 66.63791918754578,
      "citing_paper_id": "270123740",
      "cited_paper_id": 21275979
    },
    {
      "context_text": "Baselines for MRI super-resolution include SRCNN [23], VDSR [24], SwinIR [25], and Restormer [2]; for CT denoising, CNN [3], REDCNN [4], Eformer [5], and CTformer [6]; and for PET synthesis, Xiang’s method [7], DCNN [8], ARGAN [9], and Spach Trans-former [10].",
      "catation_intent": "none",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span mentions various methods and models for MRI super-resolution, CT denoising, and PET synthesis, but does not mention any specific datasets. The cited paper titles also do not indicate the use of specific datasets.",
      "processing_time": 66.63791918754578,
      "citing_paper_id": "270123740",
      "cited_paper_id": 201847718
    },
    {
      "context_text": "Baselines for MRI super-resolution include SRCNN [23], VDSR [24], SwinIR [25], and Restormer [2]; for CT denoising, CNN [3], REDCNN [4], Eformer [5], and CTformer [6]; and for PET synthesis, Xiang’s method [7], DCNN [8], ARGAN [9], and Spach Trans-former [10].",
      "catation_intent": "none",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span mentions various methods and models for MRI super-resolution, CT denoising, and PET synthesis, but does not mention any specific datasets. The cited paper titles also do not indicate the use of specific datasets.",
      "processing_time": 66.63791918754578,
      "citing_paper_id": "270123740",
      "cited_paper_id": 237266491
    },
    {
      "context_text": "Baselines for MRI super-resolution include SRCNN [23], VDSR [24], SwinIR [25], and Restormer [2]; for CT denoising, CNN [3], REDCNN [4], Eformer [5], and CTformer [6]; and for PET synthesis, Xiang’s method [7], DCNN [8], ARGAN [9], and Spach Trans-former [10].",
      "catation_intent": "none",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span mentions various methods and models for MRI super-resolution, CT denoising, and PET synthesis, but does not mention any specific datasets. The cited paper titles also do not indicate the use of specific datasets.",
      "processing_time": 66.63791918754578,
      "citing_paper_id": "270123740",
      "cited_paper_id": 245371686
    },
    {
      "context_text": "Baselines for MRI super-resolution include SRCNN [23], VDSR [24], SwinIR [25], and Restormer [2]; for CT denoising, CNN [3], REDCNN [4], Eformer [5], and CTformer [6]; and for PET synthesis, Xiang’s method [7], DCNN [8], ARGAN [9], and Spach Trans-former [10].",
      "catation_intent": "none",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span mentions various methods and models for MRI super-resolution, CT denoising, and PET synthesis, but does not mention any specific datasets. The cited paper titles also do not indicate the use of specific datasets.",
      "processing_time": 66.63791918754578,
      "citing_paper_id": "270123740",
      "cited_paper_id": 247158080
    },
    {
      "context_text": "Baselines for MRI super-resolution include SRCNN [23], VDSR [24], SwinIR [25], and Restormer [2]; for CT denoising, CNN [3], REDCNN [4], Eformer [5], and CTformer [6]; and for PET synthesis, Xiang’s method [7], DCNN [8], ARGAN [9], and Spach Trans-former [10].",
      "catation_intent": "none",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span mentions various methods and models for MRI super-resolution, CT denoising, and PET synthesis, but does not mention any specific datasets. The cited paper titles also do not indicate the use of specific datasets.",
      "processing_time": 66.63791918754578,
      "citing_paper_id": "270123740",
      "cited_paper_id": 252110918
    },
    {
      "context_text": "Mixture-of-Experts (MoE) [19] provides a potential solution, which learns to dynamically route inputs to different expert networks.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a method (Mixture-of-Experts).",
      "processing_time": 62.2832818031311,
      "citing_paper_id": "270123740",
      "cited_paper_id": 12462234
    },
    {
      "context_text": "L Balance [19] is a regularization term in MoE that prevents static routing where the same few experts are always selected. γ is a weighting parameter.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a regularization term and a weighting parameter in a neural network architecture.",
      "processing_time": 62.81136751174927,
      "citing_paper_id": "270123740",
      "cited_paper_id": 12462234
    },
    {
      "context_text": "Red values indicate that task j negatively impacts task i , while green values indicate a positive impact. denoising [3,4,5,6], and PET synthesis [7,8,9,10,11,12].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions 'denoising' and 'PET synthesis', which are tasks related to image restoration. However, no specific datasets are named in the citation span.",
      "processing_time": 64.68349289894104,
      "citing_paper_id": "270123740",
      "cited_paper_id": 21275979
    },
    {
      "context_text": "Red values indicate that task j negatively impacts task i , while green values indicate a positive impact. denoising [3,4,5,6], and PET synthesis [7,8,9,10,11,12].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions 'denoising' and 'PET synthesis', which are tasks related to image restoration. However, no specific datasets are named in the citation span.",
      "processing_time": 64.68349289894104,
      "citing_paper_id": "270123740",
      "cited_paper_id": 201847718
    },
    {
      "context_text": "Red values indicate that task j negatively impacts task i , while green values indicate a positive impact. denoising [3,4,5,6], and PET synthesis [7,8,9,10,11,12].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions 'denoising' and 'PET synthesis', which are tasks related to image restoration. However, no specific datasets are named in the citation span.",
      "processing_time": 64.68349289894104,
      "citing_paper_id": "270123740",
      "cited_paper_id": 245371686
    },
    {
      "context_text": "Red values indicate that task j negatively impacts task i , while green values indicate a positive impact. denoising [3,4,5,6], and PET synthesis [7,8,9,10,11,12].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions 'denoising' and 'PET synthesis', which are tasks related to image restoration. However, no specific datasets are named in the citation span.",
      "processing_time": 64.68349289894104,
      "citing_paper_id": "270123740",
      "cited_paper_id": 247158080
    },
    {
      "context_text": "Red values indicate that task j negatively impacts task i , while green values indicate a positive impact. denoising [3,4,5,6], and PET synthesis [7,8,9,10,11,12].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions 'denoising' and 'PET synthesis', which are tasks related to image restoration. However, no specific datasets are named in the citation span.",
      "processing_time": 64.68349289894104,
      "citing_paper_id": "270123740",
      "cited_paper_id": 252110918
    },
    {
      "context_text": "Red values indicate that task j negatively impacts task i , while green values indicate a positive impact. denoising [3,4,5,6], and PET synthesis [7,8,9,10,11,12].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions 'denoising' and 'PET synthesis', which are tasks related to image restoration. However, no specific datasets are named in the citation span.",
      "processing_time": 64.68349289894104,
      "citing_paper_id": "270123740",
      "cited_paper_id": 259766392
    },
    {
      "context_text": "Within the scope of single-task MedIR, there are also studies addressing input degradation variations, such as varying noise levels [8], imaging protocols [12], and centers [12].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions 'noise levels' and 'imaging protocols' but does not specify any datasets. The cited papers do not provide additional context to identify specific datasets.",
      "processing_time": 64.86850786209106,
      "citing_paper_id": "270123740",
      "cited_paper_id": 201847718
    },
    {
      "context_text": "Within the scope of single-task MedIR, there are also studies addressing input degradation variations, such as varying noise levels [8], imaging protocols [12], and centers [12].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions 'noise levels' and 'imaging protocols' but does not specify any datasets. The cited papers do not provide additional context to identify specific datasets.",
      "processing_time": 64.86850786209106,
      "citing_paper_id": "270123740",
      "cited_paper_id": 259766392
    },
    {
      "context_text": "MedIR has achieved remarkable success in individual tasks such as MRI super-resolution [1,2], CT ) The interference metric [13] of task j on task i at the second and last blocks in Restormer [2].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models and methods. The context is focused on the performance of models in specific tasks, not on the use of datasets.",
      "processing_time": 64.9165506362915,
      "citing_paper_id": "270123740",
      "cited_paper_id": 249538647
    },
    {
      "context_text": "1 (b), we quantify the interference metric defined in the paper [13] among different MedIR tasks and observe significant task interference.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a metric and tasks. No verifiable resources are identified.",
      "processing_time": 62.38592267036438,
      "citing_paper_id": "270123740",
      "cited_paper_id": 249538647
    },
    {
      "context_text": "These significant differences between MedIR tasks can re-All-In-One sult in task interference, a common issue in multi-task learning [17,13], where the gradient update directions between tasks are inconsistent or even opposite.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only discusses issues in multi-task learning.",
      "processing_time": 61.616389989852905,
      "citing_paper_id": "270123740",
      "cited_paper_id": 249538647
    },
    {
      "context_text": "1, revealing that our proposed AMIR surpasses the best-performing baseline models — Restormer [2], Eformer [5], and Spach Trans-former [10] — in MRI super-resolution, CT denoising, and PET synthesis tasks, respectively.",
      "catation_intent": "findings",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions specific tasks (MRI super-resolution, CT denoising, and PET synthesis) but does not mention any specific datasets. The cited paper titles do not provide additional context to identify datasets.",
      "processing_time": 65.599942445755,
      "citing_paper_id": "270123740",
      "cited_paper_id": 252110918
    },
    {
      "context_text": "Additionally, we utilize two universal models for comparison: DRMC [12], initially developed for multi-center PET synthesis, and AirNet [14], originally designed for all-in-one natural image restoration.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "AirNet"
      ],
      "dataset_descriptions": {
        "AirNet": "Used for all-in-one natural image restoration, comparing performance against other models and evaluating its effectiveness in various restoration tasks."
      },
      "confidence_score": 0.8,
      "reasoning": "The context mentions 'AirNet' as a model for all-in-one natural image restoration, which aligns with the research topic. However, 'DRMC' is not relevant as it is for PET image synthesis.",
      "processing_time": 71.42778158187866,
      "citing_paper_id": "270123740",
      "cited_paper_id": 259766392
    },
    {
      "context_text": "Recently, all-in-one image restoration [14,15,16] (also known as multi-task image restoration) has gained prominence in natural images, attempting to address multiple different restoration tasks using a single universal model.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only the concept of all-in-one image restoration. No verifiable resources are identified.",
      "processing_time": 63.49579095840454,
      "citing_paper_id": "270123740",
      "cited_paper_id": 260107992
    },
    {
      "context_text": "Except for this watermark, it is identical to the accepted version; the final published version of the proceedings is available on IEEE Xplore. erence for joint image restoration and compression solutions.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only a general reference to joint image restoration and compression solutions.",
      "processing_time": 62.375560998916626,
      "citing_paper_id": "276160959",
      "cited_paper_id": 3468628
    },
    {
      "context_text": "RD performance evaluation on the RESIDE [33], CSD [11] and Rain1400 [20] dataset, where we evaluate the results with both PSNR and MS-SSIM. signed for specific restoration tasks but serve as strong base-lines for image restoration.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "RESIDE",
        "CSD",
        "Rain1400"
      ],
      "dataset_descriptions": {
        "RESIDE": "Used to evaluate rain removal performance, focusing on PSNR and MS-SSIM metrics for image restoration tasks.",
        "CSD": "Used to assess dehazing performance, providing a benchmark for image restoration techniques.",
        "Rain1400": "Used to test rain removal effectiveness, serving as a strong baseline for image restoration methods."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions three specific datasets used for evaluating rain removal and dehazing performance. These datasets are clearly named and used for benchmarking image restoration tasks.",
      "processing_time": 79.85402131080627,
      "citing_paper_id": "276160959",
      "cited_paper_id": 17115407
    },
    {
      "context_text": "RD performance evaluation on the RESIDE [33], CSD [11] and Rain1400 [20] dataset, where we evaluate the results with both PSNR and MS-SSIM. signed for specific restoration tasks but serve as strong base-lines for image restoration.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "RESIDE",
        "CSD",
        "Rain1400"
      ],
      "dataset_descriptions": {
        "RESIDE": "Used to evaluate rain removal performance, focusing on PSNR and MS-SSIM metrics for image restoration tasks.",
        "CSD": "Used to assess dehazing performance, providing a benchmark for image restoration techniques.",
        "Rain1400": "Used to test rain removal effectiveness, serving as a strong baseline for image restoration methods."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions three specific datasets used for evaluating rain removal and dehazing performance. These datasets are clearly named and used for benchmarking image restoration tasks.",
      "processing_time": 79.85402131080627,
      "citing_paper_id": "276160959",
      "cited_paper_id": 39760169
    },
    {
      "context_text": "Compared with the well-performing Restormer+EVC (the blue curves), Ours-L achieves a BD-PSNR for 0.85 dB, 2.49 dB and 0.11 dB on the RESIDE [33], CSD [11] and Rain1400 [20] datasets, respectively.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "RESIDE",
        "CSD",
        "Rain1400"
      ],
      "dataset_descriptions": {
        "RESIDE": "Used to evaluate the BD-PSNR performance of the proposed method against Restormer+EVC, focusing on dehazing and image restoration.",
        "CSD": "Used to evaluate the BD-PSNR performance of the proposed method against Restormer+EVC, focusing on dehazing and image restoration.",
        "Rain1400": "Used to evaluate the BD-PSNR performance of the proposed method against Restormer+EVC, focusing on rain removal and image restoration."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions three specific datasets used for evaluating the performance of image restoration methods. These datasets are relevant to the topic of all-in-one image restoration.",
      "processing_time": 85.17440700531006,
      "citing_paper_id": "276160959",
      "cited_paper_id": 17115407
    },
    {
      "context_text": "Compared with the well-performing Restormer+EVC (the blue curves), Ours-L achieves a BD-PSNR for 0.85 dB, 2.49 dB and 0.11 dB on the RESIDE [33], CSD [11] and Rain1400 [20] datasets, respectively.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "RESIDE",
        "CSD",
        "Rain1400"
      ],
      "dataset_descriptions": {
        "RESIDE": "Used to evaluate the BD-PSNR performance of the proposed method against Restormer+EVC, focusing on dehazing and image restoration.",
        "CSD": "Used to evaluate the BD-PSNR performance of the proposed method against Restormer+EVC, focusing on dehazing and image restoration.",
        "Rain1400": "Used to evaluate the BD-PSNR performance of the proposed method against Restormer+EVC, focusing on rain removal and image restoration."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions three specific datasets used for evaluating the performance of image restoration methods. These datasets are relevant to the topic of all-in-one image restoration.",
      "processing_time": 85.17440700531006,
      "citing_paper_id": "276160959",
      "cited_paper_id": 39760169
    },
    {
      "context_text": "All ablation studies are conducted with Ours-S on the weather degradation setting, and evaluated on the RESIDE dataset [33].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "RESIDE"
      ],
      "dataset_descriptions": {
        "RESIDE": "Used to evaluate the performance of the proposed method on weather degradation settings, focusing on single-image dehazing tasks."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions the RESIDE dataset, which is used for evaluating the performance of the proposed method on weather degradation settings.",
      "processing_time": 68.12982392311096,
      "citing_paper_id": "276160959",
      "cited_paper_id": 39760169
    },
    {
      "context_text": "Self-attention-based methods [16,30,51,68] develop various self-attention variants to capture non-local information and achieve better RD performance.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and their performance. No dataset names are present in the citation span.",
      "processing_time": 62.698617696762085,
      "citing_paper_id": "276160959",
      "cited_paper_id": 209862064
    },
    {
      "context_text": "Compared with the CNN-based architecture, the attention mechanism benefits from large training patch sizes [19, 54].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a comparison between CNN-based architectures and attention mechanisms. No verifiable resources are identified.",
      "processing_time": 63.25021314620972,
      "citing_paper_id": "276160959",
      "cited_paper_id": 229363322
    },
    {
      "context_text": "Most image restoration methods [13,34,38–40, 59, 61 , 65] are designed to handle a",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general methods for image restoration. There are no clear identifiers for datasets in the provided context.",
      "processing_time": 63.45926022529602,
      "citing_paper_id": "276160959",
      "cited_paper_id": 235358213
    },
    {
      "context_text": "Recent image compression methods [3, 4, 22, 45] have achieved tremendous improvement with auto-regressive models.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only references to image compression methods and models.",
      "processing_time": 61.27166175842285,
      "citing_paper_id": "276160959",
      "cited_paper_id": 235458657
    },
    {
      "context_text": "We also make qualitative comparisons on REVIDE [66], Snow100K [43] and SPA+ [67], which contain realistic hazy, snowy and rainy images.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "REVIDE",
        "Snow100K",
        "SPA+"
      ],
      "dataset_descriptions": {
        "REVIDE": "Used for qualitative comparisons in hazy image restoration, evaluating the effectiveness of the proposed method against real-world hazy conditions.",
        "Snow100K": "Used for qualitative comparisons in snowy image restoration, assessing the performance of the proposed method in handling realistic snow scenes.",
        "SPA+": "Used for qualitative comparisons in rainy image restoration, testing the method's ability to restore images under various rain intensities."
      },
      "confidence_score": 1.0,
      "reasoning": "The citation mentions specific datasets used for qualitative comparisons in image restoration, particularly for hazy, snowy, and rainy images.",
      "processing_time": 80.06412768363953,
      "citing_paper_id": "276160959",
      "cited_paper_id": 235719890
    },
    {
      "context_text": "Recently, deep learning-based image compression meth-ods [23, 24, 28, 36, 42] have shown remarkable compression ratio improvement, demonstrating the superiority and flexibility over traditional standards [5, 6, 53, 56].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to deep learning-based image compression methods and traditional standards. No verifiable resources are identified.",
      "processing_time": 63.547568798065186,
      "citing_paper_id": "276160959",
      "cited_paper_id": 238243504
    },
    {
      "context_text": "Recently, deep learning-based image compression meth-ods [23, 24, 28, 36, 42] have shown remarkable compression ratio improvement, demonstrating the superiority and flexibility over traditional standards [5, 6, 53, 56].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to deep learning-based image compression methods and traditional standards. No verifiable resources are identified.",
      "processing_time": 63.547568798065186,
      "citing_paper_id": "276160959",
      "cited_paper_id": 256808230
    },
    {
      "context_text": "Recently, deep learning-based image compression meth-ods [23, 24, 28, 36, 42] have shown remarkable compression ratio improvement, demonstrating the superiority and flexibility over traditional standards [5, 6, 53, 56].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to deep learning-based image compression methods and traditional standards. No verifiable resources are identified.",
      "processing_time": 63.547568798065186,
      "citing_paper_id": "276160959",
      "cited_paper_id": 257482703
    },
    {
      "context_text": "Recently, deep learning-based image compression meth-ods [23, 24, 28, 36, 42] have shown remarkable compression ratio improvement, demonstrating the superiority and flexibility over traditional standards [5, 6, 53, 56].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to deep learning-based image compression methods and traditional standards. No verifiable resources are identified.",
      "processing_time": 63.547568798065186,
      "citing_paper_id": "276160959",
      "cited_paper_id": 268031926
    },
    {
      "context_text": "Recently, deep learning-based image compression meth-ods [23, 24, 28, 36, 42] have shown remarkable compression ratio improvement, demonstrating the superiority and flexibility over traditional standards [5, 6, 53, 56].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to deep learning-based image compression methods and traditional standards. No verifiable resources are identified.",
      "processing_time": 63.547568798065186,
      "citing_paper_id": "276160959",
      "cited_paper_id": null
    },
    {
      "context_text": "For the weather degradation setting, we consider two types of IR models for a comprehensive comparison: 1) AirNet [32] and WGWS-Net [67], which are developed for all-in-one image restoration, and 2) Restormer [65] and SwinIR [40], which are de-613 Figure 5.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only models and methods. No verifiable resources are identified.",
      "processing_time": 61.95936918258667,
      "citing_paper_id": "276160959",
      "cited_paper_id": 250551851
    },
    {
      "context_text": "For the Gaussian noise degradation setting, we select Restormer [65] and AirNet [32] as representative IR methods.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models (Restormer and AirNet).",
      "processing_time": 61.57320475578308,
      "citing_paper_id": "276160959",
      "cited_paper_id": 250551851
    },
    {
      "context_text": "However, most existing image compression meth-ods [23, 24, 28, 36, 42] are tailored for “clean” images.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general methods for image compression. No verifiable resources are identified.",
      "processing_time": 62.255385637283325,
      "citing_paper_id": "276160959",
      "cited_paper_id": 256808230
    },
    {
      "context_text": "However, most existing image compression meth-ods [23, 24, 28, 36, 42] are tailored for “clean” images.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general methods for image compression. No verifiable resources are identified.",
      "processing_time": 62.255385637283325,
      "citing_paper_id": "276160959",
      "cited_paper_id": 257482703
    },
    {
      "context_text": "However, most existing image compression meth-ods [23, 24, 28, 36, 42] are tailored for “clean” images.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general methods for image compression. No verifiable resources are identified.",
      "processing_time": 62.255385637283325,
      "citing_paper_id": "276160959",
      "cited_paper_id": 268031926
    },
    {
      "context_text": "Similarly, Ours-S (the green curves) also shows competitive performance in comparison to EVC [23].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a comparison to another method (EVC).",
      "processing_time": 61.36304044723511,
      "citing_paper_id": "276160959",
      "cited_paper_id": 256808230
    },
    {
      "context_text": "As can be seen, Ours-L (the red curves) demonstrates comparable performance with the clean-specific image codec EVC [23] (the blue curves).",
      "catation_intent": "findings",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a comparison between two methods (Ours-L and EVC).",
      "processing_time": 62.276899337768555,
      "citing_paper_id": "276160959",
      "cited_paper_id": 256808230
    },
    {
      "context_text": "Since there are rare joint solutions for this all-in-one task, we provide a joint solution (denoted as EVC*) by training EVC [23] with mixed datasets.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.3,
      "reasoning": "The context mentions 'mixed datasets' but does not specify any particular dataset names. The term 'mixed datasets' is too generic and lacks specific identifiers.",
      "processing_time": 63.31329083442688,
      "citing_paper_id": "276160959",
      "cited_paper_id": 256808230
    },
    {
      "context_text": "For degraded images, codecs tend to spend extra bits to faith-fully preserve the degradations ( e.g ., the results of the image codec EVC [23] in Fig.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (EVC) and its performance on degraded images. No verifiable dataset names are present.",
      "processing_time": 63.26690149307251,
      "citing_paper_id": "276160959",
      "cited_paper_id": 256808230
    },
    {
      "context_text": "For the compression model, we retrain the Large variant of EVC [23] on the clean datasets of each setting.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.3,
      "reasoning": "The context mentions 'clean datasets' but does not specify any particular dataset name. The term 'clean datasets' is too generic and lacks a specific identifier.",
      "processing_time": 63.45538306236267,
      "citing_paper_id": "276160959",
      "cited_paper_id": 256808230
    },
    {
      "context_text": "When evaluated with PSNR, Ours-L exceeds EVC [23] at lower bitrates, and shows only a slight performance drop at higher bitrates, achieving an overall BD-rate improvement of -0.15%.",
      "catation_intent": "findings",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a comparison of performance metrics between methods.",
      "processing_time": 60.86455750465393,
      "citing_paper_id": "276160959",
      "cited_paper_id": 256808230
    },
    {
      "context_text": "During evaluation, the degraded inputs are first restored by IR models, and then devoted to EVC [23] for image compression.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (EVC) for image compression. The context focuses on the process of using IR models and EVC, not on datasets.",
      "processing_time": 64.12571167945862,
      "citing_paper_id": "276160959",
      "cited_paper_id": 256808230
    },
    {
      "context_text": "EVC [23] leverages mask decay and sparsity regularization for efficiency and further improves the RD performance of the scalable encoder.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (EVC) and its improvements. No verifiable resources are identified.",
      "processing_time": 61.876861333847046,
      "citing_paper_id": "276160959",
      "cited_paper_id": 256808230
    },
    {
      "context_text": "In the practical scenarios ( e.g ., object detection [10, 17, 57] and autonomous driving [52, 58]) where image compression is employed, the captured images are likely to be plagued by various degradations ( e.g ., weather-related degradations, blur, and noise) due to the complex environmental…",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only general scenarios and degradations. No clear, verifiable resource names are provided.",
      "processing_time": 61.71657156944275,
      "citing_paper_id": "276160959",
      "cited_paper_id": 258211854
    },
    {
      "context_text": "1), leading to the sub-optimal compression performance and the potential disruption for down-stream tasks [62].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general reference to sub-optimal compression performance and potential disruption for down-stream tasks.",
      "processing_time": 61.4755003452301,
      "citing_paper_id": "276160959",
      "cited_paper_id": 260810536
    },
    {
      "context_text": "DCVC-FM [36] modulates features with a learnable quantization scaler and periodically refreshing mechanism to support a wide quality range and long prediction chain.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method or model. The context focuses on the technical aspects of DCVC-FM, which is a neural video compression technique.",
      "processing_time": 63.32363677024841,
      "citing_paper_id": "276160959",
      "cited_paper_id": 268031926
    },
    {
      "context_text": "Several studies have reached promising results on single-weather image restoration task, including dehazing [9,11,23,29,32,39,41], deraining [10, 14–16, 20, 34, 35], and desnowing [4, 5, 24, 37, 45].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span mentions various image restoration tasks but does not specify any particular datasets. The cited papers' titles also do not mention specific datasets.",
      "processing_time": 61.6275908946991,
      "citing_paper_id": "273233995",
      "cited_paper_id": 837707
    },
    {
      "context_text": "Several studies have reached promising results on single-weather image restoration task, including dehazing [9,11,23,29,32,39,41], deraining [10, 14–16, 20, 34, 35], and desnowing [4, 5, 24, 37, 45].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span mentions various image restoration tasks but does not specify any particular datasets. The cited papers' titles also do not mention specific datasets.",
      "processing_time": 61.6275908946991,
      "citing_paper_id": "273233995",
      "cited_paper_id": 5427519
    },
    {
      "context_text": "Several studies have reached promising results on single-weather image restoration task, including dehazing [9,11,23,29,32,39,41], deraining [10, 14–16, 20, 34, 35], and desnowing [4, 5, 24, 37, 45].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span mentions various image restoration tasks but does not specify any particular datasets. The cited papers' titles also do not mention specific datasets.",
      "processing_time": 61.6275908946991,
      "citing_paper_id": "273233995",
      "cited_paper_id": 17115407
    },
    {
      "context_text": "Several studies have reached promising results on single-weather image restoration task, including dehazing [9,11,23,29,32,39,41], deraining [10, 14–16, 20, 34, 35], and desnowing [4, 5, 24, 37, 45].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span mentions various image restoration tasks but does not specify any particular datasets. The cited papers' titles also do not mention specific datasets.",
      "processing_time": 61.6275908946991,
      "citing_paper_id": "273233995",
      "cited_paper_id": 218487055
    },
    {
      "context_text": "Several studies have reached promising results on single-weather image restoration task, including dehazing [9,11,23,29,32,39,41], deraining [10, 14–16, 20, 34, 35], and desnowing [4, 5, 24, 37, 45].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span mentions various image restoration tasks but does not specify any particular datasets. The cited papers' titles also do not mention specific datasets.",
      "processing_time": 61.6275908946991,
      "citing_paper_id": "273233995",
      "cited_paper_id": 226291870
    },
    {
      "context_text": "Several studies have reached promising results on single-weather image restoration task, including dehazing [9,11,23,29,32,39,41], deraining [10, 14–16, 20, 34, 35], and desnowing [4, 5, 24, 37, 45].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span mentions various image restoration tasks but does not specify any particular datasets. The cited papers' titles also do not mention specific datasets.",
      "processing_time": 61.6275908946991,
      "citing_paper_id": "273233995",
      "cited_paper_id": 250085592
    },
    {
      "context_text": "Several studies have reached promising results on single-weather image restoration task, including dehazing [9,11,23,29,32,39,41], deraining [10, 14–16, 20, 34, 35], and desnowing [4, 5, 24, 37, 45].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span mentions various image restoration tasks but does not specify any particular datasets. The cited papers' titles also do not mention specific datasets.",
      "processing_time": 61.6275908946991,
      "citing_paper_id": "273233995",
      "cited_paper_id": null
    },
    {
      "context_text": "Single Degradation Image Restoration With the development of deep learning, single-weather image restoration has reached remarkable results, in-4 cluding image dehazing [9, 11, 23, 29, 30, 32, 39, 41, 46], deraining [1, 7, 10, 14, 16, 20,21,31,34,35,40,42], and desnowing [4,5,24,37,45].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various methods and models for image restoration tasks such as dehazing, deraining, and desnowing.",
      "processing_time": 62.66107368469238,
      "citing_paper_id": "273233995",
      "cited_paper_id": 837707
    },
    {
      "context_text": "Single Degradation Image Restoration With the development of deep learning, single-weather image restoration has reached remarkable results, in-4 cluding image dehazing [9, 11, 23, 29, 30, 32, 39, 41, 46], deraining [1, 7, 10, 14, 16, 20,21,31,34,35,40,42], and desnowing [4,5,24,37,45].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various methods and models for image restoration tasks such as dehazing, deraining, and desnowing.",
      "processing_time": 62.66107368469238,
      "citing_paper_id": "273233995",
      "cited_paper_id": 5427519
    },
    {
      "context_text": "Single Degradation Image Restoration With the development of deep learning, single-weather image restoration has reached remarkable results, in-4 cluding image dehazing [9, 11, 23, 29, 30, 32, 39, 41, 46], deraining [1, 7, 10, 14, 16, 20,21,31,34,35,40,42], and desnowing [4,5,24,37,45].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various methods and models for image restoration tasks such as dehazing, deraining, and desnowing.",
      "processing_time": 62.66107368469238,
      "citing_paper_id": "273233995",
      "cited_paper_id": 17115407
    },
    {
      "context_text": "Single Degradation Image Restoration With the development of deep learning, single-weather image restoration has reached remarkable results, in-4 cluding image dehazing [9, 11, 23, 29, 30, 32, 39, 41, 46], deraining [1, 7, 10, 14, 16, 20,21,31,34,35,40,42], and desnowing [4,5,24,37,45].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various methods and models for image restoration tasks such as dehazing, deraining, and desnowing.",
      "processing_time": 62.66107368469238,
      "citing_paper_id": "273233995",
      "cited_paper_id": 49864080
    },
    {
      "context_text": "Single Degradation Image Restoration With the development of deep learning, single-weather image restoration has reached remarkable results, in-4 cluding image dehazing [9, 11, 23, 29, 30, 32, 39, 41, 46], deraining [1, 7, 10, 14, 16, 20,21,31,34,35,40,42], and desnowing [4,5,24,37,45].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various methods and models for image restoration tasks such as dehazing, deraining, and desnowing.",
      "processing_time": 62.66107368469238,
      "citing_paper_id": "273233995",
      "cited_paper_id": 218487055
    },
    {
      "context_text": "Single Degradation Image Restoration With the development of deep learning, single-weather image restoration has reached remarkable results, in-4 cluding image dehazing [9, 11, 23, 29, 30, 32, 39, 41, 46], deraining [1, 7, 10, 14, 16, 20,21,31,34,35,40,42], and desnowing [4,5,24,37,45].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various methods and models for image restoration tasks such as dehazing, deraining, and desnowing.",
      "processing_time": 62.66107368469238,
      "citing_paper_id": "273233995",
      "cited_paper_id": 226291870
    },
    {
      "context_text": "Single Degradation Image Restoration With the development of deep learning, single-weather image restoration has reached remarkable results, in-4 cluding image dehazing [9, 11, 23, 29, 30, 32, 39, 41, 46], deraining [1, 7, 10, 14, 16, 20,21,31,34,35,40,42], and desnowing [4,5,24,37,45].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various methods and models for image restoration tasks such as dehazing, deraining, and desnowing.",
      "processing_time": 62.66107368469238,
      "citing_paper_id": "273233995",
      "cited_paper_id": 250085592
    },
    {
      "context_text": "Single Degradation Image Restoration With the development of deep learning, single-weather image restoration has reached remarkable results, in-4 cluding image dehazing [9, 11, 23, 29, 30, 32, 39, 41, 46], deraining [1, 7, 10, 14, 16, 20,21,31,34,35,40,42], and desnowing [4,5,24,37,45].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various methods and models for image restoration tasks such as dehazing, deraining, and desnowing.",
      "processing_time": 62.66107368469238,
      "citing_paper_id": "273233995",
      "cited_paper_id": null
    },
    {
      "context_text": "We utilize three synthetic datasets to optimize TANet, including RESIDE [18] for dehazing, Rain1400 [10] for deraining, and Snow100K [24] for desnowing.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "RESIDE",
        "Rain1400",
        "Snow100K"
      ],
      "dataset_descriptions": {
        "RESIDE": "Used for dehazing images, optimizing TANet to remove haze and improve visibility in synthetic images.",
        "Rain1400": "Used for deraining images, optimizing TANet to remove rain streaks and restore clear visual content in synthetic images.",
        "Snow100K": "Used for desnowing images, optimizing TANet to remove snowflakes and improve clarity in synthetic images."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions three specific datasets used for optimizing TANet in different image restoration tasks. These datasets are clearly named and their usage is described.",
      "processing_time": 80.9305648803711,
      "citing_paper_id": "273233995",
      "cited_paper_id": 837707
    },
    {
      "context_text": "We utilize three synthetic datasets to optimize TANet, including RESIDE [18] for dehazing, Rain1400 [10] for deraining, and Snow100K [24] for desnowing.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "RESIDE",
        "Rain1400",
        "Snow100K"
      ],
      "dataset_descriptions": {
        "RESIDE": "Used for dehazing images, optimizing TANet to remove haze and improve visibility in synthetic images.",
        "Rain1400": "Used for deraining images, optimizing TANet to remove rain streaks and restore clear visual content in synthetic images.",
        "Snow100K": "Used for desnowing images, optimizing TANet to remove snowflakes and improve clarity in synthetic images."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions three specific datasets used for optimizing TANet in different image restoration tasks. These datasets are clearly named and their usage is described.",
      "processing_time": 80.9305648803711,
      "citing_paper_id": "273233995",
      "cited_paper_id": 17115407
    },
    {
      "context_text": "Quantitative comparisons on synthetic datasets, including SOTS [18] for de-hazing, Rain1400 [10] for deraining, and Snow100K-L [24] for desnowing.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "SOTS",
        "Rain1400",
        "Snow100K-L"
      ],
      "dataset_descriptions": {
        "SOTS": "Used for de-hazing, providing synthetic images to evaluate the effectiveness of haze removal algorithms.",
        "Rain1400": "Used for deraining, offering synthetic rain images to assess the performance of rain removal techniques.",
        "Snow100K-L": "Used for desnowing, supplying synthetic snow images to test the efficiency of snow removal methods."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions three specific datasets used for quantitative comparisons in image restoration tasks, which are relevant to the research topic.",
      "processing_time": 77.65895342826843,
      "citing_paper_id": "273233995",
      "cited_paper_id": 837707
    },
    {
      "context_text": "Quantitative comparisons on synthetic datasets, including SOTS [18] for de-hazing, Rain1400 [10] for deraining, and Snow100K-L [24] for desnowing.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "SOTS",
        "Rain1400",
        "Snow100K-L"
      ],
      "dataset_descriptions": {
        "SOTS": "Used for de-hazing, providing synthetic images to evaluate the effectiveness of haze removal algorithms.",
        "Rain1400": "Used for deraining, offering synthetic rain images to assess the performance of rain removal techniques.",
        "Snow100K-L": "Used for desnowing, supplying synthetic snow images to test the efficiency of snow removal methods."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions three specific datasets used for quantitative comparisons in image restoration tasks, which are relevant to the research topic.",
      "processing_time": 77.65895342826843,
      "citing_paper_id": "273233995",
      "cited_paper_id": 17115407
    },
    {
      "context_text": "…Global Distribution Attention to handle unwanted scattering artifacts caused by atmospheric phenomena. problem as weather conditions are typically non-uniform and time-varying, making adverse weather image restoration a challenging task that has been widely studied in computer vision [12,17,25].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only general challenges in the field of adverse weather image restoration.",
      "processing_time": 60.272796392440796,
      "citing_paper_id": "273233995",
      "cited_paper_id": 15197266
    },
    {
      "context_text": "[ 21] proposed a recurrent network that utilized dilated convolutions to enhance receptive fields.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method or model. The context is about a recurrent network using dilated convolutions for image deraining.",
      "processing_time": 62.31900691986084,
      "citing_paper_id": "273233995",
      "cited_paper_id": 49864080
    },
    {
      "context_text": "For image deraining, several studies [16,21,31] utilized recurrent-based networks to progressively remove rain streaks.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions 'recurrent-based networks' for image deraining but does not specify any datasets. The cited paper title suggests a method rather than a dataset.",
      "processing_time": 62.15263867378235,
      "citing_paper_id": "273233995",
      "cited_paper_id": 49864080
    },
    {
      "context_text": "Motivated by [13], since occluded artifacts, such as rainy and snowy patterns, contain degradation patterns with various orientations, we utilize strip-pooling that contains horizontal and vertical strip-shape pooling operations to project features into horizontal and vertical directions.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a method (strip-pooling) and its application. No verifiable datasets are referenced.",
      "processing_time": 61.66946816444397,
      "citing_paper_id": "273233995",
      "cited_paper_id": 214713957
    },
    {
      "context_text": "[9 ] proposed a haze-aware representation distillation module to extract haze-aware features.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method or module for feature extraction in the context of image dehazing.",
      "processing_time": 61.09797763824463,
      "citing_paper_id": "273233995",
      "cited_paper_id": 226291870
    },
    {
      "context_text": "Motivated by [3], we first split the input tensor F ∈ R H × W × C along the channel dimension by a convolutional layer as where F 1 and F 2 ∈ R H × W × C 2 .",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method or model. The context is focused on the technical details of a network architecture.",
      "processing_time": 61.3189435005188,
      "citing_paper_id": "273233995",
      "cited_paper_id": 234482841
    },
    {
      "context_text": "Recently, several works [6,19,26–28 ,33] have focused on addressing multiple adverse weather conditions in a In this paper, we propose a Triplet Attention Network (TANet) which consists of a Triplet Attention Block (TAB) with three types of attention modules to effectively and efficiently address…",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only models and methods. The cited papers' titles do not provide additional dataset information.",
      "processing_time": 60.41225051879883,
      "citing_paper_id": "273233995",
      "cited_paper_id": 244714491
    },
    {
      "context_text": "Recently, several works [6,19,26–28 ,33] have focused on addressing multiple adverse weather conditions in a In this paper, we propose a Triplet Attention Network (TANet) which consists of a Triplet Attention Block (TAB) with three types of attention modules to effectively and efficiently address…",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only models and methods. The cited papers' titles do not provide additional dataset information.",
      "processing_time": 60.41225051879883,
      "citing_paper_id": "273233995",
      "cited_paper_id": 260107992
    },
    {
      "context_text": "Haze Rain Snow Average Params (M) Time (ms) Transweather [33] where F denotes the fast Fourier transform.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (TransWeather) and a brief technical detail about the fast Fourier transform.",
      "processing_time": 60.616907358169556,
      "citing_paper_id": "273233995",
      "cited_paper_id": 244714491
    },
    {
      "context_text": "…section, we qualitatively and quantitatively compare TANet with four state-of-the-art all-in-one-image restoration methods, including Transweather [33], KCKE [6], WGWS [47], and PromptIR [28], and four state-of-the-art multiple degradation image restoration methods, including NAFNet [2], FocalNet…",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only models and methods. There are no clear identifiers for datasets within the text.",
      "processing_time": 60.62633275985718,
      "citing_paper_id": "273233995",
      "cited_paper_id": 244714491
    },
    {
      "context_text": "All-in-one Image Restoration Compared to single and multi degradation image restoration, all-in-one image restoration [6,19,26–28, 33 ] aims to address various types of degradations in a unified model, which 3 Proposed Method",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only references to other research works. The cited papers' titles do not provide additional dataset information.",
      "processing_time": 61.04218864440918,
      "citing_paper_id": "273233995",
      "cited_paper_id": 244714491
    },
    {
      "context_text": "All-in-one Image Restoration Compared to single and multi degradation image restoration, all-in-one image restoration [6,19,26–28, 33 ] aims to address various types of degradations in a unified model, which 3 Proposed Method",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only references to other research works. The cited papers' titles do not provide additional dataset information.",
      "processing_time": 61.04218864440918,
      "citing_paper_id": "273233995",
      "cited_paper_id": 260107992
    },
    {
      "context_text": "[ 11] proposed a CNN and Transformer hybrid network with transmission-aware position embedding to address hazy patterns.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for addressing hazy patterns in images.",
      "processing_time": 59.79378962516785,
      "citing_paper_id": "273233995",
      "cited_paper_id": 250085592
    },
    {
      "context_text": "…quantitatively compare TANet with four state-of-the-art all-in-one-image restoration methods, including Transweather [33], KCKE [6], WGWS [47], and PromptIR [28], and four state-of-the-art multiple degradation image restoration methods, including NAFNet [2], FocalNet [8], GRL [22], and MPRNet [43].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only models and methods. There are no clear identifiers for datasets within the text.",
      "processing_time": 60.91838049888611,
      "citing_paper_id": "273233995",
      "cited_paper_id": 257255385
    },
    {
      "context_text": "We follow [6,47] to uniformly sample 5,000 training pairs from each dataset and mix them to form a training set of 15,000 training pairs.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.3,
      "reasoning": "The context mentions 'each dataset' but does not specify the names of the datasets. Without specific names, it is not possible to identify verifiable datasets.",
      "processing_time": 61.63889408111572,
      "citing_paper_id": "273233995",
      "cited_paper_id": 259144110
    },
    {
      "context_text": "…and quantitatively compare TANet with four state-of-the-art all-in-one-image restoration methods, including Transweather [33], KCKE [6], WGWS [47], and PromptIR [28], and four state-of-the-art multiple degradation image restoration methods, including NAFNet [2], FocalNet [8], GRL [22], and…",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only models and methods. There are no clear identifiers for datasets within the text.",
      "processing_time": 61.10361862182617,
      "citing_paper_id": "273233995",
      "cited_paper_id": 259144110
    },
    {
      "context_text": "For real-world testing sets, we select WeatherStream [44] testing set that contains 4,500 pairs for dehazing, 4,800 pairs for deraining, and 3,960 pairs for desnowing.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "WeatherStream"
      ],
      "dataset_descriptions": {
        "WeatherStream": "Used for evaluating dehazing, deraining, and desnowing algorithms on real-world images, containing 4,500 pairs for dehazing, 4,800 pairs for deraining, and 3,960 pairs for desnowing."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions a specific dataset, WeatherStream, which is used for dehazing, deraining, and desnowing tasks in image restoration.",
      "processing_time": 73.71902084350586,
      "citing_paper_id": "273233995",
      "cited_paper_id": 259340208
    },
    {
      "context_text": "[ 45] proposed a multi-scale snow removal network that utilized semantic and geometric guidance in a coarse-to-fine manner.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for snow removal.",
      "processing_time": 59.88539218902588,
      "citing_paper_id": "273233995",
      "cited_paper_id": null
    },
    {
      "context_text": "Mean-while, the inherent down-sampling feature of discrete wavelet transform (DWT) [22] reduces the computational burden.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (discrete wavelet transform).",
      "processing_time": 60.19582438468933,
      "citing_paper_id": "273969914",
      "cited_paper_id": 2356353
    },
    {
      "context_text": "Inspiredby[21],[22],we introduceawaveletself-attentionrepresentationblock(WSRB).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method or representation block inspired by wavelet theory.",
      "processing_time": 60.592325925827026,
      "citing_paper_id": "273969914",
      "cited_paper_id": 2356353
    },
    {
      "context_text": "We experimentally evaluate our approach on the images degraded by single and mingled weather degradations, including rain (Rain200H [37]), haze (RESIDE-6K [38]), raindrop (Raindrop-A [39]), snow (Snow100K-L [40]), low-lightness (LOL [41]), rain-by-snow (RS100K-L [42]), rain-by-haze (Test1 [43]) and…",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Rain200H",
        "RESIDE-6K",
        "Raindrop-A",
        "Snow100K-L",
        "LOL",
        "RS100K-L",
        "Test1"
      ],
      "dataset_descriptions": {
        "Rain200H": "Used to evaluate rain removal techniques, focusing on synthetic rain streaks on images.",
        "RESIDE-6K": "Used to assess haze removal methods, containing images with varying levels of haze.",
        "Raindrop-A": "Used to test raindrop removal, focusing on realistic raindrop effects on images.",
        "Snow100K-L": "Used to evaluate snow removal techniques, containing synthetic snowflakes on images.",
        "LOL": "Used to assess low-light image enhancement, focusing on improving visibility in dark conditions.",
        "RS100K-L": "Used to evaluate combined rain and snow removal, focusing on mixed weather degradations.",
        "Test1": "Used to test rain and haze removal, focusing on combined degradations in images."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions several datasets used for evaluating image restoration techniques under various weather degradations. These datasets are specific and relevant to the research topic.",
      "processing_time": 99.26491832733154,
      "citing_paper_id": "273969914",
      "cited_paper_id": 15443600
    },
    {
      "context_text": "Meanwhile, we employ the naturalness image quality evaluator (NIQE) [46] to quantitatively evaluate the naturalness, where the better perceptual image quality corresponds to lower indicator.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a metric (NIQE) for evaluating image quality. The cited papers do not provide additional context for dataset names.",
      "processing_time": 61.85357594490051,
      "citing_paper_id": "273969914",
      "cited_paper_id": 16892725
    },
    {
      "context_text": "Meanwhile, we employ the naturalness image quality evaluator (NIQE) [46] to quantitatively evaluate the naturalness, where the better perceptual image quality corresponds to lower indicator.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a metric (NIQE) for evaluating image quality. The cited papers do not provide additional context for dataset names.",
      "processing_time": 61.85357594490051,
      "citing_paper_id": "273969914",
      "cited_paper_id": 84843405
    },
    {
      "context_text": "…evaluate our approach on the images degraded by single and mingled weather degradations, including rain (Rain200H [37]), haze (RESIDE-6K [38]), raindrop (Raindrop-A [39]), snow (Snow100K-L [40]), low-lightness (LOL [41]), rain-by-snow (RS100K-L [42]), rain-by-haze (Test1 [43]) and…",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Rain200H",
        "RESIDE-6K",
        "Raindrop-A",
        "Snow100K-L",
        "LOL",
        "RS100K-L",
        "Test1"
      ],
      "dataset_descriptions": {
        "Rain200H": "Used to evaluate image restoration under rain degradation, focusing on high-resolution images to assess the effectiveness of the restoration method.",
        "RESIDE-6K": "Used to evaluate image restoration under haze degradation, providing a large-scale dataset to test the robustness of the dehazing algorithm.",
        "Raindrop-A": "Used to evaluate image restoration under raindrop degradation, assessing the method's ability to remove raindrops from images.",
        "Snow100K-L": "Used to evaluate image restoration under snow degradation, testing the method's performance on a large dataset of snowy images.",
        "LOL": "Used to evaluate image restoration under low-light conditions, assessing the method's ability to enhance low-light images.",
        "RS100K-L": "Used to evaluate image restoration under combined rain and snow degradation, testing the method's performance on mixed weather conditions.",
        "Test1": "Used to evaluate image restoration under combined rain and haze degradation, assessing the method's effectiveness in handling multiple degradations."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions several datasets used for evaluating image restoration techniques under various weather degradations. These datasets are specific and relevant to the research topic.",
      "processing_time": 108.42873406410217,
      "citing_paper_id": "273969914",
      "cited_paper_id": 39760169
    },
    {
      "context_text": "Image Enhancement: On LOL dataset [41], our method achieves the best quantitative performance.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "LOL dataset"
      ],
      "dataset_descriptions": {
        "LOL dataset": "Used to evaluate the performance of the proposed method for low-light image enhancement, focusing on quantitative metrics."
      },
      "confidence_score": 1.0,
      "reasoning": "The citation mentions the LOL dataset, which is a specific dataset used for low-light image enhancement. The context indicates that the dataset is used to evaluate the performance of the method.",
      "processing_time": 68.60501170158386,
      "citing_paper_id": "273969914",
      "cited_paper_id": 52008443
    },
    {
      "context_text": "…on the images degraded by single and mingled weather degradations, including rain (Rain200H [37]), haze (RESIDE-6K [38]), raindrop (Raindrop-A [39]), snow (Snow100K-L [40]), low-lightness (LOL [41]), rain-by-snow (RS100K-L [42]), rain-by-haze (Test1 [43]) and rain-by-raindrop (RainDS-syn [44]).",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Rain200H",
        "RESIDE-6K",
        "Raindrop-A",
        "Snow100K-L",
        "LOL",
        "RS100K-L",
        "Test1",
        "RainDS-syn"
      ],
      "dataset_descriptions": {
        "Rain200H": "Used to evaluate rain removal techniques, focusing on high-resolution images with synthetic rain streaks.",
        "RESIDE-6K": "Used to assess haze removal methods, containing 6,000 pairs of hazy and clear images for training and testing.",
        "Raindrop-A": "Used to test raindrop removal, featuring images with realistic raindrop effects for evaluation.",
        "Snow100K-L": "Used to evaluate snow removal techniques, providing a large dataset of images with synthetic snowflakes.",
        "LOL": "Used to assess low-light enhancement methods, containing pairs of low-light and well-lit images.",
        "RS100K-L": "Used to evaluate combined rain and snow removal, featuring images with both rain and snow degradations.",
        "Test1": "Used to test rain and haze removal, containing images degraded by both rain and haze for evaluation.",
        "RainDS-syn": "Used to evaluate combined rain and raindrop removal, featuring synthetic images with both rain and raindrop effects."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions several datasets used for evaluating image restoration techniques under various weather degradations. These datasets are specific and relevant to the research topic.",
      "processing_time": 110.77429437637329,
      "citing_paper_id": "273969914",
      "cited_paper_id": 52008443
    },
    {
      "context_text": "…on the images degraded by single and mingled weather degradations, including rain (Rain200H [37]), haze (RESIDE-6K [38]), raindrop (Raindrop-A [39]), snow (Snow100K-L [40]), low-lightness (LOL [41]), rain-by-snow (RS100K-L [42]), rain-by-haze (Test1 [43]) and rain-by-raindrop (RainDS-syn [44]).",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Rain200H",
        "RESIDE-6K",
        "Raindrop-A",
        "Snow100K-L",
        "LOL",
        "RS100K-L",
        "Test1",
        "RainDS-syn"
      ],
      "dataset_descriptions": {
        "Rain200H": "Used to evaluate rain removal techniques, focusing on high-resolution images with synthetic rain streaks.",
        "RESIDE-6K": "Used to assess haze removal methods, containing 6,000 pairs of hazy and clear images for training and testing.",
        "Raindrop-A": "Used to test raindrop removal, featuring images with realistic raindrop effects for evaluation.",
        "Snow100K-L": "Used to evaluate snow removal techniques, providing a large dataset of images with synthetic snowflakes.",
        "LOL": "Used to assess low-light enhancement methods, containing pairs of low-light and well-lit images.",
        "RS100K-L": "Used to evaluate combined rain and snow removal, featuring images with both rain and snow degradations.",
        "Test1": "Used to test rain and haze removal, containing images degraded by both rain and haze for evaluation.",
        "RainDS-syn": "Used to evaluate combined rain and raindrop removal, featuring synthetic images with both rain and raindrop effects."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions several datasets used for evaluating image restoration techniques under various weather degradations. These datasets are specific and relevant to the research topic.",
      "processing_time": 110.77429437637329,
      "citing_paper_id": "273969914",
      "cited_paper_id": 235702784
    },
    {
      "context_text": "We set the number of learning iterations to 10 000 on latent prompt generators, and the number of prompted iterations",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The provided context does not mention any specific datasets, only parameters for training. No dataset names are present in the citation span.",
      "processing_time": 60.62751054763794,
      "citing_paper_id": "273969914",
      "cited_paper_id": 59316941
    },
    {
      "context_text": "We set the number of learning iterations to 10 000 on latent prompt generators, and the number of prompted iterations",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The provided context does not mention any specific datasets, only parameters for training. No dataset names are present in the citation span.",
      "processing_time": 60.62751054763794,
      "citing_paper_id": "273969914",
      "cited_paper_id": 84843405
    },
    {
      "context_text": "We set the number of learning iterations to 10 000 on latent prompt generators, and the number of prompted iterations",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The provided context does not mention any specific datasets, only parameters for training. No dataset names are present in the citation span.",
      "processing_time": 60.62751054763794,
      "citing_paper_id": "273969914",
      "cited_paper_id": 208138077
    },
    {
      "context_text": "We set the number of learning iterations to 10 000 on latent prompt generators, and the number of prompted iterations",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The provided context does not mention any specific datasets, only parameters for training. No dataset names are present in the citation span.",
      "processing_time": 60.62751054763794,
      "citing_paper_id": "273969914",
      "cited_paper_id": 226641165
    },
    {
      "context_text": "We set the number of learning iterations to 10 000 on latent prompt generators, and the number of prompted iterations",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The provided context does not mention any specific datasets, only parameters for training. No dataset names are present in the citation span.",
      "processing_time": 60.62751054763794,
      "citing_paper_id": "273969914",
      "cited_paper_id": 233296013
    },
    {
      "context_text": "We set the number of learning iterations to 10 000 on latent prompt generators, and the number of prompted iterations",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The provided context does not mention any specific datasets, only parameters for training. No dataset names are present in the citation span.",
      "processing_time": 60.62751054763794,
      "citing_paper_id": "273969914",
      "cited_paper_id": 237266491
    },
    {
      "context_text": "To this end, we utilize both ﬁxed and learnable image encoders of CLIP [16] as the core network for our latent prompt generators.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions using CLIP, but it is described as a model or method, not a dataset. No specific dataset is mentioned.",
      "processing_time": 61.226279973983765,
      "citing_paper_id": "273969914",
      "cited_paper_id": 231591445
    },
    {
      "context_text": "Our approach leverages the extensive vision-language priors in CLIP model [16], [17], enabling us to generate latent prompts speciﬁcally tailored to the weather-degraded images being processed.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only the CLIP model which is a method. No datasets are explicitly used or referenced in the given context.",
      "processing_time": 61.77840280532837,
      "citing_paper_id": "273969914",
      "cited_paper_id": 231591445
    },
    {
      "context_text": "…either create dedicated physical models for speciﬁc weather removal tasks [23], [24], [25], or develop uniﬁed solutions trained individually [6], [7], [26], [27], without exploring the shared aspects among various weather degradations needed for comprehensive weather-degraded image restoration.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to methods and approaches for image restoration. No verifiable resources are identified.",
      "processing_time": 61.62280893325806,
      "citing_paper_id": "273969914",
      "cited_paper_id": 231802205
    },
    {
      "context_text": "…either create dedicated physical models for speciﬁc weather removal tasks [23], [24], [25], or develop uniﬁed solutions trained individually [6], [7], [26], [27], without exploring the shared aspects among various weather degradations needed for comprehensive weather-degraded image restoration.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to methods and approaches for image restoration. No verifiable resources are identified.",
      "processing_time": 61.62280893325806,
      "citing_paper_id": "273969914",
      "cited_paper_id": 235358213
    },
    {
      "context_text": "As Table II(f) shown, our model obtains performance gains of 0.71 dB over MPRNet [26] in PSNR indicator.",
      "catation_intent": "findings",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a performance comparison with another model (MPRNet).",
      "processing_time": 61.002429485321045,
      "citing_paper_id": "273969914",
      "cited_paper_id": 231802205
    },
    {
      "context_text": "Given the nature of conditional generation, solutions that rely on discrete prompt engineering [12], [13], [14] are sensitive to the quality of given ﬁxed prompts.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to papers about prompt engineering. No verifiable resources are identified.",
      "processing_time": 61.2193386554718,
      "citing_paper_id": "273969914",
      "cited_paper_id": 232168628
    },
    {
      "context_text": "The task-aligned ones [6], [7] can also remove one type of degradations, using the same model with varying parameters.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models or methods. The context is about using models for image restoration, which is not sufficient to identify a dataset.",
      "processing_time": 62.62806940078735,
      "citing_paper_id": "273969914",
      "cited_paper_id": 235358213
    },
    {
      "context_text": "The task-aligned ones [6], [7] can also remove one type of degradations, using the same model with varying parameters.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models or methods. The context is about using models for image restoration, which is not sufficient to identify a dataset.",
      "processing_time": 62.62806940078735,
      "citing_paper_id": "273969914",
      "cited_paper_id": 271285150
    },
    {
      "context_text": "Therefore, the output feature of our proposed WSRB can be obtained by This addresses the limitation of traditional self-attention mechanism in representing high-frequency dependencies [30], [35].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only references to improving vision transformers and high-frequency components.",
      "processing_time": 60.59960126876831,
      "citing_paper_id": "273969914",
      "cited_paper_id": 247939260
    },
    {
      "context_text": "As Table II(h) depicted, the quantitative performance of our model is better than IDT [50].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a comparison with another model (IDT). There are no clear identifiers for datasets or other verifiable resources.",
      "processing_time": 62.327295780181885,
      "citing_paper_id": "273969914",
      "cited_paper_id": 249709314
    },
    {
      "context_text": "Both strategies exhibit limitations in adapting to diverse weather conditions.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, models, or methods. It only discusses the limitations of certain strategies in adapting to diverse weather conditions.",
      "processing_time": 61.537901401519775,
      "citing_paper_id": "273969914",
      "cited_paper_id": 262055269
    },
    {
      "context_text": "As Table II(e) illustrated, MIRNet [67] obtains the second-best performance in PSNR, and our method achieves a performance improvement by 0.78 dB. Rain-by-snow Removal: To evaluate our model in rain-by-snow weather removal, we conduct comparative experiments of the several methods and our proposed…",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only performance metrics and methods. No clear identifiers for datasets are present.",
      "processing_time": 61.382312536239624,
      "citing_paper_id": "273969914",
      "cited_paper_id": 263784964
    },
    {
      "context_text": "The task-speciﬁc methods [2], [3], [4], [5] can only eliminate one type of degradations after the training, necessitating different models and parameters.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to task-specific methods. There are no clear identifiers for datasets or other verifiable resources.",
      "processing_time": 62.31195092201233,
      "citing_paper_id": "273969914",
      "cited_paper_id": 264364007
    },
    {
      "context_text": "Real-world datasets such as LOL-v2 [45], Snow100k-R [27], NH-HAZE [14], and SPA-Data [42] were employed to evaluate AllRestorer’s performance in real-world all-in-one image restoration.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "LOL-v2",
        "Snow100k-R",
        "NH-HAZE",
        "SPA-Data"
      ],
      "dataset_descriptions": {
        "LOL-v2": "Used to evaluate AllRestorer’s performance in low-light image restoration, focusing on enhancing visibility and color accuracy in dark images.",
        "Snow100k-R": "Used to assess AllRestorer’s effectiveness in snow removal, specifically improving image clarity and detail in snowy conditions.",
        "NH-HAZE": "Used to test AllRestorer’s capability in haze removal, evaluating its ability to enhance image quality in hazy environments.",
        "SPA-Data": "Used to evaluate AllRestorer’s performance in various real-world scenarios, focusing on comprehensive image restoration tasks."
      },
      "confidence_score": 1.0,
      "reasoning": "The citation mentions specific datasets used to evaluate the performance of AllRestorer in real-world all-in-one image restoration.",
      "processing_time": 87.65947127342224,
      "citing_paper_id": "274131843",
      "cited_paper_id": 837707
    },
    {
      "context_text": "9, we compare AllRestorer with low-light enhancement baselines [22, 37, 52, 57], deraining base-lines [12, 16, 35, 46], dehazing baselines [18, 19, 33, 40], and desnowing baselines [6, 7, 27, 51], respectively.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to other research works and their methods. No verifiable datasets are identified.",
      "processing_time": 61.90348219871521,
      "citing_paper_id": "274131843",
      "cited_paper_id": 837707
    },
    {
      "context_text": "9, we compare AllRestorer with low-light enhancement baselines [22, 37, 52, 57], deraining base-lines [12, 16, 35, 46], dehazing baselines [18, 19, 33, 40], and desnowing baselines [6, 7, 27, 51], respectively.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to other research works and their methods. No verifiable datasets are identified.",
      "processing_time": 61.90348219871521,
      "citing_paper_id": "274131843",
      "cited_paper_id": 59316941
    },
    {
      "context_text": "9, we compare AllRestorer with low-light enhancement baselines [22, 37, 52, 57], deraining base-lines [12, 16, 35, 46], dehazing baselines [18, 19, 33, 40], and desnowing baselines [6, 7, 27, 51], respectively.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to other research works and their methods. No verifiable datasets are identified.",
      "processing_time": 61.90348219871521,
      "citing_paper_id": "274131843",
      "cited_paper_id": 208138077
    },
    {
      "context_text": "9, we compare AllRestorer with low-light enhancement baselines [22, 37, 52, 57], deraining base-lines [12, 16, 35, 46], dehazing baselines [18, 19, 33, 40], and desnowing baselines [6, 7, 27, 51], respectively.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to other research works and their methods. No verifiable datasets are identified.",
      "processing_time": 61.90348219871521,
      "citing_paper_id": "274131843",
      "cited_paper_id": 216562731
    },
    {
      "context_text": "9, we compare AllRestorer with low-light enhancement baselines [22, 37, 52, 57], deraining base-lines [12, 16, 35, 46], dehazing baselines [18, 19, 33, 40], and desnowing baselines [6, 7, 27, 51], respectively.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to other research works and their methods. No verifiable datasets are identified.",
      "processing_time": 61.90348219871521,
      "citing_paper_id": "274131843",
      "cited_paper_id": 221097144
    },
    {
      "context_text": "9, we compare AllRestorer with low-light enhancement baselines [22, 37, 52, 57], deraining base-lines [12, 16, 35, 46], dehazing baselines [18, 19, 33, 40], and desnowing baselines [6, 7, 27, 51], respectively.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to other research works and their methods. No verifiable datasets are identified.",
      "processing_time": 61.90348219871521,
      "citing_paper_id": "274131843",
      "cited_paper_id": 232352745
    },
    {
      "context_text": "9, we compare AllRestorer with low-light enhancement baselines [22, 37, 52, 57], deraining base-lines [12, 16, 35, 46], dehazing baselines [18, 19, 33, 40], and desnowing baselines [6, 7, 27, 51], respectively.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to other research works and their methods. No verifiable datasets are identified.",
      "processing_time": 61.90348219871521,
      "citing_paper_id": "274131843",
      "cited_paper_id": 246634936
    },
    {
      "context_text": "All-in-One [26] first introduces Neural Architecture Search (NAS) to extend the restoration task to an all-in-one approach.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only the introduction of NAS for an all-in-one restoration approach.",
      "processing_time": 60.94747710227966,
      "citing_paper_id": "274131843",
      "cited_paper_id": 219978541
    },
    {
      "context_text": "Notably, CoOp [54] and CoCoOp [53] introduce prompt learning for optimizing CLIP [34], while SAM [23] uses multimodal prompts for unified semantic segmentation.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions models and methods (CoOp, CoCoOp, CLIP, SAM) but does not reference any specific datasets. The cited paper titles do not provide additional context to identify datasets.",
      "processing_time": 64.05762982368469,
      "citing_paper_id": "274131843",
      "cited_paper_id": 231591445
    },
    {
      "context_text": "Notably, CoOp [54] and CoCoOp [53] introduce prompt learning for optimizing CLIP [34], while SAM [23] uses multimodal prompts for unified semantic segmentation.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions models and methods (CoOp, CoCoOp, CLIP, SAM) but does not reference any specific datasets. The cited paper titles do not provide additional context to identify datasets.",
      "processing_time": 64.05762982368469,
      "citing_paper_id": "274131843",
      "cited_paper_id": 237386023
    },
    {
      "context_text": "Notably, CoOp [54] and CoCoOp [53] introduce prompt learning for optimizing CLIP [34], while SAM [23] uses multimodal prompts for unified semantic segmentation.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions models and methods (CoOp, CoCoOp, CLIP, SAM) but does not reference any specific datasets. The cited paper titles do not provide additional context to identify datasets.",
      "processing_time": 64.05762982368469,
      "citing_paper_id": "274131843",
      "cited_paper_id": 257952310
    },
    {
      "context_text": "To extract these features, we utilize CLIP [34] encoders (fine-tuned on CDD-11) to generate text embeddings for each entry in the memory bank, and image embeddings for the degraded input image.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "CDD-11"
      ],
      "dataset_descriptions": {
        "CDD-11": "Used to fine-tune CLIP encoders for generating text and image embeddings, supporting the development of transferable visual models."
      },
      "confidence_score": 0.6,
      "reasoning": "The context mentions 'CDD-11' as a dataset used for fine-tuning CLIP encoders. However, there is no clear indication of how this dataset is used in the specific research context or what aspect of the research it supports.",
      "processing_time": 74.3969247341156,
      "citing_paper_id": "274131843",
      "cited_paper_id": 231591445
    },
    {
      "context_text": "Recently, prompt learning in Natural Language Processing (NLP) [3, 24, 31] has gained increasing importance in computer vision [15, 55].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (prompt learning) and its application areas (NLP and computer vision).",
      "processing_time": 62.39536714553833,
      "citing_paper_id": "274131843",
      "cited_paper_id": 246823759
    },
    {
      "context_text": "Promp-tIR [32] and TransWeather [41] utilize learnable prompts for image restoration across various impaired scenarios, and VPT [21] introduces visual prompts for this purpose.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions methods (learnable prompts, visual prompts) but does not specify any datasets. The cited paper titles do not provide additional dataset information.",
      "processing_time": 62.97269082069397,
      "citing_paper_id": "274131843",
      "cited_paper_id": 247618727
    },
    {
      "context_text": "Promp-tIR [32] and TransWeather [41] utilize learnable prompts for image restoration across various impaired scenarios, and VPT [21] introduces visual prompts for this purpose.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions methods (learnable prompts, visual prompts) but does not specify any datasets. The cited paper titles do not provide additional dataset information.",
      "processing_time": 62.97269082069397,
      "citing_paper_id": "274131843",
      "cited_paper_id": 268064460
    },
    {
      "context_text": "MAE-VQGAN [2] and Painter [43] also employ prompts to address multiple visual tasks.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models/methods. No dataset names are present in the citation span.",
      "processing_time": 61.83347964286804,
      "citing_paper_id": "274131843",
      "cited_paper_id": 254246343
    },
    {
      "context_text": "Ad-ditionally, we gather 200 real composite scene cases from Input Restormer [49] OKNet [11] PromptIR [32] TransWeather [41] OneRestore [17] AllRestorer Target Figure 7.",
      "catation_intent": "reusable resource",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions gathering composite scene cases from various sources, but does not specify any named datasets. The sources mentioned are models or methods, not datasets.",
      "processing_time": 63.3609459400177,
      "citing_paper_id": "274131843",
      "cited_paper_id": 268064460
    },
    {
      "context_text": "Fourmer [56] TransWeather [41] PromptIR [32] OneRestore [17] AllRestorer marking a 5.00 dB PSNR and 0.0615 SSIM improvement over the current state-of-the-art (SoTA) baseline.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models and methods. The context focuses on performance improvements over state-of-the-art baselines.",
      "processing_time": 62.77463960647583,
      "citing_paper_id": "274131843",
      "cited_paper_id": 268064460
    },
    {
      "context_text": "Unlike previous all-in-one restoration approaches [9, 17, 32], AiOTB incorporates a novel All-in-One Attention (AiOA) mechanism (Sec.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only a novel mechanism called All-in-One Attention (AiOA). The cited papers' titles do not provide additional context about datasets.",
      "processing_time": 64.30112051963806,
      "citing_paper_id": "274131843",
      "cited_paper_id": 268064460
    },
    {
      "context_text": "Unlike previous all-in-one restoration approaches [9, 17, 32], AiOTB incorporates a novel All-in-One Attention (AiOA) mechanism (Sec.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only a novel mechanism called All-in-One Attention (AiOA). The cited papers' titles do not provide additional context about datasets.",
      "processing_time": 64.30112051963806,
      "citing_paper_id": "274131843",
      "cited_paper_id": 271891992
    },
    {
      "context_text": "VPIP [9] and PromptIR [32] employ prompt learning with different scenario descriptors for all-in-one restoration, and OneRestore [17] further explores composite degradation restoration by improving scene descriptors (see Fig.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only methods and models. The cited papers' titles do not provide additional dataset information.",
      "processing_time": 62.297566652297974,
      "citing_paper_id": "274131843",
      "cited_paper_id": 268064460
    },
    {
      "context_text": "VPIP [9] and PromptIR [32] employ prompt learning with different scenario descriptors for all-in-one restoration, and OneRestore [17] further explores composite degradation restoration by improving scene descriptors (see Fig.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only methods and models. The cited papers' titles do not provide additional dataset information.",
      "processing_time": 62.297566652297974,
      "citing_paper_id": "274131843",
      "cited_paper_id": 271891992
    },
    {
      "context_text": "To meet the demands of autonomous driving [13, 44, 58] under various road conditions, recent studies have moved beyond image restoration for single-degradation scenarios [28, 38, 49] and have shifted their focus to all-in-one image restoration tasks [25, 32, 59].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only a shift in focus towards all-in-one image restoration tasks. No verifiable resources are named.",
      "processing_time": 63.111403703689575,
      "citing_paper_id": "274131843",
      "cited_paper_id": 268064460
    },
    {
      "context_text": "VPIP [9] introduces paired degraded and clean images corresponding to the current restoration task as visual scene descriptors, providing prior information for restoring composite degraded images.",
      "catation_intent": "reusable resource",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions 'paired degraded and clean images' but does not specify a dataset name. The term 'VPIP' is mentioned but is likely a method or model, not a dataset.",
      "processing_time": 64.82997679710388,
      "citing_paper_id": "274131843",
      "cited_paper_id": 271891992
    },
    {
      "context_text": "To evaluate our network’s performance on distorted images with different degradation levels, we compare the denoising results of our method and other state-of-the-art denoising methods, including BM3D [10], DnCNN [40], IRCNN [41], DL [11], FFDNet [42], and MPR [37].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only various denoising methods and models. No verifiable datasets are referenced.",
      "processing_time": 62.35522699356079,
      "citing_paper_id": "260680793",
      "cited_paper_id": 996788
    },
    {
      "context_text": "To evaluate our network’s performance on distorted images with different degradation levels, we compare the denoising results of our method and other state-of-the-art denoising methods, including BM3D [10], DnCNN [40], IRCNN [41], DL [11], FFDNet [42], and MPR [37].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only various denoising methods and models. No verifiable datasets are referenced.",
      "processing_time": 62.35522699356079,
      "citing_paper_id": "260680793",
      "cited_paper_id": 1900475
    },
    {
      "context_text": "To evaluate our network’s performance on distorted images with different degradation levels, we compare the denoising results of our method and other state-of-the-art denoising methods, including BM3D [10], DnCNN [40], IRCNN [41], DL [11], FFDNet [42], and MPR [37].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only various denoising methods and models. No verifiable datasets are referenced.",
      "processing_time": 62.35522699356079,
      "citing_paper_id": "260680793",
      "cited_paper_id": 10514149
    },
    {
      "context_text": "To evaluate our network’s performance on distorted images with different degradation levels, we compare the denoising results of our method and other state-of-the-art denoising methods, including BM3D [9], DnCNN [40], IRCNN [41], DL [10], FFDNet [42], and Table 2: Quantitative comparison of our method and other",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only models and methods used for image denoising. No verifiable datasets are referenced.",
      "processing_time": 62.94646406173706,
      "citing_paper_id": "260680793",
      "cited_paper_id": 996788
    },
    {
      "context_text": "To evaluate our network’s performance on distorted images with different degradation levels, we compare the denoising results of our method and other state-of-the-art denoising methods, including BM3D [9], DnCNN [40], IRCNN [41], DL [10], FFDNet [42], and Table 2: Quantitative comparison of our method and other",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only models and methods used for image denoising. No verifiable datasets are referenced.",
      "processing_time": 62.94646406173706,
      "citing_paper_id": "260680793",
      "cited_paper_id": 1900475
    },
    {
      "context_text": "To evaluate our network’s performance on distorted images with different degradation levels, we compare the denoising results of our method and other state-of-the-art denoising methods, including BM3D [9], DnCNN [40], IRCNN [41], DL [10], FFDNet [42], and Table 2: Quantitative comparison of our method and other",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only models and methods used for image denoising. No verifiable datasets are referenced.",
      "processing_time": 62.94646406173706,
      "citing_paper_id": "260680793",
      "cited_paper_id": 10514149
    },
    {
      "context_text": "To evaluate our network’s performance on distorted images with different degradation levels, we compare the denoising results of our method and other state-of-the-art denoising methods, including BM3D [9], DnCNN [40], IRCNN [41], DL [10], FFDNet [42], and Table 2: Quantitative comparison of our method and other",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only models and methods used for image denoising. No verifiable datasets are referenced.",
      "processing_time": 62.94646406173706,
      "citing_paper_id": "260680793",
      "cited_paper_id": 195787503
    },
    {
      "context_text": "The majority of the existing restoration methods [2, 5, 6, 14, 20, 32, 35, 37, 39, 40] are designed for a single degradation (e.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to other works. No dataset names are present in the text.",
      "processing_time": 62.12196898460388,
      "citing_paper_id": "260680793",
      "cited_paper_id": 996788
    },
    {
      "context_text": "The majority of the existing restoration methods [2, 5, 6, 14, 20, 32, 35, 37, 39, 40] are designed for a single degradation (e.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to other works. No dataset names are present in the text.",
      "processing_time": 62.12196898460388,
      "citing_paper_id": "260680793",
      "cited_paper_id": 26598341
    },
    {
      "context_text": "The majority of the existing restoration methods [2, 5, 6, 14, 20, 32, 35, 37, 39, 40] are designed for a single degradation (e.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to other works. No dataset names are present in the text.",
      "processing_time": 62.12196898460388,
      "citing_paper_id": "260680793",
      "cited_paper_id": 237266491
    },
    {
      "context_text": "The majority of the existing restoration methods [2, 5, 6, 14, 20, 32, 35, 37, 39, 40] are designed for a single degradation (e.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to other works. No dataset names are present in the text.",
      "processing_time": 62.12196898460388,
      "citing_paper_id": "260680793",
      "cited_paper_id": 248085491
    },
    {
      "context_text": "Traditional approaches focus on the exploration of the image prior, such as sparse [23, 25, 33], low-rank [13, 32], self-similarity [9] etc.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods and concepts. The cited papers' titles do not provide additional dataset information.",
      "processing_time": 62.54365825653076,
      "citing_paper_id": "260680793",
      "cited_paper_id": 1663191
    },
    {
      "context_text": "Traditional approaches focus on the exploration of the image prior, such as sparse [23, 25, 33], low-rank [13, 32], self-similarity [9] etc.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods and concepts. The cited papers' titles do not provide additional dataset information.",
      "processing_time": 62.54365825653076,
      "citing_paper_id": "260680793",
      "cited_paper_id": 1916689
    },
    {
      "context_text": "Traditional approaches focus on the exploration of the image prior, such as sparse [23, 25, 33], low-rank [13, 32], self-similarity [9] etc.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods and concepts. The cited papers' titles do not provide additional dataset information.",
      "processing_time": 62.54365825653076,
      "citing_paper_id": "260680793",
      "cited_paper_id": 2627705
    },
    {
      "context_text": "Traditional approaches focus on the exploration of the image prior, such as sparse [23, 25, 33], low-rank [13, 32], self-similarity [9] etc.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods and concepts. The cited papers' titles do not provide additional dataset information.",
      "processing_time": 62.54365825653076,
      "citing_paper_id": "260680793",
      "cited_paper_id": 14460720
    },
    {
      "context_text": "Traditional approaches focus on the exploration of the image prior, such as sparse [23, 25, 33], low-rank [13, 32], self-similarity [9] etc.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods and concepts. The cited papers' titles do not provide additional dataset information.",
      "processing_time": 62.54365825653076,
      "citing_paper_id": "260680793",
      "cited_paper_id": 26598341
    },
    {
      "context_text": "An AdamW [22] optimizer is adopted to optimize network parameters.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only an optimizer method. No datasets are referenced or used in the context provided.",
      "processing_time": 62.345235109329224,
      "citing_paper_id": "260680793",
      "cited_paper_id": 3312944
    },
    {
      "context_text": "An AdamW [23] optimizer is adopted to optimize network parameters.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only an optimizer method. No dataset names are present in the citation span.",
      "processing_time": 61.82588481903076,
      "citing_paper_id": "260680793",
      "cited_paper_id": 3312944
    },
    {
      "context_text": "Some degradation estimation methods [7, 12, 14, 15, 21, 28, 36] typically assume a predefined degradation category and estimate degradation level parameters, making them less suitable for scenarios with multiple unknown degradations.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods and their limitations. No verifiable resources are identified.",
      "processing_time": 62.10851860046387,
      "citing_paper_id": "260680793",
      "cited_paper_id": 15941714
    },
    {
      "context_text": "Some degradation estimation methods [7, 12, 14, 15, 21, 28, 36] typically assume a predefined degradation category and estimate degradation level parameters, making them less suitable for scenarios with multiple unknown degradations.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods and their limitations. No verifiable resources are identified.",
      "processing_time": 62.10851860046387,
      "citing_paper_id": "260680793",
      "cited_paper_id": 235656945
    },
    {
      "context_text": "For example, in denoising [14, 21], the core is to estimate the noise level when the noise type is known.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general concept of noise estimation in denoising.",
      "processing_time": 61.711169958114624,
      "citing_paper_id": "260680793",
      "cited_paper_id": 15941714
    },
    {
      "context_text": "Recently, with the support of a large number of collected paired images, many deep neural networks (DNN) methods [2, 5, 6, 20, 31, 34, 35, 37, 39] have produced impressive results on each subtask of restoration.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.2,
      "reasoning": "The context does not mention any specific datasets, only a general reference to 'a large number of collected paired images'. No specific dataset names are provided.",
      "processing_time": 63.65589761734009,
      "citing_paper_id": "260680793",
      "cited_paper_id": 128359047
    },
    {
      "context_text": "Recently, with the support of a large number of collected paired images, many deep neural networks (DNN) methods [2, 5, 6, 20, 31, 34, 35, 37, 39] have produced impressive results on each subtask of restoration.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.2,
      "reasoning": "The context does not mention any specific datasets, only a general reference to 'a large number of collected paired images'. No specific dataset names are provided.",
      "processing_time": 63.65589761734009,
      "citing_paper_id": "260680793",
      "cited_paper_id": 237266491
    },
    {
      "context_text": "Recently, with the support of a large number of collected paired images, many deep neural networks (DNN) methods [2, 5, 6, 20, 31, 34, 35, 37, 39] have produced impressive results on each subtask of restoration.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.2,
      "reasoning": "The context does not mention any specific datasets, only a general reference to 'a large number of collected paired images'. No specific dataset names are provided.",
      "processing_time": 63.65589761734009,
      "citing_paper_id": "260680793",
      "cited_paper_id": 248085491
    },
    {
      "context_text": "And in deblurring, many methods [7, 12, 15, 28, 36] usually estimate the blur kernel before non-blind deblurring.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods for deblurring. There are no verifiable resources or datasets mentioned.",
      "processing_time": 62.55927395820618,
      "citing_paper_id": "260680793",
      "cited_paper_id": 235656945
    },
    {
      "context_text": "denoising, deblurring, or super-resolution), and even some methods [5, 20, 37] that claim to handle multiple degradations require separate training on categorized and specific degraded images, which is computationally expensive, time-consuming for optimization, and not friendly for storage on mobile devices.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only general references to degraded images and computational requirements. No verifiable resource names are present.",
      "processing_time": 63.02029490470886,
      "citing_paper_id": "260680793",
      "cited_paper_id": 237266491
    },
    {
      "context_text": "denoising, deblurring, or super-resolution), and even some methods [5, 20, 37] that claim to handle multiple degradations require separate training on categorized and specific degraded images, which is computationally expensive, time-consuming for optimization, and not friendly for storage on mobile devices.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only general references to degraded images and computational requirements. No verifiable resource names are present.",
      "processing_time": 63.02029490470886,
      "citing_paper_id": "260680793",
      "cited_paper_id": 248085491
    },
    {
      "context_text": "Those methods like MPR [37], NAFNet [5], which are CNN-based methods, and SwinIR [21] is a transformer-based method.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only models and methods. No verifiable resources are identified.",
      "processing_time": 61.73134517669678,
      "citing_paper_id": "260680793",
      "cited_paper_id": 237266491
    },
    {
      "context_text": "It is evident that our method exhibits significantly reduced computational cost compared to AirNe t[16], MPR [37], and SwinIR [20].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only comparisons to other methods. The cited papers' titles do not introduce any datasets either.",
      "processing_time": 63.01331377029419,
      "citing_paper_id": "260680793",
      "cited_paper_id": 237266491
    },
    {
      "context_text": "It is evident that our method exhibits significantly reduced computational cost compared to AirNe t[16], MPR [37], and SwinIR [20].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only comparisons to other methods. The cited papers' titles do not introduce any datasets either.",
      "processing_time": 63.01331377029419,
      "citing_paper_id": "260680793",
      "cited_paper_id": 250551851
    },
    {
      "context_text": "Those methods like MPR [37], NAFNet [5], which are CNN-based methods, and SwinIR [20] is a transformer-based method.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only models and methods. No dataset names are present in the citation span.",
      "processing_time": 62.35209321975708,
      "citing_paper_id": "260680793",
      "cited_paper_id": 237266491
    },
    {
      "context_text": "Those methods like MPR [37], NAFNet [5], which are CNN-based methods, and SwinIR [20] is a transformer-based method.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only models and methods. No dataset names are present in the citation span.",
      "processing_time": 62.35209321975708,
      "citing_paper_id": "260680793",
      "cited_paper_id": 248085491
    },
    {
      "context_text": "Although somemodels [5, 6, 20, 37] can be adapted to handle multiple types of degraded images, they typically require separate training on specific degraded datasets, and they may not generalize well to other typed of degraded images without further adaptation.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.0,
      "reasoning": "The context does not mention any specific datasets, only a general reference to 'specific degraded datasets' without naming them. No clear, verifiable resources are identified.",
      "processing_time": 63.72067403793335,
      "citing_paper_id": "260680793",
      "cited_paper_id": 237266491
    },
    {
      "context_text": "Although somemodels [5, 6, 20, 37] can be adapted to handle multiple types of degraded images, they typically require separate training on specific degraded datasets, and they may not generalize well to other typed of degraded images without further adaptation.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.0,
      "reasoning": "The context does not mention any specific datasets, only a general reference to 'specific degraded datasets' without naming them. No clear, verifiable resources are identified.",
      "processing_time": 63.72067403793335,
      "citing_paper_id": "260680793",
      "cited_paper_id": 248085491
    },
    {
      "context_text": "Similar to training networks separately on individual degraded datasets [5, 20, 37], the use of the hierarchical degradation representation is aimed at making feature distributions of similar degradations more compact and easily distinguishable from dissimilar degradation features.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets by name, only referring to 'individual degraded datasets' without providing concrete identifiers.",
      "processing_time": 62.30358386039734,
      "citing_paper_id": "260680793",
      "cited_paper_id": 237266491
    },
    {
      "context_text": "Similar to training networks separately on individual degraded datasets [5, 20, 37], the use of the hierarchical degradation representation is aimed at making feature distributions of similar degradations more compact and easily distinguishable from dissimilar degradation features.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets by name, only referring to 'individual degraded datasets' without providing concrete identifiers.",
      "processing_time": 62.30358386039734,
      "citing_paper_id": "260680793",
      "cited_paper_id": 248085491
    },
    {
      "context_text": "It is evident that our method exhibits significantly reduced computational cost compared to AirNe t[17], MPR [37], and SwinIR [21].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only comparisons to other methods. No verifiable resources are identified.",
      "processing_time": 62.10639238357544,
      "citing_paper_id": "260680793",
      "cited_paper_id": 237266491
    },
    {
      "context_text": "While there is an increase in computational cost compared to NAFNet [5] and TransWeather [29], our method holds an advantage in handling multiple degradations.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only comparisons to other methods. No verifiable resources are identified.",
      "processing_time": 61.89574313163757,
      "citing_paper_id": "260680793",
      "cited_paper_id": 248085491
    },
    {
      "context_text": "Our restoration network is constructed based on NAFNet [5], which is a variant Unet with symmetric encoder-decoder architecture.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (NAFNet) used for constructing the restoration network.",
      "processing_time": 62.75405836105347,
      "citing_paper_id": "260680793",
      "cited_paper_id": 248085491
    },
    {
      "context_text": "In the decoder, efficient NAFBlocks [5] are employed to transform modulated image features.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (NAFBlocks) used in the decoder for transforming modulated image features.",
      "processing_time": 63.09602761268616,
      "citing_paper_id": "260680793",
      "cited_paper_id": 248085491
    },
    {
      "context_text": "We adopt NAFNet [5] as our restoration backbone, as it has demonstrated remarkable performance and computational advantages across multiple restoration tasks.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a method (NAFNet) used for image restoration. No verifiable datasets are referenced.",
      "processing_time": 63.43765926361084,
      "citing_paper_id": "260680793",
      "cited_paper_id": 248085491
    },
    {
      "context_text": "While some methods [16, 30] rely on contrastive learning for degradation representation, the deficient selection of positive and negative samples in these approaches makes it difficult to comprehensively describe the relationships between different degradations, which can lead to representation drift and ultimately impacts the performance of image restoration.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and their limitations. The context focuses on the challenges of using contrastive learning for degradation representation in image restoration.",
      "processing_time": 64.63434553146362,
      "citing_paper_id": "260680793",
      "cited_paper_id": 250551851
    },
    {
      "context_text": "DASR [30] and AirNet [16] realize the learning of representation by contrastive learning.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models or methods (DASR, AirNet).",
      "processing_time": 62.3063702583313,
      "citing_paper_id": "260680793",
      "cited_paper_id": 250551851
    },
    {
      "context_text": "From the illustration, we can see that the hierarchical degradation representations generated by our method exhibit greater inter-class separation and intra-class compactness compared to the results of AirNet [16].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a comparison between methods. The citation is used to reference a method, not a dataset.",
      "processing_time": 63.4270293712616,
      "citing_paper_id": "260680793",
      "cited_paper_id": 250551851
    },
    {
      "context_text": "Moreover, to measure the network’s performance on images with different degradation levels, we follow [16] to train our network on WED [24] and test it on synthetic noisy images with different noise levels from CBSD68 [26].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "WED",
        "CBSD68"
      ],
      "dataset_descriptions": {
        "WED": "Used for training the network to handle various image degradations, focusing on robustness across different corruption types.",
        "CBSD68": "Used for testing the network's performance on synthetic noisy images with varying noise levels, evaluating restoration accuracy."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions two specific datasets, WED and CBSD68, which are used for training and testing the network, respectively.",
      "processing_time": 74.53473901748657,
      "citing_paper_id": "260680793",
      "cited_paper_id": 250551851
    },
    {
      "context_text": "Besides, we also compare the performance of our method with other all-in-one approaches, like TransWeather [29] and AirNet [16].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only other methods for comparison. No verifiable resources are identified.",
      "processing_time": 62.511003255844116,
      "citing_paper_id": "260680793",
      "cited_paper_id": 250551851
    },
    {
      "context_text": "Other approaches, DASR [30] and AirNet [16] consider patches from the same image as positive samples and patches from different images as negative samples.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods (DASR and AirNet). No verifiable resources are identified.",
      "processing_time": 63.226616621017456,
      "citing_paper_id": "260680793",
      "cited_paper_id": 250551851
    },
    {
      "context_text": "1Following [16], the multi-degradation that this paper focuses on refers to a dataset with multiple degradations, differing from mixed degradations in an image.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.0,
      "reasoning": "The context does not mention any specific dataset names, only a general concept of a dataset with multiple degradations. No clear, verifiable resource is identified.",
      "processing_time": 64.37933826446533,
      "citing_paper_id": "260680793",
      "cited_paper_id": 250551851
    },
    {
      "context_text": "One such example is presented in [8], which suggests using both soft and hard contrastive regularization to enhance the performance of both specific and multiple degradations.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for enhancing performance in image restoration tasks.",
      "processing_time": 62.05608010292053,
      "citing_paper_id": "260680793",
      "cited_paper_id": 250581068
    },
    {
      "context_text": "[18] proposes to learn a latent representation space for degradations in super-resolution.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for learning latent representations for degradations in super-resolution.",
      "processing_time": 63.18135380744934,
      "citing_paper_id": "260680793",
      "cited_paper_id": 251066779
    },
    {
      "context_text": "[17] proposes to learn degradation representations with a blurry-sharp cycle framework.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for learning degradation representations.",
      "processing_time": 61.80262327194214,
      "citing_paper_id": "260680793",
      "cited_paper_id": 251468222
    },
    {
      "context_text": "The comparison algorithms include DSC [14] , GMM [15] , GiG-CoM [16] , RESCAN [7] , PReNet [17] , MSPFN [18] , RCDNet [19] , and SPDNet [20] .",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only algorithms and methods. There are no clear identifiers for datasets within the text.",
      "processing_time": 63.491827964782715,
      "citing_paper_id": "268067725",
      "cited_paper_id": 14460720
    },
    {
      "context_text": "The fourth category consists of deep learning-based methods [4] , employing deep neural networks for image denoising.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a category of methods. There are no verifiable resources or datasets mentioned.",
      "processing_time": 63.1734733581543,
      "citing_paper_id": "268067725",
      "cited_paper_id": 18774783
    },
    {
      "context_text": "The third approach is image restoration-based methods [3] , utilizing algorithms like image interpolation and inpainting to restore noise-contaminated regions.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only general methods and approaches.",
      "processing_time": 61.19631910324097,
      "citing_paper_id": "268067725",
      "cited_paper_id": 232352874
    },
    {
      "context_text": "Common deep learning methods include Convolutional Neural Networks (CNN) [5] , Recurrent Neural Networks (RNN) [6] , Generative Adversarial Networks (GAN) [7] , and transformers [8] .",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only deep learning methods. No dataset names are present in the text.",
      "processing_time": 63.149802684783936,
      "citing_paper_id": "268067725",
      "cited_paper_id": 235829480
    },
    {
      "context_text": "Comparative experiments included seven algorithms: RESCAN [7] , PReNet [8] , RCDNet [9] , Star-Net [10] , MoWE [11] , Trinity-Net [12] , and RAUNE-Net [13] .",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only algorithms. There are no verifiable resources that meet the criteria for inclusion.",
      "processing_time": 63.67444968223572,
      "citing_paper_id": "268067725",
      "cited_paper_id": 259236069
    },
    {
      "context_text": "Comparative experiments included seven algorithms: RESCAN [7] , PReNet [8] , RCDNet [9] , Star-Net [10] , MoWE [11] , Trinity-Net [12] , and RAUNE-Net [13] .",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only algorithms. There are no verifiable resources that meet the criteria for inclusion.",
      "processing_time": 63.67444968223572,
      "citing_paper_id": "268067725",
      "cited_paper_id": null
    },
    {
      "context_text": "These include SIDD [1], CBSD68 [35], DIV2K [2], GoPro [38], HIDE [53], LOL [67], SOTS-Outdoor [28], Rain100 [71], and Raindrop [42], each corresponding to their respective tasks.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "SIDD",
        "CBSD68",
        "DIV2K",
        "GoPro",
        "HIDE",
        "LOL",
        "SOTS-Outdoor",
        "Rain100",
        "Raindrop"
      ],
      "dataset_descriptions": {
        "SIDD": "Used for studying sensor and image degradation, providing real-world noisy images and their ground truth for denoising algorithms.",
        "CBSD68": "Used for benchmarking image denoising algorithms, containing 68 clean and noisy image pairs.",
        "DIV2K": "Used for super-resolution tasks, providing high-quality images and their downsampled versions.",
        "GoPro": "Used for motion deblurring, containing sharp and blurred image pairs captured from a GoPro camera.",
        "HIDE": "Used for haze removal, providing hazy and clear image pairs for evaluating dehazing algorithms.",
        "LOL": "Used for low-light image enhancement, containing low-light and well-exposed image pairs.",
        "SOTS-Outdoor": "Used for synthetic and real outdoor dehazing, providing hazy and clear image pairs.",
        "Rain100": "Used for rain removal, containing rainy and clean image pairs for training and testing rain removal algorithms.",
        "Raindrop": "Used for raindrop removal, providing images with and without raindrops for evaluating raindrop removal techniques."
      },
      "confidence_score": 1.0,
      "reasoning": "The context lists several datasets used for various image restoration tasks, which align with the research topic of all-in-one image restoration.",
      "processing_time": 111.41142201423645,
      "citing_paper_id": "270067618",
      "cited_paper_id": 64193
    },
    {
      "context_text": "These include SIDD [1], CBSD68 [35], DIV2K [2], GoPro [38], HIDE [53], LOL [67], SOTS-Outdoor [28], Rain100 [71], and Raindrop [42], each corresponding to their respective tasks.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "SIDD",
        "CBSD68",
        "DIV2K",
        "GoPro",
        "HIDE",
        "LOL",
        "SOTS-Outdoor",
        "Rain100",
        "Raindrop"
      ],
      "dataset_descriptions": {
        "SIDD": "Used for studying sensor and image degradation, providing real-world noisy images and their ground truth for denoising algorithms.",
        "CBSD68": "Used for benchmarking image denoising algorithms, containing 68 clean and noisy image pairs.",
        "DIV2K": "Used for super-resolution tasks, providing high-quality images and their downsampled versions.",
        "GoPro": "Used for motion deblurring, containing sharp and blurred image pairs captured from a GoPro camera.",
        "HIDE": "Used for haze removal, providing hazy and clear image pairs for evaluating dehazing algorithms.",
        "LOL": "Used for low-light image enhancement, containing low-light and well-exposed image pairs.",
        "SOTS-Outdoor": "Used for synthetic and real outdoor dehazing, providing hazy and clear image pairs.",
        "Rain100": "Used for rain removal, containing rainy and clean image pairs for training and testing rain removal algorithms.",
        "Raindrop": "Used for raindrop removal, providing images with and without raindrops for evaluating raindrop removal techniques."
      },
      "confidence_score": 1.0,
      "reasoning": "The context lists several datasets used for various image restoration tasks, which align with the research topic of all-in-one image restoration.",
      "processing_time": 111.41142201423645,
      "citing_paper_id": "270067618",
      "cited_paper_id": 4539586
    },
    {
      "context_text": "These include SIDD [1], CBSD68 [35], DIV2K [2], GoPro [38], HIDE [53], LOL [67], SOTS-Outdoor [28], Rain100 [71], and Raindrop [42], each corresponding to their respective tasks.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "SIDD",
        "CBSD68",
        "DIV2K",
        "GoPro",
        "HIDE",
        "LOL",
        "SOTS-Outdoor",
        "Rain100",
        "Raindrop"
      ],
      "dataset_descriptions": {
        "SIDD": "Used for studying sensor and image degradation, providing real-world noisy images and their ground truth for denoising algorithms.",
        "CBSD68": "Used for benchmarking image denoising algorithms, containing 68 clean and noisy image pairs.",
        "DIV2K": "Used for super-resolution tasks, providing high-quality images and their downsampled versions.",
        "GoPro": "Used for motion deblurring, containing sharp and blurred image pairs captured from a GoPro camera.",
        "HIDE": "Used for haze removal, providing hazy and clear image pairs for evaluating dehazing algorithms.",
        "LOL": "Used for low-light image enhancement, containing low-light and well-exposed image pairs.",
        "SOTS-Outdoor": "Used for synthetic and real outdoor dehazing, providing hazy and clear image pairs.",
        "Rain100": "Used for rain removal, containing rainy and clean image pairs for training and testing rain removal algorithms.",
        "Raindrop": "Used for raindrop removal, providing images with and without raindrops for evaluating raindrop removal techniques."
      },
      "confidence_score": 1.0,
      "reasoning": "The context lists several datasets used for various image restoration tasks, which align with the research topic of all-in-one image restoration.",
      "processing_time": 111.41142201423645,
      "citing_paper_id": "270067618",
      "cited_paper_id": 15443600
    },
    {
      "context_text": "These include SIDD [1], CBSD68 [35], DIV2K [2], GoPro [38], HIDE [53], LOL [67], SOTS-Outdoor [28], Rain100 [71], and Raindrop [42], each corresponding to their respective tasks.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "SIDD",
        "CBSD68",
        "DIV2K",
        "GoPro",
        "HIDE",
        "LOL",
        "SOTS-Outdoor",
        "Rain100",
        "Raindrop"
      ],
      "dataset_descriptions": {
        "SIDD": "Used for studying sensor and image degradation, providing real-world noisy images and their ground truth for denoising algorithms.",
        "CBSD68": "Used for benchmarking image denoising algorithms, containing 68 clean and noisy image pairs.",
        "DIV2K": "Used for super-resolution tasks, providing high-quality images and their downsampled versions.",
        "GoPro": "Used for motion deblurring, containing sharp and blurred image pairs captured from a GoPro camera.",
        "HIDE": "Used for haze removal, providing hazy and clear image pairs for evaluating dehazing algorithms.",
        "LOL": "Used for low-light image enhancement, containing low-light and well-exposed image pairs.",
        "SOTS-Outdoor": "Used for synthetic and real outdoor dehazing, providing hazy and clear image pairs.",
        "Rain100": "Used for rain removal, containing rainy and clean image pairs for training and testing rain removal algorithms.",
        "Raindrop": "Used for raindrop removal, providing images with and without raindrops for evaluating raindrop removal techniques."
      },
      "confidence_score": 1.0,
      "reasoning": "The context lists several datasets used for various image restoration tasks, which align with the research topic of all-in-one image restoration.",
      "processing_time": 111.41142201423645,
      "citing_paper_id": "270067618",
      "cited_paper_id": 201624746
    },
    {
      "context_text": "Next, we calculate the probability ˆ p ( c i | I ) and obtain the output text embedding of BIQA e auto by: During the optimization of BIQA, we freeze the parameters of the text encoder E T and finetune the image encoder E I using the multi-class fidelity loss [57].",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for optimization involving a multi-class fidelity loss.",
      "processing_time": 62.42061948776245,
      "citing_paper_id": "270067618",
      "cited_paper_id": 190906
    },
    {
      "context_text": "In addition, we use GoPro [38], LOL [67], RESIDE [28], Rain200L [71], and Raindrop [42] for deblurring, low-light enhancement, dehazing, deraining, and deraindrop, respectively.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "GoPro",
        "LOL",
        "RESIDE",
        "Rain200L",
        "Raindrop"
      ],
      "dataset_descriptions": {
        "GoPro": "Used for deblurring, focusing on motion blur correction in video frames.",
        "LOL": "Used for low-light enhancement, improving visibility in underexposed images.",
        "RESIDE": "Used for dehazing, enhancing clarity in hazy images by removing atmospheric scattering.",
        "Rain200L": "Used for deraining, removing rain streaks from images to improve visual quality.",
        "Raindrop": "Used for deraindrop, addressing raindrop removal from images to enhance clarity."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions specific datasets used for various image restoration tasks, which align with the research topic of all-in-one image restoration.",
      "processing_time": 87.7186872959137,
      "citing_paper_id": "270067618",
      "cited_paper_id": 4539586
    },
    {
      "context_text": "In addition, we use GoPro [38], LOL [67], RESIDE [28], Rain200L [71], and Raindrop [42] for deblurring, low-light enhancement, dehazing, deraining, and deraindrop, respectively.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "GoPro",
        "LOL",
        "RESIDE",
        "Rain200L",
        "Raindrop"
      ],
      "dataset_descriptions": {
        "GoPro": "Used for deblurring, focusing on motion blur correction in video frames.",
        "LOL": "Used for low-light enhancement, improving visibility in underexposed images.",
        "RESIDE": "Used for dehazing, enhancing clarity in hazy images by removing atmospheric scattering.",
        "Rain200L": "Used for deraining, removing rain streaks from images to improve visual quality.",
        "Raindrop": "Used for deraindrop, addressing raindrop removal from images to enhance clarity."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions specific datasets used for various image restoration tasks, which align with the research topic of all-in-one image restoration.",
      "processing_time": 87.7186872959137,
      "citing_paper_id": "270067618",
      "cited_paper_id": 15443600
    },
    {
      "context_text": "In addition, for completeness, we also report PSNR and LPIPS [75].",
      "catation_intent": "none",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only metrics (PSNR and LPIPS). LPIPS is a method, not a dataset.",
      "processing_time": 64.18791127204895,
      "citing_paper_id": "270067618",
      "cited_paper_id": 4766599
    },
    {
      "context_text": "…etc. Previous image restoration methods often address these image degradations separately, including de-blurring [23, 68], denoising [45, 78] deraining [24, 48], super-resolution [9, 62], low-light enhancement [37, 76], deraindrop [43], and dehaze [44, 55], by using specific single-task models.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various image restoration tasks and methods. No verifiable resources are identified.",
      "processing_time": 63.238916635513306,
      "citing_paper_id": "270067618",
      "cited_paper_id": 59316941
    },
    {
      "context_text": "…etc. Previous image restoration methods often address these image degradations separately, including de-blurring [23, 68], denoising [45, 78] deraining [24, 48], super-resolution [9, 62], low-light enhancement [37, 76], deraindrop [43], and dehaze [44, 55], by using specific single-task models.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various image restoration tasks and methods. No verifiable resources are identified.",
      "processing_time": 63.238916635513306,
      "citing_paper_id": "270067618",
      "cited_paper_id": 208138077
    },
    {
      "context_text": "…etc. Previous image restoration methods often address these image degradations separately, including de-blurring [23, 68], denoising [45, 78] deraining [24, 48], super-resolution [9, 62], low-light enhancement [37, 76], deraindrop [43], and dehaze [44, 55], by using specific single-task models.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various image restoration tasks and methods. No verifiable resources are identified.",
      "processing_time": 63.238916635513306,
      "citing_paper_id": "270067618",
      "cited_paper_id": 214623217
    },
    {
      "context_text": "…etc. Previous image restoration methods often address these image degradations separately, including de-blurring [23, 68], denoising [45, 78] deraining [24, 48], super-resolution [9, 62], low-light enhancement [37, 76], deraindrop [43], and dehaze [44, 55], by using specific single-task models.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various image restoration tasks and methods. No verifiable resources are identified.",
      "processing_time": 63.238916635513306,
      "citing_paper_id": "270067618",
      "cited_paper_id": 248069347
    },
    {
      "context_text": "…etc. Previous image restoration methods often address these image degradations separately, including de-blurring [23, 68], denoising [45, 78] deraining [24, 48], super-resolution [9, 62], low-light enhancement [37, 76], deraindrop [43], and dehaze [44, 55], by using specific single-task models.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various image restoration tasks and methods. No verifiable resources are identified.",
      "processing_time": 63.238916635513306,
      "citing_paper_id": "270067618",
      "cited_paper_id": 258615282
    },
    {
      "context_text": "For experiments with unknown degradations, we use the Under-Display Camera (TOLED) dataset [81], the Enhancing Underwater Visual Perception (EUVP) dataset [22], and the Snow Dataset (CSD) dataset [11].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Under-Display Camera (TOLED) dataset",
        "Enhancing Underwater Visual Perception (EUVP) dataset",
        "Snow Dataset (CSD)"
      ],
      "dataset_descriptions": {
        "Under-Display Camera (TOLED) dataset": "Used to test image restoration techniques under display camera conditions, focusing on degradations specific to under-display camera images.",
        "Enhancing Underwater Visual Perception (EUVP) dataset": "Applied to enhance underwater visual perception, specifically addressing color correction and clarity issues in underwater images.",
        "Snow Dataset (CSD)": "Utilized to evaluate image restoration methods in snowy environments, focusing on removing snow artifacts and improving visibility."
      },
      "confidence_score": 1.0,
      "reasoning": "The citation mentions three specific datasets used for experiments with unknown degradations, which are directly relevant to the topic of image restoration.",
      "processing_time": 86.93832206726074,
      "citing_paper_id": "270067618",
      "cited_paper_id": 85498726
    },
    {
      "context_text": "Iterative image restoration on unseen Under-Display Camera (TOLED) dataset [81] and unseen Enhancing Underwater Visual Perception (EUVP) dataset [22].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "TOLED",
        "EUVP"
      ],
      "dataset_descriptions": {
        "TOLED": "Used for iterative image restoration on unseen under-display camera images, focusing on improving visual quality and perception.",
        "EUVP": "Used for iterative image restoration on unseen underwater images, enhancing visual perception and clarity in challenging aquatic environments."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions two specific datasets, TOLED and EUVP, which are used for iterative image restoration on unseen data.",
      "processing_time": 73.93270206451416,
      "citing_paper_id": "270067618",
      "cited_paper_id": 85498726
    },
    {
      "context_text": "To show that AutoDIR is able to handle multiple unknown image degradations, we conducted experiments on two additional datasets: the Under-Display-Camera (TOLED) dataset [80] and the Enhancing Underwater Visual Perception (EUVP) dataset [22].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Under-Display-Camera (TOLED) dataset",
        "Enhancing Underwater Visual Perception (EUVP) dataset"
      ],
      "dataset_descriptions": {
        "Under-Display-Camera (TOLED) dataset": "Used to test AutoDIR's ability to handle under-display camera image degradations, focusing on improving visual quality and clarity.",
        "Enhancing Underwater Visual Perception (EUVP) dataset": "Applied to evaluate AutoDIR's performance in enhancing underwater images, specifically addressing color correction and contrast improvement."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions two specific datasets used for experiments to demonstrate the capability of AutoDIR in handling multiple unknown image degradations.",
      "processing_time": 80.49256443977356,
      "citing_paper_id": "270067618",
      "cited_paper_id": 85498726
    },
    {
      "context_text": "16 on images with multiple unknown degradations on UCD [80] and EVUP [22] datasets, to further demonstrate the performance of our method in handling such complex scenarios.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "UCD",
        "EVUP"
      ],
      "dataset_descriptions": {
        "UCD": "Used to evaluate the method's performance on images with multiple unknown degradations, focusing on complex scenarios.",
        "EVUP": "Used to evaluate the method's performance on images with multiple unknown degradations, focusing on complex scenarios."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions two specific datasets, UCD and EVUP, which are used to evaluate the performance of the method on images with multiple unknown degradations.",
      "processing_time": 75.93675923347473,
      "citing_paper_id": "270067618",
      "cited_paper_id": 85498726
    },
    {
      "context_text": "Generative priors for image restoration have been widely exploited for image restoration, in the form of generative adversarial networks (GANs) inversion and filters [3, 5, 17, 21, 36, 40], VQ cookbooks [8, 33, 59, 79], latent diffusion models [15, 25, 66].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods and models. No dataset names are present in the text.",
      "processing_time": 63.049808740615845,
      "citing_paper_id": "270067618",
      "cited_paper_id": 189762039
    },
    {
      "context_text": "Generative priors for image restoration have been widely exploited for image restoration, in the form of generative adversarial networks (GANs) inversion and filters [3, 5, 17, 21, 36, 40], VQ cookbooks [8, 33, 59, 79], latent diffusion models [15, 25, 66].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods and models. No dataset names are present in the text.",
      "processing_time": 63.049808740615845,
      "citing_paper_id": "270067618",
      "cited_paper_id": 196834421
    },
    {
      "context_text": "Generative priors for image restoration have been widely exploited for image restoration, in the form of generative adversarial networks (GANs) inversion and filters [3, 5, 17, 21, 36, 40], VQ cookbooks [8, 33, 59, 79], latent diffusion models [15, 25, 66].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods and models. No dataset names are present in the text.",
      "processing_time": 63.049808740615845,
      "citing_paper_id": "270067618",
      "cited_paper_id": 212634162
    },
    {
      "context_text": "Generative priors for image restoration have been widely exploited for image restoration, in the form of generative adversarial networks (GANs) inversion and filters [3, 5, 17, 21, 36, 40], VQ cookbooks [8, 33, 59, 79], latent diffusion models [15, 25, 66].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods and models. No dataset names are present in the text.",
      "processing_time": 63.049808740615845,
      "citing_paper_id": "270067618",
      "cited_paper_id": 249926514
    },
    {
      "context_text": "Generative priors for image restoration have been widely exploited for image restoration, in the form of generative adversarial networks (GANs) inversion and filters [3, 5, 17, 21, 36, 40], VQ cookbooks [8, 33, 59, 79], latent diffusion models [15, 25, 66].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods and models. No dataset names are present in the text.",
      "processing_time": 63.049808740615845,
      "citing_paper_id": "270067618",
      "cited_paper_id": 250264643
    },
    {
      "context_text": "Generative priors for image restoration have been widely exploited for image restoration, in the form of generative adversarial networks (GANs) inversion and filters [3, 5, 17, 21, 36, 40], VQ cookbooks [8, 33, 59, 79], latent diffusion models [15, 25, 66].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods and models. No dataset names are present in the text.",
      "processing_time": 63.049808740615845,
      "citing_paper_id": "270067618",
      "cited_paper_id": 254125609
    },
    {
      "context_text": "Generative priors for image restoration have been widely exploited for image restoration, in the form of generative adversarial networks (GANs) inversion and filters [3, 5, 17, 21, 36, 40], VQ cookbooks [8, 33, 59, 79], latent diffusion models [15, 25, 66].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods and models. No dataset names are present in the text.",
      "processing_time": 63.049808740615845,
      "citing_paper_id": "270067618",
      "cited_paper_id": 257921922
    },
    {
      "context_text": "Generative priors for image restoration have been widely exploited for image restoration, in the form of generative adversarial networks (GANs) inversion and filters [3, 5, 17, 21, 36, 40], VQ cookbooks [8, 33, 59, 79], latent diffusion models [15, 25, 66].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods and models. No dataset names are present in the text.",
      "processing_time": 63.049808740615845,
      "citing_paper_id": "270067618",
      "cited_paper_id": 259137590
    },
    {
      "context_text": "Unified All-in-One Image Restoration Previous approaches for unified all-in-one image restoration can be categorized into two main groups: unsupervised generative prior-based methods [3, 5, 15, 21, 25, 36, 40, 66, 70] and end-to-end supervised learning-based methods [12, 29, 31, 41, 58, 74].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only categories of methods. No dataset names are present in the text.",
      "processing_time": 63.0474579334259,
      "citing_paper_id": "270067618",
      "cited_paper_id": 189762039
    },
    {
      "context_text": "Unified All-in-One Image Restoration Previous approaches for unified all-in-one image restoration can be categorized into two main groups: unsupervised generative prior-based methods [3, 5, 15, 21, 25, 36, 40, 66, 70] and end-to-end supervised learning-based methods [12, 29, 31, 41, 58, 74].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only categories of methods. No dataset names are present in the text.",
      "processing_time": 63.0474579334259,
      "citing_paper_id": "270067618",
      "cited_paper_id": 196834421
    },
    {
      "context_text": "Unified All-in-One Image Restoration Previous approaches for unified all-in-one image restoration can be categorized into two main groups: unsupervised generative prior-based methods [3, 5, 15, 21, 25, 36, 40, 66, 70] and end-to-end supervised learning-based methods [12, 29, 31, 41, 58, 74].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only categories of methods. No dataset names are present in the text.",
      "processing_time": 63.0474579334259,
      "citing_paper_id": "270067618",
      "cited_paper_id": 212634162
    },
    {
      "context_text": "Unified All-in-One Image Restoration Previous approaches for unified all-in-one image restoration can be categorized into two main groups: unsupervised generative prior-based methods [3, 5, 15, 21, 25, 36, 40, 66, 70] and end-to-end supervised learning-based methods [12, 29, 31, 41, 58, 74].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only categories of methods. No dataset names are present in the text.",
      "processing_time": 63.0474579334259,
      "citing_paper_id": "270067618",
      "cited_paper_id": 219978541
    },
    {
      "context_text": "Unified All-in-One Image Restoration Previous approaches for unified all-in-one image restoration can be categorized into two main groups: unsupervised generative prior-based methods [3, 5, 15, 21, 25, 36, 40, 66, 70] and end-to-end supervised learning-based methods [12, 29, 31, 41, 58, 74].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only categories of methods. No dataset names are present in the text.",
      "processing_time": 63.0474579334259,
      "citing_paper_id": "270067618",
      "cited_paper_id": 254125609
    },
    {
      "context_text": "Unified All-in-One Image Restoration Previous approaches for unified all-in-one image restoration can be categorized into two main groups: unsupervised generative prior-based methods [3, 5, 15, 21, 25, 36, 40, 66, 70] and end-to-end supervised learning-based methods [12, 29, 31, 41, 58, 74].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only categories of methods. No dataset names are present in the text.",
      "processing_time": 63.0474579334259,
      "citing_paper_id": "270067618",
      "cited_paper_id": 257921922
    },
    {
      "context_text": "Unified All-in-One Image Restoration Previous approaches for unified all-in-one image restoration can be categorized into two main groups: unsupervised generative prior-based methods [3, 5, 15, 21, 25, 36, 40, 66, 70] and end-to-end supervised learning-based methods [12, 29, 31, 41, 58, 74].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only categories of methods. No dataset names are present in the text.",
      "processing_time": 63.0474579334259,
      "citing_paper_id": "270067618",
      "cited_paper_id": 260085391
    },
    {
      "context_text": "End-to-end supervised learning-based methods typically learn image embeddings extracted by an auxiliary degradation predictor to guide the image restoration model [29, 31, 41, 58, 74].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to methods and approaches. No dataset names are present in the text.",
      "processing_time": 63.57647752761841,
      "citing_paper_id": "270067618",
      "cited_paper_id": 219978541
    },
    {
      "context_text": "End-to-end supervised learning-based methods typically learn image embeddings extracted by an auxiliary degradation predictor to guide the image restoration model [29, 31, 41, 58, 74].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to methods and approaches. No dataset names are present in the text.",
      "processing_time": 63.57647752761841,
      "citing_paper_id": "270067618",
      "cited_paper_id": 260085391
    },
    {
      "context_text": "In our study, we employ a recently developed metric DISTS [13] which is shown to align well with human perception and optimal for image restoration [14].",
      "catation_intent": "findings",
      "resource_type": "metric",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions a metric (DISTS) but does not refer to any specific dataset. The focus is on the metric's alignment with human perception and its suitability for image restoration.",
      "processing_time": 65.66776871681213,
      "citing_paper_id": "270067618",
      "cited_paper_id": 231662712
    },
    {
      "context_text": "In our study, we employ a recently developed metric DISTS [13] which is shown to align well with human perception and optimal for image restoration [14].",
      "catation_intent": "findings",
      "resource_type": "metric",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions a metric (DISTS) but does not refer to any specific dataset. The focus is on the metric's alignment with human perception and its suitability for image restoration.",
      "processing_time": 65.66776871681213,
      "citing_paper_id": "265658862",
      "cited_paper_id": 215785896
    },
    {
      "context_text": "Inspired by prior work on vision foundation models [54, 64, 83], we conduct experiments on the unseen de-snowing task to evaluate this hypothesis.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a task (de-snowing) and a hypothesis evaluation. No verifiable resources are named.",
      "processing_time": 64.05821228027344,
      "citing_paper_id": "270067618",
      "cited_paper_id": 244954250
    },
    {
      "context_text": "Inspired by prior work on vision foundation models [54, 64, 83], we conduct experiments on the unseen de-snowing task to evaluate this hypothesis.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a task (de-snowing) and a hypothesis evaluation. No verifiable resources are named.",
      "processing_time": 64.05821228027344,
      "citing_paper_id": "270067618",
      "cited_paper_id": 246634906
    },
    {
      "context_text": "Recently, developments in diffusion models [20, 49] and language-vision models [46] have led to significant progress in text-based image synthesis and editing [7, 39, 47, 51].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only models and methods. The cited papers' titles also do not provide specific dataset names.",
      "processing_time": 63.89513278007507,
      "citing_paper_id": "270067618",
      "cited_paper_id": 248097655
    },
    {
      "context_text": "Recently, developments in diffusion models [20, 49] and language-vision models [46] have led to significant progress in text-based image synthesis and editing [7, 39, 47, 51].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only models and methods. The cited papers' titles also do not provide specific dataset names.",
      "processing_time": 63.89513278007507,
      "citing_paper_id": "270067618",
      "cited_paper_id": 248986576
    },
    {
      "context_text": "Recently, developments in diffusion models [20, 49] and language-vision models [46] have led to significant progress in text-based image synthesis and editing [7, 39, 47, 51].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only models and methods. The cited papers' titles also do not provide specific dataset names.",
      "processing_time": 63.89513278007507,
      "citing_paper_id": "270067618",
      "cited_paper_id": 256416326
    },
    {
      "context_text": "For our experiments, we utilize 90 snowy-clean image pairs of the Comprehensive Snow GT SD [4 8 ] Ours Input Figure 7.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Comprehensive Snow GT SD"
      ],
      "dataset_descriptions": {
        "Comprehensive Snow GT SD": "Used to evaluate image restoration methods, specifically focusing on snowy-clean image pairs to assess performance in real-world conditions."
      },
      "confidence_score": 0.9,
      "reasoning": "The context mentions 'Comprehensive Snow GT SD' which appears to be a dataset used for experiments involving snowy-clean image pairs.",
      "processing_time": 70.26674151420593,
      "citing_paper_id": "270067618",
      "cited_paper_id": 250264643
    },
    {
      "context_text": "For the super-resolution and deblurring task, we adopt the widely used blind perceptual metrics including CLIP-IQA [61] and MUSIQ [27] to evaluate the quality of generated images following [6, 62, 65].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions CLIP-IQA and MUSIQ as perceptual metrics for evaluating image quality in super-resolution and deblurring tasks. These are not datasets but evaluation metrics.",
      "processing_time": 65.42853164672852,
      "citing_paper_id": "270067618",
      "cited_paper_id": 251040466
    },
    {
      "context_text": "For the super-resolution and deblurring task, we adopt the widely used blind perceptual metrics including CLIP-IQA [61] and MUSIQ [27] to evaluate the quality of generated images following [6, 62, 65].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions CLIP-IQA and MUSIQ as perceptual metrics for evaluating image quality in super-resolution and deblurring tasks. These are not datasets but evaluation metrics.",
      "processing_time": 65.42853164672852,
      "citing_paper_id": "270067618",
      "cited_paper_id": 258615282
    },
    {
      "context_text": "Hertz et al. [18] improve editing efficiency by directly manipulating cross-attention maps without the need for per-image finetuning.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for image editing.",
      "processing_time": 61.73046255111694,
      "citing_paper_id": "270067618",
      "cited_paper_id": 251252882
    },
    {
      "context_text": "The emergence of diffusion-based methods has offered new avenues for text-to-image editing, which can be broadly categorized into data-driven [4] and no-extra-data-required approaches [18, 26, 77].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only categories of approaches. No dataset names are present in the text.",
      "processing_time": 63.3246693611145,
      "citing_paper_id": "270067618",
      "cited_paper_id": 251252882
    },
    {
      "context_text": "The emergence of diffusion-based methods has offered new avenues for text-to-image editing, which can be broadly categorized into data-driven [4] and no-extra-data-required approaches [18, 26, 77].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only categories of approaches. No dataset names are present in the text.",
      "processing_time": 63.3246693611145,
      "citing_paper_id": "270067618",
      "cited_paper_id": 254408758
    },
    {
      "context_text": "For instance, recent works interpolate the text embedding [26] or score estimate [77] of the input and desired images.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods or models. There are no clear identifiers for datasets in the provided context.",
      "processing_time": 63.65477466583252,
      "citing_paper_id": "270067618",
      "cited_paper_id": 254408758
    },
    {
      "context_text": "Last: zero-shot method GDP [15] for comparison. dicating the implicit correlations between them.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for comparison.",
      "processing_time": 61.508609771728516,
      "citing_paper_id": "270067618",
      "cited_paper_id": 257921922
    },
    {
      "context_text": "For comparison, we also show the results of a recent diffusion-based zero-shot image restoration method GDP [15].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions a method (GDP) but does not refer to any specific dataset. The context is about comparing results, not using a dataset.",
      "processing_time": 64.68805408477783,
      "citing_paper_id": "270067618",
      "cited_paper_id": 257921922
    },
    {
      "context_text": "Some recent work on accelerating diffusion networks such as one-step diffusion [34] provides possible solutions.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for accelerating diffusion networks.",
      "processing_time": 61.90614938735962,
      "citing_paper_id": "270067618",
      "cited_paper_id": 261697392
    },
    {
      "context_text": "…Peak Signal-to-Noise Ratio (PSNR), Structural Similarity (SSIM), and Learned Perceptual Image Patch Similarity (LPIPS) [56], as well as non-reference metrics such as the Underwater Colour Image Quality Evaluation Metric (UCIQE) [57], Underwater Image Quality Measure (UIQM) [58], and URanker [15].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions several metrics but does not refer to any specific datasets. Metrics are excluded according to the instructions.",
      "processing_time": 62.74721121788025,
      "citing_paper_id": "275789940",
      "cited_paper_id": 16568282
    },
    {
      "context_text": "Visual prior-based methods focus on adjusting image pixel values such as contrast, brightness, and saturation [8], [9], but fail to account for the physical degradation processes specific to underwater environments.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and limitations of visual prior-based approaches.",
      "processing_time": 62.39970326423645,
      "citing_paper_id": "275789940",
      "cited_paper_id": 17006174
    },
    {
      "context_text": "Non-physical model-based methods focus on enhancing image quality by adjusting pixel properties such as contrast and saturation [8], [24], [25], without considering the optical transmission process.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only general methods for enhancing image quality. No verifiable resources are identified.",
      "processing_time": 63.257137060165405,
      "citing_paper_id": "275789940",
      "cited_paper_id": 17006174
    },
    {
      "context_text": "Visual comparison of restoration results on the non-reference datasets: U45 [52], SQUID-16 [53], Challenge-60 [7], UCCS [54], and EUVP-330 [55]. contrast, and enhanced structural details.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "U45",
        "SQUID-16",
        "Challenge-60",
        "UCCS",
        "EUVP-330"
      ],
      "dataset_descriptions": {
        "U45": "Used for visual comparison of restoration results in underwater image enhancement, focusing on non-reference datasets to evaluate contrast and structural detail improvements.",
        "SQUID-16": "Used for visual comparison of restoration results in underwater image enhancement, focusing on non-reference datasets to evaluate contrast and structural detail improvements.",
        "Challenge-60": "Used for visual comparison of restoration results in underwater image enhancement, focusing on non-reference datasets to evaluate contrast and structural detail improvements.",
        "UCCS": "Used for visual comparison of restoration results in underwater image enhancement, focusing on non-reference datasets to evaluate contrast and structural detail improvements.",
        "EUVP-330": "Used for visual comparison of restoration results in underwater image enhancement, focusing on non-reference datasets to evaluate contrast and structural detail improvements."
      },
      "confidence_score": 1.0,
      "reasoning": "The citation mentions several datasets used for visual comparison of restoration results in underwater image enhancement. These datasets are specific and have clear identifiers.",
      "processing_time": 99.17266702651978,
      "citing_paper_id": "275789940",
      "cited_paper_id": 70350065
    },
    {
      "context_text": "Unpaired test datasets include U45 [52], SQUID-16 [53], Challenge-60 [7], UCCS [54], and EUVP-330 [55].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "U45",
        "SQUID-16",
        "Challenge-60",
        "UCCS",
        "EUVP-330"
      ],
      "dataset_descriptions": {
        "U45": "Used to evaluate underwater image enhancement techniques, focusing on real-world conditions under natural light.",
        "SQUID-16": "Used to assess the performance of image restoration methods in underwater environments, emphasizing natural lighting conditions.",
        "Challenge-60": "Used to benchmark image restoration algorithms in challenging underwater scenarios, particularly under natural light.",
        "UCCS": "Used to test the effectiveness of image enhancement techniques in diverse underwater settings, including natural lighting.",
        "EUVP-330": "Used to evaluate the robustness of image restoration methods in extensive underwater visual perception tasks, under natural light."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions specific datasets used for testing in the domain of underwater image enhancement. These datasets are clearly named and appear to be relevant to the research topic of image restoration.",
      "processing_time": 94.7705180644989,
      "citing_paper_id": "275789940",
      "cited_paper_id": 70350065
    },
    {
      "context_text": "NU2Net minor color casts on the Challenge-60 [7], UCCS [54], and EUVP-330 datasets [55].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Challenge-60",
        "UCCS",
        "EUVP-330"
      ],
      "dataset_descriptions": {
        "Challenge-60": "Used to evaluate NU2Net's performance on underwater image enhancement, focusing on color correction and visual quality under natural light conditions.",
        "UCCS": "Used to assess NU2Net's effectiveness in enhancing underwater images, particularly addressing minor color casts and overall image quality.",
        "EUVP-330": "Used to test NU2Net's capabilities in restoring underwater images, emphasizing the reduction of color casts and improvement of visual clarity."
      },
      "confidence_score": 1.0,
      "reasoning": "The citation mentions three specific datasets used to evaluate NU2Net's performance on image restoration tasks.",
      "processing_time": 83.62214374542236,
      "citing_paper_id": "275789940",
      "cited_paper_id": 70350065
    },
    {
      "context_text": "Reference images from the UIEB dataset are provided alongside each sample for comparison. heritage and landscapes [5].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "UIEB"
      ],
      "dataset_descriptions": {
        "UIEB": "Used to compare enhanced underwater images with reference images, focusing on visual quality and restoration effectiveness."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions the UIEB dataset, which is a specific dataset used for underwater image enhancement. It is used for comparison with reference images.",
      "processing_time": 69.16877627372742,
      "citing_paper_id": "275789940",
      "cited_paper_id": 197545095
    },
    {
      "context_text": "MMoE-UIR utilized a 4-level encoder-decoder architecture with [3, 5, 6, 6] MMoEBs and channel dimensions of [32, 64, 128, 256] for levels 1 through 4.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only architectural details of a model. No dataset names are present in the text.",
      "processing_time": 63.927095890045166,
      "citing_paper_id": "275789940",
      "cited_paper_id": 197545095
    },
    {
      "context_text": "Diffusion models (DM) [22], [23], initially developed for generative tasks, have demonstrated strong potential in image restoration by addressing inverse problems through progressive denoising.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only diffusion models and their application to image restoration.",
      "processing_time": 62.382946491241455,
      "citing_paper_id": "275789940",
      "cited_paper_id": 222140788
    },
    {
      "context_text": "Finally, to recover fine-grained details and mitigate information loss inherent in degradation priors, we leverage the powerful generative capabilities of diffusion models [22], [23].",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only the use of diffusion models for image restoration.",
      "processing_time": 62.040058612823486,
      "citing_paper_id": "275789940",
      "cited_paper_id": 222140788
    },
    {
      "context_text": "II, some early deep learning-based underwater image restoration (UIR) methods such as FUnIE [59] and S-UWnet [60], which employ simpler or shallower networks, yield unsatisfactory results.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models or methods. The context focuses on the performance of early deep learning-based underwater image restoration methods.",
      "processing_time": 64.67025446891785,
      "citing_paper_id": "275789940",
      "cited_paper_id": 230770098
    },
    {
      "context_text": "These include a traditional method MLLE [25] and the following deep learning models: WaterNet [7], FUnIE [59], S-UWNet [60], Ucolor [12], PUIE-Net [61], Ushape [13], PUGAN [14], NU2Net [15], Semi-UIR [26], GUPDM [18], HCLR-Net [16].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span mentions several deep learning models but does not refer to any specific datasets. The context is focused on comparing different methods and models rather than using a particular dataset.",
      "processing_time": 65.18853402137756,
      "citing_paper_id": "275789940",
      "cited_paper_id": 230770098
    },
    {
      "context_text": "These include a traditional method MLLE [25] and the following deep learning models: WaterNet [7], FUnIE [59], S-UWNet [60], Ucolor [12], PUIE-Net [61], Ushape [13], PUGAN [14], NU2Net [15], Semi-UIR [26], GUPDM [18], HCLR-Net [16].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span mentions several deep learning models but does not refer to any specific datasets. The context is focused on comparing different methods and models rather than using a particular dataset.",
      "processing_time": 65.18853402137756,
      "citing_paper_id": "275789940",
      "cited_paper_id": 267435558
    },
    {
      "context_text": "2(a), many of these methods rely on physical modeling [17], [18] or jointly optimized depth estimation networks [14], [19], [20], which present several challenges: 1) Depth estimation networks require paired ground-truth depth data for optimization, which is difficult and costly to obtain.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only the general challenge of obtaining paired ground-truth depth data.",
      "processing_time": 62.89707922935486,
      "citing_paper_id": "275789940",
      "cited_paper_id": 240584211
    },
    {
      "context_text": "…on Other Low-level Vision Tasks: To validate the effectiveness of the proposed UniUIR for low-light and backlit image enhancement tasks, we retrained UniUIR and compared its performance against image enhancement methods on the LOL-v1 [63], LOL-v2-real [64], SID [65], and BAID [66] datasets.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "LOL-v1",
        "LOL-v2-real",
        "SID",
        "BAID"
      ],
      "dataset_descriptions": {
        "LOL-v1": "Used to validate the effectiveness of UniUIR for low-light image enhancement, comparing performance against other methods.",
        "LOL-v2-real": "Used to validate the effectiveness of UniUIR for low-light image enhancement, comparing performance against other methods.",
        "SID": "Used to validate the effectiveness of UniUIR for low-light image enhancement, comparing performance against other methods.",
        "BAID": "Used to validate the effectiveness of UniUIR for backlit image enhancement, comparing performance against other methods."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions specific datasets used for validating the effectiveness of UniUIR on low-light and backlit image enhancement tasks.",
      "processing_time": 87.00406408309937,
      "citing_paper_id": "275789940",
      "cited_paper_id": 247494896
    },
    {
      "context_text": "Compared to task-specific [35]–[40] and general [41]–[45] restoration methods, all-in-one restoration offers advantages in model storage efficiency and practical applications.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to other papers. There are no clear identifiers for datasets, benchmarks, or other verifiable resources.",
      "processing_time": 64.85087394714355,
      "citing_paper_id": "275789940",
      "cited_paper_id": 257557425
    },
    {
      "context_text": "The depth maps and semantic segmentation results generated by Depth Anything V2 [21] and the Segment Anything Model [69] are presented in Rows 2 and 4, respectively.",
      "catation_intent": "none",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions 'Depth Anything V2' and 'Segment Anything Model', which are tools/models, not datasets. No specific datasets are mentioned.",
      "processing_time": 64.12633800506592,
      "citing_paper_id": "275789940",
      "cited_paper_id": 257952310
    },
    {
      "context_text": "The depth maps and semantic segmentation results generated by Depth Anything V2 [21] and the Segment Anything Model [69] are presented in Rows 2 and 4, respectively.",
      "catation_intent": "none",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions 'Depth Anything V2' and 'Segment Anything Model', which are tools/models, not datasets. No specific datasets are mentioned.",
      "processing_time": 64.12633800506592,
      "citing_paper_id": "275789940",
      "cited_paper_id": 270440448
    },
    {
      "context_text": "…performance on depth estimation tasks. ii) For image segmentation task, we used enhanced images from the T90 dataset produced by various UIR methods and directly applied the Segment Anything Model (SAM) [69] to evaluate the impact of underwater image restoration on segmentation performance.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "T90"
      ],
      "dataset_descriptions": {
        "T90": "Used to evaluate the impact of underwater image restoration on segmentation performance by applying the Segment Anything Model (SAM) to enhanced images from various UIR methods."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions the T90 dataset, which is used for evaluating the impact of underwater image restoration on segmentation performance.",
      "processing_time": 70.84527373313904,
      "citing_paper_id": "275789940",
      "cited_paper_id": 257952310
    },
    {
      "context_text": "Their capability to model complex data distributions has led to state-of-the-art results in low-level vision tasks such as super-resolution [46], [47] and low-light image enhancement [48], [49].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions low-level vision tasks such as super-resolution and low-light image enhancement but does not specify any datasets. The cited papers also do not mention specific datasets in their titles.",
      "processing_time": 65.70982956886292,
      "citing_paper_id": "275789940",
      "cited_paper_id": 258999258
    },
    {
      "context_text": "Their capability to model complex data distributions has led to state-of-the-art results in low-level vision tasks such as super-resolution [46], [47] and low-light image enhancement [48], [49].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions low-level vision tasks such as super-resolution and low-light image enhancement but does not specify any datasets. The cited papers also do not mention specific datasets in their titles.",
      "processing_time": 65.70982956886292,
      "citing_paper_id": "275789940",
      "cited_paper_id": 269005685
    },
    {
      "context_text": "Recently, the generative power of DM has attracted attention in underwater image restoration (UIR) [50], [51].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only the use of generative models (DM) for underwater image restoration (UIR). No dataset names are provided.",
      "processing_time": 65.08036613464355,
      "citing_paper_id": "275789940",
      "cited_paper_id": 261582781
    },
    {
      "context_text": "Recently, the generative power of DM has attracted attention in underwater image restoration (UIR) [50], [51].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only the use of generative models (DM) for underwater image restoration (UIR). No dataset names are provided.",
      "processing_time": 65.08036613464355,
      "citing_paper_id": "275789940",
      "cited_paper_id": 272326006
    },
    {
      "context_text": "For example, Tang et al. [50] proposed a diffusion-based underwater image enhancement method, while UIEDP [51] introduced a framework incorporating diffusion priors, formulating UIR as posterior sampling conditioned on degraded inputs.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions 'UIEDP' which is a framework, not a dataset. No other specific datasets are mentioned.",
      "processing_time": 63.194557666778564,
      "citing_paper_id": "275789940",
      "cited_paper_id": 261582781
    },
    {
      "context_text": "For example, Tang et al. [50] proposed a diffusion-based underwater image enhancement method, while UIEDP [51] introduced a framework incorporating diffusion priors, formulating UIR as posterior sampling conditioned on degraded inputs.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions 'UIEDP' which is a framework, not a dataset. No other specific datasets are mentioned.",
      "processing_time": 63.194557666778564,
      "citing_paper_id": "275789940",
      "cited_paper_id": 272326006
    },
    {
      "context_text": "9 (fourth row, yellow dashed box), only HCLR-Net [16] and our UniUIR correctly identified the background diver’s fins, despite occlusion by foreground fish.",
      "catation_intent": "reusable resource",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a comparison between HCLR-Net and UniUIR in identifying background diver’s fins in underwater images.",
      "processing_time": 64.94164109230042,
      "citing_paper_id": "275789940",
      "cited_paper_id": 267435558
    },
    {
      "context_text": "Recently, deep learning-based approaches [7], [12]–[16] have emerged, demonstrating notable improvements through end-to-end optimization.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to deep learning approaches. No verifiable resources are identified.",
      "processing_time": 63.3948278427124,
      "citing_paper_id": "275789940",
      "cited_paper_id": 267435558
    },
    {
      "context_text": "NU2Net [15] and HCLR-Net [16] occasionally introduce chromatic aberrations, as seen in the image 2 and image 4 samples of Fig.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models (NU2Net and HCLR-Net) and their performance issues. No verifiable resources are identified.",
      "processing_time": 65.32952666282654,
      "citing_paper_id": "275789940",
      "cited_paper_id": 267435558
    },
    {
      "context_text": "Semi-UIR [26] combines contrastive and semi-supervised learning to improve performance and robustness, and HCLR-net [16] uses hybrid contrastive learning with locally randomized perturbations to enhance image quality.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models and methods. The context focuses on the methodologies used for underwater image enhancement.",
      "processing_time": 63.73343372344971,
      "citing_paper_id": "275789940",
      "cited_paper_id": 267435558
    },
    {
      "context_text": "Although MLLE [25], NU2Net [15], and HCLR-Net [16] tend to achieve higher UCIQE and UIQM scores, Fig.",
      "catation_intent": "findings",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models and metrics. The context is focused on comparing performance scores of different methods.",
      "processing_time": 63.930474281311035,
      "citing_paper_id": "275789940",
      "cited_paper_id": 267435558
    },
    {
      "context_text": "Thanks to its effectiveness, the proposed UniUIR outperforms recent state-of-the-art (SOTA) UIR methods, including Semi-UIR [26] and HCLR-Net [16].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models or methods. The context focuses on comparing the performance of UniUIR against other methods.",
      "processing_time": 64.19511651992798,
      "citing_paper_id": "275789940",
      "cited_paper_id": 267435558
    },
    {
      "context_text": "Recently, deep learning–based methods [7], [12], [13], [15], [16], [18], [19], [26]–[28] have shown significant potential in underwater image restoration (UIR).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to deep learning-based methods for underwater image restoration.",
      "processing_time": 62.91626334190369,
      "citing_paper_id": "275789940",
      "cited_paper_id": 267435558
    },
    {
      "context_text": "Moreover, compared to the all-in-one SOTA method PromptIR [31] and the Mamba-based image restoration method MambaIR [43], UniUIR consistently achieves superior results across six metrics on two datasets.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.3,
      "reasoning": "The context mentions 'two datasets' but does not provide specific names. The cited paper titles do not help in identifying specific datasets.",
      "processing_time": 63.818368673324585,
      "citing_paper_id": "275789940",
      "cited_paper_id": 267938238
    },
    {
      "context_text": "Additionally, we compared our approach with the all-in-one image restoration method PromptIR [31] and the MambaIR [43] based on the Mamba architecture [62].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions methods (PromptIR, MambaIR) but does not reference any specific datasets. The context focuses on comparing approaches rather than using datasets.",
      "processing_time": 64.65621733665466,
      "citing_paper_id": "275789940",
      "cited_paper_id": 267938238
    },
    {
      "context_text": "…We investigate the impact of restored underwater images on downstream tasks, including depth estimation and image segmentation. i) For depth estimation task, we employed the Depth Anything V2 [21] model to extract depth maps from restored images and compared the performance of various UIR methods.",
      "catation_intent": "reusable resource",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions the use of the Depth Anything V2 model, but it does not refer to a specific dataset. The model is used for extracting depth maps from restored images.",
      "processing_time": 65.62927961349487,
      "citing_paper_id": "275789940",
      "cited_paper_id": 270440448
    },
    {
      "context_text": "Additionally, to address inconsistencies between distortions across different image regions, we incorporate scene depth information using the Depth Anything V2 [21] model, enabling the network to better distinguish and focus on visual discrepancies between foreground and background elements.",
      "catation_intent": "reusable resource",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions the use of the Depth Anything V2 model, which is not a dataset but a method or tool. No specific dataset is mentioned.",
      "processing_time": 64.83320808410645,
      "citing_paper_id": "275789940",
      "cited_paper_id": 270440448
    },
    {
      "context_text": "Depth information was extracted using the large version of the Depth Anything V2 model [21].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions 'Depth Anything V2' but does not refer to it as a dataset. It is likely a model or method, which is excluded.",
      "processing_time": 64.66652035713196,
      "citing_paper_id": "275789940",
      "cited_paper_id": 270440448
    },
    {
      "context_text": "Next, we calculate the probability ˆ p ( c i | I ) and obtain the output text embedding of BIQA e auto by: During the optimization of BIQA, we freeze the parameters of the text encoder E T and finetune the image encoder E I using the multi-class fidelity loss [55].",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (fidelity loss) used in the optimization process.",
      "processing_time": 62.89725470542908,
      "citing_paper_id": "265658862",
      "cited_paper_id": 190906
    },
    {
      "context_text": "For super-resolution, we follow previous practice and train AutoDIR with DIV2K [2] and Flickr2K [31] training sets, with jointly downsampled images at x8 and x4 scales as inputs.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "DIV2K",
        "Flickr2K"
      ],
      "dataset_descriptions": {
        "DIV2K": "Used for training AutoDIR in super-resolution tasks, specifically with jointly downsampled images at x8 and x4 scales as inputs.",
        "Flickr2K": "Used for training AutoDIR in super-resolution tasks, specifically with jointly downsampled images at x8 and x4 scales as inputs."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions specific datasets used for training in the context of super-resolution, which are relevant to the topic of image restoration.",
      "processing_time": 77.75262880325317,
      "citing_paper_id": "265658862",
      "cited_paper_id": 4493958
    },
    {
      "context_text": "These include SIDD [1], CBSD68 [34], DIV2K [2], GoPro [37], HIDE [51], LOL [65], SOTS-Outdoor [27], Rain100 [69], and Raindrop [41], each corresponding to their respective tasks.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "SIDD",
        "CBSD68",
        "DIV2K",
        "GoPro",
        "HIDE",
        "LOL",
        "SOTS-Outdoor",
        "Rain100",
        "Raindrop"
      ],
      "dataset_descriptions": {
        "SIDD": "Used for single image denoising, providing real-world noisy images and ground truth clean images for training and evaluation.",
        "CBSD68": "Used for image denoising, consisting of 68 clean images for benchmarking denoising algorithms.",
        "DIV2K": "Used for super-resolution, containing high-quality images for training and testing super-resolution models.",
        "GoPro": "Used for video deblurring, providing sharp and blurred image pairs for training and evaluating deblurring algorithms.",
        "HIDE": "Used for image dehazing, offering hazy images and corresponding clear images for dehazing tasks.",
        "LOL": "Used for low-light image enhancement, providing low-light and normal-light image pairs for training and evaluation.",
        "SOTS-Outdoor": "Used for image dehazing, containing synthetic and real outdoor hazy images for benchmarking dehazing methods.",
        "Rain100": "Used for rain removal, providing rainy and clean image pairs for training and evaluating rain removal algorithms.",
        "Raindrop": "Used for raindrop removal, offering images with raindrops and corresponding clean images for training and evaluation."
      },
      "confidence_score": 1.0,
      "reasoning": "The context lists several datasets used for various image restoration tasks, which align with the research topic of all-in-one image restoration.",
      "processing_time": 110.75331091880798,
      "citing_paper_id": "265658862",
      "cited_paper_id": 4493958
    },
    {
      "context_text": "These include SIDD [1], CBSD68 [34], DIV2K [2], GoPro [37], HIDE [51], LOL [65], SOTS-Outdoor [27], Rain100 [69], and Raindrop [41], each corresponding to their respective tasks.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "SIDD",
        "CBSD68",
        "DIV2K",
        "GoPro",
        "HIDE",
        "LOL",
        "SOTS-Outdoor",
        "Rain100",
        "Raindrop"
      ],
      "dataset_descriptions": {
        "SIDD": "Used for single image denoising, providing real-world noisy images and ground truth clean images for training and evaluation.",
        "CBSD68": "Used for image denoising, consisting of 68 clean images for benchmarking denoising algorithms.",
        "DIV2K": "Used for super-resolution, containing high-quality images for training and testing super-resolution models.",
        "GoPro": "Used for video deblurring, providing sharp and blurred image pairs for training and evaluating deblurring algorithms.",
        "HIDE": "Used for image dehazing, offering hazy images and corresponding clear images for dehazing tasks.",
        "LOL": "Used for low-light image enhancement, providing low-light and normal-light image pairs for training and evaluation.",
        "SOTS-Outdoor": "Used for image dehazing, containing synthetic and real outdoor hazy images for benchmarking dehazing methods.",
        "Rain100": "Used for rain removal, providing rainy and clean image pairs for training and evaluating rain removal algorithms.",
        "Raindrop": "Used for raindrop removal, offering images with raindrops and corresponding clean images for training and evaluation."
      },
      "confidence_score": 1.0,
      "reasoning": "The context lists several datasets used for various image restoration tasks, which align with the research topic of all-in-one image restoration.",
      "processing_time": 110.75331091880798,
      "citing_paper_id": "265658862",
      "cited_paper_id": 39760169
    },
    {
      "context_text": "For denoising, we use SIDD [1] and a synthetic noise dataset with DIV2K [2] and Flickr2K [31].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "SIDD",
        "DIV2K",
        "Flickr2K"
      ],
      "dataset_descriptions": {
        "SIDD": "Used for denoising experiments, providing real-world noisy images paired with ground truth clean images.",
        "DIV2K": "Used in conjunction with a synthetic noise dataset for denoising, offering high-resolution images for training and evaluation.",
        "Flickr2K": "Used in conjunction with a synthetic noise dataset for denoising, providing a diverse set of images for training and evaluation."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions specific datasets used for denoising experiments, including SIDD, DIV2K, and Flickr2K. These are clearly identified and used for training and evaluation.",
      "processing_time": 83.6014461517334,
      "citing_paper_id": "265658862",
      "cited_paper_id": 4493958
    },
    {
      "context_text": "In addition, for completeness, we also report PSNR and LPIPS [73].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only metrics (PSNR and LPIPS). LPIPS is a metric, not a dataset, and thus should not be included.",
      "processing_time": 65.51624011993408,
      "citing_paper_id": "265658862",
      "cited_paper_id": 4766599
    },
    {
      "context_text": "Early works [54, 67, 70, 71, 80] on text-to-image synthesis and editing primarily relied on generative adversarial networks (GANs) [16].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only generative adversarial networks (GANs) and their application in text-to-image synthesis and editing.",
      "processing_time": 64.35775852203369,
      "citing_paper_id": "265658862",
      "cited_paper_id": 8858625
    },
    {
      "context_text": "Early works [54, 67, 70, 71, 80] on text-to-image synthesis and editing primarily relied on generative adversarial networks (GANs) [16].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only generative adversarial networks (GANs) and their application in text-to-image synthesis and editing.",
      "processing_time": 64.35775852203369,
      "citing_paper_id": "265658862",
      "cited_paper_id": 91183909
    },
    {
      "context_text": "Early works [54, 67, 70, 71, 80] on text-to-image synthesis and editing primarily relied on generative adversarial networks (GANs) [16].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only generative adversarial networks (GANs) and their application in text-to-image synthesis and editing.",
      "processing_time": 64.35775852203369,
      "citing_paper_id": "265658862",
      "cited_paper_id": 235742820
    },
    {
      "context_text": "In addition, we use GoPro [37], LOL [65], RESIDE [27], Rain200L [69], and Raindrop [41] for deblurring, low-light enhancement, dehazing, deraining, and deraindrop, respectively.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "GoPro",
        "LOL",
        "RESIDE",
        "Rain200L",
        "Raindrop"
      ],
      "dataset_descriptions": {
        "GoPro": "Used for deblurring, focusing on motion blur correction in high-resolution images.",
        "LOL": "Used for low-light enhancement, addressing the challenge of improving visibility in dark images.",
        "RESIDE": "Used for dehazing, evaluating methods to remove atmospheric haze from images.",
        "Rain200L": "Used for deraining, testing the effectiveness of rain removal algorithms in synthetic rain images.",
        "Raindrop": "Used for deraindrop, assessing the performance of removing raindrops from images captured through wet surfaces."
      },
      "confidence_score": 1.0,
      "reasoning": "The citation mentions specific datasets used for various image restoration tasks, which align with the research topic of all-in-one image restoration.",
      "processing_time": 88.45452857017517,
      "citing_paper_id": "265658862",
      "cited_paper_id": 39760169
    },
    {
      "context_text": "…etc. Previous image restoration methods often address these image degradations separately, including de-blurring [22, 66], denoising [44, 76] deraining [23, 47], super-resolution [9, 60], low-light enhancement [36, 74], deraindrop [42], and dehaze [43, 53], by using specific single-task models.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various image restoration tasks and methods. No verifiable resources are identified.",
      "processing_time": 63.47152757644653,
      "citing_paper_id": "265658862",
      "cited_paper_id": 59316941
    },
    {
      "context_text": "…etc. Previous image restoration methods often address these image degradations separately, including de-blurring [22, 66], denoising [44, 76] deraining [23, 47], super-resolution [9, 60], low-light enhancement [36, 74], deraindrop [42], and dehaze [43, 53], by using specific single-task models.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various image restoration tasks and methods. No verifiable resources are identified.",
      "processing_time": 63.47152757644653,
      "citing_paper_id": "265658862",
      "cited_paper_id": 258615282
    },
    {
      "context_text": "…etc. Previous image restoration methods often address these image degradations separately, including de-blurring [22, 66], denoising [44, 76] deraining [23, 47], super-resolution [9, 60], low-light enhancement [36, 74], deraindrop [42], and dehaze [43, 53], by using specific single-task models.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various image restoration tasks and methods. No verifiable resources are identified.",
      "processing_time": 63.47152757644653,
      "citing_paper_id": "265658862",
      "cited_paper_id": 261244427
    },
    {
      "context_text": "Generative priors for image restoration have been widely exploited for image restoration, in the form of generative adversarial networks (GANs) inversion and filters [3, 5, 17, 20, 35, 39], VQ cookbooks [8, 32, 57, 77], latent diffusion models [15, 24, 64].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods and models. No dataset names are present in the text.",
      "processing_time": 63.089401960372925,
      "citing_paper_id": "265658862",
      "cited_paper_id": 196834421
    },
    {
      "context_text": "Generative priors for image restoration have been widely exploited for image restoration, in the form of generative adversarial networks (GANs) inversion and filters [3, 5, 17, 20, 35, 39], VQ cookbooks [8, 32, 57, 77], latent diffusion models [15, 24, 64].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods and models. No dataset names are present in the text.",
      "processing_time": 63.089401960372925,
      "citing_paper_id": "265658862",
      "cited_paper_id": 250264643
    },
    {
      "context_text": "Generative priors for image restoration have been widely exploited for image restoration, in the form of generative adversarial networks (GANs) inversion and filters [3, 5, 17, 20, 35, 39], VQ cookbooks [8, 32, 57, 77], latent diffusion models [15, 24, 64].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods and models. No dataset names are present in the text.",
      "processing_time": 63.089401960372925,
      "citing_paper_id": "265658862",
      "cited_paper_id": 254125609
    },
    {
      "context_text": "Unified All-in-One Image Restoration Previous approaches for unified all-in-one image restoration can be categorized into two main groups: unsupervised generative prior-based methods [3, 5, 15, 20, 24, 35, 39, 64, 68] and end-to-end supervised learning-based methods [12, 28, 30, 40, 56, 72].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only categories of methods. No dataset names are present in the text.",
      "processing_time": 63.086496353149414,
      "citing_paper_id": "265658862",
      "cited_paper_id": 196834421
    },
    {
      "context_text": "Unified All-in-One Image Restoration Previous approaches for unified all-in-one image restoration can be categorized into two main groups: unsupervised generative prior-based methods [3, 5, 15, 20, 24, 35, 39, 64, 68] and end-to-end supervised learning-based methods [12, 28, 30, 40, 56, 72].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only categories of methods. No dataset names are present in the text.",
      "processing_time": 63.086496353149414,
      "citing_paper_id": "265658862",
      "cited_paper_id": 250581068
    },
    {
      "context_text": "Unified All-in-One Image Restoration Previous approaches for unified all-in-one image restoration can be categorized into two main groups: unsupervised generative prior-based methods [3, 5, 15, 20, 24, 35, 39, 64, 68] and end-to-end supervised learning-based methods [12, 28, 30, 40, 56, 72].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only categories of methods. No dataset names are present in the text.",
      "processing_time": 63.086496353149414,
      "citing_paper_id": "265658862",
      "cited_paper_id": 254125609
    },
    {
      "context_text": "To show that AutoDIR is able to handle multiple unknown image degradations, we conducted experiments on two additional datasets: the Under-Display-Camera (TOLED) dataset [78] and the Enhancing Underwater Visual Perception (EUVP) dataset [21].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Under-Display-Camera (TOLED) dataset",
        "Enhancing Underwater Visual Perception (EUVP) dataset"
      ],
      "dataset_descriptions": {
        "Under-Display-Camera (TOLED) dataset": "Used to test AutoDIR's ability to handle under-display camera image degradations, focusing on restoration quality and robustness.",
        "Enhancing Underwater Visual Perception (EUVP) dataset": "Used to evaluate AutoDIR's performance in enhancing underwater images, assessing its effectiveness in improving visual perception in challenging underwater conditions."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions two specific datasets used for experiments to demonstrate the capability of AutoDIR in handling multiple unknown image degradations.",
      "processing_time": 80.64914798736572,
      "citing_paper_id": "265658862",
      "cited_paper_id": 212647851
    },
    {
      "context_text": "…distortion, etc. Previous image restoration methods often address these image degradations separately, including de-blurring [22, 66], denoising [44, 76] deraining [23, 47], super-resolution [9, 60], low-light enhancement [36, 74], deraindrop [42], and dehaze [43, 53], by using specific…",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various image restoration tasks and methods. No clear, verifiable datasets are identified.",
      "processing_time": 63.82896685600281,
      "citing_paper_id": "265658862",
      "cited_paper_id": 219619080
    },
    {
      "context_text": "For the super-resolution and deblurring task, we adopt the widely used blind perceptual metrics including CLIP-IQA [59] and MUSIQ [26] to evaluate the quality of generated images following [6, 60, 63].",
      "catation_intent": "reusable resource",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions perceptual metrics (CLIP-IQA and MUSIQ) but does not refer to any specific datasets. These metrics are used to evaluate image quality in super-resolution and deblurring tasks.",
      "processing_time": 66.10012769699097,
      "citing_paper_id": "265658862",
      "cited_paper_id": 236171006
    },
    {
      "context_text": "For the super-resolution and deblurring task, we adopt the widely used blind perceptual metrics including CLIP-IQA [59] and MUSIQ [26] to evaluate the quality of generated images following [6, 60, 63].",
      "catation_intent": "reusable resource",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions perceptual metrics (CLIP-IQA and MUSIQ) but does not refer to any specific datasets. These metrics are used to evaluate image quality in super-resolution and deblurring tasks.",
      "processing_time": 66.10012769699097,
      "citing_paper_id": "265658862",
      "cited_paper_id": 258615282
    },
    {
      "context_text": "Inspired by prior work on vision foundation models [52, 62, 81], we conduct experiments on the unseen de-snowing task to evaluate this hypothesis.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a task (de-snowing) and a hypothesis evaluation. No verifiable resources are named.",
      "processing_time": 64.25917744636536,
      "citing_paper_id": "265658862",
      "cited_paper_id": 244799261
    },
    {
      "context_text": "Inspired by prior work on vision foundation models [52, 62, 81], we conduct experiments on the unseen de-snowing task to evaluate this hypothesis.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a task (de-snowing) and a hypothesis evaluation. No verifiable resources are named.",
      "processing_time": 64.25917744636536,
      "citing_paper_id": "265658862",
      "cited_paper_id": 244954250
    },
    {
      "context_text": "Recently, developments in diffusion models [19, 48] and language-vision models [45] have led to significant progress in text-based image synthesis and editing [7, 38, 46, 50].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only models and methods. No dataset names are present in the text.",
      "processing_time": 62.97879600524902,
      "citing_paper_id": "265658862",
      "cited_paper_id": 245335086
    },
    {
      "context_text": "Recently, developments in diffusion models [19, 48] and language-vision models [45] have led to significant progress in text-based image synthesis and editing [7, 38, 46, 50].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only models and methods. No dataset names are present in the text.",
      "processing_time": 62.97879600524902,
      "citing_paper_id": "265658862",
      "cited_paper_id": 256416326
    },
    {
      "context_text": "The loss function for SCM is further defined as: GT SD [4 8 ] Ours Input Figure 7.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only a figure and loss function. No verifiable resources are identified.",
      "processing_time": 62.98419809341431,
      "citing_paper_id": "265658862",
      "cited_paper_id": 250264643
    },
    {
      "context_text": "The emergence of diffusion-based methods has offered new avenues for text-to-image editing, which can be broadly categorized into data-driven [4] and no-extra-data-required approaches [18, 25, 75].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only categories of approaches. No dataset names are present in the text.",
      "processing_time": 63.21833896636963,
      "citing_paper_id": "265658862",
      "cited_paper_id": 252918469
    },
    {
      "context_text": "The emergence of diffusion-based methods has offered new avenues for text-to-image editing, which can be broadly categorized into data-driven [4] and no-extra-data-required approaches [18, 25, 75].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only categories of approaches. No dataset names are present in the text.",
      "processing_time": 63.21833896636963,
      "citing_paper_id": "265658862",
      "cited_paper_id": 254408758
    },
    {
      "context_text": "For instance, recent works interpolate the text embedding [25] or score estimate [75] of the input and desired images.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only methods or models. The cited papers' titles also do not indicate the use of specific datasets.",
      "processing_time": 64.01116371154785,
      "citing_paper_id": "265658862",
      "cited_paper_id": 252918469
    },
    {
      "context_text": "For instance, recent works interpolate the text embedding [25] or score estimate [75] of the input and desired images.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only methods or models. The cited papers' titles also do not indicate the use of specific datasets.",
      "processing_time": 64.01116371154785,
      "citing_paper_id": "265658862",
      "cited_paper_id": 254408758
    },
    {
      "context_text": "Benchmarks and evaluation metric: For the image denoising task, we conduct testing on the BSD68 [36] dataset.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "BSD68"
      ],
      "dataset_descriptions": {
        "BSD68": "Used to test image denoising algorithms, focusing on performance evaluation and benchmarking."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions the BSD68 dataset, which is a specific, verifiable dataset used for image denoising tasks.",
      "processing_time": 68.18047499656677,
      "citing_paper_id": "270869793",
      "cited_paper_id": 64193
    },
    {
      "context_text": "Denoising Deblurring Methods BSD [36] Urban100 [21] GoPro [37] HIDE [49] Plain Text Commands.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "BSD",
        "Urban100",
        "GoPro",
        "HIDE"
      ],
      "dataset_descriptions": {
        "BSD": "Used for denoising and deblurring experiments, providing a diverse set of natural images with ground truth annotations.",
        "Urban100": "Used for evaluating image restoration methods, focusing on high-resolution urban scenes with detailed textures.",
        "GoPro": "Used for deblurring tasks, containing real-world motion-blurred images captured with a GoPro camera.",
        "HIDE": "Used for denoising and deblurring, featuring a large collection of images with various types of noise and blur."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions several datasets that are commonly used in image restoration tasks, which align with the research topic of all-in-one image restoration.",
      "processing_time": 86.06591439247131,
      "citing_paper_id": "270869793",
      "cited_paper_id": 64193
    },
    {
      "context_text": "Denoising Deblurring Methods BSD [36] Urban100 [21] GoPro [37] HIDE [49] Plain Text Commands.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "BSD",
        "Urban100",
        "GoPro",
        "HIDE"
      ],
      "dataset_descriptions": {
        "BSD": "Used for denoising and deblurring experiments, providing a diverse set of natural images with ground truth annotations.",
        "Urban100": "Used for evaluating image restoration methods, focusing on high-resolution urban scenes with detailed textures.",
        "GoPro": "Used for deblurring tasks, containing real-world motion-blurred images captured with a GoPro camera.",
        "HIDE": "Used for denoising and deblurring, featuring a large collection of images with various types of noise and blur."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions several datasets that are commonly used in image restoration tasks, which align with the research topic of all-in-one image restoration.",
      "processing_time": 86.06591439247131,
      "citing_paper_id": "270869793",
      "cited_paper_id": 8282555
    },
    {
      "context_text": "We evaluate the quality of generated images using four metrics: LPIPS(Learned Perceptual Image Patch Similarity) [64], DISTS(Deep Image Structure and Texture Similarity) [15], FID(Fréchet Inception Distance) [19], and IS(Inception Score) [48].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions several metrics but does not refer to any specific datasets. Metrics are excluded as per the instructions.",
      "processing_time": 62.40824246406555,
      "citing_paper_id": "270869793",
      "cited_paper_id": 326772
    },
    {
      "context_text": "We evaluate the quality of generated images using four metrics: LPIPS(Learned Perceptual Image Patch Similarity) [64], DISTS(Deep Image Structure and Texture Similarity) [15], FID(Fréchet Inception Distance) [19], and IS(Inception Score) [48].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions several metrics but does not refer to any specific datasets. Metrics are excluded as per the instructions.",
      "processing_time": 62.40824246406555,
      "citing_paper_id": "270869793",
      "cited_paper_id": 4766599
    },
    {
      "context_text": "We evaluate the quality of generated images using four metrics: LPIPS(Learned Perceptual Image Patch Similarity) [64], DISTS(Deep Image Structure and Texture Similarity) [15], FID(Fréchet Inception Distance) [19], and IS(Inception Score) [48].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions several metrics but does not refer to any specific datasets. Metrics are excluded as per the instructions.",
      "processing_time": 62.40824246406555,
      "citing_paper_id": "270869793",
      "cited_paper_id": 215785896
    },
    {
      "context_text": "We evaluate the quality of generated images using four metrics: LPIPS(Learned Perceptual Image Patch Similarity) [64], DISTS(Deep Image Structure and Texture Similarity) [15], FID(Fréchet Inception Distance) [19], and IS(Inception Score) [48].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions several metrics but does not refer to any specific datasets. Metrics are excluded as per the instructions.",
      "processing_time": 62.40824246406555,
      "citing_paper_id": "270869793",
      "cited_paper_id": null
    },
    {
      "context_text": "In the image dehazing task, we utilize the standard outdoor test set of the SOTS [25] dataset, which consists of 500 images for testing.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "SOTS"
      ],
      "dataset_descriptions": {
        "SOTS": "Used for testing in the image dehazing task, consisting of 500 outdoor images to evaluate dehazing algorithms."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions the SOTS dataset, which is used for testing in the image dehazing task. The dataset is clearly identified and its purpose is specified.",
      "processing_time": 69.70098996162415,
      "citing_paper_id": "270869793",
      "cited_paper_id": 39760169
    },
    {
      "context_text": "[25] is reported.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not provide any specific information about datasets or their usage. There are no multi-word proper nouns, acronyms, or other specific identifiers that could be considered datasets.",
      "processing_time": 64.73561429977417,
      "citing_paper_id": "270869793",
      "cited_paper_id": 39760169
    },
    {
      "context_text": "The effectiveness of Transformers [54] has been verified on various vision tasks, including image classification [16], object detection [3], segmentation [66].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only tasks and models. The cited papers' titles do not introduce any specific datasets either.",
      "processing_time": 63.37631940841675,
      "citing_paper_id": "270869793",
      "cited_paper_id": 218889832
    },
    {
      "context_text": "The effectiveness of Transformers [54] has been verified on various vision tasks, including image classification [16], object detection [3], segmentation [66].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only tasks and models. The cited papers' titles do not introduce any specific datasets either.",
      "processing_time": 63.37631940841675,
      "citing_paper_id": "270869793",
      "cited_paper_id": 225039882
    },
    {
      "context_text": "The effectiveness of Transformers [54] has been verified on various vision tasks, including image classification [16], object detection [3], segmentation [66].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only tasks and models. The cited papers' titles do not introduce any specific datasets either.",
      "processing_time": 63.37631940841675,
      "citing_paper_id": "270869793",
      "cited_paper_id": 229924195
    },
    {
      "context_text": "For the image deraining task, we employ the Rain100L [59] dataset, which contains 100 pairs of original images and their corresponding rainy images.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Rain100L"
      ],
      "dataset_descriptions": {
        "Rain100L": "Used for the image deraining task, containing 100 pairs of original and rainy images to train and evaluate deraining algorithms."
      },
      "confidence_score": 1.0,
      "reasoning": "The context clearly mentions the use of the Rain100L dataset for the image deraining task, providing a specific dataset name and its purpose.",
      "processing_time": 70.55482339859009,
      "citing_paper_id": "270869793",
      "cited_paper_id": 219530930
    },
    {
      "context_text": "Implementation details: Our training set is a large combination of various datasets from multiple tasks, mainly following the datasets in All-in-One image restoration methods [39, 38, 4].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.3,
      "reasoning": "The context mentions 'various datasets from multiple tasks' but does not specify any particular dataset names. The cited papers do not provide additional specific dataset names.",
      "processing_time": 63.98126029968262,
      "citing_paper_id": "270869793",
      "cited_paper_id": 227239228
    },
    {
      "context_text": "Implementation details: Our training set is a large combination of various datasets from multiple tasks, mainly following the datasets in All-in-One image restoration methods [39, 38, 4].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.3,
      "reasoning": "The context mentions 'various datasets from multiple tasks' but does not specify any particular dataset names. The cited papers do not provide additional specific dataset names.",
      "processing_time": 63.98126029968262,
      "citing_paper_id": "270869793",
      "cited_paper_id": 260107992
    },
    {
      "context_text": "Some works [4, 31] and some recent developments [60, 56, 53] have also introduced transformer backbones to low-level image restoration tasks.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to transformer backbones in image restoration tasks.",
      "processing_time": 62.56082272529602,
      "citing_paper_id": "270869793",
      "cited_paper_id": 227239228
    },
    {
      "context_text": "Some works [4, 31] and some recent developments [60, 56, 53] have also introduced transformer backbones to low-level image restoration tasks.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to transformer backbones in image restoration tasks.",
      "processing_time": 62.56082272529602,
      "citing_paper_id": "270869793",
      "cited_paper_id": 244346144
    },
    {
      "context_text": "Some works [4, 31] and some recent developments [60, 56, 53] have also introduced transformer backbones to low-level image restoration tasks.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to transformer backbones in image restoration tasks.",
      "processing_time": 62.56082272529602,
      "citing_paper_id": "270869793",
      "cited_paper_id": 268819753
    },
    {
      "context_text": "As an improved version of IPT [4], IPT-V2 [53] achieves outstanding performance on low-level image restoration tasks, and thus being a good base model for our experiment.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only models and methods. The citation is focused on the performance of IPT-V2 as a base model for experiments.",
      "processing_time": 64.1516170501709,
      "citing_paper_id": "270869793",
      "cited_paper_id": 227239228
    },
    {
      "context_text": "As an improved version of IPT [4], IPT-V2 [53] achieves outstanding performance on low-level image restoration tasks, and thus being a good base model for our experiment.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only models and methods. The citation is focused on the performance of IPT-V2 as a base model for experiments.",
      "processing_time": 64.1516170501709,
      "citing_paper_id": "270869793",
      "cited_paper_id": 268819753
    },
    {
      "context_text": "Several transformer-based approaches [4, 26, 27, 38, 56] have been proposed to tackle various degradation restoration tasks.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only transformer-based approaches for image restoration tasks.",
      "processing_time": 62.18962121009827,
      "citing_paper_id": "270869793",
      "cited_paper_id": 227239228
    },
    {
      "context_text": "Several transformer-based approaches [4, 26, 27, 38, 56] have been proposed to tackle various degradation restoration tasks.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only transformer-based approaches for image restoration tasks.",
      "processing_time": 62.18962121009827,
      "citing_paper_id": "270869793",
      "cited_paper_id": 250551851
    },
    {
      "context_text": "Several transformer-based approaches [4, 26, 27, 38, 56] have been proposed to tackle various degradation restoration tasks.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only transformer-based approaches for image restoration tasks.",
      "processing_time": 62.18962121009827,
      "citing_paper_id": "270869793",
      "cited_paper_id": 260107992
    },
    {
      "context_text": "Among these methods, IPT [4] first introduced a standard transformer-based framework with multi-heads and multi-tails structure to handle multiple image restoration tasks.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (IPT) and its application to image restoration tasks.",
      "processing_time": 62.725526094436646,
      "citing_paper_id": "270869793",
      "cited_paper_id": 227239228
    },
    {
      "context_text": "Inspired by previous low-rank decomposition works [14, 65, 20], we resort to structured compression of biases by decomposing it into the multiplication of two low-rank matrices: for B task ∈ R n α × n β , given low-rank n γ ≪ min( n α , n β ) , we have To demonstrate the feasibility of rank-based…",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and mathematical concepts. There are no clear identifiers for datasets in the provided context.",
      "processing_time": 63.213078022003174,
      "citing_paper_id": "270869793",
      "cited_paper_id": 235458009
    },
    {
      "context_text": "Other transformer-based methods, such as Restormer [60] and IPTv2 [53], leveraged channel self-attention to capture longer-range dependencies.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions Restormer and IPTv2 as transformer-based methods for image restoration, but does not specify the use of any datasets. The focus is on the methods themselves.",
      "processing_time": 64.5655906200409,
      "citing_paper_id": "270869793",
      "cited_paper_id": 244346144
    },
    {
      "context_text": "Other transformer-based methods, such as Restormer [60] and IPTv2 [53], leveraged channel self-attention to capture longer-range dependencies.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions Restormer and IPTv2 as transformer-based methods for image restoration, but does not specify the use of any datasets. The focus is on the methods themselves.",
      "processing_time": 64.5655906200409,
      "citing_paper_id": "270869793",
      "cited_paper_id": 268819753
    },
    {
      "context_text": "Text-to-image generation that generates images based on the semantic meaning of texts has been widely studied in the realm of diffusion models [44, 46, 47, 63].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general research areas and models. No verifiable resources are identified.",
      "processing_time": 62.761093854904175,
      "citing_paper_id": "270869793",
      "cited_paper_id": 245335280
    },
    {
      "context_text": "Text-to-image generation that generates images based on the semantic meaning of texts has been widely studied in the realm of diffusion models [44, 46, 47, 63].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general research areas and models. No verifiable resources are identified.",
      "processing_time": 62.761093854904175,
      "citing_paper_id": "270869793",
      "cited_paper_id": 248986576
    },
    {
      "context_text": "Text-to-image generation that generates images based on the semantic meaning of texts has been widely studied in the realm of diffusion models [44, 46, 47, 63].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general research areas and models. No verifiable resources are identified.",
      "processing_time": 62.761093854904175,
      "citing_paper_id": "270869793",
      "cited_paper_id": 256827727
    },
    {
      "context_text": "All-in-One works like AirNet [26] and PromptIR [39] are indeed smarter in the sense that their backbones are universally applicable to .",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only other works (AirNet, PromptIR) which are methods/models. No datasets are referenced or used in the described context.",
      "processing_time": 64.29029726982117,
      "citing_paper_id": "270869793",
      "cited_paper_id": 250551851
    },
    {
      "context_text": "Among these methods [26, 18, 23, 30, 35, 39], contextual information might include images, degradation context, and input features.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods and contextual information. No verifiable resources are identified.",
      "processing_time": 62.32967758178711,
      "citing_paper_id": "270869793",
      "cited_paper_id": 250551851
    },
    {
      "context_text": "Among these methods [26, 18, 23, 30, 35, 39], contextual information might include images, degradation context, and input features.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods and contextual information. No verifiable resources are identified.",
      "processing_time": 62.32967758178711,
      "citing_paper_id": "270869793",
      "cited_paper_id": 259243623
    },
    {
      "context_text": "Among these methods [26, 18, 23, 30, 35, 39], contextual information might include images, degradation context, and input features.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods and contextual information. No verifiable resources are identified.",
      "processing_time": 62.32967758178711,
      "citing_paper_id": "270869793",
      "cited_paper_id": 266149517
    },
    {
      "context_text": "…are injected at each U-Net stage transition; InstructIR [10]: a series of task-specific weights perform channel-wise affine feature mapping; External-Control : inspired by ControlNet [63], a degradation-sensitive parallel encoder network is added that intervenes features in the original backbone.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods and models. There are no clear identifiers for datasets within the text.",
      "processing_time": 62.93285083770752,
      "citing_paper_id": "270869793",
      "cited_paper_id": 256827727
    },
    {
      "context_text": "As diffusion models have also been applied to low-level vision tasks for image restoration [1, 55, 57], text commands have been used to guide the model’s judgment on the type of image degradation beyond facilitating image content generation.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets by name. It only refers to the application of diffusion models and text commands in image restoration tasks.",
      "processing_time": 63.20540952682495,
      "citing_paper_id": "270869793",
      "cited_paper_id": 258615282
    },
    {
      "context_text": "As diffusion models have also been applied to low-level vision tasks for image restoration [1, 55, 57], text commands have been used to guide the model’s judgment on the type of image degradation beyond facilitating image content generation.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets by name. It only refers to the application of diffusion models and text commands in image restoration tasks.",
      "processing_time": 63.20540952682495,
      "citing_paper_id": "270869793",
      "cited_paper_id": 265466382
    },
    {
      "context_text": "We leverage UniControl [42] as a baseline for validating our approach within the diffusion model setting, UniControl is a multi-task unified framework based on Stable Diffusion, capable of handling diverse image generation tasks.",
      "catation_intent": "reusable resource",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions UniControl as a baseline for validating the approach, but it does not refer to a specific dataset. UniControl is described as a method or framework.",
      "processing_time": 64.18299078941345,
      "citing_paper_id": "270869793",
      "cited_paper_id": 258762776
    },
    {
      "context_text": "Previous measures [38] used unstructured-sparse biases for compression, but this involves the training of dense biases that bring huge additional costs.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only discusses methods and their limitations.",
      "processing_time": 61.191017150878906,
      "citing_paper_id": "270869793",
      "cited_paper_id": 260107992
    },
    {
      "context_text": "Park et al. [38] proposed ADMS, which employed adaptive unstructured-sparse filters with independent parameters for different degradations.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method called ADMS. The context focuses on the method's ability to handle different degradations, not on a particular dataset.",
      "processing_time": 64.44092321395874,
      "citing_paper_id": "270869793",
      "cited_paper_id": 260107992
    },
    {
      "context_text": "Previously, there are some methods that modifies weights for specific tasks [38, 67].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods for modifying weights for specific tasks.",
      "processing_time": 61.573227882385254,
      "citing_paper_id": "270869793",
      "cited_paper_id": 260107992
    },
    {
      "context_text": "TIP [41] employed degradation-related text instructions as well.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.2,
      "reasoning": "The citation does not mention any specific datasets, only a method or tool called TIP. The context is too vague to infer any dataset usage.",
      "processing_time": 63.36509585380554,
      "citing_paper_id": "270869793",
      "cited_paper_id": 266362278
    },
    {
      "context_text": "For instance, PromptSR [8] introduced additional text priors that encoded the way of degradation; Lin [32] proposed an SD-based image restoration method that introduces text-based degradation priors in addition to image-based content priors for multi-task image restoration.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions 'PromptSR' and 'SD-based image restoration method', which are methods, not datasets. No specific datasets are mentioned or used in the described research.",
      "processing_time": 63.86454200744629,
      "citing_paper_id": "270869793",
      "cited_paper_id": 266690738
    },
    {
      "context_text": "We align these feature-based methods to the IPT-V2 [53] model.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions 'IPT-V2' which is a model, not a dataset. No datasets are explicitly mentioned or used in the given context.",
      "processing_time": 63.28847408294678,
      "citing_paper_id": "270869793",
      "cited_paper_id": 268819753
    },
    {
      "context_text": "Our training hyperparameters largely follow IPTV2 [53].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a reference to another paper for training hyperparameters.",
      "processing_time": 61.691752672195435,
      "citing_paper_id": "270869793",
      "cited_paper_id": 268819753
    },
    {
      "context_text": "This is a complex inverse problem since multiple different solutions can exist for restoring any given image [16, 20, 44, 59, 102, 103].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only the complexity of the image restoration problem. No dataset names are present in the text.",
      "processing_time": 62.593785524368286,
      "citing_paper_id": "267320695",
      "cited_paper_id": 1724361
    },
    {
      "context_text": "This is a complex inverse problem since multiple different solutions can exist for restoring any given image [16, 20, 44, 59, 102, 103].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only the complexity of the image restoration problem. No dataset names are present in the text.",
      "processing_time": 62.593785524368286,
      "citing_paper_id": "267320695",
      "cited_paper_id": 1900475
    },
    {
      "context_text": "This is a complex inverse problem since multiple different solutions can exist for restoring any given image [16, 20, 44, 59, 102, 103].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only the complexity of the image restoration problem. No dataset names are present in the text.",
      "processing_time": 62.593785524368286,
      "citing_paper_id": "267320695",
      "cited_paper_id": 237266491
    },
    {
      "context_text": "Some methods focus on specific degradations, for instance reducing noise (denoising) [64, 102, 103], removing blur (deblurring) [58, 105], or clearing haze (dehazing) [16, 66].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only general methods and degradations. No dataset names are present in the text.",
      "processing_time": 62.55177283287048,
      "citing_paper_id": "267320695",
      "cited_paper_id": 1900475
    },
    {
      "context_text": "Some methods focus on specific degradations, for instance reducing noise (denoising) [64, 102, 103], removing blur (deblurring) [58, 105], or clearing haze (dehazing) [16, 66].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only general methods and degradations. No dataset names are present in the text.",
      "processing_time": 62.55177283287048,
      "citing_paper_id": "267320695",
      "cited_paper_id": 214802286
    },
    {
      "context_text": "We use NAFNet [10] as the image model, an efficient image restoration model that follows a U-Net architecture [68].",
      "catation_intent": "none",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions NAFNet and U-Net but does not refer to any specific dataset. Both NAFNet and U-Net are models, not datasets.",
      "processing_time": 63.12055444717407,
      "citing_paper_id": "267320695",
      "cited_paper_id": 3719281
    },
    {
      "context_text": "We use NAFNet [10] as the image model, an efficient image restoration model that follows a U-Net architecture [68].",
      "catation_intent": "none",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions NAFNet and U-Net but does not refer to any specific dataset. Both NAFNet and U-Net are models, not datasets.",
      "processing_time": 63.12055444717407,
      "citing_paper_id": "267320695",
      "cited_paper_id": 248085491
    },
    {
      "context_text": "Since our model does not perform upsampling, we use the Bicubic degradation model [1, 15] for generating the low-resolution images (LR), and the upsampled versions (HR) that are fed into our model to enhance them.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions the Bicubic degradation model but does not refer to a specific dataset. The model is used for generating low-resolution and high-resolution images, which is a method rather than a dataset.",
      "processing_time": 64.53890681266785,
      "citing_paper_id": "267320695",
      "cited_paper_id": 6593498
    },
    {
      "context_text": "We train use a batch size of 32 and AdamW [36] optimizer with learning rate 5 e − 4 for 500 epochs (approximately 1 day using a single NVIDIA A100).",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (AdamW optimizer).",
      "processing_time": 60.84391713142395,
      "citing_paper_id": "267320695",
      "cited_paper_id": 6628106
    },
    {
      "context_text": "Recent deep learning methods [16, 44, 58, 64, 74, 95] have shown consistently better results compared to traditional techniques for blind image restoration [18, 29, 35, 37, 54, 73].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to deep learning methods and traditional techniques for blind image restoration.",
      "processing_time": 61.87333369255066,
      "citing_paper_id": "267320695",
      "cited_paper_id": 7044126
    },
    {
      "context_text": "Recent deep learning methods [16, 44, 58, 64, 74, 95] have shown consistently better results compared to traditional techniques for blind image restoration [18, 29, 35, 37, 54, 73].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to deep learning methods and traditional techniques for blind image restoration.",
      "processing_time": 61.87333369255066,
      "citing_paper_id": "267320695",
      "cited_paper_id": 14877173
    },
    {
      "context_text": "Recent deep learning methods [16, 44, 58, 64, 74, 95] have shown consistently better results compared to traditional techniques for blind image restoration [18, 29, 35, 37, 54, 73].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to deep learning methods and traditional techniques for blind image restoration.",
      "processing_time": 61.87333369255066,
      "citing_paper_id": "267320695",
      "cited_paper_id": 14925285
    },
    {
      "context_text": "Recent deep learning methods [16, 44, 58, 64, 74, 95] have shown consistently better results compared to traditional techniques for blind image restoration [18, 29, 35, 37, 54, 73].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to deep learning methods and traditional techniques for blind image restoration.",
      "processing_time": 61.87333369255066,
      "citing_paper_id": "267320695",
      "cited_paper_id": 206769988
    },
    {
      "context_text": "Recent deep learning methods [16, 44, 58, 64, 74, 95] have shown consistently better results compared to traditional techniques for blind image restoration [18, 29, 35, 37, 54, 73].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to deep learning methods and traditional techniques for blind image restoration.",
      "processing_time": 61.87333369255066,
      "citing_paper_id": "267320695",
      "cited_paper_id": 237266491
    },
    {
      "context_text": "Recent deep learning methods [16, 44, 58, 64, 74, 95] have shown consistently better results compared to traditional techniques for blind image restoration [18, 29, 35, 37, 54, 73].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to deep learning methods and traditional techniques for blind image restoration.",
      "processing_time": 61.87333369255066,
      "citing_paper_id": "267320695",
      "cited_paper_id": 244346144
    },
    {
      "context_text": "We use the GoPro dataset for motion deblurring [57] which consist of 2103 images for training, and 1111 for testing.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "GoPro dataset"
      ],
      "dataset_descriptions": {
        "GoPro dataset": "Used for motion deblurring experiments, consisting of 2103 images for training and 1111 for testing, focusing on dynamic scene deblurring using a deep multi-scale convolutional neural network."
      },
      "confidence_score": 1.0,
      "reasoning": "The context clearly mentions the GoPro dataset, which is used for motion deblurring experiments. The dataset is described with specific details about its training and testing splits.",
      "processing_time": 72.99526715278625,
      "citing_paper_id": "267320695",
      "cited_paper_id": 8671030
    },
    {
      "context_text": "Deblurring GoPro [57] Dehazing SOTS [41] Method PSNR/SSIM Method PSNR/SSIM Limitations Our method achieves state-of-the-art results in five tasks, proving the potential of using instructions to guide deep blind restoration models.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "GoPro",
        "SOTS"
      ],
      "dataset_descriptions": {
        "GoPro": "Used for dynamic scene deblurring, evaluating the performance of deep multi-scale convolutional neural networks in restoring blurred images.",
        "SOTS": "Used for single-image dehazing, benchmarking the effectiveness of dehazing algorithms and comparing them against state-of-the-art methods."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions two datasets, GoPro and SOTS, which are used for deblurring and dehazing tasks, respectively. These are specific datasets relevant to image restoration.",
      "processing_time": 76.75990319252014,
      "citing_paper_id": "267320695",
      "cited_paper_id": 8671030
    },
    {
      "context_text": "Deblurring GoPro [57] Dehazing SOTS [41] Method PSNR/SSIM Method PSNR/SSIM Limitations Our method achieves state-of-the-art results in five tasks, proving the potential of using instructions to guide deep blind restoration models.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "GoPro",
        "SOTS"
      ],
      "dataset_descriptions": {
        "GoPro": "Used for dynamic scene deblurring, evaluating the performance of deep multi-scale convolutional neural networks in restoring blurred images.",
        "SOTS": "Used for single-image dehazing, benchmarking the effectiveness of dehazing algorithms and comparing them against state-of-the-art methods."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions two datasets, GoPro and SOTS, which are used for deblurring and dehazing tasks, respectively. These are specific datasets relevant to image restoration.",
      "processing_time": 76.75990319252014,
      "citing_paper_id": "267320695",
      "cited_paper_id": 39760169
    },
    {
      "context_text": "Additional high-resolution qualitative results using the LOL [83] dataset (low-light image enhancement), and the GoPro [57] dataset (motion deblurring).",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "LOL",
        "GoPro"
      ],
      "dataset_descriptions": {
        "LOL": "Used for low-light image enhancement, evaluating the effectiveness of the proposed method in improving image quality under low-light conditions.",
        "GoPro": "Used for motion deblurring, assessing the performance of the method in removing motion blur from images captured in dynamic scenes."
      },
      "confidence_score": 1.0,
      "reasoning": "The citation mentions two specific datasets, LOL and GoPro, which are used for low-light image enhancement and motion deblurring, respectively. These datasets are directly relevant to the research topic of all-in-one image restoration.",
      "processing_time": 77.27249574661255,
      "citing_paper_id": "267320695",
      "cited_paper_id": 8671030
    },
    {
      "context_text": "Additional high-resolution qualitative results using the LOL [83] dataset (low-light image enhancement), and the GoPro [57] dataset (motion deblurring).",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "LOL",
        "GoPro"
      ],
      "dataset_descriptions": {
        "LOL": "Used for low-light image enhancement, evaluating the effectiveness of the proposed method in improving image quality under low-light conditions.",
        "GoPro": "Used for motion deblurring, assessing the performance of the method in removing motion blur from images captured in dynamic scenes."
      },
      "confidence_score": 1.0,
      "reasoning": "The citation mentions two specific datasets, LOL and GoPro, which are used for low-light image enhancement and motion deblurring, respectively. These datasets are directly relevant to the research topic of all-in-one image restoration.",
      "processing_time": 77.27249574661255,
      "citing_paper_id": "267320695",
      "cited_paper_id": 52008443
    },
    {
      "context_text": "In Figure 10, we compare methods on the motion deblurring task using the GoPro [57] dataset.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "GoPro"
      ],
      "dataset_descriptions": {
        "GoPro": "Used to compare methods on the motion deblurring task, focusing on performance evaluation of dynamic scene deblurring techniques."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions the GoPro dataset, which is a specific, verifiable dataset used for motion deblurring tasks.",
      "processing_time": 68.03891706466675,
      "citing_paper_id": "267320695",
      "cited_paper_id": 8671030
    },
    {
      "context_text": "The proposed neural networks are based on convolutional neural networks (CNNs) and Transformers [76] (or related attention mechanisms).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models and methods. No verifiable resources are identified.",
      "processing_time": 61.068241357803345,
      "citing_paper_id": "267320695",
      "cited_paper_id": 13756489
    },
    {
      "context_text": "In Figure 11, we compare with different methods for the dehazing task on SOTS (outdoor) [41].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "SOTS (outdoor)"
      ],
      "dataset_descriptions": {
        "SOTS (outdoor)": "Used to evaluate and compare dehazing methods, focusing on outdoor scenes to assess performance in realistic conditions."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions the use of SOTS (outdoor) for comparing dehazing methods, which is a specific dataset used in image restoration.",
      "processing_time": 69.65900874137878,
      "citing_paper_id": "267320695",
      "cited_paper_id": 39760169
    },
    {
      "context_text": "Dehazing comparisons for all-in-one methods on images from the SOTS outdoor dataset [41].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "SOTS outdoor"
      ],
      "dataset_descriptions": {
        "SOTS outdoor": "Used for dehazing comparisons in all-in-one image restoration methods, focusing on outdoor image quality enhancement."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions the SOTS outdoor dataset, which is a specific dataset used for dehazing comparisons in image restoration.",
      "processing_time": 68.09421873092651,
      "citing_paper_id": "267320695",
      "cited_paper_id": 39760169
    },
    {
      "context_text": "We utilize the Reside (outdoor) SOTS [41] dataset, which contains ≈ 72 K training images.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "SOTS"
      ],
      "dataset_descriptions": {
        "SOTS": "Used to train models for single-image dehazing, focusing on outdoor scenes with approximately 72,000 training images."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions the SOTS dataset, which is a specific dataset used for single-image dehazing. It is clearly identified and used for training.",
      "processing_time": 69.6972725391388,
      "citing_paper_id": "267320695",
      "cited_paper_id": 39760169
    },
    {
      "context_text": "We use the LOL [83] dataset (v1), and we adopt its official split of 485 training images, and 15 testing images.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "LOL"
      ],
      "dataset_descriptions": {
        "LOL": "Used for low-light image enhancement, specifically adopting the official split of 485 training images and 15 testing images to evaluate the effectiveness of the proposed method."
      },
      "confidence_score": 1.0,
      "reasoning": "The context clearly mentions the use of the LOL dataset (v1) with a specific split for training and testing, which is relevant to the research topic of image restoration.",
      "processing_time": 70.94350218772888,
      "citing_paper_id": "267320695",
      "cited_paper_id": 52008443
    },
    {
      "context_text": "In Figure 9, we show results on the LOL [83] dataset.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "LOL"
      ],
      "dataset_descriptions": {
        "LOL": "Used to demonstrate low-light enhancement results, specifically applying deep retinex decomposition techniques to improve image quality in low-light conditions."
      },
      "confidence_score": 1.0,
      "reasoning": "The citation mentions the use of the LOL dataset, which is likely a specific dataset used for low-light image enhancement. The context indicates that the dataset is used to show results.",
      "processing_time": 69.4788830280304,
      "citing_paper_id": "267320695",
      "cited_paper_id": 52008443
    },
    {
      "context_text": "We use the Instruction Condition Block (ICB) for task-routing [71] only in the encoder and decoder.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a method (Instruction Condition Block) used for task-routing. The cited paper title confirms this is about a method, not a dataset.",
      "processing_time": 63.62357997894287,
      "citing_paper_id": "267320695",
      "cited_paper_id": 85544221
    },
    {
      "context_text": "Conventional task routing [71] applies task-specific binary masks to the channel features.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for task routing in machine learning.",
      "processing_time": 60.59492111206055,
      "citing_paper_id": "267320695",
      "cited_paper_id": 85544221
    },
    {
      "context_text": "As [71], task routing is applied as the channel-wise multiplication ⊙ for masking features depending on the task.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method or technique called 'task routing'.",
      "processing_time": 60.718416690826416,
      "citing_paper_id": "267320695",
      "cited_paper_id": 85544221
    },
    {
      "context_text": "Indirectly, this also enforces to learn diverse filters and reduce sparsity [14, 71].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only references to papers. No verifiable resources are identified.",
      "processing_time": 61.084067583084106,
      "citing_paper_id": "267320695",
      "cited_paper_id": 85544221
    },
    {
      "context_text": "Inspired in task routing for many-task learning [14, 69, 71], we propose an “Instruction Condition Block” (ICB) to enable task-specific transformations within the model.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method or approach for task routing in many-task learning.",
      "processing_time": 61.2567880153656,
      "citing_paper_id": "267320695",
      "cited_paper_id": 85544221
    },
    {
      "context_text": "Although we do not condition explicitly the filters of the neural network, as in [71], the mask allows the model to select the most relevant channels depending on the image information and the instruction.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method or approach. There are no clear identifiers for datasets in the citation context.",
      "processing_time": 61.82383751869202,
      "citing_paper_id": "267320695",
      "cited_paper_id": 85544221
    },
    {
      "context_text": "We illustrate our task-routing ICB block in Figure 4.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, models, or methods. It only refers to a figure illustrating a task-routing ICB block.",
      "processing_time": 62.20722961425781,
      "citing_paper_id": "267320695",
      "cited_paper_id": 119308964
    },
    {
      "context_text": "In Figure 12, we compare with image restoration methods for deraining on Rain100L [21].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Rain100L"
      ],
      "dataset_descriptions": {
        "Rain100L": "Used to compare image restoration methods for deraining, focusing on the effectiveness of different deraining techniques in the context of all-in-one image restoration."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions 'Rain100L' as a dataset used for comparing image restoration methods for deraining. The dataset name is specific and relevant to the research topic.",
      "processing_time": 71.82374739646912,
      "citing_paper_id": "267320695",
      "cited_paper_id": 195787503
    },
    {
      "context_text": "Following Zhang et al. [100], we compare InstructIR with several state-of-the-art methods for general image restoration [9, 10, 44, 93, 95], and all-in-one image restoration methods [21, 42, 45, 75, 100].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods and models for image restoration. No verifiable resources are identified.",
      "processing_time": 61.49927258491516,
      "citing_paper_id": "267320695",
      "cited_paper_id": 195787503
    },
    {
      "context_text": "Following Zhang et al. [100], we compare InstructIR with several state-of-the-art methods for general image restoration [9, 10, 44, 93, 95], and all-in-one image restoration methods [21, 42, 45, 75, 100].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods and models for image restoration. No verifiable resources are identified.",
      "processing_time": 61.49927258491516,
      "citing_paper_id": "267320695",
      "cited_paper_id": 234482841
    },
    {
      "context_text": "Following Zhang et al. [100], we compare InstructIR with several state-of-the-art methods for general image restoration [9, 10, 44, 93, 95], and all-in-one image restoration methods [21, 42, 45, 75, 100].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods and models for image restoration. No verifiable resources are identified.",
      "processing_time": 61.49927258491516,
      "citing_paper_id": "267320695",
      "cited_paper_id": 237266491
    },
    {
      "context_text": "Following Zhang et al. [100], we compare InstructIR with several state-of-the-art methods for general image restoration [9, 10, 44, 93, 95], and all-in-one image restoration methods [21, 42, 45, 75, 100].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods and models for image restoration. No verifiable resources are identified.",
      "processing_time": 61.49927258491516,
      "citing_paper_id": "267320695",
      "cited_paper_id": 244346144
    },
    {
      "context_text": "Following Zhang et al. [100], we compare InstructIR with several state-of-the-art methods for general image restoration [9, 10, 44, 93, 95], and all-in-one image restoration methods [21, 42, 45, 75, 100].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods and models for image restoration. No verifiable resources are identified.",
      "processing_time": 61.49927258491516,
      "citing_paper_id": "267320695",
      "cited_paper_id": 244714491
    },
    {
      "context_text": "Following Zhang et al. [100], we compare InstructIR with several state-of-the-art methods for general image restoration [9, 10, 44, 93, 95], and all-in-one image restoration methods [21, 42, 45, 75, 100].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods and models for image restoration. No verifiable resources are identified.",
      "processing_time": 61.49927258491516,
      "citing_paper_id": "267320695",
      "cited_paper_id": 248085491
    },
    {
      "context_text": "Following Zhang et al. [100], we compare InstructIR with several state-of-the-art methods for general image restoration [9, 10, 44, 93, 95], and all-in-one image restoration methods [21, 42, 45, 75, 100].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods and models for image restoration. No verifiable resources are identified.",
      "processing_time": 61.49927258491516,
      "citing_paper_id": "267320695",
      "cited_paper_id": 263784964
    },
    {
      "context_text": "We use a combination of BSD400 [2] and WED [50] datasets for training.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "BSD400",
        "WED"
      ],
      "dataset_descriptions": {
        "BSD400": "Used for training image restoration models, focusing on contour detection and hierarchical image segmentation.",
        "WED": "Used for training image restoration models, providing a diverse set of images for evaluation and testing."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions two specific datasets, BSD400 and WED, which are used for training in the research.",
      "processing_time": 72.10428738594055,
      "citing_paper_id": "267320695",
      "cited_paper_id": 206764694
    },
    {
      "context_text": "The related methods for text-based image generation [67] and manipulation [3, 4] often use the text encoder of a CLIP model [62] to encode user prompts as CLIP excels in visual prompts.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only models and methods. The citation is focused on the use of CLIP for encoding user prompts, which is a method rather than a dataset.",
      "processing_time": 63.52317261695862,
      "citing_paper_id": "267320695",
      "cited_paper_id": 231591445
    },
    {
      "context_text": "The related methods for text-based image generation [67] and manipulation [3, 4] often use the text encoder of a CLIP model [62] to encode user prompts as CLIP excels in visual prompts.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only models and methods. The citation is focused on the use of CLIP for encoding user prompts, which is a method rather than a dataset.",
      "processing_time": 63.52317261695862,
      "citing_paper_id": "267320695",
      "cited_paper_id": 245335280
    },
    {
      "context_text": "The related methods for text-based image generation [67] and manipulation [3, 4] often use the text encoder of a CLIP model [62] to encode user prompts as CLIP excels in visual prompts.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only models and methods. The citation is focused on the use of CLIP for encoding user prompts, which is a method rather than a dataset.",
      "processing_time": 63.52317261695862,
      "citing_paper_id": "267320695",
      "cited_paper_id": 253581213
    },
    {
      "context_text": "However, the performance on SR benchmarks is far from classical super-resolution methods [1, 44].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general reference to SR benchmarks which are excluded as they are primarily used for score comparison.",
      "processing_time": 61.59651494026184,
      "citing_paper_id": "267320695",
      "cited_paper_id": 237266491
    },
    {
      "context_text": "Despite it is not the focus of this work, we acknowledge that real-world image super-resolution is a related problem [12, 44, 48, 106], since the models aim to solve an inverse problem considering multiple degradations (blur, noise and downsampling).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only related problems and models. No verifiable resources are identified.",
      "processing_time": 60.7220094203949,
      "citing_paper_id": "267320695",
      "cited_paper_id": 237266491
    },
    {
      "context_text": "Despite it is not the focus of this work, we acknowledge that real-world image super-resolution is a related problem [12, 44, 48, 106], since the models aim to solve an inverse problem considering multiple degradations (blur, noise and downsampling).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only related problems and models. No verifiable resources are identified.",
      "processing_time": 60.7220094203949,
      "citing_paper_id": "267320695",
      "cited_paper_id": 247362538
    },
    {
      "context_text": "We focus on general-purpose restoration models [10, 44, 82, 95].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to general-purpose restoration models. No dataset names are provided in the context.",
      "processing_time": 61.609092473983765,
      "citing_paper_id": "267320695",
      "cited_paper_id": 237266491
    },
    {
      "context_text": "We focus on general-purpose restoration models [10, 44, 82, 95].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to general-purpose restoration models. No dataset names are provided in the context.",
      "processing_time": 61.609092473983765,
      "citing_paper_id": "267320695",
      "cited_paper_id": 244346144
    },
    {
      "context_text": "We focus on general-purpose restoration models [10, 44, 82, 95].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to general-purpose restoration models. No dataset names are provided in the context.",
      "processing_time": 61.609092473983765,
      "citing_paper_id": "267320695",
      "cited_paper_id": 248085491
    },
    {
      "context_text": "For example, SwinIR [44], MAXIM [74] and Uformer [82].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions models (SwinIR, MAXIM, Uformer) but does not specify any datasets. The context is focused on methods rather than datasets.",
      "processing_time": 62.36259579658508,
      "citing_paper_id": "267320695",
      "cited_paper_id": 237266491
    },
    {
      "context_text": "Other approaches use a general neural network for diverse tasks [10, 74, 82, 95], yet training the neural network for each specific task independently.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general approaches and neural networks. No verifiable resources are identified.",
      "processing_time": 61.756450176239014,
      "citing_paper_id": "267320695",
      "cited_paper_id": 244346144
    },
    {
      "context_text": "Other approaches use a general neural network for diverse tasks [10, 74, 82, 95], yet training the neural network for each specific task independently.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general approaches and neural networks. No verifiable resources are identified.",
      "processing_time": 61.756450176239014,
      "citing_paper_id": "267320695",
      "cited_paper_id": 248085491
    },
    {
      "context_text": "For instance, Restormer [95] uses non-local blocks [79] to capture complex features across the image.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (Restormer) and a technique (non-local blocks).",
      "processing_time": 62.14865684509277,
      "citing_paper_id": "267320695",
      "cited_paper_id": 244346144
    },
    {
      "context_text": "All-in-One (also known as multi-degradation or multi-task) image restoration is emerging as a new research field in low-level computer vision [42, 49, 60, 61, 75, 91, 97, 98].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only the emergence of all-in-one image restoration as a research field. No dataset names are present in the context.",
      "processing_time": 63.25273609161377,
      "citing_paper_id": "267320695",
      "cited_paper_id": 244714491
    },
    {
      "context_text": "All-in-One (also known as multi-degradation or multi-task) image restoration is emerging as a new research field in low-level computer vision [42, 49, 60, 61, 75, 91, 97, 98].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only the emergence of all-in-one image restoration as a research field. No dataset names are present in the context.",
      "processing_time": 63.25273609161377,
      "citing_paper_id": "267320695",
      "cited_paper_id": 259224666
    },
    {
      "context_text": "All-in-One (also known as multi-degradation or multi-task) image restoration is emerging as a new research field in low-level computer vision [42, 49, 60, 61, 75, 91, 97, 98].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only the emergence of all-in-one image restoration as a research field. No dataset names are present in the context.",
      "processing_time": 63.25273609161377,
      "citing_paper_id": "267320695",
      "cited_paper_id": 259243623
    },
    {
      "context_text": "All-in-One (also known as multi-degradation or multi-task) image restoration is emerging as a new research field in low-level computer vision [42, 49, 60, 61, 75, 91, 97, 98].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only the emergence of all-in-one image restoration as a research field. No dataset names are present in the context.",
      "processing_time": 63.25273609161377,
      "citing_paper_id": "267320695",
      "cited_paper_id": 260107992
    },
    {
      "context_text": "All-in-One (also known as multi-degradation or multi-task) image restoration is emerging as a new research field in low-level computer vision [42, 49, 60, 61, 75, 91, 97, 98].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only the emergence of all-in-one image restoration as a research field. No dataset names are present in the context.",
      "processing_time": 63.25273609161377,
      "citing_paper_id": "267320695",
      "cited_paper_id": 260680793
    },
    {
      "context_text": "All-in-One (also known as multi-degradation or multi-task) image restoration is emerging as a new research field in low-level computer vision [42, 49, 60, 61, 75, 91, 97, 98].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only the emergence of all-in-one image restoration as a research field. No dataset names are present in the context.",
      "processing_time": 63.25273609161377,
      "citing_paper_id": "267320695",
      "cited_paper_id": 264305756
    },
    {
      "context_text": "We can conclude that diffusion-based methods [4, 53, 67] for image manipulation require complex “tuning” of several (hyper-)parameters, and strong regularization to enforce fidelity and reduce hallucinations.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only methods and their requirements. No dataset names are present in the text.",
      "processing_time": 61.83138418197632,
      "citing_paper_id": "267320695",
      "cited_paper_id": 245335280
    },
    {
      "context_text": "We can conclude that diffusion-based methods [4, 53, 67] for image manipulation require complex “tuning” of several (hyper-)parameters, and strong regularization to enforce fidelity and reduce hallucinations.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only methods and their requirements. No dataset names are present in the text.",
      "processing_time": 61.83138418197632,
      "citing_paper_id": "267320695",
      "cited_paper_id": 253581213
    },
    {
      "context_text": "NAFNet [10] is an efficient alternative to complex transformer-based methods.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (NAFNet) which is not included as per instructions.",
      "processing_time": 61.83002305030823,
      "citing_paper_id": "267320695",
      "cited_paper_id": 248085491
    },
    {
      "context_text": "We use the same fixed image model (based on NAFNet [10]).",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a model (NAFNet).",
      "processing_time": 60.53650116920471,
      "citing_paper_id": "267320695",
      "cited_paper_id": 248085491
    },
    {
      "context_text": "Model Design Based on our experiments, given a trained text-guided image model ( e.g . based on NAFNet [10]), we can switch language models without performance loss.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a model (NAFNet) which is excluded according to the rules.",
      "processing_time": 61.63324594497681,
      "citing_paper_id": "267320695",
      "cited_paper_id": 248085491
    },
    {
      "context_text": "We use NAFNet [10] as image model.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions NAFNet as an image model, which is a method or architecture, not a dataset. No datasets are mentioned.",
      "processing_time": 62.276662826538086,
      "citing_paper_id": "267320695",
      "cited_paper_id": 248085491
    },
    {
      "context_text": "The conditioned features are further enhanced using a NAFBlock [10] ( Block ).",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (NAFBlock).",
      "processing_time": 60.58079528808594,
      "citing_paper_id": "267320695",
      "cited_paper_id": 248085491
    },
    {
      "context_text": "We use “regular” NAF-Blocks [10], followed by ICBs to condition the features, at both encoder and decoder blocks.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only model components. No dataset names are present in the citation span.",
      "processing_time": 61.898996353149414,
      "citing_paper_id": "267320695",
      "cited_paper_id": 248085491
    },
    {
      "context_text": "In the recent years, multiple methods have been proposed for text-to-image generation and text-based image editing works [4, 30, 34, 53, 70].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to methods and works in the field of text-to-image generation and text-based image editing.",
      "processing_time": 63.01134204864502,
      "citing_paper_id": "267320695",
      "cited_paper_id": 251252882
    },
    {
      "context_text": "In the recent years, multiple methods have been proposed for text-to-image generation and text-based image editing works [4, 30, 34, 53, 70].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to methods and works in the field of text-to-image generation and text-based image editing.",
      "processing_time": 63.01134204864502,
      "citing_paper_id": "267320695",
      "cited_paper_id": 252918469
    },
    {
      "context_text": "In the recent years, multiple methods have been proposed for text-to-image generation and text-based image editing works [4, 30, 34, 53, 70].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to methods and works in the field of text-to-image generation and text-based image editing.",
      "processing_time": 63.01134204864502,
      "citing_paper_id": "267320695",
      "cited_paper_id": 253581213
    },
    {
      "context_text": "We treat instruction-based image restoration as a supervised learning problem similar to previous works [4].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.2,
      "reasoning": "The citation does not mention any specific datasets, only a method or approach. The context is too generic to infer the use of a specific dataset.",
      "processing_time": 62.85371708869934,
      "citing_paper_id": "267320695",
      "cited_paper_id": 253581213
    },
    {
      "context_text": "InstructPix2Pix [4] cannot Table 9.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (InstructPix2Pix). No dataset names are present in the citation span.",
      "processing_time": 63.0089328289032,
      "citing_paper_id": "267320695",
      "cited_paper_id": 253581213
    },
    {
      "context_text": "In Figure 8 we compare with InstructPix2Pix [4].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (InstructPix2Pix).",
      "processing_time": 61.452574729919434,
      "citing_paper_id": "267320695",
      "cited_paper_id": 253581213
    },
    {
      "context_text": "Inspired by InstructPix2Pix [4], we adopt human written instructions as the mechanism of con-trol for our model.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (InstructPix2Pix) which is not included as per the rules.",
      "processing_time": 62.659393072128296,
      "citing_paper_id": "267320695",
      "cited_paper_id": 253581213
    },
    {
      "context_text": "Our main reference is In-structPix2Pix [4], this method enables editing from instructions that tell the model what action to perform, as opposed to text labels, captions or descriptions of the input or output images.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a method called InstructPix2Pix. The method is described as enabling editing from instructions, but no datasets are referenced.",
      "processing_time": 63.92089366912842,
      "citing_paper_id": "267320695",
      "cited_paper_id": 253581213
    },
    {
      "context_text": "In parallel, recent works such as InstructPix2Pix [4] show the potential of using text prompts to guide image generation and editing models.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (InstructPix2Pix) for using text prompts to guide image generation and editing models.",
      "processing_time": 63.148772954940796,
      "citing_paper_id": "267320695",
      "cited_paper_id": 253581213
    },
    {
      "context_text": "We also consider the contemporary work PromptIR [61].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method called PromptIR. The context does not provide information about datasets used.",
      "processing_time": 62.35756802558899,
      "citing_paper_id": "267320695",
      "cited_paper_id": 259224666
    },
    {
      "context_text": "Following previous works [42, 61, 100], we prepare the datasets for different restoration tasks.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.3,
      "reasoning": "The context does not provide specific dataset names, only a general statement about preparing datasets for different restoration tasks.",
      "processing_time": 62.17702770233154,
      "citing_paper_id": "267320695",
      "cited_paper_id": 259224666
    },
    {
      "context_text": "For instance, an auxiliary model for degradation classification [42, 60], or multi-dimensional guidance vectors (also known as “prompts”) [49, 61] that help the model to discriminate the different types of degradation in the image.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only methods and models. The cited papers' titles do not provide additional dataset information.",
      "processing_time": 62.47958779335022,
      "citing_paper_id": "267320695",
      "cited_paper_id": 259224666
    },
    {
      "context_text": "For instance, an auxiliary model for degradation classification [42, 60], or multi-dimensional guidance vectors (also known as “prompts”) [49, 61] that help the model to discriminate the different types of degradation in the image.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only methods and models. The cited papers' titles do not provide additional dataset information.",
      "processing_time": 62.47958779335022,
      "citing_paper_id": "267320695",
      "cited_paper_id": 259243623
    },
    {
      "context_text": "For instance, an auxiliary model for degradation classification [42, 60], or multi-dimensional guidance vectors (also known as “prompts”) [49, 61] that help the model to discriminate the different types of degradation in the image.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only methods and models. The cited papers' titles do not provide additional dataset information.",
      "processing_time": 62.47958779335022,
      "citing_paper_id": "267320695",
      "cited_paper_id": 260107992
    },
    {
      "context_text": "We evaluate InstructIR as previous works [42, 61, 100].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only references to previous works. There is no indication of dataset usage or evaluation.",
      "processing_time": 62.638542890548706,
      "citing_paper_id": "267320695",
      "cited_paper_id": 259224666
    },
    {
      "context_text": "C V ] 7 J u l 2024 intensive, recent approaches propose All-in-One restoration models [42, 60, 61, 100].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to other papers proposing All-in-One restoration models.",
      "processing_time": 62.235984563827515,
      "citing_paper_id": "267320695",
      "cited_paper_id": 259224666
    },
    {
      "context_text": "C V ] 7 J u l 2024 intensive, recent approaches propose All-in-One restoration models [42, 60, 61, 100].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to other papers proposing All-in-One restoration models.",
      "processing_time": 62.235984563827515,
      "citing_paper_id": "267320695",
      "cited_paper_id": 260107992
    },
    {
      "context_text": "InstructPix2Pix #1 InstructPix2Pix #2 Input AirNet [42] PromptIR [61] InstructIR Reference Figure 19.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions 'AirNet' and 'PromptIR', but these are likely models or methods rather than datasets. No clear dataset names are identified.",
      "processing_time": 63.53281831741333,
      "citing_paper_id": "267320695",
      "cited_paper_id": 259224666
    },
    {
      "context_text": "Finally, as previous works [42, 61, 100], we combine all the aforementioned training datasets, and we train our unified model for all-in-one restoration.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.3,
      "reasoning": "The context mentions 'all the aforementioned training datasets' but does not specify any particular dataset names. The reference to 'previous works' suggests that the datasets are likely known in the field but not explicitly listed here.",
      "processing_time": 66.59167456626892,
      "citing_paper_id": "267320695",
      "cited_paper_id": 259224666
    },
    {
      "context_text": "Input AirNet [42] PromptIR [61] InstructIR Reference Input AirNet [42] PromptIR [61] InstructIR Reference Figure 21.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only models or methods. The context is focused on comparing different image restoration techniques.",
      "processing_time": 62.92661762237549,
      "citing_paper_id": "267320695",
      "cited_paper_id": 259224666
    },
    {
      "context_text": "Contemporary works such as Promp-tIR [61] or ProRes [49] utilize a unified model for blind image restoration using learned guidance vectors, also known as “prompt embeddings ”, in contrast to raw user prompts in text form, which we use in this work.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only models and methods. The focus is on the use of 'prompt embeddings' versus 'raw user prompts'.",
      "processing_time": 64.10315918922424,
      "citing_paper_id": "267320695",
      "cited_paper_id": 259224666
    },
    {
      "context_text": "Contemporary works such as Promp-tIR [61] or ProRes [49] utilize a unified model for blind image restoration using learned guidance vectors, also known as “prompt embeddings ”, in contrast to raw user prompts in text form, which we use in this work.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only models and methods. The focus is on the use of 'prompt embeddings' versus 'raw user prompts'.",
      "processing_time": 64.10315918922424,
      "citing_paper_id": "267320695",
      "cited_paper_id": 259243623
    },
    {
      "context_text": "Note that PromptIR, ProRes [49] and Amirnet [98] are contemporary works (presented or published by Dec 2023).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only contemporary works. The titles do not provide additional context to identify datasets.",
      "processing_time": 62.92280316352844,
      "citing_paper_id": "267320695",
      "cited_paper_id": 259243623
    },
    {
      "context_text": "Similarly to previous works that use auxiliary image-based degradation classification [42, 60].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only references to previous works using auxiliary image-based degradation classification.",
      "processing_time": 62.401217460632324,
      "citing_paper_id": "267320695",
      "cited_paper_id": 260107992
    },
    {
      "context_text": "We use as reference AirNet [42], IDR [100] and ADMS [60].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods or models. The context is about using certain methods as references, not datasets.",
      "processing_time": 64.18653392791748,
      "citing_paper_id": "267320695",
      "cited_paper_id": 260107992
    },
    {
      "context_text": "Inspired by these works, we argue that text guidance can help to guide blind restoration models better than the image-based degradation classification used in previous works [42, 60, 100].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only references to previous works and a general discussion about image restoration methods.",
      "processing_time": 62.92308044433594,
      "citing_paper_id": "267320695",
      "cited_paper_id": 260107992
    },
    {
      "context_text": "To the best of our knowledge, IDR [100] and ADMS [60] do not provide open-source code, models or results, thus we cannot compare with them qualitatively.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models or methods that are not open-source. No verifiable resources are identified.",
      "processing_time": 63.34113550186157,
      "citing_paper_id": "267320695",
      "cited_paper_id": 260107992
    },
    {
      "context_text": "In deblurring, algorithms like DeblurGAN [28] and MIMO-UNet [13] and others stand out.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions algorithms but does not refer to any specific datasets. The context is about methods and their performance in deblurring.",
      "processing_time": 63.73901629447937,
      "citing_paper_id": "271432364",
      "cited_paper_id": 4552226
    },
    {
      "context_text": "Traditional methods are usually tailored to specific tasks like denoising [55, 61, 49, 29, 30, 12, 3], super-resolution [56, 34, 53, 4, 45, 47, 48], and deblur-ring [28, 22, 51, 32, 44, 17].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to methods and tasks. No dataset names are present in the text.",
      "processing_time": 63.54348945617676,
      "citing_paper_id": "271432364",
      "cited_paper_id": 4552226
    },
    {
      "context_text": "Traditional methods are usually tailored to specific tasks like denoising [55, 61, 49, 29, 30, 12, 3], super-resolution [56, 34, 53, 4, 45, 47, 48], and deblur-ring [28, 22, 51, 32, 44, 17].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to methods and tasks. No dataset names are present in the text.",
      "processing_time": 63.54348945617676,
      "citing_paper_id": "271432364",
      "cited_paper_id": 236171006
    },
    {
      "context_text": "Traditional methods are usually tailored to specific tasks like denoising [55, 61, 49, 29, 30, 12, 3], super-resolution [56, 34, 53, 4, 45, 47, 48], and deblur-ring [28, 22, 51, 32, 44, 17].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to methods and tasks. No dataset names are present in the text.",
      "processing_time": 63.54348945617676,
      "citing_paper_id": "271432364",
      "cited_paper_id": 244714180
    },
    {
      "context_text": "Traditional methods are usually tailored to specific tasks like denoising [55, 61, 49, 29, 30, 12, 3], super-resolution [56, 34, 53, 4, 45, 47, 48], and deblur-ring [28, 22, 51, 32, 44, 17].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to methods and tasks. No dataset names are present in the text.",
      "processing_time": 63.54348945617676,
      "citing_paper_id": "271432364",
      "cited_paper_id": 244908890
    },
    {
      "context_text": "Traditional methods are usually tailored to specific tasks like denoising [55, 61, 49, 29, 30, 12, 3], super-resolution [56, 34, 53, 4, 45, 47, 48], and deblur-ring [28, 22, 51, 32, 44, 17].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to methods and tasks. No dataset names are present in the text.",
      "processing_time": 63.54348945617676,
      "citing_paper_id": "271432364",
      "cited_paper_id": 250615985
    },
    {
      "context_text": "Traditional methods are usually tailored to specific tasks like denoising [55, 61, 49, 29, 30, 12, 3], super-resolution [56, 34, 53, 4, 45, 47, 48], and deblur-ring [28, 22, 51, 32, 44, 17].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to methods and tasks. No dataset names are present in the text.",
      "processing_time": 63.54348945617676,
      "citing_paper_id": "271432364",
      "cited_paper_id": 258615282
    },
    {
      "context_text": "Traditional methods are usually tailored to specific tasks like denoising [55, 61, 49, 29, 30, 12, 3], super-resolution [56, 34, 53, 4, 45, 47, 48], and deblur-ring [28, 22, 51, 32, 44, 17].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to methods and tasks. No dataset names are present in the text.",
      "processing_time": 63.54348945617676,
      "citing_paper_id": "271432364",
      "cited_paper_id": 260735888
    },
    {
      "context_text": "Traditional methods are usually tailored to specific tasks like denoising [55, 61, 49, 29, 30, 12, 3], super-resolution [56, 34, 53, 4, 45, 47, 48], and deblur-ring [28, 22, 51, 32, 44, 17].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to methods and tasks. No dataset names are present in the text.",
      "processing_time": 63.54348945617676,
      "citing_paper_id": "271432364",
      "cited_paper_id": 260869421
    },
    {
      "context_text": "Traditional methods are usually tailored to specific tasks like denoising [55, 61, 49, 29, 30, 12, 3], super-resolution [56, 34, 53, 4, 45, 47, 48], and deblur-ring [28, 22, 51, 32, 44, 17].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to methods and tasks. No dataset names are present in the text.",
      "processing_time": 63.54348945617676,
      "citing_paper_id": "271432364",
      "cited_paper_id": 265013798
    },
    {
      "context_text": "Traditional methods are usually tailored to specific tasks like denoising [55, 61, 49, 29, 30, 12, 3], super-resolution [56, 34, 53, 4, 45, 47, 48], and deblur-ring [28, 22, 51, 32, 44, 17].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to methods and tasks. No dataset names are present in the text.",
      "processing_time": 63.54348945617676,
      "citing_paper_id": "271432364",
      "cited_paper_id": 265466382
    },
    {
      "context_text": "Traditional methods are usually tailored to specific tasks like denoising [55, 61, 49, 29, 30, 12, 3], super-resolution [56, 34, 53, 4, 45, 47, 48], and deblur-ring [28, 22, 51, 32, 44, 17].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to methods and tasks. No dataset names are present in the text.",
      "processing_time": 63.54348945617676,
      "citing_paper_id": "271432364",
      "cited_paper_id": 267199774
    },
    {
      "context_text": "RL-Restore [57] prepares a toolbox consisting of small-scale convolutional networks, each specialized in different tasks.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a toolbox of small-scale convolutional networks. No verifiable datasets are referenced.",
      "processing_time": 63.539262771606445,
      "citing_paper_id": "271432364",
      "cited_paper_id": 4746623
    },
    {
      "context_text": "Image restoration assistants that utilize tool libraries, such as Clarity ChatGPT [50] and RL-Restore [57].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions 'Clarity ChatGPT' and 'RL-Restore', which are tools or systems, not datasets. No specific datasets are mentioned.",
      "processing_time": 64.9463279247284,
      "citing_paper_id": "271432364",
      "cited_paper_id": 4746623
    },
    {
      "context_text": "Image restoration assistants that utilize tool libraries, such as Clarity ChatGPT [50] and RL-Restore [57].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions 'Clarity ChatGPT' and 'RL-Restore', which are tools or systems, not datasets. No specific datasets are mentioned.",
      "processing_time": 64.9463279247284,
      "citing_paper_id": "271432364",
      "cited_paper_id": 265295126
    },
    {
      "context_text": "Follow [26, 20], evaluation metrics primarily include PSNR, SSIM, LPIPS [60], and DISTS [16].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context mentions evaluation metrics but does not specify any datasets. The cited papers' titles do not provide additional context to identify specific datasets.",
      "processing_time": 63.82245659828186,
      "citing_paper_id": "271432364",
      "cited_paper_id": 4766599
    },
    {
      "context_text": "Follow [26, 20], evaluation metrics primarily include PSNR, SSIM, LPIPS [60], and DISTS [16].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context mentions evaluation metrics but does not specify any datasets. The cited papers' titles do not provide additional context to identify specific datasets.",
      "processing_time": 63.82245659828186,
      "citing_paper_id": "271432364",
      "cited_paper_id": 215785896
    },
    {
      "context_text": "Follow [26, 20], evaluation metrics primarily include PSNR, SSIM, LPIPS [60], and DISTS [16].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context mentions evaluation metrics but does not specify any datasets. The cited papers' titles do not provide additional context to identify specific datasets.",
      "processing_time": 63.82245659828186,
      "citing_paper_id": "271432364",
      "cited_paper_id": 227238849
    },
    {
      "context_text": "In denoising, models like DnCNN [59] and RNAN [63] have demonstrated significant effectiveness, among others.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models and their effectiveness in denoising.",
      "processing_time": 62.20106506347656,
      "citing_paper_id": "271432364",
      "cited_paper_id": 85501306
    },
    {
      "context_text": "Additionally, there are specialized methods for restoration under adverse weather conditions, including dehazing [52, 41], deraining [11, 7], and desnow-ing [8, 9, 5].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context mentions specialized methods for image restoration under adverse weather conditions but does not specify any datasets. The cited papers' titles do not provide additional dataset information.",
      "processing_time": 65.00261569023132,
      "citing_paper_id": "271432364",
      "cited_paper_id": 208138077
    },
    {
      "context_text": "Additionally, there are specialized methods for restoration under adverse weather conditions, including dehazing [52, 41], deraining [11, 7], and desnow-ing [8, 9, 5].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context mentions specialized methods for image restoration under adverse weather conditions but does not specify any datasets. The cited papers' titles do not provide additional dataset information.",
      "processing_time": 65.00261569023132,
      "citing_paper_id": "271432364",
      "cited_paper_id": 267026544
    },
    {
      "context_text": "The primary limitation of our study is the confined scope of models and tasks examined.",
      "catation_intent": "limitation",
      "resource_type": "limitation",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only a limitation regarding the scope of models and tasks.",
      "processing_time": 62.77258253097534,
      "citing_paper_id": "271432364",
      "cited_paper_id": 231591445
    },
    {
      "context_text": "LoRA [21] is utilized to fine-tune both the vision and language modules.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions LoRA but does not refer to it as a dataset. It is described as a method for fine-tuning models.",
      "processing_time": 63.60909962654114,
      "citing_paper_id": "271432364",
      "cited_paper_id": 235458009
    },
    {
      "context_text": "For reducing JPEG artifacts, methods such as DCSC [19] and FBCNN [23] are particularly well-suited.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods (DCSC and FBCNN) for reducing JPEG artifacts.",
      "processing_time": 63.16117835044861,
      "citing_paper_id": "271432364",
      "cited_paper_id": 238215243
    },
    {
      "context_text": "TAPE [36] embeds a task-agnostic prior into a transformer, utilizing a two-stage process of pre-training and fine-tuning to enhance image restoration.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions TAPE, which is a method for image restoration, not a dataset. No specific datasets are mentioned.",
      "processing_time": 62.988933086395264,
      "citing_paper_id": "271432364",
      "cited_paper_id": 247411392
    },
    {
      "context_text": "We introduce an advanced image restoration agent, dubbed RestoreAgent, implemented using the state-of-the-art multimodal model Llava-Llama3-8b [46].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a model (Llava-Llama3-8b) which is not a dataset. The context is about introducing an image restoration agent using a multimodal model.",
      "processing_time": 66.89899396896362,
      "citing_paper_id": "271432364",
      "cited_paper_id": 257219404
    },
    {
      "context_text": "The MLLM’s exposure to vast and diverse data endows it with superior generalization capabilities and has show-cased remarkable performance in visual understanding and logical reasoning [46, 35, 18, 39, 43, 6, 62].",
      "catation_intent": "findings",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to the model's capabilities and performance. The cited paper titles do not provide additional context to identify datasets.",
      "processing_time": 64.99184465408325,
      "citing_paper_id": "271432364",
      "cited_paper_id": 257219404
    },
    {
      "context_text": "The MLLM’s exposure to vast and diverse data endows it with superior generalization capabilities and has show-cased remarkable performance in visual understanding and logical reasoning [46, 35, 18, 39, 43, 6, 62].",
      "catation_intent": "findings",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to the model's capabilities and performance. The cited paper titles do not provide additional context to identify datasets.",
      "processing_time": 64.99184465408325,
      "citing_paper_id": "271432364",
      "cited_paper_id": 264146906
    },
    {
      "context_text": "The MLLM’s exposure to vast and diverse data endows it with superior generalization capabilities and has show-cased remarkable performance in visual understanding and logical reasoning [46, 35, 18, 39, 43, 6, 62].",
      "catation_intent": "findings",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to the model's capabilities and performance. The cited paper titles do not provide additional context to identify datasets.",
      "processing_time": 64.99184465408325,
      "citing_paper_id": "271432364",
      "cited_paper_id": 265043538
    },
    {
      "context_text": "The MLLM’s exposure to vast and diverse data endows it with superior generalization capabilities and has show-cased remarkable performance in visual understanding and logical reasoning [46, 35, 18, 39, 43, 6, 62].",
      "catation_intent": "findings",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to the model's capabilities and performance. The cited paper titles do not provide additional context to identify datasets.",
      "processing_time": 64.99184465408325,
      "citing_paper_id": "271432364",
      "cited_paper_id": 267750841
    },
    {
      "context_text": "All-in-one models [38, 31, 24, 40, 33, 14, 27, 37, 1, 25] seek to use a single framework to handle multiple degradations simultaneously.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to all-in-one models handling multiple degradations. No verifiable resources are identified.",
      "processing_time": 64.60637617111206,
      "citing_paper_id": "271432364",
      "cited_paper_id": 259224666
    },
    {
      "context_text": "All-in-one models [38, 31, 24, 40, 33, 14, 27, 37, 1, 25] seek to use a single framework to handle multiple degradations simultaneously.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to all-in-one models handling multiple degradations. No verifiable resources are identified.",
      "processing_time": 64.60637617111206,
      "citing_paper_id": "271432364",
      "cited_paper_id": 266149517
    },
    {
      "context_text": "All-in-one models [38, 31, 24, 40, 33, 14, 27, 37, 1, 25] seek to use a single framework to handle multiple degradations simultaneously.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to all-in-one models handling multiple degradations. No verifiable resources are identified.",
      "processing_time": 64.60637617111206,
      "citing_paper_id": "271432364",
      "cited_paper_id": 266844752
    },
    {
      "context_text": "PromptIR [40] and PIP [33] both utilize uniquely designed prompts to guide their networks.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions 'PromptIR' and 'PIP', which are methods/models, not datasets. No specific datasets are mentioned or used according to the given context.",
      "processing_time": 64.02749133110046,
      "citing_paper_id": "271432364",
      "cited_paper_id": 259224666
    },
    {
      "context_text": "PromptIR [40] and PIP [33] both utilize uniquely designed prompts to guide their networks.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions 'PromptIR' and 'PIP', which are methods/models, not datasets. No specific datasets are mentioned or used according to the given context.",
      "processing_time": 64.02749133110046,
      "citing_paper_id": "271432364",
      "cited_paper_id": 266149517
    },
    {
      "context_text": "In some scenarios, the system may opt to use a single model for a specific task or randomly select a model from a pool of available options [50].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a potential method or system for image restoration and enhancement.",
      "processing_time": 62.58047938346863,
      "citing_paper_id": "271432364",
      "cited_paper_id": 265295126
    },
    {
      "context_text": "Clarity ChatGPT [50] combines the conversational intelligence of ChatGPT with multiple image restoration methods.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a combination of conversational intelligence and image restoration methods.",
      "processing_time": 62.57672333717346,
      "citing_paper_id": "271432364",
      "cited_paper_id": 265295126
    },
    {
      "context_text": "Current methods [50, 24, 14] typically detect the types of degradation in an image and apply the appropriate restoration models in a predetermined order, or manually selected by experts, or chosen at random.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general methods and approaches. There are no clear identifiers for datasets or other verifiable resources.",
      "processing_time": 64.18035221099854,
      "citing_paper_id": "271432364",
      "cited_paper_id": 265295126
    },
    {
      "context_text": "MiOIR [27] employs sequential and prompt learning strategies, which guide the network to incrementally learn individual IR tasks in a sequential manner.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (MiOIR) and its learning strategies. No verifiable resources are identified.",
      "processing_time": 64.17696380615234,
      "citing_paper_id": "271432364",
      "cited_paper_id": 266844752
    },
    {
      "context_text": "This limitation greatly narrows our selection of model tools, requiring us to choose more robust and generalizable model tools.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, models, or methods. It only discusses the limitations in selecting model tools.",
      "processing_time": 63.194429874420166,
      "citing_paper_id": "271432364",
      "cited_paper_id": null
    },
    {
      "context_text": "For haze removal , early efforts [5], [6], [40], [41] employ hand-crafted priors or deep neural networks to estimate the parameter of physical model [42].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only methods and models for haze removal.",
      "processing_time": 61.98560285568237,
      "citing_paper_id": "277622341",
      "cited_paper_id": 4054776
    },
    {
      "context_text": "For haze removal , early efforts [5], [6], [40], [41] employ hand-crafted priors or deep neural networks to estimate the parameter of physical model [42].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only methods and models for haze removal.",
      "processing_time": 61.98560285568237,
      "citing_paper_id": "277622341",
      "cited_paper_id": 14092238
    },
    {
      "context_text": "Specifically, for raindrop removal, the comparison includes pix2pix [73], DuRN [74], AttentiveGAN [11], RaindropAttn [45], CCN [66], IDT [75], and UDR-S 2 Former [76].",
      "catation_intent": "reusable resource",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span mentions several methods/models for raindrop removal but does not explicitly reference any datasets. The cited paper titles do not provide additional context about datasets.",
      "processing_time": 64.68725800514221,
      "citing_paper_id": "277622341",
      "cited_paper_id": 4539586
    },
    {
      "context_text": "Specifically, for raindrop removal, the comparison includes pix2pix [73], DuRN [74], AttentiveGAN [11], RaindropAttn [45], CCN [66], IDT [75], and UDR-S 2 Former [76].",
      "catation_intent": "reusable resource",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span mentions several methods/models for raindrop removal but does not explicitly reference any datasets. The cited paper titles do not provide additional context about datasets.",
      "processing_time": 64.68725800514221,
      "citing_paper_id": "277622341",
      "cited_paper_id": 6200260
    },
    {
      "context_text": "Specifically, for raindrop removal, the comparison includes pix2pix [73], DuRN [74], AttentiveGAN [11], RaindropAttn [45], CCN [66], IDT [75], and UDR-S 2 Former [76].",
      "catation_intent": "reusable resource",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span mentions several methods/models for raindrop removal but does not explicitly reference any datasets. The cited paper titles do not provide additional context about datasets.",
      "processing_time": 64.68725800514221,
      "citing_paper_id": "277622341",
      "cited_paper_id": 84843405
    },
    {
      "context_text": "Specifically, for raindrop removal, the comparison includes pix2pix [73], DuRN [74], AttentiveGAN [11], RaindropAttn [45], CCN [66], IDT [75], and UDR-S 2 Former [76].",
      "catation_intent": "reusable resource",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span mentions several methods/models for raindrop removal but does not explicitly reference any datasets. The cited paper titles do not provide additional context about datasets.",
      "processing_time": 64.68725800514221,
      "citing_paper_id": "277622341",
      "cited_paper_id": 249709314
    },
    {
      "context_text": "Specifically, for raindrop removal, the comparison includes pix2pix [73], DuRN [74], AttentiveGAN [11], RaindropAttn [45], CCN [66], IDT [75], and UDR-S 2 Former [76].",
      "catation_intent": "reusable resource",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span mentions several methods/models for raindrop removal but does not explicitly reference any datasets. The cited paper titles do not provide additional context about datasets.",
      "processing_time": 64.68725800514221,
      "citing_paper_id": "277622341",
      "cited_paper_id": 261245351
    },
    {
      "context_text": "All-weather is a combination of three subsets derived from the datasets: Raindrop [11], Outdoor-Rain [65], and Snow100K [16], with images collected under raindrop, heavy rain with rain streaks and haze, and snowy conditions.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Raindrop",
        "Outdoor-Rain",
        "Snow100K"
      ],
      "dataset_descriptions": {
        "Raindrop": "Used to collect images with raindrops for training and evaluating rain removal algorithms, focusing on realistic raindrop effects.",
        "Outdoor-Rain": "Used to gather images under heavy rain conditions with rain streaks and haze, for training and evaluating rain removal and restoration techniques.",
        "Snow100K": "Used to collect images under snowy conditions, for training and evaluating snow removal and image restoration algorithms."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions specific datasets used for image restoration under various weather conditions, which are directly relevant to the research topic.",
      "processing_time": 82.3750672340393,
      "citing_paper_id": "277622341",
      "cited_paper_id": 4539586
    },
    {
      "context_text": "All-weather is a combination of three subsets derived from the datasets: Raindrop [11], Outdoor-Rain [65], and Snow100K [16], with images collected under raindrop, heavy rain with rain streaks and haze, and snowy conditions.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Raindrop",
        "Outdoor-Rain",
        "Snow100K"
      ],
      "dataset_descriptions": {
        "Raindrop": "Used to collect images with raindrops for training and evaluating rain removal algorithms, focusing on realistic raindrop effects.",
        "Outdoor-Rain": "Used to gather images under heavy rain conditions with rain streaks and haze, for training and evaluating rain removal and restoration techniques.",
        "Snow100K": "Used to collect images under snowy conditions, for training and evaluating snow removal and image restoration algorithms."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions specific datasets used for image restoration under various weather conditions, which are directly relevant to the research topic.",
      "processing_time": 82.3750672340393,
      "citing_paper_id": "277622341",
      "cited_paper_id": 131773964
    },
    {
      "context_text": "The other line of work adopts attentive GANs [11] or mathematical descriptions [45] to raindrop removal.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and approaches for raindrop removal.",
      "processing_time": 62.141610622406006,
      "citing_paper_id": "277622341",
      "cited_paper_id": 4539586
    },
    {
      "context_text": "The test set encompasses 58 images from RainDrop test set [11], 750 images from Test1 [65], and 16,081 images from Snow100K-L test set [16].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "RainDrop test set",
        "Test1",
        "Snow100K-L test set"
      ],
      "dataset_descriptions": {
        "RainDrop test set": "Used to evaluate raindrop removal performance, containing 58 images specifically designed to test raindrop removal algorithms.",
        "Test1": "Used to assess general image restoration quality, comprising 750 images for evaluating various restoration techniques.",
        "Snow100K-L test set": "Used to test snow removal effectiveness, including 16,081 images for evaluating snow removal algorithms."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions three specific datasets used for testing image restoration methods. These datasets are clearly identified and are relevant to the topic of all-in-one image restoration.",
      "processing_time": 84.45765042304993,
      "citing_paper_id": "277622341",
      "cited_paper_id": 4539586
    },
    {
      "context_text": "The test set encompasses 58 images from RainDrop test set [11], 750 images from Test1 [65], and 16,081 images from Snow100K-L test set [16].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "RainDrop test set",
        "Test1",
        "Snow100K-L test set"
      ],
      "dataset_descriptions": {
        "RainDrop test set": "Used to evaluate raindrop removal performance, containing 58 images specifically designed to test raindrop removal algorithms.",
        "Test1": "Used to assess general image restoration quality, comprising 750 images for evaluating various restoration techniques.",
        "Snow100K-L test set": "Used to test snow removal effectiveness, including 16,081 images for evaluating snow removal algorithms."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions three specific datasets used for testing image restoration methods. These datasets are clearly identified and are relevant to the topic of all-in-one image restoration.",
      "processing_time": 84.45765042304993,
      "citing_paper_id": "277622341",
      "cited_paper_id": 131773964
    },
    {
      "context_text": "The training set contains 18,069 image pairs, including 818 from Raindrop [11], 8,250 from Outdoor-Rain [65], and 9,001 from Snow100K [16].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Raindrop",
        "Outdoor-Rain",
        "Snow100K"
      ],
      "dataset_descriptions": {
        "Raindrop": "Used for training raindrop removal models, providing 818 image pairs for the task of removing raindrops from images.",
        "Outdoor-Rain": "Used for training heavy rain image restoration models, providing 8,250 image pairs to improve image quality under rainy conditions.",
        "Snow100K": "Used for training snow removal models, providing 9,001 image pairs to enhance image clarity in snowy environments."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions three specific datasets used for training: Raindrop, Outdoor-Rain, and Snow100K. These datasets are clearly identified and used for image restoration tasks.",
      "processing_time": 85.5548300743103,
      "citing_paper_id": "277622341",
      "cited_paper_id": 4539586
    },
    {
      "context_text": "The training set contains 18,069 image pairs, including 818 from Raindrop [11], 8,250 from Outdoor-Rain [65], and 9,001 from Snow100K [16].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Raindrop",
        "Outdoor-Rain",
        "Snow100K"
      ],
      "dataset_descriptions": {
        "Raindrop": "Used for training raindrop removal models, providing 818 image pairs for the task of removing raindrops from images.",
        "Outdoor-Rain": "Used for training heavy rain image restoration models, providing 8,250 image pairs to improve image quality under rainy conditions.",
        "Snow100K": "Used for training snow removal models, providing 9,001 image pairs to enhance image clarity in snowy environments."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions three specific datasets used for training: Raindrop, Outdoor-Rain, and Snow100K. These datasets are clearly identified and used for image restoration tasks.",
      "processing_time": 85.5548300743103,
      "citing_paper_id": "277622341",
      "cited_paper_id": 131773964
    },
    {
      "context_text": "For rain + haze removal, we evaluate against the methods CycleGAN [77], pix2pix [73], HRGAN [65], and PCNet [78].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only models and methods used for evaluation. No verifiable resources are identified.",
      "processing_time": 63.260069131851196,
      "citing_paper_id": "277622341",
      "cited_paper_id": 6200260
    },
    {
      "context_text": "For rain + haze removal, we evaluate against the methods CycleGAN [77], pix2pix [73], HRGAN [65], and PCNet [78].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only models and methods used for evaluation. No verifiable resources are identified.",
      "processing_time": 63.260069131851196,
      "citing_paper_id": "277622341",
      "cited_paper_id": 131773964
    },
    {
      "context_text": "SMoE is mainly employed in natural language processing [39], [55] and computer vision [56]–[60].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only the application areas of SMoE. No dataset names are present in the citation span.",
      "processing_time": 64.04334473609924,
      "citing_paper_id": "277622341",
      "cited_paper_id": 7593476
    },
    {
      "context_text": "SMoE is mainly employed in natural language processing [39], [55] and computer vision [56]–[60].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only the application areas of SMoE. No dataset names are present in the citation span.",
      "processing_time": 64.04334473609924,
      "citing_paper_id": "277622341",
      "cited_paper_id": 12462234
    },
    {
      "context_text": "The sparse mixture of experts (SMoE) [39], a variant of MoE, exploits a router mechanism to activate relevant experts selectively, improving the model’s scalability and efficiency.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (sparse mixture of experts).",
      "processing_time": 62.39619469642639,
      "citing_paper_id": "277622341",
      "cited_paper_id": 12462234
    },
    {
      "context_text": "The gating function in SMoE structure often leads to a load imbalance problem [39], where few same experts are repeatedly activated while others remain underutilized.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a methodological issue in a neural network architecture.",
      "processing_time": 62.912275552749634,
      "citing_paper_id": "277622341",
      "cited_paper_id": 12462234
    },
    {
      "context_text": "Unlike [39] with a fixed number of activated experts, we dynamically adjust the number of activated experts for every input, improving computational efficiency and restoration performance.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for dynamically adjusting the number of activated experts in a neural network.",
      "processing_time": 63.620861530303955,
      "citing_paper_id": "277622341",
      "cited_paper_id": 12462234
    },
    {
      "context_text": "SMoE [39] is a network architecture with a learnable gating mechanism, which sparsely routes input tokens to specialized expert sub-networks.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (SMoE) which is a network architecture. No datasets are referenced for training or evaluation.",
      "processing_time": 64.70249438285828,
      "citing_paper_id": "277622341",
      "cited_paper_id": 12462234
    },
    {
      "context_text": "For rain removal , a line of works focuses on rain streak removal with some techniques, such as recurrent network [10], spatial attention [12], or conditional VAEs [13].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods and techniques for rain removal.",
      "processing_time": 62.215172290802,
      "citing_paper_id": "277622341",
      "cited_paper_id": 49864080
    },
    {
      "context_text": "(e-mail: xpzhang@ieee.org). weather degradation, such as dehazing [5]–[9], deraining [10]– [15], and desnowing [16]–[19].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to various image restoration tasks. No clear, verifiable datasets are identified.",
      "processing_time": 63.72287845611572,
      "citing_paper_id": "277622341",
      "cited_paper_id": 49864080
    },
    {
      "context_text": "For snow removal, we compare with SPANet [12], RESCAN [10], DesnowNet [16], JSTASR [17], and DDMSNet [19].",
      "catation_intent": "none",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context mentions several methods/models for snow removal but does not specify any datasets. The cited paper titles do not provide additional dataset information.",
      "processing_time": 64.02885580062866,
      "citing_paper_id": "277622341",
      "cited_paper_id": 49864080
    },
    {
      "context_text": "For snow removal, we compare with SPANet [12], RESCAN [10], DesnowNet [16], JSTASR [17], and DDMSNet [19].",
      "catation_intent": "none",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context mentions several methods/models for snow removal but does not specify any datasets. The cited paper titles do not provide additional dataset information.",
      "processing_time": 64.02885580062866,
      "citing_paper_id": "277622341",
      "cited_paper_id": 226308542
    },
    {
      "context_text": "Subsequently, learning-based methods directly restore haze-free images from hazy images using attention mechanisms [7], GANs [43], or Transformers [44].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only methods and models used for image dehazing.",
      "processing_time": 62.98727083206177,
      "citing_paper_id": "277622341",
      "cited_paper_id": 199528450
    },
    {
      "context_text": "Subsequently, learning-based methods directly restore haze-free images from hazy images using attention mechanisms [7], GANs [43], or Transformers [44].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only methods and models used for image dehazing.",
      "processing_time": 62.98727083206177,
      "citing_paper_id": "277622341",
      "cited_paper_id": 248069347
    },
    {
      "context_text": "Based on Markov chain and reparameterization technology [61], [62], the above equation is reformulated in closed form: where ¯ α t = δ t , and ϵ t ∼ N ( 0 , I ) .",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only mathematical concepts and techniques.",
      "processing_time": 61.69061470031738,
      "citing_paper_id": "277622341",
      "cited_paper_id": 211146177
    },
    {
      "context_text": "…DDMSNet [19] TIP’21 28.85 0.8772 MPRNet [20] CVPR’21 29.76 0.8949 NAFNet [21] ECCV’22 30.06 0.9017 Restormer [22] CVPR’22 30.52 0.9092 All-in-One [24] CVPR’20 28.33 0.8820 TransWeather [26] CVPR’22 29.31 0.8879 TUM [25] CVPR’22 30.24 0.9020 WGWSNet [27] CVPR’23 approaches: All-in-One [24],…",
      "catation_intent": "reusable resource",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only model names and their performance metrics. The title 'All in One Bad Weather Removal Using Architectural Search' does not provide additional information about datasets.",
      "processing_time": 66.64429950714111,
      "citing_paper_id": "277622341",
      "cited_paper_id": 219978541
    },
    {
      "context_text": "Recent works [24]–[31] have developed a unified model for multiple weather restoration in an all-in-one manner.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to recent works developing unified models for weather restoration.",
      "processing_time": 63.294273138046265,
      "citing_paper_id": "277622341",
      "cited_paper_id": 219978541
    },
    {
      "context_text": "Recent works [24]–[31] have developed a unified model for multiple weather restoration in an all-in-one manner.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to recent works developing unified models for weather restoration.",
      "processing_time": 63.294273138046265,
      "citing_paper_id": "277622341",
      "cited_paper_id": 272956729
    },
    {
      "context_text": "Similar to [24], [26], our method is trained on the mixed dataset [26] and tested on the specific dataset.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.3,
      "reasoning": "The context mentions 'mixed dataset' and 'specific dataset' but does not provide specific names or identifiers. These terms are too generic and do not meet the criteria for inclusion.",
      "processing_time": 65.79387855529785,
      "citing_paper_id": "277622341",
      "cited_paper_id": 219978541
    },
    {
      "context_text": "Among them, All-in-One [24] deploys multiple encoders to restore multiple weather conditions, each tailored for specific weather degradations.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a method or model called 'All-in-One'. No verifiable datasets are referenced.",
      "processing_time": 63.84132218360901,
      "citing_paper_id": "277622341",
      "cited_paper_id": 219978541
    },
    {
      "context_text": "Specifically, multiple encoders [24] or knowledge learning techniques [25] are used to tailor the model for each weather type, but these networks are complicated and burdensome.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and models. The context focuses on the complexity of using multiple encoders or knowledge learning techniques for weather type adaptation.",
      "processing_time": 65.11142539978027,
      "citing_paper_id": "277622341",
      "cited_paper_id": 219978541
    },
    {
      "context_text": "For fair comparisons, we evaluate the performance of DA 2 Diff on the All-weather [26] benchmark, as the previous methods [24], [26], [28].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "All-weather"
      ],
      "dataset_descriptions": {
        "All-weather": "Used to evaluate the performance of DA 2 Diff for image restoration in adverse weather conditions, comparing it with previous methods."
      },
      "confidence_score": 0.9,
      "reasoning": "The context mentions 'All-weather' benchmark, which is likely a dataset used for evaluating image restoration methods in adverse weather conditions. The cited papers support this interpretation.",
      "processing_time": 71.4742796421051,
      "citing_paper_id": "277622341",
      "cited_paper_id": 219978541
    },
    {
      "context_text": "For fair comparisons, we evaluate the performance of DA 2 Diff on the All-weather [26] benchmark, as the previous methods [24], [26], [28].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "All-weather"
      ],
      "dataset_descriptions": {
        "All-weather": "Used to evaluate the performance of DA 2 Diff for image restoration in adverse weather conditions, comparing it with previous methods."
      },
      "confidence_score": 0.9,
      "reasoning": "The context mentions 'All-weather' benchmark, which is likely a dataset used for evaluating image restoration methods in adverse weather conditions. The cited papers support this interpretation.",
      "processing_time": 71.4742796421051,
      "citing_paper_id": "277622341",
      "cited_paper_id": 251197000
    },
    {
      "context_text": "…0.9017 Restormer [22] CVPR’22 30.52 0.9092 All-in-One [24] CVPR’20 28.33 0.8820 TransWeather [26] CVPR’22 29.31 0.8879 TUM [25] CVPR’22 30.24 0.9020 WGWSNet [27] CVPR’23 approaches: All-in-One [24], TransWeather [26], TUM [25], WGWSNet [27], WeatherDiff [28], MW-ConvNet [31], and MWFormer [30].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions several methods and models but does not explicitly refer to any specific datasets. The names mentioned are all models or methods used for image restoration under adverse weather conditions.",
      "processing_time": 65.10582876205444,
      "citing_paper_id": "277622341",
      "cited_paper_id": 219978541
    },
    {
      "context_text": "…0.9017 Restormer [22] CVPR’22 30.52 0.9092 All-in-One [24] CVPR’20 28.33 0.8820 TransWeather [26] CVPR’22 29.31 0.8879 TUM [25] CVPR’22 30.24 0.9020 WGWSNet [27] CVPR’23 approaches: All-in-One [24], TransWeather [26], TUM [25], WGWSNet [27], WeatherDiff [28], MW-ConvNet [31], and MWFormer [30].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions several methods and models but does not explicitly refer to any specific datasets. The names mentioned are all models or methods used for image restoration under adverse weather conditions.",
      "processing_time": 65.10582876205444,
      "citing_paper_id": "277622341",
      "cited_paper_id": 251197000
    },
    {
      "context_text": "…0.9017 Restormer [22] CVPR’22 30.52 0.9092 All-in-One [24] CVPR’20 28.33 0.8820 TransWeather [26] CVPR’22 29.31 0.8879 TUM [25] CVPR’22 30.24 0.9020 WGWSNet [27] CVPR’23 approaches: All-in-One [24], TransWeather [26], TUM [25], WGWSNet [27], WeatherDiff [28], MW-ConvNet [31], and MWFormer [30].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions several methods and models but does not explicitly refer to any specific datasets. The names mentioned are all models or methods used for image restoration under adverse weather conditions.",
      "processing_time": 65.10582876205444,
      "citing_paper_id": "277622341",
      "cited_paper_id": 259144110
    },
    {
      "context_text": "…0.9017 Restormer [22] CVPR’22 30.52 0.9092 All-in-One [24] CVPR’20 28.33 0.8820 TransWeather [26] CVPR’22 29.31 0.8879 TUM [25] CVPR’22 30.24 0.9020 WGWSNet [27] CVPR’23 approaches: All-in-One [24], TransWeather [26], TUM [25], WGWSNet [27], WeatherDiff [28], MW-ConvNet [31], and MWFormer [30].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions several methods and models but does not explicitly refer to any specific datasets. The names mentioned are all models or methods used for image restoration under adverse weather conditions.",
      "processing_time": 65.10582876205444,
      "citing_paper_id": "277622341",
      "cited_paper_id": 272956729
    },
    {
      "context_text": "…0.9017 Restormer [22] CVPR’22 30.52 0.9092 All-in-One [24] CVPR’20 28.33 0.8820 TransWeather [26] CVPR’22 29.31 0.8879 TUM [25] CVPR’22 30.24 0.9020 WGWSNet [27] CVPR’23 approaches: All-in-One [24], TransWeather [26], TUM [25], WGWSNet [27], WeatherDiff [28], MW-ConvNet [31], and MWFormer [30].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions several methods and models but does not explicitly refer to any specific datasets. The names mentioned are all models or methods used for image restoration under adverse weather conditions.",
      "processing_time": 65.10582876205444,
      "citing_paper_id": "277622341",
      "cited_paper_id": 274281468
    },
    {
      "context_text": "Several works [24]–[31] develop all-in-one models to simultaneously handle multiple weather types with a set of pre-trained weights.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to works developing all-in-one models for weather restoration.",
      "processing_time": 63.170886278152466,
      "citing_paper_id": "277622341",
      "cited_paper_id": 219978541
    },
    {
      "context_text": "Several works [24]–[31] develop all-in-one models to simultaneously handle multiple weather types with a set of pre-trained weights.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to works developing all-in-one models for weather restoration.",
      "processing_time": 63.170886278152466,
      "citing_paper_id": "277622341",
      "cited_paper_id": 272956729
    },
    {
      "context_text": "Furthermore, we perform the comparisons with all-in-one weather restoration ECCV’18 26.08 0.8108 DesnowNet [16] TIP’18 27.17 0.8983 JSTASR [17] ECCV’20 25.32 0.8076 DDMSNet [19] TIP’21 28.85 0.8772 MPRNet [20] CVPR’21 29.76 0.8949 NAFNet [21] ECCV’22 30.06 0.9017 Restormer [22] CVPR’22 30.52 0.9092…",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only model names and their performance metrics. No verifiable resources are identified.",
      "processing_time": 63.47910737991333,
      "citing_paper_id": "277622341",
      "cited_paper_id": 226308542
    },
    {
      "context_text": "JSTASR [17] develops a joint size and transparency-aware network to eliminate the veiling effect of snow.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for snow removal. The context and title do not provide information about datasets used.",
      "processing_time": 64.04668474197388,
      "citing_paper_id": "277622341",
      "cited_paper_id": 226308542
    },
    {
      "context_text": "In the second stage, apart from the residual estimation loss L res , we also employ the load balance loss L balance [64].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only losses used in the model training process.",
      "processing_time": 62.24555039405823,
      "citing_paper_id": "277622341",
      "cited_paper_id": 231573431
    },
    {
      "context_text": "Moreover, we compare our method with general weather restoration methods, including MAXIM [23], Restormer [22], MPRNet [20], and NAFNet [21], which employ a single model to tackle multiple weather degradations with task-specific pre-trained weight.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only models and methods. There are no clear identifiers for datasets.",
      "processing_time": 63.15659546852112,
      "citing_paper_id": "277622341",
      "cited_paper_id": 231802205
    },
    {
      "context_text": "Moreover, we compare our method with general weather restoration methods, including MAXIM [23], Restormer [22], MPRNet [20], and NAFNet [21], which employ a single model to tackle multiple weather degradations with task-specific pre-trained weight.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only models and methods. There are no clear identifiers for datasets.",
      "processing_time": 63.15659546852112,
      "citing_paper_id": "277622341",
      "cited_paper_id": 244346144
    },
    {
      "context_text": "Since then, various methods [20]– [23] have exploited a single model to tackle multiple degradations with task-specific pre-trained weights.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and approaches. There are no clear identifiers for datasets in the given context.",
      "processing_time": 63.810874938964844,
      "citing_paper_id": "277622341",
      "cited_paper_id": 231802205
    },
    {
      "context_text": "For instance, MPRNet [20] exploits a multi-stage strategy to refine restored images progressively.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions MPRNet, which is a method, not a dataset. No specific dataset is mentioned in the context.",
      "processing_time": 63.5942964553833,
      "citing_paper_id": "277622341",
      "cited_paper_id": 231802205
    },
    {
      "context_text": "Several approaches [20]–[23] explore general networks to tackle multiple degradations.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general approaches and methods. There are no clear identifiers for datasets.",
      "processing_time": 63.430908203125,
      "citing_paper_id": "277622341",
      "cited_paper_id": 231802205
    },
    {
      "context_text": "…restoration ECCV’18 26.08 0.8108 DesnowNet [16] TIP’18 27.17 0.8983 JSTASR [17] ECCV’20 25.32 0.8076 DDMSNet [19] TIP’21 28.85 0.8772 MPRNet [20] CVPR’21 29.76 0.8949 NAFNet [21] ECCV’22 30.06 0.9017 Restormer [22] CVPR’22 30.52 0.9092 All-in-One [24] CVPR’20 28.33 0.8820 TransWeather [26]…",
      "catation_intent": "findings",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various methods and their performance metrics. No dataset names are present in the text.",
      "processing_time": 64.1787633895874,
      "citing_paper_id": "277622341",
      "cited_paper_id": 231802205
    },
    {
      "context_text": "With the remarkable cross-modal representations and zero-shot capabilities, the large-scale vision-language model CLIP [35] is widely used in various tasks, such as image manipulation [46], [47], image generation [48], dense prediction [49], [50], and image restoration [37], [51], [52].",
      "catation_intent": "reusable resource",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions CLIP being used in various tasks, including image restoration, but does not specify any datasets. The cited papers do not provide additional context about datasets.",
      "processing_time": 65.23352026939392,
      "citing_paper_id": "277622341",
      "cited_paper_id": 232428282
    },
    {
      "context_text": "With the remarkable cross-modal representations and zero-shot capabilities, the large-scale vision-language model CLIP [35] is widely used in various tasks, such as image manipulation [46], [47], image generation [48], dense prediction [49], [50], and image restoration [37], [51], [52].",
      "catation_intent": "reusable resource",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions CLIP being used in various tasks, including image restoration, but does not specify any datasets. The cited papers do not provide additional context about datasets.",
      "processing_time": 65.23352026939392,
      "citing_paper_id": "277622341",
      "cited_paper_id": 244729320
    },
    {
      "context_text": "With the remarkable cross-modal representations and zero-shot capabilities, the large-scale vision-language model CLIP [35] is widely used in various tasks, such as image manipulation [46], [47], image generation [48], dense prediction [49], [50], and image restoration [37], [51], [52].",
      "catation_intent": "reusable resource",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions CLIP being used in various tasks, including image restoration, but does not specify any datasets. The cited papers do not provide additional context about datasets.",
      "processing_time": 65.23352026939392,
      "citing_paper_id": "277622341",
      "cited_paper_id": 265466453
    },
    {
      "context_text": "…JSTASR [17] ECCV’20 25.32 0.8076 DDMSNet [19] TIP’21 28.85 0.8772 MPRNet [20] CVPR’21 29.76 0.8949 NAFNet [21] ECCV’22 30.06 0.9017 Restormer [22] CVPR’22 30.52 0.9092 All-in-One [24] CVPR’20 28.33 0.8820 TransWeather [26] CVPR’22 29.31 0.8879 TUM [25] CVPR’22 30.24 0.9020 WGWSNet [27]…",
      "catation_intent": "findings",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only model names and their performance metrics. There are no clear identifiers for datasets.",
      "processing_time": 63.80259466171265,
      "citing_paper_id": "277622341",
      "cited_paper_id": 244346144
    },
    {
      "context_text": "Restormer [22] introduces an efficient Transformer that captures global dependency features in channel dimension for effective image restoration.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions Restormer, which is a method for image restoration, not a dataset. No specific datasets are mentioned.",
      "processing_time": 63.95836901664734,
      "citing_paper_id": "277622341",
      "cited_paper_id": 244346144
    },
    {
      "context_text": "Additionally, we utilize the three no-referenced metrics for perceptual quality evaluation without reference images, i.e. , NIQE [70], CLIP-IQA [71], and MANIQA [72].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions no-referenced metrics for perceptual quality evaluation, but does not specify any datasets. The cited paper titles do not indicate the presence of a dataset.",
      "processing_time": 65.49274826049805,
      "citing_paper_id": "277622341",
      "cited_paper_id": 248240148
    },
    {
      "context_text": "Although [36] overcomes some limitations of the diffusion paradigm used in WeatherDiffu-sion [28], it ignores the unique degradation characteristics across different weather conditions.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a comparison between methods. No verifiable resources are identified.",
      "processing_time": 63.359625577926636,
      "citing_paper_id": "277622341",
      "cited_paper_id": 251197000
    },
    {
      "context_text": "WeatherDiffusion [28] is the first attempt to employ the diffusion model for adverse weather restoration, yet there remains room for further performance improvement.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions 'WeatherDiffusion' but does not refer to it as a dataset. It is described as a method or model for adverse weather restoration.",
      "processing_time": 64.09971976280212,
      "citing_paper_id": "277622341",
      "cited_paper_id": 251197000
    },
    {
      "context_text": "As the power of generative paradigms, the diffusion model succeeds in restoring realistic and natural images [28], [33], a r X i v : 2504 .",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only generative paradigms and diffusion models. No clear identifiers for datasets are present.",
      "processing_time": 64.25580978393555,
      "citing_paper_id": "277622341",
      "cited_paper_id": 251197000
    },
    {
      "context_text": "WeatherDiffusion [28] is the first work that harnesses the diffusion model for adverse weather removal.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (diffusion model) for adverse weather removal.",
      "processing_time": 63.35275888442993,
      "citing_paper_id": "277622341",
      "cited_paper_id": 251197000
    },
    {
      "context_text": "As analyzed in [27], distinct weather degradations share common attributes, such as low contrast and color distortion.",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only general attributes of weather degradations. No clear, verifiable resource names are provided.",
      "processing_time": 63.930588483810425,
      "citing_paper_id": "277622341",
      "cited_paper_id": 259144110
    },
    {
      "context_text": "Furthermore, WGWSNet [27] adopts a two-stage training strategy to learn the general and specific characteristics of different weather degradations.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (WGWSNet) and its training strategy. No verifiable resources are identified.",
      "processing_time": 64.79769849777222,
      "citing_paper_id": "277622341",
      "cited_paper_id": 259144110
    },
    {
      "context_text": "To address this, Zhu et al. [27] propose a two-stage framework that separately extracts weather-general and weather-specific features.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for image restoration under multiple adverse weather conditions.",
      "processing_time": 63.34235715866089,
      "citing_paper_id": "277622341",
      "cited_paper_id": 259144110
    },
    {
      "context_text": "Moreover, learnable queries [26] or codebook priors [32] are introduced to facilitate weather-specific feature learning.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods or approaches. The context focuses on the introduction of learnable queries or codebook priors for weather-specific feature learning.",
      "processing_time": 65.81338715553284,
      "citing_paper_id": "277622341",
      "cited_paper_id": 265453871
    },
    {
      "context_text": "Sun et al. [52] explore the potential of pre-trained CLIP image encoder to extract cognitive information of preprocessed low-resolution images for real-world image super-resolution.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (CLIP) and a research topic (image super-resolution).",
      "processing_time": 64.58583068847656,
      "citing_paper_id": "277622341",
      "cited_paper_id": 265466453
    },
    {
      "context_text": "W EATHER conditions, as common climatic phenomena, inevitably degrade the visibility of images and hamper the performance of downstream vision tasks like object detection [1], [2] and scene understanding [3], [4].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general issues related to weather conditions affecting image visibility and downstream vision tasks.",
      "processing_time": 64.23195767402649,
      "citing_paper_id": "277622341",
      "cited_paper_id": 268008125
    },
    {
      "context_text": "The trained models are evaluated on BSD68 [74] and Urban100 [78].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "BSD68",
        "Urban100"
      ],
      "dataset_descriptions": {
        "BSD68": "Used to evaluate trained models on image restoration tasks, focusing on natural images with human-segmented annotations.",
        "Urban100": "Used to evaluate trained models on image restoration tasks, focusing on urban scenes with high-resolution images."
      },
      "confidence_score": 1.0,
      "reasoning": "The citation mentions two specific datasets, BSD68 and Urban100, which are used for evaluating trained models in the context of image restoration.",
      "processing_time": 77.80790877342224,
      "citing_paper_id": "271274210",
      "cited_paper_id": 64193
    },
    {
      "context_text": "In particular, on Urban100 [78], which contains images of higher resolution and quality than the BSD68 [74] dataset, the performance gain can be as large as 0.26 dB for σ = 50.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Urban100",
        "BSD68"
      ],
      "dataset_descriptions": {
        "Urban100": "Used to evaluate image restoration performance, focusing on higher resolution and quality images, demonstrating significant performance gains.",
        "BSD68": "Used as a baseline for comparison in image restoration evaluation, highlighting the performance improvement on Urban100."
      },
      "confidence_score": 1.0,
      "reasoning": "The citation mentions two specific datasets, Urban100 and BSD68, both of which are used to evaluate image restoration performance.",
      "processing_time": 77.17715430259705,
      "citing_paper_id": "271274210",
      "cited_paper_id": 64193
    },
    {
      "context_text": "2) Image Desnowing Results: We evaluate the proposed model on three widely used datasets for image desnowing, including Snow100K [6], SRRS [58], and CSD [21].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Snow100K",
        "SRRS",
        "CSD"
      ],
      "dataset_descriptions": {
        "Snow100K": "Used to evaluate the proposed desnowing model, focusing on removing snow from images using a large-scale synthetic dataset.",
        "SRRS": "Used to assess the effectiveness of the desnowing model, specifically targeting real-world snow removal scenarios.",
        "CSD": "Used to test the performance of the desnowing model, emphasizing the removal of snow from complex scenes."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions three specific datasets used for evaluating an image desnowing model. These datasets are clearly named and relevant to the research topic of all-in-one image restoration.",
      "processing_time": 84.26862788200378,
      "citing_paper_id": "271274210",
      "cited_paper_id": 837707
    },
    {
      "context_text": "2) Image Desnowing Results: We evaluate the proposed model on three widely used datasets for image desnowing, including Snow100K [6], SRRS [58], and CSD [21].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Snow100K",
        "SRRS",
        "CSD"
      ],
      "dataset_descriptions": {
        "Snow100K": "Used to evaluate the proposed desnowing model, focusing on removing snow from images using a large-scale synthetic dataset.",
        "SRRS": "Used to assess the effectiveness of the desnowing model, specifically targeting real-world snow removal scenarios.",
        "CSD": "Used to test the performance of the desnowing model, emphasizing the removal of snow from complex scenes."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions three specific datasets used for evaluating an image desnowing model. These datasets are clearly named and relevant to the research topic of all-in-one image restoration.",
      "processing_time": 84.26862788200378,
      "citing_paper_id": "271274210",
      "cited_paper_id": 226308542
    },
    {
      "context_text": "2) Image Desnowing Results: We evaluate the proposed model on three widely used datasets for image desnowing, including Snow100K [6], SRRS [58], and CSD [21].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Snow100K",
        "SRRS",
        "CSD"
      ],
      "dataset_descriptions": {
        "Snow100K": "Used to evaluate the proposed desnowing model, focusing on removing snow from images using a large-scale synthetic dataset.",
        "SRRS": "Used to assess the effectiveness of the desnowing model, specifically targeting real-world snow removal scenarios.",
        "CSD": "Used to test the performance of the desnowing model, emphasizing the removal of snow from complex scenes."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions three specific datasets used for evaluating an image desnowing model. These datasets are clearly named and relevant to the research topic of all-in-one image restoration.",
      "processing_time": 84.26862788200378,
      "citing_paper_id": "271274210",
      "cited_paper_id": 244461440
    },
    {
      "context_text": "In recent years, convolutional neural networks (CNNs) have demonstrated superior performance over traditional approaches on image restoration tasks by learning generalizable priors from large-scale datasets [5], [6], [7].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general reference to 'large-scale datasets'. No multi-word proper nouns, acronyms, or hyphenated names with digits are present.",
      "processing_time": 66.90173316001892,
      "citing_paper_id": "271274210",
      "cited_paper_id": 837707
    },
    {
      "context_text": "In recent years, convolutional neural networks (CNNs) have demonstrated superior performance over traditional approaches on image restoration tasks by learning generalizable priors from large-scale datasets [5], [6], [7].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general reference to 'large-scale datasets'. No multi-word proper nouns, acronyms, or hyphenated names with digits are present.",
      "processing_time": 66.90173316001892,
      "citing_paper_id": "271274210",
      "cited_paper_id": 265006592
    },
    {
      "context_text": "2) Image Desnowing: We evaluate our model on three datasets for image desnowing, including Snow100K [6], SRRS [58], and CSD [21].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Snow100K",
        "SRRS",
        "CSD"
      ],
      "dataset_descriptions": {
        "Snow100K": "Used to evaluate the model's performance on image desnowing, focusing on removing snow from synthetic images with varying levels of snow intensity.",
        "SRRS": "Used to assess the effectiveness of the model in removing snow from real-world images, emphasizing realistic snow removal scenarios.",
        "CSD": "Used to test the model's ability to handle diverse snow conditions, including both synthetic and real images, ensuring robust desnowing performance."
      },
      "confidence_score": 1.0,
      "reasoning": "The context explicitly mentions three datasets used for evaluating the model on image desnowing tasks. These datasets are Snow100K, SRRS, and CSD.",
      "processing_time": 86.73660945892334,
      "citing_paper_id": "271274210",
      "cited_paper_id": 837707
    },
    {
      "context_text": "2) Image Desnowing: We evaluate our model on three datasets for image desnowing, including Snow100K [6], SRRS [58], and CSD [21].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Snow100K",
        "SRRS",
        "CSD"
      ],
      "dataset_descriptions": {
        "Snow100K": "Used to evaluate the model's performance on image desnowing, focusing on removing snow from synthetic images with varying levels of snow intensity.",
        "SRRS": "Used to assess the effectiveness of the model in removing snow from real-world images, emphasizing realistic snow removal scenarios.",
        "CSD": "Used to test the model's ability to handle diverse snow conditions, including both synthetic and real images, ensuring robust desnowing performance."
      },
      "confidence_score": 1.0,
      "reasoning": "The context explicitly mentions three datasets used for evaluating the model on image desnowing tasks. These datasets are Snow100K, SRRS, and CSD.",
      "processing_time": 86.73660945892334,
      "citing_paper_id": "271274210",
      "cited_paper_id": 226308542
    },
    {
      "context_text": "2) Image Desnowing: We evaluate our model on three datasets for image desnowing, including Snow100K [6], SRRS [58], and CSD [21].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Snow100K",
        "SRRS",
        "CSD"
      ],
      "dataset_descriptions": {
        "Snow100K": "Used to evaluate the model's performance on image desnowing, focusing on removing snow from synthetic images with varying levels of snow intensity.",
        "SRRS": "Used to assess the effectiveness of the model in removing snow from real-world images, emphasizing realistic snow removal scenarios.",
        "CSD": "Used to test the model's ability to handle diverse snow conditions, including both synthetic and real images, ensuring robust desnowing performance."
      },
      "confidence_score": 1.0,
      "reasoning": "The context explicitly mentions three datasets used for evaluating the model on image desnowing tasks. These datasets are Snow100K, SRRS, and CSD.",
      "processing_time": 86.73660945892334,
      "citing_paper_id": "271274210",
      "cited_paper_id": 244461440
    },
    {
      "context_text": "SRRS [58], AND S NOW 100K [6] D ATASETS which contains more complex snow scenarios, the superiority of our model is more pronounced, indicating the efficacy of our approach in addressing intricate snow degradations.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "SRRS",
        "SNOW 100K"
      ],
      "dataset_descriptions": {
        "SRRS": "Used to evaluate the model's performance in complex snow scenarios, demonstrating its effectiveness in addressing intricate snow degradations.",
        "SNOW 100K": "Used to evaluate the model's performance in complex snow scenarios, demonstrating its effectiveness in addressing intricate snow degradations."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions two datasets, SRRS and SNOW 100K, which are used to evaluate the performance of the model in handling complex snow scenarios.",
      "processing_time": 80.40618419647217,
      "citing_paper_id": "271274210",
      "cited_paper_id": 837707
    },
    {
      "context_text": "SRRS [58], AND S NOW 100K [6] D ATASETS which contains more complex snow scenarios, the superiority of our model is more pronounced, indicating the efficacy of our approach in addressing intricate snow degradations.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "SRRS",
        "SNOW 100K"
      ],
      "dataset_descriptions": {
        "SRRS": "Used to evaluate the model's performance in complex snow scenarios, demonstrating its effectiveness in addressing intricate snow degradations.",
        "SNOW 100K": "Used to evaluate the model's performance in complex snow scenarios, demonstrating its effectiveness in addressing intricate snow degradations."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions two datasets, SRRS and SNOW 100K, which are used to evaluate the performance of the model in handling complex snow scenarios.",
      "processing_time": 80.40618419647217,
      "citing_paper_id": "271274210",
      "cited_paper_id": 226308542
    },
    {
      "context_text": "Specifically, for image denoising, we employ BSD400 [76] and WED [77] for training.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "BSD400",
        "WED"
      ],
      "dataset_descriptions": {
        "BSD400": "Used for training image denoising models, focusing on improving denoising performance through a diverse set of images.",
        "WED": "Used for training image denoising models, providing a variety of watermarked and enhanced images to improve robustness."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions two specific datasets, BSD400 and WED, which are used for training in the context of image denoising.",
      "processing_time": 78.26281809806824,
      "citing_paper_id": "271274210",
      "cited_paper_id": 4840263
    },
    {
      "context_text": "To model long-range dependencies, Transformer [31] has been incorporated into image restoration [14], [32].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only models and methods. The cited papers do not provide additional information about datasets.",
      "processing_time": 64.24327850341797,
      "citing_paper_id": "271274210",
      "cited_paper_id": 13756489
    },
    {
      "context_text": "To model long-range dependencies, Transformer [31] has been incorporated into image restoration [14], [32].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only models and methods. The cited papers do not provide additional information about datasets.",
      "processing_time": 64.24327850341797,
      "citing_paper_id": "271274210",
      "cited_paper_id": 227239228
    },
    {
      "context_text": "For image dehazing, RESIDE-β [3] and SOTS-Outdoor [79] are used for training and testing, respectively.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "RESIDE-β",
        "SOTS-Outdoor"
      ],
      "dataset_descriptions": {
        "RESIDE-β": "Used for training models in image dehazing, focusing on improving visibility and quality of hazy images through deep learning techniques.",
        "SOTS-Outdoor": "Used for testing the performance of dehazing models, evaluating their effectiveness in enhancing outdoor scenes with varying degrees of haze."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions two specific datasets, RESIDE-β and SOTS-Outdoor, which are used for training and testing in the context of image dehazing.",
      "processing_time": 80.0968325138092,
      "citing_paper_id": "271274210",
      "cited_paper_id": 49864080
    },
    {
      "context_text": "We conduct dehazing experiments on four kinds of datasets: a daytime synthetic dataset (RESIDE [3]), four daytime real-world datasets (Dense-Haze [44], NH-HAZE [45], O-Haze [46], and I-Haze [47]), two nighttime datasets (NHR [56], GTA5 [25]), and a remote sensing dataset (SateHaze1k [57]).",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "RESIDE",
        "Dense-Haze",
        "NH-HAZE",
        "O-Haze",
        "I-Haze",
        "NHR",
        "GTA5",
        "SateHaze1k"
      ],
      "dataset_descriptions": {
        "RESIDE": "Used for dehazing experiments with synthetic daytime images, providing a controlled environment to test and validate dehazing algorithms.",
        "Dense-Haze": "Used for dehazing experiments with real-world daytime images, focusing on dense haze conditions to evaluate algorithm performance.",
        "NH-HAZE": "Used for dehazing experiments with real-world daytime images, emphasizing natural hazy conditions to assess algorithm robustness.",
        "O-Haze": "Used for dehazing experiments with real-world outdoor images, providing a benchmark for evaluating dehazing methods under various outdoor conditions.",
        "I-Haze": "Used for dehazing experiments with real-world indoor images, assessing the effectiveness of dehazing algorithms in indoor environments.",
        "NHR": "Used for dehazing experiments with real-world nighttime images, focusing on low-light conditions to evaluate the performance of dehazing algorithms.",
        "GTA5": "Used to study image restoration in nighttime scenes, focusing on realistic lighting and atmospheric conditions to evaluate dehazing algorithms.",
        "SateHaze1k": "Used for dehazing experiments with satellite imagery, focusing on remote sensing applications to improve image clarity and quality."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions several specific datasets used for dehazing experiments, including both synthetic and real-world datasets, as well as nighttime and remote sensing datasets.",
      "processing_time": 102.11522340774536,
      "citing_paper_id": "271274210",
      "cited_paper_id": 102350705
    },
    {
      "context_text": "We conduct dehazing experiments on four kinds of datasets: a daytime synthetic dataset (RESIDE [3]), four daytime real-world datasets (Dense-Haze [44], NH-HAZE [45], O-Haze [46], and I-Haze [47]), two nighttime datasets (NHR [56], GTA5 [25]), and a remote sensing dataset (SateHaze1k [57]).",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "RESIDE",
        "Dense-Haze",
        "NH-HAZE",
        "O-Haze",
        "I-Haze",
        "NHR",
        "GTA5",
        "SateHaze1k"
      ],
      "dataset_descriptions": {
        "RESIDE": "Used for dehazing experiments with synthetic daytime images, providing a controlled environment to test and validate dehazing algorithms.",
        "Dense-Haze": "Used for dehazing experiments with real-world daytime images, focusing on dense haze conditions to evaluate algorithm performance.",
        "NH-HAZE": "Used for dehazing experiments with real-world daytime images, emphasizing natural hazy conditions to assess algorithm robustness.",
        "O-Haze": "Used for dehazing experiments with real-world outdoor images, providing a benchmark for evaluating dehazing methods under various outdoor conditions.",
        "I-Haze": "Used for dehazing experiments with real-world indoor images, assessing the effectiveness of dehazing algorithms in indoor environments.",
        "NHR": "Used for dehazing experiments with real-world nighttime images, focusing on low-light conditions to evaluate the performance of dehazing algorithms.",
        "GTA5": "Used to study image restoration in nighttime scenes, focusing on realistic lighting and atmospheric conditions to evaluate dehazing algorithms.",
        "SateHaze1k": "Used for dehazing experiments with satellite imagery, focusing on remote sensing applications to improve image clarity and quality."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions several specific datasets used for dehazing experiments, including both synthetic and real-world datasets, as well as nighttime and remote sensing datasets.",
      "processing_time": 102.11522340774536,
      "citing_paper_id": "271274210",
      "cited_paper_id": 206598041
    },
    {
      "context_text": "We conduct dehazing experiments on four kinds of datasets: a daytime synthetic dataset (RESIDE [3]), four daytime real-world datasets (Dense-Haze [44], NH-HAZE [45], O-Haze [46], and I-Haze [47]), two nighttime datasets (NHR [56], GTA5 [25]), and a remote sensing dataset (SateHaze1k [57]).",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "RESIDE",
        "Dense-Haze",
        "NH-HAZE",
        "O-Haze",
        "I-Haze",
        "NHR",
        "GTA5",
        "SateHaze1k"
      ],
      "dataset_descriptions": {
        "RESIDE": "Used for dehazing experiments with synthetic daytime images, providing a controlled environment to test and validate dehazing algorithms.",
        "Dense-Haze": "Used for dehazing experiments with real-world daytime images, focusing on dense haze conditions to evaluate algorithm performance.",
        "NH-HAZE": "Used for dehazing experiments with real-world daytime images, emphasizing natural hazy conditions to assess algorithm robustness.",
        "O-Haze": "Used for dehazing experiments with real-world outdoor images, providing a benchmark for evaluating dehazing methods under various outdoor conditions.",
        "I-Haze": "Used for dehazing experiments with real-world indoor images, assessing the effectiveness of dehazing algorithms in indoor environments.",
        "NHR": "Used for dehazing experiments with real-world nighttime images, focusing on low-light conditions to evaluate the performance of dehazing algorithms.",
        "GTA5": "Used to study image restoration in nighttime scenes, focusing on realistic lighting and atmospheric conditions to evaluate dehazing algorithms.",
        "SateHaze1k": "Used for dehazing experiments with satellite imagery, focusing on remote sensing applications to improve image clarity and quality."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions several specific datasets used for dehazing experiments, including both synthetic and real-world datasets, as well as nighttime and remote sensing datasets.",
      "processing_time": 102.11522340774536,
      "citing_paper_id": "271274210",
      "cited_paper_id": 211732223
    },
    {
      "context_text": "To address these issues, all-in-one image restoration has emerged as a promising topic by dealing with multiple degradations using a unified model [39], [40].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general topic and approach. There are no clear identifiers for datasets in the text.",
      "processing_time": 64.55180311203003,
      "citing_paper_id": "271274210",
      "cited_paper_id": 195787503
    },
    {
      "context_text": "Figure 7 illustrates that the Asimage dehazing plays an important role in remote sensing, we apply our model to three subsets of a widely used remote sensing dehazing dataset, SateHaze1k [57].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "SateHaze1k"
      ],
      "dataset_descriptions": {
        "SateHaze1k": "Used to evaluate the effectiveness of the Asimage dehazing model in remote sensing applications, focusing on the performance of dehazing algorithms on satellite imagery."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions 'SateHaze1k' as a dataset used for remote sensing dehazing, which is directly relevant to the research topic of image restoration.",
      "processing_time": 75.04626536369324,
      "citing_paper_id": "271274210",
      "cited_paper_id": 211732223
    },
    {
      "context_text": "Besides, the model is applied to remote sensing with Sate-Haze1k [57].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Sate-Haze1k"
      ],
      "dataset_descriptions": {
        "Sate-Haze1k": "Used to apply the model for remote sensing dehazing, focusing on improving image clarity using SAR image prior and conditional GANs."
      },
      "confidence_score": 0.9,
      "reasoning": "The context mentions 'Sate-Haze1k' which appears to be a specific dataset used for remote sensing dehazing experiments.",
      "processing_time": 73.02024364471436,
      "citing_paper_id": "271274210",
      "cited_paper_id": 211732223
    },
    {
      "context_text": "(b ) The number of parameters vs. PSNR on the DPDD [4] dataset for image defocus deblurring.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "DPDD"
      ],
      "dataset_descriptions": {
        "DPDD": "Used to evaluate image defocus deblurring methods, focusing on the relationship between the number of parameters and PSNR. The dataset provides dual-pixel data for this purpose."
      },
      "confidence_score": 1.0,
      "reasoning": "The citation mentions the DPDD dataset, which is used for evaluating image defocus deblurring methods. The dataset is clearly identified and relevant to the research topic.",
      "processing_time": 73.96092581748962,
      "citing_paper_id": "271274210",
      "cited_paper_id": 218470249
    },
    {
      "context_text": "For single-image defocus deblurring, our model achieves a performance gain of 0.2 dB PSNR over the strong Trans-former model Restormer [1] in the combined category of the DPDD [4] dataset, while consuming 46% fewer parameters.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "DPDD"
      ],
      "dataset_descriptions": {
        "DPDD": "Used to evaluate the performance of the model in single-image defocus deblurring, comparing PSNR gains and parameter efficiency against the Restormer model."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions the DPDD dataset, which is used for evaluating the performance of the model in single-image defocus deblurring. The dataset is specific and relevant to the research topic.",
      "processing_time": 74.16202569007874,
      "citing_paper_id": "271274210",
      "cited_paper_id": 218470249
    },
    {
      "context_text": "For single-image defocus deblurring, our model achieves a performance gain of 0.2 dB PSNR over the strong Trans-former model Restormer [1] in the combined category of the DPDD [4] dataset, while consuming 46% fewer parameters.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "DPDD"
      ],
      "dataset_descriptions": {
        "DPDD": "Used to evaluate the performance of the model in single-image defocus deblurring, comparing PSNR gains and parameter efficiency against the Restormer model."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions the DPDD dataset, which is used for evaluating the performance of the model in single-image defocus deblurring. The dataset is specific and relevant to the research topic.",
      "processing_time": 74.16202569007874,
      "citing_paper_id": "271274210",
      "cited_paper_id": 244346144
    },
    {
      "context_text": "3) Image Defocus Deblurring Results: We verify the effectiveness of the network for single-image defocus deblurring using the DPDD [4] dataset.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "DPDD"
      ],
      "dataset_descriptions": {
        "DPDD": "Used to verify the effectiveness of the network for single-image defocus deblurring, focusing on the performance of the model in removing defocus blur from images."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions the use of the DPDD dataset for verifying the effectiveness of the network in single-image defocus deblurring.",
      "processing_time": 72.11058497428894,
      "citing_paper_id": "271274210",
      "cited_paper_id": 218470249
    },
    {
      "context_text": "3) Image Defocus Deblurring: Consistent with [1] and [9], we utilize DPDD [4] for evaluation.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "DPDD"
      ],
      "dataset_descriptions": {
        "DPDD": "Used for evaluating image defocus deblurring methods, providing dual-pixel data for consistent performance assessment."
      },
      "confidence_score": 1.0,
      "reasoning": "DPDD is identified as a dataset used for evaluation in the context of image defocus deblurring. The cited paper 'Defocus Deblurring Using Dual-Pixel Data' confirms DPDD as a dataset.",
      "processing_time": 72.6407299041748,
      "citing_paper_id": "271274210",
      "cited_paper_id": 218470249
    },
    {
      "context_text": "3) Image Defocus Deblurring: Consistent with [1] and [9], we utilize DPDD [4] for evaluation.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "DPDD"
      ],
      "dataset_descriptions": {
        "DPDD": "Used for evaluating image defocus deblurring methods, providing dual-pixel data for consistent performance assessment."
      },
      "confidence_score": 1.0,
      "reasoning": "DPDD is identified as a dataset used for evaluation in the context of image defocus deblurring. The cited paper 'Defocus Deblurring Using Dual-Pixel Data' confirms DPDD as a dataset.",
      "processing_time": 72.6407299041748,
      "citing_paper_id": "271274210",
      "cited_paper_id": 244346144
    },
    {
      "context_text": "3) Image Defocus Deblurring: Consistent with [1] and [9], we utilize DPDD [4] for evaluation.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "DPDD"
      ],
      "dataset_descriptions": {
        "DPDD": "Used for evaluating image defocus deblurring methods, providing dual-pixel data for consistent performance assessment."
      },
      "confidence_score": 1.0,
      "reasoning": "DPDD is identified as a dataset used for evaluation in the context of image defocus deblurring. The cited paper 'Defocus Deblurring Using Dual-Pixel Data' confirms DPDD as a dataset.",
      "processing_time": 72.6407299041748,
      "citing_paper_id": "271274210",
      "cited_paper_id": 247922539
    },
    {
      "context_text": "We adopt the same dataset configuration as previous methods [12], [37], [58] for fairness.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.2,
      "reasoning": "The citation does not mention any specific datasets, only that the same dataset configuration as previous methods is adopted. No clear, verifiable dataset names are provided.",
      "processing_time": 65.71439719200134,
      "citing_paper_id": "271274210",
      "cited_paper_id": 226308542
    },
    {
      "context_text": "We adopt the same dataset configuration as previous methods [12], [37], [58] for fairness.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.2,
      "reasoning": "The citation does not mention any specific datasets, only that the same dataset configuration as previous methods is adopted. No clear, verifiable dataset names are provided.",
      "processing_time": 65.71439719200134,
      "citing_paper_id": "271274210",
      "cited_paper_id": 265537221
    },
    {
      "context_text": "More recently, Transformer models have been introduced into image restoration, significantly advancing the state-of-the-art performance [1], [13], [14].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only the introduction of Transformer models into image restoration. No verifiable resources are named.",
      "processing_time": 64.75258326530457,
      "citing_paper_id": "271274210",
      "cited_paper_id": 227239228
    },
    {
      "context_text": "More recently, Transformer models have been introduced into image restoration, significantly advancing the state-of-the-art performance [1], [13], [14].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only the introduction of Transformer models into image restoration. No verifiable resources are named.",
      "processing_time": 64.75258326530457,
      "citing_paper_id": "271274210",
      "cited_paper_id": 235358213
    },
    {
      "context_text": "More recently, Transformer models have been introduced into image restoration, significantly advancing the state-of-the-art performance [1], [13], [14].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only the introduction of Transformer models into image restoration. No verifiable resources are named.",
      "processing_time": 64.75258326530457,
      "citing_paper_id": "271274210",
      "cited_paper_id": 244346144
    },
    {
      "context_text": "To boost performance, many sophisticated functional units have been developed or borrowed from other domains for image restoration, such as the encoder-decoder architecture [8], residual connection [9], dilated convolution [10], and attention mechanisms [11], [12].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only methods and architectures. The cited papers' titles do not provide additional information about datasets.",
      "processing_time": 64.97609329223633,
      "citing_paper_id": "271274210",
      "cited_paper_id": 231802205
    },
    {
      "context_text": "To boost performance, many sophisticated functional units have been developed or borrowed from other domains for image restoration, such as the encoder-decoder architecture [8], residual connection [9], dilated convolution [10], and attention mechanisms [11], [12].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only methods and architectures. The cited papers' titles do not provide additional information about datasets.",
      "processing_time": 64.97609329223633,
      "citing_paper_id": "271274210",
      "cited_paper_id": 236976210
    },
    {
      "context_text": "To boost performance, many sophisticated functional units have been developed or borrowed from other domains for image restoration, such as the encoder-decoder architecture [8], residual connection [9], dilated convolution [10], and attention mechanisms [11], [12].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only methods and architectures. The cited papers' titles do not provide additional information about datasets.",
      "processing_time": 64.97609329223633,
      "citing_paper_id": "271274210",
      "cited_paper_id": 238634689
    },
    {
      "context_text": "To boost performance, many sophisticated functional units have been developed or borrowed from other domains for image restoration, such as the encoder-decoder architecture [8], residual connection [9], dilated convolution [10], and attention mechanisms [11], [12].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only methods and architectures. The cited papers' titles do not provide additional information about datasets.",
      "processing_time": 64.97609329223633,
      "citing_paper_id": "271274210",
      "cited_paper_id": 247922539
    },
    {
      "context_text": "3) Local Branch: In light of the crucial role played by local information in image restoration [1], [13], we introduce an extremely lightweight yet effective local branch for local signals modulation using a 1 × 1 depth-wise convolutional layer, as illustrated in Figure 2 (b).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only the importance of local information in image restoration and the introduction of a local branch in the model.",
      "processing_time": 65.43824005126953,
      "citing_paper_id": "271274210",
      "cited_paper_id": 235358213
    },
    {
      "context_text": "3) Local Branch: In light of the crucial role played by local information in image restoration [1], [13], we introduce an extremely lightweight yet effective local branch for local signals modulation using a 1 × 1 depth-wise convolutional layer, as illustrated in Figure 2 (b).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only the importance of local information in image restoration and the introduction of a local branch in the model.",
      "processing_time": 65.43824005126953,
      "citing_paper_id": "271274210",
      "cited_paper_id": 244346144
    },
    {
      "context_text": "To reduce the complexity of self-attention, common practices include restricting self-attention regions [13], [33] and switching from the spatial dimension to the channel dimension [1].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only methods and approaches for reducing the complexity of self-attention in image restoration.",
      "processing_time": 64.58087611198425,
      "citing_paper_id": "271274210",
      "cited_paper_id": 235358213
    },
    {
      "context_text": "To reduce the complexity of self-attention, common practices include restricting self-attention regions [13], [33] and switching from the spatial dimension to the channel dimension [1].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only methods and approaches for reducing the complexity of self-attention in image restoration.",
      "processing_time": 64.58087611198425,
      "citing_paper_id": "271274210",
      "cited_paper_id": 244346144
    },
    {
      "context_text": "To reduce the complexity of self-attention, common practices include restricting self-attention regions [13], [33] and switching from the spatial dimension to the channel dimension [1].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only methods and approaches for reducing the complexity of self-attention in image restoration.",
      "processing_time": 64.58087611198425,
      "citing_paper_id": "271274210",
      "cited_paper_id": 257255385
    },
    {
      "context_text": "Transformer has become renowned for their capacity for modeling long-range dependencies [1], [13], [34], [35].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only references to the capacity of transformers for modeling long-range dependencies. None of the cited papers' titles suggest the use of specific datasets.",
      "processing_time": 66.53757882118225,
      "citing_paper_id": "271274210",
      "cited_paper_id": 235358213
    },
    {
      "context_text": "Transformer has become renowned for their capacity for modeling long-range dependencies [1], [13], [34], [35].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only references to the capacity of transformers for modeling long-range dependencies. None of the cited papers' titles suggest the use of specific datasets.",
      "processing_time": 66.53757882118225,
      "citing_paper_id": "271274210",
      "cited_paper_id": 244346144
    },
    {
      "context_text": "Transformer has become renowned for their capacity for modeling long-range dependencies [1], [13], [34], [35].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only references to the capacity of transformers for modeling long-range dependencies. None of the cited papers' titles suggest the use of specific datasets.",
      "processing_time": 66.53757882118225,
      "citing_paper_id": "271274210",
      "cited_paper_id": 248069347
    },
    {
      "context_text": "Transformer has become renowned for their capacity for modeling long-range dependencies [1], [13], [34], [35].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only references to the capacity of transformers for modeling long-range dependencies. None of the cited papers' titles suggest the use of specific datasets.",
      "processing_time": 66.53757882118225,
      "citing_paper_id": "271274210",
      "cited_paper_id": 250085592
    },
    {
      "context_text": "In addition to the regular depth-wise convolution, we also employ 1 × K and K × 1 depth-wise convolutions in parallel to the square one to harvest strip-shaped contextual information, inspired by strip-based self-attention [33], [43].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only methods and architectural choices. The cited papers do not provide additional context to identify datasets.",
      "processing_time": 64.49018740653992,
      "citing_paper_id": "271274210",
      "cited_paper_id": 235694312
    },
    {
      "context_text": "In addition to the regular depth-wise convolution, we also employ 1 × K and K × 1 depth-wise convolutions in parallel to the square one to harvest strip-shaped contextual information, inspired by strip-based self-attention [33], [43].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only methods and architectural choices. The cited papers do not provide additional context to identify datasets.",
      "processing_time": 64.49018740653992,
      "citing_paper_id": "271274210",
      "cited_paper_id": 257255385
    },
    {
      "context_text": "A common practice is to adopt existing tools, such as the fast Fourier transform [36], wavelet transform [10], and global average pooling [37], to generate explicit or implicit frequency features from spatial inputs [38].",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only methods and tools. The cited papers' titles do not introduce any specific datasets either.",
      "processing_time": 64.33799910545349,
      "citing_paper_id": "271274210",
      "cited_paper_id": 238634689
    },
    {
      "context_text": "A common practice is to adopt existing tools, such as the fast Fourier transform [36], wavelet transform [10], and global average pooling [37], to generate explicit or implicit frequency features from spatial inputs [38].",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only methods and tools. The cited papers' titles do not introduce any specific datasets either.",
      "processing_time": 64.33799910545349,
      "citing_paper_id": "271274210",
      "cited_paper_id": 257999356
    },
    {
      "context_text": "A common practice is to adopt existing tools, such as the fast Fourier transform [36], wavelet transform [10], and global average pooling [37], to generate explicit or implicit frequency features from spatial inputs [38].",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only methods and tools. The cited papers' titles do not introduce any specific datasets either.",
      "processing_time": 64.33799910545349,
      "citing_paper_id": "271274210",
      "cited_paper_id": 265537221
    },
    {
      "context_text": "Moreover, our method provides an average performance improvement of 0.11 dB over Restormer [1] while utilizing a lower memory footprint.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a comparison with a method called Restormer.",
      "processing_time": 63.16315817832947,
      "citing_paper_id": "271274210",
      "cited_paper_id": 244346144
    },
    {
      "context_text": "In particular, our model achieves a notable improvement of 0.27 dB PSNR over the strong Transformer model Restormer [1] in outdoor scenes while utilizing only 54% parameters.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a comparison with a model. No dataset names are present in the citation span.",
      "processing_time": 64.17960262298584,
      "citing_paper_id": "271274210",
      "cited_paper_id": 244346144
    },
    {
      "context_text": "Moreover, the model demonstrates remarkable proficiency in the image desnowing task, outperforming the recent algorithm IRNeXt [12] by 0.7 dB PSNR on the widely used CSD [21] dataset.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "CSD"
      ],
      "dataset_descriptions": {
        "CSD": "Used to evaluate the performance of the image desnowing task, specifically comparing the proposed model against IRNeXt using PSNR as the metric."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions the CSD dataset, which is used for evaluating the performance of the image desnowing task. The dataset is clearly identified and used for benchmarking.",
      "processing_time": 73.11140084266663,
      "citing_paper_id": "271274210",
      "cited_paper_id": 244461440
    },
    {
      "context_text": "Specifically, it outperforms the expensive DehazeFormer-L [34] by 0.74 dB PSNR on the SOTS-Indoor [3] dataset with only 14% FLOPs, as shown in Figure 1 (a).",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "SOTS-Indoor"
      ],
      "dataset_descriptions": {
        "SOTS-Indoor": "Used to evaluate the performance of the proposed method against DehazeFormer-L, focusing on PSNR and FLOPs, specifically for indoor dehazing tasks."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions the SOTS-Indoor dataset, which is used to evaluate the performance of the proposed method against DehazeFormer-L in terms of PSNR and FLOPs.",
      "processing_time": 75.62772583961487,
      "citing_paper_id": "271274210",
      "cited_paper_id": 248069347
    },
    {
      "context_text": "Following [35], [37], the models are trained for 5000 epochs on a patch size of 800 × 1200 with a batch size of 2.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only training parameters and model configurations. No verifiable resources are identified.",
      "processing_time": 63.46079158782959,
      "citing_paper_id": "271274210",
      "cited_paper_id": 250085592
    },
    {
      "context_text": "Following [35], [37], the models are trained for 5000 epochs on a patch size of 800 × 1200 with a batch size of 2.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only training parameters and model configurations. No verifiable resources are identified.",
      "processing_time": 63.46079158782959,
      "citing_paper_id": "271274210",
      "cited_paper_id": 265537221
    },
    {
      "context_text": "Inspired by this operation, a few recent works on CNNs strike back by designing efficient frameworks with large kernel convolutions, such as 31 × 31 in RepLKNet [15] and 51 × 51 in SLaK [16].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only models and methods. The citation intent is to reference research work on large kernel convolutions in CNNs.",
      "processing_time": 65.17956829071045,
      "citing_paper_id": "271274210",
      "cited_paper_id": 250408169
    },
    {
      "context_text": "Inspired by this operation, a few recent works on CNNs strike back by designing efficient frameworks with large kernel convolutions, such as 31 × 31 in RepLKNet [15] and 51 × 51 in SLaK [16].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only models and methods. The citation intent is to reference research work on large kernel convolutions in CNNs.",
      "processing_time": 65.17956829071045,
      "citing_paper_id": "271274210",
      "cited_paper_id": 265039963
    },
    {
      "context_text": "SLaK [16] employs sparse factorized 51 × 51 kernels to confront Transformer methods.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (SLaK) and its application to confront Transformer methods.",
      "processing_time": 63.79626703262329,
      "citing_paper_id": "271274210",
      "cited_paper_id": 250408169
    },
    {
      "context_text": "LKD-Net [17] decomposes a depth-wise convolution into a smaller depth-wise convolution and a depth-wise dilated convolution.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (LKD-Net).",
      "processing_time": 62.893728256225586,
      "citing_paper_id": "271274210",
      "cited_paper_id": 252090171
    },
    {
      "context_text": "In the context of image restoration, LKDNet [17] decomposes a 21 × 21 convolution into a smaller depth-wise convolution and a depth-wise dilated convolution for image dehazing.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (LKDNet) for image dehazing.",
      "processing_time": 63.19402837753296,
      "citing_paper_id": "271274210",
      "cited_paper_id": 252090171
    },
    {
      "context_text": "MAN [19] develops the large kernel attention by decomposing a convolution into three distinct types of convolutions.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for image super-resolution.",
      "processing_time": 62.17456126213074,
      "citing_paper_id": "271274210",
      "cited_paper_id": 252595608
    },
    {
      "context_text": "MAN [19] decomposes a large kernel convolution into a depth-wise convolution, a depth-wise dilated convolution, and a point-wise convolution.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for image super-resolution.",
      "processing_time": 62.21128249168396,
      "citing_paper_id": "271274210",
      "cited_paper_id": 252595608
    },
    {
      "context_text": "Moreover, compared to LaKDNet [18], which also employs large kernel convolutions, our model yields performance gains of 0.03 dB PSNR and 0.002 SSIM on the combined category with 21% fewer parameters, as illustrated in Figure 1 (b).",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only performance metrics and parameter counts. No verifiable resources are identified.",
      "processing_time": 63.37514352798462,
      "citing_paper_id": "271274210",
      "cited_paper_id": 256615401
    },
    {
      "context_text": "In the realm of image restoration, LaKDNet [18] employs a combination of large kernel (9 × 9) depth-wise convolutions and point-wise convolutions to expand the effective receptive field.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (LaKDNet) and its components. No verifiable resources are identified.",
      "processing_time": 64.28746676445007,
      "citing_paper_id": "271274210",
      "cited_paper_id": 256615401
    },
    {
      "context_text": "LaKDNet [18] leverages large kernel convolutions ( e.g., 9 × 9) followed by point-wise convolutions to obtain large effective receptive fields for image deblurring.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a method (LaKDNet) and its application to image deblurring.",
      "processing_time": 63.80794906616211,
      "citing_paper_id": "271274210",
      "cited_paper_id": 256615401
    },
    {
      "context_text": "Furthermore, our model demonstrates superior performance on all real-world datasets compared to SDCE [38], which is elaborately designed for real-world scenarios.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.0,
      "reasoning": "The context does not mention any specific, verifiable datasets. It only refers to 'real-world datasets' in a generic sense.",
      "processing_time": 64.49253439903259,
      "citing_paper_id": "271274210",
      "cited_paper_id": 257999356
    },
    {
      "context_text": "CNN-based methods have long dominated image restoration by designing advanced functional units [27].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method or approach. The context focuses on CNN-based methods and their role in image restoration.",
      "processing_time": 64.48728156089783,
      "citing_paper_id": "271274210",
      "cited_paper_id": 260862867
    },
    {
      "context_text": "For example, RepLKNet [15] achieves a kernel size of 31 × 31 by following several guidelines for designing large convolutions, thereby significantly narrowing the performance gap between CNNs and Transformer models.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (RepLKNet) and its performance compared to other models.",
      "processing_time": 64.05710434913635,
      "citing_paper_id": "271274210",
      "cited_paper_id": 265039963
    },
    {
      "context_text": "Table III shows that our method outperforms the recent FocalNet [37] by 2.57 dB PSNR and 0.01 SSIM.",
      "catation_intent": "findings",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only performance metrics against a method.",
      "processing_time": 62.28875207901001,
      "citing_paper_id": "271274210",
      "cited_paper_id": 265537221
    },
    {
      "context_text": "Table II shows that our method exhibits a notable enhancement in performance compared to the general method [37] and the specially designed method [59].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only comparisons to other methods. No verifiable resources are identified.",
      "processing_time": 63.111621141433716,
      "citing_paper_id": "271274210",
      "cited_paper_id": 265537221
    },
    {
      "context_text": "Moreover, Table XII shows that our model outperforms PromptIR [22] by 0.1 dB PSNR when averaging across two denoising datasets.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.3,
      "reasoning": "The context mentions 'two denoising datasets' but does not provide specific names. The term 'denoising datasets' is too generic and lacks specific identifiers.",
      "processing_time": 65.17681813240051,
      "citing_paper_id": "271274210",
      "cited_paper_id": 268064460
    },
    {
      "context_text": "When incorporated into a pure Transformer-based backbone, our model outperforms PromptIR [22] by 0.19 dB PSNR when averaged across three image restoration tasks under the all-in-one setting.",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a comparison in performance metrics across image restoration tasks.",
      "processing_time": 62.72458624839783,
      "citing_paper_id": "271274210",
      "cited_paper_id": 268064460
    },
    {
      "context_text": "Our method outperforms the recent state-of-the-art PromptIR [22] on most metrics while using 12% fewer parameters and 10% lower complexity (see Table XIII).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a comparison with another method (PromptIR).",
      "processing_time": 62.42738461494446,
      "citing_paper_id": "271274210",
      "cited_paper_id": 268064460
    },
    {
      "context_text": "The OKM-based model exhibits superior performance compared to the recent PromptIR [22] under the three-task setting, with a 0.19 dB PSNR improvement.",
      "catation_intent": "findings",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a comparison of model performance. The context is focused on the results rather than the data used.",
      "processing_time": 64.24116134643555,
      "citing_paper_id": "271274210",
      "cited_paper_id": 268064460
    },
    {
      "context_text": "Additionally, we present experimental results for the single-task setting in accordance with PromptIR [22] and for evaluating the generalization ability following [23].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only references to other papers. No verifiable resources are identified.",
      "processing_time": 62.62147879600525,
      "citing_paper_id": "271274210",
      "cited_paper_id": 268064460
    },
    {
      "context_text": "The deraining results are presented in performance enhancement of 2.01 dB PSNR compared to the all-in-one PromptIR [22] method.",
      "catation_intent": "findings",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a performance comparison with a method called PromptIR.",
      "processing_time": 62.1534698009491,
      "citing_paper_id": "271274210",
      "cited_paper_id": 268064460
    },
    {
      "context_text": "PromptIR [22] pre-defines a pool of prompts to guide the restoration process.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method called PromptIR. No verifiable resources are identified.",
      "processing_time": 62.91358709335327,
      "citing_paper_id": "271274210",
      "cited_paper_id": 268064460
    },
    {
      "context_text": "Furthermore, following [22] and [41], we conduct experiments by training distinct models for each task.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only that distinct models are trained for each task. There is no clear identifier for a dataset.",
      "processing_time": 63.63325357437134,
      "citing_paper_id": "271274210",
      "cited_paper_id": 268064460
    },
    {
      "context_text": "We choose image denoising, deraining, and dehazing tasks for the three-task all-in-one setting, closely following [22].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only tasks within the all-in-one image restoration setting.",
      "processing_time": 62.09171390533447,
      "citing_paper_id": "271274210",
      "cited_paper_id": 268064460
    },
    {
      "context_text": "This paper is an extension of the conference paper [24].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, models, or methods. It only indicates that the current paper is an extension of a previous conference paper.",
      "processing_time": 63.92548632621765,
      "citing_paper_id": "271274210",
      "cited_paper_id": 268678266
    },
    {
      "context_text": "The models are trained using the Adam optimizer with dual-domain L 1 loss functions [24].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only the use of an optimizer and loss functions.",
      "processing_time": 62.092511892318726,
      "citing_paper_id": "271274210",
      "cited_paper_id": 268678266
    },
    {
      "context_text": "For the synthetic images, the Rain1400 dataset [3] contains 12,600 pairs of rainy-clean images for training and 1,400 for testing, with rain streaks of different levels included.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Rain1400"
      ],
      "dataset_descriptions": {
        "Rain1400": "Used to train and test rain removal models, containing 12,600 pairs of rainy-clean images for training and 1,400 for testing, with varying levels of rain streaks."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions a specific dataset, Rain1400, which is used for training and testing models for rain removal from images.",
      "processing_time": 73.56045937538147,
      "citing_paper_id": "277668333",
      "cited_paper_id": 17115407
    },
    {
      "context_text": "All ablation studies are conducted with Ours-S on the weather degradation setting, and evaluated on the RE-SIDE dataset [8].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "RE-SIDE"
      ],
      "dataset_descriptions": {
        "RE-SIDE": "Used to evaluate the performance of the proposed method on weather degradation settings, focusing on single-image dehazing and beyond."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions the RE-SIDE dataset, which is used for evaluating the performance of the proposed method on weather degradation settings.",
      "processing_time": 69.84266352653503,
      "citing_paper_id": "277668333",
      "cited_paper_id": 39760169
    },
    {
      "context_text": "We adopt the pre-trained Swin Trans-former [11] for Object Detection (OD) and Depth Any-thing [15] for Monocular Depth Estimation (MDE) on the compressed results of RESIDE dataset [8].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "RESIDE"
      ],
      "dataset_descriptions": {
        "RESIDE": "Used to evaluate monocular depth estimation on compressed results, focusing on single-image dehazing and restoration techniques."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions the RESIDE dataset, which is used for evaluating single-image dehazing methods. The dataset is relevant to the research topic of image restoration.",
      "processing_time": 70.35229444503784,
      "citing_paper_id": "277668333",
      "cited_paper_id": 39760169
    },
    {
      "context_text": "We adopt the pre-trained Swin Trans-former [11] for Object Detection (OD) and Depth Any-thing [15] for Monocular Depth Estimation (MDE) on the compressed results of RESIDE dataset [8].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "RESIDE"
      ],
      "dataset_descriptions": {
        "RESIDE": "Used to evaluate monocular depth estimation on compressed results, focusing on single-image dehazing and restoration techniques."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions the RESIDE dataset, which is used for evaluating single-image dehazing methods. The dataset is relevant to the research topic of image restoration.",
      "processing_time": 70.35229444503784,
      "citing_paper_id": "277668333",
      "cited_paper_id": 232352874
    },
    {
      "context_text": "The RESIDE dataset [8] comprises the ITS dataset (72,135 images) for training and the OTS dataset (500 images) for testing.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "RESIDE",
        "ITS",
        "OTS"
      ],
      "dataset_descriptions": {
        "RESIDE": "Used for benchmarking single-image dehazing methods, comprising the ITS dataset for training and the OTS dataset for testing.",
        "ITS": "Training dataset within RESIDE, containing 72,135 images for developing and refining dehazing algorithms.",
        "OTS": "Testing dataset within RESIDE, consisting of 500 images to evaluate the performance of dehazing models."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions specific datasets used for training and testing in the context of single-image dehazing. The RESIDE dataset is clearly identified and broken down into ITS and OTS subsets.",
      "processing_time": 82.85524868965149,
      "citing_paper_id": "277668333",
      "cited_paper_id": 39760169
    },
    {
      "context_text": "Based on SPA [14], SPA+ [18] removes images with repetitive backgrounds and further densifies the rain streaks.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions 'SPA' and 'SPA+', which are likely methods or datasets. However, without more specific information or a clear identifier, they do not meet the criteria for inclusion as datasets.",
      "processing_time": 65.17458152770996,
      "citing_paper_id": "277668333",
      "cited_paper_id": 91184545
    },
    {
      "context_text": "The CSD dataset [1] includes 8,000 snowy images for training and 2,000 images for testing.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "CSD"
      ],
      "dataset_descriptions": {
        "CSD": "Used to train and test a single image desnowing algorithm, focusing on hierarchical dual-tree complex wavelet representation and contradict channel loss."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions a specific dataset, CSD, which is used for training and testing a desnowing algorithm. The dataset is clearly identified and its purpose is described.",
      "processing_time": 71.54433846473694,
      "citing_paper_id": "277668333",
      "cited_paper_id": 244461440
    },
    {
      "context_text": "Recent years have witnessed a great paradigm shift from conventional restoration methods to learning-based methods, due to their impressive performance on various image restoration tasks, such as denoising [15, 24, 61], deraining [20, 52, 55], debluring [8, 40, 42], dehazing [26, 44, 45], low-light enhancement [14, 34, 50], etc.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general tasks and methods. No clear, verifiable datasets are identified.",
      "processing_time": 63.066662073135376,
      "citing_paper_id": "260085391",
      "cited_paper_id": 996788
    },
    {
      "context_text": "Since most existing methods have been dedicated into single degradation removal, such as denoising [15,24,61], deraining [20,52,55], debluring [8, 40, 42], dehazing [26, 44, 45], low-light enhancement [14, 34, 50], etc.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various image restoration tasks. No dataset names are provided.",
      "processing_time": 62.753323554992676,
      "citing_paper_id": "260085391",
      "cited_paper_id": 996788
    },
    {
      "context_text": "Multi-task learning [1] was introduced before the rise of deep learning, and has been applied to a series of fields, such as computer vision [31, 37], natural language processing [16], speech synthesis [51] and reinforcement learn-",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general application areas of multi-task learning. No verifiable resources are identified.",
      "processing_time": 63.01934218406677,
      "citing_paper_id": "260085391",
      "cited_paper_id": 2213896
    },
    {
      "context_text": "Multi-task learning [1] was introduced before the rise of deep learning, and has been applied to a series of fields, such as computer vision [31, 37], natural language processing [16], speech synthesis [51] and reinforcement learn-",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general application areas of multi-task learning. No verifiable resources are identified.",
      "processing_time": 63.01934218406677,
      "citing_paper_id": "260085391",
      "cited_paper_id": 45998148
    },
    {
      "context_text": "[6] proposed to utilize gradient magnitudes to balance the loss functions for each task.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for balancing loss functions in multitask networks.",
      "processing_time": 62.22354602813721,
      "citing_paper_id": "260085391",
      "cited_paper_id": 4703661
    },
    {
      "context_text": "[22] proposed a weighting mechanism by considering the homoscedastic uncertainty of each task.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for multi-task learning using uncertainty to weigh losses.",
      "processing_time": 62.222145795822144,
      "citing_paper_id": "260085391",
      "cited_paper_id": 4800342
    },
    {
      "context_text": "including Rain200L [53] for deraining, RESIDE [27] for dehazing, BSD400 [35] and WED [33] for denoising, GoPro [39] for deblurring and LOL [7] for low-light enhancement.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Rain200L",
        "RESIDE",
        "BSD400",
        "WED",
        "GoPro",
        "LOL"
      ],
      "dataset_descriptions": {
        "Rain200L": "Used for deraining, evaluating the effectiveness of the proposed method in removing rain streaks from images.",
        "RESIDE": "Used for dehazing, assessing the ability of the method to improve visibility in hazy images.",
        "BSD400": "Used for denoising, testing the performance of the method in reducing noise while preserving image details.",
        "WED": "Used for denoising, further validating the denoising capabilities of the proposed method.",
        "GoPro": "Used for deblurring, evaluating the method's ability to restore sharpness in blurred images.",
        "LOL": "Used for low-light enhancement, assessing the method's effectiveness in improving image quality under low-light conditions."
      },
      "confidence_score": 1.0,
      "reasoning": "The citation mentions multiple datasets used for various image restoration tasks, which align with the research topic of all-in-one image restoration.",
      "processing_time": 92.95864653587341,
      "citing_paper_id": "260085391",
      "cited_paper_id": 4840263
    },
    {
      "context_text": "We train our method on a combination of multiple image degradation datasets, following [25],\nincluding Rain200L [53] for deraining, RESIDE [27] for dehazing, BSD400 [35] and WED [33] for denoising, GoPro [39] for deblurring and LOL [7] for low-light enhancement.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Rain200L",
        "RESIDE",
        "BSD400",
        "WED",
        "GoPro",
        "LOL"
      ],
      "dataset_descriptions": {
        "Rain200L": "Used for deraining, providing synthetic rain images to train the model on rain removal techniques.",
        "RESIDE": "Used for dehazing, offering hazy images to improve the model's ability to remove atmospheric haze.",
        "BSD400": "Used for denoising, providing noisy images to enhance the model's noise reduction capabilities.",
        "WED": "Used for denoising, supplying additional noisy images to further refine the model's denoising performance.",
        "GoPro": "Used for deblurring, providing blurred images to train the model on motion blur correction.",
        "LOL": "Used for low-light enhancement, offering low-light images to improve the model's ability to enhance dark scenes."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions specific datasets used for training the method on various image restoration tasks, which aligns with the research topic of all-in-one image restoration.",
      "processing_time": 93.32514262199402,
      "citing_paper_id": "260085391",
      "cited_paper_id": 4840263
    },
    {
      "context_text": "[32, 37] proposed to construct relationship networks and cross-stitch networks to discover the task relationships and learn optimal combination of shared and task-specific representations.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and approaches. The context focuses on the construction of relationship networks and cross-stitch networks.",
      "processing_time": 63.4382643699646,
      "citing_paper_id": "260085391",
      "cited_paper_id": 4962395
    },
    {
      "context_text": "[19, 21] proposed to cluster tasks into groups, resulting in a convex optimization formulation for multi-task learning.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for clustering tasks in multi-task learning.",
      "processing_time": 62.119850635528564,
      "citing_paper_id": "260085391",
      "cited_paper_id": 12817931
    },
    {
      "context_text": "which is the general term of image dehazing [45] and lowlight enhancement [50], according to the atmosphere scattering model [36, 41] and the Retinex theory [23].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only theoretical concepts and models. No verifiable resources are identified.",
      "processing_time": 62.57927441596985,
      "citing_paper_id": "260085391",
      "cited_paper_id": 14873001
    },
    {
      "context_text": "which is the general term of image dehazing [45] and lowlight enhancement [50], according to the atmosphere scattering model [36, 41] and the Retinex theory [23].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only theoretical concepts and models. No verifiable resources are identified.",
      "processing_time": 62.57927441596985,
      "citing_paper_id": "260085391",
      "cited_paper_id": 30651858
    },
    {
      "context_text": "To solve the above problem, we ask two questions: i) ’whether there are commonalities between different degradations?’ During past decades, few of works have been devoted to this field, [10] presented the interrelationship between image dehazing and low-light image enhancement.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a relationship between image dehazing and low-light image enhancement.",
      "processing_time": 62.76653695106506,
      "citing_paper_id": "260085391",
      "cited_paper_id": 15901990
    },
    {
      "context_text": "[43,58,59] proposed the collective goals of maintaining contextual information and spatial details.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only collective goals for image restoration. No dataset names are present in the context.",
      "processing_time": 63.10030817985535,
      "citing_paper_id": "260085391",
      "cited_paper_id": 231802205
    },
    {
      "context_text": "[43,58,59] proposed the collective goals of maintaining contextual information and spatial details.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only collective goals for image restoration. No dataset names are present in the context.",
      "processing_time": 63.10030817985535,
      "citing_paper_id": "260085391",
      "cited_paper_id": 263784964
    },
    {
      "context_text": "Given the input of the degraded feature x ∈ RH×W×C and a set of prior embeddings P ∈ RN×Cd , where N is the capacity of the prior hub and N ≪ Cd, we first employ the supervised degradation attention module (SDAM) on x to dilute the background content while intensifying the latent subtle dergadation for content-agnostic prior learning, which is inspired by [59] with merely replaced degradation supervision.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method or model. The context focuses on the technical details of the method and does not reference any dataset.",
      "processing_time": 63.82263684272766,
      "citing_paper_id": "260085391",
      "cited_paper_id": 231802205
    },
    {
      "context_text": "which is the general term of image deblurring [60], and A represents the blur kernel.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general term and a method (blur kernel).",
      "processing_time": 62.38312101364136,
      "citing_paper_id": "260085391",
      "cited_paper_id": 246285588
    },
    {
      "context_text": "[3] proposed a simple baseline for image restoration, which is a nonlinear activation free network.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for image restoration.",
      "processing_time": 61.27724552154541,
      "citing_paper_id": "260085391",
      "cited_paper_id": 248085491
    },
    {
      "context_text": "With the flourish of vision transformers, their global modeling capability as well as the adaptability to input content have spawned a series of image restoration works, such as window-attention based [29, 49] , channelinteraction based [57] and latent-attention based [5].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods and approaches for image restoration using vision transformers.",
      "processing_time": 62.44588780403137,
      "citing_paper_id": "260085391",
      "cited_paper_id": 268838239
    },
    {
      "context_text": "BSD68 [35] Urban100 [18] Kodak24 [12] Method σ=15 σ=25 σ=50 σ=15 σ=25 σ=50 σ=15 σ=25 σ=50",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "BSD68",
        "Urban100",
        "Kodak24"
      ],
      "dataset_descriptions": {
        "BSD68": "Used to evaluate image denoising performance at various noise levels, focusing on the effectiveness of the method across different σ values.",
        "Urban100": "Used to assess image restoration quality in urban scenes, evaluating the method's performance at multiple noise levels.",
        "Kodak24": "Used to test image restoration algorithms on a standard set of high-quality images, measuring performance at different noise levels."
      },
      "confidence_score": 1.0,
      "reasoning": "The citation mentions specific datasets used for image restoration experiments, which are relevant to the 'All-in-One Image Restoration' topic.",
      "processing_time": 83.08933401107788,
      "citing_paper_id": "260085391",
      "cited_paper_id": null
    },
    {
      "context_text": "For evaluation, Rain100L [53], SOTS-Outdoor [27], BSD68 [35], Urban100 [18], Kodak24 [12], GoPro [39], LOL [7] are utilized as the test sets.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Rain100L",
        "SOTS-Outdoor",
        "BSD68",
        "Urban100",
        "Kodak24",
        "GoPro",
        "LOL"
      ],
      "dataset_descriptions": {
        "Rain100L": "Used for evaluating image deraining algorithms, focusing on synthetic rain streaks on images.",
        "SOTS-Outdoor": "Utilized to assess outdoor scene dehazing methods, emphasizing real-world hazy conditions.",
        "BSD68": "Employed for denoising tasks, specifically targeting grayscale images with Gaussian noise.",
        "Urban100": "Used to evaluate super-resolution techniques, focusing on high-resolution urban scenes.",
        "Kodak24": "Applied for general image quality assessment, providing a standard set of 24 color images.",
        "GoPro": "Utilized for video deblurring, assessing the performance on motion-blurred frames.",
        "LOL": "Used to evaluate low-light image enhancement, focusing on improving visibility in dark conditions."
      },
      "confidence_score": 1.0,
      "reasoning": "The citation clearly lists multiple datasets used for evaluation in the context of image restoration. Each dataset is named and appears to be a specific, verifiable resource.",
      "processing_time": 93.78110408782959,
      "citing_paper_id": "260085391",
      "cited_paper_id": null
    },
    {
      "context_text": "[54] formulated the meta learning with multiple objectives as multi-objective bi-level optimization problem, and devised a gradient-based optimization algorithm to find a common descent direction.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a methodological approach to meta-learning.",
      "processing_time": 62.04805850982666,
      "citing_paper_id": "260085391",
      "cited_paper_id": null
    },
    {
      "context_text": "Testing is conducted on the BSD68 [32].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "BSD68"
      ],
      "dataset_descriptions": {
        "BSD68": "Used for testing image restoration methods, specifically focusing on denoising performance on natural images."
      },
      "confidence_score": 1.0,
      "reasoning": "BSD68 is a well-known dataset in image processing and computer vision, specifically for image denoising tasks. The context indicates it is used for testing.",
      "processing_time": 69.86304235458374,
      "citing_paper_id": "274305801",
      "cited_paper_id": 64193
    },
    {
      "context_text": "2a, our pipeline features a U-shaped architecture [44, 65] with an asymmetric encoder-decoder design.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (U-Net).",
      "processing_time": 61.57521843910217,
      "citing_paper_id": "274305801",
      "cited_paper_id": 3719281
    },
    {
      "context_text": "Adverse conditions such as noise [22, 67], haze [5, 38], or rain [7, 59] significantly impact the practical utility of images in downstream applications across various domains, including autonomous navigation [7, 49] or augmented reality [10, 15, 45].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets by name. It only refers to adverse conditions affecting image quality and their impact on various applications.",
      "processing_time": 62.99710011482239,
      "citing_paper_id": "274305801",
      "cited_paper_id": 3846544
    },
    {
      "context_text": "Adverse conditions such as noise [22, 67], haze [5, 38], or rain [7, 59] significantly impact the practical utility of images in downstream applications across various domains, including autonomous navigation [7, 49] or augmented reality [10, 15, 45].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets by name. It only refers to adverse conditions affecting image quality and their impact on various applications.",
      "processing_time": 62.99710011482239,
      "citing_paper_id": "274305801",
      "cited_paper_id": 14092238
    },
    {
      "context_text": "Adverse conditions such as noise [22, 67], haze [5, 38], or rain [7, 59] significantly impact the practical utility of images in downstream applications across various domains, including autonomous navigation [7, 49] or augmented reality [10, 15, 45].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets by name. It only refers to adverse conditions affecting image quality and their impact on various applications.",
      "processing_time": 62.99710011482239,
      "citing_paper_id": "274305801",
      "cited_paper_id": 55358798
    },
    {
      "context_text": "Adverse conditions such as noise [22, 67], haze [5, 38], or rain [7, 59] significantly impact the practical utility of images in downstream applications across various domains, including autonomous navigation [7, 49] or augmented reality [10, 15, 45].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets by name. It only refers to adverse conditions affecting image quality and their impact on various applications.",
      "processing_time": 62.99710011482239,
      "citing_paper_id": "274305801",
      "cited_paper_id": 195657934
    },
    {
      "context_text": "Adverse conditions such as noise [22, 67], haze [5, 38], or rain [7, 59] significantly impact the practical utility of images in downstream applications across various domains, including autonomous navigation [7, 49] or augmented reality [10, 15, 45].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets by name. It only refers to adverse conditions affecting image quality and their impact on various applications.",
      "processing_time": 62.99710011482239,
      "citing_paper_id": "274305801",
      "cited_paper_id": 244714491
    },
    {
      "context_text": "Reconstructing the clean image from its degraded counterpart is a highly ill-posed problem, however, a great body of work have addressed image restoration from a data-driven learning perspective, achieving tremendous results compared to prior hand-crafted methods [22, 26, 47, 52, 65, 67, 68].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only a general reference to data-driven learning approaches in image restoration.",
      "processing_time": 62.50720691680908,
      "citing_paper_id": "274305801",
      "cited_paper_id": 3846544
    },
    {
      "context_text": "Reconstructing the clean image from its degraded counterpart is a highly ill-posed problem, however, a great body of work have addressed image restoration from a data-driven learning perspective, achieving tremendous results compared to prior hand-crafted methods [22, 26, 47, 52, 65, 67, 68].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only a general reference to data-driven learning approaches in image restoration.",
      "processing_time": 62.50720691680908,
      "citing_paper_id": "274305801",
      "cited_paper_id": 8550762
    },
    {
      "context_text": "Reconstructing the clean image from its degraded counterpart is a highly ill-posed problem, however, a great body of work have addressed image restoration from a data-driven learning perspective, achieving tremendous results compared to prior hand-crafted methods [22, 26, 47, 52, 65, 67, 68].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only a general reference to data-driven learning approaches in image restoration.",
      "processing_time": 62.50720691680908,
      "citing_paper_id": "274305801",
      "cited_paper_id": 85501306
    },
    {
      "context_text": "Reconstructing the clean image from its degraded counterpart is a highly ill-posed problem, however, a great body of work have addressed image restoration from a data-driven learning perspective, achieving tremendous results compared to prior hand-crafted methods [22, 26, 47, 52, 65, 67, 68].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only a general reference to data-driven learning approaches in image restoration.",
      "processing_time": 62.50720691680908,
      "citing_paper_id": "274305801",
      "cited_paper_id": 235358213
    },
    {
      "context_text": "Most proposed solutions build on convolutional [6, 47, 67, 68] or Transformer-based architectures [8, 28, 52, 65] addressing single degradation tasks, such as denoising [8, 67, 68], de-hazing [41, 42, 55] or deraining [18, 40].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only various image restoration tasks and architectural approaches. No verifiable resources are identified.",
      "processing_time": 62.76447296142578,
      "citing_paper_id": "274305801",
      "cited_paper_id": 4563057
    },
    {
      "context_text": "Most proposed solutions build on convolutional [6, 47, 67, 68] or Transformer-based architectures [8, 28, 52, 65] addressing single degradation tasks, such as denoising [8, 67, 68], de-hazing [41, 42, 55] or deraining [18, 40].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only various image restoration tasks and architectural approaches. No verifiable resources are identified.",
      "processing_time": 62.76447296142578,
      "citing_paper_id": "274305801",
      "cited_paper_id": 8550762
    },
    {
      "context_text": "Most proposed solutions build on convolutional [6, 47, 67, 68] or Transformer-based architectures [8, 28, 52, 65] addressing single degradation tasks, such as denoising [8, 67, 68], de-hazing [41, 42, 55] or deraining [18, 40].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only various image restoration tasks and architectural approaches. No verifiable resources are identified.",
      "processing_time": 62.76447296142578,
      "citing_paper_id": "274305801",
      "cited_paper_id": 85501306
    },
    {
      "context_text": "Most proposed solutions build on convolutional [6, 47, 67, 68] or Transformer-based architectures [8, 28, 52, 65] addressing single degradation tasks, such as denoising [8, 67, 68], de-hazing [41, 42, 55] or deraining [18, 40].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only various image restoration tasks and architectural approaches. No verifiable resources are identified.",
      "processing_time": 62.76447296142578,
      "citing_paper_id": "274305801",
      "cited_paper_id": 202733344
    },
    {
      "context_text": "Most proposed solutions build on convolutional [6, 47, 67, 68] or Transformer-based architectures [8, 28, 52, 65] addressing single degradation tasks, such as denoising [8, 67, 68], de-hazing [41, 42, 55] or deraining [18, 40].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only various image restoration tasks and architectural approaches. No verifiable resources are identified.",
      "processing_time": 62.76447296142578,
      "citing_paper_id": "274305801",
      "cited_paper_id": 233296013
    },
    {
      "context_text": "Most proposed solutions build on convolutional [6, 47, 67, 68] or Transformer-based architectures [8, 28, 52, 65] addressing single degradation tasks, such as denoising [8, 67, 68], de-hazing [41, 42, 55] or deraining [18, 40].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only various image restoration tasks and architectural approaches. No verifiable resources are identified.",
      "processing_time": 62.76447296142578,
      "citing_paper_id": "274305801",
      "cited_paper_id": 235358213
    },
    {
      "context_text": "Most proposed solutions build on convolutional [6, 47, 67, 68] or Transformer-based architectures [8, 28, 52, 65] addressing single degradation tasks, such as denoising [8, 67, 68], de-hazing [41, 42, 55] or deraining [18, 40].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only various image restoration tasks and architectural approaches. No verifiable resources are identified.",
      "processing_time": 62.76447296142578,
      "citing_paper_id": "274305801",
      "cited_paper_id": 254018182
    },
    {
      "context_text": "For all-in-one, we follow existing work [24, 36] and include following datasets: For image denoising, we combine the BSD400 [2] and WED [31] datasets, adding Gaussian noise at levels σ ∈ [15 , 25 , 50] to create noisy images.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "BSD400",
        "WED"
      ],
      "dataset_descriptions": {
        "BSD400": "Used for image denoising by adding Gaussian noise at levels σ ∈ [15, 25, 50] to create noisy images for training and evaluation.",
        "WED": "Used for image denoising by adding Gaussian noise at levels σ ∈ [15, 25, 50] to create noisy images for training and evaluation."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions two specific datasets, BSD400 and WED, which are used for image denoising by adding Gaussian noise at specified levels.",
      "processing_time": 81.29964399337769,
      "citing_paper_id": "274305801",
      "cited_paper_id": 4840263
    },
    {
      "context_text": "For all-in-one, we follow existing work [24, 36] and include following datasets: For image denoising, we combine the BSD400 [2] and WED [31] datasets, adding Gaussian noise at levels σ ∈ [15 , 25 , 50] to create noisy images.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "BSD400",
        "WED"
      ],
      "dataset_descriptions": {
        "BSD400": "Used for image denoising by adding Gaussian noise at levels σ ∈ [15, 25, 50] to create noisy images for training and evaluation.",
        "WED": "Used for image denoising by adding Gaussian noise at levels σ ∈ [15, 25, 50] to create noisy images for training and evaluation."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions two specific datasets, BSD400 and WED, which are used for image denoising by adding Gaussian noise at specified levels.",
      "processing_time": 81.29964399337769,
      "citing_paper_id": "274305801",
      "cited_paper_id": 55358798
    },
    {
      "context_text": "For all-in-one, we follow existing work [24, 36] and include following datasets: For image denoising, we combine the BSD400 [2] and WED [31] datasets, adding Gaussian noise at levels σ ∈ [15 , 25 , 50] to create noisy images.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "BSD400",
        "WED"
      ],
      "dataset_descriptions": {
        "BSD400": "Used for image denoising by adding Gaussian noise at levels σ ∈ [15, 25, 50] to create noisy images for training and evaluation.",
        "WED": "Used for image denoising by adding Gaussian noise at levels σ ∈ [15, 25, 50] to create noisy images for training and evaluation."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions two specific datasets, BSD400 and WED, which are used for image denoising by adding Gaussian noise at specified levels.",
      "processing_time": 81.29964399337769,
      "citing_paper_id": "274305801",
      "cited_paper_id": 206764694
    },
    {
      "context_text": "For all-in-one, we follow existing work [24, 36] and include following datasets: For image denoising, we combine the BSD400 [2] and WED [31] datasets, adding Gaussian noise at levels σ ∈ [15 , 25 , 50] to create noisy images.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "BSD400",
        "WED"
      ],
      "dataset_descriptions": {
        "BSD400": "Used for image denoising by adding Gaussian noise at levels σ ∈ [15, 25, 50] to create noisy images for training and evaluation.",
        "WED": "Used for image denoising by adding Gaussian noise at levels σ ∈ [15, 25, 50] to create noisy images for training and evaluation."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions two specific datasets, BSD400 and WED, which are used for image denoising by adding Gaussian noise at specified levels.",
      "processing_time": 81.29964399337769,
      "citing_paper_id": "274305801",
      "cited_paper_id": 250551851
    },
    {
      "context_text": "For all-in-one, we follow existing work [24, 36] and include following datasets: For image denoising, we combine the BSD400 [2] and WED [31] datasets, adding Gaussian noise at levels σ ∈ [15 , 25 , 50] to create noisy images.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "BSD400",
        "WED"
      ],
      "dataset_descriptions": {
        "BSD400": "Used for image denoising by adding Gaussian noise at levels σ ∈ [15, 25, 50] to create noisy images for training and evaluation.",
        "WED": "Used for image denoising by adding Gaussian noise at levels σ ∈ [15, 25, 50] to create noisy images for training and evaluation."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions two specific datasets, BSD400 and WED, which are used for image denoising by adding Gaussian noise at specified levels.",
      "processing_time": 81.29964399337769,
      "citing_paper_id": "274305801",
      "cited_paper_id": 268030812
    },
    {
      "context_text": "Image restoration [47, 67, 68] is a fundamental problem in computer vision, dealing with reconstructing high-quality images from deteriorated observations.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only the general problem of image restoration. No dataset names are provided in the context.",
      "processing_time": 63.08901000022888,
      "citing_paper_id": "274305801",
      "cited_paper_id": 8550762
    },
    {
      "context_text": "Image restoration [47, 67, 68] is a fundamental problem in computer vision, dealing with reconstructing high-quality images from deteriorated observations.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only the general problem of image restoration. No dataset names are provided in the context.",
      "processing_time": 63.08901000022888,
      "citing_paper_id": "274305801",
      "cited_paper_id": 85501306
    },
    {
      "context_text": "For deblurring and low-light enhancement, we employ the GoPro [34] and the LOL-v1 [53] dataset, respectively.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "GoPro",
        "LOL-v1"
      ],
      "dataset_descriptions": {
        "GoPro": "Used for deblurring dynamic scenes, employing a deep multi-scale convolutional neural network to improve image clarity.",
        "LOL-v1": "Used for low-light enhancement, focusing on improving image quality in low-light conditions."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions two specific datasets, GoPro and LOL-v1, which are used for deblurring and low-light enhancement, respectively.",
      "processing_time": 74.9038462638855,
      "citing_paper_id": "274305801",
      "cited_paper_id": 8671030
    },
    {
      "context_text": "Dynamic networks have evolved from basic conditional computation [3] to sophisticated Mixture-of-Experts (MoE) architectures that expand model capacity while maintaining efficient inference costs [37, 43, 46].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only models and methods. There are no verifiable resources that meet the criteria for inclusion.",
      "processing_time": 63.51234173774719,
      "citing_paper_id": "274305801",
      "cited_paper_id": 18406556
    },
    {
      "context_text": "Dynamic networks have evolved from basic conditional computation [3] to sophisticated Mixture-of-Experts (MoE) architectures that expand model capacity while maintaining efficient inference costs [37, 43, 46].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only models and methods. There are no verifiable resources that meet the criteria for inclusion.",
      "processing_time": 63.51234173774719,
      "citing_paper_id": "274305801",
      "cited_paper_id": null
    },
    {
      "context_text": "The dehazing task utilises the SOTS [23] dataset.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "SOTS"
      ],
      "dataset_descriptions": {
        "SOTS": "Used to benchmark single-image dehazing methods, evaluating performance on synthetic and real-world hazy images."
      },
      "confidence_score": 1.0,
      "reasoning": "The context clearly mentions the use of the SOTS dataset for the dehazing task, which is directly relevant to the topic of image restoration.",
      "processing_time": 68.88595581054688,
      "citing_paper_id": "274305801",
      "cited_paper_id": 39760169
    },
    {
      "context_text": "In contrast, our contribution lies in a novel, parameter-efficient approach that focuses on selective activation, effectively addressing diverse degradations while remaining lightweight and adaptable.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, models, or methods. It focuses on the novel approach of the current work.",
      "processing_time": 62.954452991485596,
      "citing_paper_id": "274305801",
      "cited_paper_id": 128362447
    },
    {
      "context_text": "Recent all-in-onerestoration models [9, 12, 13, 24, 27, 36, 49, 66], however, demonstrate the possibility of handling multiple degradation types within a single model, offering more practical solutions compared to traditional task-specific approaches without the extensive re-training need for each…",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to models and methods. The cited papers' titles do not provide additional context to identify datasets.",
      "processing_time": 64.16281080245972,
      "citing_paper_id": "274305801",
      "cited_paper_id": 195787503
    },
    {
      "context_text": "Recent all-in-onerestoration models [9, 12, 13, 24, 27, 36, 49, 66], however, demonstrate the possibility of handling multiple degradation types within a single model, offering more practical solutions compared to traditional task-specific approaches without the extensive re-training need for each…",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to models and methods. The cited papers' titles do not provide additional context to identify datasets.",
      "processing_time": 64.16281080245972,
      "citing_paper_id": "274305801",
      "cited_paper_id": 244714491
    },
    {
      "context_text": "Recent all-in-onerestoration models [9, 12, 13, 24, 27, 36, 49, 66], however, demonstrate the possibility of handling multiple degradation types within a single model, offering more practical solutions compared to traditional task-specific approaches without the extensive re-training need for each…",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to models and methods. The cited papers' titles do not provide additional context to identify datasets.",
      "processing_time": 64.16281080245972,
      "citing_paper_id": "274305801",
      "cited_paper_id": 250551851
    },
    {
      "context_text": "Recent all-in-onerestoration models [9, 12, 13, 24, 27, 36, 49, 66], however, demonstrate the possibility of handling multiple degradation types within a single model, offering more practical solutions compared to traditional task-specific approaches without the extensive re-training need for each…",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to models and methods. The cited papers' titles do not provide additional context to identify datasets.",
      "processing_time": 64.16281080245972,
      "citing_paper_id": "274305801",
      "cited_paper_id": 260085391
    },
    {
      "context_text": "The decoder includes three levels, containing [2 , 4 , 4] transformer blocks, respectively.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only the architecture details of the decoder. No dataset names are present.",
      "processing_time": 62.402297258377075,
      "citing_paper_id": "274305801",
      "cited_paper_id": 206764694
    },
    {
      "context_text": "An emerging field known as all-in-one image restoration is advancing in low-level computer vision, utilizing a single deep blind restoration model to tackle multiple degradation types simultaneously [7, 19, 36, 49, 64, 66].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only the concept of all-in-one image restoration. No dataset names are present in the text.",
      "processing_time": 63.92473649978638,
      "citing_paper_id": "274305801",
      "cited_paper_id": 231802205
    },
    {
      "context_text": "An emerging field known as all-in-one image restoration is advancing in low-level computer vision, utilizing a single deep blind restoration model to tackle multiple degradation types simultaneously [7, 19, 36, 49, 64, 66].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only the concept of all-in-one image restoration. No dataset names are present in the text.",
      "processing_time": 63.92473649978638,
      "citing_paper_id": "274305801",
      "cited_paper_id": 244714491
    },
    {
      "context_text": "An emerging field known as all-in-one image restoration is advancing in low-level computer vision, utilizing a single deep blind restoration model to tackle multiple degradation types simultaneously [7, 19, 36, 49, 64, 66].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only the concept of all-in-one image restoration. No dataset names are present in the text.",
      "processing_time": 63.92473649978638,
      "citing_paper_id": "274305801",
      "cited_paper_id": 260085391
    },
    {
      "context_text": "We compare our all-in-one restoration approach with specialized restoration methods, including both image-only models, such as MPR-Net [64], AirNet [24], PromptIR [36], and Gridformer [51], and vision-language models like DA-CLIP [30], Instruc-tIR [9], and UniProcessor [12].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only models and methods. No verifiable resources are identified.",
      "processing_time": 62.19276762008667,
      "citing_paper_id": "274305801",
      "cited_paper_id": 231802205
    },
    {
      "context_text": "We compare our all-in-one restoration approach with specialized restoration methods, including both image-only models, such as MPR-Net [64], AirNet [24], PromptIR [36], and Gridformer [51], and vision-language models like DA-CLIP [30], Instruc-tIR [9], and UniProcessor [12].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only models and methods. No verifiable resources are identified.",
      "processing_time": 62.19276762008667,
      "citing_paper_id": "274305801",
      "cited_paper_id": 250551851
    },
    {
      "context_text": "This makes it particularly effective for all-in-one restoration meth-ods, where both task-specific processing and cross-degradation knowledge sharing are crucial. field, particularly for task-specific image restoration problems [6, 8, 16, 28, 52, 65].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to other papers. No dataset names are present in the text.",
      "processing_time": 62.87795639038086,
      "citing_paper_id": "274305801",
      "cited_paper_id": 235358213
    },
    {
      "context_text": "This makes it particularly effective for all-in-one restoration meth-ods, where both task-specific processing and cross-degradation knowledge sharing are crucial. field, particularly for task-specific image restoration problems [6, 8, 16, 28, 52, 65].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to other papers. No dataset names are present in the text.",
      "processing_time": 62.87795639038086,
      "citing_paper_id": "274305801",
      "cited_paper_id": 254018182
    },
    {
      "context_text": "This makes it particularly effective for all-in-one restoration meth-ods, where both task-specific processing and cross-degradation knowledge sharing are crucial. field, particularly for task-specific image restoration problems [6, 8, 16, 28, 52, 65].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to other papers. No dataset names are present in the text.",
      "processing_time": 62.87795639038086,
      "citing_paper_id": "274305801",
      "cited_paper_id": 267938238
    },
    {
      "context_text": "Typically, MoE models [37, 39, 43, 70] aim to balance expert utilization, preventing the model from collapsing into a single-expert dependency.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only references to MoE models. There are no verifiable resources or datasets mentioned.",
      "processing_time": 63.31341004371643,
      "citing_paper_id": "274305801",
      "cited_paper_id": 247011948
    },
    {
      "context_text": "Noteable works employ visual [25, 36, 50] or language-based prompt-1 ing [1, 9, 30], contrastive learning [24, 66], and diffusion-based models [1, 30].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods and models. The cited papers' titles do not provide additional context about datasets.",
      "processing_time": 63.56232476234436,
      "citing_paper_id": "274305801",
      "cited_paper_id": 250551851
    },
    {
      "context_text": "Noteable works employ visual [25, 36, 50] or language-based prompt-1 ing [1, 9, 30], contrastive learning [24, 66], and diffusion-based models [1, 30].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods and models. The cited papers' titles do not provide additional context about datasets.",
      "processing_time": 63.56232476234436,
      "citing_paper_id": "274305801",
      "cited_paper_id": 260085391
    },
    {
      "context_text": "Noteable works employ visual [25, 36, 50] or language-based prompt-1 ing [1, 9, 30], contrastive learning [24, 66], and diffusion-based models [1, 30].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods and models. The cited papers' titles do not provide additional context about datasets.",
      "processing_time": 63.56232476234436,
      "citing_paper_id": "274305801",
      "cited_paper_id": 268030812
    },
    {
      "context_text": "Noteable works employ visual [25, 36, 50] or language-based prompt-1 ing [1, 9, 30], contrastive learning [24, 66], and diffusion-based models [1, 30].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods and models. The cited papers' titles do not provide additional context about datasets.",
      "processing_time": 63.56232476234436,
      "citing_paper_id": "274305801",
      "cited_paper_id": 272724853
    },
    {
      "context_text": "31 dB over AirNet [24].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific dataset names, only a performance metric over a method or dataset called 'AirNet'. There is no clear indication of a reusable dataset.",
      "processing_time": 64.34102940559387,
      "citing_paper_id": "274305801",
      "cited_paper_id": 250551851
    },
    {
      "context_text": "The seminal work, AirNet [24] achieves blind all-in-one image restoration using contrastive learning to extract degradation representations, which guide the restoration process.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions AirNet, which is a method or model, not a dataset. No specific dataset is mentioned or used in the context provided.",
      "processing_time": 63.65305280685425,
      "citing_paper_id": "274305801",
      "cited_paper_id": 250551851
    },
    {
      "context_text": "Based upon [24, 66], we expand the three-degradation setting to include deblurring and low-light image enhancement, further validating our method’s effectiveness.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and degradation settings. The context focuses on expanding the degradation settings to include deblurring and low-light image enhancement.",
      "processing_time": 64.68033337593079,
      "citing_paper_id": "274305801",
      "cited_paper_id": 250551851
    },
    {
      "context_text": "Based upon [24, 66], we expand the three-degradation setting to include deblurring and low-light image enhancement, further validating our method’s effectiveness.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and degradation settings. The context focuses on expanding the degradation settings to include deblurring and low-light image enhancement.",
      "processing_time": 64.68033337593079,
      "citing_paper_id": "274305801",
      "cited_paper_id": 260085391
    },
    {
      "context_text": "37 dB, surpassing other models like AirNet [24].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a comparison with another model (AirNet). There are no verifiable resources or datasets mentioned.",
      "processing_time": 63.83248209953308,
      "citing_paper_id": "274305801",
      "cited_paper_id": 250551851
    },
    {
      "context_text": "We compare MoCE-IR-S to AirNet [24], and PromptIR [36] in the all-in-one setting with three degradations.",
      "catation_intent": "reusable resource",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models or methods. The context focuses on comparing different models in an all-in-one image restoration setting.",
      "processing_time": 64.26815724372864,
      "citing_paper_id": "274305801",
      "cited_paper_id": 250551851
    },
    {
      "context_text": "Deep learning-based approaches have remarkably advanced this (a) Dense all-in-one restoration meth-ods [24, 57] often inefficiently allocate parameters when handling multiple degradation types.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods and approaches. No verifiable resources are identified.",
      "processing_time": 62.554378271102905,
      "citing_paper_id": "274305801",
      "cited_paper_id": 250551851
    },
    {
      "context_text": "Deep learning-based approaches have remarkably advanced this (a) Dense all-in-one restoration meth-ods [24, 57] often inefficiently allocate parameters when handling multiple degradation types.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods and approaches. No verifiable resources are identified.",
      "processing_time": 62.554378271102905,
      "citing_paper_id": "274305801",
      "cited_paper_id": 265609786
    },
    {
      "context_text": "To optimize performance, we employ an FFT-based approximation [21] for efficient matrix multiplication between queries Q and keys K in the Fourier domain.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for efficient matrix multiplication.",
      "processing_time": 61.458494901657104,
      "citing_paper_id": "274305801",
      "cited_paper_id": 253761139
    },
    {
      "context_text": "The encoder consists of four levels with varying numbers of transformer blocks, arranged as [4 , 6 , 6 , 8] from the top level to the lowest.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The provided context does not mention any specific datasets, only details about the encoder architecture. No verifiable resources are identified.",
      "processing_time": 62.79163599014282,
      "citing_paper_id": "274305801",
      "cited_paper_id": 254018182
    },
    {
      "context_text": "This efficient model utilization not only boosts image restoration performance, consistently surpassing related adaptive restoration methods [29, 58], but also enhances out-of-distribution generalization as MoCE-IR effectively handles degradations that with significant deviation from the training…",
      "catation_intent": "reusable resource",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.2,
      "reasoning": "The context does not mention any specific datasets, only references to model performance and out-of-distribution generalization. No clear identifiers for datasets are present.",
      "processing_time": 64.34898567199707,
      "citing_paper_id": "274305801",
      "cited_paper_id": 257757038
    },
    {
      "context_text": "This efficient model utilization not only boosts image restoration performance, consistently surpassing related adaptive restoration methods [29, 58], but also enhances out-of-distribution generalization as MoCE-IR effectively handles degradations that with significant deviation from the training…",
      "catation_intent": "reusable resource",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.2,
      "reasoning": "The context does not mention any specific datasets, only references to model performance and out-of-distribution generalization. No clear identifiers for datasets are present.",
      "processing_time": 64.34898567199707,
      "citing_paper_id": "274305801",
      "cited_paper_id": 270123740
    },
    {
      "context_text": "Yet, current MoE-based approaches [29, 57, 62] typically incorporate routing mechanisms based on language [57] or degradation priors [29, 62], leading to imbalanced optimization, where some experts generalize well, while others struggle with their intended tasks.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only references to methods and approaches. The cited papers' titles do not provide additional dataset information.",
      "processing_time": 63.84485578536987,
      "citing_paper_id": "274305801",
      "cited_paper_id": 257757038
    },
    {
      "context_text": "Yet, current MoE-based approaches [29, 57, 62] typically incorporate routing mechanisms based on language [57] or degradation priors [29, 62], leading to imbalanced optimization, where some experts generalize well, while others struggle with their intended tasks.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only references to methods and approaches. The cited papers' titles do not provide additional dataset information.",
      "processing_time": 63.84485578536987,
      "citing_paper_id": "274305801",
      "cited_paper_id": 265609786
    },
    {
      "context_text": "Yet, current MoE-based approaches [29, 57, 62] typically incorporate routing mechanisms based on language [57] or degradation priors [29, 62], leading to imbalanced optimization, where some experts generalize well, while others struggle with their intended tasks.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only references to methods and approaches. The cited papers' titles do not provide additional dataset information.",
      "processing_time": 63.84485578536987,
      "citing_paper_id": "274305801",
      "cited_paper_id": 270045495
    },
    {
      "context_text": "In image restoration, Path-Restore [60] introduced content-aware patch routing with difficulty-regulated rewards, while recent all-in-one models [29, 57, 62] leverage various priors for expert routing.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only models and methods. The cited papers' titles do not provide additional dataset information.",
      "processing_time": 63.40045642852783,
      "citing_paper_id": "274305801",
      "cited_paper_id": 257757038
    },
    {
      "context_text": "In image restoration, Path-Restore [60] introduced content-aware patch routing with difficulty-regulated rewards, while recent all-in-one models [29, 57, 62] leverage various priors for expert routing.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only models and methods. The cited papers' titles do not provide additional dataset information.",
      "processing_time": 63.40045642852783,
      "citing_paper_id": "274305801",
      "cited_paper_id": 265609786
    },
    {
      "context_text": "In image restoration, Path-Restore [60] introduced content-aware patch routing with difficulty-regulated rewards, while recent all-in-one models [29, 57, 62] leverage various priors for expert routing.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only models and methods. The cited papers' titles do not provide additional dataset information.",
      "processing_time": 63.40045642852783,
      "citing_paper_id": "274305801",
      "cited_paper_id": 270045495
    },
    {
      "context_text": "Similarly, IDR [66] employs a meta-learning-based two-stage approach to model degradation through underlying physical principles.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (IDR) and its approach to image restoration.",
      "processing_time": 62.65110754966736,
      "citing_paper_id": "274305801",
      "cited_paper_id": 260085391
    },
    {
      "context_text": "We conduct experiments by strictly following previous works in general image restoration [36, 66] under two different settings: (i) All-in-One and (ii) Composited degradations .",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only general settings for image restoration experiments.",
      "processing_time": 61.57208728790283,
      "citing_paper_id": "274305801",
      "cited_paper_id": 260085391
    },
    {
      "context_text": "Despite their success, we observe that the aforementioned models often suffer from inefficiencies, as parameters tied to specific degradations often remain inactive or underutilized when addressing unrelated tasks [57].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models and their inefficiencies. There are no clear identifiers for datasets in the given context.",
      "processing_time": 63.48284029960632,
      "citing_paper_id": "274305801",
      "cited_paper_id": 265609786
    },
    {
      "context_text": "Prompt-based learning [25, 36, 50] has also gained traction, with [36] introducing tunable prompts to encode degradation-specific in-2 [57, 62, 63], inconsistent expert behavior—where some experts over-generalize while others underperform—limits their computational efficiency.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only methods and concepts. The cited papers' titles do not provide additional dataset information.",
      "processing_time": 63.48512506484985,
      "citing_paper_id": "274305801",
      "cited_paper_id": 265609786
    },
    {
      "context_text": "Prompt-based learning [25, 36, 50] has also gained traction, with [36] introducing tunable prompts to encode degradation-specific in-2 [57, 62, 63], inconsistent expert behavior—where some experts over-generalize while others underperform—limits their computational efficiency.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only methods and concepts. The cited papers' titles do not provide additional dataset information.",
      "processing_time": 63.48512506484985,
      "citing_paper_id": "274305801",
      "cited_paper_id": 267499648
    },
    {
      "context_text": "Prompt-based learning [25, 36, 50] has also gained traction, with [36] introducing tunable prompts to encode degradation-specific in-2 [57, 62, 63], inconsistent expert behavior—where some experts over-generalize while others underperform—limits their computational efficiency.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only methods and concepts. The cited papers' titles do not provide additional dataset information.",
      "processing_time": 63.48512506484985,
      "citing_paper_id": "274305801",
      "cited_paper_id": 268030812
    },
    {
      "context_text": "Prompt-based learning [25, 36, 50] has also gained traction, with [36] introducing tunable prompts to encode degradation-specific in-2 [57, 62, 63], inconsistent expert behavior—where some experts over-generalize while others underperform—limits their computational efficiency.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only methods and concepts. The cited papers' titles do not provide additional dataset information.",
      "processing_time": 63.48512506484985,
      "citing_paper_id": "274305801",
      "cited_paper_id": 270045495
    },
    {
      "context_text": "(b) While recent Mixture-of-Experts (MoE) approaches [61, 62] address this through sparse computation, their rigid routing mechanisms uniformly distribute inputs across experts without considering the natural relationships be-tween degradations.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and approaches. There are no clear identifiers for datasets in the provided context.",
      "processing_time": 63.30673861503601,
      "citing_paper_id": "274305801",
      "cited_paper_id": 270045495
    },
    {
      "context_text": "While previous MoE approaches [37, 43, 46] employ a token-based routing, we opt for an image-level routing strategy instead, where we select experts for entire input image.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a methodological choice regarding routing strategies in MoE approaches.",
      "processing_time": 62.805615186691284,
      "citing_paper_id": "274305801",
      "cited_paper_id": null
    },
    {
      "context_text": "2b, our experts are designed with increasing computational complexity and progressively larger receptive fields, in contrast to the uniform architecture of traditional MoE designs [37, 43, 46].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only discusses architectural design aspects of models.",
      "processing_time": 61.842657804489136,
      "citing_paper_id": "274305801",
      "cited_paper_id": null
    },
    {
      "context_text": "Drawing inspiration from sparse MoE [43, 46], we integrate linear layers within each decoder block to enable a routing mechanism, associating input features x ∈ R H × W × C with their corresponding specialized complexity experts E .",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a methodological approach. There are no verifiable resources or datasets mentioned.",
      "processing_time": 62.980539083480835,
      "citing_paper_id": "274305801",
      "cited_paper_id": null
    },
    {
      "context_text": "We optimize the L 1 loss in the RGB and Fourier domain using the Adam [20] optimizer ( β 1 = 0 .",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only optimization techniques and loss functions.",
      "processing_time": 61.432491302490234,
      "citing_paper_id": "274305801",
      "cited_paper_id": null
    },
    {
      "context_text": "Testing is conducted on the BSD68 [28] and Urban100 [16] datasets.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "BSD68",
        "Urban100"
      ],
      "dataset_descriptions": {
        "BSD68": "Used to test image restoration methods, focusing on natural images with human-segmented annotations.",
        "Urban100": "Used to evaluate image restoration techniques, specifically targeting urban scenes with high-resolution images."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions two specific datasets, BSD68 and Urban100, which are used for testing image restoration methods.",
      "processing_time": 74.08092403411865,
      "citing_paper_id": "270045495",
      "cited_paper_id": 64193
    },
    {
      "context_text": "Most proposed solutions build on convolutional [55, 41, 58, 3] or Transformer-based architectures [26, 45, 53, 5] addressing single degradation tasks, such as denoising [55, 58, 5], dehazing [36, 35, 47] or deraining [17, 34].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various image restoration tasks and model architectures. No verifiable resources are identified.",
      "processing_time": 63.33805584907532,
      "citing_paper_id": "270045495",
      "cited_paper_id": 1900475
    },
    {
      "context_text": "Most proposed solutions build on convolutional [55, 41, 58, 3] or Transformer-based architectures [26, 45, 53, 5] addressing single degradation tasks, such as denoising [55, 58, 5], dehazing [36, 35, 47] or deraining [17, 34].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various image restoration tasks and model architectures. No verifiable resources are identified.",
      "processing_time": 63.33805584907532,
      "citing_paper_id": "270045495",
      "cited_paper_id": 8550762
    },
    {
      "context_text": "Most proposed solutions build on convolutional [55, 41, 58, 3] or Transformer-based architectures [26, 45, 53, 5] addressing single degradation tasks, such as denoising [55, 58, 5], dehazing [36, 35, 47] or deraining [17, 34].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various image restoration tasks and model architectures. No verifiable resources are identified.",
      "processing_time": 63.33805584907532,
      "citing_paper_id": "270045495",
      "cited_paper_id": 202733344
    },
    {
      "context_text": "Most proposed solutions build on convolutional [55, 41, 58, 3] or Transformer-based architectures [26, 45, 53, 5] addressing single degradation tasks, such as denoising [55, 58, 5], dehazing [36, 35, 47] or deraining [17, 34].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various image restoration tasks and model architectures. No verifiable resources are identified.",
      "processing_time": 63.33805584907532,
      "citing_paper_id": "270045495",
      "cited_paper_id": 233296013
    },
    {
      "context_text": "Most proposed solutions build on convolutional [55, 41, 58, 3] or Transformer-based architectures [26, 45, 53, 5] addressing single degradation tasks, such as denoising [55, 58, 5], dehazing [36, 35, 47] or deraining [17, 34].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various image restoration tasks and model architectures. No verifiable resources are identified.",
      "processing_time": 63.33805584907532,
      "citing_paper_id": "270045495",
      "cited_paper_id": 235358213
    },
    {
      "context_text": "Most proposed solutions build on convolutional [55, 41, 58, 3] or Transformer-based architectures [26, 45, 53, 5] addressing single degradation tasks, such as denoising [55, 58, 5], dehazing [36, 35, 47] or deraining [17, 34].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various image restoration tasks and model architectures. No verifiable resources are identified.",
      "processing_time": 63.33805584907532,
      "citing_paper_id": "270045495",
      "cited_paper_id": 244346144
    },
    {
      "context_text": "Recent advances in Deep learning-based approaches have shown great achievements in image restoration [55, 41, 20, 58, 26, 3, 53, 5, 23, 7, 10].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to deep learning approaches and image restoration. No verifiable resources are identified.",
      "processing_time": 63.86294913291931,
      "citing_paper_id": "270045495",
      "cited_paper_id": 1900475
    },
    {
      "context_text": "Recent advances in Deep learning-based approaches have shown great achievements in image restoration [55, 41, 20, 58, 26, 3, 53, 5, 23, 7, 10].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to deep learning approaches and image restoration. No verifiable resources are identified.",
      "processing_time": 63.86294913291931,
      "citing_paper_id": "270045495",
      "cited_paper_id": 3846544
    },
    {
      "context_text": "Recent advances in Deep learning-based approaches have shown great achievements in image restoration [55, 41, 20, 58, 26, 3, 53, 5, 23, 7, 10].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to deep learning approaches and image restoration. No verifiable resources are identified.",
      "processing_time": 63.86294913291931,
      "citing_paper_id": "270045495",
      "cited_paper_id": 8550762
    },
    {
      "context_text": "Recent advances in Deep learning-based approaches have shown great achievements in image restoration [55, 41, 20, 58, 26, 3, 53, 5, 23, 7, 10].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to deep learning approaches and image restoration. No verifiable resources are identified.",
      "processing_time": 63.86294913291931,
      "citing_paper_id": "270045495",
      "cited_paper_id": 244346144
    },
    {
      "context_text": "Recent advances in Deep learning-based approaches have shown great achievements in image restoration [55, 41, 20, 58, 26, 3, 53, 5, 23, 7, 10].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to deep learning approaches and image restoration. No verifiable resources are identified.",
      "processing_time": 63.86294913291931,
      "citing_paper_id": "270045495",
      "cited_paper_id": 268856875
    },
    {
      "context_text": "Reconstructing the clean image from its degraded counterpart is a highly ill-posed problem, however, a great body of work have addressed image restoration from a data-driven learning perspective, achieving tremendous results compared to prior hand-crafted methods [55, 41, 20, 58, 24, 53, 45].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to prior work on image restoration. The cited papers' titles also do not explicitly mention datasets.",
      "processing_time": 64.2532958984375,
      "citing_paper_id": "270045495",
      "cited_paper_id": 1900475
    },
    {
      "context_text": "Reconstructing the clean image from its degraded counterpart is a highly ill-posed problem, however, a great body of work have addressed image restoration from a data-driven learning perspective, achieving tremendous results compared to prior hand-crafted methods [55, 41, 20, 58, 24, 53, 45].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to prior work on image restoration. The cited papers' titles also do not explicitly mention datasets.",
      "processing_time": 64.2532958984375,
      "citing_paper_id": "270045495",
      "cited_paper_id": 3846544
    },
    {
      "context_text": "Reconstructing the clean image from its degraded counterpart is a highly ill-posed problem, however, a great body of work have addressed image restoration from a data-driven learning perspective, achieving tremendous results compared to prior hand-crafted methods [55, 41, 20, 58, 24, 53, 45].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to prior work on image restoration. The cited papers' titles also do not explicitly mention datasets.",
      "processing_time": 64.2532958984375,
      "citing_paper_id": "270045495",
      "cited_paper_id": 8550762
    },
    {
      "context_text": "Reconstructing the clean image from its degraded counterpart is a highly ill-posed problem, however, a great body of work have addressed image restoration from a data-driven learning perspective, achieving tremendous results compared to prior hand-crafted methods [55, 41, 20, 58, 24, 53, 45].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to prior work on image restoration. The cited papers' titles also do not explicitly mention datasets.",
      "processing_time": 64.2532958984375,
      "citing_paper_id": "270045495",
      "cited_paper_id": 235358213
    },
    {
      "context_text": "Reconstructing the clean image from its degraded counterpart is a highly ill-posed problem, however, a great body of work have addressed image restoration from a data-driven learning perspective, achieving tremendous results compared to prior hand-crafted methods [55, 41, 20, 58, 24, 53, 45].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to prior work on image restoration. The cited papers' titles also do not explicitly mention datasets.",
      "processing_time": 64.2532958984375,
      "citing_paper_id": "270045495",
      "cited_paper_id": 244346144
    },
    {
      "context_text": "Drawing inspiration from sparse Mixture-of-Experts concepts [40, 37], we integrate linear layers within each DaLe to enable a routing mechanism, associating input features x ∈ R H × W × C with their respective specialised degradation experts E D .",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a methodological approach inspired by sparse Mixture-of-Experts concepts.",
      "processing_time": 63.5499427318573,
      "citing_paper_id": "270045495",
      "cited_paper_id": 12462234
    },
    {
      "context_text": "The architecture comprises a four-level encoder-decoder structure, with each level containing a different number of transformer blocks, namely [2 , 3 , 3 , 4] from highest to lowest level.",
      "catation_intent": "none",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The provided context does not mention any specific datasets, only the architecture details of a model. No dataset names or specific data sources are mentioned.",
      "processing_time": 63.87701654434204,
      "citing_paper_id": "270045495",
      "cited_paper_id": 14092238
    },
    {
      "context_text": "The architecture comprises a four-level encoder-decoder structure, with each level containing a different number of transformer blocks, namely [2 , 3 , 3 , 4] from highest to lowest level.",
      "catation_intent": "none",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The provided context does not mention any specific datasets, only the architecture details of a model. No dataset names or specific data sources are mentioned.",
      "processing_time": 63.87701654434204,
      "citing_paper_id": "270045495",
      "cited_paper_id": 264555119
    },
    {
      "context_text": "For deblurring and low-light enhancement, we employ the GoPro [30] and the LOL-v1 [46] dataset, respectively.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "GoPro",
        "LOL-v1"
      ],
      "dataset_descriptions": {
        "GoPro": "Used for deblurring images, focusing on improving image clarity and sharpness using deep learning techniques.",
        "LOL-v1": "Used for low-light enhancement, specifically to improve the quality of images captured in low-light conditions using deep retinex decomposition."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions two specific datasets, GoPro and LOL-v1, which are used for deblurring and low-light enhancement, respectively.",
      "processing_time": 76.44138073921204,
      "citing_paper_id": "270045495",
      "cited_paper_id": 52008443
    },
    {
      "context_text": "Adverse conditions such as noise, haze, or rain significantly impact the practical utility of images in downstream tasks across various domains, including autonomous navigation [43, 4] and augmented reality[13, 39, 8].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only general conditions affecting image quality. The cited papers' titles suggest a focus on restoration techniques but do not explicitly name datasets.",
      "processing_time": 65.3476459980011,
      "citing_paper_id": "270045495",
      "cited_paper_id": 55358798
    },
    {
      "context_text": "Adverse conditions such as noise, haze, or rain significantly impact the practical utility of images in downstream tasks across various domains, including autonomous navigation [43, 4] and augmented reality[13, 39, 8].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only general conditions affecting image quality. The cited papers' titles suggest a focus on restoration techniques but do not explicitly name datasets.",
      "processing_time": 65.3476459980011,
      "citing_paper_id": "270045495",
      "cited_paper_id": 244714491
    },
    {
      "context_text": "Adverse conditions such as noise, haze, or rain significantly impact the practical utility of images in downstream tasks across various domains, including autonomous navigation [43, 4] and augmented reality[13, 39, 8].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only general conditions affecting image quality. The cited papers' titles suggest a focus on restoration techniques but do not explicitly name datasets.",
      "processing_time": 65.3476459980011,
      "citing_paper_id": "270045495",
      "cited_paper_id": 264555119
    },
    {
      "context_text": "We compare our All-in-One restorer with specialized All-in-One restoration methods, including BRDNet [42], LPNet [12], FDGAN [9], DL [11], MPRNet [52], AirNet [22], and PromptIR [31], trained simultaneously on three degradations: dehazing, deraining, and denoising.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions several methods/models but does not specify any datasets. The comparison is made using specialized restoration methods, not datasets.",
      "processing_time": 63.86728501319885,
      "citing_paper_id": "270045495",
      "cited_paper_id": 198185751
    },
    {
      "context_text": "We compare our All-in-One restorer with specialized All-in-One restoration methods, including BRDNet [42], LPNet [12], FDGAN [9], DL [11], MPRNet [52], AirNet [22], and PromptIR [31], trained simultaneously on three degradations: dehazing, deraining, and denoising.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions several methods/models but does not specify any datasets. The comparison is made using specialized restoration methods, not datasets.",
      "processing_time": 63.86728501319885,
      "citing_paper_id": "270045495",
      "cited_paper_id": 210838848
    },
    {
      "context_text": "We compare our All-in-One restorer with specialized All-in-One restoration methods, including BRDNet [42], LPNet [12], FDGAN [9], DL [11], MPRNet [52], AirNet [22], and PromptIR [31], trained simultaneously on three degradations: dehazing, deraining, and denoising.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions several methods/models but does not specify any datasets. The comparison is made using specialized restoration methods, not datasets.",
      "processing_time": 63.86728501319885,
      "citing_paper_id": "270045495",
      "cited_paper_id": 231802205
    },
    {
      "context_text": "For single-task deraining, we use Rain100L [48].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Rain100L"
      ],
      "dataset_descriptions": {
        "Rain100L": "Used for single-task deraining, specifically to train and evaluate deraining algorithms, focusing on texture preservation and noise reduction in rainy images."
      },
      "confidence_score": 1.0,
      "reasoning": "Rain100L is a specific dataset used for image deraining, which is a subtask of image restoration.",
      "processing_time": 71.11202430725098,
      "citing_paper_id": "270045495",
      "cited_paper_id": 219530930
    },
    {
      "context_text": "An emerging field known as All-in-One image restoration is advancing in low-level computer vision, utilizing a single deep blind restoration model to tackle multiple degradation types simultaneously [52, 43, 4, 18, 31, 54].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only the concept of All-in-One image restoration. No dataset names are present in the text.",
      "processing_time": 64.39740371704102,
      "citing_paper_id": "270045495",
      "cited_paper_id": 231802205
    },
    {
      "context_text": "An emerging field known as All-in-One image restoration is advancing in low-level computer vision, utilizing a single deep blind restoration model to tackle multiple degradation types simultaneously [52, 43, 4, 18, 31, 54].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only the concept of All-in-One image restoration. No dataset names are present in the text.",
      "processing_time": 64.39740371704102,
      "citing_paper_id": "270045495",
      "cited_paper_id": 244714491
    },
    {
      "context_text": "An emerging field known as All-in-One image restoration is advancing in low-level computer vision, utilizing a single deep blind restoration model to tackle multiple degradation types simultaneously [52, 43, 4, 18, 31, 54].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only the concept of All-in-One image restoration. No dataset names are present in the text.",
      "processing_time": 64.39740371704102,
      "citing_paper_id": "270045495",
      "cited_paper_id": 264555119
    },
    {
      "context_text": "An emerging field known as All-in-One image restoration is advancing in low-level computer vision, utilizing a single deep blind restoration model to tackle multiple degradation types simultaneously [52, 43, 4, 18, 31, 54].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only the concept of All-in-One image restoration. No dataset names are present in the text.",
      "processing_time": 64.39740371704102,
      "citing_paper_id": "270045495",
      "cited_paper_id": 268064460
    },
    {
      "context_text": "Notable works in this direction [22, 52, 43, 25, 11, 31, 54] employ contrastive [22], meta-learning [54] or visual prompting techniques [31, 44].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only techniques and methods. The cited paper titles do not provide additional context about datasets.",
      "processing_time": 64.07396864891052,
      "citing_paper_id": "270045495",
      "cited_paper_id": 231802205
    },
    {
      "context_text": "Notable works in this direction [22, 52, 43, 25, 11, 31, 54] employ contrastive [22], meta-learning [54] or visual prompting techniques [31, 44].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only techniques and methods. The cited paper titles do not provide additional context about datasets.",
      "processing_time": 64.07396864891052,
      "citing_paper_id": "270045495",
      "cited_paper_id": 244714491
    },
    {
      "context_text": "Notable works in this direction [22, 52, 43, 25, 11, 31, 54] employ contrastive [22], meta-learning [54] or visual prompting techniques [31, 44].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only techniques and methods. The cited paper titles do not provide additional context about datasets.",
      "processing_time": 64.07396864891052,
      "citing_paper_id": "270045495",
      "cited_paper_id": 247411392
    },
    {
      "context_text": "Notable works in this direction [22, 52, 43, 25, 11, 31, 54] employ contrastive [22], meta-learning [54] or visual prompting techniques [31, 44].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only techniques and methods. The cited paper titles do not provide additional context about datasets.",
      "processing_time": 64.07396864891052,
      "citing_paper_id": "270045495",
      "cited_paper_id": 268064460
    },
    {
      "context_text": "Notably, our method surpasses Restormer [53] and NAFNet [3] on the GoPro test set by 2 .",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "GoPro test set"
      ],
      "dataset_descriptions": {
        "GoPro test set": "Used to evaluate and compare the performance of the proposed method against Restormer and NAFNet, focusing on high-resolution image restoration tasks."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions the GoPro test set, which is a specific dataset used for evaluating image restoration methods. The dataset is used to compare the performance of the current method against Restormer and NAFNet.",
      "processing_time": 74.31537222862244,
      "citing_paper_id": "270045495",
      "cited_paper_id": 244346144
    },
    {
      "context_text": "3, our pipeline uses a U-shaped architecture [38, 53] with an asymmetric encoder-decoder design.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a U-shaped architecture and an asymmetric encoder-decoder design. No verifiable resources are identified.",
      "processing_time": 64.60239100456238,
      "citing_paper_id": "270045495",
      "cited_paper_id": 244346144
    },
    {
      "context_text": "Unlike previous Transformer-based methods [24, 53, 31], our decoder blocks receive signals from the encoder blocks to help recover the clean output via controllers .",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a methodological approach using Transformer-based models.",
      "processing_time": 62.753751277923584,
      "citing_paper_id": "270045495",
      "cited_paper_id": 244346144
    },
    {
      "context_text": "Unlike previous Transformer-based methods [24, 53, 31], our decoder blocks receive signals from the encoder blocks to help recover the clean output via controllers .",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a methodological approach using Transformer-based models.",
      "processing_time": 62.753751277923584,
      "citing_paper_id": "270045495",
      "cited_paper_id": 268064460
    },
    {
      "context_text": "Contrary to CNN-based networks, Transformer offer strong modeling capabilities for capturing global dependencies, which makes them outstanding image restorers [26, 53, 59].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only the capabilities of Transformer models for image restoration.",
      "processing_time": 62.7657949924469,
      "citing_paper_id": "270045495",
      "cited_paper_id": 244346144
    },
    {
      "context_text": "Each level includes several Transformer blocks [53, 31], incorporating our De gradation-a ware L earner (DaLe) before the attention layer.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models and methods. The context focuses on the architecture of the models used for image restoration.",
      "processing_time": 64.41817665100098,
      "citing_paper_id": "270045495",
      "cited_paper_id": 244346144
    },
    {
      "context_text": "Each level includes several Transformer blocks [53, 31], incorporating our De gradation-a ware L earner (DaLe) before the attention layer.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models and methods. The context focuses on the architecture of the models used for image restoration.",
      "processing_time": 64.41817665100098,
      "citing_paper_id": "270045495",
      "cited_paper_id": 268064460
    },
    {
      "context_text": "However, most of the existing works adopt task-specific learning, targeting a single known degradation at once [57, 26, 53, 5, 3].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to other works. There are no clear identifiers for datasets or other verifiable resources.",
      "processing_time": 64.64002799987793,
      "citing_paper_id": "270045495",
      "cited_paper_id": 244346144
    },
    {
      "context_text": "In this work, we enhance a Transformer-based architecture [53] by incorporating a dedicated degradation-aware learner for efficiently capturing the shared and distinct context of each degradation.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a method (Transformer-based architecture) and a concept (degradation-aware learner).",
      "processing_time": 64.19668054580688,
      "citing_paper_id": "270045495",
      "cited_paper_id": 244346144
    },
    {
      "context_text": "For All-in-One and single-task settings, we follow existing work [22, 31] and include following datasets: For image denoising in single task setting, we combine the BSD400 [1] and WED [27] datasets, adding Gaussian noise at levels σ ∈ [15 , 25 , 50] to create noisy images.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "BSD400",
        "WED"
      ],
      "dataset_descriptions": {
        "BSD400": "Used for image denoising in a single-task setting, combined with WED and augmented with Gaussian noise at levels σ ∈ [15, 25, 50] to create noisy images.",
        "WED": "Used for image denoising in a single-task setting, combined with BSD400 and augmented with Gaussian noise at levels σ ∈ [15, 25, 50] to create noisy images."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions two specific datasets, BSD400 and WED, which are used for image denoising in a single-task setting. These datasets are combined and Gaussian noise is added to create noisy images.",
      "processing_time": 85.92662024497986,
      "citing_paper_id": "270045495",
      "cited_paper_id": 247411392
    },
    {
      "context_text": "Even when certain approaches [25, 54] account for these characteristics, they often rely on external prior information or complex progressive meta-learning, thus failing to harness the inherent potential benefits of self-learning within the network.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only approaches and methods. There are no clear identifiers for datasets in the given context.",
      "processing_time": 63.99804067611694,
      "citing_paper_id": "270045495",
      "cited_paper_id": 247411392
    },
    {
      "context_text": "Next, a large-kernel convolutional block [14], denoted as Conv2Former, captures correlations among pixels within a local neighbourhood, mimicking window-based SA layers [24, 6, 5], while preserving the efficiency benefits of convolutions.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (Conv2Former) and its relation to other methods. No datasets are referenced for training or evaluation.",
      "processing_time": 64.96793222427368,
      "citing_paper_id": "270045495",
      "cited_paper_id": 253761491
    },
    {
      "context_text": "Second, leveraging auxiliary priors through prompt-learning [31, 44, 23, 10] is a prominent approach.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only methods or approaches. The cited paper titles do not provide additional information about datasets.",
      "processing_time": 64.05919241905212,
      "citing_paper_id": "270045495",
      "cited_paper_id": 268064460
    },
    {
      "context_text": "Second, leveraging auxiliary priors through prompt-learning [31, 44, 23, 10] is a prominent approach.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only methods or approaches. The cited paper titles do not provide additional information about datasets.",
      "processing_time": 64.05919241905212,
      "citing_paper_id": "270045495",
      "cited_paper_id": 268856875
    },
    {
      "context_text": "We conduct experiments by strictly following previous works in general image restoration [31, 54] under two different settings: (i) All-in-One and (ii) Single-task .",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only general settings for image restoration experiments.",
      "processing_time": 62.21004056930542,
      "citing_paper_id": "270045495",
      "cited_paper_id": 268064460
    },
    {
      "context_text": "Notably, [31] introduces tunable prompts that encode discriminative information about degradation types, albeit involving a large number of parameters.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (tunable prompts) for image restoration. The context is focused on the method's ability to encode information about degradation types.",
      "processing_time": 65.90605306625366,
      "citing_paper_id": "270045495",
      "cited_paper_id": 268064460
    },
    {
      "context_text": "Prompt-based learning [31, 44, 23] has emerged as a promising research direction.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a research direction. No dataset names are present in the citation span.",
      "processing_time": 63.50088715553284,
      "citing_paper_id": "270045495",
      "cited_paper_id": 268064460
    },
    {
      "context_text": "Prompt-based techniques [31, 23, 44] embed task-specific details into trainable parameters that interact with features to enhance them based on degradation types.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only techniques and methods. No dataset names are present in the citation span.",
      "processing_time": 63.49948024749756,
      "citing_paper_id": "270045495",
      "cited_paper_id": 268064460
    },
    {
      "context_text": "Our model is constructed upon the Restormer architecture [53], akin to previous All-in-One models [31, 10].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only references to models and architectures. There are no verifiable resources or datasets mentioned.",
      "processing_time": 63.875732421875,
      "citing_paper_id": "270045495",
      "cited_paper_id": 268856875
    },
    {
      "context_text": "A majority of generative models (e.g., WGAN [60], WGAN-GP [61]) use OT cost as the loss for the generative network.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only generative models and their use of OT cost as a loss function.",
      "processing_time": 63.31301307678223,
      "citing_paper_id": "273811722",
      "cited_paper_id": 2057420
    },
    {
      "context_text": "A primary body of works [18], [19], [20], [21], [22], [23], [44], [45], [46], [47] are driven by efficient network architectures, optimizing the model under pixel-wise L 1 or L 2 distances to produce deterministic results, which often yields decent quantitative performance.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only discusses network architectures and optimization methods.",
      "processing_time": 62.37950944900513,
      "citing_paper_id": "273811722",
      "cited_paper_id": 2141622
    },
    {
      "context_text": "A primary body of works [18], [19], [20], [21], [22], [23], [44], [45], [46], [47] are driven by efficient network architectures, optimizing the model under pixel-wise L 1 or L 2 distances to produce deterministic results, which often yields decent quantitative performance.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only discusses network architectures and optimization methods.",
      "processing_time": 62.37950944900513,
      "citing_paper_id": "273811722",
      "cited_paper_id": 244346144
    },
    {
      "context_text": "A primary body of works [18], [19], [20], [21], [22], [23], [44], [45], [46], [47] are driven by efficient network architectures, optimizing the model under pixel-wise L 1 or L 2 distances to produce deterministic results, which often yields decent quantitative performance.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only discusses network architectures and optimization methods.",
      "processing_time": 62.37950944900513,
      "citing_paper_id": "273811722",
      "cited_paper_id": 257636509
    },
    {
      "context_text": "A primary body of works [18], [19], [20], [21], [22], [23], [44], [45], [46], [47] are driven by efficient network architectures, optimizing the model under pixel-wise L 1 or L 2 distances to produce deterministic results, which often yields decent quantitative performance.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only discusses network architectures and optimization methods.",
      "processing_time": 62.37950944900513,
      "citing_paper_id": "273811722",
      "cited_paper_id": 260957038
    },
    {
      "context_text": "Ambient-GAN [52] generates clean images from noisy input, assuming the degradation satisfies certain constraints.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific dataset names, only a method (Ambient-GAN). The context focuses on the capabilities of the method rather than the use of a specific dataset.",
      "processing_time": 65.70207142829895,
      "citing_paper_id": "273811722",
      "cited_paper_id": 3481010
    },
    {
      "context_text": "Another line of task-specific methods tackles image restoration from a distribution-fitting perspective with deep generative models [48], [49], [50], [51], [52] and utilizes the degraded input as a condition for generation.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and models. The context focuses on the use of deep generative models for image restoration.",
      "processing_time": 64.580637216568,
      "citing_paper_id": "273811722",
      "cited_paper_id": 3481010
    },
    {
      "context_text": "Traditional methods focus on designing an optimization problem that exploits priors of natural image [1], [2], [3], [4], [5] or corresponding image-induced signals such as gradients [6], [7] and residuals or noise maps [8], [9], [10].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only general concepts and methods. No dataset names are present in the text.",
      "processing_time": 63.678587436676025,
      "citing_paper_id": "273811722",
      "cited_paper_id": 4531078
    },
    {
      "context_text": "Traditional methods focus on designing an optimization problem that exploits priors of natural image [1], [2], [3], [4], [5] or corresponding image-induced signals such as gradients [6], [7] and residuals or noise maps [8], [9], [10].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only general concepts and methods. No dataset names are present in the text.",
      "processing_time": 63.678587436676025,
      "citing_paper_id": "273811722",
      "cited_paper_id": 16113001
    },
    {
      "context_text": "Traditional methods focus on designing an optimization problem that exploits priors of natural image [1], [2], [3], [4], [5] or corresponding image-induced signals such as gradients [6], [7] and residuals or noise maps [8], [9], [10].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only general concepts and methods. No dataset names are present in the text.",
      "processing_time": 63.678587436676025,
      "citing_paper_id": "273811722",
      "cited_paper_id": 40060575
    },
    {
      "context_text": "Traditional methods focus on designing an optimization problem that exploits priors of natural image [1], [2], [3], [4], [5] or corresponding image-induced signals such as gradients [6], [7] and residuals or noise maps [8], [9], [10].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only general concepts and methods. No dataset names are present in the text.",
      "processing_time": 63.678587436676025,
      "citing_paper_id": "273811722",
      "cited_paper_id": 206724273
    },
    {
      "context_text": "The deblurring and low-light enhancement tasks leverage GoPro [42] and LOL-v1 [43] datasets, respectively.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "GoPro",
        "LOL-v1"
      ],
      "dataset_descriptions": {
        "GoPro": "Used for dynamic scene deblurring, leveraging multi-scale convolutional neural networks to improve image clarity and reduce motion blur.",
        "LOL-v1": "Used for low-light image enhancement, focusing on improving visibility and color fidelity in underexposed images."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions two specific datasets, GoPro and LOL-v1, which are used for deblurring and low-light enhancement tasks, respectively.",
      "processing_time": 76.9359998703003,
      "citing_paper_id": "273811722",
      "cited_paper_id": 8671030
    },
    {
      "context_text": "3) We perform extensive experiments under AIR setting on a mixed dataset collected from multiple benchmarks, including Rain100L [39] for deraining, BSD68 [40] for denoising, SOTS [41] for dehazing, GoPro [42] for deblurring, and LOL [43] for low-light enhancement.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Rain100L",
        "BSD68",
        "SOTS",
        "GoPro",
        "LOL"
      ],
      "dataset_descriptions": {
        "Rain100L": "Used for deraining experiments, evaluating the effectiveness of the proposed method in removing rain streaks from images.",
        "BSD68": "Used for denoising experiments, assessing the performance of the method in reducing noise while preserving image details.",
        "SOTS": "Used for dehazing experiments, testing the ability of the method to enhance visibility in hazy conditions.",
        "GoPro": "Used for deblurring experiments, evaluating the method's capability to restore sharpness in blurred images.",
        "LOL": "Used for low-light enhancement experiments, assessing the method's performance in improving image quality in low-light conditions."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions specific datasets used for various image restoration tasks, which are relevant to the All-in-One Image Restoration research topic.",
      "processing_time": 88.3519458770752,
      "citing_paper_id": "273811722",
      "cited_paper_id": 8671030
    },
    {
      "context_text": "3) We perform extensive experiments under AIR setting on a mixed dataset collected from multiple benchmarks, including Rain100L [39] for deraining, BSD68 [40] for denoising, SOTS [41] for dehazing, GoPro [42] for deblurring, and LOL [43] for low-light enhancement.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Rain100L",
        "BSD68",
        "SOTS",
        "GoPro",
        "LOL"
      ],
      "dataset_descriptions": {
        "Rain100L": "Used for deraining experiments, evaluating the effectiveness of the proposed method in removing rain streaks from images.",
        "BSD68": "Used for denoising experiments, assessing the performance of the method in reducing noise while preserving image details.",
        "SOTS": "Used for dehazing experiments, testing the ability of the method to enhance visibility in hazy conditions.",
        "GoPro": "Used for deblurring experiments, evaluating the method's capability to restore sharpness in blurred images.",
        "LOL": "Used for low-light enhancement experiments, assessing the method's performance in improving image quality in low-light conditions."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions specific datasets used for various image restoration tasks, which are relevant to the All-in-One Image Restoration research topic.",
      "processing_time": 88.3519458770752,
      "citing_paper_id": "273811722",
      "cited_paper_id": 39760169
    },
    {
      "context_text": "For the noise and blur case, we combine the noisy images from BSD400 [76] and WED [77] and blurry images from GoPro [42] for co-training.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "BSD400",
        "WED",
        "GoPro"
      ],
      "dataset_descriptions": {
        "BSD400": "Used for co-training with noisy images to improve image restoration performance, specifically addressing noise and blur issues.",
        "WED": "Used for co-training with noisy images to enhance image restoration, focusing on noise and blur challenges.",
        "GoPro": "Used for co-training with blurry images to improve image deblurring, specifically addressing motion blur in dynamic scenes."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions specific datasets used for co-training in the context of image restoration, particularly for noise and blur.",
      "processing_time": 81.22772002220154,
      "citing_paper_id": "273811722",
      "cited_paper_id": 8671030
    },
    {
      "context_text": "Recent advances in deep learning techniques [11], [12], [13], [14] have triggered great achievements in image restoration, in which most methods [15], [16], [17], [18], [19], [20], [21] [22], [23] train task-specific restoration models over degraded-clean image pairs of a single known degradation.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only general references to training task-specific restoration models over degraded-clean image pairs.",
      "processing_time": 63.89269018173218,
      "citing_paper_id": "273811722",
      "cited_paper_id": 10514149
    },
    {
      "context_text": "Recent advances in deep learning techniques [11], [12], [13], [14] have triggered great achievements in image restoration, in which most methods [15], [16], [17], [18], [19], [20], [21] [22], [23] train task-specific restoration models over degraded-clean image pairs of a single known degradation.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only general references to training task-specific restoration models over degraded-clean image pairs.",
      "processing_time": 63.89269018173218,
      "citing_paper_id": "273811722",
      "cited_paper_id": 26229170
    },
    {
      "context_text": "Recent advances in deep learning techniques [11], [12], [13], [14] have triggered great achievements in image restoration, in which most methods [15], [16], [17], [18], [19], [20], [21] [22], [23] train task-specific restoration models over degraded-clean image pairs of a single known degradation.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only general references to training task-specific restoration models over degraded-clean image pairs.",
      "processing_time": 63.89269018173218,
      "citing_paper_id": "273811722",
      "cited_paper_id": 206594692
    },
    {
      "context_text": "Recent advances in deep learning techniques [11], [12], [13], [14] have triggered great achievements in image restoration, in which most methods [15], [16], [17], [18], [19], [20], [21] [22], [23] train task-specific restoration models over degraded-clean image pairs of a single known degradation.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only general references to training task-specific restoration models over degraded-clean image pairs.",
      "processing_time": 63.89269018173218,
      "citing_paper_id": "273811722",
      "cited_paper_id": 232352874
    },
    {
      "context_text": "Recent advances in deep learning techniques [11], [12], [13], [14] have triggered great achievements in image restoration, in which most methods [15], [16], [17], [18], [19], [20], [21] [22], [23] train task-specific restoration models over degraded-clean image pairs of a single known degradation.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only general references to training task-specific restoration models over degraded-clean image pairs.",
      "processing_time": 63.89269018173218,
      "citing_paper_id": "273811722",
      "cited_paper_id": 244346144
    },
    {
      "context_text": "Recent advances in deep learning techniques [11], [12], [13], [14] have triggered great achievements in image restoration, in which most methods [15], [16], [17], [18], [19], [20], [21] [22], [23] train task-specific restoration models over degraded-clean image pairs of a single known degradation.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only general references to training task-specific restoration models over degraded-clean image pairs.",
      "processing_time": 63.89269018173218,
      "citing_paper_id": "273811722",
      "cited_paper_id": 260957038
    },
    {
      "context_text": "We use PSNR/SSIM for measuring pixel-wise similarity, LPIPS [71]/FID [72] for measuring perceptual deviation, and two well-known non-reference indexes NIQE [73] and PIQE [74] to assess real-world multiple-degradation images.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions several metrics but does not refer to any specific datasets. Metrics are excluded according to the instructions.",
      "processing_time": 62.9286105632782,
      "citing_paper_id": "273811722",
      "cited_paper_id": 16892725
    },
    {
      "context_text": "For dehazing, we employ SOTS [41] and real-world O-HAZE [80].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "SOTS",
        "O-HAZE"
      ],
      "dataset_descriptions": {
        "SOTS": "Used for benchmarking single-image dehazing methods, providing synthetic and real-world hazy images for evaluation.",
        "O-HAZE": "Used for evaluating dehazing algorithms with real-world hazy and haze-free outdoor images, focusing on realistic conditions."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions two datasets, SOTS and O-HAZE, which are used for dehazing tasks. Both are specific and verifiable datasets.",
      "processing_time": 76.8482735157013,
      "citing_paper_id": "273811722",
      "cited_paper_id": 39760169
    },
    {
      "context_text": "For dehazing, we employ SOTS [41] and real-world O-HAZE [80].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "SOTS",
        "O-HAZE"
      ],
      "dataset_descriptions": {
        "SOTS": "Used for benchmarking single-image dehazing methods, providing synthetic and real-world hazy images for evaluation.",
        "O-HAZE": "Used for evaluating dehazing algorithms with real-world hazy and haze-free outdoor images, focusing on realistic conditions."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions two datasets, SOTS and O-HAZE, which are used for dehazing tasks. Both are specific and verifiable datasets.",
      "processing_time": 76.8482735157013,
      "citing_paper_id": "273811722",
      "cited_paper_id": 206598041
    },
    {
      "context_text": "We evaluate the approaches for comparison on Rain100L [39] and real-world SPANet [79] for deraining, SOTS [41] and real-world O-HAZE [80] for dehazing, BSD68 [40] for denoising.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Rain100L",
        "SPANet",
        "SOTS",
        "O-HAZE",
        "BSD68"
      ],
      "dataset_descriptions": {
        "Rain100L": "Used for evaluating deraining algorithms, focusing on synthetic rain effects in images.",
        "SPANet": "Used for evaluating deraining algorithms, focusing on real-world rain effects in images.",
        "SOTS": "Used for evaluating dehazing algorithms, focusing on synthetic hazy conditions in images.",
        "O-HAZE": "Used for evaluating dehazing algorithms, focusing on real-world hazy conditions in outdoor images.",
        "BSD68": "Used for evaluating denoising algorithms, focusing on synthetic noise in images."
      },
      "confidence_score": 1.0,
      "reasoning": "The citation mentions specific datasets used for evaluating image restoration techniques, including deraining, dehazing, and denoising.",
      "processing_time": 86.75940251350403,
      "citing_paper_id": "273811722",
      "cited_paper_id": 39760169
    },
    {
      "context_text": "We evaluate the approaches for comparison on Rain100L [39] and real-world SPANet [79] for deraining, SOTS [41] and real-world O-HAZE [80] for dehazing, BSD68 [40] for denoising.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Rain100L",
        "SPANet",
        "SOTS",
        "O-HAZE",
        "BSD68"
      ],
      "dataset_descriptions": {
        "Rain100L": "Used for evaluating deraining algorithms, focusing on synthetic rain effects in images.",
        "SPANet": "Used for evaluating deraining algorithms, focusing on real-world rain effects in images.",
        "SOTS": "Used for evaluating dehazing algorithms, focusing on synthetic hazy conditions in images.",
        "O-HAZE": "Used for evaluating dehazing algorithms, focusing on real-world hazy conditions in outdoor images.",
        "BSD68": "Used for evaluating denoising algorithms, focusing on synthetic noise in images."
      },
      "confidence_score": 1.0,
      "reasoning": "The citation mentions specific datasets used for evaluating image restoration techniques, including deraining, dehazing, and denoising.",
      "processing_time": 86.75940251350403,
      "citing_paper_id": "273811722",
      "cited_paper_id": 206598041
    },
    {
      "context_text": "To tackle the AIR problem, several works [28], [29], [30], [31], [32], [33], [34] directly fit joint regression models over multi-domain degraded-clean image pairs, minimizing pixel-wise distortion measures such as L 1 or L 2 distances.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general methods and approaches. No dataset names are present in the text.",
      "processing_time": 63.854830503463745,
      "citing_paper_id": "273811722",
      "cited_paper_id": 195787503
    },
    {
      "context_text": "To tackle the AIR problem, several works [28], [29], [30], [31], [32], [33], [34] directly fit joint regression models over multi-domain degraded-clean image pairs, minimizing pixel-wise distortion measures such as L 1 or L 2 distances.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general methods and approaches. No dataset names are present in the text.",
      "processing_time": 63.854830503463745,
      "citing_paper_id": "273811722",
      "cited_paper_id": 259224666
    },
    {
      "context_text": "To tackle the AIR problem, several works [28], [29], [30], [31], [32], [33], [34] directly fit joint regression models over multi-domain degraded-clean image pairs, minimizing pixel-wise distortion measures such as L 1 or L 2 distances.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general methods and approaches. No dataset names are present in the text.",
      "processing_time": 63.854830503463745,
      "citing_paper_id": "273811722",
      "cited_paper_id": 260085391
    },
    {
      "context_text": "We have now included a comparison on Rain100L dataset [28] between the MPRNet [18], NAFNet [84], Restormer [19] methods and the corresponding meth-ods with the proposed REC mechanism.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Rain100L"
      ],
      "dataset_descriptions": {
        "Rain100L": "Used to evaluate and compare the performance of MPRNet, NAFNet, Restormer, and their variants with the proposed REC mechanism on high-resolution image restoration tasks."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions the Rain100L dataset, which is a specific dataset used for evaluating image restoration methods. The dataset is used to compare the performance of different models.",
      "processing_time": 74.64874768257141,
      "citing_paper_id": "273811722",
      "cited_paper_id": 195787503
    },
    {
      "context_text": "We have now included a comparison on Rain100L dataset [28] between the MPRNet [18], NAFNet [84], Restormer [19] methods and the corresponding meth-ods with the proposed REC mechanism.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Rain100L"
      ],
      "dataset_descriptions": {
        "Rain100L": "Used to evaluate and compare the performance of MPRNet, NAFNet, Restormer, and their variants with the proposed REC mechanism on high-resolution image restoration tasks."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions the Rain100L dataset, which is a specific dataset used for evaluating image restoration methods. The dataset is used to compare the performance of different models.",
      "processing_time": 74.64874768257141,
      "citing_paper_id": "273811722",
      "cited_paper_id": 244346144
    },
    {
      "context_text": "We compare our All-in-One DA-RCOT map with several state-of-the-art methods including three general restoration restorers, i.e., MPRNet [18], Restormer [19], IR-SDE [22]; and five specialized All-in-One models, i.e., DL [28], AirNet [29], IDR [31], PromptIR [32], DA-CLIP [33].",
      "catation_intent": "reusable resource",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions various methods and models but does not refer to any specific datasets. The names mentioned are models or methods, not datasets.",
      "processing_time": 64.30354595184326,
      "citing_paper_id": "273811722",
      "cited_paper_id": 195787503
    },
    {
      "context_text": "We compare our All-in-One DA-RCOT map with several state-of-the-art methods including three general restoration restorers, i.e., MPRNet [18], Restormer [19], IR-SDE [22]; and five specialized All-in-One models, i.e., DL [28], AirNet [29], IDR [31], PromptIR [32], DA-CLIP [33].",
      "catation_intent": "reusable resource",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions various methods and models but does not refer to any specific datasets. The names mentioned are models or methods, not datasets.",
      "processing_time": 64.30354595184326,
      "citing_paper_id": "273811722",
      "cited_paper_id": 244346144
    },
    {
      "context_text": "We compare our All-in-One DA-RCOT map with several state-of-the-art methods including three general restoration restorers, i.e., MPRNet [18], Restormer [19], IR-SDE [22]; and five specialized All-in-One models, i.e., DL [28], AirNet [29], IDR [31], PromptIR [32], DA-CLIP [33].",
      "catation_intent": "reusable resource",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions various methods and models but does not refer to any specific datasets. The names mentioned are models or methods, not datasets.",
      "processing_time": 64.30354595184326,
      "citing_paper_id": "273811722",
      "cited_paper_id": 259224666
    },
    {
      "context_text": "We compare our All-in-One DA-RCOT map with several state-of-the-art methods including three general restoration restorers, i.e., MPRNet [18], Restormer [19], IR-SDE [22]; and five specialized All-in-One models, i.e., DL [28], AirNet [29], IDR [31], PromptIR [32], DA-CLIP [33].",
      "catation_intent": "reusable resource",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions various methods and models but does not refer to any specific datasets. The names mentioned are models or methods, not datasets.",
      "processing_time": 64.30354595184326,
      "citing_paper_id": "273811722",
      "cited_paper_id": 260085391
    },
    {
      "context_text": "For the haze and rain case, we train models combining hazy images from SOTS [80] and rainy images from SPANet [79].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "SOTS",
        "SPANet"
      ],
      "dataset_descriptions": {
        "SOTS": "Used to train models on hazy images, focusing on image dehazing techniques and improving visual quality.",
        "SPANet": "Used to train models on rainy images, focusing on rain removal techniques and enhancing image clarity."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions two specific datasets, SOTS and SPANet, which are used for training models to handle hazy and rainy images, respectively.",
      "processing_time": 75.63023591041565,
      "citing_paper_id": "273811722",
      "cited_paper_id": 206598041
    },
    {
      "context_text": "Consequently, they tend to produce results with compromised structural details [35], [36], [37], e.g., distorted color and excessively smoothed textures (see Figure 1 (b)).",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only issues with image restoration results. No verifiable resources are identified.",
      "processing_time": 63.750234842300415,
      "citing_paper_id": "273811722",
      "cited_paper_id": 215763824
    },
    {
      "context_text": "Consequently, they tend to produce results with compromised structural details [35], [36], [37], e.g., distorted color and excessively smoothed textures (see Figure 1 (b)).",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only issues with image restoration results. No verifiable resources are identified.",
      "processing_time": 63.750234842300415,
      "citing_paper_id": "273811722",
      "cited_paper_id": 259075580
    },
    {
      "context_text": "However, this specificity limits their practicability in real-world applications, e.g., autonomous navigation [24], [25], surveillance systems [26], and digital photography [27], where varied and unexpected degradations usually occur.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only application areas. There are no clear identifiers for datasets or other verifiable resources.",
      "processing_time": 64.12014818191528,
      "citing_paper_id": "273811722",
      "cited_paper_id": 233148602
    },
    {
      "context_text": "Snips [54], DDRM [55], and DPS [56] assume the degradation and its parameters are known at test time.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only methods or models. The cited papers' titles do not provide additional information about datasets.",
      "processing_time": 64.47929835319519,
      "citing_paper_id": "273811722",
      "cited_paper_id": 235253954
    },
    {
      "context_text": "Snips [54], DDRM [55], and DPS [56] assume the degradation and its parameters are known at test time.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only methods or models. The cited papers' titles do not provide additional information about datasets.",
      "processing_time": 64.47929835319519,
      "citing_paper_id": "273811722",
      "cited_paper_id": 252596252
    },
    {
      "context_text": "We can observe from the figure that as compared to the general restorers [18], [19], the All-in-One models are generally more robust as the number of degradations increases.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a comparison between models. No dataset names are provided in the context.",
      "processing_time": 63.9061541557312,
      "citing_paper_id": "273811722",
      "cited_paper_id": 244346144
    },
    {
      "context_text": "Four methods are selected for comparisons, including general restorers Restormer [19], IR-SDE [22] and All-in-One PromptIR [32] and DA-CLIP [33].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions several methods but does not refer to any specific datasets. The context is focused on comparing different image restoration methods.",
      "processing_time": 63.901270151138306,
      "citing_paper_id": "273811722",
      "cited_paper_id": 244346144
    },
    {
      "context_text": "Four methods are selected for comparisons, including general restorers Restormer [19], IR-SDE [22] and All-in-One PromptIR [32] and DA-CLIP [33].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions several methods but does not refer to any specific datasets. The context is focused on comparing different image restoration methods.",
      "processing_time": 63.901270151138306,
      "citing_paper_id": "273811722",
      "cited_paper_id": 259224666
    },
    {
      "context_text": "In all experiments, the transport map T θ is implemented with the backbone in Restormer [19] and the potential network is the same as [75].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions Restormer but does not indicate it is a dataset. It is referenced as a method or model for image restoration.",
      "processing_time": 64.29642844200134,
      "citing_paper_id": "273811722",
      "cited_paper_id": 244346144
    },
    {
      "context_text": "The blocks consist of two key sub-modules: Multi-Dconv head transposed attention (MDTA), and Gated-Dconv feedforward network (GDFN) [19].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only model components. The context is focused on describing the architecture of the model rather than the data used.",
      "processing_time": 64.83977794647217,
      "citing_paper_id": "273811722",
      "cited_paper_id": 244346144
    },
    {
      "context_text": "NOT [63], KNOT [64] estimate OT maps and plans using neural networks under the duality framework and apply their method to unpaired image translation.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and approaches. The context is about estimating optimal transport maps and plans using neural networks.",
      "processing_time": 64.44632887840271,
      "citing_paper_id": "273811722",
      "cited_paper_id": 246411466
    },
    {
      "context_text": "For the unpaired setting, although datasets that contain paired data are used for training, we randomly shuffle the target x and degraded input y to ensure the loss is isolated from paired information, which is a common strategy [63], [64], [65] for unpaired restoration.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.2,
      "reasoning": "The context mentions 'datasets that contain paired data' but does not specify any named datasets. The usage is described in a general sense without referencing any specific dataset.",
      "processing_time": 65.51361465454102,
      "citing_paper_id": "273811722",
      "cited_paper_id": 246411466
    },
    {
      "context_text": "For each task, we add a task-specific state-of-the-art method for each comparison, i.e., SFNet [81] for deraining, Dehazeformer [82] for dehazing, and RCD [83] for denoising.",
      "catation_intent": "none",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions methods (SFNet, Dehazeformer, RCD) but does not specify any datasets. The cited papers' titles do not provide additional dataset information.",
      "processing_time": 65.6946632862091,
      "citing_paper_id": "273811722",
      "cited_paper_id": 257804691
    },
    {
      "context_text": "For each task, we add a task-specific state-of-the-art method for each comparison, i.e., SFNet [81] for deraining, Dehazeformer [82] for dehazing, and RCD [83] for denoising.",
      "catation_intent": "none",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions methods (SFNet, Dehazeformer, RCD) but does not specify any datasets. The cited papers' titles do not provide additional dataset information.",
      "processing_time": 65.6946632862091,
      "citing_paper_id": "273811722",
      "cited_paper_id": 259298517
    },
    {
      "context_text": "IDM [57] introduces a scale-adaptive condition on low-resolution images to achieve high-fidelity super-resolution, while IR-SDE [22] proposes a loss function based on maximum likelihood to train a mean-reverting score-based model.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models and methods. The context focuses on describing the methodologies used in the cited papers.",
      "processing_time": 64.27437710762024,
      "citing_paper_id": "273811722",
      "cited_paper_id": 257804739
    },
    {
      "context_text": "As shown in Figure 4, compared to the prompt-based degradation embeddings in Promp-tIR [32], the residual embeddings R 1 of DA-RCOT are clearly separated according to specific tasks.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a comparison between methods. The cited paper title suggests a focus on image restoration but does not introduce a dataset.",
      "processing_time": 65.31022262573242,
      "citing_paper_id": "273811722",
      "cited_paper_id": 259224666
    },
    {
      "context_text": "Here, we compare DA-RCOT with PromptIR [32] and DA-CLIP [33] to explore their capability in handling such complex scenarios.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models or methods. The context focuses on comparing different approaches for image restoration.",
      "processing_time": 64.07138729095459,
      "citing_paper_id": "273811722",
      "cited_paper_id": 259224666
    },
    {
      "context_text": "In the paired setting, DA-RCOT outpforms other Approaches in terms of both distortion measures (PSNR and SSIM) and perceptual quality measures (LPIPS and FID), offering an average improvement of 0.55 db in PSNR compared to the PromptIR [32].",
      "catation_intent": "findings",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only performance metrics and a comparison to another method (PromptIR).",
      "processing_time": 63.50900983810425,
      "citing_paper_id": "273811722",
      "cited_paper_id": 259224666
    },
    {
      "context_text": "We compare our residual embeddings with the prompt-based degradation embeddings from the state-of-the-art method PromptIR [32].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a comparison with a method called PromptIR. The title of the cited paper confirms that PromptIR is a method, not a dataset.",
      "processing_time": 66.37096357345581,
      "citing_paper_id": "273811722",
      "cited_paper_id": 259224666
    },
    {
      "context_text": "PromptIR [32] employs a learnable visual prompt module to adaptively encode the information of degradation type and further guide the restoration.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (PromptIR) and its components. No verifiable resources are identified.",
      "processing_time": 64.28834557533264,
      "citing_paper_id": "273811722",
      "cited_paper_id": 259224666
    },
    {
      "context_text": "As shown in Table 2, DA-RCOT stands out with an average improvement of 0.68 dB over PromptIR [32] in PSNR and 10.13 reduction over DA-CLIP [33] in FID, which demonstrates the adapt-ability of the DA-RCOT map for more diverse degradations.",
      "catation_intent": "findings",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only performance metrics and comparisons between methods.",
      "processing_time": 62.42049956321716,
      "citing_paper_id": "273811722",
      "cited_paper_id": 259224666
    },
    {
      "context_text": "Specifically, in terms of PSNR, DA-RCOT surpasses PromptIR [32] by 1.94 dB on Rain100L and outperforms DA-CLIP [33] by 2.49 dB on SPANet.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Rain100L",
        "SPANet"
      ],
      "dataset_descriptions": {
        "Rain100L": "Used to evaluate the performance of DA-RCOT in image restoration, specifically comparing PSNR scores against PromptIR. The dataset focuses on rain removal from images.",
        "SPANet": "Used to evaluate the performance of DA-RCOT in image restoration, specifically comparing PSNR scores against DA-CLIP. The dataset focuses on various image degradation scenarios."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions two datasets, Rain100L and SPANet, which are used to evaluate the performance of image restoration methods. The datasets are specific and relevant to the research topic.",
      "processing_time": 83.32500100135803,
      "citing_paper_id": "273811722",
      "cited_paper_id": 259224666
    },
    {
      "context_text": "IDR [31] breaks down image degradations into their physical principles and implements AIR using a two-stage meta-learning based approach.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for image restoration. The context focuses on the approach and principles rather than data sources.",
      "processing_time": 65.03077530860901,
      "citing_paper_id": "273811722",
      "cited_paper_id": 260085391
    },
    {
      "context_text": "This work extends our conference paper [38] published at ICML.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a reference to a previous conference paper. There is no information about datasets used in the research.",
      "processing_time": 64.7211172580719,
      "citing_paper_id": "273811722",
      "cited_paper_id": 269605449
    },
    {
      "context_text": "The visual resultsPar-ticularly, as compared to the preliminary work RCOT [38], DA-RCOT still offers a non-trivial improvement. and colors).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a comparison to a method (RCOT). No verifiable resources are identified.",
      "processing_time": 64.31552457809448,
      "citing_paper_id": "273811722",
      "cited_paper_id": 269605449
    },
    {
      "context_text": "Definition 2 (MP) : the Monge problem [58] for optimal transport can be formulated as follows: where T : Y → X is a transport map that pushes forward P to Q , denoted as T # P = Q .",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a mathematical formulation of the Monge problem for optimal transport.",
      "processing_time": 63.5796012878418,
      "citing_paper_id": "273811722",
      "cited_paper_id": null
    },
    {
      "context_text": "OT problem seeks to determine the optimal transport map (Monge Problem [58], also known as MP) or transport plan (Kantorovich Problem [59], also known as KP) to transform a distribution into another with the minimal cost.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only theoretical concepts related to optimal transport problems.",
      "processing_time": 63.388676404953,
      "citing_paper_id": "273811722",
      "cited_paper_id": null
    },
    {
      "context_text": "Based on the Retinex theory, researchers have conducted extensive research and improved enhancement performance [43, 44, 45].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only references to research papers. No dataset names are provided in the context.",
      "processing_time": 64.31318712234497,
      "citing_paper_id": "267499701",
      "cited_paper_id": 2972940
    },
    {
      "context_text": "Based on the Retinex theory, researchers have conducted extensive research and improved enhancement performance [43, 44, 45].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only references to research papers. No dataset names are provided in the context.",
      "processing_time": 64.31318712234497,
      "citing_paper_id": "267499701",
      "cited_paper_id": 4095486
    },
    {
      "context_text": "For example, Peng et al. [32] incorporated adaptive color correction into the image formation model (IFM) and proposed a generalized dark channel prior for single image restoration.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method and a model. The context focuses on the technique used for image restoration.",
      "processing_time": 64.70358848571777,
      "citing_paper_id": "267499701",
      "cited_paper_id": 4077800
    },
    {
      "context_text": "Traditional learning methods mainly use LS or GC for color correction, which uses the corrected features as prior features or post-optimization of the main repair model (e.g., DCP-[32, 33, 50] and Retinex-driven [8]).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and models. The context focuses on color correction techniques and their application in image restoration.",
      "processing_time": 64.9616310596466,
      "citing_paper_id": "267499701",
      "cited_paper_id": 4077800
    },
    {
      "context_text": "Consequently, learning methods that incorporate physical prior models (such as DCP-[20] and Retinex-guided [21]) have been proposed.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods or models. The context focuses on physical prior models for image restoration.",
      "processing_time": 64.53336381912231,
      "citing_paper_id": "267499701",
      "cited_paper_id": 4077800
    },
    {
      "context_text": "…(2022) ACDC [11] JOE (2022) CEEF [12] TMM (2022) ROP+ [1] TPAMI (2023) PCDE [61] SPL (2023) SMNet [62] TMM (2023) TOENet [37] TIM (2023) AoSRNet — dataset comprises RESIDED-OTS (which incorporates depth information) [56] for land scenes, the Singapore maritime dataset (SMD) [57] for water scenes.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "RESIDED-OTS",
        "Singapore maritime dataset (SMD)"
      ],
      "dataset_descriptions": {
        "RESIDED-OTS": "Used for land scene restoration, incorporating depth information to enhance image quality and detail.",
        "Singapore maritime dataset (SMD)": "Used for water scene restoration, focusing on maritime environments to improve visibility and clarity in images."
      },
      "confidence_score": 1.0,
      "reasoning": "The citation mentions two specific datasets, RESIDED-OTS and SMD, which are used for image restoration in different environments.",
      "processing_time": 77.62542700767517,
      "citing_paper_id": "267499701",
      "cited_paper_id": 11905372
    },
    {
      "context_text": "In comparison, the Table 3: PSNR, SSIM, and NIQE results of various dehazing methods on RESIDE-OTS [56] and SMD [57]. utilization of DEM and CRM enabled AoSRNet to evolve beyond exclusive reliance.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "RESIDE-OTS",
        "SMD"
      ],
      "dataset_descriptions": {
        "RESIDE-OTS": "Used to evaluate dehazing methods, specifically comparing PSNR, SSIM, and NIQE results. The dataset provides outdoor scenes for testing image restoration techniques.",
        "SMD": "Used to evaluate dehazing methods, specifically comparing PSNR, SSIM, and NIQE results. The dataset provides synthetic and real-world hazy images for testing image restoration techniques."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions two datasets, RESIDE-OTS and SMD, which are used to evaluate dehazing methods. These datasets are specific and relevant to the research topic of image restoration.",
      "processing_time": 83.04506969451904,
      "citing_paper_id": "267499701",
      "cited_paper_id": 11905372
    },
    {
      "context_text": "Hereby, The performance of the classic DCP-based methods [4, 31] in the dust image enhancement task is not significant.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods. There are no clear identifiers for datasets in the given context.",
      "processing_time": 64.07878375053406,
      "citing_paper_id": "267499701",
      "cited_paper_id": 20658546
    },
    {
      "context_text": "AOD-Net [29] reconstructs the atmospheric scattering model, reducing unnatural dehazed images generated due to inaccurate estimates of transmittance and atmospheric light values.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions AOD-Net, which is a method for dehazing images, not a dataset. No specific datasets are mentioned.",
      "processing_time": 64.76462650299072,
      "citing_paper_id": "267499701",
      "cited_paper_id": 30151664
    },
    {
      "context_text": "Usually, Retinex theory is used as the basic physical model to guide learning methods (such as RetinexNet [49], KinD+ [21], and CSDNet[15]).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets. It only refers to models and methods used for low-light image enhancement.",
      "processing_time": 63.921900510787964,
      "citing_paper_id": "267499701",
      "cited_paper_id": 52008443
    },
    {
      "context_text": "Usually, Retinex theory is used as the basic physical model to guide learning methods (such as RetinexNet [49], KinD+ [21], and CSDNet[15]).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets. It only refers to models and methods used for low-light image enhancement.",
      "processing_time": 63.921900510787964,
      "citing_paper_id": "267499701",
      "cited_paper_id": 233471949
    },
    {
      "context_text": "Traditional methods mainly include histogram equalization (HE)- [40], dehazing-[7] and Retinex-based [5] methods, etc. HE-based methods [40, 41] accomplish image enhancement by uniformly distributing the histogram of the image’s pixel intensity or color distribution.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific, verifiable datasets. It only refers to traditional methods and techniques such as histogram equalization, dehazing, and Retinex-based methods.",
      "processing_time": 66.38451671600342,
      "citing_paper_id": "267499701",
      "cited_paper_id": 62771950
    },
    {
      "context_text": "However, since the DCP-based methods are not fully applicable in the bright areas of the image (such as the sky and water surface), the image after dehazing is visually locally distorted [26].",
      "catation_intent": "limitation",
      "resource_type": "limitation",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a limitation of DCP-based methods in image dehazing.",
      "processing_time": 63.942726373672485,
      "citing_paper_id": "267499701",
      "cited_paper_id": 67751636
    },
    {
      "context_text": "The hybrid deep network proposed by Ren et al. [46] incorporates gradient features to improve the network’s extraction of edge features covered by darkness.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (hybrid deep network) and its purpose (improving edge feature extraction in low-light images).",
      "processing_time": 66.02984571456909,
      "citing_paper_id": "267499701",
      "cited_paper_id": 122575721
    },
    {
      "context_text": "Learning-based methods have been successfully applied to low-light enhancement tasks in recent years and are mainly researched from two aspects: end-to-end [46, 47, 48] and physical model-guided learning.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only research approaches. No dataset names are present in the text.",
      "processing_time": 63.920650005340576,
      "citing_paper_id": "267499701",
      "cited_paper_id": 122575721
    },
    {
      "context_text": "Learning-based methods have been successfully applied to low-light enhancement tasks in recent years and are mainly researched from two aspects: end-to-end [46, 47, 48] and physical model-guided learning.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only research approaches. No dataset names are present in the text.",
      "processing_time": 63.920650005340576,
      "citing_paper_id": "267499701",
      "cited_paper_id": 247689711
    },
    {
      "context_text": "Learning-based methods have been successfully applied to low-light enhancement tasks in recent years and are mainly researched from two aspects: end-to-end [46, 47, 48] and physical model-guided learning.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only research approaches. No dataset names are present in the text.",
      "processing_time": 63.920650005340576,
      "citing_paper_id": "267499701",
      "cited_paper_id": 258587991
    },
    {
      "context_text": "End-to-end methods (such as FFANet [27] and FSADNet [23]) directly model the mapping from haze to haze-free images, reducing the need for manual feature extraction and focusing on the training data itself.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions 'training data' but does not specify a named dataset. The cited papers are models, not datasets.",
      "processing_time": 63.92070984840393,
      "citing_paper_id": "267499701",
      "cited_paper_id": 208138077
    },
    {
      "context_text": "End-to-end methods (such as FFANet [27] and FSADNet [23]) directly model the mapping from haze to haze-free images, reducing the need for manual feature extraction and focusing on the training data itself.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions 'training data' but does not specify a named dataset. The cited papers are models, not datasets.",
      "processing_time": 63.92070984840393,
      "citing_paper_id": "267499701",
      "cited_paper_id": 246650660
    },
    {
      "context_text": "As shown in Table 1, the training [8] ICIP (2014) CBF [50] TIP (2017) SDD [58] TMM (2020) CCDID [59] TCSVT (2021) Kind+ [21] IJCV (2021) Ako [60] MTA (2022) ACDC [11] JOE (2022) CEEF [12] TMM (2022) ROP+ [1] TPAMI (2023) PCDE [61] SPL (2023) SMNet [62] TMM (2023) TOENet [37] TIM (2023) AoSRNet —…",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not provide specific dataset names, only references to papers and their publication venues. No clear, verifiable datasets are mentioned.",
      "processing_time": 64.74032044410706,
      "citing_paper_id": "267499701",
      "cited_paper_id": 213209603
    },
    {
      "context_text": "In addition, to better compensate for the missing channel information, literature [35] and [36] map the image to the LAB space.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions mapping images to the LAB space but does not specify a dataset. No verifiable dataset names are present in the citation span.",
      "processing_time": 64.85058617591858,
      "citing_paper_id": "267499701",
      "cited_paper_id": 213782253
    },
    {
      "context_text": "In addition, to better compensate for the missing channel information, literature [35] and [36] map the image to the LAB space.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions mapping images to the LAB space but does not specify a dataset. No verifiable dataset names are present in the citation span.",
      "processing_time": 64.85058617591858,
      "citing_paper_id": "267499701",
      "cited_paper_id": 247179672
    },
    {
      "context_text": "Gamma correction (GC) [10] and linear stretching (LS) [6] are usually used to optimize the luminance and contrast of the image restored by the DCP-or Retinex-based methods [11, 12] so that the restored image has a more natural visual performance.",
      "catation_intent": "none",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods for image restoration. No dataset names are present in the text.",
      "processing_time": 64.65939950942993,
      "citing_paper_id": "267499701",
      "cited_paper_id": 219050837
    },
    {
      "context_text": "The blue channel suffers from severe information loss [30].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for enhancing sand-dust images using blue channel compensation.",
      "processing_time": 64.11391043663025,
      "citing_paper_id": "267499701",
      "cited_paper_id": 226850925
    },
    {
      "context_text": "Some learning-based methods have achieved excellent performance in the field of image restoration, such as convolutional neural networks (CNN) [13, 14], generative adversarial networks (GAN) [15, 16], Transformer [17, 18], and denoising diffusion probabilistic models (DDPM) [19].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various machine learning models and methods used in image restoration. No verifiable resources are identified.",
      "processing_time": 65.22802305221558,
      "citing_paper_id": "267499701",
      "cited_paper_id": 233471949
    },
    {
      "context_text": "Some learning-based methods have achieved excellent performance in the field of image restoration, such as convolutional neural networks (CNN) [13, 14], generative adversarial networks (GAN) [15, 16], Transformer [17, 18], and denoising diffusion probabilistic models (DDPM) [19].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various machine learning models and methods used in image restoration. No verifiable resources are identified.",
      "processing_time": 65.22802305221558,
      "citing_paper_id": "267499701",
      "cited_paper_id": 259051233
    },
    {
      "context_text": "Some learning-based methods have achieved excellent performance in the field of image restoration, such as convolutional neural networks (CNN) [13, 14], generative adversarial networks (GAN) [15, 16], Transformer [17, 18], and denoising diffusion probabilistic models (DDPM) [19].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various machine learning models and methods used in image restoration. No verifiable resources are identified.",
      "processing_time": 65.22802305221558,
      "citing_paper_id": "267499701",
      "cited_paper_id": 266180641
    },
    {
      "context_text": "Image dehazing methods are generally classified into physical model-[4, 6, 1] and learning-based [20, 22, 23, 24, 25].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods and classifications of image dehazing techniques.",
      "processing_time": 63.66050577163696,
      "citing_paper_id": "267499701",
      "cited_paper_id": 246650660
    },
    {
      "context_text": "Image dehazing methods are generally classified into physical model-[4, 6, 1] and learning-based [20, 22, 23, 24, 25].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods and classifications of image dehazing techniques.",
      "processing_time": 63.66050577163696,
      "citing_paper_id": "267499701",
      "cited_paper_id": 248069347
    },
    {
      "context_text": "Image dehazing methods are generally classified into physical model-[4, 6, 1] and learning-based [20, 22, 23, 24, 25].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods and classifications of image dehazing techniques.",
      "processing_time": 63.66050577163696,
      "citing_paper_id": "267499701",
      "cited_paper_id": 258179535
    },
    {
      "context_text": "Limited by paired sand/sand-free image datasets, the learning method is less researched than the traditional method, but it is still mainly driven by end-to-end [37] and physical prior models [38, 39].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.3,
      "reasoning": "The context mentions 'paired sand/sand-free image datasets' but does not provide specific names. The cited papers do not clarify the dataset names either.",
      "processing_time": 65.58630919456482,
      "citing_paper_id": "267499701",
      "cited_paper_id": 251281305
    },
    {
      "context_text": "Limited by paired sand/sand-free image datasets, the learning method is less researched than the traditional method, but it is still mainly driven by end-to-end [37] and physical prior models [38, 39].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.3,
      "reasoning": "The context mentions 'paired sand/sand-free image datasets' but does not provide specific names. The cited papers do not clarify the dataset names either.",
      "processing_time": 65.58630919456482,
      "citing_paper_id": "267499701",
      "cited_paper_id": 251704647
    },
    {
      "context_text": "Limited by paired sand/sand-free image datasets, the learning method is less researched than the traditional method, but it is still mainly driven by end-to-end [37] and physical prior models [38, 39].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.3,
      "reasoning": "The context mentions 'paired sand/sand-free image datasets' but does not provide specific names. The cited papers do not clarify the dataset names either.",
      "processing_time": 65.58630919456482,
      "citing_paper_id": "267499701",
      "cited_paper_id": 260920447
    },
    {
      "context_text": "Ding et al. [39] combined the GAN and DCP to improve the visibility of sand dust images through unsupervised learning.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method combining GAN and DCP for image restoration.",
      "processing_time": 64.30544781684875,
      "citing_paper_id": "267499701",
      "cited_paper_id": 251704647
    },
    {
      "context_text": "Physical model-guided methods can embed the atmospheric scattering model into the network so that the imaging model is still followed in the restoration process of the degraded image, which can reduce the risk of overfitting [28].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method or approach. There are no clear identifiers for datasets in the provided context.",
      "processing_time": 64.96030807495117,
      "citing_paper_id": "267499701",
      "cited_paper_id": 254737528
    },
    {
      "context_text": "Learning-based multi-scene restoration methods have been proposed successively, such as TOENet (for haze and sand dust) [37], LYSNet (for haze and low-light) [51] and DIA (for haze and low-light) [52].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions specific methods (TOENet, LYSNet, DIA) but does not refer to any specific datasets. The methods are described for their applications in image restoration under various conditions.",
      "processing_time": 67.27575659751892,
      "citing_paper_id": "267499701",
      "cited_paper_id": 255682697
    },
    {
      "context_text": "Learning-based multi-scene restoration methods have been proposed successively, such as TOENet (for haze and sand dust) [37], LYSNet (for haze and low-light) [51] and DIA (for haze and low-light) [52].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions specific methods (TOENet, LYSNet, DIA) but does not refer to any specific datasets. The methods are described for their applications in image restoration under various conditions.",
      "processing_time": 67.27575659751892,
      "citing_paper_id": "267499701",
      "cited_paper_id": 260920447
    },
    {
      "context_text": "…TMM (2020) CCDID [59] TCSVT (2021) Kind+ [21] IJCV (2021) Ako [60] MTA (2022) ACDC [11] JOE (2022) CEEF [12] TMM (2022) ROP+ [1] TPAMI (2023) PCDE [61] SPL (2023) SMNet [62] TMM (2023) TOENet [37] TIM (2023) AoSRNet — dataset comprises RESIDED-OTS (which incorporates depth information) [56] for…",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "RESIDED-OTS"
      ],
      "dataset_descriptions": {
        "RESIDED-OTS": "Used for underwater image enhancement, incorporating depth information to improve color correction and contrast in images."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions 'RESIDED-OTS' as a dataset that incorporates depth information for underwater image enhancement.",
      "processing_time": 69.87229013442993,
      "citing_paper_id": "267499701",
      "cited_paper_id": 257461627
    },
    {
      "context_text": "…TCSVT (2021) Kind+ [21] IJCV (2021) Ako [60] MTA (2022) ACDC [11] JOE (2022) CEEF [12] TMM (2022) ROP+ [1] TPAMI (2023) PCDE [61] SPL (2023) SMNet [62] TMM (2023) TOENet [37] TIM (2023) AoSRNet — dataset comprises RESIDED-OTS (which incorporates depth information) [56] for land scenes, the…",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "RESIDED-OTS"
      ],
      "dataset_descriptions": {
        "RESIDED-OTS": "Used to train and evaluate image restoration models for land scenes, incorporating depth information to enhance low-light images."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions 'RESIDED-OTS' as a dataset used for land scenes incorporating depth information. This is a specific dataset name that fits the criteria.",
      "processing_time": 72.2011353969574,
      "citing_paper_id": "267499701",
      "cited_paper_id": 257466355
    },
    {
      "context_text": "TOENet [37] reconstructs the features between the three RGB channels through the channel correlation extraction module (CCEM) to restore the degraded image.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (TOENet) and its components. There are no clear identifiers for datasets in the provided context.",
      "processing_time": 66.09507751464844,
      "citing_paper_id": "267499701",
      "cited_paper_id": 260920447
    },
    {
      "context_text": "…IJCV (2021) Ako [60] MTA (2022) ACDC [11] JOE (2022) CEEF [12] TMM (2022) ROP+ [1] TPAMI (2023) PCDE [61] SPL (2023) SMNet [62] TMM (2023) TOENet [37] TIM (2023) AoSRNet — dataset comprises RESIDED-OTS (which incorporates depth information) [56] for land scenes, the Singapore maritime dataset…",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "RESIDED-OTS",
        "Singapore maritime dataset"
      ],
      "dataset_descriptions": {
        "RESIDED-OTS": "Used for enhancing low-visibility land scenes by incorporating depth information, focusing on image restoration techniques.",
        "Singapore maritime dataset": "Applied to enhance visibility in maritime scenes, specifically addressing challenges in low-visibility conditions."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions 'RESIDED-OTS' and 'Singapore maritime dataset' as specific datasets used in the research. These are multi-word proper nouns that fit the criteria for inclusion.",
      "processing_time": 79.04913592338562,
      "citing_paper_id": "267499701",
      "cited_paper_id": 260920447
    },
    {
      "context_text": "Existing approaches [24, 35, 41, 53, 61] generally combine several public image restoration datasets [1, 17, 23, 36, 38, 39, 52, 63] as their training sets, and then evaluate the model on test sets corresponding to specific degradation.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.3,
      "reasoning": "The context mentions 'public image restoration datasets' but does not specify any particular dataset names. The cited papers do not provide additional specific dataset names either.",
      "processing_time": 65.59877133369446,
      "citing_paper_id": "274437500",
      "cited_paper_id": 64193
    },
    {
      "context_text": "Existing approaches [24, 35, 41, 53, 61] generally combine several public image restoration datasets [1, 17, 23, 36, 38, 39, 52, 63] as their training sets, and then evaluate the model on test sets corresponding to specific degradation.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.3,
      "reasoning": "The context mentions 'public image restoration datasets' but does not specify any particular dataset names. The cited papers do not provide additional specific dataset names either.",
      "processing_time": 65.59877133369446,
      "citing_paper_id": "274437500",
      "cited_paper_id": 8671030
    },
    {
      "context_text": "Existing approaches [24, 35, 41, 53, 61] generally combine several public image restoration datasets [1, 17, 23, 36, 38, 39, 52, 63] as their training sets, and then evaluate the model on test sets corresponding to specific degradation.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.3,
      "reasoning": "The context mentions 'public image restoration datasets' but does not specify any particular dataset names. The cited papers do not provide additional specific dataset names either.",
      "processing_time": 65.59877133369446,
      "citing_paper_id": "274437500",
      "cited_paper_id": 39760169
    },
    {
      "context_text": "Existing approaches [24, 35, 41, 53, 61] generally combine several public image restoration datasets [1, 17, 23, 36, 38, 39, 52, 63] as their training sets, and then evaluate the model on test sets corresponding to specific degradation.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.3,
      "reasoning": "The context mentions 'public image restoration datasets' but does not specify any particular dataset names. The cited papers do not provide additional specific dataset names either.",
      "processing_time": 65.59877133369446,
      "citing_paper_id": "274437500",
      "cited_paper_id": 246634936
    },
    {
      "context_text": "Existing approaches [24, 35, 41, 53, 61] generally combine several public image restoration datasets [1, 17, 23, 36, 38, 39, 52, 63] as their training sets, and then evaluate the model on test sets corresponding to specific degradation.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.3,
      "reasoning": "The context mentions 'public image restoration datasets' but does not specify any particular dataset names. The cited papers do not provide additional specific dataset names either.",
      "processing_time": 65.59877133369446,
      "citing_paper_id": "274437500",
      "cited_paper_id": 259243623
    },
    {
      "context_text": "For example, AirNet [24] and Promp-tIR [41] use a combined training set with different degradations for three image restoration tasks ( e.g ., BSD400 [36] and Urban100 [16] for denoising, Rain100L [17] for de-raining, and RESIDE [23] for dehazing).",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "BSD400",
        "Urban100",
        "Rain100L",
        "RESIDE"
      ],
      "dataset_descriptions": {
        "BSD400": "Used for denoising tasks, providing a diverse set of natural images with ground truth annotations for evaluating image restoration models.",
        "Urban100": "Used for denoising tasks, offering high-resolution urban scenes to assess the performance of image restoration techniques.",
        "Rain100L": "Used for de-raining tasks, containing synthetic rain images paired with clean versions to train and evaluate de-raining algorithms.",
        "RESIDE": "Used for dehazing tasks, providing a comprehensive dataset of hazy images and their corresponding clear images to benchmark dehazing methods."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions specific datasets used for training and evaluation in image restoration tasks, which are directly relevant to the research topic.",
      "processing_time": 85.51079964637756,
      "citing_paper_id": "274437500",
      "cited_paper_id": 64193
    },
    {
      "context_text": "For example, AirNet [24] and Promp-tIR [41] use a combined training set with different degradations for three image restoration tasks ( e.g ., BSD400 [36] and Urban100 [16] for denoising, Rain100L [17] for de-raining, and RESIDE [23] for dehazing).",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "BSD400",
        "Urban100",
        "Rain100L",
        "RESIDE"
      ],
      "dataset_descriptions": {
        "BSD400": "Used for denoising tasks, providing a diverse set of natural images with ground truth annotations for evaluating image restoration models.",
        "Urban100": "Used for denoising tasks, offering high-resolution urban scenes to assess the performance of image restoration techniques.",
        "Rain100L": "Used for de-raining tasks, containing synthetic rain images paired with clean versions to train and evaluate de-raining algorithms.",
        "RESIDE": "Used for dehazing tasks, providing a comprehensive dataset of hazy images and their corresponding clear images to benchmark dehazing methods."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions specific datasets used for training and evaluation in image restoration tasks, which are directly relevant to the research topic.",
      "processing_time": 85.51079964637756,
      "citing_paper_id": "274437500",
      "cited_paper_id": 8282555
    },
    {
      "context_text": "For example, AirNet [24] and Promp-tIR [41] use a combined training set with different degradations for three image restoration tasks ( e.g ., BSD400 [36] and Urban100 [16] for denoising, Rain100L [17] for de-raining, and RESIDE [23] for dehazing).",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "BSD400",
        "Urban100",
        "Rain100L",
        "RESIDE"
      ],
      "dataset_descriptions": {
        "BSD400": "Used for denoising tasks, providing a diverse set of natural images with ground truth annotations for evaluating image restoration models.",
        "Urban100": "Used for denoising tasks, offering high-resolution urban scenes to assess the performance of image restoration techniques.",
        "Rain100L": "Used for de-raining tasks, containing synthetic rain images paired with clean versions to train and evaluate de-raining algorithms.",
        "RESIDE": "Used for dehazing tasks, providing a comprehensive dataset of hazy images and their corresponding clear images to benchmark dehazing methods."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions specific datasets used for training and evaluation in image restoration tasks, which are directly relevant to the research topic.",
      "processing_time": 85.51079964637756,
      "citing_paper_id": "274437500",
      "cited_paper_id": 39760169
    },
    {
      "context_text": "However, these approaches [24, 35, 41, 53, 61] simply combine several public synthetic datasets [1, 17, 23, 27, 36, 38, 39, 52, 63] as their corresponding all-in-one training sets.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.2,
      "reasoning": "The context mentions 'public synthetic datasets' but does not provide specific names. The cited papers do not clarify the names of the datasets used.",
      "processing_time": 65.26184105873108,
      "citing_paper_id": "274437500",
      "cited_paper_id": 64193
    },
    {
      "context_text": "However, these approaches [24, 35, 41, 53, 61] simply combine several public synthetic datasets [1, 17, 23, 27, 36, 38, 39, 52, 63] as their corresponding all-in-one training sets.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.2,
      "reasoning": "The context mentions 'public synthetic datasets' but does not provide specific names. The cited papers do not clarify the names of the datasets used.",
      "processing_time": 65.26184105873108,
      "citing_paper_id": "274437500",
      "cited_paper_id": 8671030
    },
    {
      "context_text": "However, these approaches [24, 35, 41, 53, 61] simply combine several public synthetic datasets [1, 17, 23, 27, 36, 38, 39, 52, 63] as their corresponding all-in-one training sets.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.2,
      "reasoning": "The context mentions 'public synthetic datasets' but does not provide specific names. The cited papers do not clarify the names of the datasets used.",
      "processing_time": 65.26184105873108,
      "citing_paper_id": "274437500",
      "cited_paper_id": 39760169
    },
    {
      "context_text": "However, these approaches [24, 35, 41, 53, 61] simply combine several public synthetic datasets [1, 17, 23, 27, 36, 38, 39, 52, 63] as their corresponding all-in-one training sets.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.2,
      "reasoning": "The context mentions 'public synthetic datasets' but does not provide specific names. The cited papers do not clarify the names of the datasets used.",
      "processing_time": 65.26184105873108,
      "citing_paper_id": "274437500",
      "cited_paper_id": 246634936
    },
    {
      "context_text": "However, these approaches [24, 35, 41, 53, 61] simply combine several public synthetic datasets [1, 17, 23, 27, 36, 38, 39, 52, 63] as their corresponding all-in-one training sets.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.2,
      "reasoning": "The context mentions 'public synthetic datasets' but does not provide specific names. The cited papers do not clarify the names of the datasets used.",
      "processing_time": 65.26184105873108,
      "citing_paper_id": "274437500",
      "cited_paper_id": 259243623
    },
    {
      "context_text": "However, these approaches [24, 35, 41, 53, 61] simply combine several public synthetic datasets [1, 17, 23, 27, 36, 38, 39, 52, 63] as their corresponding all-in-one training sets.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.2,
      "reasoning": "The context mentions 'public synthetic datasets' but does not provide specific names. The cited papers do not clarify the names of the datasets used.",
      "processing_time": 65.26184105873108,
      "citing_paper_id": "274437500",
      "cited_paper_id": 260843368
    },
    {
      "context_text": "Incremental learning, as a basic machine learning paradigm, has been widely applied to various computer vision tasks [7, 40, 45]",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general application of incremental learning to computer vision tasks.",
      "processing_time": 63.80215573310852,
      "citing_paper_id": "274437500",
      "cited_paper_id": 2658359
    },
    {
      "context_text": "The degradation caused by rain in real-world scenarios typically involves rain streaks in the air and rain-drops attached to the camera lens or windshields [42, 43].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only the phenomenon of rain degradation in images.",
      "processing_time": 62.41731595993042,
      "citing_paper_id": "274437500",
      "cited_paper_id": 4539586
    },
    {
      "context_text": "We utilize the Adam [19] optimizer for 2 × 10 6 iterations training.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions the Adam optimizer, which is a method, not a dataset. No datasets are mentioned.",
      "processing_time": 63.83190631866455,
      "citing_paper_id": "274437500",
      "cited_paper_id": 6628106
    },
    {
      "context_text": "We also collect a total of 120,000 LQ-GT image pairs from public real-world datasets [23, 38, 52, 54, 59] to train our model, using it as the baseline result.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.3,
      "reasoning": "The context mentions 'public real-world datasets' but does not specify any particular dataset names. The cited papers do not provide additional specific dataset names either.",
      "processing_time": 66.00347113609314,
      "citing_paper_id": "274437500",
      "cited_paper_id": 8671030
    },
    {
      "context_text": "We also collect a total of 120,000 LQ-GT image pairs from public real-world datasets [23, 38, 52, 54, 59] to train our model, using it as the baseline result.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.3,
      "reasoning": "The context mentions 'public real-world datasets' but does not specify any particular dataset names. The cited papers do not provide additional specific dataset names either.",
      "processing_time": 66.00347113609314,
      "citing_paper_id": "274437500",
      "cited_paper_id": 39760169
    },
    {
      "context_text": "However, as the scale of training data increases sharply, the model tends to forget previous knowledge and degrade performance on the early task sequences, making model optimization more difficult [62].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general issue with increasing training data scales.",
      "processing_time": 63.51701021194458,
      "citing_paper_id": "274437500",
      "cited_paper_id": 235719471
    },
    {
      "context_text": "However, as the scale of training data from different distributions increases, these methods are prone to the common issue of catastrophic forgetting [22, 62] in machine learning, potentially weakening the model’s performance.",
      "catation_intent": "research work",
      "resource_type": "limitation",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general issue related to training data and catastrophic forgetting.",
      "processing_time": 64.27362108230591,
      "citing_paper_id": "274437500",
      "cited_paper_id": 235719471
    },
    {
      "context_text": "We compare our proposed FoundIR with 16 recent general / universal image restoration methods, including Real-ESRGAN [51], AirNet [24], DGUNet [37], Restormer [57], [41]) and diffusion-based models ( e.g ., AutoDIR [18]), our FoundIR restores better structures and details for both isolated and…",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models and methods. The context focuses on comparing the proposed method with other image restoration methods.",
      "processing_time": 65.29587769508362,
      "citing_paper_id": "274437500",
      "cited_paper_id": 236171006
    },
    {
      "context_text": "The foundation model [5] is an emerging paradigm that denotes a model trained on broad data that is capable of being adapted to a wide range of tasks.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general concept of foundation models. There are no verifiable resources or datasets mentioned.",
      "processing_time": 65.13526439666748,
      "citing_paper_id": "274437500",
      "cited_paper_id": 237091588
    },
    {
      "context_text": "Motivated by ensemble learning in large language models (LLM) [29, 46, 56], we incorporate one generalist model and multiple specialist models into our FoundIR to form an ensemble framework, which enables better generalization across a broader scope of real-world scenarios while achieving higher…",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models and methods. The context is about incorporating models into an ensemble framework for better generalization.",
      "processing_time": 65.5063693523407,
      "citing_paper_id": "274437500",
      "cited_paper_id": 244130382
    },
    {
      "context_text": "By continuously adding existing real-world datasets [43, 54, 63] to the training set, restoration performance can be significantly improved in real-world scenarios.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.3,
      "reasoning": "The context mentions 'real-world datasets' but does not specify any particular dataset names. The cited paper titles do not provide additional information to disambiguate.",
      "processing_time": 65.50235557556152,
      "citing_paper_id": "274437500",
      "cited_paper_id": 246634936
    },
    {
      "context_text": "By this way, a well-trained model on isolated degradation class can provide a solid starting point for training on coupled degradation class [31, 32].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a methodological approach. No dataset names are present in the citation span.",
      "processing_time": 64.9675190448761,
      "citing_paper_id": "274437500",
      "cited_paper_id": 247411392
    },
    {
      "context_text": "Recently, NLP and high-level CV foundation models, such as GPT-4 [2], CLIP [44], SAM [20], have achieved remarkable generalization capabilities across a broader scope of scenes.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only models and their general capabilities. There are no verifiable resources that meet the criteria.",
      "processing_time": 65.49756717681885,
      "citing_paper_id": "274437500",
      "cited_paper_id": 257532815
    },
    {
      "context_text": "PromptIR [41] and ProRes [35] are proposed to leverage the learnable prompts from the input to guide their networks.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods (PromptIR and ProRes).",
      "processing_time": 63.80417060852051,
      "citing_paper_id": "274437500",
      "cited_paper_id": 259243623
    },
    {
      "context_text": "We note that existing universal restoration methods [24, 35, 41, 61] often construct training batches by simply combining all data or selecting mini-batches from different degradation types for training.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only general practices in constructing training batches for universal restoration methods.",
      "processing_time": 64.16779851913452,
      "citing_paper_id": "274437500",
      "cited_paper_id": 259243623
    },
    {
      "context_text": "Inspired by the residual diffsion process I t = I t − 1 + α t ( I LQ − I HQ ) + δ t ϵ t − 1 in [30, 61], we introduce additional constraints on the input degraded images I LQ , as an explicit condition in the forward process, replacing the traditional representation of I t = ϵ in denoising…",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method or model. The context focuses on the mathematical formulation and process of residual diffusion.",
      "processing_time": 65.33880615234375,
      "citing_paper_id": "274437500",
      "cited_paper_id": 261244079
    },
    {
      "context_text": "Diffusion-based methods [30, 53, 61] are developed to learn the data distributions from various degradations.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods. No dataset names are present in the citation span.",
      "processing_time": 64.15739488601685,
      "citing_paper_id": "274437500",
      "cited_paper_id": 261244079
    },
    {
      "context_text": "Recently, researchers integrate pre-trained large-scale vision models [34, 55] and multimodal large language models [3, 8, 55] 2.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.0,
      "reasoning": "The citation span does not mention any specific datasets, only models and methods. The context is about integrating pre-trained models for image restoration, which does not provide enough information to identify a specific dataset.",
      "processing_time": 66.72994875907898,
      "citing_paper_id": "274437500",
      "cited_paper_id": 267199774
    },
    {
      "context_text": "Although these methods can offer better generality compared to task-specific image restoration approaches [9, 21, 50], they require separate training for different restoration tasks, which is resource-intensive.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only discusses the resource intensity of training different restoration tasks.",
      "processing_time": 63.7394962310791,
      "citing_paper_id": "274437500",
      "cited_paper_id": 268531408
    },
    {
      "context_text": "Compared to task-specific image restoration [9, 21, 50], universal image restoration poses greater challenges due to the distinct and even mutually exclusive nature of different image degradation factors [6, 14, 22].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general concepts and challenges in image restoration. No verifiable resources are identified.",
      "processing_time": 64.58293128013611,
      "citing_paper_id": "274437500",
      "cited_paper_id": 268531408
    },
    {
      "context_text": "Compared to task-specific image restoration [9, 21, 50], universal image restoration poses greater challenges due to the distinct and even mutually exclusive nature of different image degradation factors [6, 14, 22].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general concepts and challenges in image restoration. No verifiable resources are identified.",
      "processing_time": 64.58293128013611,
      "citing_paper_id": "274437500",
      "cited_paper_id": 271039782
    },
    {
      "context_text": "Recent studies [6, 47] have demonstrated that a generalist model typically underperforms compared to a specialist model on certain tasks, which can be attributed to task interference.",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general observation about model performance.",
      "processing_time": 62.891889810562134,
      "citing_paper_id": "274437500",
      "cited_paper_id": 271244778
    },
    {
      "context_text": "For grayscale image denoising, we evaluate our method on Set12 [118], BSD68 [66], and Ur-ban100 [40].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Set12",
        "BSD68",
        "Urban100"
      ],
      "dataset_descriptions": {
        "Set12": "Used to evaluate the grayscale image denoising method, focusing on performance metrics and visual quality.",
        "BSD68": "Used to evaluate the grayscale image denoising method, focusing on performance metrics and visual quality.",
        "Urban100": "Used to evaluate the grayscale image denoising method, focusing on performance metrics and visual quality."
      },
      "confidence_score": 1.0,
      "reasoning": "The citation mentions specific datasets used for evaluating the denoising method, which are relevant to the topic of image restoration.",
      "processing_time": 80.33516788482666,
      "citing_paper_id": "266690738",
      "cited_paper_id": 996788
    },
    {
      "context_text": "Most existing meth-ods design deep networks for specific restoration tasks, including image denoising [57, 114, 118, 120, 122], deblur-ring [14, 15, 19, 95, 113], deraining [17, 95, 100, 113], de-hazing [9, 20, 37, 90, 92], etc .",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various image restoration tasks. No dataset names are provided.",
      "processing_time": 63.55718660354614,
      "citing_paper_id": "266690738",
      "cited_paper_id": 996788
    },
    {
      "context_text": "Most existing meth-ods design deep networks for specific restoration tasks, including image denoising [57, 114, 118, 120, 122], deblur-ring [14, 15, 19, 95, 113], deraining [17, 95, 100, 113], de-hazing [9, 20, 37, 90, 92], etc .",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various image restoration tasks. No dataset names are provided.",
      "processing_time": 63.55718660354614,
      "citing_paper_id": "266690738",
      "cited_paper_id": 221377171
    },
    {
      "context_text": "Most existing meth-ods design deep networks for specific restoration tasks, including image denoising [57, 114, 118, 120, 122], deblur-ring [14, 15, 19, 95, 113], deraining [17, 95, 100, 113], de-hazing [9, 20, 37, 90, 92], etc .",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various image restoration tasks. No dataset names are provided.",
      "processing_time": 63.55718660354614,
      "citing_paper_id": "266690738",
      "cited_paper_id": 231802205
    },
    {
      "context_text": "Most existing meth-ods design deep networks for specific restoration tasks, including image denoising [57, 114, 118, 120, 122], deblur-ring [14, 15, 19, 95, 113], deraining [17, 95, 100, 113], de-hazing [9, 20, 37, 90, 92], etc .",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various image restoration tasks. No dataset names are provided.",
      "processing_time": 63.55718660354614,
      "citing_paper_id": "266690738",
      "cited_paper_id": 235358213
    },
    {
      "context_text": "Most existing meth-ods design deep networks for specific restoration tasks, including image denoising [57, 114, 118, 120, 122], deblur-ring [14, 15, 19, 95, 113], deraining [17, 95, 100, 113], de-hazing [9, 20, 37, 90, 92], etc .",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various image restoration tasks. No dataset names are provided.",
      "processing_time": 63.55718660354614,
      "citing_paper_id": "266690738",
      "cited_paper_id": 237266491
    },
    {
      "context_text": "Most existing meth-ods design deep networks for specific restoration tasks, including image denoising [57, 114, 118, 120, 122], deblur-ring [14, 15, 19, 95, 113], deraining [17, 95, 100, 113], de-hazing [9, 20, 37, 90, 92], etc .",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various image restoration tasks. No dataset names are provided.",
      "processing_time": 63.55718660354614,
      "citing_paper_id": "266690738",
      "cited_paper_id": 244346144
    },
    {
      "context_text": "Most existing meth-ods design deep networks for specific restoration tasks, including image denoising [57, 114, 118, 120, 122], deblur-ring [14, 15, 19, 95, 113], deraining [17, 95, 100, 113], de-hazing [9, 20, 37, 90, 92], etc .",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various image restoration tasks. No dataset names are provided.",
      "processing_time": 63.55718660354614,
      "citing_paper_id": "266690738",
      "cited_paper_id": 245837508
    },
    {
      "context_text": "Most existing meth-ods design deep networks for specific restoration tasks, including image denoising [57, 114, 118, 120, 122], deblur-ring [14, 15, 19, 95, 113], deraining [17, 95, 100, 113], de-hazing [9, 20, 37, 90, 92], etc .",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various image restoration tasks. No dataset names are provided.",
      "processing_time": 63.55718660354614,
      "citing_paper_id": "266690738",
      "cited_paper_id": 248085491
    },
    {
      "context_text": "Most existing meth-ods design deep networks for specific restoration tasks, including image denoising [57, 114, 118, 120, 122], deblur-ring [14, 15, 19, 95, 113], deraining [17, 95, 100, 113], de-hazing [9, 20, 37, 90, 92], etc .",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various image restoration tasks. No dataset names are provided.",
      "processing_time": 63.55718660354614,
      "citing_paper_id": "266690738",
      "cited_paper_id": 249709314
    },
    {
      "context_text": "Starting from some simple convolutional neural networks (CNNs) [25, 118], the introduction of channel-attention [124], spatial-attention [36, 112], non-local operation [59, 125], skip-connection architectures [55, 112] and multi-stage scheme [95, 113] enables image restoration performance…",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various techniques and methods used in image restoration. No verifiable resources are identified.",
      "processing_time": 64.57718324661255,
      "citing_paper_id": "266690738",
      "cited_paper_id": 996788
    },
    {
      "context_text": "Starting from some simple convolutional neural networks (CNNs) [25, 118], the introduction of channel-attention [124], spatial-attention [36, 112], non-local operation [59, 125], skip-connection architectures [55, 112] and multi-stage scheme [95, 113] enables image restoration performance…",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various techniques and methods used in image restoration. No verifiable resources are identified.",
      "processing_time": 64.57718324661255,
      "citing_paper_id": "266690738",
      "cited_paper_id": 49657846
    },
    {
      "context_text": "Starting from some simple convolutional neural networks (CNNs) [25, 118], the introduction of channel-attention [124], spatial-attention [36, 112], non-local operation [59, 125], skip-connection architectures [55, 112] and multi-stage scheme [95, 113] enables image restoration performance…",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various techniques and methods used in image restoration. No verifiable resources are identified.",
      "processing_time": 64.57718324661255,
      "citing_paper_id": "266690738",
      "cited_paper_id": 204955838
    },
    {
      "context_text": "Following [17], we adopt 7 stages for main restoration network, the number of blocks for each stage is [4, 6, 6 ,8, 6, 6, 4], network width is set to 48, the number of heads for each stage is [1, 2, 4, 8, 4, 2, 1].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only network architecture details. The cited paper titles do not provide additional context to identify datasets.",
      "processing_time": 64.57327842712402,
      "citing_paper_id": "266690738",
      "cited_paper_id": 4493958
    },
    {
      "context_text": "Following [17], we adopt 7 stages for main restoration network, the number of blocks for each stage is [4, 6, 6 ,8, 6, 6, 4], network width is set to 48, the number of heads for each stage is [1, 2, 4, 8, 4, 2, 1].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only network architecture details. The cited paper titles do not provide additional context to identify datasets.",
      "processing_time": 64.57327842712402,
      "citing_paper_id": "266690738",
      "cited_paper_id": 206764694
    },
    {
      "context_text": "Following [73], network has 8 stages (the first 7 stages as main network, the last stage as refinement), the number of blocks for each stages is [4, 6, 6, 8, 6, 6, 4, 4], network width is 48, the number of heads for each stages is [1, 2, .",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only network architecture details. The cited paper title suggests a dataset, but it is not mentioned in the citation context.",
      "processing_time": 65.51328349113464,
      "citing_paper_id": "266690738",
      "cited_paper_id": 4493958
    },
    {
      "context_text": "For training, we separately train and evaluate the proposed method on four datasets with synthetic rainstreak degradation, including Rain200L [104], Rain200H [104], DID-Data [115], and DDN-Data [30] Guided-Restoration .",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Rain200L",
        "Rain200H",
        "DID-Data",
        "DDN-Data"
      ],
      "dataset_descriptions": {
        "Rain200L": "Used to train and evaluate the proposed method for synthetic rainstreak degradation, focusing on low-resolution images.",
        "Rain200H": "Used to train and evaluate the proposed method for synthetic rainstreak degradation, focusing on high-resolution images.",
        "DID-Data": "Used to train and evaluate the proposed method for synthetic rainstreak degradation, providing diverse image conditions.",
        "DDN-Data": "Used to train and evaluate the proposed method for synthetic rainstreak degradation, emphasizing realistic rain effects."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions four specific datasets used for training and evaluating the proposed method for image restoration, particularly for synthetic rainstreak degradation.",
      "processing_time": 83.97728848457336,
      "citing_paper_id": "266690738",
      "cited_paper_id": 15443600
    },
    {
      "context_text": "Following [17], we train and evaluate the proposed method separately on Rain200L [104], Rain200H [104], DID-Data [115], and DDN-Data [30] datasets.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Rain200L",
        "Rain200H",
        "DID-Data",
        "DDN-Data"
      ],
      "dataset_descriptions": {
        "Rain200L": "Used to train and evaluate the proposed method for rain removal from low-resolution images, focusing on the effectiveness of the model in handling light rain.",
        "Rain200H": "Used to train and evaluate the proposed method for rain removal from high-resolution images, focusing on the effectiveness of the model in handling heavy rain.",
        "DID-Data": "Used to train and evaluate the proposed method for image denoising, focusing on the model's ability to handle diverse noise levels and types.",
        "DDN-Data": "Used to train and evaluate the model for all-in-one image restoration, focusing on the model's performance in removing various types of degradations."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions specific datasets used for training and evaluating the proposed method in the domain of image restoration, particularly rain removal and denoising.",
      "processing_time": 85.2356367111206,
      "citing_paper_id": "266690738",
      "cited_paper_id": 15443600
    },
    {
      "context_text": "Following [17], we train and evaluate the proposed method separately on Rain200L [104], Rain200H [104], DID-Data [115], and DDN-Data [30] datasets.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Rain200L",
        "Rain200H",
        "DID-Data",
        "DDN-Data"
      ],
      "dataset_descriptions": {
        "Rain200L": "Used to train and evaluate the proposed method for rain removal from low-resolution images, focusing on the effectiveness of the model in handling light rain.",
        "Rain200H": "Used to train and evaluate the proposed method for rain removal from high-resolution images, focusing on the effectiveness of the model in handling heavy rain.",
        "DID-Data": "Used to train and evaluate the proposed method for image denoising, focusing on the model's ability to handle diverse noise levels and types.",
        "DDN-Data": "Used to train and evaluate the model for all-in-one image restoration, focusing on the model's performance in removing various types of degradations."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions specific datasets used for training and evaluating the proposed method in the domain of image restoration, particularly rain removal and denoising.",
      "processing_time": 85.2356367111206,
      "citing_paper_id": "266690738",
      "cited_paper_id": 235702920
    },
    {
      "context_text": "In perspective of training data, we adopt concatenation of 400 images from BSD [6] and 4,744 images from WED [64] dataset as denoising training data, 200 images from Rain100L [104] for deraining task, 72,135 images from SOTS for dehazing task.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "BSD",
        "WED",
        "Rain100L",
        "SOTS"
      ],
      "dataset_descriptions": {
        "BSD": "Used for denoising training, providing 400 images concatenated with WED to form the training set.",
        "WED": "Used for denoising training, providing 4,744 images concatenated with BSD to form the training set.",
        "Rain100L": "Used for deraining task, providing 200 images for training.",
        "SOTS": "Used for dehazing task, providing 72,135 images for training."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions specific datasets used for training various image restoration tasks, including denoising, deraining, and dehazing.",
      "processing_time": 82.60364365577698,
      "citing_paper_id": "266690738",
      "cited_paper_id": 15443600
    },
    {
      "context_text": "In perspective of training data, we adopt concatenation of 400 images from BSD [6] and 4,744 images from WED [64] dataset as denoising training data, 200 images from Rain100L [104] for deraining task, 72,135 images from SOTS for dehazing task.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "BSD",
        "WED",
        "Rain100L",
        "SOTS"
      ],
      "dataset_descriptions": {
        "BSD": "Used for denoising training, providing 400 images concatenated with WED to form the training set.",
        "WED": "Used for denoising training, providing 4,744 images concatenated with BSD to form the training set.",
        "Rain100L": "Used for deraining task, providing 200 images for training.",
        "SOTS": "Used for dehazing task, providing 72,135 images for training."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions specific datasets used for training various image restoration tasks, including denoising, deraining, and dehazing.",
      "processing_time": 82.60364365577698,
      "citing_paper_id": "266690738",
      "cited_paper_id": 206764694
    },
    {
      "context_text": "…some simple convolutional neural networks (CNNs) [25, 118], the introduction of channel-attention [124], spatial-attention [36, 112], non-local operation [59, 125], skip-connection architectures [55, 112] and multi-stage scheme [95, 113] enables image restoration performance continuously improve.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various methods and architectures used in image restoration. No verifiable resources are identified.",
      "processing_time": 64.53120350837708,
      "citing_paper_id": "266690738",
      "cited_paper_id": 49864080
    },
    {
      "context_text": "…some simple convolutional neural networks (CNNs) [25, 118], the introduction of channel-attention [124], spatial-attention [36, 112], non-local operation [59, 125], skip-connection architectures [55, 112] and multi-stage scheme [95, 113] enables image restoration performance continuously improve.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various methods and architectures used in image restoration. No verifiable resources are identified.",
      "processing_time": 64.53120350837708,
      "citing_paper_id": "266690738",
      "cited_paper_id": 85501306
    },
    {
      "context_text": "…some simple convolutional neural networks (CNNs) [25, 118], the introduction of channel-attention [124], spatial-attention [36, 112], non-local operation [59, 125], skip-connection architectures [55, 112] and multi-stage scheme [95, 113] enables image restoration performance continuously improve.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various methods and architectures used in image restoration. No verifiable resources are identified.",
      "processing_time": 64.53120350837708,
      "citing_paper_id": "266690738",
      "cited_paper_id": 231802205
    },
    {
      "context_text": "…some simple convolutional neural networks (CNNs) [25, 118], the introduction of channel-attention [124], spatial-attention [36, 112], non-local operation [59, 125], skip-connection architectures [55, 112] and multi-stage scheme [95, 113] enables image restoration performance continuously improve.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various methods and architectures used in image restoration. No verifiable resources are identified.",
      "processing_time": 64.53120350837708,
      "citing_paper_id": "266690738",
      "cited_paper_id": 235358213
    },
    {
      "context_text": "Following [73], we adopt BSD400 [6] and WED [64] for color image denoising, Rain100L [28] for image deraining, and SOTS [50] for image dehazing.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "BSD400",
        "WED",
        "Rain100L",
        "SOTS"
      ],
      "dataset_descriptions": {
        "BSD400": "Used for color image denoising, focusing on improving image quality by reducing noise while preserving important features.",
        "WED": "Used for color image denoising, providing a diverse set of images to evaluate denoising algorithms.",
        "Rain100L": "Used for image deraining, specifically addressing the challenge of removing rain streaks from images.",
        "SOTS": "Used for image dehazing, aimed at enhancing visibility by removing atmospheric haze from images."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions specific datasets used for different tasks in image restoration, which aligns with the research topic of All-in-One Image Restoration.",
      "processing_time": 82.71906924247742,
      "citing_paper_id": "266690738",
      "cited_paper_id": 195787503
    },
    {
      "context_text": "Following [73], we adopt BSD400 [6] and WED [64] for color image denoising, Rain100L [28] for image deraining, and SOTS [50] for image dehazing.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "BSD400",
        "WED",
        "Rain100L",
        "SOTS"
      ],
      "dataset_descriptions": {
        "BSD400": "Used for color image denoising, focusing on improving image quality by reducing noise while preserving important features.",
        "WED": "Used for color image denoising, providing a diverse set of images to evaluate denoising algorithms.",
        "Rain100L": "Used for image deraining, specifically addressing the challenge of removing rain streaks from images.",
        "SOTS": "Used for image dehazing, aimed at enhancing visibility by removing atmospheric haze from images."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions specific datasets used for different tasks in image restoration, which aligns with the research topic of All-in-One Image Restoration.",
      "processing_time": 82.71906924247742,
      "citing_paper_id": "266690738",
      "cited_paper_id": 206764694
    },
    {
      "context_text": "Following [73], we adopt BSD400 [6] and WED [64] for color image denoising, Rain100L [28] for image deraining, and SOTS [50] for image dehazing.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "BSD400",
        "WED",
        "Rain100L",
        "SOTS"
      ],
      "dataset_descriptions": {
        "BSD400": "Used for color image denoising, focusing on improving image quality by reducing noise while preserving important features.",
        "WED": "Used for color image denoising, providing a diverse set of images to evaluate denoising algorithms.",
        "Rain100L": "Used for image deraining, specifically addressing the challenge of removing rain streaks from images.",
        "SOTS": "Used for image dehazing, aimed at enhancing visibility by removing atmospheric haze from images."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions specific datasets used for different tasks in image restoration, which aligns with the research topic of All-in-One Image Restoration.",
      "processing_time": 82.71906924247742,
      "citing_paper_id": "266690738",
      "cited_paper_id": 259224666
    },
    {
      "context_text": "Following [15, 114], we train the proposed method on GoPro training data and evaluate our method on GoPro [68], HIDE [86], and real-world datasets (RealBlur-R [81] and RealBlur-J [81]).",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "GoPro",
        "HIDE",
        "RealBlur-R",
        "RealBlur-J"
      ],
      "dataset_descriptions": {
        "GoPro": "Used for training the proposed image restoration method, focusing on motion deblurring and high-resolution image restoration.",
        "HIDE": "Used for evaluating the proposed method, focusing on high-resolution image restoration and deblurring performance.",
        "RealBlur-R": "Used for evaluating the proposed method on real-world datasets, focusing on the robustness of the restoration technique.",
        "RealBlur-J": "Used for evaluating the proposed method on real-world datasets, focusing on the robustness of the restoration technique."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions specific datasets used for training and evaluation in the context of image restoration. These datasets are clearly identified and used for evaluating the proposed method.",
      "processing_time": 82.77273964881897,
      "citing_paper_id": "266690738",
      "cited_paper_id": 201624746
    },
    {
      "context_text": "Following [15, 114], we train the proposed method on GoPro training data and evaluate our method on GoPro [68], HIDE [86], and real-world datasets (RealBlur-R [81] and RealBlur-J [81]).",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "GoPro",
        "HIDE",
        "RealBlur-R",
        "RealBlur-J"
      ],
      "dataset_descriptions": {
        "GoPro": "Used for training the proposed image restoration method, focusing on motion deblurring and high-resolution image restoration.",
        "HIDE": "Used for evaluating the proposed method, focusing on high-resolution image restoration and deblurring performance.",
        "RealBlur-R": "Used for evaluating the proposed method on real-world datasets, focusing on the robustness of the restoration technique.",
        "RealBlur-J": "Used for evaluating the proposed method on real-world datasets, focusing on the robustness of the restoration technique."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions specific datasets used for training and evaluation in the context of image restoration. These datasets are clearly identified and used for evaluating the proposed method.",
      "processing_time": 82.77273964881897,
      "citing_paper_id": "266690738",
      "cited_paper_id": 244346144
    },
    {
      "context_text": "Following [15, 114], we train the proposed method on GoPro training data and evaluate our method on GoPro [68], HIDE [86], and real-world datasets (RealBlur-R [81] and RealBlur-J [81]).",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "GoPro",
        "HIDE",
        "RealBlur-R",
        "RealBlur-J"
      ],
      "dataset_descriptions": {
        "GoPro": "Used for training the proposed image restoration method, focusing on motion deblurring and high-resolution image restoration.",
        "HIDE": "Used for evaluating the proposed method, focusing on high-resolution image restoration and deblurring performance.",
        "RealBlur-R": "Used for evaluating the proposed method on real-world datasets, focusing on the robustness of the restoration technique.",
        "RealBlur-J": "Used for evaluating the proposed method on real-world datasets, focusing on the robustness of the restoration technique."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions specific datasets used for training and evaluation in the context of image restoration. These datasets are clearly identified and used for evaluating the proposed method.",
      "processing_time": 82.77273964881897,
      "citing_paper_id": "266690738",
      "cited_paper_id": 248085491
    },
    {
      "context_text": "We adopt GoPro [68] as our training data and directly evaluated the trained model on GoPro validation set, HIDE [86] testing set, and Realblur [81] .",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "GoPro",
        "HIDE",
        "Realblur"
      ],
      "dataset_descriptions": {
        "GoPro": "Used for training and validating the motion deblurring model, focusing on human-aware motion deblurring techniques.",
        "HIDE": "Used for testing the trained model, evaluating its performance on a diverse set of images with various blurring conditions.",
        "Realblur": "Used for testing the trained model, assessing its effectiveness on real-world blurred images."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions specific datasets used for training and evaluation in the context of image restoration. GoPro, HIDE, and Realblur are all datasets used in this field.",
      "processing_time": 79.99955129623413,
      "citing_paper_id": "266690738",
      "cited_paper_id": 201624746
    },
    {
      "context_text": "Initial studies [49, 85, 98] mainly rely on GAN-based architectures, and train a conditional model from given paired image-caption datasets to generate samples.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.3,
      "reasoning": "The context mentions 'paired image-caption datasets' but does not specify any particular dataset names. The cited papers do not provide additional specific dataset names either.",
      "processing_time": 65.00492739677429,
      "citing_paper_id": "266690738",
      "cited_paper_id": 202577442
    },
    {
      "context_text": "Initial studies [49, 85, 98] mainly rely on GAN-based architectures, and train a conditional model from given paired image-caption datasets to generate samples.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.3,
      "reasoning": "The context mentions 'paired image-caption datasets' but does not specify any particular dataset names. The cited papers do not provide additional specific dataset names either.",
      "processing_time": 65.00492739677429,
      "citing_paper_id": "266690738",
      "cited_paper_id": 235702618
    },
    {
      "context_text": "Initial studies [49, 85, 98] mainly rely on GAN-based architectures, and train a conditional model from given paired image-caption datasets to generate samples.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.3,
      "reasoning": "The context mentions 'paired image-caption datasets' but does not specify any particular dataset names. The cited papers do not provide additional specific dataset names either.",
      "processing_time": 65.00492739677429,
      "citing_paper_id": "266690738",
      "cited_paper_id": 256105441
    },
    {
      "context_text": "With the development of diffusion models [39, 89], text conditioned image synthesis has shown remarkable improvement.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only the improvement in text-conditioned image synthesis due to diffusion models. No verifiable resources are identified.",
      "processing_time": 65.00110912322998,
      "citing_paper_id": "266690738",
      "cited_paper_id": 219955663
    },
    {
      "context_text": "With the development of diffusion models [39, 89], text conditioned image synthesis has shown remarkable improvement.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only the improvement in text-conditioned image synthesis due to diffusion models. No verifiable resources are identified.",
      "processing_time": 65.00110912322998,
      "citing_paper_id": "266690738",
      "cited_paper_id": 222140788
    },
    {
      "context_text": "With the emergence of vision transformers [27, 62], the capability of capturing long-range dependencies in the image allows transformer-based methods to achieve better performance, gradually replacing previous CNN-based methods.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only discusses the transition from CNN-based methods to transformer-based methods in image recognition.",
      "processing_time": 63.929863691329956,
      "citing_paper_id": "266690738",
      "cited_paper_id": 225039882
    },
    {
      "context_text": "With the emergence of vision transformers [27, 62], the capability of capturing long-range dependencies in the image allows transformer-based methods to achieve better performance, gradually replacing previous CNN-based methods.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only discusses the transition from CNN-based methods to transformer-based methods in image recognition.",
      "processing_time": 63.929863691329956,
      "citing_paper_id": "266690738",
      "cited_paper_id": 232352874
    },
    {
      "context_text": "Label DMPHN [116] MPRNet [113] Input image DPDNet [2] RDPD [3] Restormer [114] Ours raining task, and +0.5 dB PSNR on average improvement.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only models and methods. There are no clear identifiers for datasets in the given citation span.",
      "processing_time": 64.17962694168091,
      "citing_paper_id": "266690738",
      "cited_paper_id": 231802205
    },
    {
      "context_text": "Label DMPHN [116] MPRNet [113] Input image DPDNet [2] RDPD [3] Restormer [114] Ours raining task, and +0.5 dB PSNR on average improvement.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only models and methods. There are no clear identifiers for datasets in the given citation span.",
      "processing_time": 64.17962694168091,
      "citing_paper_id": "266690738",
      "cited_paper_id": 244346144
    },
    {
      "context_text": "Text-to-image generation models [23, 32, 69, 77, 78, 85, 98] have attracted intensive attention in recent years due to its ability to generate high-quality and diverse images based on given text descriptions.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only text-to-image generation models. No dataset names are present in the text.",
      "processing_time": 64.04909586906433,
      "citing_paper_id": "266690738",
      "cited_paper_id": 232035663
    },
    {
      "context_text": "Text-to-image generation models [23, 32, 69, 77, 78, 85, 98] have attracted intensive attention in recent years due to its ability to generate high-quality and diverse images based on given text descriptions.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only text-to-image generation models. No dataset names are present in the text.",
      "processing_time": 64.04909586906433,
      "citing_paper_id": "266690738",
      "cited_paper_id": 235702618
    },
    {
      "context_text": "Text-to-image generation models [23, 32, 69, 77, 78, 85, 98] have attracted intensive attention in recent years due to its ability to generate high-quality and diverse images based on given text descriptions.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only text-to-image generation models. No dataset names are present in the text.",
      "processing_time": 64.04909586906433,
      "citing_paper_id": "266690738",
      "cited_paper_id": 247628171
    },
    {
      "context_text": "Text-to-image generation models [23, 32, 69, 77, 78, 85, 98] have attracted intensive attention in recent years due to its ability to generate high-quality and diverse images based on given text descriptions.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only text-to-image generation models. No dataset names are present in the text.",
      "processing_time": 64.04909586906433,
      "citing_paper_id": "266690738",
      "cited_paper_id": 248097655
    },
    {
      "context_text": "Text-to-image generation models [23, 32, 69, 77, 78, 85, 98] have attracted intensive attention in recent years due to its ability to generate high-quality and diverse images based on given text descriptions.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only text-to-image generation models. No dataset names are present in the text.",
      "processing_time": 64.04909586906433,
      "citing_paper_id": "266690738",
      "cited_paper_id": 256105441
    },
    {
      "context_text": "Some efforts focus on autoregressive models [11, 22, 32, 77, 107] have also shown exciting results.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to autoregressive models. No dataset names are present in the text.",
      "processing_time": 63.97102212905884,
      "citing_paper_id": "266690738",
      "cited_paper_id": 232035663
    },
    {
      "context_text": "Some efforts focus on autoregressive models [11, 22, 32, 77, 107] have also shown exciting results.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to autoregressive models. No dataset names are present in the text.",
      "processing_time": 63.97102212905884,
      "citing_paper_id": "266690738",
      "cited_paper_id": 235212350
    },
    {
      "context_text": "Some efforts focus on autoregressive models [11, 22, 32, 77, 107] have also shown exciting results.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to autoregressive models. No dataset names are present in the text.",
      "processing_time": 63.97102212905884,
      "citing_paper_id": "266690738",
      "cited_paper_id": 247628171
    },
    {
      "context_text": "Some efforts focus on autoregressive models [11, 22, 32, 77, 107] have also shown exciting results.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to autoregressive models. No dataset names are present in the text.",
      "processing_time": 63.97102212905884,
      "citing_paper_id": "266690738",
      "cited_paper_id": 249926846
    },
    {
      "context_text": "Some efforts focus on autoregressive models [11, 22, 32, 77, 107] have also shown exciting results.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to autoregressive models. No dataset names are present in the text.",
      "processing_time": 63.97102212905884,
      "citing_paper_id": "266690738",
      "cited_paper_id": 255372955
    },
    {
      "context_text": "These models, such as CogView [22] and Muse [11] first learn a discrete code-book through training an autoencoder, and then adopt an autoregressive transformer to predict the tokens sequentially.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context describes the methodology of using autoencoders and autoregressive transformers in text-to-image generation models, but does not mention any specific datasets.",
      "processing_time": 64.59600472450256,
      "citing_paper_id": "266690738",
      "cited_paper_id": 235212350
    },
    {
      "context_text": "These models, such as CogView [22] and Muse [11] first learn a discrete code-book through training an autoencoder, and then adopt an autoregressive transformer to predict the tokens sequentially.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context describes the methodology of using autoencoders and autoregressive transformers in text-to-image generation models, but does not mention any specific datasets.",
      "processing_time": 64.59600472450256,
      "citing_paper_id": "266690738",
      "cited_paper_id": 255372955
    },
    {
      "context_text": "For instance, some works [10, 43, 63] adopt information from the high-resolution reference image to help improve the performance of super-resolution.",
      "catation_intent": "reusable resource",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.2,
      "reasoning": "The context mentions 'high-resolution reference image' but does not specify a named dataset. The cited papers do not provide additional specific dataset names.",
      "processing_time": 64.47438454627991,
      "citing_paper_id": "266690738",
      "cited_paper_id": 235313426
    },
    {
      "context_text": "For instance, some works [10, 43, 63] adopt information from the high-resolution reference image to help improve the performance of super-resolution.",
      "catation_intent": "reusable resource",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.2,
      "reasoning": "The context mentions 'high-resolution reference image' but does not specify a named dataset. The cited papers do not provide additional specific dataset names.",
      "processing_time": 64.47438454627991,
      "citing_paper_id": "266690738",
      "cited_paper_id": 235353006
    },
    {
      "context_text": "First, we perform feature matching [63] between the guidance G and given degraded image X .",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for feature matching. No verifiable resources are identified.",
      "processing_time": 63.283374547958374,
      "citing_paper_id": "266690738",
      "cited_paper_id": 235353006
    },
    {
      "context_text": "Following [63], the dynamic aggregation includes two steps: feature matching and feature aggregation.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method or process. The context is about feature matching and aggregation, which are parts of a method.",
      "processing_time": 64.77450323104858,
      "citing_paper_id": "266690738",
      "cited_paper_id": 235353006
    },
    {
      "context_text": "ADMS [71] exploits FAIG [101] in all-in-one image restoration, by learning specific filters and degradation classifiers, achieving better performance on deraining-denoising-deblurring and deraining-desnowing-dehazing tasks.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and models. The context focuses on the use of FAIG for learning filters and classifiers in image restoration tasks.",
      "processing_time": 64.77321910858154,
      "citing_paper_id": "266690738",
      "cited_paper_id": 236772838
    },
    {
      "context_text": "To balance computational cost, window-based attention [18, 57] and transposed attention [114] are also introduced into image restoration tasks.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only methods and approaches for image restoration tasks.",
      "processing_time": 62.5958731174469,
      "citing_paper_id": "266690738",
      "cited_paper_id": 237266491
    },
    {
      "context_text": "To balance computational cost, window-based attention [18, 57] and transposed attention [114] are also introduced into image restoration tasks.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only methods and approaches for image restoration tasks.",
      "processing_time": 62.5958731174469,
      "citing_paper_id": "266690738",
      "cited_paper_id": 244346144
    },
    {
      "context_text": "To balance computational cost, window-based attention [18, 57] and transposed attention [114] are also introduced into image restoration tasks.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only methods and approaches for image restoration tasks.",
      "processing_time": 62.5958731174469,
      "citing_paper_id": "266690738",
      "cited_paper_id": 248572065
    },
    {
      "context_text": "Both CNN-based and transformer-based restoration networks can be adopted.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only types of neural network architectures.",
      "processing_time": 62.03532552719116,
      "citing_paper_id": "266690738",
      "cited_paper_id": 244346144
    },
    {
      "context_text": "Simply fusing the features of G and X is not enough to improve image restoration.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a general statement about feature fusion in image restoration.",
      "processing_time": 62.47140598297119,
      "citing_paper_id": "266690738",
      "cited_paper_id": 244346144
    },
    {
      "context_text": "Following [114], we train and evaluate the proposed method on DPDD [2] dataset for single-image defocus deblurring and dual-pixel defocus deblur-ring.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "DPDD"
      ],
      "dataset_descriptions": {
        "DPDD": "Used for training and evaluating the proposed method for single-image defocus deblurring and dual-pixel defocus deblurring, focusing on high-resolution image restoration."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions the DPDD dataset, which is used for training and evaluating the proposed method for single-image defocus deblurring and dual-pixel defocus deblurring.",
      "processing_time": 74.26446008682251,
      "citing_paper_id": "266690738",
      "cited_paper_id": 244346144
    },
    {
      "context_text": "After searching, we get a set of guidance features ˆ F g , whose content is aligned spatially with the input content.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only a method for aligning guidance features with input content.",
      "processing_time": 63.05190587043762,
      "citing_paper_id": "266690738",
      "cited_paper_id": 244346144
    },
    {
      "context_text": "Instead, we suggest a dynamic aggregation module to extract and exploit helpful information from G .",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method or module for image restoration.",
      "processing_time": 62.23825144767761,
      "citing_paper_id": "266690738",
      "cited_paper_id": 244346144
    },
    {
      "context_text": "Comparison results in Table 1 show that our method outperforms PromptIR [73] across all the benchmark datasets, achieving +1.05 dB PSNR on dehazing task, +1.21 dB PSNR on de-5 Input image MIMOUnet [19] HINet [14] NAFNet [15] Ours",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions 'benchmark datasets' but does not specify any particular dataset names. The citation is focused on comparing performance metrics rather than detailing the use of specific datasets.",
      "processing_time": 64.54981064796448,
      "citing_paper_id": "266690738",
      "cited_paper_id": 248085491
    },
    {
      "context_text": "Comparison results in Table 1 show that our method outperforms PromptIR [73] across all the benchmark datasets, achieving +1.05 dB PSNR on dehazing task, +1.21 dB PSNR on de-5 Input image MIMOUnet [19] HINet [14] NAFNet [15] Ours",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions 'benchmark datasets' but does not specify any particular dataset names. The citation is focused on comparing performance metrics rather than detailing the use of specific datasets.",
      "processing_time": 64.54981064796448,
      "citing_paper_id": "266690738",
      "cited_paper_id": 259224666
    },
    {
      "context_text": "We evaluate the proposed method on five image restoration tasks, including (1) all-in-one image restoration [73], (2) image deblurring [15, 114] ( i.e ., motion image deblurring and defocus image deblurring), (3) image dehazing [20], (4) image deraining [17], and (5) image denoising [114] ( i.e .,…",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only various image restoration tasks. No clear, verifiable datasets are identified.",
      "processing_time": 63.29415702819824,
      "citing_paper_id": "266690738",
      "cited_paper_id": 248085491
    },
    {
      "context_text": "We evaluate the proposed method on five image restoration tasks, including (1) all-in-one image restoration [73], (2) image deblurring [15, 114] ( i.e ., motion image deblurring and defocus image deblurring), (3) image dehazing [20], (4) image deraining [17], and (5) image denoising [114] ( i.e .,…",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only various image restoration tasks. No clear, verifiable datasets are identified.",
      "processing_time": 63.29415702819824,
      "citing_paper_id": "266690738",
      "cited_paper_id": 259224666
    },
    {
      "context_text": "…image restoration backbone in different image restoration tasks: (1) we adopt PromptIR [73] in all-in-one image restoration, (2) we adopt NAFNet [15] and Restormer [114] in single-image motion deblurring, and Restormer [114] in defocus deblurring, (3) we adopt SFNet [20] in image dehazing, (4)…",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only models and methods used for various image restoration tasks.",
      "processing_time": 62.79382276535034,
      "citing_paper_id": "266690738",
      "cited_paper_id": 248085491
    },
    {
      "context_text": "Performance reported in Table 2 and Table 3 is referred to [15] and [114].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to other papers. The cited paper title 'Simple Baselines for Image Restoration' suggests a method or baseline rather than a dataset.",
      "processing_time": 65.2425184249878,
      "citing_paper_id": "266690738",
      "cited_paper_id": 248085491
    },
    {
      "context_text": "In this section, we list experimental details for different image restoration task ( i.e ., all-in-one image restoration [73], image deblurring [15, 114], image dehazing [20], image deraining [17], and image denoising [114]), the proposed degradation-free guidance generation process ( i.e .,…",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various image restoration tasks. No clear, verifiable datasets are identified.",
      "processing_time": 63.273157596588135,
      "citing_paper_id": "266690738",
      "cited_paper_id": 248085491
    },
    {
      "context_text": "In this section, we list experimental details for different image restoration task ( i.e ., all-in-one image restoration [73], image deblurring [15, 114], image dehazing [20], image deraining [17], and image denoising [114]), the proposed degradation-free guidance generation process ( i.e .,…",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various image restoration tasks. No clear, verifiable datasets are identified.",
      "processing_time": 63.273157596588135,
      "citing_paper_id": "266690738",
      "cited_paper_id": 259224666
    },
    {
      "context_text": "In single-image motion deblurring task, we follow [15], main restoration network has 9 stages, and the number of blocks for each stage is [1, 1, 1, 28, 1, 1, 1, 1, 1], network width is 64.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only network architecture details. No verifiable resources are identified.",
      "processing_time": 63.142725229263306,
      "citing_paper_id": "266690738",
      "cited_paper_id": 248085491
    },
    {
      "context_text": "We adopt NAFNet [15] as our back-bone in single-image motion deblurring, Restormer [114] as backbone in defocus deblurring.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions NAFNet and Restormer as backbones for different image restoration tasks but does not refer to them as datasets. They are models or methods.",
      "processing_time": 64.34040188789368,
      "citing_paper_id": "266690738",
      "cited_paper_id": 248085491
    },
    {
      "context_text": "By training with huge corpora, large diffusion models, such as DALLE-2 [78], Imagen [84], Stable Diffusion [82], and DALLE-3 [8] have demonstrated excellent semantic understanding, and can generate diverse and photo-realistic images.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models and their capabilities. There are no verifiable resources that meet the criteria for inclusion.",
      "processing_time": 63.68352675437927,
      "citing_paper_id": "266690738",
      "cited_paper_id": 248097655
    },
    {
      "context_text": "Recent works [16, 51, 71, 73, 117, 127] expect to explore a unified model for multiple * Correspondence author. degradations.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only a general reference to exploring a unified model for multiple degradations. No verifiable resources are identified.",
      "processing_time": 64.73056244850159,
      "citing_paper_id": "266690738",
      "cited_paper_id": 250551851
    },
    {
      "context_text": "Recent works [16, 51, 71, 73, 117, 127] expect to explore a unified model for multiple * Correspondence author. degradations.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only a general reference to exploring a unified model for multiple degradations. No verifiable resources are identified.",
      "processing_time": 64.73056244850159,
      "citing_paper_id": "266690738",
      "cited_paper_id": 259224666
    },
    {
      "context_text": "Recent works [16, 51, 71, 73, 117, 127] expect to explore a unified model for multiple * Correspondence author. degradations.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only a general reference to exploring a unified model for multiple degradations. No verifiable resources are identified.",
      "processing_time": 64.73056244850159,
      "citing_paper_id": "266690738",
      "cited_paper_id": 260085391
    },
    {
      "context_text": "Promoting by the development of unified model, some works focus on the so-lution to the all-in-one image restoration [16, 51, 71, 73, 117, 127].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to works focusing on all-in-one image restoration. No verifiable resources are identified.",
      "processing_time": 64.12312483787537,
      "citing_paper_id": "266690738",
      "cited_paper_id": 250551851
    },
    {
      "context_text": "Promoting by the development of unified model, some works focus on the so-lution to the all-in-one image restoration [16, 51, 71, 73, 117, 127].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to works focusing on all-in-one image restoration. No verifiable resources are identified.",
      "processing_time": 64.12312483787537,
      "citing_paper_id": "266690738",
      "cited_paper_id": 259224666
    },
    {
      "context_text": "Promoting by the development of unified model, some works focus on the so-lution to the all-in-one image restoration [16, 51, 71, 73, 117, 127].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to works focusing on all-in-one image restoration. No verifiable resources are identified.",
      "processing_time": 64.12312483787537,
      "citing_paper_id": "266690738",
      "cited_paper_id": 260085391
    },
    {
      "context_text": "AirNet [51] develops the unified model for denoising-deraining-dehazing, which utilizes contrastive learning to capture degradation representations of different tasks and adaptively injects the degradation priors into backbone restoration framework for aiding in learning better results.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions AirNet, which is a method/model, not a dataset. No specific datasets are mentioned in the context.",
      "processing_time": 63.57052969932556,
      "citing_paper_id": "266690738",
      "cited_paper_id": 250551851
    },
    {
      "context_text": "Recent works [58, 94, 103] utilize pre-trained generative priors to restore more realistic and natural results.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only the use of pre-trained generative priors. No clear, verifiable dataset names are present.",
      "processing_time": 64.37278985977173,
      "citing_paper_id": "266690738",
      "cited_paper_id": 258615282
    },
    {
      "context_text": "Especially for all-in-one restoration, 0.5 dB PSNR improvement is obtained in comparison with PromptIR [73].",
      "catation_intent": "findings",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a performance improvement over another method.",
      "processing_time": 62.3640239238739,
      "citing_paper_id": "266690738",
      "cited_paper_id": 259224666
    },
    {
      "context_text": "Following Promp-tIR [73], we train and evaluate the proposed method in all-in-one image restoration task, our method outperforms PromptIR across all the benchmark datasets. degradation-free guidance images G from degraded images with one unified model (as shown in Fig.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.3,
      "reasoning": "The context mentions 'benchmark datasets' but does not specify any particular dataset names. The term 'benchmark datasets' is too generic and lacks specific identifiers.",
      "processing_time": 64.46502566337585,
      "citing_paper_id": "266690738",
      "cited_paper_id": 259224666
    },
    {
      "context_text": "Promp-tIR [73] learns to encode and adopt degradation information as prompts for dehazing-deraining-denoising task.",
      "catation_intent": "reusable resource",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions 'degradation information' which is relevant to image restoration tasks, but does not specify any dataset names. The cited paper title confirms the focus on image restoration but does not introduce a specific dataset.",
      "processing_time": 66.63409614562988,
      "citing_paper_id": "266690738",
      "cited_paper_id": 259224666
    },
    {
      "context_text": "Moreover, we employ the state-of-the-art methods as our image restoration backbone in different image restoration tasks: (1) we adopt PromptIR [73] in all-in-one image restoration, (2) we adopt NAFNet [15] and Restormer [114] in single-image motion deblurring, and Restormer [114] in defocus…",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only models and methods used for image restoration tasks.",
      "processing_time": 62.478235483169556,
      "citing_paper_id": "266690738",
      "cited_paper_id": 259224666
    },
    {
      "context_text": "IDR [117] proposes a two-stage training strategy, which first learns separate task-oriented hubs for each degradation.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for image restoration.",
      "processing_time": 62.018837451934814,
      "citing_paper_id": "266690738",
      "cited_paper_id": 260085391
    },
    {
      "context_text": "…For training, we adopt Rain100L [95] dataset for deraining, RESIDE [39] dataset for dehazing, DFBW (DIV2K [2], Flickr2K [50], BSD [61], and WED [60]) datasets for Gaussian color image denoising and compression artifacts removal, GoPro [64] dataset for motion deblurring, LOLv1…",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Rain100L",
        "RESIDE",
        "DIV2K",
        "Flickr2K",
        "BSD",
        "WED",
        "GoPro",
        "LOLv1"
      ],
      "dataset_descriptions": {
        "Rain100L": "Used for deraining, providing synthetic rain images to train and evaluate deraining models.",
        "RESIDE": "Used for dehazing, offering synthetic hazy images to train and evaluate dehazing models.",
        "DIV2K": "Part of DFBW, used for Gaussian color image denoising and compression artifacts removal, providing high-quality images for training.",
        "Flickr2K": "Used for training and evaluation, providing a large set of diverse images for image restoration tasks.",
        "BSD": "Part of DFBW, used for Gaussian color image denoising and compression artifacts removal, offering a variety of natural images.",
        "WED": "Part of DFBW, used for Gaussian color image denoising and compression artifacts removal, providing web images for training.",
        "GoPro": "Used for motion deblurring, providing sharp and blurred image pairs to train and evaluate deblurring models.",
        "LOLv1": "Mentioned but not described in detail, likely used for low-light image enhancement."
      },
      "confidence_score": 0.9,
      "reasoning": "The citation mentions several datasets used for training various image restoration tasks, which align with the research topic of all-in-one image restoration.",
      "processing_time": 85.05195546150208,
      "citing_paper_id": "275134298",
      "cited_paper_id": 64193
    },
    {
      "context_text": "Following [67, 100], We evaluate our method on Rain100L [95] for deraining, SOTS [39] for dehazing, BSD68 [61] for Gaussian color image denoising, GoPro [64] for motion deblur-ring, LOLv1 [86] for low-light enhancement, Snow100K-L [57] for desnowing, and LIVE1 [76] for compression artifacts removal.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Rain100L",
        "SOTS",
        "BSD68",
        "GoPro",
        "LOLv1",
        "Snow100K-L",
        "LIVE1"
      ],
      "dataset_descriptions": {
        "Rain100L": "Used for evaluating deraining performance, focusing on removing rain streaks from images.",
        "SOTS": "Used for evaluating dehazing performance, focusing on improving visibility in hazy images.",
        "BSD68": "Used for evaluating Gaussian color image denoising, focusing on reducing noise in color images.",
        "GoPro": "Used for evaluating motion deblurring performance, focusing on restoring sharpness in blurred images.",
        "LOL": "Used for evaluating the proposed method on low-light enhancement, focusing on improving brightness and clarity in dark images.",
        "Snow100K-L": "Used for evaluating desnowing performance, focusing on removing snow from images.",
        "LIVE1": "Used for evaluating compression artifacts removal, focusing on enhancing image quality after compression."
      },
      "confidence_score": 1.0,
      "reasoning": "The citation explicitly mentions several datasets used for evaluating the method across various image restoration tasks.",
      "processing_time": 82.67372632026672,
      "citing_paper_id": "275134298",
      "cited_paper_id": 64193
    },
    {
      "context_text": "Following [67, 100], We evaluate our method on Rain100L [95] for deraining, SOTS [39] for dehazing, BSD68 [61] for Gaussian color image denoising, GoPro [64] for motion deblur-ring, LOLv1 [86] for low-light enhancement, Snow100K-L [57] for desnowing, and LIVE1 [76] for compression artifacts removal.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Rain100L",
        "SOTS",
        "BSD68",
        "GoPro",
        "LOLv1",
        "Snow100K-L",
        "LIVE1"
      ],
      "dataset_descriptions": {
        "Rain100L": "Used for evaluating deraining performance, focusing on removing rain streaks from images.",
        "SOTS": "Used for evaluating dehazing performance, focusing on improving visibility in hazy images.",
        "BSD68": "Used for evaluating Gaussian color image denoising, focusing on reducing noise in color images.",
        "GoPro": "Used for evaluating motion deblurring performance, focusing on restoring sharpness in blurred images.",
        "LOL": "Used for evaluating the proposed method on low-light enhancement, focusing on improving brightness and clarity in dark images.",
        "Snow100K-L": "Used for evaluating desnowing performance, focusing on removing snow from images.",
        "LIVE1": "Used for evaluating compression artifacts removal, focusing on enhancing image quality after compression."
      },
      "confidence_score": 1.0,
      "reasoning": "The citation explicitly mentions several datasets used for evaluating the method across various image restoration tasks.",
      "processing_time": 82.67372632026672,
      "citing_paper_id": "275134298",
      "cited_paper_id": 39760169
    },
    {
      "context_text": "Following [67, 100], We evaluate our method on Rain100L [95] for deraining, SOTS [39] for dehazing, BSD68 [61] for Gaussian color image denoising, GoPro [64] for motion deblur-ring, LOLv1 [86] for low-light enhancement, Snow100K-L [57] for desnowing, and LIVE1 [76] for compression artifacts removal.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Rain100L",
        "SOTS",
        "BSD68",
        "GoPro",
        "LOLv1",
        "Snow100K-L",
        "LIVE1"
      ],
      "dataset_descriptions": {
        "Rain100L": "Used for evaluating deraining performance, focusing on removing rain streaks from images.",
        "SOTS": "Used for evaluating dehazing performance, focusing on improving visibility in hazy images.",
        "BSD68": "Used for evaluating Gaussian color image denoising, focusing on reducing noise in color images.",
        "GoPro": "Used for evaluating motion deblurring performance, focusing on restoring sharpness in blurred images.",
        "LOL": "Used for evaluating the proposed method on low-light enhancement, focusing on improving brightness and clarity in dark images.",
        "Snow100K-L": "Used for evaluating desnowing performance, focusing on removing snow from images.",
        "LIVE1": "Used for evaluating compression artifacts removal, focusing on enhancing image quality after compression."
      },
      "confidence_score": 1.0,
      "reasoning": "The citation explicitly mentions several datasets used for evaluating the method across various image restoration tasks.",
      "processing_time": 82.67372632026672,
      "citing_paper_id": "275134298",
      "cited_paper_id": null
    },
    {
      "context_text": "To be more efficient, DaAIR [96], Instruc-tIPT [81], and LoRA-IR [4] learn individual LoRA weights or adapters for each task.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and models. The context is about learning individual LoRA weights or adapters for each task, which is not related to datasets.",
      "processing_time": 65.38479375839233,
      "citing_paper_id": "275134298",
      "cited_paper_id": 572361
    },
    {
      "context_text": "To be more efficient, DaAIR [96], Instruc-tIPT [81], and LoRA-IR [4] learn individual LoRA weights or adapters for each task.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and models. The context is about learning individual LoRA weights or adapters for each task, which is not related to datasets.",
      "processing_time": 65.38479375839233,
      "citing_paper_id": "275134298",
      "cited_paper_id": 270045495
    },
    {
      "context_text": "In the past decade, advanced deep neural architectures [28, 58, 70, 83, 108] have driven image restoration methods to evolve from task-specific [21, 34, 72, 79, 101 ?",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to deep neural architectures and task-specific methods. No verifiable resources are identified.",
      "processing_time": 63.83428454399109,
      "citing_paper_id": "275134298",
      "cited_paper_id": 996788
    },
    {
      "context_text": "In the past decade, advanced deep neural architectures [28, 58, 70, 83, 108] have driven image restoration methods to evolve from task-specific [21, 34, 72, 79, 101 ?",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to deep neural architectures and task-specific methods. No verifiable resources are identified.",
      "processing_time": 63.83428454399109,
      "citing_paper_id": "275134298",
      "cited_paper_id": 231591445
    },
    {
      "context_text": "Pioneer works pro-pose tailored frameworks for specific degradations, e.g ., DnCNN [101] for denoising, SRCNN [21] for superres-olution, DeblurGANv2 [34] for deblurring, PReNet [72] for draining, Dehazeformer [79] for dehazing, and DesnowNet [ ?",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span mentions several methods/models but does not refer to any specific datasets. The context is about different models for various image restoration tasks, not datasets.",
      "processing_time": 64.62337470054626,
      "citing_paper_id": "275134298",
      "cited_paper_id": 996788
    },
    {
      "context_text": "To adapt to various corruptions, pioneer all-in-one image restoration methods [5, 40, 67, 82, 90, 100] integrate PEFT-based techniques [10, 29] and feature modulation [102] into the restoration backbone.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods and techniques. The cited papers' titles do not provide additional context to identify datasets.",
      "processing_time": 64.2824444770813,
      "citing_paper_id": "275134298",
      "cited_paper_id": 2141622
    },
    {
      "context_text": "To adapt to various corruptions, pioneer all-in-one image restoration methods [5, 40, 67, 82, 90, 100] integrate PEFT-based techniques [10, 29] and feature modulation [102] into the restoration backbone.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods and techniques. The cited papers' titles do not provide additional context to identify datasets.",
      "processing_time": 64.2824444770813,
      "citing_paper_id": "275134298",
      "cited_paper_id": 59599816
    },
    {
      "context_text": "To adapt to various corruptions, pioneer all-in-one image restoration methods [5, 40, 67, 82, 90, 100] integrate PEFT-based techniques [10, 29] and feature modulation [102] into the restoration backbone.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods and techniques. The cited papers' titles do not provide additional context to identify datasets.",
      "processing_time": 64.2824444770813,
      "citing_paper_id": "275134298",
      "cited_paper_id": 233296808
    },
    {
      "context_text": "To adapt to various corruptions, pioneer all-in-one image restoration methods [5, 40, 67, 82, 90, 100] integrate PEFT-based techniques [10, 29] and feature modulation [102] into the restoration backbone.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods and techniques. The cited papers' titles do not provide additional context to identify datasets.",
      "processing_time": 64.2824444770813,
      "citing_paper_id": "275134298",
      "cited_paper_id": 272724853
    },
    {
      "context_text": "Following [102], we synthesize blurring degradation as, where k is the blur kernel.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for synthesizing blurring degradation.",
      "processing_time": 62.319531202316284,
      "citing_paper_id": "275134298",
      "cited_paper_id": 2141622
    },
    {
      "context_text": "For stable training, and to avoid the result of G g collapse to the finest-grained level, we further adopt load-balance loss as used in [75], where µ and σ are mean and standard deviation of experts load, respectively.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for stable training using a load-balance loss. The cited paper is about a neural network architecture, not a dataset.",
      "processing_time": 65.23205804824829,
      "citing_paper_id": "275134298",
      "cited_paper_id": 12462234
    },
    {
      "context_text": "Mixture-of-Experts [23, 37, 68, 71, 75, 116] is a promising way for scaling up and deploying large or gigantic models due to its competitive computational bounds and latency compared to the vanilla single model.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only methods and models. The cited papers also do not provide any dataset names.",
      "processing_time": 63.52132225036621,
      "citing_paper_id": "275134298",
      "cited_paper_id": 12462234
    },
    {
      "context_text": "Mixture-of-Experts [23, 37, 68, 71, 75, 116] is a promising way for scaling up and deploying large or gigantic models due to its competitive computational bounds and latency compared to the vanilla single model.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only methods and models. The cited papers also do not provide any dataset names.",
      "processing_time": 63.52132225036621,
      "citing_paper_id": "275134298",
      "cited_paper_id": 245986500
    },
    {
      "context_text": "4, we learn experts using data clustered by different DR extractors, including VGG [78], DDR [54], DA-CLIP [59], manually division by publicly released dataset (Manual div.), and our method.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.3,
      "reasoning": "The context mentions 'publicly released dataset' but does not specify a name. No other specific datasets are mentioned.",
      "processing_time": 63.28414487838745,
      "citing_paper_id": "275134298",
      "cited_paper_id": 14124313
    },
    {
      "context_text": "4, we learn experts using data clustered by different DR extractors, including VGG [78], DDR [54], DA-CLIP [59], manually division by publicly released dataset (Manual div.), and our method.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.3,
      "reasoning": "The context mentions 'publicly released dataset' but does not specify a name. No other specific datasets are mentioned.",
      "processing_time": 63.28414487838745,
      "citing_paper_id": "275134298",
      "cited_paper_id": 248986859
    },
    {
      "context_text": "To enlarge degradation space and the number of training data for deraining task, we collect Rain200H [95], Rain200L [95], DID [99], and DDN [24] as training data.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Rain200H",
        "Rain200L",
        "DID",
        "DDN"
      ],
      "dataset_descriptions": {
        "Rain200H": "Used to train a deep detail network for deraining, providing high-quality synthetic rain images to enhance model robustness.",
        "Rain200L": "Used to train a deep detail network for deraining, providing low-quality synthetic rain images to enhance model robustness.",
        "DID": "Used to train a deep detail network for deraining, offering diverse real-world rain images to improve generalization.",
        "DDN": "Used to train and evaluate an all-in-one image restoration model, providing a comprehensive set of deraining examples."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions specific datasets used for training a model for the deraining task. These datasets are clearly named and are relevant to the topic of image restoration.",
      "processing_time": 80.22050642967224,
      "citing_paper_id": "275134298",
      "cited_paper_id": 17115407
    },
    {
      "context_text": "Image restoration is a fundamental task in computer vision, it aims to restore high-quality (HQ) images from corresponding low-quality (LQ) counterparts, including denoising [1, 109], deblurring [64, 73, 110], de-weathering [24, 53, 57, 99], low-light enhancement[86, 92, 111], etc .",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.2,
      "reasoning": "The context mentions various image restoration tasks but does not specify any particular datasets. The cited papers' titles suggest they might contain datasets, but the context itself does not confirm their usage.",
      "processing_time": 65.21135115623474,
      "citing_paper_id": "275134298",
      "cited_paper_id": 17115407
    },
    {
      "context_text": "Image restoration is a fundamental task in computer vision, it aims to restore high-quality (HQ) images from corresponding low-quality (LQ) counterparts, including denoising [1, 109], deblurring [64, 73, 110], de-weathering [24, 53, 57, 99], low-light enhancement[86, 92, 111], etc .",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.2,
      "reasoning": "The context mentions various image restoration tasks but does not specify any particular datasets. The cited papers' titles suggest they might contain datasets, but the context itself does not confirm their usage.",
      "processing_time": 65.21135115623474,
      "citing_paper_id": "275134298",
      "cited_paper_id": 52059988
    },
    {
      "context_text": "Image restoration is a fundamental task in computer vision, it aims to restore high-quality (HQ) images from corresponding low-quality (LQ) counterparts, including denoising [1, 109], deblurring [64, 73, 110], de-weathering [24, 53, 57, 99], low-light enhancement[86, 92, 111], etc .",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.2,
      "reasoning": "The context mentions various image restoration tasks but does not specify any particular datasets. The cited papers' titles suggest they might contain datasets, but the context itself does not confirm their usage.",
      "processing_time": 65.21135115623474,
      "citing_paper_id": "275134298",
      "cited_paper_id": 250520618
    },
    {
      "context_text": "Image restoration is a fundamental task in computer vision, it aims to restore high-quality (HQ) images from corresponding low-quality (LQ) counterparts, including denoising [1, 109], deblurring [64, 73, 110], de-weathering [24, 53, 57, 99], low-light enhancement[86, 92, 111], etc .",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.2,
      "reasoning": "The context mentions various image restoration tasks but does not specify any particular datasets. The cited papers' titles suggest they might contain datasets, but the context itself does not confirm their usage.",
      "processing_time": 65.21135115623474,
      "citing_paper_id": "275134298",
      "cited_paper_id": 253510104
    },
    {
      "context_text": "Image restoration is a fundamental task in computer vision, it aims to restore high-quality (HQ) images from corresponding low-quality (LQ) counterparts, including denoising [1, 109], deblurring [64, 73, 110], de-weathering [24, 53, 57, 99], low-light enhancement[86, 92, 111], etc .",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.2,
      "reasoning": "The context mentions various image restoration tasks but does not specify any particular datasets. The cited papers' titles suggest they might contain datasets, but the context itself does not confirm their usage.",
      "processing_time": 65.21135115623474,
      "citing_paper_id": "275134298",
      "cited_paper_id": 264591546
    },
    {
      "context_text": "Following [39], hazing degradation is synthesized as, where a is the global atmospheric light, T ( y ) is the transition matrix, D ( y ) is the depth map, and β is the scattering coefficient of the atmosphere.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for synthesizing hazing degradation. No dataset names are present in the citation span.",
      "processing_time": 63.54647088050842,
      "citing_paper_id": "275134298",
      "cited_paper_id": 39760169
    },
    {
      "context_text": "…our models on single-degradation and mixed-degradation: (1) Single-degradation: For training, we adopt Rain100L [95] dataset for deraining, RESIDE [39] dataset for dehazing, DFBW (DIV2K [2], Flickr2K [50], BSD [61], and WED [60]) datasets for Gaussian color image denoising and compression…",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Rain100L",
        "RESIDE",
        "DIV2K",
        "Flickr2K",
        "BSD",
        "WED"
      ],
      "dataset_descriptions": {
        "Rain100L": "Used for training deraining models, focusing on removing rain streaks from images using synthetic rain data.",
        "RESIDE": "Used for training dehazing models, focusing on improving visibility in hazy images using synthetic and real-world hazy images.",
        "DIV2K": "Part of DFBW, used for Gaussian color image denoising and compression, providing high-quality images for training.",
        "Flickr2K": "Used to train models for Gaussian color image denoising and compression, providing a diverse set of images for robustness.",
        "BSD": "Part of DFBW, used for Gaussian color image denoising and compression, providing a benchmark for evaluating restoration performance.",
        "WED": "Part of DFBW, used for Gaussian color image denoising and compression, focusing on web images with various distortions."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions specific datasets used for training models for different image restoration tasks, which are directly relevant to the research topic of all-in-one image restoration.",
      "processing_time": 82.91798758506775,
      "citing_paper_id": "275134298",
      "cited_paper_id": 39760169
    },
    {
      "context_text": "Inspired by parameter-efficient fine-tuning (PEFT) meth-ods [10, 29], learnable prompts [26, 40, 67, 82, 90, 100] and adapters [5] are integrated into the restoration model to modulate intermediate features (see Fig.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only methods and models. The cited papers do not provide additional context about datasets.",
      "processing_time": 63.525004625320435,
      "citing_paper_id": "275134298",
      "cited_paper_id": 59599816
    },
    {
      "context_text": "Inspired by parameter-efficient fine-tuning (PEFT) meth-ods [10, 29], learnable prompts [26, 40, 67, 82, 90, 100] and adapters [5] are integrated into the restoration model to modulate intermediate features (see Fig.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only methods and models. The cited papers do not provide additional context about datasets.",
      "processing_time": 63.525004625320435,
      "citing_paper_id": "275134298",
      "cited_paper_id": 233296808
    },
    {
      "context_text": "Inspired by parameter-efficient fine-tuning (PEFT) meth-ods [10, 29], learnable prompts [26, 40, 67, 82, 90, 100] and adapters [5] are integrated into the restoration model to modulate intermediate features (see Fig.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only methods and models. The cited papers do not provide additional context about datasets.",
      "processing_time": 63.525004625320435,
      "citing_paper_id": "275134298",
      "cited_paper_id": 271039782
    },
    {
      "context_text": "Inspired by parameter-efficient fine-tuning (PEFT) meth-ods [10, 29], learnable prompts [26, 40, 67, 82, 90, 100] and adapters [5] are integrated into the restoration model to modulate intermediate features (see Fig.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only methods and models. The cited papers do not provide additional context about datasets.",
      "processing_time": 63.525004625320435,
      "citing_paper_id": "275134298",
      "cited_paper_id": 272724853
    },
    {
      "context_text": "…by the success of foundation models in natural language processing and high-level vision, there has been increasing interest in addressing multiple image restoration [85, 103] train a shared backbone using data from all tasks and are limited in leveraging degradation-specific restoration.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a general reference to 'data from all tasks'. No specific, verifiable datasets are named.",
      "processing_time": 64.31399250030518,
      "citing_paper_id": "275134298",
      "cited_paper_id": 232352764
    },
    {
      "context_text": "…by the success of foundation models in natural language processing and high-level vision, there has been increasing interest in addressing multiple image restoration [85, 103] train a shared backbone using data from all tasks and are limited in leveraging degradation-specific restoration.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a general reference to 'data from all tasks'. No specific, verifiable datasets are named.",
      "processing_time": 64.31399250030518,
      "citing_paper_id": "275134298",
      "cited_paper_id": 236171006
    },
    {
      "context_text": "Previous works [85] had exploited the benefits brought by scaling up the training data, we thus plan to collect more high-quality training data to bring further improvement on our method.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.2,
      "reasoning": "The context mentions 'training data' but does not specify a named dataset. The cited paper title does not provide additional information about a specific dataset.",
      "processing_time": 64.43686580657959,
      "citing_paper_id": "275134298",
      "cited_paper_id": 236171006
    },
    {
      "context_text": "(c) Expert-based methods [12, 13, 15, 20, 47, 49, 66, 87, 93, 94, 115] train specific MoE, LoRA, or filters for specific tasks.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.3,
      "reasoning": "The citation context does not mention any specific datasets, only methods and models. However, the cited papers' titles suggest a focus on image restoration, which may involve specific datasets.",
      "processing_time": 65.50309896469116,
      "citing_paper_id": "275134298",
      "cited_paper_id": 237500626
    },
    {
      "context_text": "(c) Expert-based methods [12, 13, 15, 20, 47, 49, 66, 87, 93, 94, 115] train specific MoE, LoRA, or filters for specific tasks.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.3,
      "reasoning": "The citation context does not mention any specific datasets, only methods and models. However, the cited papers' titles suggest a focus on image restoration, which may involve specific datasets.",
      "processing_time": 65.50309896469116,
      "citing_paper_id": "275134298",
      "cited_paper_id": 247762315
    },
    {
      "context_text": "(c) Expert-based methods [12, 13, 15, 20, 47, 49, 66, 87, 93, 94, 115] train specific MoE, LoRA, or filters for specific tasks.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.3,
      "reasoning": "The citation context does not mention any specific datasets, only methods and models. However, the cited papers' titles suggest a focus on image restoration, which may involve specific datasets.",
      "processing_time": 65.50309896469116,
      "citing_paper_id": "275134298",
      "cited_paper_id": 259108489
    },
    {
      "context_text": "(c) Expert-based methods [12, 13, 15, 20, 47, 49, 66, 87, 93, 94, 115] train specific MoE, LoRA, or filters for specific tasks.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.3,
      "reasoning": "The citation context does not mention any specific datasets, only methods and models. However, the cited papers' titles suggest a focus on image restoration, which may involve specific datasets.",
      "processing_time": 65.50309896469116,
      "citing_paper_id": "275134298",
      "cited_paper_id": 268030722
    },
    {
      "context_text": "(c) Expert-based methods [12, 13, 15, 20, 47, 49, 66, 87, 93, 94, 115] train specific MoE, LoRA, or filters for specific tasks.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.3,
      "reasoning": "The citation context does not mention any specific datasets, only methods and models. However, the cited papers' titles suggest a focus on image restoration, which may involve specific datasets.",
      "processing_time": 65.50309896469116,
      "citing_paper_id": "275134298",
      "cited_paper_id": 271874811
    },
    {
      "context_text": "Furthermore, some methods introduce mixture-of-experts (MoE) [13, 15, 47, 87, 94] (MoE-like using filters [12, 20, 49, 66, 93, 115] and low-rank adaptation [3, 81]) modules, routing the current feature to specific experts based on deep features and estimated degradation representations (DRs), as…",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only methods and models. The cited papers' titles suggest a focus on image restoration, but do not explicitly name datasets within the given context.",
      "processing_time": 66.07206273078918,
      "citing_paper_id": "275134298",
      "cited_paper_id": 237500626
    },
    {
      "context_text": "Furthermore, some methods introduce mixture-of-experts (MoE) [13, 15, 47, 87, 94] (MoE-like using filters [12, 20, 49, 66, 93, 115] and low-rank adaptation [3, 81]) modules, routing the current feature to specific experts based on deep features and estimated degradation representations (DRs), as…",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only methods and models. The cited papers' titles suggest a focus on image restoration, but do not explicitly name datasets within the given context.",
      "processing_time": 66.07206273078918,
      "citing_paper_id": "275134298",
      "cited_paper_id": 247762315
    },
    {
      "context_text": "Furthermore, some methods introduce mixture-of-experts (MoE) [13, 15, 47, 87, 94] (MoE-like using filters [12, 20, 49, 66, 93, 115] and low-rank adaptation [3, 81]) modules, routing the current feature to specific experts based on deep features and estimated degradation representations (DRs), as…",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only methods and models. The cited papers' titles suggest a focus on image restoration, but do not explicitly name datasets within the given context.",
      "processing_time": 66.07206273078918,
      "citing_paper_id": "275134298",
      "cited_paper_id": 259108489
    },
    {
      "context_text": "Furthermore, some methods introduce mixture-of-experts (MoE) [13, 15, 47, 87, 94] (MoE-like using filters [12, 20, 49, 66, 93, 115] and low-rank adaptation [3, 81]) modules, routing the current feature to specific experts based on deep features and estimated degradation representations (DRs), as…",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only methods and models. The cited papers' titles suggest a focus on image restoration, but do not explicitly name datasets within the given context.",
      "processing_time": 66.07206273078918,
      "citing_paper_id": "275134298",
      "cited_paper_id": 268030722
    },
    {
      "context_text": "Furthermore, some methods introduce mixture-of-experts (MoE) [13, 15, 47, 87, 94] (MoE-like using filters [12, 20, 49, 66, 93, 115] and low-rank adaptation [3, 81]) modules, routing the current feature to specific experts based on deep features and estimated degradation representations (DRs), as…",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only methods and models. The cited papers' titles suggest a focus on image restoration, but do not explicitly name datasets within the given context.",
      "processing_time": 66.07206273078918,
      "citing_paper_id": "275134298",
      "cited_paper_id": 271874811
    },
    {
      "context_text": "Following [59], we employ BLIP [43, 44] to generate captions for clean patches.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions BLIP but does not refer to it as a dataset. It is used as a method for generating captions, which is not a dataset.",
      "processing_time": 64.42332935333252,
      "citing_paper_id": "275134298",
      "cited_paper_id": 246411402
    },
    {
      "context_text": "Following [59], we employ BLIP [43, 44] to generate captions for clean patches.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions BLIP but does not refer to it as a dataset. It is used as a method for generating captions, which is not a dataset.",
      "processing_time": 64.42332935333252,
      "citing_paper_id": "275134298",
      "cited_paper_id": 256390509
    },
    {
      "context_text": "We follow the degradation pipeline of [106] to construct synthetic low-quality datasets for the training of the degradation extractor and all-in-one image restoration in mixed-degradation setup.",
      "catation_intent": "reusable resource",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions constructing synthetic low-quality datasets but does not specify any named, verifiable datasets. The reference to [106] is for a method or process, not a dataset.",
      "processing_time": 65.57481384277344,
      "citing_paper_id": "275134298",
      "cited_paper_id": 248665570
    },
    {
      "context_text": "By combining knowledge of the shared backbone and task-specific modules, these methods can adapt to different tasks effectively.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only a methodological approach. No dataset names are present in the text.",
      "processing_time": 63.60834050178528,
      "citing_paper_id": "275134298",
      "cited_paper_id": 249394802
    },
    {
      "context_text": "Mixture-of-Experts (MoE) has proven to be a solution to multi-task learning due to its conditional processing capability [17, 18, 114].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only references to Mixture-of-Experts (MoE) as a solution for multi-task learning. The cited papers' titles do not provide additional information about datasets.",
      "processing_time": 67.1610951423645,
      "citing_paper_id": "275134298",
      "cited_paper_id": 249538647
    },
    {
      "context_text": "Mixture-of-Experts (MoE) has proven to be a solution to multi-task learning due to its conditional processing capability [17, 18, 114].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only references to Mixture-of-Experts (MoE) as a solution for multi-task learning. The cited papers' titles do not provide additional information about datasets.",
      "processing_time": 67.1610951423645,
      "citing_paper_id": "275134298",
      "cited_paper_id": 254685665
    },
    {
      "context_text": "Mixture-of-Experts (MoE) has proven to be a solution to multi-task learning due to its conditional processing capability [17, 18, 114].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only references to Mixture-of-Experts (MoE) as a solution for multi-task learning. The cited papers' titles do not provide additional information about datasets.",
      "processing_time": 67.1610951423645,
      "citing_paper_id": "275134298",
      "cited_paper_id": 259287066
    },
    {
      "context_text": "…ability, we conduct comparisons on real-world degradations and un-seen degradations, including LHP [25] for real-world de-raining, LOLv2 [92] for real-world low-light enhancement, RealSnow [57] for real-world desnowing, RainDrop [69] for raindrop removal, TOLED [113] for under-display…",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "LHP",
        "LOLv2",
        "RealSnow",
        "RainDrop",
        "TOLED"
      ],
      "dataset_descriptions": {
        "LHP": "Used for evaluating real-world de-raining techniques, focusing on the performance of image restoration methods on realistic rain conditions.",
        "LOLv2": "Used for evaluating real-world low-light enhancement, assessing the effectiveness of image restoration methods in improving visibility in dark environments.",
        "RealSnow": "Used for evaluating real-world desnowing, testing the ability of image restoration methods to remove snow from images.",
        "RainDrop": "Used for evaluating raindrop removal, assessing the performance of image restoration methods in removing raindrops from images.",
        "TOLED": "Used for evaluating under-display image restoration, focusing on enhancing image quality in displays with underlying components."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions several datasets by name, which are used for evaluating image restoration techniques on various real-world degradations.",
      "processing_time": 80.01294374465942,
      "citing_paper_id": "275134298",
      "cited_paper_id": 250520618
    },
    {
      "context_text": "Recent MoE-based all-in-one image restoration methods can be categorized into implicit MoE [12, 20, 93, 94] and explicit MoE [4, 13, 15, 66, 81, 96, 115].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.2,
      "reasoning": "The citation span does not mention any specific datasets, only categories of methods. However, the cited paper titles suggest potential datasets.",
      "processing_time": 63.42539072036743,
      "citing_paper_id": "275134298",
      "cited_paper_id": 259108489
    },
    {
      "context_text": "Recent MoE-based all-in-one image restoration methods can be categorized into implicit MoE [12, 20, 93, 94] and explicit MoE [4, 13, 15, 66, 81, 96, 115].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.2,
      "reasoning": "The citation span does not mention any specific datasets, only categories of methods. However, the cited paper titles suggest potential datasets.",
      "processing_time": 63.42539072036743,
      "citing_paper_id": "275134298",
      "cited_paper_id": 270045495
    },
    {
      "context_text": "Recent MoE-based all-in-one image restoration methods can be categorized into implicit MoE [12, 20, 93, 94] and explicit MoE [4, 13, 15, 66, 81, 96, 115].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.2,
      "reasoning": "The citation span does not mention any specific datasets, only categories of methods. However, the cited paper titles suggest potential datasets.",
      "processing_time": 63.42539072036743,
      "citing_paper_id": "275134298",
      "cited_paper_id": 271874811
    },
    {
      "context_text": "To demonstrate the generalization ability, we conduct comparisons on real-world degradations and un-seen degradations, including LHP [25] for real-world de-raining, LOLv2 [92] for real-world low-light enhancement, RealSnow [57] for real-world desnowing, RainDrop [69] for raindrop removal, TOLED…",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "LHP",
        "LOLv2",
        "RealSnow",
        "RainDrop"
      ],
      "dataset_descriptions": {
        "LHP": "Used for real-world de-raining, evaluating the model's ability to remove rain from images under real-world conditions.",
        "LOLv2": "Used for real-world low-light enhancement, assessing the model's performance in improving visibility in low-light images.",
        "RealSnow": "Used for real-world desnowing, testing the model's effectiveness in removing snow from images.",
        "RainDrop": "Used for raindrop removal, evaluating the model's capability to handle raindrop-like distortions."
      },
      "confidence_score": 0.9,
      "reasoning": "The context mentions several datasets by name, which are used for evaluating the generalization ability of the model on various real-world image restoration tasks.",
      "processing_time": 78.78120422363281,
      "citing_paper_id": "275134298",
      "cited_paper_id": 260704481
    },
    {
      "context_text": "For examples, early work [87] in real-world super-resolution has adopted this way.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method or approach in real-world super-resolution.",
      "processing_time": 62.924649238586426,
      "citing_paper_id": "275134298",
      "cited_paper_id": 268030722
    },
    {
      "context_text": "#FLOPs (G) Latency (s) Restormer [98] 1128.9 0.368 NAFNet [16] 505.5 0.186 PromptIR [67] 1266.2 0.355 OneRestore [26] 94 ence based on given task instructions, i.e . the task name.",
      "catation_intent": "reusable resource",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only model names and performance metrics. There are no clear identifiers for datasets.",
      "processing_time": 63.557395935058594,
      "citing_paper_id": "275134298",
      "cited_paper_id": 271039782
    },
    {
      "context_text": "(b) PEFT-based methods [5, 26, 40, 67, 82, 90, 100] apply learnable prompts or adapters to the backbone to adapt various tasks.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods and approaches. The cited paper titles do not provide additional context about datasets.",
      "processing_time": 63.903263568878174,
      "citing_paper_id": "275134298",
      "cited_paper_id": 271039782
    },
    {
      "context_text": "(b) PEFT-based methods [5, 26, 40, 67, 82, 90, 100] apply learnable prompts or adapters to the backbone to adapt various tasks.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods and approaches. The cited paper titles do not provide additional context about datasets.",
      "processing_time": 63.903263568878174,
      "citing_paper_id": "275134298",
      "cited_paper_id": 272724853
    },
    {
      "context_text": "We compare our proposed method with task-agnostic methods [16, 48, 97, 98] and all-in-one methods [20, 22, 26, 40, 52, 67, 82, 100].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only comparisons with other methods. There are no clear identifiers for datasets.",
      "processing_time": 63.37156367301941,
      "citing_paper_id": "275134298",
      "cited_paper_id": 271039782
    },
    {
      "context_text": "Following [26], lowlight degradation is synthesized as, where I ( y ) is the illusion map obtained from Retinex-Former [11] and γ is the darkening coefficient.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (Retinex-Former) and a parameter (darkening coefficient).",
      "processing_time": 64.08463644981384,
      "citing_paper_id": "275134298",
      "cited_paper_id": 271039782
    },
    {
      "context_text": "By adaptively estimating image degradation at proper granularity, our UniRestorer can be effective in leveraging degradation-specific restoration while being robust to degradation estimation error. tasks within a single framework, known as all-in-one image restoration [32].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general concept of all-in-one image restoration. No verifiable resources are identified.",
      "processing_time": 64.08089709281921,
      "citing_paper_id": "275134298",
      "cited_paper_id": 273502246
    },
    {
      "context_text": "In this paper, we introduced our UniRestorer, a new universal image restoration framework that aims to leverage degradation priors to improve restoration performance while alleviating the inevitable error in degradation estimation.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a new framework called UniRestorer. There are no clear identifiers for datasets in the provided context.",
      "processing_time": 64.93689155578613,
      "citing_paper_id": "275134298",
      "cited_paper_id": 275921949
    },
    {
      "context_text": "Based on its routing result, it further selects a set of experts that can solve degradations belonging to DR group { e } n,k and are trained in different granularity levels from 0 to n .",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a methodological approach involving experts and degradation groups.",
      "processing_time": 62.6425085067749,
      "citing_paper_id": "275134298",
      "cited_paper_id": null
    },
    {
      "context_text": "…feature aggregation. c , k , s , head , ws , ps , and n express the number of output channels , kernel size , stride , attention heads , window size , patch size , and the number of blocks , respectively. ds - conv refers to depthwise separable convolution [9], bn stands for batch normalization.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only technical details and abbreviations. The cited paper title is about a method, not a dataset.",
      "processing_time": 64.12479281425476,
      "citing_paper_id": "276929056",
      "cited_paper_id": 2375110
    },
    {
      "context_text": "Due to the conditions of satellite sensors and signal interference during transmission, HSIs often encounter band-missing issues in addition to noise [59].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only general issues with HSIs. No clear, verifiable dataset names are present.",
      "processing_time": 63.25540280342102,
      "citing_paper_id": "276929056",
      "cited_paper_id": 3523851
    },
    {
      "context_text": "Because materials reflect different spectral bands, each substance can have a unique signature for material identification [1, 38, 45, 52].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a general property of materials in hyperspectral imaging. No dataset names are present in the citation span.",
      "processing_time": 64.47393894195557,
      "citing_paper_id": "276929056",
      "cited_paper_id": 10913825
    },
    {
      "context_text": "Because materials reflect different spectral bands, each substance can have a unique signature for material identification [1, 38, 45, 52].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a general property of materials in hyperspectral imaging. No dataset names are present in the citation span.",
      "processing_time": 64.47393894195557,
      "citing_paper_id": "276929056",
      "cited_paper_id": 11987926
    },
    {
      "context_text": "Because materials reflect different spectral bands, each substance can have a unique signature for material identification [1, 38, 45, 52].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a general property of materials in hyperspectral imaging. No dataset names are present in the citation span.",
      "processing_time": 64.47393894195557,
      "citing_paper_id": "276929056",
      "cited_paper_id": 272367542
    },
    {
      "context_text": "Optimization-based methods [5, 44, 49, 63] were initially used for HSI restoration.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only optimization-based methods for HSI restoration.",
      "processing_time": 62.29076862335205,
      "citing_paper_id": "276929056",
      "cited_paper_id": 13119501
    },
    {
      "context_text": "To address these issues, low-rank tensor models [42, 49] are widely used in hyperspectral image restoration, relying on the assumption that the clean image X exhibits low-rank properties [13, 50] across spatial and spectral dimensions.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only methods and assumptions about the properties of images. No verifiable resources are identified.",
      "processing_time": 63.958943367004395,
      "citing_paper_id": "276929056",
      "cited_paper_id": 13119501
    },
    {
      "context_text": "To address these issues, low-rank tensor models [42, 49] are widely used in hyperspectral image restoration, relying on the assumption that the clean image X exhibits low-rank properties [13, 50] across spatial and spectral dimensions.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only methods and assumptions about the properties of images. No verifiable resources are identified.",
      "processing_time": 63.958943367004395,
      "citing_paper_id": "276929056",
      "cited_paper_id": 207991682
    },
    {
      "context_text": "Unlike RGB image restoration, HSI restoration must account for complex spatial-spectral correlations across hundreds of spectral bands [58].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a general reference to hyperspectral image restoration. The cited paper title suggests a method rather than a dataset.",
      "processing_time": 63.95615482330322,
      "citing_paper_id": "276929056",
      "cited_paper_id": 44131640
    },
    {
      "context_text": "…networks [53, 54], which effectively capture global semantic representations for overall restoration but have difficulty with fine details, or (ii) plain residual-in-residual networks [19, 31, 62], which enhance spatial features and high-frequency information but lack global awareness.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only types of neural network architectures. No dataset names are present in the text.",
      "processing_time": 63.7399377822876,
      "citing_paper_id": "276929056",
      "cited_paper_id": 49657846
    },
    {
      "context_text": "Conventional approaches are based on the structural properties of HSI, such as low-rank and sparse priors [13, 50].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only references to conventional approaches and structural properties of HSI.",
      "processing_time": 62.50950217247009,
      "citing_paper_id": "276929056",
      "cited_paper_id": 207991682
    },
    {
      "context_text": "In particular, the group convolution effectively leverages the low-rank prior [25, 50] inherent in HSI while achieving a reduced parameter-size compared to standard convolutions.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only methods and techniques. The cited papers' titles suggest a focus on hyperspectral image restoration, but no specific datasets are named.",
      "processing_time": 65.33632850646973,
      "citing_paper_id": "276929056",
      "cited_paper_id": 207991682
    },
    {
      "context_text": "In particular, the group convolution effectively leverages the low-rank prior [25, 50] inherent in HSI while achieving a reduced parameter-size compared to standard convolutions.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only methods and techniques. The cited papers' titles suggest a focus on hyperspectral image restoration, but no specific datasets are named.",
      "processing_time": 65.33632850646973,
      "citing_paper_id": "276929056",
      "cited_paper_id": 246088517
    },
    {
      "context_text": "In this study, we employ CLIP [48], which integrates vision and language information and performs effectively in text feature extraction.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions CLIP, which is a method/model, not a dataset. No specific dataset is mentioned in the context.",
      "processing_time": 63.031266927719116,
      "citing_paper_id": "276929056",
      "cited_paper_id": 231591445
    },
    {
      "context_text": "In our first configuration, we use RGB bands from HSIs as visual prompts, encoded via CLIP [48] and guided by PGFM.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (CLIP) and a technique (PGFM).",
      "processing_time": 63.02521634101868,
      "citing_paper_id": "276929056",
      "cited_paper_id": 231591445
    },
    {
      "context_text": "As discussed previously, both data and features extracted by pre-trained CLIP [48] exhibit domain gaps with HSI.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions 'HSI' which could be a dataset, but there is no clear indication that it is a specific, verifiable dataset. The reference to CLIP is a method, not a dataset.",
      "processing_time": 67.1966872215271,
      "citing_paper_id": "276929056",
      "cited_paper_id": 231591445
    },
    {
      "context_text": "…the performance of PromptHSI, we compared it with eight other supervised HSI restoration methods for specific degradation, including HSIDwRD [60], SERT [29], SRDNet [39], SNLSR [21], SGNet [43], AACNet [56], SST [28], and HSDT [26] in the synthesized AVIRIS dataset with composite degradations.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "AVIRIS"
      ],
      "dataset_descriptions": {
        "AVIRIS": "Used to evaluate the performance of PromptHSI against other HSI restoration methods, focusing on synthesized composite degradations in hyperspectral images."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions the 'AVIRIS dataset' which is a specific, verifiable dataset used for hyperspectral image restoration experiments.",
      "processing_time": 71.67744326591492,
      "citing_paper_id": "276929056",
      "cited_paper_id": 248370717
    },
    {
      "context_text": "…the performance of PromptHSI, we compared it with eight other supervised HSI restoration methods for specific degradation, including HSIDwRD [60], SERT [29], SRDNet [39], SNLSR [21], SGNet [43], AACNet [56], SST [28], and HSDT [26] in the synthesized AVIRIS dataset with composite degradations.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "AVIRIS"
      ],
      "dataset_descriptions": {
        "AVIRIS": "Used to evaluate the performance of PromptHSI against other HSI restoration methods, focusing on synthesized composite degradations in hyperspectral images."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions the 'AVIRIS dataset' which is a specific, verifiable dataset used for hyperspectral image restoration experiments.",
      "processing_time": 71.67744326591492,
      "citing_paper_id": "276929056",
      "cited_paper_id": 254017867
    },
    {
      "context_text": "…the performance of PromptHSI, we compared it with eight other supervised HSI restoration methods for specific degradation, including HSIDwRD [60], SERT [29], SRDNet [39], SNLSR [21], SGNet [43], AACNet [56], SST [28], and HSDT [26] in the synthesized AVIRIS dataset with composite degradations.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "AVIRIS"
      ],
      "dataset_descriptions": {
        "AVIRIS": "Used to evaluate the performance of PromptHSI against other HSI restoration methods, focusing on synthesized composite degradations in hyperspectral images."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions the 'AVIRIS dataset' which is a specific, verifiable dataset used for hyperspectral image restoration experiments.",
      "processing_time": 71.67744326591492,
      "citing_paper_id": "276929056",
      "cited_paper_id": 260704626
    },
    {
      "context_text": "…the performance of PromptHSI, we compared it with eight other supervised HSI restoration methods for specific degradation, including HSIDwRD [60], SERT [29], SRDNet [39], SNLSR [21], SGNet [43], AACNet [56], SST [28], and HSDT [26] in the synthesized AVIRIS dataset with composite degradations.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "AVIRIS"
      ],
      "dataset_descriptions": {
        "AVIRIS": "Used to evaluate the performance of PromptHSI against other HSI restoration methods, focusing on synthesized composite degradations in hyperspectral images."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions the 'AVIRIS dataset' which is a specific, verifiable dataset used for hyperspectral image restoration experiments.",
      "processing_time": 71.67744326591492,
      "citing_paper_id": "276929056",
      "cited_paper_id": 272755634
    },
    {
      "context_text": "This architecture incorporates Swin Transformer [31, 40] with dense connections, demonstrating a more lightweight and efficient parameter-size compared to the Residual Hybrid Attention Groups (RHAG) in HAT [7].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models and methods. There are no verifiable resources that meet the criteria for inclusion.",
      "processing_time": 64.30649828910828,
      "citing_paper_id": "276929056",
      "cited_paper_id": 248572065
    },
    {
      "context_text": "Several works against different types of restoration model, such as denoising [17, 28, 46] for Gaussian noise and impulse noise, in-painting [5, 34, 63] for stripe effect, and spatial blurring [20, 23, 30] or limited spectral resolution [2, 3, 16].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only types of restoration models and their applications. No verifiable resources are identified.",
      "processing_time": 64.12468576431274,
      "citing_paper_id": "276929056",
      "cited_paper_id": 254017867
    },
    {
      "context_text": "Several works against different types of restoration model, such as denoising [17, 28, 46] for Gaussian noise and impulse noise, in-painting [5, 34, 63] for stripe effect, and spatial blurring [20, 23, 30] or limited spectral resolution [2, 3, 16].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only types of restoration models and their applications. No verifiable resources are identified.",
      "processing_time": 64.12468576431274,
      "citing_paper_id": "276929056",
      "cited_paper_id": 258303516
    },
    {
      "context_text": "Several works against different types of restoration model, such as denoising [17, 28, 46] for Gaussian noise and impulse noise, in-painting [5, 34, 63] for stripe effect, and spatial blurring [20, 23, 30] or limited spectral resolution [2, 3, 16].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only types of restoration models and their applications. No verifiable resources are identified.",
      "processing_time": 64.12468576431274,
      "citing_paper_id": "276929056",
      "cited_paper_id": 270845666
    },
    {
      "context_text": "By embedding plain-residual-in-residual modules [18, 19, 31] within a U-shaped cascade decoder [8, 54], our framework achieves both fine-grained and global restoration, effectively addressing various HSI degradations.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods and architectures. No verifiable resources are identified.",
      "processing_time": 63.123053789138794,
      "citing_paper_id": "276929056",
      "cited_paper_id": 264289165
    },
    {
      "context_text": "By embedding plain-residual-in-residual modules [18, 19, 31] within a U-shaped cascade decoder [8, 54], our framework achieves both fine-grained and global restoration, effectively addressing various HSI degradations.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods and architectures. No verifiable resources are identified.",
      "processing_time": 63.123053789138794,
      "citing_paper_id": "276929056",
      "cited_paper_id": 268529518
    },
    {
      "context_text": "Effective HSI restoration requires an architecture capable of addressing diverse degradations within a single framework [8, 55].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only the need for an architecture capable of addressing diverse degradations. No verifiable resources are identified.",
      "processing_time": 64.68305325508118,
      "citing_paper_id": "276929056",
      "cited_paper_id": 264289165
    },
    {
      "context_text": "Effective HSI restoration requires an architecture capable of addressing diverse degradations within a single framework [8, 55].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only the need for an architecture capable of addressing diverse degradations. No verifiable resources are identified.",
      "processing_time": 64.68305325508118,
      "citing_paper_id": "276929056",
      "cited_paper_id": 270155352
    },
    {
      "context_text": "The emergence of AiO RGB image restoration meth-ods [4, 10, 11, 15, 36, 47] has demonstrated remarkable success in handling diverse degradations within a unified network.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to methods and approaches in image restoration.",
      "processing_time": 62.7622754573822,
      "citing_paper_id": "276929056",
      "cited_paper_id": 266690738
    },
    {
      "context_text": "The emergence of AiO RGB image restoration meth-ods [4, 10, 11, 15, 36, 47] has demonstrated remarkable success in handling diverse degradations within a unified network.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to methods and approaches in image restoration.",
      "processing_time": 62.7622754573822,
      "citing_paper_id": "276929056",
      "cited_paper_id": 267320695
    },
    {
      "context_text": "The emergence of AiO RGB image restoration meth-ods [4, 10, 11, 15, 36, 47] has demonstrated remarkable success in handling diverse degradations within a unified network.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to methods and approaches in image restoration.",
      "processing_time": 62.7622754573822,
      "citing_paper_id": "276929056",
      "cited_paper_id": null
    },
    {
      "context_text": "These methods leverage prompt learning, either through visual prompts [4, 11, 47] or text-based prompts [10, 15], to adaptively restore corrupted images.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only methods and approaches. No dataset names are present in the text.",
      "processing_time": 63.620845794677734,
      "citing_paper_id": "276929056",
      "cited_paper_id": 267320695
    },
    {
      "context_text": "These methods leverage prompt learning, either through visual prompts [4, 11, 47] or text-based prompts [10, 15], to adaptively restore corrupted images.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only methods and approaches. No dataset names are present in the text.",
      "processing_time": 63.620845794677734,
      "citing_paper_id": "276929056",
      "cited_paper_id": null
    },
    {
      "context_text": "The SAM metric is es-Figure For the text-prompt-based methods [10, 15] and our PromptHSI, we employ the standard text-prompts presented in this study for training network and inference for related experiments. sential for material identification and is less sensitive to intensity variations.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only text prompts which are not considered datasets. The context focuses on methods and metrics.",
      "processing_time": 64.23461103439331,
      "citing_paper_id": "276929056",
      "cited_paper_id": 267320695
    },
    {
      "context_text": "Because there is no AiO HSI restoration method, we compare four AiO RGB image restoration methods in the experiments: AirNet [27], PromptIR [47], HAIR [4], InstructIR [10], AdaIR [11], and OneRestore [15].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only methods for image restoration. No verifiable resources are identified.",
      "processing_time": 63.18594741821289,
      "citing_paper_id": "276929056",
      "cited_paper_id": 267320695
    },
    {
      "context_text": "Because there is no AiO HSI restoration method, we compare four AiO RGB image restoration methods in the experiments: AirNet [27], PromptIR [47], HAIR [4], InstructIR [10], AdaIR [11], and OneRestore [15].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only methods for image restoration. No verifiable resources are identified.",
      "processing_time": 63.18594741821289,
      "citing_paper_id": "276929056",
      "cited_paper_id": null
    },
    {
      "context_text": "A recently emerging paradigm in RGB image restoration designs single models to handle multiple degradations [4, 6, 10, 14, 15, 27, 47].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only a general trend in the field of RGB image restoration.",
      "processing_time": 63.418123722076416,
      "citing_paper_id": "276929056",
      "cited_paper_id": 267320695
    },
    {
      "context_text": "…learn degradation representations and restoration; PromptIR [47] introduced visual prompts for degradation perception during decoding; InstructIR [10] employed text prompts to guide the network; HAIR [4] utilized a hypernetwork to generate degradation-specific parameters; OneRestore [15]…",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods and models. The context is focused on describing various approaches to image restoration, none of which are associated with a specific dataset.",
      "processing_time": 66.73154592514038,
      "citing_paper_id": "276929056",
      "cited_paper_id": 267320695
    },
    {
      "context_text": "The PGFM output is first refined using sequential RDG [19] and FRDB [18] modules to enhance fine details.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods or modules used for enhancing fine details.",
      "processing_time": 62.55242109298706,
      "citing_paper_id": "276929056",
      "cited_paper_id": 268529518
    },
    {
      "context_text": "We simulate this degradation and categorize it into three types as follows [18].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a categorization of degradation types. No verifiable resources are identified.",
      "processing_time": 63.84298539161682,
      "citing_paper_id": "276929056",
      "cited_paper_id": 268529518
    },
    {
      "context_text": "We adopt the Residual Dense Group (RDG) [19] for the spatial feature extraction branch , and the Fast Residual Dense Block (FRDB) [18] for the spectral feature extraction branch.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions models/methods (RDG, FRDB) but does not reference any specific datasets. The context is focused on the use of these methods for feature extraction in image restoration.",
      "processing_time": 66.88187837600708,
      "citing_paper_id": "276929056",
      "cited_paper_id": 268529518
    },
    {
      "context_text": "N α and N β denote the depth of the RDG [19] and FRDB [18] blocks in the dual-branch designed feature aggregation. c , k , s , head , ws , ps , and n express the number of output channels , kernel size , stride , attention heads , window size , patch size , and the number of blocks , respectively.…",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only model components and parameters. No verifiable resources are identified.",
      "processing_time": 63.65656089782715,
      "citing_paper_id": "276929056",
      "cited_paper_id": 268529518
    },
    {
      "context_text": "Afterwards, SSAFEBs is based on a depth-wise separable convolution to extract features while inherently taking advantage of this low-rank prior, thereby reducing computational complexity without compromising performance [18].",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method or technique. There are no clear identifiers for datasets in the provided context.",
      "processing_time": 64.27736282348633,
      "citing_paper_id": "276929056",
      "cited_paper_id": 268529518
    },
    {
      "context_text": "…each measuring 259 × 259 pixels, composed of 224 spectral bands spanning wavelengths from 400 to 2500 nm. Consistent with prior research [20, 34, 35], spectral bands identified as lower quality (1-10, 104-116, 152-170, and 215-224) were excluded, resulting in HSIs with 172 spectral bands.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context describes the processing of hyperspectral images but does not mention any specific dataset name. The details about the image size and spectral bands are provided, but no named dataset is referenced.",
      "processing_time": 66.69044065475464,
      "citing_paper_id": "276929056",
      "cited_paper_id": 268603780
    },
    {
      "context_text": "This study employed the AVIRIS dataset provided by Lin et al. [20, 35], an extensive HSI dataset with numerous bands spanning diverse geographical areas, for experiments.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "AVIRIS"
      ],
      "dataset_descriptions": {
        "AVIRIS": "Used for experiments involving hyperspectral imaging, specifically leveraging its extensive HSI dataset with numerous bands across diverse geographical areas."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions the AVIRIS dataset, which is a specific, verifiable dataset used for hyperspectral imaging experiments.",
      "processing_time": 70.86692643165588,
      "citing_paper_id": "276929056",
      "cited_paper_id": 268603780
    },
    {
      "context_text": "This study employed the AVIRIS dataset provided by Lin et al. [20, 35], an extensive HSI dataset with numerous bands spanning diverse geographical areas, for experiments.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "AVIRIS"
      ],
      "dataset_descriptions": {
        "AVIRIS": "Used for experiments involving hyperspectral imaging, specifically leveraging its extensive HSI dataset with numerous bands across diverse geographical areas."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions the AVIRIS dataset, which is a specific, verifiable dataset used for hyperspectral imaging experiments.",
      "processing_time": 70.86692643165588,
      "citing_paper_id": "276929056",
      "cited_paper_id": 270845666
    },
    {
      "context_text": "This study used a dataset captured by the Airborne Visible/Infrared Imaging Spectrometer (AVIRIS) [35, 52].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.3,
      "reasoning": "The context mentions a dataset captured by AVIRIS, which is a specific imaging spectrometer. However, the dataset itself is not named, only the instrument used to capture it.",
      "processing_time": 66.34348964691162,
      "citing_paper_id": "276929056",
      "cited_paper_id": 268603780
    },
    {
      "context_text": "…subimages, each measuring 259 × 259 pixels, composed of 224 spectral bands spanning wavelengths from 400 to 2500 nm. Consistent with prior research [20, 34, 35], spectral bands identified as lower quality (1-10, 104-116, 152-170, and 215-224) were excluded, resulting in HSIs with 172 spectral…",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions spectral bands and hyperspectral images (HSIs) but does not specify a named dataset. The exclusion of certain spectral bands is described, but no specific dataset name is provided.",
      "processing_time": 67.04329514503479,
      "citing_paper_id": "276929056",
      "cited_paper_id": 270845666
    },
    {
      "context_text": "AiO restoration models [4, 15, 47] and HSI-specific methods [21, 24, 39] favor the former, limiting fine-detail recovery.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only references to models and methods. No verifiable resources are identified.",
      "processing_time": 63.708411693573,
      "citing_paper_id": "276929056",
      "cited_paper_id": 272755634
    },
    {
      "context_text": "…parameters; OneRestore [15] extended AiO restoration to composite degradations, addressing multiple corrup-tions in a single image; and AdaIR [11] transforms feature maps into the frequency domain via Fourier transform and uses an adaptive mask, guided by the degraded image, to decouple…",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods and models. There are no clear identifiers for datasets within the text.",
      "processing_time": 64.21688914299011,
      "citing_paper_id": "276929056",
      "cited_paper_id": null
    },
    {
      "context_text": "As shown in Figure 4 (right), previous AiO restoration methods [4, 11] utilize adaptive modules to classify degradation types.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only adaptive modules in previous methods. There are no clear identifiers for datasets.",
      "processing_time": 63.72528266906738,
      "citing_paper_id": "276929056",
      "cited_paper_id": null
    },
    {
      "context_text": "As the same settings as in [42, 43, 44, 45], three levels of Gaussian noise σ = { 15 , 25 , 50 } are respectively added to clear images, generating corresponding noisy images for training and evaluation.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific dataset names, only the process of adding Gaussian noise to clear images for training and evaluation.",
      "processing_time": 64.08111238479614,
      "citing_paper_id": "266164339",
      "cited_paper_id": 1900475
    },
    {
      "context_text": "Skip connections[60] between encoder-decoder layers at the same level are adopted in concatenation way for better feature reuse.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (U-Net) and its architectural feature (skip connections).",
      "processing_time": 64.07688641548157,
      "citing_paper_id": "266164339",
      "cited_paper_id": 3719281
    },
    {
      "context_text": "Specifically, given an image feature mapping X ∈ R H × W × C , a layer normalization[61] is first performed to obtain X 0 .",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific dataset, only a method (layer normalization).",
      "processing_time": 62.57738375663757,
      "citing_paper_id": "266164339",
      "cited_paper_id": 8236317
    },
    {
      "context_text": "Following previous work [47, 48, 42], we employ Peak Signal-to-Noise Ratio (PSNR)[49] and Structural Similarity (SSIM)[50] as our quantitative evaluation metrics.",
      "catation_intent": "findings",
      "resource_type": "metric",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context mentions PSNR and SSIM as evaluation metrics but does not refer to any specific datasets. The cited papers are about image quality assessment methods, not datasets.",
      "processing_time": 66.28416180610657,
      "citing_paper_id": "266164339",
      "cited_paper_id": 21712570
    },
    {
      "context_text": "Following previous work [47, 48, 42], we employ Peak Signal-to-Noise Ratio (PSNR)[49] and Structural Similarity (SSIM)[50] as our quantitative evaluation metrics.",
      "catation_intent": "findings",
      "resource_type": "metric",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context mentions PSNR and SSIM as evaluation metrics but does not refer to any specific datasets. The cited papers are about image quality assessment methods, not datasets.",
      "processing_time": 66.28416180610657,
      "citing_paper_id": "266164339",
      "cited_paper_id": 62732555
    },
    {
      "context_text": "Following previous work [47, 48, 42], we employ Peak Signal-to-Noise Ratio (PSNR)[49] and Structural Similarity (SSIM)[50] as our quantitative evaluation metrics.",
      "catation_intent": "findings",
      "resource_type": "metric",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context mentions PSNR and SSIM as evaluation metrics but does not refer to any specific datasets. The cited papers are about image quality assessment methods, not datasets.",
      "processing_time": 66.28416180610657,
      "citing_paper_id": "266164339",
      "cited_paper_id": 207761262
    },
    {
      "context_text": "Their channel numbers are respective [48,96,192,384].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only channel numbers in a network architecture.",
      "processing_time": 61.58282423019409,
      "citing_paper_id": "266164339",
      "cited_paper_id": 21712570
    },
    {
      "context_text": ", BRDNet[42], LPNet[48], FDGAN[47] and MPRNet[52]) and five methods on image restoration for multiple degradation (IRMD) ( i . e .",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only models and methods. There are no clear identifiers for datasets within the given context.",
      "processing_time": 64.87726330757141,
      "citing_paper_id": "266164339",
      "cited_paper_id": 21712570
    },
    {
      "context_text": ", BRDNet[42], LPNet[48], FDGAN[47] and MPRNet[52]) and five methods on image restoration for multiple degradation (IRMD) ( i . e .",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only models and methods. There are no clear identifiers for datasets within the given context.",
      "processing_time": 64.87726330757141,
      "citing_paper_id": "266164339",
      "cited_paper_id": 231802205
    },
    {
      "context_text": "…element-wise multiplication, Equ.1 can be reformulated to Equ.3: In many work, Equ.3 is then employed as the basic physic model of image dehazing[14], such as the atmosphere scattering model[36][37] and the Retinex theory[38], where ˆ λ represents the transmission map or the atmospheric light.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only theoretical models and equations. The cited papers do not introduce new datasets either.",
      "processing_time": 64.39966893196106,
      "citing_paper_id": "266164339",
      "cited_paper_id": 30651858
    },
    {
      "context_text": "…element-wise multiplication, Equ.1 can be reformulated to Equ.3: In many work, Equ.3 is then employed as the basic physic model of image dehazing[14], such as the atmosphere scattering model[36][37] and the Retinex theory[38], where ˆ λ represents the transmission map or the atmospheric light.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only theoretical models and equations. The cited papers do not introduce new datasets either.",
      "processing_time": 64.39966893196106,
      "citing_paper_id": "266164339",
      "cited_paper_id": 248069347
    },
    {
      "context_text": "In terms of dehazing task, within RESIDE[46], SOTS dataset with 72,135 images is taken as training set.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "SOTS"
      ],
      "dataset_descriptions": {
        "SOTS": "Used as a training set for the dehazing task, containing 72,135 images to improve image restoration techniques."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions the SOTS dataset, which is part of the RESIDE benchmark. It is used for training in the dehazing task.",
      "processing_time": 70.93208456039429,
      "citing_paper_id": "266164339",
      "cited_paper_id": 39760169
    },
    {
      "context_text": "BSD400[39], CBSD68[39], WED[40], and Urban100[56] are employed for image denoising, Rain100L[41] for image de-raining, and RESIDE[46] for image dehazing.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "BSD400",
        "CBSD68",
        "WED",
        "Urban100",
        "Rain100L",
        "RESIDE"
      ],
      "dataset_descriptions": {
        "BSD400": "Used for image denoising, focusing on natural image quality enhancement through denoising algorithms.",
        "CBSD68": "Used for image denoising, focusing on natural image quality enhancement through denoising algorithms.",
        "WED": "Used for image denoising, focusing on natural image quality enhancement through denoising techniques.",
        "Urban100": "Used for image denoising, focusing on urban scene image quality enhancement through denoising algorithms.",
        "Rain100L": "Used for image de-raining, focusing on removing rain streaks from images to improve visual clarity.",
        "RESIDE": "Used for image dehazing, focusing on improving visibility in hazy images through dehazing algorithms."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions several datasets used for specific image restoration tasks, which align with the research topic of all-in-one image restoration.",
      "processing_time": 79.10358428955078,
      "citing_paper_id": "266164339",
      "cited_paper_id": 39760169
    },
    {
      "context_text": "4: Visual comparisons with SOTA all-in-one models on Rain100L[41], SOTS[46] and CBSD68[39] sample images.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Rain100L",
        "SOTS",
        "CBSD68"
      ],
      "dataset_descriptions": {
        "Rain100L": "Used for visual comparisons with state-of-the-art all-in-one models, focusing on rain removal performance on synthetic rainy images.",
        "SOTS": "Used for visual comparisons with state-of-the-art all-in-one models, focusing on dehazing performance on synthetic and real-world hazy images.",
        "CBSD68": "Used for evaluating all-in-one image restoration performance, focusing on denoising and deblurring capabilities on a diverse set of clean images."
      },
      "confidence_score": 1.0,
      "reasoning": "The citation mentions specific datasets used for visual comparisons with state-of-the-art all-in-one models. These datasets are relevant to the topic of all-in-one image restoration.",
      "processing_time": 76.58865356445312,
      "citing_paper_id": "266164339",
      "cited_paper_id": 39760169
    },
    {
      "context_text": ", TKMANet[19], DL[53], AirNet[18], DA-CLIP[54] and PromptIR[20]).",
      "catation_intent": "none",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only models and methods. No dataset names are present in the context.",
      "processing_time": 63.95156216621399,
      "citing_paper_id": "266164339",
      "cited_paper_id": 195787503
    },
    {
      "context_text": ", TKMANet[19], DL[53], AirNet[18], DA-CLIP[54] and PromptIR[20]).",
      "catation_intent": "none",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only models and methods. No dataset names are present in the context.",
      "processing_time": 63.95156216621399,
      "citing_paper_id": "266164339",
      "cited_paper_id": 250581068
    },
    {
      "context_text": "A task-specific BERT[58][23] for image restoration is utilized to generate semantic guidance.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (BERT) for generating semantic guidance in image restoration.",
      "processing_time": 63.95106077194214,
      "citing_paper_id": "266164339",
      "cited_paper_id": 201646309
    },
    {
      "context_text": "[17] proposed an all-in-one model to address multiple degradation caused by unideal weather (e.g., rain, fog, snow).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a model or method for addressing multiple degradations caused by bad weather.",
      "processing_time": 64.67686653137207,
      "citing_paper_id": "266164339",
      "cited_paper_id": 219978541
    },
    {
      "context_text": "1 (a), early ”all-in-one” models such as [16][17] proposed to train specialized headers and corresponding tails to handle each degradation of concern.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models and methods. There are no clear identifiers for datasets in the provided context.",
      "processing_time": 65.02208089828491,
      "citing_paper_id": "266164339",
      "cited_paper_id": 219978541
    },
    {
      "context_text": "Specifically, From Table 2, we can observe that TextPromptIR achieves an average 0.13dB PSNR higher than the second-best method Restormer[55], on the Urban100, Rain100L and SOTS datasets.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Urban100",
        "Rain100L",
        "SOTS"
      ],
      "dataset_descriptions": {
        "Urban100": "Used to evaluate image restoration methods, focusing on high-resolution urban scenes with diverse textures and structures.",
        "Rain100L": "Used to assess rain removal performance, containing images with varying rain intensities and complex backgrounds.",
        "SOTS": "Used to test dehazing algorithms, featuring synthetic and real-world hazy images with natural outdoor scenes."
      },
      "confidence_score": 1.0,
      "reasoning": "The citation mentions specific datasets used for evaluating image restoration methods, which are relevant to the 'All-in-One Image Restoration' topic.",
      "processing_time": 75.12506222724915,
      "citing_paper_id": "266164339",
      "cited_paper_id": 244346144
    },
    {
      "context_text": "Traditionally, most of methods are trained for specific tasks, such as denoising[74, 2, 3, 4], deraining[9, 10, 5, 6, 7, 8, 73], and dehazing[12, 70, 71, 72], and cannot cope with di ff erent types of degraded images in a single unified model.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general tasks and methods. No dataset names are present in the text.",
      "processing_time": 64.77390909194946,
      "citing_paper_id": "266164339",
      "cited_paper_id": 245095057
    },
    {
      "context_text": "Traditionally, most of methods are trained for specific tasks, such as denoising[74, 2, 3, 4], deraining[9, 10, 5, 6, 7, 8, 73], and dehazing[12, 70, 71, 72], and cannot cope with di ff erent types of degraded images in a single unified model.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general tasks and methods. No dataset names are present in the text.",
      "processing_time": 64.77390909194946,
      "citing_paper_id": "266164339",
      "cited_paper_id": 249474184
    },
    {
      "context_text": "Traditionally, most of methods are trained for specific tasks, such as denoising[74, 2, 3, 4], deraining[9, 10, 5, 6, 7, 8, 73], and dehazing[12, 70, 71, 72], and cannot cope with di ff erent types of degraded images in a single unified model.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general tasks and methods. No dataset names are present in the text.",
      "processing_time": 64.77390909194946,
      "citing_paper_id": "266164339",
      "cited_paper_id": 251197000
    },
    {
      "context_text": "Traditionally, most of methods are trained for specific tasks, such as denoising[74, 2, 3, 4], deraining[9, 10, 5, 6, 7, 8, 73], and dehazing[12, 70, 71, 72], and cannot cope with di ff erent types of degraded images in a single unified model.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general tasks and methods. No dataset names are present in the text.",
      "processing_time": 64.77390909194946,
      "citing_paper_id": "266164339",
      "cited_paper_id": 255089439
    },
    {
      "context_text": "Traditionally, most of methods are trained for specific tasks, such as denoising[74, 2, 3, 4], deraining[9, 10, 5, 6, 7, 8, 73], and dehazing[12, 70, 71, 72], and cannot cope with di ff erent types of degraded images in a single unified model.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general tasks and methods. No dataset names are present in the text.",
      "processing_time": 64.77390909194946,
      "citing_paper_id": "266164339",
      "cited_paper_id": null
    },
    {
      "context_text": "The numbers of ITBlocks in the proposed TextPromptIR are respectively set [4,6,6,8] ranging from Level1 to Level4.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only architectural details of a model. No verifiable resources are identified.",
      "processing_time": 64.42631387710571,
      "citing_paper_id": "266164339",
      "cited_paper_id": 249474184
    },
    {
      "context_text": "The attention heads are correspondingly set as [1,2,4,8].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only the configuration of attention heads. No dataset names are present in the citation span.",
      "processing_time": 65.03240060806274,
      "citing_paper_id": "266164339",
      "cited_paper_id": 249474184
    },
    {
      "context_text": "The attention heads are correspondingly set as [1,2,4,8].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only the configuration of attention heads. No dataset names are present in the citation span.",
      "processing_time": 65.03240060806274,
      "citing_paper_id": "266164339",
      "cited_paper_id": 260905883
    },
    {
      "context_text": "6, the resulted images reveal that TextPromptIR exhibits more pronounced removal results compared to other all-in-one models’ (e.g. AirNet[18], PromptIR[20], TKMANet[19]) and preserves better image’s structural details.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only comparisons between different models. No verifiable resources are identified.",
      "processing_time": 63.950950145721436,
      "citing_paper_id": "266164339",
      "cited_paper_id": 250581068
    },
    {
      "context_text": "To address the above issues, we have proposed an e ff ec-Preprint Compared to most of state-of-the-art methods[18, 19, 20, 21, 22], TextPromptIR does not require cumbersome construction of di ff erent headers for di ff erent tasks.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only a method called TextPromptIR. The cited papers' titles do not provide additional dataset information.",
      "processing_time": 65.70425415039062,
      "citing_paper_id": "266164339",
      "cited_paper_id": 250581068
    },
    {
      "context_text": "To address the above issues, we have proposed an e ff ec-Preprint Compared to most of state-of-the-art methods[18, 19, 20, 21, 22], TextPromptIR does not require cumbersome construction of di ff erent headers for di ff erent tasks.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only a method called TextPromptIR. The cited papers' titles do not provide additional dataset information.",
      "processing_time": 65.70425415039062,
      "citing_paper_id": "266164339",
      "cited_paper_id": 272680196
    },
    {
      "context_text": "[19] achieved comparable results through learning an unified model across multiple restoration models based on knowledge distillation.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for achieving comparable results through knowledge distillation.",
      "processing_time": 63.684499979019165,
      "citing_paper_id": "266164339",
      "cited_paper_id": 250581068
    },
    {
      "context_text": "To address this dilemma, some models[18][19] proposed to directly and blindly restore degraded image with a single branch network in an end-to-end way, as shown in Fig.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only a general approach to image restoration. No verifiable resources are identified.",
      "processing_time": 64.99618911743164,
      "citing_paper_id": "266164339",
      "cited_paper_id": 250581068
    },
    {
      "context_text": "[30] utilized text-based question and answer pairs for object detection in videos.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions 'text-based question and answer pairs' but does not specify a dataset name. The title suggests a method or approach rather than a dataset.",
      "processing_time": 66.15875339508057,
      "citing_paper_id": "266164339",
      "cited_paper_id": 256655517
    },
    {
      "context_text": "[21] encoded degradation-specific information and dynamically guided restoration network by utilizing learning prompts.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method involving learning prompts for image restoration.",
      "processing_time": 63.2321891784668,
      "citing_paper_id": "266164339",
      "cited_paper_id": 272680196
    },
    {
      "context_text": "More recent models such as PromptIR[20], CAPTNet[21] and IDR[22], have been proposed to leverage learnable prompts to guide restoration, as shown in Fig.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models and methods. The context focuses on the use of learnable prompts for image restoration.",
      "processing_time": 65.50620102882385,
      "citing_paper_id": "266164339",
      "cited_paper_id": 272680196
    },
    {
      "context_text": "Pixel-unshu ffl e and pixel-shu ffl e operations[59] are respectively employed on feature upsampling and downsampling layers, for better preservation of feature information.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only operations used in feature upsampling and downsampling.",
      "processing_time": 63.4832968711853,
      "citing_paper_id": "266164339",
      "cited_paper_id": null
    },
    {
      "context_text": "For image SR, we choose DIV2K [34] and Flickr2K [35] as the training set, and we evaluate on Set5 [36], Set14 [37], BSDS100 [38], Urban100 [39], and Manga109 [40].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "DIV2K",
        "Flickr2K",
        "Set5",
        "Set14",
        "BSDS100",
        "Urban100",
        "Manga109"
      ],
      "dataset_descriptions": {
        "DIV2K": "Used for training single image super-resolution models, providing high-quality images for upscaling.",
        "Flickr2K": "Used for training single image super-resolution models, offering a diverse set of images for robust model training.",
        "ImageNet": "Used for training and evaluating single image super-resolution models, providing a large and diverse set of images.",
        "Set5": "Used for evaluating single image super-resolution models, focusing on a small but widely recognized set of images.",
        "Set14": "Used for evaluating single image super-resolution models, providing a moderate-sized set of images for benchmarking.",
        "BSDS100": "Used for evaluating single image super-resolution models, containing a variety of natural images for performance assessment.",
        "Urban100": "Used for evaluating single image super-resolution models, specifically designed for urban scenes and architectural details.",
        "Manga109": "Used for evaluating single image super-resolution models, focusing on manga images to assess performance on line art and text."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions several datasets used for training and evaluation in the context of single image super-resolution. These datasets are specific and relevant to the research topic.",
      "processing_time": 80.76060271263123,
      "citing_paper_id": "266210445",
      "cited_paper_id": 484327
    },
    {
      "context_text": "For image SR, we choose DIV2K [34] and Flickr2K [35] as the training set, and we evaluate on Set5 [36], Set14 [37], BSDS100 [38], Urban100 [39], and Manga109 [40].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "DIV2K",
        "Flickr2K",
        "Set5",
        "Set14",
        "BSDS100",
        "Urban100",
        "Manga109"
      ],
      "dataset_descriptions": {
        "DIV2K": "Used for training single image super-resolution models, providing high-quality images for upscaling.",
        "Flickr2K": "Used for training single image super-resolution models, offering a diverse set of images for robust model training.",
        "ImageNet": "Used for training and evaluating single image super-resolution models, providing a large and diverse set of images.",
        "Set5": "Used for evaluating single image super-resolution models, focusing on a small but widely recognized set of images.",
        "Set14": "Used for evaluating single image super-resolution models, providing a moderate-sized set of images for benchmarking.",
        "BSDS100": "Used for evaluating single image super-resolution models, containing a variety of natural images for performance assessment.",
        "Urban100": "Used for evaluating single image super-resolution models, specifically designed for urban scenes and architectural details.",
        "Manga109": "Used for evaluating single image super-resolution models, focusing on manga images to assess performance on line art and text."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions several datasets used for training and evaluation in the context of single image super-resolution. These datasets are specific and relevant to the research topic.",
      "processing_time": 80.76060271263123,
      "citing_paper_id": "266210445",
      "cited_paper_id": 4493958
    },
    {
      "context_text": "For image SR, we choose DIV2K [34] and Flickr2K [35] as the training set, and we evaluate on Set5 [36], Set14 [37], BSDS100 [38], Urban100 [39], and Manga109 [40].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "DIV2K",
        "Flickr2K",
        "Set5",
        "Set14",
        "BSDS100",
        "Urban100",
        "Manga109"
      ],
      "dataset_descriptions": {
        "DIV2K": "Used for training single image super-resolution models, providing high-quality images for upscaling.",
        "Flickr2K": "Used for training single image super-resolution models, offering a diverse set of images for robust model training.",
        "ImageNet": "Used for training and evaluating single image super-resolution models, providing a large and diverse set of images.",
        "Set5": "Used for evaluating single image super-resolution models, focusing on a small but widely recognized set of images.",
        "Set14": "Used for evaluating single image super-resolution models, providing a moderate-sized set of images for benchmarking.",
        "BSDS100": "Used for evaluating single image super-resolution models, containing a variety of natural images for performance assessment.",
        "Urban100": "Used for evaluating single image super-resolution models, specifically designed for urban scenes and architectural details.",
        "Manga109": "Used for evaluating single image super-resolution models, focusing on manga images to assess performance on line art and text."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions several datasets used for training and evaluation in the context of single image super-resolution. These datasets are specific and relevant to the research topic.",
      "processing_time": 80.76060271263123,
      "citing_paper_id": "266210445",
      "cited_paper_id": 5250573
    },
    {
      "context_text": "For image SR, we choose DIV2K [34] and Flickr2K [35] as the training set, and we evaluate on Set5 [36], Set14 [37], BSDS100 [38], Urban100 [39], and Manga109 [40].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "DIV2K",
        "Flickr2K",
        "Set5",
        "Set14",
        "BSDS100",
        "Urban100",
        "Manga109"
      ],
      "dataset_descriptions": {
        "DIV2K": "Used for training single image super-resolution models, providing high-quality images for upscaling.",
        "Flickr2K": "Used for training single image super-resolution models, offering a diverse set of images for robust model training.",
        "ImageNet": "Used for training and evaluating single image super-resolution models, providing a large and diverse set of images.",
        "Set5": "Used for evaluating single image super-resolution models, focusing on a small but widely recognized set of images.",
        "Set14": "Used for evaluating single image super-resolution models, providing a moderate-sized set of images for benchmarking.",
        "BSDS100": "Used for evaluating single image super-resolution models, containing a variety of natural images for performance assessment.",
        "Urban100": "Used for evaluating single image super-resolution models, specifically designed for urban scenes and architectural details.",
        "Manga109": "Used for evaluating single image super-resolution models, focusing on manga images to assess performance on line art and text."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions several datasets used for training and evaluation in the context of single image super-resolution. These datasets are specific and relevant to the research topic.",
      "processing_time": 80.76060271263123,
      "citing_paper_id": "266210445",
      "cited_paper_id": 8887614
    },
    {
      "context_text": "For color image denoising, training sets consist of DIV2K [34], Flickr2K [35], BSD400 [38], and WED [41], and we have two testing sets: CBSD68 [42] and Urban100 [39].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "DIV2K",
        "Flickr2K",
        "BSD400",
        "WED",
        "CBSD68",
        "Urban100"
      ],
      "dataset_descriptions": {
        "DIV2K": "Used for training color image denoising models, providing high-resolution images for super-resolution tasks.",
        "Flickr2K": "Used for training color image denoising models, offering a diverse set of images from Flickr.",
        "CIFAR-10": "Used for training color image denoising models, containing a variety of natural images.",
        "WED": "Used for training color image denoising models, focusing on web-enhanced images.",
        "CBSD68": "Used for testing color image denoising models, consisting of 68 challenging images for benchmarking.",
        "Urban100": "Used for testing color image denoising models, featuring 100 urban scene images for evaluation."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions several datasets used for training and testing in the domain of color image denoising, which is relevant to the topic of all-in-one image restoration.",
      "processing_time": 78.80677390098572,
      "citing_paper_id": "266210445",
      "cited_paper_id": 484327
    },
    {
      "context_text": "For color image denoising, training sets consist of DIV2K [34], Flickr2K [35], BSD400 [38], and WED [41], and we have two testing sets: CBSD68 [42] and Urban100 [39].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "DIV2K",
        "Flickr2K",
        "BSD400",
        "WED",
        "CBSD68",
        "Urban100"
      ],
      "dataset_descriptions": {
        "DIV2K": "Used for training color image denoising models, providing high-resolution images for super-resolution tasks.",
        "Flickr2K": "Used for training color image denoising models, offering a diverse set of images from Flickr.",
        "CIFAR-10": "Used for training color image denoising models, containing a variety of natural images.",
        "WED": "Used for training color image denoising models, focusing on web-enhanced images.",
        "CBSD68": "Used for testing color image denoising models, consisting of 68 challenging images for benchmarking.",
        "Urban100": "Used for testing color image denoising models, featuring 100 urban scene images for evaluation."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions several datasets used for training and testing in the domain of color image denoising, which is relevant to the topic of all-in-one image restoration.",
      "processing_time": 78.80677390098572,
      "citing_paper_id": "266210445",
      "cited_paper_id": 4493958
    },
    {
      "context_text": "Due to its ill-posed nature, early research efforts [1, 2, 3] typically focus on developing single-task models, with each model handling only one specific degradation.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only general research efforts and models. No verifiable resources are identified.",
      "processing_time": 64.79189538955688,
      "citing_paper_id": "266210445",
      "cited_paper_id": 996788
    },
    {
      "context_text": "Due to its ill-posed nature, early research efforts [1, 2, 3] typically focus on developing single-task models, with each model handling only one specific degradation.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only general research efforts and models. No verifiable resources are identified.",
      "processing_time": 64.79189538955688,
      "citing_paper_id": "266210445",
      "cited_paper_id": 18874645
    },
    {
      "context_text": "Due to its ill-posed nature, early research efforts [1, 2, 3] typically focus on developing single-task models, with each model handling only one specific degradation.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only general research efforts and models. No verifiable resources are identified.",
      "processing_time": 64.79189538955688,
      "citing_paper_id": "266210445",
      "cited_paper_id": 59316941
    },
    {
      "context_text": "Due to the challenging ill-posed nature, some early research paradigms typically study each sub-task in image restoration independently and have recently achieved favorable progress in their respective fields [13, 14, 15, 2].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only general research paradigms and sub-tasks in image restoration. No verifiable resources are identified.",
      "processing_time": 66.5758445262909,
      "citing_paper_id": "266210445",
      "cited_paper_id": 996788
    },
    {
      "context_text": "Due to the challenging ill-posed nature, some early research paradigms typically study each sub-task in image restoration independently and have recently achieved favorable progress in their respective fields [13, 14, 15, 2].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only general research paradigms and sub-tasks in image restoration. No verifiable resources are identified.",
      "processing_time": 66.5758445262909,
      "citing_paper_id": "266210445",
      "cited_paper_id": 1900475
    },
    {
      "context_text": "Due to the challenging ill-posed nature, some early research paradigms typically study each sub-task in image restoration independently and have recently achieved favorable progress in their respective fields [13, 14, 15, 2].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only general research paradigms and sub-tasks in image restoration. No verifiable resources are identified.",
      "processing_time": 66.5758445262909,
      "citing_paper_id": "266210445",
      "cited_paper_id": 267938238
    },
    {
      "context_text": "After that, X head is flattened into a 1D sequence on the spatial dimension and is input to the transformer body which contains several stacked transformer blocks with each block containing multiple transformer layers [33].",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (transformer blocks) from the cited paper.",
      "processing_time": 64.30202341079712,
      "citing_paper_id": "266210445",
      "cited_paper_id": 13756489
    },
    {
      "context_text": "A possible solution is to introduce the attention mechanism [33] which has a global receptive field.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a method (attention mechanism).",
      "processing_time": 62.95578455924988,
      "citing_paper_id": "266210445",
      "cited_paper_id": 13756489
    },
    {
      "context_text": "7(a), prepends learnable prompt tokens in the input of one trans-former layer [33].",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (transformer layer).",
      "processing_time": 63.216920375823975,
      "citing_paper_id": "266210445",
      "cited_paper_id": 13756489
    },
    {
      "context_text": "Following the vanilla Adapter design [16], we insert the Adapter both after the Multi-head Self-Attention (MSA) and Multi-Layer Perceptron (MLP).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only architectural components of a neural network model.",
      "processing_time": 63.527178049087524,
      "citing_paper_id": "266210445",
      "cited_paper_id": 59599816
    },
    {
      "context_text": "VPT[9] Adapter[16] LoRA[44] AdaptFor.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only model names which are excluded according to the rules.",
      "processing_time": 64.13065266609192,
      "citing_paper_id": "266210445",
      "cited_paper_id": 59599816
    },
    {
      "context_text": "Since very little work has been done to study PETL in low-level vision, we re-implement the current state-of-the-art PETL methods in this work, such as VPT [9], Adapter [16], LoRA [20], AdaptFormer [7], SSF [23], and FacT [10].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only methods and models. The citation intent is to reference existing research work.",
      "processing_time": 64.88542819023132,
      "citing_paper_id": "266210445",
      "cited_paper_id": 59599816
    },
    {
      "context_text": "Since very little work has been done to study PETL in low-level vision, we re-implement the current state-of-the-art PETL methods in this work, such as VPT [9], Adapter [16], LoRA [20], AdaptFormer [7], SSF [23], and FacT [10].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only methods and models. The citation intent is to reference existing research work.",
      "processing_time": 64.88542819023132,
      "citing_paper_id": "266210445",
      "cited_paper_id": 252918870
    },
    {
      "context_text": "• Adapter [16], shown in Fig.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only a method (Adapter).",
      "processing_time": 62.90042448043823,
      "citing_paper_id": "266210445",
      "cited_paper_id": 59599816
    },
    {
      "context_text": "Adapter [16] employs a bottleneck structure to adapt the pre-trained model.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (adapter) and its structure. No verifiable resources are identified.",
      "processing_time": 64.91446900367737,
      "citing_paper_id": "266210445",
      "cited_paper_id": 59599816
    },
    {
      "context_text": "Parameter efficient transfer learning, which initially came from with NLP [16, 17, 18, 8, 19, 20, 21, 22], aims to catch up with full fine-tuning by training a small number of parameters.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only parameter-efficient transfer learning methods. The cited papers do not provide additional context to identify datasets.",
      "processing_time": 65.60747385025024,
      "citing_paper_id": "266210445",
      "cited_paper_id": 59599816
    },
    {
      "context_text": "Parameter efficient transfer learning, which initially came from with NLP [16, 17, 18, 8, 19, 20, 21, 22], aims to catch up with full fine-tuning by training a small number of parameters.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only parameter-efficient transfer learning methods. The cited papers do not provide additional context to identify datasets.",
      "processing_time": 65.60747385025024,
      "citing_paper_id": "266210445",
      "cited_paper_id": 236493269
    },
    {
      "context_text": "Parameter efficient transfer learning, which initially came from with NLP [16, 17, 18, 8, 19, 20, 21, 22], aims to catch up with full fine-tuning by training a small number of parameters.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only parameter-efficient transfer learning methods. The cited papers do not provide additional context to identify datasets.",
      "processing_time": 65.60747385025024,
      "citing_paper_id": "266210445",
      "cited_paper_id": 238583580
    },
    {
      "context_text": "…are inserted as the input token of transformer layers, and we compare VPT Deep [9] in experiments because of its better performance. ii) Adapter [16], which introduces bottleneck structure placed after Attention and MLP. iii) LoRA [20], which adds parallel sub-networks to learn low-rank…",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods and models. There are no clear identifiers for datasets within the text.",
      "processing_time": 65.21708488464355,
      "citing_paper_id": "266210445",
      "cited_paper_id": 59599816
    },
    {
      "context_text": "7(f), tensorises Vision Transformer [49] and introduces a low-rank approximation to the incremental matrix similar to LoRA.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (Vision Transformer) and a technique (low-rank approximation).",
      "processing_time": 65.21169638633728,
      "citing_paper_id": "266210445",
      "cited_paper_id": 225039882
    },
    {
      "context_text": "To this end, we design preliminary experiments, in which we fine-tune the pre-trained restoration model [11] using existing PETL schemes, and then use Fourier analysis [12] to observe the frequency characteristics of features from these methods.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only methods and models. The context focuses on fine-tuning a pre-trained model and using Fourier analysis.",
      "processing_time": 66.37475633621216,
      "citing_paper_id": "266210445",
      "cited_paper_id": 227239228
    },
    {
      "context_text": "To this end, we design preliminary experiments, in which we fine-tune the pre-trained restoration model [11] using existing PETL schemes, and then use Fourier analysis [12] to observe the frequency characteristics of features from these methods.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only methods and models. The context focuses on fine-tuning a pre-trained model and using Fourier analysis.",
      "processing_time": 66.37475633621216,
      "citing_paper_id": "266210445",
      "cited_paper_id": 246823327
    },
    {
      "context_text": "In order to demonstrate the generalizability of our AdaptIR, we choose IPT [11] and EDT [32] as pre-trained base models to evaluate the performance of different PETL methods.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only pre-trained models. The context focuses on evaluating performance using pre-trained models, not datasets.",
      "processing_time": 65.82657051086426,
      "citing_paper_id": "266210445",
      "cited_paper_id": 227239228
    },
    {
      "context_text": "We use two pre-trained transformer-based restoration models, i.e. , IPT [11] and EDT [32], as the base models to evaluate different PETL methods.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions pre-trained models (IPT and EDT) but does not refer to any specific datasets. The focus is on evaluating different PETL methods using these models.",
      "processing_time": 66.74626731872559,
      "citing_paper_id": "266210445",
      "cited_paper_id": 227239228
    },
    {
      "context_text": "Assume that the hidden dimension of the pre-trained restoration model [11, 32] is d and the dimension of intrinsic space in AdaptIR is d ′ .",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only pre-trained models. There are no verifiable resources that meet the criteria for inclusion.",
      "processing_time": 65.17778325080872,
      "citing_paper_id": "266210445",
      "cited_paper_id": 227239228
    },
    {
      "context_text": "The above experiments employ IPT [11] as the base model.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a model (IPT).",
      "processing_time": 63.278125286102295,
      "citing_paper_id": "266210445",
      "cited_paper_id": 227239228
    },
    {
      "context_text": "2, a typical pre-trained restoration model [11, 32] usually contains one large transformer body as well as task-specific heads and tails.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only pre-trained models and their components. No verifiable resources are identified.",
      "processing_time": 64.73253750801086,
      "citing_paper_id": "266210445",
      "cited_paper_id": 227239228
    },
    {
      "context_text": "He et al. [22] go further to identify all the above three approaches from a unified perspective.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a methodological approach.",
      "processing_time": 62.56684160232544,
      "citing_paper_id": "266210445",
      "cited_paper_id": 238583580
    },
    {
      "context_text": "Recently, this technique has emerged in the field of computer vision with promising results [9, 7, 23, 10, 24, 25, 26, 27, 28, 29].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, models, or methods. It only references a technique that has shown promising results in computer vision.",
      "processing_time": 65.61272358894348,
      "citing_paper_id": "266210445",
      "cited_paper_id": 248834106
    },
    {
      "context_text": "Recently, this technique has emerged in the field of computer vision with promising results [9, 7, 23, 10, 24, 25, 26, 27, 28, 29].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, models, or methods. It only references a technique that has shown promising results in computer vision.",
      "processing_time": 65.61272358894348,
      "citing_paper_id": "266210445",
      "cited_paper_id": 249538657
    },
    {
      "context_text": "Recently, this technique has emerged in the field of computer vision with promising results [9, 7, 23, 10, 24, 25, 26, 27, 28, 29].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, models, or methods. It only references a technique that has shown promising results in computer vision.",
      "processing_time": 65.61272358894348,
      "citing_paper_id": "266210445",
      "cited_paper_id": 252918870
    },
    {
      "context_text": "Recently, this technique has emerged in the field of computer vision with promising results [9, 7, 23, 10, 24, 25, 26, 27, 28, 29].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, models, or methods. It only references a technique that has shown promising results in computer vision.",
      "processing_time": 65.61272358894348,
      "citing_paper_id": "266210445",
      "cited_paper_id": 259144860
    },
    {
      "context_text": "Recently, this technique has emerged in the field of computer vision with promising results [9, 7, 23, 10, 24, 25, 26, 27, 28, 29].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, models, or methods. It only references a technique that has shown promising results in computer vision.",
      "processing_time": 65.61272358894348,
      "citing_paper_id": "266210445",
      "cited_paper_id": 260333877
    },
    {
      "context_text": "Recently, this technique has emerged in the field of computer vision with promising results [9, 7, 23, 10, 24, 25, 26, 27, 28, 29].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, models, or methods. It only references a technique that has shown promising results in computer vision.",
      "processing_time": 65.61272358894348,
      "citing_paper_id": "266210445",
      "cited_paper_id": null
    },
    {
      "context_text": "Moreover, we also include the classic MoE [45, 46, 47], which also employs the multi-branch structure but the design of each branch is the same, to give the impact of the multi-branch structure on the performance.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models or methods. There are no clear identifiers for datasets in the provided context.",
      "processing_time": 65.2455518245697,
      "citing_paper_id": "266210445",
      "cited_paper_id": 249538647
    },
    {
      "context_text": "In addition, NOAH [25] and GLoRA [24] introduce Neural Architecture Search (NAS) to combine different methods.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and models. The context is about combining different methods using Neural Architecture Search (NAS).",
      "processing_time": 65.59595680236816,
      "citing_paper_id": "266210445",
      "cited_paper_id": 249538657
    },
    {
      "context_text": "In addition, NOAH [25] and GLoRA [24] introduce Neural Architecture Search (NAS) to combine different methods.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and models. The context is about combining different methods using Neural Architecture Search (NAS).",
      "processing_time": 65.59595680236816,
      "citing_paper_id": "266210445",
      "cited_paper_id": 259144860
    },
    {
      "context_text": "• SSF [23], shown in Fig.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method or model. The context is too limited to infer any dataset usage.",
      "processing_time": 65.07541418075562,
      "citing_paper_id": "266210445",
      "cited_paper_id": 252918870
    },
    {
      "context_text": "Based on the settings in [23], we place the SSF layer behind all the attention QKV projection, the LayerNorm, and the MLP layers.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only model architecture details. No verifiable resources are identified.",
      "processing_time": 63.78434634208679,
      "citing_paper_id": "266210445",
      "cited_paper_id": 252918870
    },
    {
      "context_text": "…sub-networks to learn low-rank incremental matrices of query and value. iv) AdaptFormer [7], which inserts a tunable module parallel to MLP. v) SSF [23], where learnable scale and shift factors are used to modulate the frozen features. vi) FacT [10], which tensorises a ViT and then decomposes the…",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods and models. There are no clear identifiers for datasets within the text.",
      "processing_time": 65.04584622383118,
      "citing_paper_id": "266210445",
      "cited_paper_id": 252918870
    },
    {
      "context_text": "SSF [23] performs a learnable affine transformation on features of the pre-trained model.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific dataset, only a method (SSF) that performs a learnable affine transformation on features of a pre-trained model.",
      "processing_time": 65.58741545677185,
      "citing_paper_id": "266210445",
      "cited_paper_id": 252918870
    },
    {
      "context_text": "Some attempts also introduce parameterized hypercomplex multiplication layers [21] and re-parameterisation [30] to adapter-based methods.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and approaches. There are no verifiable resources that meet the criteria for inclusion.",
      "processing_time": 65.04022884368896,
      "citing_paper_id": "266210445",
      "cited_paper_id": 256900990
    },
    {
      "context_text": "To improve generalization ability, all-in-one image restoration methods [4, 5, 6] have recently been proposed and have attracted great research interest.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to methods and research interest in all-in-one image restoration.",
      "processing_time": 64.44855856895447,
      "citing_paper_id": "266210445",
      "cited_paper_id": 260107992
    },
    {
      "context_text": "As demonstrated in previous work [6], different restoration tasks prefer certain representations for optimal results, we thus hypothesize that the performance drop occurs when the representation needed to address one specific degradation does not match the homogeneous representation of existing…",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a general discussion about restoration tasks and representations. No clear, verifiable resource names are provided.",
      "processing_time": 65.37644958496094,
      "citing_paper_id": "266210445",
      "cited_paper_id": 260107992
    },
    {
      "context_text": "Recently, all-in-one image restoration [4, 5, 6] has offered a way to improve the generalization of image restoration models.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.0,
      "reasoning": "The citation does not mention any specific datasets, only a method or approach. The context is too generic to infer any specific dataset usage.",
      "processing_time": 65.06415152549744,
      "citing_paper_id": "266210445",
      "cited_paper_id": 260107992
    },
    {
      "context_text": "To restore degraded input image, most of previous works design task specific model for each adverse environment such as underwater [9, 39], low-light [37], rainy day [2,22,26,31,32,42,48], haze [1,10,14,18,20,41,44], and snow [6,7,28,50].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various adverse environments and corresponding papers. No clear, verifiable datasets are identified.",
      "processing_time": 65.21363401412964,
      "citing_paper_id": "264555119",
      "cited_paper_id": 837707
    },
    {
      "context_text": "To restore degraded input image, most of previous works design task specific model for each adverse environment such as underwater [9, 39], low-light [37], rainy day [2,22,26,31,32,42,48], haze [1,10,14,18,20,41,44], and snow [6,7,28,50].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various adverse environments and corresponding papers. No clear, verifiable datasets are identified.",
      "processing_time": 65.21363401412964,
      "citing_paper_id": "264555119",
      "cited_paper_id": 3406592
    },
    {
      "context_text": "To restore degraded input image, most of previous works design task specific model for each adverse environment such as underwater [9, 39], low-light [37], rainy day [2,22,26,31,32,42,48], haze [1,10,14,18,20,41,44], and snow [6,7,28,50].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various adverse environments and corresponding papers. No clear, verifiable datasets are identified.",
      "processing_time": 65.21363401412964,
      "citing_paper_id": "264555119",
      "cited_paper_id": 4539586
    },
    {
      "context_text": "To restore degraded input image, most of previous works design task specific model for each adverse environment such as underwater [9, 39], low-light [37], rainy day [2,22,26,31,32,42,48], haze [1,10,14,18,20,41,44], and snow [6,7,28,50].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various adverse environments and corresponding papers. No clear, verifiable datasets are identified.",
      "processing_time": 65.21363401412964,
      "citing_paper_id": "264555119",
      "cited_paper_id": 15443600
    },
    {
      "context_text": "To restore degraded input image, most of previous works design task specific model for each adverse environment such as underwater [9, 39], low-light [37], rainy day [2,22,26,31,32,42,48], haze [1,10,14,18,20,41,44], and snow [6,7,28,50].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various adverse environments and corresponding papers. No clear, verifiable datasets are identified.",
      "processing_time": 65.21363401412964,
      "citing_paper_id": "264555119",
      "cited_paper_id": 49333383
    },
    {
      "context_text": "To restore degraded input image, most of previous works design task specific model for each adverse environment such as underwater [9, 39], low-light [37], rainy day [2,22,26,31,32,42,48], haze [1,10,14,18,20,41,44], and snow [6,7,28,50].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various adverse environments and corresponding papers. No clear, verifiable datasets are identified.",
      "processing_time": 65.21363401412964,
      "citing_paper_id": "264555119",
      "cited_paper_id": 220835852
    },
    {
      "context_text": "To restore degraded input image, most of previous works design task specific model for each adverse environment such as underwater [9, 39], low-light [37], rainy day [2,22,26,31,32,42,48], haze [1,10,14,18,20,41,44], and snow [6,7,28,50].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various adverse environments and corresponding papers. No clear, verifiable datasets are identified.",
      "processing_time": 65.21363401412964,
      "citing_paper_id": "264555119",
      "cited_paper_id": 221181117
    },
    {
      "context_text": "To restore degraded input image, most of previous works design task specific model for each adverse environment such as underwater [9, 39], low-light [37], rainy day [2,22,26,31,32,42,48], haze [1,10,14,18,20,41,44], and snow [6,7,28,50].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various adverse environments and corresponding papers. No clear, verifiable datasets are identified.",
      "processing_time": 65.21363401412964,
      "citing_paper_id": "264555119",
      "cited_paper_id": 226291870
    },
    {
      "context_text": "To restore degraded input image, most of previous works design task specific model for each adverse environment such as underwater [9, 39], low-light [37], rainy day [2,22,26,31,32,42,48], haze [1,10,14,18,20,41,44], and snow [6,7,28,50].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various adverse environments and corresponding papers. No clear, verifiable datasets are identified.",
      "processing_time": 65.21363401412964,
      "citing_paper_id": "264555119",
      "cited_paper_id": 246904703
    },
    {
      "context_text": "The model is trained and evaluated on All-Weather dataset [36], which contain three kind of weather degradation datasets, i.e. Raindrop dataset [31] for rain-drop, Outdoor-Rain [22] for rain+fog, and Snow100K-L [28] for snow.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "All-Weather dataset",
        "Raindrop dataset",
        "Outdoor-Rain",
        "Snow100K-L"
      ],
      "dataset_descriptions": {
        "All-Weather dataset": "Used to train and evaluate the model on various weather degradations, including rain, fog, and snow, to improve image restoration quality.",
        "Raindrop dataset": "Specifically used to train the model on rain-drop removal, enhancing the ability to restore images degraded by rain.",
        "Outdoors": "Used to evaluate the model's performance on rain and fog degradation, focusing on outdoor scenes to ensure robustness in real-world conditions.",
        "Snow100K-L": "Used to train the model on snow removal, specifically addressing the challenges of heavy snowfall in image restoration."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions specific datasets used for training and evaluation of a model focused on image restoration under various weather conditions.",
      "processing_time": 75.27049970626831,
      "citing_paper_id": "264555119",
      "cited_paper_id": 837707
    },
    {
      "context_text": "The model is trained and evaluated on All-Weather dataset [36], which contain three kind of weather degradation datasets, i.e. Raindrop dataset [31] for rain-drop, Outdoor-Rain [22] for rain+fog, and Snow100K-L [28] for snow.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "All-Weather dataset",
        "Raindrop dataset",
        "Outdoor-Rain",
        "Snow100K-L"
      ],
      "dataset_descriptions": {
        "All-Weather dataset": "Used to train and evaluate the model on various weather degradations, including rain, fog, and snow, to improve image restoration quality.",
        "Raindrop dataset": "Specifically used to train the model on rain-drop removal, enhancing the ability to restore images degraded by rain.",
        "Outdoors": "Used to evaluate the model's performance on rain and fog degradation, focusing on outdoor scenes to ensure robustness in real-world conditions.",
        "Snow100K-L": "Used to train the model on snow removal, specifically addressing the challenges of heavy snowfall in image restoration."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions specific datasets used for training and evaluation of a model focused on image restoration under various weather conditions.",
      "processing_time": 75.27049970626831,
      "citing_paper_id": "264555119",
      "cited_paper_id": 4539586
    },
    {
      "context_text": "The model is trained and evaluated on All-Weather dataset [36], which contain three kind of weather degradation datasets, i.e. Raindrop dataset [31] for rain-drop, Outdoor-Rain [22] for rain+fog, and Snow100K-L [28] for snow.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "All-Weather dataset",
        "Raindrop dataset",
        "Outdoor-Rain",
        "Snow100K-L"
      ],
      "dataset_descriptions": {
        "All-Weather dataset": "Used to train and evaluate the model on various weather degradations, including rain, fog, and snow, to improve image restoration quality.",
        "Raindrop dataset": "Specifically used to train the model on rain-drop removal, enhancing the ability to restore images degraded by rain.",
        "Outdoors": "Used to evaluate the model's performance on rain and fog degradation, focusing on outdoor scenes to ensure robustness in real-world conditions.",
        "Snow100K-L": "Used to train the model on snow removal, specifically addressing the challenges of heavy snowfall in image restoration."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions specific datasets used for training and evaluation of a model focused on image restoration under various weather conditions.",
      "processing_time": 75.27049970626831,
      "citing_paper_id": "264555119",
      "cited_paper_id": 244714491
    },
    {
      "context_text": "Severity Aware Image Restoration Apart from degradation type, degradation severity also bring various appearance and difficulty to restore, many prior works [2, 6, 11, 21,39,48] have been demonstrated the effectiveness of taking degradation severity into account for image restoration.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only prior works that consider degradation severity in image restoration.",
      "processing_time": 64.26816511154175,
      "citing_paper_id": "264555119",
      "cited_paper_id": 3406592
    },
    {
      "context_text": "DID-MDN [48] proposed a rain density aware model and divide rain level to light, medium and heavy rain for better restore images, a similar work also done by [2].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for rain density-aware image restoration.",
      "processing_time": 63.826680183410645,
      "citing_paper_id": "264555119",
      "cited_paper_id": 3406592
    },
    {
      "context_text": "For image deraining [2, 22, 26, 31, 32, 42, 48], JORDER [42] based on additive rain model to jointly learning detect and remove rain streak.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions 'JORDER' which is a method, not a dataset. No other specific datasets are mentioned.",
      "processing_time": 64.88574504852295,
      "citing_paper_id": "264555119",
      "cited_paper_id": 3406592
    },
    {
      "context_text": "For image deraining [2, 22, 26, 31, 32, 42, 48], JORDER [42] based on additive rain model to jointly learning detect and remove rain streak.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions 'JORDER' which is a method, not a dataset. No other specific datasets are mentioned.",
      "processing_time": 64.88574504852295,
      "citing_paper_id": "264555119",
      "cited_paper_id": 4539586
    },
    {
      "context_text": "For image deraining [2, 22, 26, 31, 32, 42, 48], JORDER [42] based on additive rain model to jointly learning detect and remove rain streak.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions 'JORDER' which is a method, not a dataset. No other specific datasets are mentioned.",
      "processing_time": 64.88574504852295,
      "citing_paper_id": "264555119",
      "cited_paper_id": 15443600
    },
    {
      "context_text": "[31] remove raindrop from single image through learning raindrop mask attention.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific dataset names, only a method for raindrop removal.",
      "processing_time": 63.81953501701355,
      "citing_paper_id": "264555119",
      "cited_paper_id": 4539586
    },
    {
      "context_text": "We select fourteen state-of-the-art methods for comparison, include four paradigms, i.e. single task specific (AttentiveGAN [31] and DuRN [26] for raindrop. pix2pix [17] and HRGAN [22] for rain+fog.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only models and methods. No verifiable resources are identified.",
      "processing_time": 64.2546021938324,
      "citing_paper_id": "264555119",
      "cited_paper_id": 4539586
    },
    {
      "context_text": "We select fourteen state-of-the-art methods for comparison, include four paradigms, i.e. single task specific (AttentiveGAN [31] and DuRN [26] for raindrop. pix2pix [17] and HRGAN [22] for rain+fog.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only models and methods. No verifiable resources are identified.",
      "processing_time": 64.2546021938324,
      "citing_paper_id": "264555119",
      "cited_paper_id": 6200260
    },
    {
      "context_text": "Thanks to the success of AdaIN [15] and feature affine transform in image restoration literature [3, 9, 10, 13, 19], we injected the information into model through proposed Degradation Information Local-Global Adaptive Instance Normalization (DI-LGAdaIN), which is inspired by LG-AdaIN [16].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only methods and models. The cited papers' titles do not provide additional dataset names.",
      "processing_time": 65.33542990684509,
      "citing_paper_id": "264555119",
      "cited_paper_id": 6576859
    },
    {
      "context_text": "Thanks to the success of AdaIN [15] and feature affine transform in image restoration literature [3, 9, 10, 13, 19], we injected the information into model through proposed Degradation Information Local-Global Adaptive Instance Normalization (DI-LGAdaIN), which is inspired by LG-AdaIN [16].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only methods and models. The cited papers' titles do not provide additional dataset names.",
      "processing_time": 65.33542990684509,
      "citing_paper_id": "264555119",
      "cited_paper_id": 221802293
    },
    {
      "context_text": "Thanks to the success of AdaIN [15] and feature affine transform in image restoration literature [3, 9, 10, 13, 19], we injected the information into model through proposed Degradation Information Local-Global Adaptive Instance Normalization (DI-LGAdaIN), which is inspired by LG-AdaIN [16].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only methods and models. The cited papers' titles do not provide additional dataset names.",
      "processing_time": 65.33542990684509,
      "citing_paper_id": "264555119",
      "cited_paper_id": 226291870
    },
    {
      "context_text": "Thanks to the success of AdaIN [15] and feature affine transform in image restoration literature [3, 9, 10, 13, 19], we injected the information into model through proposed Degradation Information Local-Global Adaptive Instance Normalization (DI-LGAdaIN), which is inspired by LG-AdaIN [16].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only methods and models. The cited papers' titles do not provide additional dataset names.",
      "processing_time": 65.33542990684509,
      "citing_paper_id": "264555119",
      "cited_paper_id": 246904703
    },
    {
      "context_text": "Note that the ranker is usually guided by MRL [27] in [39,51] rather than directly regress GT IQA score since what we care is not exact IQA value but the ranking information [51].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only methods and approaches. The cited papers' titles do not provide additional dataset names.",
      "processing_time": 65.07593059539795,
      "citing_paper_id": "264555119",
      "cited_paper_id": 6736352
    },
    {
      "context_text": "Note that the ranker is usually guided by MRL [27] in [39,51] rather than directly regress GT IQA score since what we care is not exact IQA value but the ranking information [51].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only methods and approaches. The cited papers' titles do not provide additional dataset names.",
      "processing_time": 65.07593059539795,
      "citing_paper_id": "264555119",
      "cited_paper_id": 201070037
    },
    {
      "context_text": "Since the severity information is only meaningful in latent space, we utilize latent space manipulation as [9, 33] to modulate restoration level.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only the use of latent space manipulation for modulating restoration levels. No verifiable datasets are referenced.",
      "processing_time": 65.5226399898529,
      "citing_paper_id": "264555119",
      "cited_paper_id": 198897678
    },
    {
      "context_text": "Since the severity information is only meaningful in latent space, we utilize latent space manipulation as [9, 33] to modulate restoration level.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only the use of latent space manipulation for modulating restoration levels. No verifiable datasets are referenced.",
      "processing_time": 65.5226399898529,
      "citing_paper_id": "264555119",
      "cited_paper_id": 246904703
    },
    {
      "context_text": "Motivated by previous works [39, 51] that demonstrate the effectiveness of learning a image quality ranker with Marginal Ranking Loss (MRL) to benefit the following image restoration, we train a ranker to predict weather severity information.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a method (Marginal Ranking Loss) and a general concept (image quality ranker).",
      "processing_time": 65.06920385360718,
      "citing_paper_id": "264555119",
      "cited_paper_id": 201070037
    },
    {
      "context_text": "Based on the intuition of severe weather image result in worse IQA score, we follow prior works [39, 51] to learn a image quality ranker to boost learning of image restoration.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (RankSRGAN) and a general concept (image quality ranker).",
      "processing_time": 65.36236834526062,
      "citing_paper_id": "264555119",
      "cited_paper_id": 201070037
    },
    {
      "context_text": "Different from [51] that utilize ranker to provide novel rank-content loss, and [39] leverage ranker to initial model parameter, our ranker is adopted to directly extracted severity information for restoration level signal.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and models. The context focuses on the use of a ranker in the context of image super-resolution and restoration.",
      "processing_time": 66.44147086143494,
      "citing_paper_id": "264555119",
      "cited_paper_id": 201070037
    },
    {
      "context_text": "All-in-one bad weather removal [8,23,29,36,38,52] is thus emerging topic to resolve this issue, which aims to learn an unified model to restore image degraded by different adverse weather conditions.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only the topic of all-in-one bad weather removal. No dataset names are provided in the citation span.",
      "processing_time": 65.63790082931519,
      "citing_paper_id": "264555119",
      "cited_paper_id": 219978541
    },
    {
      "context_text": "All-in-one bad weather removal [8,23,29,36,38,52] is thus emerging topic to resolve this issue, which aims to learn an unified model to restore image degraded by different adverse weather conditions.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only the topic of all-in-one bad weather removal. No dataset names are provided in the citation span.",
      "processing_time": 65.63790082931519,
      "citing_paper_id": "264555119",
      "cited_paper_id": 244714491
    },
    {
      "context_text": "All-in-one bad weather removal [8,23,29,36,38,52] is thus emerging topic to resolve this issue, which aims to learn an unified model to restore image degraded by different adverse weather conditions.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only the topic of all-in-one bad weather removal. No dataset names are provided in the citation span.",
      "processing_time": 65.63790082931519,
      "citing_paper_id": "264555119",
      "cited_paper_id": 251197000
    },
    {
      "context_text": "JSTASR [6] and DDMSNet [50] for snow), single task agnostic (MPR-Net [47], Restormer [45] and SwinIR [25]), non-blindall-in-one (All-in-one network [23] and WGWS-Net [52]), and blind all-in-one (Transweather [36], Unified model [8] and Weatherdiff [29]).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions various models and methods but does not explicitly refer to any specific datasets. The cited paper titles do not provide additional information about datasets.",
      "processing_time": 65.63335585594177,
      "citing_paper_id": "264555119",
      "cited_paper_id": 219978541
    },
    {
      "context_text": "JSTASR [6] and DDMSNet [50] for snow), single task agnostic (MPR-Net [47], Restormer [45] and SwinIR [25]), non-blindall-in-one (All-in-one network [23] and WGWS-Net [52]), and blind all-in-one (Transweather [36], Unified model [8] and Weatherdiff [29]).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions various models and methods but does not explicitly refer to any specific datasets. The cited paper titles do not provide additional information about datasets.",
      "processing_time": 65.63335585594177,
      "citing_paper_id": "264555119",
      "cited_paper_id": 231802205
    },
    {
      "context_text": "JSTASR [6] and DDMSNet [50] for snow), single task agnostic (MPR-Net [47], Restormer [45] and SwinIR [25]), non-blindall-in-one (All-in-one network [23] and WGWS-Net [52]), and blind all-in-one (Transweather [36], Unified model [8] and Weatherdiff [29]).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions various models and methods but does not explicitly refer to any specific datasets. The cited paper titles do not provide additional information about datasets.",
      "processing_time": 65.63335585594177,
      "citing_paper_id": "264555119",
      "cited_paper_id": 251197000
    },
    {
      "context_text": "We align our experiment setting with [23,29,36] for fair comparison.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to other works for experimental settings. No dataset names are provided in the context.",
      "processing_time": 65.33868598937988,
      "citing_paper_id": "264555119",
      "cited_paper_id": 219978541
    },
    {
      "context_text": "We align our experiment setting with [23,29,36] for fair comparison.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to other works for experimental settings. No dataset names are provided in the context.",
      "processing_time": 65.33868598937988,
      "citing_paper_id": "264555119",
      "cited_paper_id": 244714491
    },
    {
      "context_text": "We align our experiment setting with [23,29,36] for fair comparison.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to other works for experimental settings. No dataset names are provided in the context.",
      "processing_time": 65.33868598937988,
      "citing_paper_id": "264555119",
      "cited_paper_id": 251197000
    },
    {
      "context_text": "…challenge, some prior works [8, 19] utilize contrastive learning [12] to separate weather type feature and learning the multi-domain of target distribution, another technique such as classifier [30], multiple weather specific operation/encoder [23, 52], and learnable query [36] are also adopted.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various techniques and methods used in image restoration. No clear, verifiable datasets are identified.",
      "processing_time": 65.62167048454285,
      "citing_paper_id": "264555119",
      "cited_paper_id": 219978541
    },
    {
      "context_text": "…challenge, some prior works [8, 19] utilize contrastive learning [12] to separate weather type feature and learning the multi-domain of target distribution, another technique such as classifier [30], multiple weather specific operation/encoder [23, 52], and learnable query [36] are also adopted.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various techniques and methods used in image restoration. No clear, verifiable datasets are identified.",
      "processing_time": 65.62167048454285,
      "citing_paper_id": "264555119",
      "cited_paper_id": 244714491
    },
    {
      "context_text": "…challenge, some prior works [8, 19] utilize contrastive learning [12] to separate weather type feature and learning the multi-domain of target distribution, another technique such as classifier [30], multiple weather specific operation/encoder [23, 52], and learnable query [36] are also adopted.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various techniques and methods used in image restoration. No clear, verifiable datasets are identified.",
      "processing_time": 65.62167048454285,
      "citing_paper_id": "264555119",
      "cited_paper_id": 260107992
    },
    {
      "context_text": "To confront the first challenge, previous works adopted various techniques for effective and efficient architecture, e.g. NAS [23], ViT [36,38], knowledge distillation [8], de-formable convolution and feature affine [19], FAIG [30], weather general&specific operation [52], and diffusion model [29],…",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only various techniques and methods used in image restoration. No verifiable resources are identified.",
      "processing_time": 65.33241629600525,
      "citing_paper_id": "264555119",
      "cited_paper_id": 219978541
    },
    {
      "context_text": "To confront the first challenge, previous works adopted various techniques for effective and efficient architecture, e.g. NAS [23], ViT [36,38], knowledge distillation [8], de-formable convolution and feature affine [19], FAIG [30], weather general&specific operation [52], and diffusion model [29],…",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only various techniques and methods used in image restoration. No verifiable resources are identified.",
      "processing_time": 65.33241629600525,
      "citing_paper_id": "264555119",
      "cited_paper_id": 244714491
    },
    {
      "context_text": "For non-blind all-in-one image restoration [4,23,52], All-in-one network [23] first proposed task-specific encoders and common decoder based on neural architecture search.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (All-in-one network) and a technique (neural architecture search).",
      "processing_time": 65.39474582672119,
      "citing_paper_id": "264555119",
      "cited_paper_id": 219978541
    },
    {
      "context_text": "It is worthy to note that in some works [23,52], the weather type label is available while testing, namely Non-blind all-in-one image restoration.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method or approach. The context is about the availability of weather type labels during testing, which is not a dataset.",
      "processing_time": 66.44112300872803,
      "citing_paper_id": "264555119",
      "cited_paper_id": 219978541
    },
    {
      "context_text": "The proposed method can outperform the state-of-the-art methods on All-Weather dataset [23,36] subjectively and objectively, and can restore the combined multiple degradation weather without training these type data, but enjoy less parameters compared with other blind all-in-one image restoration…",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "All-Weather dataset"
      ],
      "dataset_descriptions": {
        "All-Weather dataset": "Used to evaluate the proposed method's performance in restoring images degraded by multiple weather conditions, demonstrating both subjective and objective improvements over state-of-the-art methods."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions the 'All-Weather dataset' which is a specific dataset used for evaluating image restoration methods under various weather conditions.",
      "processing_time": 69.76332879066467,
      "citing_paper_id": "264555119",
      "cited_paper_id": 219978541
    },
    {
      "context_text": "The proposed method can outperform the state-of-the-art methods on All-Weather dataset [23,36] subjectively and objectively, and can restore the combined multiple degradation weather without training these type data, but enjoy less parameters compared with other blind all-in-one image restoration…",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "All-Weather dataset"
      ],
      "dataset_descriptions": {
        "All-Weather dataset": "Used to evaluate the proposed method's performance in restoring images degraded by multiple weather conditions, demonstrating both subjective and objective improvements over state-of-the-art methods."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions the 'All-Weather dataset' which is a specific dataset used for evaluating image restoration methods under various weather conditions.",
      "processing_time": 69.76332879066467,
      "citing_paper_id": "264555119",
      "cited_paper_id": 244714491
    },
    {
      "context_text": "ZID [18] based on dual-tree complex wavelet transform.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method or technique.",
      "processing_time": 63.08666944503784,
      "citing_paper_id": "264555119",
      "cited_paper_id": 221181117
    },
    {
      "context_text": "For image dehazing [1, 10, 18, 20, 41, 44], DehazeNet [1] learning to estimate transmission map.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions 'DehazeNet' which is a method for estimating transmission maps in image dehazing. No specific dataset is mentioned.",
      "processing_time": 65.64408612251282,
      "citing_paper_id": "264555119",
      "cited_paper_id": 221181117
    },
    {
      "context_text": "For image dehazing [1, 10, 18, 20, 41, 44], DehazeNet [1] learning to estimate transmission map.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions 'DehazeNet' which is a method for estimating transmission maps in image dehazing. No specific dataset is mentioned.",
      "processing_time": 65.64408612251282,
      "citing_paper_id": "264555119",
      "cited_paper_id": 226291870
    },
    {
      "context_text": "Another trend of research lie in task-agnostic model for general image restoration [5, 24, 25, 35, 45–47].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only a trend in research. No dataset names are present in the context.",
      "processing_time": 65.03755211830139,
      "citing_paper_id": "264555119",
      "cited_paper_id": 231802205
    },
    {
      "context_text": "Another trend of research lie in task-agnostic model for general image restoration [5, 24, 25, 35, 45–47].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only a trend in research. No dataset names are present in the context.",
      "processing_time": 65.03755211830139,
      "citing_paper_id": "264555119",
      "cited_paper_id": 248085491
    },
    {
      "context_text": "Another trend of research lie in task-agnostic model for general image restoration [5, 24, 25, 35, 45–47].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only a trend in research. No dataset names are present in the context.",
      "processing_time": 65.03755211830139,
      "citing_paper_id": "264555119",
      "cited_paper_id": 257255385
    },
    {
      "context_text": "Weather Severity",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, models, or methods. It only mentions 'Weather Severity', which is not a dataset.",
      "processing_time": 65.39633083343506,
      "citing_paper_id": "264555119",
      "cited_paper_id": 238055733
    },
    {
      "context_text": "Despite the fact that learning based method usher in a dramatic growth and success on computer vision in last decade, such as image classification and segmentation, the performance of many high-level vision algorithms usually be degraded while applied in real-world adverse environ-Raindrop",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a general reference to real-world adverse environments, which is too generic.",
      "processing_time": 64.74914717674255,
      "citing_paper_id": "264555119",
      "cited_paper_id": 238055733
    },
    {
      "context_text": "Despite the fact that learning based method usher in a dramatic growth and success on computer vision in last decade, such as image classification and segmentation, the performance of many high-level vision algorithms usually be degraded while applied in real-world adverse environ-Raindrop",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a general reference to real-world adverse environments, which is too generic.",
      "processing_time": 64.74914717674255,
      "citing_paper_id": "264555119",
      "cited_paper_id": 265453871
    },
    {
      "context_text": "To achieve blind all-in-one image restoration [8,19,29,30,36,43,49], Transweather [36] based on transformer model and learnable weather query.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions 'Transweather' but does not specify a dataset. It refers to a method or model, not a dataset. No other specific datasets are mentioned.",
      "processing_time": 66.28832149505615,
      "citing_paper_id": "264555119",
      "cited_paper_id": 244714491
    },
    {
      "context_text": "To achieve blind all-in-one image restoration [8,19,29,30,36,43,49], Transweather [36] based on transformer model and learnable weather query.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions 'Transweather' but does not specify a dataset. It refers to a method or model, not a dataset. No other specific datasets are mentioned.",
      "processing_time": 66.28832149505615,
      "citing_paper_id": "264555119",
      "cited_paper_id": 251197000
    },
    {
      "context_text": "To achieve blind all-in-one image restoration [8,19,29,30,36,43,49], Transweather [36] based on transformer model and learnable weather query.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions 'Transweather' but does not specify a dataset. It refers to a method or model, not a dataset. No other specific datasets are mentioned.",
      "processing_time": 66.28832149505615,
      "citing_paper_id": "264555119",
      "cited_paper_id": 260107992
    },
    {
      "context_text": "To achieve blind all-in-one image restoration [8,19,29,30,36,43,49], Transweather [36] based on transformer model and learnable weather query.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions 'Transweather' but does not specify a dataset. It refers to a method or model, not a dataset. No other specific datasets are mentioned.",
      "processing_time": 66.28832149505615,
      "citing_paper_id": "264555119",
      "cited_paper_id": 260810536
    },
    {
      "context_text": "A more challenging setting would be contrary situation [8,19,30,36] that input image is unknown degradation while inferencing, which is the Blind all-in-one image restoration, and we focus on the latter one in this paper.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a research direction and a challenge setting. No verifiable resources are named.",
      "processing_time": 65.01078605651855,
      "citing_paper_id": "264555119",
      "cited_paper_id": 244714491
    },
    {
      "context_text": "A more challenging setting would be contrary situation [8,19,30,36] that input image is unknown degradation while inferencing, which is the Blind all-in-one image restoration, and we focus on the latter one in this paper.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a research direction and a challenge setting. No verifiable resources are named.",
      "processing_time": 65.01078605651855,
      "citing_paper_id": "264555119",
      "cited_paper_id": 260107992
    },
    {
      "context_text": "…might not be suitable for local degradation, considering degradation appearance mainly depend on weather type, we further utilize Multi-head Cross-Attention (MHCA) [40], that take weather type to serve as query to guide the self-attention feature refinement as [36], the DGCA is depicted in Fig.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and models. The context focuses on the use of Multi-head Cross-Attention (MHCA) and weather types for image restoration.",
      "processing_time": 66.36037373542786,
      "citing_paper_id": "264555119",
      "cited_paper_id": 244714491
    },
    {
      "context_text": "Weatherdiff [29] removal multiple weather degradation based on diffusion model.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for removing weather degradation using a diffusion model.",
      "processing_time": 63.72322487831116,
      "citing_paper_id": "264555119",
      "cited_paper_id": 251197000
    },
    {
      "context_text": "…the first challenge, previous works adopted various techniques for effective and efficient architecture, e.g. NAS [23], ViT [36,38], knowledge distillation [8], de-formable convolution and feature affine [19], FAIG [30], weather general&specific operation [52], and diffusion model [29], etc..",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various techniques and methods used in image restoration. No verifiable resources are identified.",
      "processing_time": 65.09804797172546,
      "citing_paper_id": "264555119",
      "cited_paper_id": 251197000
    },
    {
      "context_text": "…the first challenge, previous works adopted various techniques for effective and efficient architecture, e.g. NAS [23], ViT [36,38], knowledge distillation [8], de-formable convolution and feature affine [19], FAIG [30], weather general&specific operation [52], and diffusion model [29], etc..",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various techniques and methods used in image restoration. No verifiable resources are identified.",
      "processing_time": 65.09804797172546,
      "citing_paper_id": "264555119",
      "cited_paper_id": 260107992
    },
    {
      "context_text": "…average pooling on input feature to obtain quality feature vector and regress IQA score by a simple two-layers MLP. Similar to mapping network in [34] and CBDE in [19], our DIE can be integrated to any task-specific image restoration backbone network, and extend to all-in-one image restoration…",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and approaches. The context focuses on the integration of distortion information into image restoration networks.",
      "processing_time": 65.25801372528076,
      "citing_paper_id": "264555119",
      "cited_paper_id": 251772829
    },
    {
      "context_text": "ADMS [30] utilize FAIG to obtain task specific discriminative filter and synergize with a degradation classifier.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only methods and tools. The cited paper title confirms the focus on methods rather than datasets.",
      "processing_time": 64.97284078598022,
      "citing_paper_id": "264555119",
      "cited_paper_id": 260107992
    },
    {
      "context_text": "Thanks to the success of progressive restoration, we further perform restoration level modulation to demonstrate the representative of extracted severity information.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, models, or methods. It focuses on the concept of progressive restoration and restoration level modulation.",
      "processing_time": 65.009281873703,
      "citing_paper_id": "264555119",
      "cited_paper_id": 267023529
    },
    {
      "context_text": "We start by randomly sampling the attenuation coefficient β from a uniform distribution within the range of [0.3, 0.6] and adopt [41] to estimate the scene depth d s ( x ) .",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific dataset names, only a method for estimating scene depth. The context focuses on the methodology rather than a specific dataset.",
      "processing_time": 64.9396767616272,
      "citing_paper_id": "261884429",
      "cited_paper_id": 15774646
    },
    {
      "context_text": "Images captured in sandstorm weather are often affected by Mie scattering, resulting in low visibility, color degradation, and halo artifacts [1].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general problem affecting images captured in sandstorm weather.",
      "processing_time": 63.044825077056885,
      "citing_paper_id": "261884429",
      "cited_paper_id": 116174761
    },
    {
      "context_text": "The commonly utilized traditional techniques include gamma correction [2], [3], Retinex theory [4]–[6] and CLAHE [7]–[9].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only traditional image processing techniques. No verifiable resources are identified.",
      "processing_time": 63.88305687904358,
      "citing_paper_id": "261884429",
      "cited_paper_id": 201811641
    },
    {
      "context_text": "The commonly utilized traditional techniques include gamma correction [2], [3], Retinex theory [4]–[6] and CLAHE [7]–[9].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only traditional image processing techniques. No verifiable resources are identified.",
      "processing_time": 63.88305687904358,
      "citing_paper_id": "261884429",
      "cited_paper_id": 213782253
    },
    {
      "context_text": "To boost the nonlinear representation ability of the network, we embed a residual group composed of three residual blocks [35] cascaded in high-dimensional latent space.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (residual blocks) used in the network architecture.",
      "processing_time": 63.755295276641846,
      "citing_paper_id": "261884429",
      "cited_paper_id": 206594692
    },
    {
      "context_text": "The DCP theory will fail for sandstorm images, to solve the issue, Gao et al. [23] designed a reversed blue channel prior (RBCP) to calculate middle parameters and restore the clear image.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific, verifiable datasets. It discusses a method (RBCP) for restoring sandstorm images but does not reference a dataset.",
      "processing_time": 65.13548421859741,
      "citing_paper_id": "261884429",
      "cited_paper_id": 212785907
    },
    {
      "context_text": "…first used to balance the degraded color in the red channel and then fused the corrected image with the feature map processed by BDPR. Dhara et al. [20] proposed a cast-adaptive nonlinear transformation for color correcting and introduced a cast-adaptive airlight refinement method to ensure the…",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific, verifiable datasets. It describes methods and techniques but does not reference any named datasets.",
      "processing_time": 64.09187340736389,
      "citing_paper_id": "261884429",
      "cited_paper_id": 225792549
    },
    {
      "context_text": "Huang et al. [31] presented a synthetic strategy for getting paired training sandstorm images.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.3,
      "reasoning": "The citation mentions a synthetic strategy for generating paired training sandstorm images, which suggests the creation of a synthetic dataset. However, no specific dataset name is provided.",
      "processing_time": 64.98122215270996,
      "citing_paper_id": "261884429",
      "cited_paper_id": 226301401
    },
    {
      "context_text": "Following the benchmark, they proposed an image dedusting network with feature fusion [32].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific dataset. It only refers to a method or model, which is not included according to the instructions.",
      "processing_time": 64.34838533401489,
      "citing_paper_id": "261884429",
      "cited_paper_id": 237778076
    },
    {
      "context_text": "Recently, the remarkable success of convolutional neural networks (CNN) [24]–[29] has led to an increasing interest of the researchers in learning-based sandstorm image reconstruction algorithms [16], [30]–[32].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to research papers and a general interest in learning-based sandstorm image reconstruction algorithms.",
      "processing_time": 64.43581795692444,
      "citing_paper_id": "261884429",
      "cited_paper_id": 237778076
    },
    {
      "context_text": "The degradation characteristics of sandstorm images make most model-based sandstorm restoration methods [18]–[22] utilized the step-by-step strategy i.e., they first pre-process the degraded image with color correction and then use the physical model to eliminate the dust by following the process…",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific, verifiable datasets. It only discusses a methodological approach to sandstorm image restoration.",
      "processing_time": 63.86220192909241,
      "citing_paper_id": "261884429",
      "cited_paper_id": 247338236
    },
    {
      "context_text": "Based on DCP theory, Shi et al. [18] introduced a sandstorm removal method, which is composed of red channel-based correction (RCC) and blue channel-based dust particle removal (BDPR) two parts.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for sandstorm removal using red and blue channels.",
      "processing_time": 63.28834271430969,
      "citing_paper_id": "261884429",
      "cited_paper_id": 247338236
    },
    {
      "context_text": "For the denoising task, the WED [45] and BSD400 [46] datasets are combined and used as training data, while Urban100 is used as test data.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "WED",
        "BSD400",
        "Urban100"
      ],
      "dataset_descriptions": {
        "WED": "Used for training in the denoising task, providing diverse images to improve model robustness.",
        "BSD400": "Used for training in the denoising task, providing a diverse set of images to enhance model performance.",
        "Urban100": "Used as test data to evaluate the denoising model's performance on urban scenes."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions specific datasets used for training and testing in the image restoration task. WED and BSD400 are used for training, while Urban100 is used for testing.",
      "processing_time": 70.62194895744324,
      "citing_paper_id": "278141813",
      "cited_paper_id": 4840263
    },
    {
      "context_text": "PIQE [49] and NIQE [50]",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions PIQE and NIQE, which are methods for blind image quality evaluation, not datasets. No specific datasets are mentioned in the context.",
      "processing_time": 63.937315464019775,
      "citing_paper_id": "278141813",
      "cited_paper_id": 6917137
    },
    {
      "context_text": "Finally, for dehazing, the RESIDE [47] dataset is used as training and test data.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "RESIDE"
      ],
      "dataset_descriptions": {
        "RESIDE": "Used for training and testing dehazing models, focusing on improving image clarity and quality in hazy conditions."
      },
      "confidence_score": 1.0,
      "reasoning": "The context clearly mentions the use of the RESIDE dataset for training and testing dehazing models, which is directly relevant to the topic of image restoration.",
      "processing_time": 66.98769021034241,
      "citing_paper_id": "278141813",
      "cited_paper_id": 39760169
    },
    {
      "context_text": "These advantages make CNN very effective in learning the representation of important features of an image, which in turn significantly improves the performance of image restoration methods [30].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only the effectiveness of CNNs in image restoration.",
      "processing_time": 62.57265830039978,
      "citing_paper_id": "278141813",
      "cited_paper_id": 219752103
    },
    {
      "context_text": "Other studies have also developed pre-trained image restoration frameworks with specific outputs for each type of degradation [29].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only pre-trained frameworks for image restoration.",
      "processing_time": 62.443201780319214,
      "citing_paper_id": "278141813",
      "cited_paper_id": 227239228
    },
    {
      "context_text": "For deraining, the Rain13K [43] dataset is used as training data to train the model, while Rain100L [44] is used as test data to evaluate model performance.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Rain13K",
        "Rain100L"
      ],
      "dataset_descriptions": {
        "Rain13K": "Used as training data to train the model for deraining images, focusing on improving the model's ability to remove rain streaks.",
        "Rain100L": "Used as evaluation data to evaluate model performance, specifically evaluating the effectiveness of the deraining model on synthetic rain images."
      },
      "confidence_score": 1.0,
      "reasoning": "The context clearly mentions two datasets, Rain13K and Rain100L, which are used for training and testing, respectively, in the context of image deraining.",
      "processing_time": 69.49123883247375,
      "citing_paper_id": "278141813",
      "cited_paper_id": 231802205
    },
    {
      "context_text": "In addition, some task-aligned models are SwinIR [24], using residual SWIN transformer blocks for deep feature extraction.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions 'SwinIR' but does not refer to it as a dataset. It is described as a model using residual SWIN transformer blocks for deep feature extraction.",
      "processing_time": 64.21936345100403,
      "citing_paper_id": "278141813",
      "cited_paper_id": 235358213
    },
    {
      "context_text": "In addition, some task-aligned models are SwinIR [24], using residual SWIN transformer blocks for deep feature extraction.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions 'SwinIR' but does not refer to it as a dataset. It is described as a model using residual SWIN transformer blocks for deep feature extraction.",
      "processing_time": 64.21936345100403,
      "citing_paper_id": "278141813",
      "cited_paper_id": 237266491
    },
    {
      "context_text": "Uformer [25] introduces locally-enhanced window transformer blocks and multi-scale restoration modulators.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (Uformer) and its components. No verifiable resources are referenced.",
      "processing_time": 63.5870156288147,
      "citing_paper_id": "278141813",
      "cited_paper_id": 244346144
    },
    {
      "context_text": "Restormer [26] combines multi-head attention and a feed-forward network to capture remote pixel interactions.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (Restormer) and its components. No verifiable resources are referenced.",
      "processing_time": 63.611913442611694,
      "citing_paper_id": "278141813",
      "cited_paper_id": 245837508
    },
    {
      "context_text": "Prompting is an efficient [41] and an appropriate method to equip the model with relevant knowledge about the type of degradation to produce better image restoration [14].",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for improving image restoration through prompting.",
      "processing_time": 62.50390100479126,
      "citing_paper_id": "278141813",
      "cited_paper_id": 247618727
    },
    {
      "context_text": "The method proposed by [33] uses two-stage knowledge distillation of task-specific models to handle various weather disturbances.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for handling weather disturbances using knowledge distillation.",
      "processing_time": 62.90754175186157,
      "citing_paper_id": "278141813",
      "cited_paper_id": 250581068
    },
    {
      "context_text": "Many previous studies have focused on developing methods that often handle image degradation separately or are specific to only one type of degradation (single degradation), such as deblurring [1], denoising [2], deraining [3], dehazing [4], and low-light enhancement [5].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various types of image degradation. The cited papers do not provide additional context to identify specific datasets.",
      "processing_time": 63.784913778305054,
      "citing_paper_id": "278141813",
      "cited_paper_id": 251937856
    },
    {
      "context_text": "Many previous studies have focused on developing methods that often handle image degradation separately or are specific to only one type of degradation (single degradation), such as deblurring [1], denoising [2], deraining [3], dehazing [4], and low-light enhancement [5].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various types of image degradation. The cited papers do not provide additional context to identify specific datasets.",
      "processing_time": 63.784913778305054,
      "citing_paper_id": "278141813",
      "cited_paper_id": 260081620
    },
    {
      "context_text": "Many previous studies have focused on developing methods that often handle image degradation separately or are specific to only one type of degradation (single degradation), such as deblurring [1], denoising [2], deraining [3], dehazing [4], and low-light enhancement [5].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various types of image degradation. The cited papers do not provide additional context to identify specific datasets.",
      "processing_time": 63.784913778305054,
      "citing_paper_id": "278141813",
      "cited_paper_id": 269151233
    },
    {
      "context_text": "Finally, in deraining, DPNet [22] uses transfer learning and frequency domain processing, while UC-former [23] introduces a transformer architecture with channel across attention and multi-scale feature fusion for more efficient and accurate restoration.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions DPNet and UC-former, which are methods/models, not datasets. No specific datasets are mentioned or used in the described research.",
      "processing_time": 63.70673394203186,
      "citing_paper_id": "278141813",
      "cited_paper_id": 252257747
    },
    {
      "context_text": "Finally, in deraining, DPNet [22] uses transfer learning and frequency domain processing, while UC-former [23] introduces a transformer architecture with channel across attention and multi-scale feature fusion for more efficient and accurate restoration.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions DPNet and UC-former, which are methods/models, not datasets. No specific datasets are mentioned or used in the described research.",
      "processing_time": 63.70673394203186,
      "citing_paper_id": "278141813",
      "cited_paper_id": 271633341
    },
    {
      "context_text": "In image dehazing, HEDehazeNet [16] produces a diverse transmission map, LID-Net [17] uses a multi-scale lightweight architecture, and DADRNet [18] utilizes domain adaptation and disentangled representation.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions methods and architectures (HEDehazeNet, LID-Net, DADRNet) but does not reference any specific datasets. The cited papers' titles confirm these are methods, not datasets.",
      "processing_time": 64.764728307724,
      "citing_paper_id": "278141813",
      "cited_paper_id": 254044390
    },
    {
      "context_text": "In image dehazing, HEDehazeNet [16] produces a diverse transmission map, LID-Net [17] uses a multi-scale lightweight architecture, and DADRNet [18] utilizes domain adaptation and disentangled representation.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions methods and architectures (HEDehazeNet, LID-Net, DADRNet) but does not reference any specific datasets. The cited papers' titles confirm these are methods, not datasets.",
      "processing_time": 64.764728307724,
      "citing_paper_id": "278141813",
      "cited_paper_id": 258494659
    },
    {
      "context_text": "In image dehazing, HEDehazeNet [16] produces a diverse transmission map, LID-Net [17] uses a multi-scale lightweight architecture, and DADRNet [18] utilizes domain adaptation and disentangled representation.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions methods and architectures (HEDehazeNet, LID-Net, DADRNet) but does not reference any specific datasets. The cited papers' titles confirm these are methods, not datasets.",
      "processing_time": 64.764728307724,
      "citing_paper_id": "278141813",
      "cited_paper_id": 271177978
    },
    {
      "context_text": "In image dehazing, HEDehazeNet [16] produces a diverse transmission map, LID-Net [17] uses a multi-scale lightweight architecture, and DADRNet [18] utilizes domain adaptation and disentangled representation.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions methods and architectures (HEDehazeNet, LID-Net, DADRNet) but does not reference any specific datasets. The cited papers' titles confirm these are methods, not datasets.",
      "processing_time": 64.764728307724,
      "citing_paper_id": "278141813",
      "cited_paper_id": 272018886
    },
    {
      "context_text": "The approaches used are also diverse, such as the use of domain translation [8] with multi-attentive feature learning and progressive multi-domain deformable alignment (PMDA), and IDR [9] which carries the concept of ingredient-oriented to improve scalability.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only methods and approaches. The cited papers' titles do not provide additional dataset information.",
      "processing_time": 62.99727416038513,
      "citing_paper_id": "278141813",
      "cited_paper_id": 260085391
    },
    {
      "context_text": "The approaches used are also diverse, such as the use of domain translation [8] with multi-attentive feature learning and progressive multi-domain deformable alignment (PMDA), and IDR [9] which carries the concept of ingredient-oriented to improve scalability.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only methods and approaches. The cited papers' titles do not provide additional dataset information.",
      "processing_time": 62.99727416038513,
      "citing_paper_id": "278141813",
      "cited_paper_id": 267023529
    },
    {
      "context_text": "While both approaches have yielded very effective results, they have difficulties in real-world scenarios due to their limited generalization ability [6].",
      "catation_intent": "limitation",
      "resource_type": "limitation",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a limitation of certain approaches in real-world scenarios.",
      "processing_time": 62.44980335235596,
      "citing_paper_id": "278141813",
      "cited_paper_id": 260271048
    },
    {
      "context_text": "AIRFormer [6] uses transformers with selective spatial frequency processing through the frequency-guided encoder and frequency-refined decoder to handle weather-induced degradation.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (transformers with selective spatial frequency processing).",
      "processing_time": 62.53895306587219,
      "citing_paper_id": "278141813",
      "cited_paper_id": 260271048
    },
    {
      "context_text": "There has been an increasing research interest in developing all-in-one image restoration methods capable of handling different types of degradation with a single model [7].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general trend in research. No verifiable resources are identified.",
      "processing_time": 62.47850036621094,
      "citing_paper_id": "278141813",
      "cited_paper_id": 260680793
    },
    {
      "context_text": "The convolutional block, as demonstrated in NAFNet [42] and CAPTNet [13], offers proven effectiveness and efficiency for image processing tasks.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models (NAFNet and CAPTNet). The context focuses on the effectiveness and efficiency of the convolutional block in image processing tasks.",
      "processing_time": 63.733339071273804,
      "citing_paper_id": "278141813",
      "cited_paper_id": 261556765
    },
    {
      "context_text": "One is by combining prompt learning with the convolution block [13], which is expected to produce an image restoration model that is lower in computational cost and complexity without sacrificing performance.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method combining prompt learning with convolution blocks for image restoration.",
      "processing_time": 62.23001217842102,
      "citing_paper_id": "278141813",
      "cited_paper_id": 261556765
    },
    {
      "context_text": "The convolution block was adopted from CAPTNet [13], namely the Nonlinear Activation Free Block (NAFBlock) developed by Chen et al. [42].",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (NAFBlock) from CAPTNet. No datasets are referenced for image restoration or other purposes.",
      "processing_time": 63.05556774139404,
      "citing_paper_id": "278141813",
      "cited_paper_id": 261556765
    },
    {
      "context_text": "This study offers modifications and developments to the PromptIR architecture [14], namely offering a hybrid approach that combines the advantages of the convolution block (Conv Block) of CAPTNet [13] based on Nonlinear Activation Free Block (NAFBlock) [42] with the framework of PromptIR [14].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models and architectures. There are no clear identifiers for datasets in the provided context.",
      "processing_time": 62.52239441871643,
      "citing_paper_id": "278141813",
      "cited_paper_id": 261556765
    },
    {
      "context_text": "Mperceiver [11], DA-CLIP [12], CAPTNet [13], and PromptIR [14] have been examples of all-in-one image restoration models based on prompt learning and have shown awe-inspiring capabilities in handling different types of degradation with one unified model.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions several models but does not refer to any specific datasets. The cited papers' titles do not provide additional information about datasets.",
      "processing_time": 62.783817529678345,
      "citing_paper_id": "278141813",
      "cited_paper_id": 261556765
    },
    {
      "context_text": "Mperceiver [11], DA-CLIP [12], CAPTNet [13], and PromptIR [14] have been examples of all-in-one image restoration models based on prompt learning and have shown awe-inspiring capabilities in handling different types of degradation with one unified model.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions several models but does not refer to any specific datasets. The cited papers' titles do not provide additional information about datasets.",
      "processing_time": 62.783817529678345,
      "citing_paper_id": "278141813",
      "cited_paper_id": 263605463
    },
    {
      "context_text": "Mperceiver [11], DA-CLIP [12], CAPTNet [13], and PromptIR [14] have been examples of all-in-one image restoration models based on prompt learning and have shown awe-inspiring capabilities in handling different types of degradation with one unified model.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions several models but does not refer to any specific datasets. The cited papers' titles do not provide additional information about datasets.",
      "processing_time": 62.783817529678345,
      "citing_paper_id": "278141813",
      "cited_paper_id": 265659287
    },
    {
      "context_text": "This study evaluates the effectiveness of the convolution block [13] in improving the computational efficiency of the model.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (convolution block).",
      "processing_time": 61.63629746437073,
      "citing_paper_id": "278141813",
      "cited_paper_id": 261556765
    },
    {
      "context_text": "[13] to extract local features more effectively in haze images.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for extracting local features in haze images.",
      "processing_time": 61.86114454269409,
      "citing_paper_id": "278141813",
      "cited_paper_id": 261556765
    },
    {
      "context_text": "The measurement of the number of FLOPs and parameters will show how much computational cost and model complexity reduction is achieved by replacing the transformer block with the convolution block [13].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a comparison of computational costs and model complexity between transformer and convolution blocks.",
      "processing_time": 62.30928587913513,
      "citing_paper_id": "278141813",
      "cited_paper_id": 261556765
    },
    {
      "context_text": "…restoration is Mperceiver [11], which uses multimodal prompt learning (textual and visual) with Stable Diffusion to improve adaptation and restoration quality, then DA-CLIP [12] utilizes the vision-language model (CLIP) and prompt learning to guide restoration based on degradation information.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only models and methods. No dataset names are present in the text.",
      "processing_time": 62.484814405441284,
      "citing_paper_id": "278141813",
      "cited_paper_id": 263605463
    },
    {
      "context_text": "Several all-in-one method that utilizes prompt learning for image restoration is Mperceiver [11], which uses multimodal prompt learning (textual and visual) with Stable Diffusion to improve adaptation and restoration quality, then DA-CLIP [12] utilizes the vision-language model (CLIP) and prompt…",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only models and methods. No dataset names are present in the citation span.",
      "processing_time": 62.481855392456055,
      "citing_paper_id": "278141813",
      "cited_paper_id": 265659287
    },
    {
      "context_text": "AirNet [31] uses contrastive learning and deformable convolution but has a high computational cost, so U-WADN [32] was developed with an adaptive backbone width.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models and methods. The context focuses on the development and comparison of neural network architectures.",
      "processing_time": 62.58277368545532,
      "citing_paper_id": "278141813",
      "cited_paper_id": 267199899
    },
    {
      "context_text": "In the field of remote sensing, Gao et al. [40] applies source-free domain adaptation segmentation for remote sensing images that focuses on the use of vision foundation models and prompt learning without the need for direct source data.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for domain adaptation in remote sensing images.",
      "processing_time": 62.049665451049805,
      "citing_paper_id": "278141813",
      "cited_paper_id": 271029800
    },
    {
      "context_text": "…they occur sequentially in most real-world cases: (1) degradations in the scene ( e.g ., low light and rain) [45, 53, 57], (2) degradations introduced by the imaging process ( e.g ., noise and blur) [17, 44, 80], and (3) degradations caused by post-processings ( e.g ., JPEG compression) [43, 56].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only types of image degradations. No dataset names are present in the text.",
      "processing_time": 62.50607180595398,
      "citing_paper_id": "276938207",
      "cited_paper_id": 1665683
    },
    {
      "context_text": "…they occur sequentially in most real-world cases: (1) degradations in the scene ( e.g ., low light and rain) [45, 53, 57], (2) degradations introduced by the imaging process ( e.g ., noise and blur) [17, 44, 80], and (3) degradations caused by post-processings ( e.g ., JPEG compression) [43, 56].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only types of image degradations. No dataset names are present in the text.",
      "processing_time": 62.50607180595398,
      "citing_paper_id": "276938207",
      "cited_paper_id": 7051992
    },
    {
      "context_text": "Finally , once captured, the image is often post-processed by several information-lossy digital compression techniques like JPEG [56] to reduce its storage requirement.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a compression technique. No verifiable resources are identified.",
      "processing_time": 61.975550174713135,
      "citing_paper_id": "276938207",
      "cited_paper_id": 7051992
    },
    {
      "context_text": "…into three types, and assume that they occur sequentially in most real-world cases: (1) degradations in the scene ( e.g ., low light and rain) [45, 53, 57], (2) degradations introduced by the imaging process ( e.g ., noise and blur) [17, 44, 80], and (3) degradations caused by…",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general types of image degradations. No verifiable resources are identified.",
      "processing_time": 62.499706983566284,
      "citing_paper_id": "276938207",
      "cited_paper_id": 18785401
    },
    {
      "context_text": "First , the scene itself introduces inherent degradations due to environmental factors such as low light, rain, and haze [20, 23, 32, 33, 35].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only environmental factors affecting image degradation. The cited papers do not introduce any specific datasets either.",
      "processing_time": 62.75169396400452,
      "citing_paper_id": "276938207",
      "cited_paper_id": 49333383
    },
    {
      "context_text": "First , the scene itself introduces inherent degradations due to environmental factors such as low light, rain, and haze [20, 23, 32, 33, 35].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only environmental factors affecting image degradation. The cited papers do not introduce any specific datasets either.",
      "processing_time": 62.75169396400452,
      "citing_paper_id": "276938207",
      "cited_paper_id": 235390405
    },
    {
      "context_text": "First , the scene itself introduces inherent degradations due to environmental factors such as low light, rain, and haze [20, 23, 32, 33, 35].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only environmental factors affecting image degradation. The cited papers do not introduce any specific datasets either.",
      "processing_time": 62.75169396400452,
      "citing_paper_id": "276938207",
      "cited_paper_id": 266844752
    },
    {
      "context_text": "In the past, a large number of methods have focused on solving single-degradation problems, such as denoising [67, 83], deraining [20, 21], dehazing [18, 23, 52], and super-resolution [8, 59, 65, 76].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various image restoration tasks. No dataset names are provided.",
      "processing_time": 62.3178277015686,
      "citing_paper_id": "276938207",
      "cited_paper_id": 49333383
    },
    {
      "context_text": "In the past, a large number of methods have focused on solving single-degradation problems, such as denoising [67, 83], deraining [20, 21], dehazing [18, 23, 52], and super-resolution [8, 59, 65, 76].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various image restoration tasks. No dataset names are provided.",
      "processing_time": 62.3178277015686,
      "citing_paper_id": "276938207",
      "cited_paper_id": 260704481
    },
    {
      "context_text": "…T-OLED [84] (T-OLED-Val), 85 and 85 low-resolution LQ-HQ pairs from RealSR [6] and DRealSR [62] (RealSR-Val and DRealSR-Val), 75 and 25 rain-degraded LQ-HQ pairs from LHP-Rain [21] and RealRain [38], merged into a single set (LHP-/Real-Rain-Val), and 100 noisy LQ-HQ pairs from SIDD [1] (SIDD-Val).",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "T-OLED-Val",
        "RealSR-Val",
        "DRealSR-Val",
        "LHP-Rain-Val",
        "RealRain-Val",
        "wage data from English professional football"
      ],
      "dataset_descriptions": {
        "T-OLED-Val": "Used for validating low-resolution image restoration, focusing on OLED display quality and resolution enhancement.",
        "RealSR-Val": "Used for validating super-resolution techniques, focusing on real-world low-resolution to high-resolution image pairs.",
        "DRealSR-Val": "Used for validating super-resolution techniques, focusing on real-world low-resolution to high-resolution image pairs.",
        "LHP-Rain-Val": "Used for validating rain removal techniques, focusing on synthetic rain-degraded images.",
        "RealRain-Val": "Used for validating rain removal techniques, focusing on real-world rain-degraded images.",
        "wage data from English professional football": "EXCLUDED: Not relevant to the topic of image restoration."
      },
      "confidence_score": 0.9,
      "reasoning": "The context mentions several datasets used for validation in image restoration tasks, including T-OLED, RealSR, DRealSR, LHP-Rain, RealRain, and SIDD. These datasets are specific and relevant to the topic of all-in-one image restoration.",
      "processing_time": 72.32957291603088,
      "citing_paper_id": "276938207",
      "cited_paper_id": 52059988
    },
    {
      "context_text": "…T-OLED [84] (T-OLED-Val), 85 and 85 low-resolution LQ-HQ pairs from RealSR [6] and DRealSR [62] (RealSR-Val and DRealSR-Val), 75 and 25 rain-degraded LQ-HQ pairs from LHP-Rain [21] and RealRain [38], merged into a single set (LHP-/Real-Rain-Val), and 100 noisy LQ-HQ pairs from SIDD [1] (SIDD-Val).",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "T-OLED-Val",
        "RealSR-Val",
        "DRealSR-Val",
        "LHP-Rain-Val",
        "RealRain-Val",
        "wage data from English professional football"
      ],
      "dataset_descriptions": {
        "T-OLED-Val": "Used for validating low-resolution image restoration, focusing on OLED display quality and resolution enhancement.",
        "RealSR-Val": "Used for validating super-resolution techniques, focusing on real-world low-resolution to high-resolution image pairs.",
        "DRealSR-Val": "Used for validating super-resolution techniques, focusing on real-world low-resolution to high-resolution image pairs.",
        "LHP-Rain-Val": "Used for validating rain removal techniques, focusing on synthetic rain-degraded images.",
        "RealRain-Val": "Used for validating rain removal techniques, focusing on real-world rain-degraded images.",
        "wage data from English professional football": "EXCLUDED: Not relevant to the topic of image restoration."
      },
      "confidence_score": 0.9,
      "reasoning": "The context mentions several datasets used for validation in image restoration tasks, including T-OLED, RealSR, DRealSR, LHP-Rain, RealRain, and SIDD. These datasets are specific and relevant to the topic of all-in-one image restoration.",
      "processing_time": 72.32957291603088,
      "citing_paper_id": "276938207",
      "cited_paper_id": 249625528
    },
    {
      "context_text": "…T-OLED [84] (T-OLED-Val), 85 and 85 low-resolution LQ-HQ pairs from RealSR [6] and DRealSR [62] (RealSR-Val and DRealSR-Val), 75 and 25 rain-degraded LQ-HQ pairs from LHP-Rain [21] and RealRain [38], merged into a single set (LHP-/Real-Rain-Val), and 100 noisy LQ-HQ pairs from SIDD [1] (SIDD-Val).",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "T-OLED-Val",
        "RealSR-Val",
        "DRealSR-Val",
        "LHP-Rain-Val",
        "RealRain-Val",
        "wage data from English professional football"
      ],
      "dataset_descriptions": {
        "T-OLED-Val": "Used for validating low-resolution image restoration, focusing on OLED display quality and resolution enhancement.",
        "RealSR-Val": "Used for validating super-resolution techniques, focusing on real-world low-resolution to high-resolution image pairs.",
        "DRealSR-Val": "Used for validating super-resolution techniques, focusing on real-world low-resolution to high-resolution image pairs.",
        "LHP-Rain-Val": "Used for validating rain removal techniques, focusing on synthetic rain-degraded images.",
        "RealRain-Val": "Used for validating rain removal techniques, focusing on real-world rain-degraded images.",
        "wage data from English professional football": "EXCLUDED: Not relevant to the topic of image restoration."
      },
      "confidence_score": 0.9,
      "reasoning": "The context mentions several datasets used for validation in image restoration tasks, including T-OLED, RealSR, DRealSR, LHP-Rain, RealRain, and SIDD. These datasets are specific and relevant to the topic of all-in-one image restoration.",
      "processing_time": 72.32957291603088,
      "citing_paper_id": "276938207",
      "cited_paper_id": 260704481
    },
    {
      "context_text": "The first set includes 100 paired LQ-HQ image pairs: 10, 10, 15, 15, 10, 20, and 20 pairs from I-Haze [4], NH-Haze [5], DRealSR [62], RealSR [6], T-OLED [84], SIDD [1], and LHP-Rain [21], which do not overlap with the validation sets in Tab.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "I-Haze",
        "NH-Haze",
        "DRealSR",
        "RealSR",
        "T-OLED",
        "SIDD",
        "LHP-Rain"
      ],
      "dataset_descriptions": {
        "I-Haze": "Used for evaluating dehazing algorithms, specifically assessing performance on real-world images with varying haze conditions.",
        "NH-Haze": "Used for evaluating dehazing algorithms, specifically addressing non-homogeneous hazy and haze-free images.",
        "DRealSR": "Used for evaluating super-resolution algorithms, specifically focusing on real-world low-resolution images.",
        "RealSR": "Used for evaluating super-resolution algorithms, specifically focusing on real-world low-resolution images.",
        "T-OLED": "Used for evaluating image restoration on under-display camera images, specifically addressing issues unique to OLED displays.",
        "SIDD": "Used for evaluating denoising algorithms, specifically focusing on noise in smartphone camera images.",
        "LHP-Rain": "Used for evaluating rain removal algorithms, specifically addressing heavy rain streaks in real-world images."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions specific datasets used for training and evaluation in image restoration tasks, including dehazing, super-resolution, and denoising.",
      "processing_time": 71.96716833114624,
      "citing_paper_id": "276938207",
      "cited_paper_id": 52059988
    },
    {
      "context_text": "The first set includes 100 paired LQ-HQ image pairs: 10, 10, 15, 15, 10, 20, and 20 pairs from I-Haze [4], NH-Haze [5], DRealSR [62], RealSR [6], T-OLED [84], SIDD [1], and LHP-Rain [21], which do not overlap with the validation sets in Tab.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "I-Haze",
        "NH-Haze",
        "DRealSR",
        "RealSR",
        "T-OLED",
        "SIDD",
        "LHP-Rain"
      ],
      "dataset_descriptions": {
        "I-Haze": "Used for evaluating dehazing algorithms, specifically assessing performance on real-world images with varying haze conditions.",
        "NH-Haze": "Used for evaluating dehazing algorithms, specifically addressing non-homogeneous hazy and haze-free images.",
        "DRealSR": "Used for evaluating super-resolution algorithms, specifically focusing on real-world low-resolution images.",
        "RealSR": "Used for evaluating super-resolution algorithms, specifically focusing on real-world low-resolution images.",
        "T-OLED": "Used for evaluating image restoration on under-display camera images, specifically addressing issues unique to OLED displays.",
        "SIDD": "Used for evaluating denoising algorithms, specifically focusing on noise in smartphone camera images.",
        "LHP-Rain": "Used for evaluating rain removal algorithms, specifically addressing heavy rain streaks in real-world images."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions specific datasets used for training and evaluation in image restoration tasks, including dehazing, super-resolution, and denoising.",
      "processing_time": 71.96716833114624,
      "citing_paper_id": "276938207",
      "cited_paper_id": 212647851
    },
    {
      "context_text": "The first set includes 100 paired LQ-HQ image pairs: 10, 10, 15, 15, 10, 20, and 20 pairs from I-Haze [4], NH-Haze [5], DRealSR [62], RealSR [6], T-OLED [84], SIDD [1], and LHP-Rain [21], which do not overlap with the validation sets in Tab.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "I-Haze",
        "NH-Haze",
        "DRealSR",
        "RealSR",
        "T-OLED",
        "SIDD",
        "LHP-Rain"
      ],
      "dataset_descriptions": {
        "I-Haze": "Used for evaluating dehazing algorithms, specifically assessing performance on real-world images with varying haze conditions.",
        "NH-Haze": "Used for evaluating dehazing algorithms, specifically addressing non-homogeneous hazy and haze-free images.",
        "DRealSR": "Used for evaluating super-resolution algorithms, specifically focusing on real-world low-resolution images.",
        "RealSR": "Used for evaluating super-resolution algorithms, specifically focusing on real-world low-resolution images.",
        "T-OLED": "Used for evaluating image restoration on under-display camera images, specifically addressing issues unique to OLED displays.",
        "SIDD": "Used for evaluating denoising algorithms, specifically focusing on noise in smartphone camera images.",
        "LHP-Rain": "Used for evaluating rain removal algorithms, specifically addressing heavy rain streaks in real-world images."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions specific datasets used for training and evaluation in image restoration tasks, including dehazing, super-resolution, and denoising.",
      "processing_time": 71.96716833114624,
      "citing_paper_id": "276938207",
      "cited_paper_id": 218538083
    },
    {
      "context_text": "The first set includes 100 paired LQ-HQ image pairs: 10, 10, 15, 15, 10, 20, and 20 pairs from I-Haze [4], NH-Haze [5], DRealSR [62], RealSR [6], T-OLED [84], SIDD [1], and LHP-Rain [21], which do not overlap with the validation sets in Tab.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "I-Haze",
        "NH-Haze",
        "DRealSR",
        "RealSR",
        "T-OLED",
        "SIDD",
        "LHP-Rain"
      ],
      "dataset_descriptions": {
        "I-Haze": "Used for evaluating dehazing algorithms, specifically assessing performance on real-world images with varying haze conditions.",
        "NH-Haze": "Used for evaluating dehazing algorithms, specifically addressing non-homogeneous hazy and haze-free images.",
        "DRealSR": "Used for evaluating super-resolution algorithms, specifically focusing on real-world low-resolution images.",
        "RealSR": "Used for evaluating super-resolution algorithms, specifically focusing on real-world low-resolution images.",
        "T-OLED": "Used for evaluating image restoration on under-display camera images, specifically addressing issues unique to OLED displays.",
        "SIDD": "Used for evaluating denoising algorithms, specifically focusing on noise in smartphone camera images.",
        "LHP-Rain": "Used for evaluating rain removal algorithms, specifically addressing heavy rain streaks in real-world images."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions specific datasets used for training and evaluation in image restoration tasks, including dehazing, super-resolution, and denoising.",
      "processing_time": 71.96716833114624,
      "citing_paper_id": "276938207",
      "cited_paper_id": 260704481
    },
    {
      "context_text": "To cover a wide range of degradations, the validation datasets include 50 LQ-HQ pairs captured with under-display cameras in T-OLED [84] (T-OLED-Val), 85 and 85 low-resolution LQ-HQ pairs from RealSR [6] and DRealSR [62] (RealSR-Val and DRealSR-Val), 75 and 25 rain-degraded LQ-HQ pairs from…",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "T-OLED-Val",
        "RealSR-Val",
        "DRealSR-Val"
      ],
      "dataset_descriptions": {
        "T-OLED-Val": "Used to evaluate the performance of all-in-one image restoration methods on under-display camera images, focusing on low-quality (LQ) and high-quality (HQ) pairs.",
        "RealSR-Val": "Used to assess the effectiveness of image restoration techniques on real-world super-resolution tasks, specifically with low-resolution LQ-HQ pairs.",
        "DRealSR-Val": "Used to validate the performance of image restoration algorithms on diverse real-world super-resolution scenarios, using low-resolution LQ-HQ pairs."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions specific datasets used for validation in image restoration, including T-OLED-Val, RealSR-Val, and DRealSR-Val. These datasets are clearly identified and used for evaluating the performance of the restoration methods.",
      "processing_time": 70.04153895378113,
      "citing_paper_id": "276938207",
      "cited_paper_id": 212647851
    },
    {
      "context_text": "Evaluation Metrics for assessing the quality of recovered image results include four full-reference IQA metrics: PSNR, SSIM [60], LPIPS [82], and DISTS [16], as well as the four no-reference IQA metrics: NIQE [81], MANIQA [72], CLIP-IQA [58], and MUSIQ [30].",
      "catation_intent": "findings",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions evaluation metrics but does not refer to any specific datasets. The cited papers are about image quality assessment methods, not datasets.",
      "processing_time": 62.66130185127258,
      "citing_paper_id": "276938207",
      "cited_paper_id": 215785896
    },
    {
      "context_text": "Evaluation Metrics for assessing the quality of recovered image results include four full-reference IQA metrics: PSNR, SSIM [60], LPIPS [82], and DISTS [16], as well as the four no-reference IQA metrics: NIQE [81], MANIQA [72], CLIP-IQA [58], and MUSIQ [30].",
      "catation_intent": "findings",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions evaluation metrics but does not refer to any specific datasets. The cited papers are about image quality assessment methods, not datasets.",
      "processing_time": 62.66130185127258,
      "citing_paper_id": "276938207",
      "cited_paper_id": 251040466
    },
    {
      "context_text": "Evaluation Metrics for assessing the quality of recovered image results include four full-reference IQA metrics: PSNR, SSIM [60], LPIPS [82], and DISTS [16], as well as the four no-reference IQA metrics: NIQE [81], MANIQA [72], CLIP-IQA [58], and MUSIQ [30].",
      "catation_intent": "findings",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions evaluation metrics but does not refer to any specific datasets. The cited papers are about image quality assessment methods, not datasets.",
      "processing_time": 62.66130185127258,
      "citing_paper_id": "276938207",
      "cited_paper_id": 262546756
    },
    {
      "context_text": "To evaluate plans, we employ an extended version of the scoring function in [10], which aggregates multiple image quality assessment (IQA) metrics, including PSNR, SSIM [60], LPIPS [82], DISTS [16], MANIQA [72], CLIP-IQA [58], and MUSIQ [30].",
      "catation_intent": "reusable resource",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions multiple IQA metrics but does not specify any datasets. The metrics are used to evaluate image restoration plans, but no specific datasets are named.",
      "processing_time": 62.88781762123108,
      "citing_paper_id": "276938207",
      "cited_paper_id": 215785896
    },
    {
      "context_text": "To evaluate plans, we employ an extended version of the scoring function in [10], which aggregates multiple image quality assessment (IQA) metrics, including PSNR, SSIM [60], LPIPS [82], DISTS [16], MANIQA [72], CLIP-IQA [58], and MUSIQ [30].",
      "catation_intent": "reusable resource",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions multiple IQA metrics but does not specify any datasets. The metrics are used to evaluate image restoration plans, but no specific datasets are named.",
      "processing_time": 62.88781762123108,
      "citing_paper_id": "276938207",
      "cited_paper_id": 251040466
    },
    {
      "context_text": "To evaluate plans, we employ an extended version of the scoring function in [10], which aggregates multiple image quality assessment (IQA) metrics, including PSNR, SSIM [60], LPIPS [82], DISTS [16], MANIQA [72], CLIP-IQA [58], and MUSIQ [30].",
      "catation_intent": "reusable resource",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions multiple IQA metrics but does not specify any datasets. The metrics are used to evaluate image restoration plans, but no specific datasets are named.",
      "processing_time": 62.88781762123108,
      "citing_paper_id": "276938207",
      "cited_paper_id": 271432364
    },
    {
      "context_text": "…pre-trained models of SwinIR [40], FBCNN [27], DiffBIR [41], Restormer [78], X-Restormer [12], DRBNet [50], DehazeFormer [52], RIDCP [66], MPRNet [77], MAIXM [55], and HAT [11] along with traditional operations including gamma correction, constant shift, and histogram equalization as tools for…",
      "catation_intent": "reusable resource",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only models and methods. There are no clear identifiers for datasets within the text.",
      "processing_time": 62.543360233306885,
      "citing_paper_id": "276938207",
      "cited_paper_id": 231802205
    },
    {
      "context_text": "We refer to this structured degradation sequence as real-world degradation prior , which extends the previous synthesis pipelines [59, 79] by incorporating scene degradations and broadening the scope from blind super-resolution to general IR tasks.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method or model for image degradation. No verifiable resources are identified.",
      "processing_time": 62.21670460700989,
      "citing_paper_id": "276938207",
      "cited_paper_id": 232352764
    },
    {
      "context_text": "For real-world images, we incorporate RetinexFormer [7], DWGAN [19], and CoTF “ Thought ”: “ Given that the image restoration involves denoising, deblurring, and super-resolution with high degradation levels, the priority will be to handle noise and blur before improving the resolution.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only models and methods. The context focuses on the use of RetinexFormer, DWGAN, and CoTF for image restoration tasks.",
      "processing_time": 63.29435348510742,
      "citing_paper_id": "276938207",
      "cited_paper_id": 233296606
    },
    {
      "context_text": "For real-world images, we incorporate RetinexFormer [7], DWGAN [19], and CoTF “ Thought ”: “ Given that the image restoration involves denoising, deblurring, and super-resolution with high degradation levels, the priority will be to handle noise and blur before improving the resolution.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only models and methods. The context focuses on the use of RetinexFormer, DWGAN, and CoTF for image restoration tasks.",
      "processing_time": 63.29435348510742,
      "citing_paper_id": "276938207",
      "cited_paper_id": 257496232
    },
    {
      "context_text": "Specifically, we pre-organize tool information into a set of “registry” forms, each corresponding to a specific tool ( e.g ., SwinIR [40]).",
      "catation_intent": "none",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific, verifiable datasets. It only refers to a method (SwinIR) and a 'registry' form, which is not a dataset.",
      "processing_time": 62.93205666542053,
      "citing_paper_id": "276938207",
      "cited_paper_id": 237266491
    },
    {
      "context_text": "For synthesized datasets, following [85], we employ the pre-trained models of SwinIR [40], FBCNN [27], DiffBIR [41], Restormer [78], X-Restormer [12], DRBNet [50], DehazeFormer [52], RIDCP [66], MPRNet [77], MAIXM [55], and HAT [11] along with traditional operations including gamma correction,…",
      "catation_intent": "reusable resource",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions pre-trained models and traditional operations but does not specify any datasets. The cited papers are about image restoration methods, not datasets.",
      "processing_time": 62.428582429885864,
      "citing_paper_id": "276938207",
      "cited_paper_id": 237266491
    },
    {
      "context_text": "For synthesized datasets, following [85], we employ the pre-trained models of SwinIR [40], FBCNN [27], DiffBIR [41], Restormer [78], X-Restormer [12], DRBNet [50], DehazeFormer [52], RIDCP [66], MPRNet [77], MAIXM [55], and HAT [11] along with traditional operations including gamma correction,…",
      "catation_intent": "reusable resource",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions pre-trained models and traditional operations but does not specify any datasets. The cited papers are about image restoration methods, not datasets.",
      "processing_time": 62.428582429885864,
      "citing_paper_id": "276938207",
      "cited_paper_id": 238215243
    },
    {
      "context_text": "For synthesized datasets, following [85], we employ the pre-trained models of SwinIR [40], FBCNN [27], DiffBIR [41], Restormer [78], X-Restormer [12], DRBNet [50], DehazeFormer [52], RIDCP [66], MPRNet [77], MAIXM [55], and HAT [11] along with traditional operations including gamma correction,…",
      "catation_intent": "reusable resource",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions pre-trained models and traditional operations but does not specify any datasets. The cited papers are about image restoration methods, not datasets.",
      "processing_time": 62.428582429885864,
      "citing_paper_id": "276938207",
      "cited_paper_id": 264289165
    },
    {
      "context_text": "For synthesized datasets, following [85], we employ the pre-trained models of SwinIR [40], FBCNN [27], DiffBIR [41], Restormer [78], X-Restormer [12], DRBNet [50], DehazeFormer [52], RIDCP [66], MPRNet [77], MAIXM [55], and HAT [11] along with traditional operations including gamma correction,…",
      "catation_intent": "reusable resource",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions pre-trained models and traditional operations but does not specify any datasets. The cited papers are about image restoration methods, not datasets.",
      "processing_time": 62.428582429885864,
      "citing_paper_id": "276938207",
      "cited_paper_id": 274435587
    },
    {
      "context_text": "…datasets, following [85], we employ the pre-trained models of SwinIR [40], FBCNN [27], DiffBIR [41], Restormer [78], X-Restormer [12], DRBNet [50], DehazeFormer [52], RIDCP [66], MPRNet [77], MAIXM [55], and HAT [11] along with traditional operations including gamma correction, constant…",
      "catation_intent": "reusable resource",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions several models and methods but does not specify any datasets. The citation is focused on using pre-trained models and traditional operations, not on datasets.",
      "processing_time": 62.63525319099426,
      "citing_paper_id": "276938207",
      "cited_paper_id": 247922539
    },
    {
      "context_text": "Recent research has explored AiO IR methods [3, 14, 29, 32, 34, 42, 46, 48, 69], aiming to develop a unified framework capable of handling multiple degradations.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to research works exploring All-in-One Image Restoration methods.",
      "processing_time": 61.87338376045227,
      "citing_paper_id": "276938207",
      "cited_paper_id": 250551851
    },
    {
      "context_text": "Recent research has explored AiO IR methods [3, 14, 29, 32, 34, 42, 46, 48, 69], aiming to develop a unified framework capable of handling multiple degradations.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to research works exploring All-in-One Image Restoration methods.",
      "processing_time": 61.87338376045227,
      "citing_paper_id": "276938207",
      "cited_paper_id": 266844752
    },
    {
      "context_text": "Recent research has explored AiO IR methods [3, 14, 29, 32, 34, 42, 46, 48, 69], aiming to develop a unified framework capable of handling multiple degradations.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to research works exploring All-in-One Image Restoration methods.",
      "processing_time": 61.87338376045227,
      "citing_paper_id": "276938207",
      "cited_paper_id": 267320695
    },
    {
      "context_text": "Recent research has explored AiO IR methods [3, 14, 29, 32, 34, 42, 46, 48, 69], aiming to develop a unified framework capable of handling multiple degradations.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to research works exploring All-in-One Image Restoration methods.",
      "processing_time": 61.87338376045227,
      "citing_paper_id": "276938207",
      "cited_paper_id": null
    },
    {
      "context_text": "For example, AirNet [34] employs contrastive learning to help the network distinguish image features between different IR tasks and apply the most appropriate processing.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (AirNet) and its application in image restoration. No verifiable dataset names are present in the context.",
      "processing_time": 62.87853789329529,
      "citing_paper_id": "276938207",
      "cited_paper_id": 250551851
    },
    {
      "context_text": "To address this, researchers have developed All-in-One (AiO) IR methods [3, 14, 29, 32, 34, 42, 46, 48, 69] that can handle multiple degradations simultaneously using one unified model.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to methods and approaches. No dataset names are present in the text.",
      "processing_time": 62.225536823272705,
      "citing_paper_id": "276938207",
      "cited_paper_id": 250551851
    },
    {
      "context_text": "To address this, researchers have developed All-in-One (AiO) IR methods [3, 14, 29, 32, 34, 42, 46, 48, 69] that can handle multiple degradations simultaneously using one unified model.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to methods and approaches. No dataset names are present in the text.",
      "processing_time": 62.225536823272705,
      "citing_paper_id": "276938207",
      "cited_paper_id": 266844752
    },
    {
      "context_text": "To address this, researchers have developed All-in-One (AiO) IR methods [3, 14, 29, 32, 34, 42, 46, 48, 69] that can handle multiple degradations simultaneously using one unified model.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to methods and approaches. No dataset names are present in the text.",
      "processing_time": 62.225536823272705,
      "citing_paper_id": "276938207",
      "cited_paper_id": 267320695
    },
    {
      "context_text": "To address this, researchers have developed All-in-One (AiO) IR methods [3, 14, 29, 32, 34, 42, 46, 48, 69] that can handle multiple degradations simultaneously using one unified model.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to methods and approaches. No dataset names are present in the text.",
      "processing_time": 62.225536823272705,
      "citing_paper_id": "276938207",
      "cited_paper_id": null
    },
    {
      "context_text": "In recent years, a growing body of research has explored the use of LLMs as core controllers in autonomous agents [51, 63, 70, 71].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only the use of LLMs in autonomous agents. No verifiable resources are identified.",
      "processing_time": 62.22162175178528,
      "citing_paper_id": "276938207",
      "cited_paper_id": 257833781
    },
    {
      "context_text": "In recent years, a growing body of research has explored the use of LLMs as core controllers in autonomous agents [51, 63, 70, 71].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only the use of LLMs in autonomous agents. No verifiable resources are identified.",
      "processing_time": 62.22162175178528,
      "citing_paper_id": "276938207",
      "cited_paper_id": 258967184
    },
    {
      "context_text": "In recent years, a growing body of research has explored the use of LLMs as core controllers in autonomous agents [51, 63, 70, 71].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only the use of LLMs in autonomous agents. No verifiable resources are identified.",
      "processing_time": 62.22162175178528,
      "citing_paper_id": "276938207",
      "cited_paper_id": 259075577
    },
    {
      "context_text": "The success of large language model (LLM) [2, 54, 68]- based autonomous AI agents [47, 51, 63] in handling complex tasks has inspired researchers [10, 85] to develop intelligent systems to improve the practical applicability of IR methods.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, models, or methods. It only discusses the general impact of large language models on AI agents and IR methods.",
      "processing_time": 62.63310980796814,
      "citing_paper_id": "276938207",
      "cited_paper_id": 257833781
    },
    {
      "context_text": "The success of large language model (LLM) [2, 54, 68]- based autonomous AI agents [47, 51, 63] in handling complex tasks has inspired researchers [10, 85] to develop intelligent systems to improve the practical applicability of IR methods.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, models, or methods. It only discusses the general impact of large language models on AI agents and IR methods.",
      "processing_time": 62.63310980796814,
      "citing_paper_id": "276938207",
      "cited_paper_id": 271432364
    },
    {
      "context_text": "For instance, MetaGPT [25] introduces human role struc-2 tures into multi-agent systems, assigning different respon-sibilities to agents in software development tasks.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method or system called MetaGPT. There are no verifiable resources that meet the criteria for inclusion.",
      "processing_time": 62.571208477020264,
      "citing_paper_id": "276938207",
      "cited_paper_id": 260351380
    },
    {
      "context_text": "To enhance the ability of AI systems to solve complex problems, researchers have designed various multi-agent frameworks [22, 25, 31, 61, 64] that enable agents to specialize, coordinate, and collaborate.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only multi-agent frameworks. There are no verifiable resources that meet the criteria.",
      "processing_time": 62.02407240867615,
      "citing_paper_id": "276938207",
      "cited_paper_id": 260351380
    },
    {
      "context_text": "To enhance the ability of AI systems to solve complex problems, researchers have designed various multi-agent frameworks [22, 25, 31, 61, 64] that enable agents to specialize, coordinate, and collaborate.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only multi-agent frameworks. There are no verifiable resources that meet the criteria.",
      "processing_time": 62.02407240867615,
      "citing_paper_id": "276938207",
      "cited_paper_id": 279000557
    },
    {
      "context_text": "Compared Methods include six AiO IR models: AirNet [46], PromptIR [48], MiOIR [32], DA-CLIP [42], Instruc-tIR [14], and AutoDIR [29], along with AgenticIR [85].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions several models but does not refer to any specific datasets. The names mentioned are models or methods, not datasets.",
      "processing_time": 61.934242725372314,
      "citing_paper_id": "276938207",
      "cited_paper_id": 266844752
    },
    {
      "context_text": "Compared Methods include six AiO IR models: AirNet [46], PromptIR [48], MiOIR [32], DA-CLIP [42], Instruc-tIR [14], and AutoDIR [29], along with AgenticIR [85].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions several models but does not refer to any specific datasets. The names mentioned are models or methods, not datasets.",
      "processing_time": 61.934242725372314,
      "citing_paper_id": "276938207",
      "cited_paper_id": 267320695
    },
    {
      "context_text": "Compared Methods include six AiO IR models: AirNet [46], PromptIR [48], MiOIR [32], DA-CLIP [42], Instruc-tIR [14], and AutoDIR [29], along with AgenticIR [85].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions several models but does not refer to any specific datasets. The names mentioned are models or methods, not datasets.",
      "processing_time": 61.934242725372314,
      "citing_paper_id": "276938207",
      "cited_paper_id": null
    },
    {
      "context_text": "The three synthesized test sets contain 1,440 LQ images processed with 16 combinations of mixed 2 or 3 types of degradations applied to images from MiO100 [32].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "MiO100"
      ],
      "dataset_descriptions": {
        "MiO100": "Used to generate low-quality images with multiple degradations for testing image restoration methods, focusing on sequential and prompt learning strategies."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions 'MiO100' which appears to be a specific dataset used for generating low-quality images with multiple degradations.",
      "processing_time": 64.9909656047821,
      "citing_paper_id": "276938207",
      "cited_paper_id": 266844752
    },
    {
      "context_text": "MiOIR [32] incorporates sequential and prompt learning strategies to enable the network to incrementally learn individual IR tasks in an organized manner.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (MiOIR) and its learning strategies. No verifiable resources are identified.",
      "processing_time": 62.112831354141235,
      "citing_paper_id": "276938207",
      "cited_paper_id": 266844752
    },
    {
      "context_text": "PromptIR [48] and InstructIR [14] introduce additional degradation context to guide the restoration model.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods/models. The context focuses on the introduction of degradation context to guide restoration models.",
      "processing_time": 62.10991621017456,
      "citing_paper_id": "276938207",
      "cited_paper_id": 267320695
    },
    {
      "context_text": "…and degradation removal by finetuning perception and planning MLLMs ( e.g ., LLaVA-Llama3-8B) on pairs of LQ images and tool execution sequences [10], or by directly attempting all tools corresponding to a single degradation [85], they could either require resource-intensive dataset…",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific, verifiable datasets. It only refers to generic 'resource-intensive dataset' without providing a name or specific identifier.",
      "processing_time": 62.34798622131348,
      "citing_paper_id": "276938207",
      "cited_paper_id": 271432364
    },
    {
      "context_text": "RestoreAgent [10] finetunes an MLLM [54] as the perception and planning model of agent on synthetic datasets, enabling autonomous evaluation and tool execution.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.2,
      "reasoning": "The context mentions 'synthetic datasets' but does not provide a specific name. The term 'synthetic datasets' is too generic and lacks a clear identifier.",
      "processing_time": 62.52075982093811,
      "citing_paper_id": "276938207",
      "cited_paper_id": 271432364
    },
    {
      "context_text": "For instance, RestoreAgent [10] finetunes a multi-modal LLM (MLLM) [54] to serve as the perception and planning model of an agent, enabling it to solve complex IR problems step-by-step.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (RestoreAgent) and a model (multi-modal LLM).",
      "processing_time": 61.81786608695984,
      "citing_paper_id": "276938207",
      "cited_paper_id": 271432364
    },
    {
      "context_text": "These metrics are standardized and summed as in [10] to compute an overall score for each recovered image, reflecting the effectiveness of a plan in restoring each given LQ input, with higher scores indicating better restoration performance.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only metrics and scoring methods. No verifiable resources are identified.",
      "processing_time": 61.39748406410217,
      "citing_paper_id": "276938207",
      "cited_paper_id": 271432364
    },
    {
      "context_text": "While current AiO IR networks are more effective than traditional ones designed for single degradations, training them is more challenging due to the potential conflicts among different optimization objectives [10, 28, 85].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only discusses challenges in training all-in-one image restoration networks.",
      "processing_time": 61.561365604400635,
      "citing_paper_id": "276938207",
      "cited_paper_id": 271432364
    },
    {
      "context_text": "While current AiO IR networks are more effective than traditional ones designed for single degradations, training them is more challenging due to the potential conflicts among different optimization objectives [10, 28, 85].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only discusses challenges in training all-in-one image restoration networks.",
      "processing_time": 61.561365604400635,
      "citing_paper_id": "276938207",
      "cited_paper_id": 273502246
    },
    {
      "context_text": "Despite holding potential for autonomous and intelligent IR, existing agentic approaches [10, 85] still suffer from issues in performance and efficiency, as shown in Fig.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only issues with existing agentic approaches in image restoration.",
      "processing_time": 61.55718469619751,
      "citing_paper_id": "276938207",
      "cited_paper_id": 271432364
    },
    {
      "context_text": "…can be expressed as: ( Under the two assumptions about the degradations and restoration tools outlined above, and building on previous works [10, 85], the problem in the context of agentic IR is to determine the optimal selection of tools and their execution order ( i.e ., a plan for their…",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only discusses methodological approaches and assumptions.",
      "processing_time": 61.29998779296875,
      "citing_paper_id": "276938207",
      "cited_paper_id": 271432364
    },
    {
      "context_text": "Compared to existing single-agent approaches [10, 85], as shown in Fig.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, models, or methods. It only refers to existing single-agent approaches without naming any resources.",
      "processing_time": 62.125110387802124,
      "citing_paper_id": "276938207",
      "cited_paper_id": 271432364
    },
    {
      "context_text": "AgenticIR [85] incorporates statistical experience from pre-collected effective tool execution sequences into LLM [26]’s text prompts to guide agent in planning, leading to improved quality and consistency of recovered results.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method or system (GPT-4o) and a general reference to 'pre-collected effective tool execution sequences'. No clear, verifiable dataset names are provided.",
      "processing_time": 63.31813073158264,
      "citing_paper_id": "276938207",
      "cited_paper_id": 273662196
    },
    {
      "context_text": "2 (left), it then processes the LQ image, user instruction, and coarse perception results (in text form) using MLLM GPT-4o [26] to generate a plan that follows our three-stage framework and adheres to the instruction to meet user’s specific needs.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (GPT-4o) which is excluded according to the rules.",
      "processing_time": 61.95869731903076,
      "citing_paper_id": "276938207",
      "cited_paper_id": 273662196
    },
    {
      "context_text": "Traditional deep IR networks [8, 37, 41, 59, 65, 67, 73, 76, 83] are typically designed for specific IR tasks, focusing on single degradations such as rain, haze, noise, blur, and JPEG compression.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general categories of image degradations. No verifiable resources are identified.",
      "processing_time": 61.95732402801514,
      "citing_paper_id": "276938207",
      "cited_paper_id": 274435587
    },
    {
      "context_text": "5-7B [13]), as in [85], to perceive the degradations present in the LQ im-4 age.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a reference to perceiving degradations in low-quality images.",
      "processing_time": 61.386878967285156,
      "citing_paper_id": "276938207",
      "cited_paper_id": null
    },
    {
      "context_text": "DA-CLIP [42] integrates a pre-trained CLIP model [49] within a restoration network to enhance image quality.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (DA-CLIP) that integrates a pre-trained model (CLIP).",
      "processing_time": 61.95132517814636,
      "citing_paper_id": "276938207",
      "cited_paper_id": null
    },
    {
      "context_text": "At the first level, a scheduler employs a perception model De-pictQA [75] (a finetuned MLLM Vicuna-v1.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a method (De-pictQA) which is a finetuned model (Vicuna-v1). No datasets are explicitly referenced.",
      "processing_time": 62.565415143966675,
      "citing_paper_id": "276938207",
      "cited_paper_id": null
    },
    {
      "context_text": "Specifically, we employ the MLLM DepictQA [75] of AgenticIR as the perception model to identify the degradations present in LQ inputs.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (MLLM DepictQA) used for identifying degradations in low-quality inputs.",
      "processing_time": 61.54495286941528,
      "citing_paper_id": "276938207",
      "cited_paper_id": null
    },
    {
      "context_text": "A wide range of methods have emerged, including regression-based techniques [45, 32, 44, 8, 41, 103] and generative model-based pipelines [22, 80, 50, 92, 105], built upon convolutional [14, 101, 100, 79], MLP-based [73], state space models [24, 108, 23, 12], and Vision Transformer (ViT)-based…",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The provided context does not mention any specific datasets, only various methods and models. The cited paper titles also do not provide clear dataset names.",
      "processing_time": 61.6798369884491,
      "citing_paper_id": "278905292",
      "cited_paper_id": 1543021
    },
    {
      "context_text": "A wide range of methods have emerged, including regression-based techniques [45, 32, 44, 8, 41, 103] and generative model-based pipelines [22, 80, 50, 92, 105], built upon convolutional [14, 101, 100, 79], MLP-based [73], state space models [24, 108, 23, 12], and Vision Transformer (ViT)-based…",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The provided context does not mention any specific datasets, only various methods and models. The cited paper titles also do not provide clear dataset names.",
      "processing_time": 61.6798369884491,
      "citing_paper_id": "278905292",
      "cited_paper_id": 229221619
    },
    {
      "context_text": "A wide range of methods have emerged, including regression-based techniques [45, 32, 44, 8, 41, 103] and generative model-based pipelines [22, 80, 50, 92, 105], built upon convolutional [14, 101, 100, 79], MLP-based [73], state space models [24, 108, 23, 12], and Vision Transformer (ViT)-based…",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The provided context does not mention any specific datasets, only various methods and models. The cited paper titles also do not provide clear dataset names.",
      "processing_time": 61.6798369884491,
      "citing_paper_id": "278905292",
      "cited_paper_id": 254125609
    },
    {
      "context_text": "A wide range of methods have emerged, including regression-based techniques [45, 32, 44, 8, 41, 103] and generative model-based pipelines [22, 80, 50, 92, 105], built upon convolutional [14, 101, 100, 79], MLP-based [73], state space models [24, 108, 23, 12], and Vision Transformer (ViT)-based…",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The provided context does not mention any specific datasets, only various methods and models. The cited paper titles also do not provide clear dataset names.",
      "processing_time": 61.6798369884491,
      "citing_paper_id": "278905292",
      "cited_paper_id": 257255385
    },
    {
      "context_text": "Despite the impressive performance of recent state-of-the-art methods, most IR solutions are still designed to address specific degradation types, such as denoising [101, 104], dehazing [69, 84], deraining [29, 66], deblurring [31, 67], and others.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only various image restoration tasks. The cited papers' titles also do not provide specific dataset names.",
      "processing_time": 61.67643356323242,
      "citing_paper_id": "278905292",
      "cited_paper_id": 1900475
    },
    {
      "context_text": "Despite the impressive performance of recent state-of-the-art methods, most IR solutions are still designed to address specific degradation types, such as denoising [101, 104], dehazing [69, 84], deraining [29, 66], deblurring [31, 67], and others.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only various image restoration tasks. The cited papers' titles also do not provide specific dataset names.",
      "processing_time": 61.67643356323242,
      "citing_paper_id": "278905292",
      "cited_paper_id": 59316941
    },
    {
      "context_text": "Despite the impressive performance of recent state-of-the-art methods, most IR solutions are still designed to address specific degradation types, such as denoising [101, 104], dehazing [69, 84], deraining [29, 66], deblurring [31, 67], and others.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only various image restoration tasks. The cited papers' titles also do not provide specific dataset names.",
      "processing_time": 61.67643356323242,
      "citing_paper_id": "278905292",
      "cited_paper_id": 85501306
    },
    {
      "context_text": "Despite the impressive performance of recent state-of-the-art methods, most IR solutions are still designed to address specific degradation types, such as denoising [101, 104], dehazing [69, 84], deraining [29, 66], deblurring [31, 67], and others.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only various image restoration tasks. The cited papers' titles also do not provide specific dataset names.",
      "processing_time": 61.67643356323242,
      "citing_paper_id": "278905292",
      "cited_paper_id": 214623217
    },
    {
      "context_text": "…including regression-based techniques [45, 32, 44, 8, 41, 103] and generative model-based pipelines [22, 80, 50, 92, 105], built upon convolutional [14, 101, 100, 79], MLP-based [73], state space models [24, 108, 23, 12], and Vision Transformer (ViT)-based architectures [44, 65, 41, 95, 17, 46].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various techniques and architectures. The cited paper titles do not provide additional context to identify datasets.",
      "processing_time": 61.74834942817688,
      "citing_paper_id": "278905292",
      "cited_paper_id": 1900475
    },
    {
      "context_text": "…including regression-based techniques [45, 32, 44, 8, 41, 103] and generative model-based pipelines [22, 80, 50, 92, 105], built upon convolutional [14, 101, 100, 79], MLP-based [73], state space models [24, 108, 23, 12], and Vision Transformer (ViT)-based architectures [44, 65, 41, 95, 17, 46].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various techniques and architectures. The cited paper titles do not provide additional context to identify datasets.",
      "processing_time": 61.74834942817688,
      "citing_paper_id": "278905292",
      "cited_paper_id": 4710407
    },
    {
      "context_text": "…including regression-based techniques [45, 32, 44, 8, 41, 103] and generative model-based pipelines [22, 80, 50, 92, 105], built upon convolutional [14, 101, 100, 79], MLP-based [73], state space models [24, 108, 23, 12], and Vision Transformer (ViT)-based architectures [44, 65, 41, 95, 17, 46].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various techniques and architectures. The cited paper titles do not provide additional context to identify datasets.",
      "processing_time": 61.74834942817688,
      "citing_paper_id": "278905292",
      "cited_paper_id": 9569924
    },
    {
      "context_text": "…including regression-based techniques [45, 32, 44, 8, 41, 103] and generative model-based pipelines [22, 80, 50, 92, 105], built upon convolutional [14, 101, 100, 79], MLP-based [73], state space models [24, 108, 23, 12], and Vision Transformer (ViT)-based architectures [44, 65, 41, 95, 17, 46].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various techniques and architectures. The cited paper titles do not provide additional context to identify datasets.",
      "processing_time": 61.74834942817688,
      "citing_paper_id": "278905292",
      "cited_paper_id": 244346144
    },
    {
      "context_text": "…including regression-based techniques [45, 32, 44, 8, 41, 103] and generative model-based pipelines [22, 80, 50, 92, 105], built upon convolutional [14, 101, 100, 79], MLP-based [73], state space models [24, 108, 23, 12], and Vision Transformer (ViT)-based architectures [44, 65, 41, 95, 17, 46].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various techniques and architectures. The cited paper titles do not provide additional context to identify datasets.",
      "processing_time": 61.74834942817688,
      "citing_paper_id": "278905292",
      "cited_paper_id": 257255385
    },
    {
      "context_text": "…including regression-based techniques [45, 32, 44, 8, 41, 103] and generative model-based pipelines [22, 80, 50, 92, 105], built upon convolutional [14, 101, 100, 79], MLP-based [73], state space models [24, 108, 23, 12], and Vision Transformer (ViT)-based architectures [44, 65, 41, 95, 17, 46].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various techniques and architectures. The cited paper titles do not provide additional context to identify datasets.",
      "processing_time": 61.74834942817688,
      "citing_paper_id": "278905292",
      "cited_paper_id": 257365045
    },
    {
      "context_text": "…including regression-based techniques [45, 32, 44, 8, 41, 103] and generative model-based pipelines [22, 80, 50, 92, 105], built upon convolutional [14, 101, 100, 79], MLP-based [73], state space models [24, 108, 23, 12], and Vision Transformer (ViT)-based architectures [44, 65, 41, 95, 17, 46].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various techniques and architectures. The cited paper titles do not provide additional context to identify datasets.",
      "processing_time": 61.74834942817688,
      "citing_paper_id": "278905292",
      "cited_paper_id": 265551773
    },
    {
      "context_text": "…including regression-based techniques [45, 32, 44, 8, 41, 103] and generative model-based pipelines [22, 80, 50, 92, 105], built upon convolutional [14, 101, 100, 79], MLP-based [73], state space models [24, 108, 23, 12], and Vision Transformer (ViT)-based architectures [44, 65, 41, 95, 17, 46].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various techniques and architectures. The cited paper titles do not provide additional context to identify datasets.",
      "processing_time": 61.74834942817688,
      "citing_paper_id": "278905292",
      "cited_paper_id": 267938238
    },
    {
      "context_text": "…settings, we follow the evaluation protocols established in prior works [36, 60, 93], utilizing the following datasets: For image denoising in the single-task setting, we combine the BSD400 [2] and WED [52] datasets, and corrupt the images with Gaussian noise at levels σ ∈ { 15 , 25 , 50 } .",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "BSD400",
        "WED"
      ],
      "dataset_descriptions": {
        "BSD400": "Used for image denoising experiments, evaluating the performance of models trained on images corrupted with Gaussian noise at levels σ ∈ {15, 25, 50}.",
        "WED": "Used for image denoising experiments, evaluating the performance of models trained on images corrupted with Gaussian noise at levels σ ∈ {15, 25, 50}."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions two specific datasets, BSD400 and WED, which are used for image denoising experiments. The datasets are combined and corrupted with Gaussian noise at specified levels.",
      "processing_time": 66.81786680221558,
      "citing_paper_id": "278905292",
      "cited_paper_id": 4840263
    },
    {
      "context_text": "Given its broad importance, IR has been widely applied in numerous applications [70, 86, 3, 42, 94].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general applications of image restoration. No dataset names are present in the text.",
      "processing_time": 61.2124342918396,
      "citing_paper_id": "278905292",
      "cited_paper_id": 8235201
    },
    {
      "context_text": "Given its broad importance, IR has been widely applied in numerous applications [70, 86, 3, 42, 94].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general applications of image restoration. No dataset names are present in the text.",
      "processing_time": 61.2124342918396,
      "citing_paper_id": "278905292",
      "cited_paper_id": 267499648
    },
    {
      "context_text": "The architecture adopts a robust 4-level encoder-decoder structure, with a varying number of Mixed Degradation Attention Blocks (MDAB) at each level—specifically [3 , 5 , 5 , 7] from highest to lowest resolution in the Tiny variant.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only the architecture details of the model. There are no verifiable resources or datasets mentioned.",
      "processing_time": 61.29305934906006,
      "citing_paper_id": "278905292",
      "cited_paper_id": 8235201
    },
    {
      "context_text": "We evaluate the denoising performance on BSD68 [54] and Urban100 [26].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "BSD68",
        "Urban100"
      ],
      "dataset_descriptions": {
        "BSD68": "Used to evaluate denoising performance, focusing on image restoration techniques in image processing.",
        "Urban100": "Used to assess denoising effectiveness, specifically in urban scene images, emphasizing high-resolution details."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions two specific datasets, BSD68 and Urban100, which are used to evaluate denoising performance. These are well-known datasets in the field of image restoration.",
      "processing_time": 65.09236645698547,
      "citing_paper_id": "278905292",
      "cited_paper_id": 8282555
    },
    {
      "context_text": "For single-task dehazing, we adopt the SOTS dataset [35], consisting of 72,135 training images and 500 testing images.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "SOTS"
      ],
      "dataset_descriptions": {
        "SOTS": "Used for single-task dehazing, specifically training and testing dehazing models with 72,135 training images and 500 testing images."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions the SOTS dataset, which is a specific dataset used for single-task dehazing. The dataset is clearly identified and its usage is described.",
      "processing_time": 64.11645078659058,
      "citing_paper_id": "278905292",
      "cited_paper_id": 39760169
    },
    {
      "context_text": "For the zero-shot underwater image enhancement setting, we follow the evaluation protocol of DCPT [28] by directly applying our model, trained under the 5-degradation setting, on the UIEB dataset [37] without any finetuning.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "UIEB"
      ],
      "dataset_descriptions": {
        "UIEB": "Used to evaluate zero-shot underwater image enhancement, applying a model trained on a 5-degradation setting without finetuning."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions the UIEB dataset, which is used for evaluating the zero-shot underwater image enhancement setting without any finetuning.",
      "processing_time": 63.154024600982666,
      "citing_paper_id": "278905292",
      "cited_paper_id": 58014237
    },
    {
      "context_text": "The training set, referred to as “AllWeather”, is composed of images from the Snow100K [49], Raindrop [62], and Outdoor-Rain [39] datasets.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Snow100K",
        "Raindrop",
        "Outdoor-Rain"
      ],
      "dataset_descriptions": {
        "Snow100K": "Used to train the AllWeather model for snow image restoration, providing diverse and realistic images of degraded images.",
        "Raindrop": "Used to train the AllWeather model for raindrop removal, offering images with varying degrees of raindrop occlusions.",
        "Outdoor-Rain": "Used to train the AllWeather model for rain image restoration, supplying real-world outdoor images affected by rain."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions specific datasets used for composing the 'AllWeather' training set, which are relevant to image restoration under various weather conditions.",
      "processing_time": 65.93999886512756,
      "citing_paper_id": "278905292",
      "cited_paper_id": 131773964
    },
    {
      "context_text": "In this work, we revisit the widely observed redundancy in attention mechanisms, particularly along the channel dimension [76, 27, 15].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only references to attention mechanisms in neural networks.",
      "processing_time": 60.38759756088257,
      "citing_paper_id": "278905292",
      "cited_paper_id": 232134936
    },
    {
      "context_text": "Image Restoration (IR) is a fundamental yet challenging problem in computer vision, aiming to reconstruct clean images from their degraded versions affected by a variety of real-world corruptions, including noise, blur, haze, rain, low-light conditions, and more [95, 41, 64, 93, 53, 60].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general problems in image restoration. No dataset names are present in the text.",
      "processing_time": 61.03418278694153,
      "citing_paper_id": "278905292",
      "cited_paper_id": 244346144
    },
    {
      "context_text": "Image Restoration (IR) is a fundamental yet challenging problem in computer vision, aiming to reconstruct clean images from their degraded versions affected by a variety of real-world corruptions, including noise, blur, haze, rain, low-light conditions, and more [95, 41, 64, 93, 53, 60].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general problems in image restoration. No dataset names are present in the text.",
      "processing_time": 61.03418278694153,
      "citing_paper_id": "278905292",
      "cited_paper_id": 257255385
    },
    {
      "context_text": "Image Restoration (IR) is a fundamental yet challenging problem in computer vision, aiming to reconstruct clean images from their degraded versions affected by a variety of real-world corruptions, including noise, blur, haze, rain, low-light conditions, and more [95, 41, 64, 93, 53, 60].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general problems in image restoration. No dataset names are present in the text.",
      "processing_time": 61.03418278694153,
      "citing_paper_id": "278905292",
      "cited_paper_id": 268064460
    },
    {
      "context_text": "Image Restoration (IR) is a fundamental yet challenging problem in computer vision, aiming to reconstruct clean images from their degraded versions affected by a variety of real-world corruptions, including noise, blur, haze, rain, low-light conditions, and more [95, 41, 64, 93, 53, 60].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general problems in image restoration. No dataset names are present in the text.",
      "processing_time": 61.03418278694153,
      "citing_paper_id": "278905292",
      "cited_paper_id": 271909255
    },
    {
      "context_text": "Image Restoration (IR) is a fundamental yet challenging problem in computer vision, aiming to reconstruct clean images from their degraded versions affected by a variety of real-world corruptions, including noise, blur, haze, rain, low-light conditions, and more [95, 41, 64, 93, 53, 60].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general problems in image restoration. No dataset names are present in the text.",
      "processing_time": 61.03418278694153,
      "citing_paper_id": "278905292",
      "cited_paper_id": 274305801
    },
    {
      "context_text": "Redundancy has long been recognized as a fundamental limitation in multi-head self-attention (MHA), the core building block of Transformers, in both NLP and vision domains [58, 57, 85, 4, 78, 76].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only references to limitations in multi-head self-attention in Transformers. No verifiable resources are identified.",
      "processing_time": 61.1713445186615,
      "citing_paper_id": "278905292",
      "cited_paper_id": 249626057
    },
    {
      "context_text": "Redundancy has long been recognized as a fundamental limitation in multi-head self-attention (MHA), the core building block of Transformers, in both NLP and vision domains [58, 57, 85, 4, 78, 76].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only references to limitations in multi-head self-attention in Transformers. No verifiable resources are identified.",
      "processing_time": 61.1713445186615,
      "citing_paper_id": "278905292",
      "cited_paper_id": 269761529
    },
    {
      "context_text": "Redundancy has long been recognized as a fundamental limitation in multi-head self-attention (MHA), the core building block of Transformers, in both NLP and vision domains [58, 57, 85, 4, 78, 76].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only references to limitations in multi-head self-attention in Transformers. No verifiable resources are identified.",
      "processing_time": 61.1713445186615,
      "citing_paper_id": "278905292",
      "cited_paper_id": 273346034
    },
    {
      "context_text": "We conduct experiments adhering to the protocols of prior general image restoration works [60, 99] under four settings: (i) All-in-One (3Degradations) , (ii) All-in-One (5Degradations) , (iii) Mixed Degradation Setting , (iv) Adverse Weather Removal Setting , and (v) Zero-Shot Setting .",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only experimental settings and protocols. The context is focused on methodologies and experimental setups rather than datasets.",
      "processing_time": 61.084670305252075,
      "citing_paper_id": "278905292",
      "cited_paper_id": 260085391
    },
    {
      "context_text": "We conduct experiments adhering to the protocols of prior general image restoration works [60, 99] under four settings: (i) All-in-One (3Degradations) , (ii) All-in-One (5Degradations) , (iii) Mixed Degradation Setting , (iv) Adverse Weather Removal Setting , and (v) Zero-Shot Setting .",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only experimental settings and protocols. The context is focused on methodologies and experimental setups rather than datasets.",
      "processing_time": 61.084670305252075,
      "citing_paper_id": "278905292",
      "cited_paper_id": 268064460
    },
    {
      "context_text": "Building on this, IDR [99] tackles the problem by decomposing degradations into fundamental physical components and applying a two-stage meta-learning strategy.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (IDR) and its approach to image restoration.",
      "processing_time": 60.68747925758362,
      "citing_paper_id": "278905292",
      "cited_paper_id": 260085391
    },
    {
      "context_text": "Extending the three degradation tasks to include deblurring and low-light enhancement [36, 99], we validate our method’s comprehensive performance in an All-in-One setting.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only degradation tasks. No clear identifiers for datasets are present.",
      "processing_time": 60.52337670326233,
      "citing_paper_id": "278905292",
      "cited_paper_id": 260085391
    },
    {
      "context_text": "…wide range of methods have emerged, including regression-based techniques [45, 32, 44, 8, 41, 103] and generative model-based pipelines [22, 80, 50, 92, 105], built upon convolutional [14, 101, 100, 79], MLP-based [73], state space models [24, 108, 23, 12], and Vision Transformer (ViT)-based…",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various methods and models. There are no clear identifiers for datasets within the text.",
      "processing_time": 60.999319553375244,
      "citing_paper_id": "278905292",
      "cited_paper_id": 260125321
    },
    {
      "context_text": "…strides by designing powerful architectures based on Convolutional Neural Networks (CNNs), Multi-Layer Perceptrons (MLPs), and Transformers [17, 64, 107], most existing methods view restoration as a direct mapping problem—learning a global function that transforms the corrupted input into its…",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only models and methods. There are no clear identifiers for datasets or other verifiable resources.",
      "processing_time": 61.153361320495605,
      "citing_paper_id": "278905292",
      "cited_paper_id": 260125438
    },
    {
      "context_text": "More recently, the prompt-based paradigm [60, 77, 43] has introduced a visual prompt learning module, enabling a single model to better handle diverse degradation types by leveraging the discriminative capacity of learned visual prompts.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a methodological approach using visual prompts for image restoration.",
      "processing_time": 60.59651303291321,
      "citing_paper_id": "278905292",
      "cited_paper_id": 266149517
    },
    {
      "context_text": "More recently, the prompt-based paradigm [60, 77, 43] has introduced a visual prompt learning module, enabling a single model to better handle diverse degradation types by leveraging the discriminative capacity of learned visual prompts.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a methodological approach using visual prompts for image restoration.",
      "processing_time": 60.59651303291321,
      "citing_paper_id": "278905292",
      "cited_paper_id": 268030812
    },
    {
      "context_text": "More recently, the prompt-based paradigm [60, 77, 43] has introduced a visual prompt learning module, enabling a single model to better handle diverse degradation types by leveraging the discriminative capacity of learned visual prompts.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a methodological approach using visual prompts for image restoration.",
      "processing_time": 60.59651303291321,
      "citing_paper_id": "278905292",
      "cited_paper_id": 268064460
    },
    {
      "context_text": "To address this, future work will explore scaling up the model size to be on par with larger architectures such as PromptIR [60], MoCE-IR [93], and AdaIR [11], aiming to further boost performance while maintaining the architectural elegance and efficiency of our design.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models and methods. The context focuses on future work and scaling up model sizes.",
      "processing_time": 60.91375017166138,
      "citing_paper_id": "278905292",
      "cited_paper_id": 268064460
    },
    {
      "context_text": "To address this, future work will explore scaling up the model size to be on par with larger architectures such as PromptIR [60], MoCE-IR [93], and AdaIR [11], aiming to further boost performance while maintaining the architectural elegance and efficiency of our design.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models and methods. The context focuses on future work and scaling up model sizes.",
      "processing_time": 60.91375017166138,
      "citing_paper_id": "278905292",
      "cited_paper_id": 268553835
    },
    {
      "context_text": "For both the All-in-One and single-task settings, we follow the evaluation protocols established in prior works [36, 60, 93], utilizing the following datasets: For image denoising in the single-task setting, we combine the BSD400 [2] and WED [52] datasets, and corrupt the images with Gaussian noise…",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "BSD400",
        "WED"
      ],
      "dataset_descriptions": {
        "BSD400": "Used for evaluating image restoration methods, specifically for image denoising. Images are corrupted with Gaussian noise to test denoising performance.",
        "WED": "Used for evaluating image restoration methods, specifically for image denoising. Images are corrupted with Gaussian noise to test denoising performance."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions two specific datasets, BSD400 and WED, which are used for image denoising in the single-task setting. These datasets are combined and corrupted with Gaussian noise for evaluation.",
      "processing_time": 65.31742525100708,
      "citing_paper_id": "278905292",
      "cited_paper_id": 268064460
    },
    {
      "context_text": "For both the All-in-One and single-task settings, we follow the evaluation protocols established in prior works [36, 60, 93], utilizing the following datasets: For image denoising in the single-task setting, we combine the BSD400 [2] and WED [52] datasets, and corrupt the images with Gaussian noise…",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "BSD400",
        "WED"
      ],
      "dataset_descriptions": {
        "BSD400": "Used for evaluating image restoration methods, specifically for image denoising. Images are corrupted with Gaussian noise to test denoising performance.",
        "WED": "Used for evaluating image restoration methods, specifically for image denoising. Images are corrupted with Gaussian noise to test denoising performance."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions two specific datasets, BSD400 and WED, which are used for image denoising in the single-task setting. These datasets are combined and corrupted with Gaussian noise for evaluation.",
      "processing_time": 65.31742525100708,
      "citing_paper_id": "278905292",
      "cited_paper_id": 274305801
    },
    {
      "context_text": "S effectively leverages degradation-specific features, surpassing Promp-tIR [60], MoCE-IR-S [93], AdaIR [11], and VLU-Net [97] by an average of 1.53dB, 0.6dB, 0.48dB, and 0.57dB with lower parameters.",
      "catation_intent": "findings",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only comparisons to other methods. No dataset names are present in the text.",
      "processing_time": 60.67966413497925,
      "citing_paper_id": "278905292",
      "cited_paper_id": 268064460
    },
    {
      "context_text": "S effectively leverages degradation-specific features, surpassing Promp-tIR [60], MoCE-IR-S [93], AdaIR [11], and VLU-Net [97] by an average of 1.53dB, 0.6dB, 0.48dB, and 0.57dB with lower parameters.",
      "catation_intent": "findings",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only comparisons to other methods. No dataset names are present in the text.",
      "processing_time": 60.67966413497925,
      "citing_paper_id": "278905292",
      "cited_paper_id": 268553835
    },
    {
      "context_text": "S effectively leverages degradation-specific features, surpassing Promp-tIR [60], MoCE-IR-S [93], AdaIR [11], and VLU-Net [97] by an average of 1.53dB, 0.6dB, 0.48dB, and 0.57dB with lower parameters.",
      "catation_intent": "findings",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only comparisons to other methods. No dataset names are present in the text.",
      "processing_time": 60.67966413497925,
      "citing_paper_id": "278905292",
      "cited_paper_id": 274305801
    },
    {
      "context_text": "Notably, even our 6M tiny model outperforms the baseline method Promp-tIR [60] by 0.71dB on average.",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a comparison of performance metrics between models.",
      "processing_time": 60.190967082977295,
      "citing_paper_id": "278905292",
      "cited_paper_id": 268064460
    },
    {
      "context_text": "Specifically, we adopt an ℓ 1 loss that adopted in IR tasks [60, 93, 36, 11, 64], defined as L 1 = | ˆ x − x | 1 , to enforce pixel-wise similarity between the restored image ˆ x and the ground-truth image x .",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only a loss function used in image restoration tasks.",
      "processing_time": 60.35398864746094,
      "citing_paper_id": "278905292",
      "cited_paper_id": 268064460
    },
    {
      "context_text": "Specifically, we adopt an ℓ 1 loss that adopted in IR tasks [60, 93, 36, 11, 64], defined as L 1 = | ˆ x − x | 1 , to enforce pixel-wise similarity between the restored image ˆ x and the ground-truth image x .",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only a loss function used in image restoration tasks.",
      "processing_time": 60.35398864746094,
      "citing_paper_id": "278905292",
      "cited_paper_id": 268553835
    },
    {
      "context_text": "Specifically, we adopt an ℓ 1 loss that adopted in IR tasks [60, 93, 36, 11, 64], defined as L 1 = | ˆ x − x | 1 , to enforce pixel-wise similarity between the restored image ˆ x and the ground-truth image x .",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only a loss function used in image restoration tasks.",
      "processing_time": 60.35398864746094,
      "citing_paper_id": "278905292",
      "cited_paper_id": 274305801
    },
    {
      "context_text": "Following prior works [60, 93], we train the model for 120 epochs with a batch size of 32 in both the 3-Degradation All-in-One and single-task settings.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only training parameters and settings. No verifiable resources are identified.",
      "processing_time": 60.331308126449585,
      "citing_paper_id": "278905292",
      "cited_paper_id": 268064460
    },
    {
      "context_text": "Following prior works [60, 93], we train the model for 120 epochs with a batch size of 32 in both the 3-Degradation All-in-One and single-task settings.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only training parameters and settings. No verifiable resources are identified.",
      "processing_time": 60.331308126449585,
      "citing_paper_id": "278905292",
      "cited_paper_id": 274305801
    },
    {
      "context_text": "However, visual prompt modules often result in increased training time and decreased efficiency [11].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method or model. The context focuses on the efficiency and training time of visual prompt modules.",
      "processing_time": 61.03134608268738,
      "citing_paper_id": "278905292",
      "cited_paper_id": 268553835
    },
    {
      "context_text": "L Fourier , as utilized in MoCE-IR [93, 11], to enhance frequency-domain consistency, the real-valued Fourier loss, is defined as: where ˆ x and x denote the restored and ground-truth images, respectively.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a method (Fourier loss) used in image restoration. No verifiable resources are identified.",
      "processing_time": 60.872923374176025,
      "citing_paper_id": "278905292",
      "cited_paper_id": 268553835
    },
    {
      "context_text": "L Fourier , as utilized in MoCE-IR [93, 11], to enhance frequency-domain consistency, the real-valued Fourier loss, is defined as: where ˆ x and x denote the restored and ground-truth images, respectively.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a method (Fourier loss) used in image restoration. No verifiable resources are identified.",
      "processing_time": 60.872923374176025,
      "citing_paper_id": "278905292",
      "cited_paper_id": 274305801
    },
    {
      "context_text": "Extending this idea, some works further model prompts from a frequency perspective [11] or propose more complex architectures with additional datasets [19].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only references to other works and concepts. No clear, verifiable datasets are identified.",
      "processing_time": 60.717519998550415,
      "citing_paper_id": "278905292",
      "cited_paper_id": 268553835
    },
    {
      "context_text": "And the 10M small model achieves the best performance across all the metrics, while maintaining 60% fewer parameters compared to MoCE-IR [93].",
      "catation_intent": "findings",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a comparison of model performance. The cited paper title suggests image restoration but does not specify datasets.",
      "processing_time": 61.021767377853394,
      "citing_paper_id": "278905292",
      "cited_paper_id": 274305801
    },
    {
      "context_text": "To overcome these limitations, the emerging field of All-in-One image restoration focuses on single-blind models capable of handling multiple degradation types simultaneously [93, 97, 106].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only the concept of All-in-One image restoration. No verifiable resources are identified.",
      "processing_time": 60.65177392959595,
      "citing_paper_id": "278905292",
      "cited_paper_id": 274305801
    },
    {
      "context_text": "Compared to the recent state-of-the-art MoCE-IR [93] (11M), our Small model still achieves 0.28 dB higher performance while being more compact (10M vs. 11M).",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a comparison of model performance and size.",
      "processing_time": 60.01833987236023,
      "citing_paper_id": "278905292",
      "cited_paper_id": 274305801
    },
    {
      "context_text": "We train our method for 150 epochs (fewer than 200 epochs than MoCE-IR [93]), and we keep all other settings unchanged.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a comparison in training epochs with another method (MoCE-IR).",
      "processing_time": 60.394312143325806,
      "citing_paper_id": "278905292",
      "cited_paper_id": 274305801
    },
    {
      "context_text": "FR IQA metrics, such as PSNR, SSIM [44] and LPIPS [54], assess image quality by comparing a degraded or restored image with its pristine counterpart.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions metrics (PSNR, SSIM, LPIPS) but does not refer to any specific datasets. These metrics are used for image quality assessment, which is relevant to the topic of image restoration, but they are not datasets themselves.",
      "processing_time": 61.95970869064331,
      "citing_paper_id": "277667714",
      "cited_paper_id": 4766599
    },
    {
      "context_text": "FR IQA metrics, such as PSNR, SSIM [44] and LPIPS [54], assess image quality by comparing a degraded or restored image with its pristine counterpart.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions metrics (PSNR, SSIM, LPIPS) but does not refer to any specific datasets. These metrics are used for image quality assessment, which is relevant to the topic of image restoration, but they are not datasets themselves.",
      "processing_time": 61.95970869064331,
      "citing_paper_id": "277667714",
      "cited_paper_id": 207761262
    },
    {
      "context_text": "2 st Restoration 3 st Restoration ③ ④ ⑧ ⑨ 0.6 ② ③ Unlike reference-based IQA metrics such as PSNR, SSIM [44], and LPIPS [54], which require ground-truth images as reference, the non-reference (NR) IQA methods are employed in the agent, enabling the prediction and monitoring of image quality…",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only metrics and methods. The cited papers do not introduce new datasets either.",
      "processing_time": 60.38668370246887,
      "citing_paper_id": "277667714",
      "cited_paper_id": 4766599
    },
    {
      "context_text": "2 st Restoration 3 st Restoration ③ ④ ⑧ ⑨ 0.6 ② ③ Unlike reference-based IQA metrics such as PSNR, SSIM [44], and LPIPS [54], which require ground-truth images as reference, the non-reference (NR) IQA methods are employed in the agent, enabling the prediction and monitoring of image quality…",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only metrics and methods. The cited papers do not introduce new datasets either.",
      "processing_time": 60.38668370246887,
      "citing_paper_id": "277667714",
      "cited_paper_id": 207761262
    },
    {
      "context_text": "In addition, to evaluate the effectiveness of the quality-driven greedy restoration process, we employ four widely used metrics in IR: PSNR, SSIM [44], LPIPS [54], and DISTS [9].",
      "catation_intent": "none",
      "resource_type": "metric",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions metrics (PSNR, SSIM, LPIPS, DISTS) but does not refer to any specific datasets. The cited papers are about metrics, not datasets.",
      "processing_time": 61.13287663459778,
      "citing_paper_id": "277667714",
      "cited_paper_id": 4766599
    },
    {
      "context_text": "In addition, to evaluate the effectiveness of the quality-driven greedy restoration process, we employ four widely used metrics in IR: PSNR, SSIM [44], LPIPS [54], and DISTS [9].",
      "catation_intent": "none",
      "resource_type": "metric",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions metrics (PSNR, SSIM, LPIPS, DISTS) but does not refer to any specific datasets. The cited papers are about metrics, not datasets.",
      "processing_time": 61.13287663459778,
      "citing_paper_id": "277667714",
      "cited_paper_id": 207761262
    },
    {
      "context_text": "To preserve the image quality, targeted restoration of degraded images on commonly used datasets [1, 10, 22, 25, 31, 34, 42, 49] is essential.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.2,
      "reasoning": "The context mentions 'commonly used datasets' without specifying names, making it impossible to identify specific datasets. The cited papers do not provide additional clarity on specific datasets used.",
      "processing_time": 61.0060875415802,
      "citing_paper_id": "277667714",
      "cited_paper_id": 8671030
    },
    {
      "context_text": "To preserve the image quality, targeted restoration of degraded images on commonly used datasets [1, 10, 22, 25, 31, 34, 42, 49] is essential.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.2,
      "reasoning": "The context mentions 'commonly used datasets' without specifying names, making it impossible to identify specific datasets. The cited papers do not provide additional clarity on specific datasets used.",
      "processing_time": 61.0060875415802,
      "citing_paper_id": "277667714",
      "cited_paper_id": 201624746
    },
    {
      "context_text": "To preserve the image quality, targeted restoration of degraded images on commonly used datasets [1, 10, 22, 25, 31, 34, 42, 49] is essential.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.2,
      "reasoning": "The context mentions 'commonly used datasets' without specifying names, making it impossible to identify specific datasets. The cited papers do not provide additional clarity on specific datasets used.",
      "processing_time": 61.0060875415802,
      "citing_paper_id": "277667714",
      "cited_paper_id": 218470249
    },
    {
      "context_text": "To preserve the image quality, targeted restoration of degraded images on commonly used datasets [1, 10, 22, 25, 31, 34, 42, 49] is essential.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.2,
      "reasoning": "The context mentions 'commonly used datasets' without specifying names, making it impossible to identify specific datasets. The cited papers do not provide additional clarity on specific datasets used.",
      "processing_time": 61.0060875415802,
      "citing_paper_id": "277667714",
      "cited_paper_id": 222104551
    },
    {
      "context_text": "To preserve the image quality, targeted restoration of degraded images on commonly used datasets [1, 10, 22, 25, 31, 34, 42, 49] is essential.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.2,
      "reasoning": "The context mentions 'commonly used datasets' without specifying names, making it impossible to identify specific datasets. The cited papers do not provide additional clarity on specific datasets used.",
      "processing_time": 61.0060875415802,
      "citing_paper_id": "277667714",
      "cited_paper_id": 244114613
    },
    {
      "context_text": "To preserve the image quality, targeted restoration of degraded images on commonly used datasets [1, 10, 22, 25, 31, 34, 42, 49] is essential.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.2,
      "reasoning": "The context mentions 'commonly used datasets' without specifying names, making it impossible to identify specific datasets. The cited papers do not provide additional clarity on specific datasets used.",
      "processing_time": 61.0060875415802,
      "citing_paper_id": "277667714",
      "cited_paper_id": 274130569
    },
    {
      "context_text": "To preserve the image quality, targeted restoration of degraded images on commonly used datasets [1, 10, 22, 25, 31, 34, 42, 49] is essential.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.2,
      "reasoning": "The context mentions 'commonly used datasets' without specifying names, making it impossible to identify specific datasets. The cited papers do not provide additional clarity on specific datasets used.",
      "processing_time": 61.0060875415802,
      "citing_paper_id": "277667714",
      "cited_paper_id": null
    },
    {
      "context_text": "For instance, methods for restoring old photos [38] could be incorporated into the Q-Agent framework, thereby creating a more comprehensive and complete IR solution.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for restoring old photos. The context is about incorporating such methods into a framework, not using a dataset.",
      "processing_time": 60.94231677055359,
      "citing_paper_id": "277667714",
      "cited_paper_id": 215827763
    },
    {
      "context_text": "Additionally, CLIP-IQA [39] emphasizes the semantic information of the image, while Hyper-IQA [35] is capable of detecting local distortions.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only methods (CLIP-IQA and Hyper-IQA). No dataset names are present in the context.",
      "processing_time": 60.876099586486816,
      "citing_paper_id": "277667714",
      "cited_paper_id": 219633388
    },
    {
      "context_text": "Additionally, CLIP-IQA [39] emphasizes the semantic information of the image, while Hyper-IQA [35] is capable of detecting local distortions.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only methods (CLIP-IQA and Hyper-IQA). No dataset names are present in the context.",
      "processing_time": 60.876099586486816,
      "citing_paper_id": "277667714",
      "cited_paper_id": 251040466
    },
    {
      "context_text": "The evolution of IR methods can be categorized into two paradigms: Task-Specific [3, 47, 53, 64] and All-in-One models [7, 14, 15, 17, 20, 29, 40, 43].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only categories of image restoration methods. The cited papers' titles do not provide additional context about datasets.",
      "processing_time": 60.81457734107971,
      "citing_paper_id": "277667714",
      "cited_paper_id": 236171006
    },
    {
      "context_text": "The evolution of IR methods can be categorized into two paradigms: Task-Specific [3, 47, 53, 64] and All-in-One models [7, 14, 15, 17, 20, 29, 40, 43].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only categories of image restoration methods. The cited papers' titles do not provide additional context about datasets.",
      "processing_time": 60.81457734107971,
      "citing_paper_id": "277667714",
      "cited_paper_id": 247748724
    },
    {
      "context_text": "The evolution of IR methods can be categorized into two paradigms: Task-Specific [3, 47, 53, 64] and All-in-One models [7, 14, 15, 17, 20, 29, 40, 43].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only categories of image restoration methods. The cited papers' titles do not provide additional context about datasets.",
      "processing_time": 60.81457734107971,
      "citing_paper_id": "277667714",
      "cited_paper_id": 250551851
    },
    {
      "context_text": "The evolution of IR methods can be categorized into two paradigms: Task-Specific [3, 47, 53, 64] and All-in-One models [7, 14, 15, 17, 20, 29, 40, 43].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only categories of image restoration methods. The cited papers' titles do not provide additional context about datasets.",
      "processing_time": 60.81457734107971,
      "citing_paper_id": "277667714",
      "cited_paper_id": 257496232
    },
    {
      "context_text": "The evolution of IR methods can be categorized into two paradigms: Task-Specific [3, 47, 53, 64] and All-in-One models [7, 14, 15, 17, 20, 29, 40, 43].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only categories of image restoration methods. The cited papers' titles do not provide additional context about datasets.",
      "processing_time": 60.81457734107971,
      "citing_paper_id": "277667714",
      "cited_paper_id": 258048853
    },
    {
      "context_text": "The evolution of IR methods can be categorized into two paradigms: Task-Specific [3, 47, 53, 64] and All-in-One models [7, 14, 15, 17, 20, 29, 40, 43].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only categories of image restoration methods. The cited papers' titles do not provide additional context about datasets.",
      "processing_time": 60.81457734107971,
      "citing_paper_id": "277667714",
      "cited_paper_id": 259224666
    },
    {
      "context_text": "The evolution of IR methods can be categorized into two paradigms: Task-Specific [3, 47, 53, 64] and All-in-One models [7, 14, 15, 17, 20, 29, 40, 43].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only categories of image restoration methods. The cited papers' titles do not provide additional context about datasets.",
      "processing_time": 60.81457734107971,
      "citing_paper_id": "277667714",
      "cited_paper_id": 267320695
    },
    {
      "context_text": "…[14] 18.4029 0.4320 0.2313 0.1885 DA-CLIP [20] 20.8141 0.6471 0.2876 0.1546 InstructIR [7] 17.5059 0.3829 0.2595 0.2356 PromptIR [29] 20.0148 0.5259 0.2434 0.2201 Real-ESRGAN [43] 13.6988 0.2055 0.2589 0.2600 StableSR [40] 12.4668 0.1821 0.4235 0.2924 Q-Agent (Ours) 22.5847 0.6856 0.1529 0.0778",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only model names and performance metrics. No verifiable resources are identified.",
      "processing_time": 60.36415362358093,
      "citing_paper_id": "277667714",
      "cited_paper_id": 236171006
    },
    {
      "context_text": "Over the years, Task-Specific IR methods [3, 47, 53, 64] have addressed specific types of IR.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general references to task-specific IR methods. No verifiable resources are identified.",
      "processing_time": 60.388692140579224,
      "citing_paper_id": "277667714",
      "cited_paper_id": 247748724
    },
    {
      "context_text": "Over the years, Task-Specific IR methods [3, 47, 53, 64] have addressed specific types of IR.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general references to task-specific IR methods. No verifiable resources are identified.",
      "processing_time": 60.388692140579224,
      "citing_paper_id": "277667714",
      "cited_paper_id": 257496232
    },
    {
      "context_text": "Over the years, Task-Specific IR methods [3, 47, 53, 64] have addressed specific types of IR.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general references to task-specific IR methods. No verifiable resources are identified.",
      "processing_time": 60.388692140579224,
      "citing_paper_id": "277667714",
      "cited_paper_id": 258048853
    },
    {
      "context_text": "PSNR ↑ SSIM ↑ LPIPS ↓ DISTS ↓ AirNet [17] 17.7129 0.4744 0.2531 0.2352 AutoDIR [14] 18.4029 0.4320 0.2313 0.1885 DA-CLIP [20] 20.8141 0.6471 0.2876 0.1546 InstructIR [7] 17.5059 0.3829 0.2595 0.2356 PromptIR [29] 20.0148 0.5259 0.2434 0.2201 Real-ESRGAN [43] 13.6988 0.2055 0.2589 0.2600 StableSR…",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only model performance metrics. No dataset names are present in the text.",
      "processing_time": 60.02808928489685,
      "citing_paper_id": "277667714",
      "cited_paper_id": 250551851
    },
    {
      "context_text": "Objective IQA methods are further classified into Full-Reference (FR) [57], Reduced-Reference (RR) [59], and No-Reference (NR) [61, 63] approaches.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only categories of image quality assessment methods.",
      "processing_time": 59.558547019958496,
      "citing_paper_id": "277667714",
      "cited_paper_id": 252383409
    },
    {
      "context_text": "While these methods improve generality, studies [4, 48] have shown that they often fall short in effectiveness and flexibility compared to Task-Specific models.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a comparison between general methods and task-specific models.",
      "processing_time": 59.789355754852295,
      "citing_paper_id": "277667714",
      "cited_paper_id": 252421835
    },
    {
      "context_text": "To ensure the highest quality and most diverse image dataset, we select 12,225 images from the Laion-High-Resolution dataset [33] as source images.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Laion-High-Resolution"
      ],
      "dataset_descriptions": {
        "Laion-High-Resolution": "Used to select 12,225 images for training an all-in-one image restoration model, focusing on high-resolution and diverse image content."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions the Laion-High-Resolution dataset, which is a specific, verifiable dataset used for selecting source images.",
      "processing_time": 62.71640920639038,
      "citing_paper_id": "277667714",
      "cited_paper_id": 252917726
    },
    {
      "context_text": "…proposed by Zhang et al. [46] provides a benchmark for comparing the degradation-awareness capabilities of various MLLMs [5, 6, 8, 11, 16, 18, 50, 65], from which it can be seen that the performance of most MLLMs in dealing with image degradation related problems is far from satisfying the…",
      "catation_intent": "reusable resource",
      "resource_type": "benchmark",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a benchmark for comparing MLLMs. The context is about evaluating models' performance on image degradation tasks, but no dataset names are provided.",
      "processing_time": 61.1004695892334,
      "citing_paper_id": "277667714",
      "cited_paper_id": 258291930
    },
    {
      "context_text": "…proposed by Zhang et al. [46] provides a benchmark for comparing the degradation-awareness capabilities of various MLLMs [5, 6, 8, 11, 16, 18, 50, 65], from which it can be seen that the performance of most MLLMs in dealing with image degradation related problems is far from satisfying the…",
      "catation_intent": "reusable resource",
      "resource_type": "benchmark",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a benchmark for comparing MLLMs. The context is about evaluating models' performance on image degradation tasks, but no dataset names are provided.",
      "processing_time": 61.1004695892334,
      "citing_paper_id": "277667714",
      "cited_paper_id": 258352455
    },
    {
      "context_text": "…Q-Bench proposed by Zhang et al. [46] provides a benchmark for comparing the degradation-awareness capabilities of various MLLMs [5, 6, 8, 11, 16, 18, 50, 65], from which it can be seen that the performance of most MLLMs in dealing with image degradation related problems is far from satisfying…",
      "catation_intent": "findings",
      "resource_type": "benchmark",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions Q-Bench as a benchmark for comparing degradation-awareness capabilities of MLLMs, but does not specify it as a dataset. It is used to highlight the performance gap of MLLMs in image degradation tasks.",
      "processing_time": 61.28320288658142,
      "citing_paper_id": "277667714",
      "cited_paper_id": 258547300
    },
    {
      "context_text": "4, the Q-Bench proposed by Zhang et al. [46] provides a benchmark for comparing the degradation-awareness capabilities of various MLLMs [5, 6, 8, 11, 16, 18, 50, 65], from which it can be seen that the performance of most MLLMs in dealing with image degradation related problems is far from…",
      "catation_intent": "reusable resource",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.3,
      "reasoning": "The context mentions 'Q-Bench' as a benchmark for comparing degradation-awareness capabilities of MLLMs, which is relevant to the topic of image restoration. However, it does not specify that Q-Bench is a dataset, and it is more likely a benchmark or leaderboard.",
      "processing_time": 61.764190435409546,
      "citing_paper_id": "277667714",
      "cited_paper_id": 258615266
    },
    {
      "context_text": "4, the Q-Bench proposed by Zhang et al. [46] provides a benchmark for comparing the degradation-awareness capabilities of various MLLMs [5, 6, 8, 11, 16, 18, 50, 65], from which it can be seen that the performance of most MLLMs in dealing with image degradation related problems is far from…",
      "catation_intent": "reusable resource",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.3,
      "reasoning": "The context mentions 'Q-Bench' as a benchmark for comparing degradation-awareness capabilities of MLLMs, which is relevant to the topic of image restoration. However, it does not specify that Q-Bench is a dataset, and it is more likely a benchmark or leaderboard.",
      "processing_time": 61.764190435409546,
      "citing_paper_id": "277667714",
      "cited_paper_id": 259262082
    },
    {
      "context_text": "…0.2352 AutoDIR [14] 18.4029 0.4320 0.2313 0.1885 DA-CLIP [20] 20.8141 0.6471 0.2876 0.1546 InstructIR [7] 17.5059 0.3829 0.2595 0.2356 PromptIR [29] 20.0148 0.5259 0.2434 0.2201 Real-ESRGAN [43] 13.6988 0.2055 0.2589 0.2600 StableSR [40] 12.4668 0.1821 0.4235 0.2924 Q-Agent (Ours) 22.5847…",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only model names and their performance metrics. No dataset names are present in the text.",
      "processing_time": 60.184521198272705,
      "citing_paper_id": "277667714",
      "cited_paper_id": 259224666
    },
    {
      "context_text": "Notably, unlike prior works [4, 55, 56], the selected degradation types are applied in a specific order, a decision motivated by two factors: 1) Although the degradation sequence cannot be directly observed in the image, it exists objectively throughout the image degradation process; 2) As…",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, models, or methods. It only discusses the rationale behind the application of degradation types in a specific order.",
      "processing_time": 60.438652992248535,
      "citing_paper_id": "277667714",
      "cited_paper_id": 259360671
    },
    {
      "context_text": "However, MLLMs still suffer from hallucinations [13], which can reduce the reliability of their outputs.",
      "catation_intent": "limitation",
      "resource_type": "limitation",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a limitation of multi-modal large language models.",
      "processing_time": 59.46187877655029,
      "citing_paper_id": "277667714",
      "cited_paper_id": 265498818
    },
    {
      "context_text": "…↓ DISTS ↓ AirNet [17] 17.7129 0.4744 0.2531 0.2352 AutoDIR [14] 18.4029 0.4320 0.2313 0.1885 DA-CLIP [20] 20.8141 0.6471 0.2876 0.1546 InstructIR [7] 17.5059 0.3829 0.2595 0.2356 PromptIR [29] 20.0148 0.5259 0.2434 0.2201 Real-ESRGAN [43] 13.6988 0.2055 0.2589 0.2600 StableSR [40] 12.4668 0.1821…",
      "catation_intent": "reusable resource",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only model names and performance metrics. There are no clear identifiers for datasets.",
      "processing_time": 59.983951807022095,
      "citing_paper_id": "277667714",
      "cited_paper_id": 267320695
    },
    {
      "context_text": "Existing agent meth-ods based on rolling back [4] and reinforcement learning (RL) [4, 27, 30] may lead to high computational costs.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only methods and computational costs associated with them.",
      "processing_time": 59.53166460990906,
      "citing_paper_id": "277667714",
      "cited_paper_id": 267752981
    },
    {
      "context_text": "Existing agent meth-ods based on rolling back [4] and reinforcement learning (RL) [4, 27, 30] may lead to high computational costs.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only methods and computational costs associated with them.",
      "processing_time": 59.53166460990906,
      "citing_paper_id": "277667714",
      "cited_paper_id": 270199505
    },
    {
      "context_text": "5-Pro [ 36] 0.8208 0.7796 0.7570 0.6306 0.9252 0.3908 0.9070 0.8455 0.6047 0.5237 0.0424 Llava-Llama3-8B (In RestoreAgent) [19, 37] 0.0030 0.0125 0.0753 0.0131 0.7045 0.0414 0.8238 0.8236 0.0000 0.0000 0.0701 Qwen2-VL-7B-Instruct (In Q-Agent) [41] 0.7542 0.7233 0.7446 0.5056 0.8263 0.3772 0.7675…",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only model names and performance metrics. There are no clear identifiers for datasets.",
      "processing_time": 59.905760765075684,
      "citing_paper_id": "277667714",
      "cited_paper_id": 268297180
    },
    {
      "context_text": "5-Pro [ 36] 0.8208 0.7796 0.7570 0.6306 0.9252 0.3908 0.9070 0.8455 0.6047 0.5237 0.0424 Llava-Llama3-8B (In RestoreAgent) [19, 37] 0.0030 0.0125 0.0753 0.0131 0.7045 0.0414 0.8238 0.8236 0.0000 0.0000 0.0701 Qwen2-VL-7B-Instruct (In Q-Agent) [41] 0.7542 0.7233 0.7446 0.5056 0.8263 0.3772 0.7675…",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only model names and performance metrics. There are no clear identifiers for datasets.",
      "processing_time": 59.905760765075684,
      "citing_paper_id": "277667714",
      "cited_paper_id": null
    },
    {
      "context_text": "5-Pro [ 36] 0.8208 0.7796 0.7570 0.6306 0.9252 0.3908 0.9070 0.8455 0.6047 0.5237 0.0424 Llava-Llama3-8B (In RestoreAgent) [19, 37] 0.0030 0.0125 0.0753 0.0131 0.7045 0.0414 0.8238 0.8236 0.0000 0.0000 0.0701 Qwen2-VL-7B-Instruct (In Q-Agent) [41] 0.7542 0.7233 0.7446 0.5056 0.8263 0.3772 0.7675…",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only model names and performance metrics. There are no clear identifiers for datasets.",
      "processing_time": 59.905760765075684,
      "citing_paper_id": "277667714",
      "cited_paper_id": null
    },
    {
      "context_text": "5-Pro [ 36] 0.8444 0.7535 0.8562 0.7481 0.9243 0.7978 0.9232 0.8998 0.8549 0.7690 0.5593 Llava-Llama3-8B (In RestoreAgent) [19, 37] 0.6323 0.5989 0.5577 0.6834 0.8144 0.5206 0.7890 0.8389 0.5047 0.4683 0.4779 Qwen2-VL-7B-Instruct (In Q-Agent) [41] 0.9097 0.7478 0.8022 0.7978 0.8790 0.8464 0.8829…",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only model names and performance metrics. No verifiable resources are identified.",
      "processing_time": 59.58619928359985,
      "citing_paper_id": "277667714",
      "cited_paper_id": 268297180
    },
    {
      "context_text": "5-Pro [ 36] 0.8444 0.7535 0.8562 0.7481 0.9243 0.7978 0.9232 0.8998 0.8549 0.7690 0.5593 Llava-Llama3-8B (In RestoreAgent) [19, 37] 0.6323 0.5989 0.5577 0.6834 0.8144 0.5206 0.7890 0.8389 0.5047 0.4683 0.4779 Qwen2-VL-7B-Instruct (In Q-Agent) [41] 0.9097 0.7478 0.8022 0.7978 0.8790 0.8464 0.8829…",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only model names and performance metrics. No verifiable resources are identified.",
      "processing_time": 59.58619928359985,
      "citing_paper_id": "277667714",
      "cited_paper_id": null
    },
    {
      "context_text": "5-Pro [ 36] 0.8444 0.7535 0.8562 0.7481 0.9243 0.7978 0.9232 0.8998 0.8549 0.7690 0.5593 Llava-Llama3-8B (In RestoreAgent) [19, 37] 0.6323 0.5989 0.5577 0.6834 0.8144 0.5206 0.7890 0.8389 0.5047 0.4683 0.4779 Qwen2-VL-7B-Instruct (In Q-Agent) [41] 0.9097 0.7478 0.8022 0.7978 0.8790 0.8464 0.8829…",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only model names and performance metrics. No verifiable resources are identified.",
      "processing_time": 59.58619928359985,
      "citing_paper_id": "277667714",
      "cited_paper_id": null
    },
    {
      "context_text": "5-Pro [36] are closed-source models, while Llava-Llama3-8B, which is employed in Re-storeAgent [4], implemented using open-source code provided by author.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models and code. There are no verifiable resources that meet the criteria.",
      "processing_time": 59.65464949607849,
      "citing_paper_id": "277667714",
      "cited_paper_id": 268297180
    },
    {
      "context_text": "…(In RestoreAgent) [19, 37] 0.6323 0.5989 0.5577 0.6834 0.8144 0.5206 0.7890 0.8389 0.5047 0.4683 0.4779 Qwen2-VL-7B-Instruct (In Q-Agent) [41] 0.9097 0.7478 0.8022 0.7978 0.8790 0.8464 0.8829 0.9180 0.7836 0.7771 0.6033 Fine-tuned + CoT Qwen2-VL-7B-Instruct (In Q-Agent) [41] 0.9495…",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only model performance metrics. There are no clear identifiers for datasets or other verifiable resources.",
      "processing_time": 59.91906762123108,
      "citing_paper_id": "277667714",
      "cited_paper_id": 272704132
    },
    {
      "context_text": "…(In RestoreAgent) [19, 37] 0.0030 0.0125 0.0753 0.0131 0.7045 0.0414 0.8238 0.8236 0.0000 0.0000 0.0701 Qwen2-VL-7B-Instruct (In Q-Agent) [41] 0.7542 0.7233 0.7446 0.5056 0.8263 0.3772 0.7675 0.7989 0.3417 0.2049 0.1053 Zero-shot + CoT GPT-4o [28] 0.8742 0.7366 0.8890 0.8199 0.8021…",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only model performance metrics. There are no clear identifiers for datasets or other verifiable resources.",
      "processing_time": 59.78278112411499,
      "citing_paper_id": "277667714",
      "cited_paper_id": 272704132
    },
    {
      "context_text": "…0.4779 Qwen2-VL-7B-Instruct (In Q-Agent) [41] 0.9097 0.7478 0.8022 0.7978 0.8790 0.8464 0.8829 0.9180 0.7836 0.7771 0.6033 Fine-tuned + CoT Qwen2-VL-7B-Instruct (In Q-Agent) [41] 0.9495 0.9235 0.9441 0.8819 0.9010 0.8908 0.9430 0.9208 0.8875 0.9588 0.7882 sence of a specific type of degradation.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The provided context does not mention any specific datasets, only model performance metrics. There are no clear identifiers for datasets or other verifiable resources.",
      "processing_time": 59.718087673187256,
      "citing_paper_id": "277667714",
      "cited_paper_id": 272704132
    },
    {
      "context_text": "The training set is used to fine-tune the Qwen2-VL-7B-Instruct [41], which is utilized in Q-Agent for degradation perceptions, while all experimental evaluations are carried out on the test set.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.2,
      "reasoning": "The context mentions 'training set' and 'test set' but does not provide specific names or identifiers for these datasets. The names are too generic and do not meet the criteria for inclusion.",
      "processing_time": 60.24203157424927,
      "citing_paper_id": "277667714",
      "cited_paper_id": 272704132
    },
    {
      "context_text": "…perceiving individual degradation types, all existing MLLMs exhibit significant limitations in perceiving multiple degradation types; 2) Qwen2-VL [41] achieves the best multiple degradation perception performance in all cases, justifying its selection as the foundational model in the Q-Agent…",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a model (Qwen2-VL) and its performance. No verifiable resources are identified.",
      "processing_time": 59.79075360298157,
      "citing_paper_id": "277667714",
      "cited_paper_id": 272704132
    },
    {
      "context_text": "…few cases, the degradation perception performance of MLLMs improves, especially in multiple degradation awareness, demonstrating the effectiveness of utilized techniques; 4) The Qwen2-VL-7B [41] employed in Q-Agent outperforms Llava-Llama3-8B [19, 37] utilized in RestoreAgent [4] with smaller size.",
      "catation_intent": "findings",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models and their performance comparisons.",
      "processing_time": 58.75006437301636,
      "citing_paper_id": "277667714",
      "cited_paper_id": 272704132
    },
    {
      "context_text": "…few cases, the degradation perception performance of MLLMs improves, especially in multiple degradation awareness, demonstrating the effectiveness of utilized techniques; 4) The Qwen2-VL-7B [41] employed in Q-Agent outperforms Llava-Llama3-8B [19, 37] utilized in RestoreAgent [4] with smaller size.",
      "catation_intent": "findings",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models and their performance comparisons.",
      "processing_time": 58.75006437301636,
      "citing_paper_id": "277667714",
      "cited_paper_id": null
    },
    {
      "context_text": "…few cases, the degradation perception performance of MLLMs improves, especially in multiple degradation awareness, demonstrating the effectiveness of utilized techniques; 4) The Qwen2-VL-7B [41] employed in Q-Agent outperforms Llava-Llama3-8B [19, 37] utilized in RestoreAgent [4] with smaller size.",
      "catation_intent": "findings",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models and their performance comparisons.",
      "processing_time": 58.75006437301636,
      "citing_paper_id": "277667714",
      "cited_paper_id": null
    },
    {
      "context_text": "For this reason, we introduce the Degradation Accuracy DACC , which is adapted from Zhou et al. [62] and defined as follows: where S i represents the set of images with the i -th type of degradation, and ˆ S i refers to the subset of images detected by the MLLM as exhibiting the i -th degradation…",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a metric (DACC) and a method (MLLM). The context is focused on defining a metric rather than using a dataset.",
      "processing_time": 60.17404532432556,
      "citing_paper_id": "277667714",
      "cited_paper_id": 274130620
    },
    {
      "context_text": "For one-by-one IR methods, we compared our VLU-Net with eight specific IR (trained single time for single task) methods (DnCNN [53], FFDNet [54], De-hazeNet [4], EPDN [34], FDGAN [13], UMR [44], SIRR [41], MSPFN [19]) and six general IR (trained multiple times for multiple tasks) methods (MPRNet…",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only models and methods used for image restoration. No verifiable resources are identified.",
      "processing_time": 59.278444051742554,
      "citing_paper_id": "277244267",
      "cited_paper_id": 996788
    },
    {
      "context_text": "For one-by-one IR methods, we compared our VLU-Net with eight specific IR (trained single time for single task) methods (DnCNN [53], FFDNet [54], De-hazeNet [4], EPDN [34], FDGAN [13], UMR [44], SIRR [41], MSPFN [19]) and six general IR (trained multiple times for multiple tasks) methods (MPRNet…",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only models and methods used for image restoration. No verifiable resources are identified.",
      "processing_time": 59.278444051742554,
      "citing_paper_id": "277244267",
      "cited_paper_id": 10514149
    },
    {
      "context_text": "Building on model-based optimization approaches [2], DUNs translate the logic of iterative optimization algorithms, such as Proximal Gradient Descent (PGD) [1] and Half-Quadratic Splitting (HQS) [15, 42], into a deep learning framework.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only models and methods. The cited papers do not provide additional context to identify datasets.",
      "processing_time": 59.40099835395813,
      "citing_paper_id": "277244267",
      "cited_paper_id": 3072879
    },
    {
      "context_text": "Building on model-based optimization approaches [2], DUNs translate the logic of iterative optimization algorithms, such as Proximal Gradient Descent (PGD) [1] and Half-Quadratic Splitting (HQS) [15, 42], into a deep learning framework.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only models and methods. The cited papers do not provide additional context to identify datasets.",
      "processing_time": 59.40099835395813,
      "citing_paper_id": "277244267",
      "cited_paper_id": 3072879
    },
    {
      "context_text": "Building on model-based optimization approaches [2], DUNs translate the logic of iterative optimization algorithms, such as Proximal Gradient Descent (PGD) [1] and Half-Quadratic Splitting (HQS) [15, 42], into a deep learning framework.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only models and methods. The cited papers do not provide additional context to identify datasets.",
      "processing_time": 59.40099835395813,
      "citing_paper_id": "277244267",
      "cited_paper_id": 250601992
    },
    {
      "context_text": "Image restoration (IR) aims to recover original images x , from degraded observations y , affected by factors such as optical distortions [10, 17, 31, 50] due to lens imperfections and shooting conditions, noise from photoelectric conversion, motion blur, and weather-induced artifacts like rain or…",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general factors affecting image degradation. No dataset names are present in the text.",
      "processing_time": 59.336081981658936,
      "citing_paper_id": "277244267",
      "cited_paper_id": 3831826
    },
    {
      "context_text": "Image restoration (IR) aims to recover original images x , from degraded observations y , affected by factors such as optical distortions [10, 17, 31, 50] due to lens imperfections and shooting conditions, noise from photoelectric conversion, motion blur, and weather-induced artifacts like rain or…",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general factors affecting image degradation. No dataset names are present in the text.",
      "processing_time": 59.336081981658936,
      "citing_paper_id": "277244267",
      "cited_paper_id": 214803044
    },
    {
      "context_text": "We utilize two linear convolution operations to learn the transforms and: where × 3 denotes the mode-3 product [21] and Thus optimization can be unfolded in feature level, removing restriction of dimensional compression between stages and expanding high-dimensional features into D-GDM.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only mathematical operations and tensor decompositions.",
      "processing_time": 58.76336669921875,
      "citing_paper_id": "277244267",
      "cited_paper_id": 16074195
    },
    {
      "context_text": "…eight specific IR (trained single time for single task) methods (DnCNN [53], FFDNet [54], De-hazeNet [4], EPDN [34], FDGAN [13], UMR [44], SIRR [41], MSPFN [19]) and six general IR (trained multiple times for multiple tasks) methods (MPRNet [46], SwinIR [24], DGUNet [30], FSNet [12], Restormer…",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various image restoration methods. No verifiable resources are identified.",
      "processing_time": 59.20855975151062,
      "citing_paper_id": "277244267",
      "cited_paper_id": 119188226
    },
    {
      "context_text": "We add two learnable modules and each module is a three-layer MLP with layer normalization and Gaussian Error Linear Unit (GELU) [16], in front of the image and text encoders of CLIP as adapters.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (GELU).",
      "processing_time": 58.535494327545166,
      "citing_paper_id": "277244267",
      "cited_paper_id": 125617073
    },
    {
      "context_text": "4.1, we present Figure 6 for detailed diagram of the transformer block we adopted in the experiments following the design and hyper-parameters in [44].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method or model. The context is focused on the transformer block design and hyper-parameters.",
      "processing_time": 59.15953040122986,
      "citing_paper_id": "277244267",
      "cited_paper_id": 195657934
    },
    {
      "context_text": "…VLU-Net with eight specific IR (trained single time for single task) methods (DnCNN [53], FFDNet [54], De-hazeNet [4], EPDN [34], FDGAN [13], UMR [44], SIRR [41], MSPFN [19]) and six general IR (trained multiple times for multiple tasks) methods (MPRNet [46], SwinIR [24], DGUNet [30], FSNet…",
      "catation_intent": "none",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only models and methods. There are no clear identifiers for datasets within the text.",
      "processing_time": 58.979923486709595,
      "citing_paper_id": "277244267",
      "cited_paper_id": 195657934
    },
    {
      "context_text": "(3), the Proximal Mapping Module (PMM), updates the output z ( k ) from the GDM and acts as a denoising step under Gaussian noise level √ ρλ [36, 38, 49].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only methods and models. No dataset names are present in the citation span.",
      "processing_time": 58.976523876190186,
      "citing_paper_id": "277244267",
      "cited_paper_id": 206677332
    },
    {
      "context_text": "(3), the Proximal Mapping Module (PMM), updates the output z ( k ) from the GDM and acts as a denoising step under Gaussian noise level √ ρλ [36, 38, 49].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only methods and models. No dataset names are present in the citation span.",
      "processing_time": 58.976523876190186,
      "citing_paper_id": "277244267",
      "cited_paper_id": 206677332
    },
    {
      "context_text": "Model-based methods [3, 9, 32, 45, 48] primarily rely on hand-crafted priors as regularizers, tailored for single task with specific type of degradation.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only model-based methods and their reliance on hand-crafted priors. No verifiable resources are identified.",
      "processing_time": 59.375104904174805,
      "citing_paper_id": "277244267",
      "cited_paper_id": 220734147
    },
    {
      "context_text": "VLMs, such as those detailed in [18, 23, 35], have demonstrated robust feature extraction and classification capabilities in high-dimensional spaces.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only VLMs and their capabilities. The cited papers' titles do not provide additional context to identify datasets.",
      "processing_time": 59.21072006225586,
      "citing_paper_id": "277244267",
      "cited_paper_id": 231591445
    },
    {
      "context_text": "VLMs, such as those detailed in [18, 23, 35], have demonstrated robust feature extraction and classification capabilities in high-dimensional spaces.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only VLMs and their capabilities. The cited papers' titles do not provide additional context to identify datasets.",
      "processing_time": 59.21072006225586,
      "citing_paper_id": "277244267",
      "cited_paper_id": 231879586
    },
    {
      "context_text": "VLMs, such as those detailed in [18, 23, 35], have demonstrated robust feature extraction and classification capabilities in high-dimensional spaces.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only VLMs and their capabilities. The cited papers' titles do not provide additional context to identify datasets.",
      "processing_time": 59.21072006225586,
      "citing_paper_id": "277244267",
      "cited_paper_id": 246411402
    },
    {
      "context_text": "For instance, the Contrastive Language-Image Pre-training (CLIP) model introduced in [35] leverages contrastive learning to achieve remarkable alignment between images and their corresponding textual descriptions.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions CLIP, which is a model, not a dataset. No specific dataset is referenced in the citation context.",
      "processing_time": 58.81749653816223,
      "citing_paper_id": "277244267",
      "cited_paper_id": 231591445
    },
    {
      "context_text": "…(trained single time for single task) methods (DnCNN [53], FFDNet [54], De-hazeNet [4], EPDN [34], FDGAN [13], UMR [44], SIRR [41], MSPFN [19]) and six general IR (trained multiple times for multiple tasks) methods (MPRNet [46], SwinIR [24], DGUNet [30], FSNet [12], Restormer [47], MambaIR [14]).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions various methods and models but does not specify any datasets. The cited papers' titles do not provide additional information about datasets.",
      "processing_time": 58.8275408744812,
      "citing_paper_id": "277244267",
      "cited_paper_id": 244346144
    },
    {
      "context_text": "…(trained single time for single task) methods (DnCNN [53], FFDNet [54], De-hazeNet [4], EPDN [34], FDGAN [13], UMR [44], SIRR [41], MSPFN [19]) and six general IR (trained multiple times for multiple tasks) methods (MPRNet [46], SwinIR [24], DGUNet [30], FSNet [12], Restormer [47], MambaIR [14]).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions various methods and models but does not specify any datasets. The cited papers' titles do not provide additional information about datasets.",
      "processing_time": 58.8275408744812,
      "citing_paper_id": "277244267",
      "cited_paper_id": 265042422
    },
    {
      "context_text": "…(trained single time for single task) methods (DnCNN [53], FFDNet [54], De-hazeNet [4], EPDN [34], FDGAN [13], UMR [44], SIRR [41], MSPFN [19]) and six general IR (trained multiple times for multiple tasks) methods (MPRNet [46], SwinIR [24], DGUNet [30], FSNet [12], Restormer [47], MambaIR [14]).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions various methods and models but does not specify any datasets. The cited papers' titles do not provide additional information about datasets.",
      "processing_time": 58.8275408744812,
      "citing_paper_id": "277244267",
      "cited_paper_id": 267938238
    },
    {
      "context_text": "…DUN and our hierarchical DUN in the supplementary materials for better framework clarification, secondly we compare the results under NHRBL setting in Table 5 among our VLU-Net, the w/ CLIP version, the w/o CLIP version, end-to-end deep learning method Restormer [47] and DUN method DGUNet [30].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and models. The context focuses on comparing different models and their performance.",
      "processing_time": 58.70911431312561,
      "citing_paper_id": "277244267",
      "cited_paper_id": 244346144
    },
    {
      "context_text": "To address this issue, several all-in-one restoration meth-ods have emerged [7, 22, 33, 47, 52, 55].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to methods or approaches. No dataset names are present in the text.",
      "processing_time": 58.598477363586426,
      "citing_paper_id": "277244267",
      "cited_paper_id": 244346144
    },
    {
      "context_text": "To address this issue, several all-in-one restoration meth-ods have emerged [7, 22, 33, 47, 52, 55].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to methods or approaches. No dataset names are present in the text.",
      "processing_time": 58.598477363586426,
      "citing_paper_id": "277244267",
      "cited_paper_id": 250551851
    },
    {
      "context_text": "To address this issue, several all-in-one restoration meth-ods have emerged [7, 22, 33, 47, 52, 55].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to methods or approaches. No dataset names are present in the text.",
      "processing_time": 58.598477363586426,
      "citing_paper_id": "277244267",
      "cited_paper_id": 271874811
    },
    {
      "context_text": "Deep learning-based methods [5, 6, 20, 29, 46], utilizing informative data and efficient architectures, have successfully extracted richer and more generalized underlying patterns.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general references to 'informative data'. No clear, verifiable datasets are identified.",
      "processing_time": 58.650901317596436,
      "citing_paper_id": "277244267",
      "cited_paper_id": 246411364
    },
    {
      "context_text": "Deep learning-based methods [5, 6, 20, 29, 46], utilizing informative data and efficient architectures, have successfully extracted richer and more generalized underlying patterns.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general references to 'informative data'. No clear, verifiable datasets are identified.",
      "processing_time": 58.650901317596436,
      "citing_paper_id": "277244267",
      "cited_paper_id": 257496264
    },
    {
      "context_text": "Building upon this, BLIP [23] enhances pretraining effectiveness through the generation of synthetic captions.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (BLIP) for pretraining models. The context focuses on the method's effectiveness and synthetic caption generation.",
      "processing_time": 58.909019947052,
      "citing_paper_id": "277244267",
      "cited_paper_id": 246411402
    },
    {
      "context_text": "Air-Net [22] uses contrastive learning to differentiate inputs from various tasks, guiding appropriate processing paths.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (Air-Net) and its application. No verifiable resources are identified.",
      "processing_time": 58.57662582397461,
      "citing_paper_id": "277244267",
      "cited_paper_id": 250551851
    },
    {
      "context_text": "These approaches introduce unified frameworks for dealing with diverse degradation types by employing techniques such as prompt-based restoration [33], contrastive loss for specialized modules [22], and degradation-specific expert selection [7].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods and techniques. The cited paper titles do not provide additional context about datasets.",
      "processing_time": 58.5196008682251,
      "citing_paper_id": "277244267",
      "cited_paper_id": 250551851
    },
    {
      "context_text": "These approaches introduce unified frameworks for dealing with diverse degradation types by employing techniques such as prompt-based restoration [33], contrastive loss for specialized modules [22], and degradation-specific expert selection [7].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods and techniques. The cited paper titles do not provide additional context about datasets.",
      "processing_time": 58.5196008682251,
      "citing_paper_id": "277244267",
      "cited_paper_id": 271874811
    },
    {
      "context_text": "And seven all-in-one IR (trained single time for multiple tasks) methods (AirNet [22], Transweather [37], IDR [52], PromptIR [33], NDR [43], Gridformer [39], Instruc-tIR [11]) are adopted.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions several methods/models but does not refer to any specific datasets. The context is focused on describing various all-in-one image restoration methods rather than datasets.",
      "processing_time": 58.79381084442139,
      "citing_paper_id": "277244267",
      "cited_paper_id": 250551851
    },
    {
      "context_text": "And seven all-in-one IR (trained single time for multiple tasks) methods (AirNet [22], Transweather [37], IDR [52], PromptIR [33], NDR [43], Gridformer [39], Instruc-tIR [11]) are adopted.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions several methods/models but does not refer to any specific datasets. The context is focused on describing various all-in-one image restoration methods rather than datasets.",
      "processing_time": 58.79381084442139,
      "citing_paper_id": "277244267",
      "cited_paper_id": 258959359
    },
    {
      "context_text": "Additionally, DA-CLIP [25, 26] [30] further builds on these concepts by integrating a flexible GDM for dynamic, manual selection of the degradation matrix, applicable to image deblurring and CS. Additionally, UFC-net [40] enhances feature extraction between stages with fixed-point optimization.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only models and methods. The cited papers' titles do not provide additional dataset information.",
      "processing_time": 58.56894850730896,
      "citing_paper_id": "277244267",
      "cited_paper_id": 269148707
    },
    {
      "context_text": "Additionally, DA-CLIP [25, 26] [30] further builds on these concepts by integrating a flexible GDM for dynamic, manual selection of the degradation matrix, applicable to image deblurring and CS. Additionally, UFC-net [40] enhances feature extraction between stages with fixed-point optimization.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only models and methods. The cited papers' titles do not provide additional dataset information.",
      "processing_time": 58.56894850730896,
      "citing_paper_id": "277244267",
      "cited_paper_id": 272724641
    },
    {
      "context_text": "Adverse weather removal problems like deraining [16, 21,25,51,61,65,76], dehazing [1,2,10,22,42,69], desnowing [29,44,44,73] and rain drop removal [37,39,40,66] have been extensively explored in the literature.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various image restoration tasks. No dataset names are provided in the context.",
      "processing_time": 58.26229476928711,
      "citing_paper_id": "244714491",
      "cited_paper_id": 254848
    },
    {
      "context_text": "Adverse weather removal problems like deraining [16, 21,25,51,61,65,76], dehazing [1,2,10,22,42,69], desnowing [29,44,44,73] and rain drop removal [37,39,40,66] have been extensively explored in the literature.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various image restoration tasks. No dataset names are provided in the context.",
      "processing_time": 58.26229476928711,
      "citing_paper_id": "244714491",
      "cited_paper_id": 4539586
    },
    {
      "context_text": "Adverse weather removal problems like deraining [16, 21,25,51,61,65,76], dehazing [1,2,10,22,42,69], desnowing [29,44,44,73] and rain drop removal [37,39,40,66] have been extensively explored in the literature.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various image restoration tasks. No dataset names are provided in the context.",
      "processing_time": 58.26229476928711,
      "citing_paper_id": "244714491",
      "cited_paper_id": 22540825
    },
    {
      "context_text": "Adverse weather removal problems like deraining [16, 21,25,51,61,65,76], dehazing [1,2,10,22,42,69], desnowing [29,44,44,73] and rain drop removal [37,39,40,66] have been extensively explored in the literature.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various image restoration tasks. No dataset names are provided in the context.",
      "processing_time": 58.26229476928711,
      "citing_paper_id": "244714491",
      "cited_paper_id": 73439498
    },
    {
      "context_text": "Adverse weather removal problems like deraining [16, 21,25,51,61,65,76], dehazing [1,2,10,22,42,69], desnowing [29,44,44,73] and rain drop removal [37,39,40,66] have been extensively explored in the literature.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various image restoration tasks. No dataset names are provided in the context.",
      "processing_time": 58.26229476928711,
      "citing_paper_id": "244714491",
      "cited_paper_id": 207217221
    },
    {
      "context_text": "Adverse weather removal problems like deraining [16, 21,25,51,61,65,76], dehazing [1,2,10,22,42,69], desnowing [29,44,44,73] and rain drop removal [37,39,40,66] have been extensively explored in the literature.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various image restoration tasks. No dataset names are provided in the context.",
      "processing_time": 58.26229476928711,
      "citing_paper_id": "244714491",
      "cited_paper_id": 218487055
    },
    {
      "context_text": "Adverse weather removal problems like deraining [16, 21,25,51,61,65,76], dehazing [1,2,10,22,42,69], desnowing [29,44,44,73] and rain drop removal [37,39,40,66] have been extensively explored in the literature.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various image restoration tasks. No dataset names are provided in the context.",
      "processing_time": 58.26229476928711,
      "citing_paper_id": "244714491",
      "cited_paper_id": 235702784
    },
    {
      "context_text": "[66] proposed using temporal information to perform video-based raindrop removal.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for raindrop removal in videos.",
      "processing_time": 57.90666055679321,
      "citing_paper_id": "244714491",
      "cited_paper_id": 254848
    },
    {
      "context_text": "Recently, Convolutional Neural Networks (CNNs) based solutions have been explored extensively for deraining [11, 37, 53, 56, 61, 63, 69, 70, 76], dehazing [8, 19, 41, 57, 69, 71, 72], desnowing [29, 44, 73] and raindrop removal [37, 40, 66].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various CNN-based methods for image restoration tasks. No verifiable resources are identified.",
      "processing_time": 58.42325472831726,
      "citing_paper_id": "244714491",
      "cited_paper_id": 254848
    },
    {
      "context_text": "Recently, Convolutional Neural Networks (CNNs) based solutions have been explored extensively for deraining [11, 37, 53, 56, 61, 63, 69, 70, 76], dehazing [8, 19, 41, 57, 69, 71, 72], desnowing [29, 44, 73] and raindrop removal [37, 40, 66].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various CNN-based methods for image restoration tasks. No verifiable resources are identified.",
      "processing_time": 58.42325472831726,
      "citing_paper_id": "244714491",
      "cited_paper_id": 4539586
    },
    {
      "context_text": "Recently, Convolutional Neural Networks (CNNs) based solutions have been explored extensively for deraining [11, 37, 53, 56, 61, 63, 69, 70, 76], dehazing [8, 19, 41, 57, 69, 71, 72], desnowing [29, 44, 73] and raindrop removal [37, 40, 66].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various CNN-based methods for image restoration tasks. No verifiable resources are identified.",
      "processing_time": 58.42325472831726,
      "citing_paper_id": "244714491",
      "cited_paper_id": 11922819
    },
    {
      "context_text": "Recently, Convolutional Neural Networks (CNNs) based solutions have been explored extensively for deraining [11, 37, 53, 56, 61, 63, 69, 70, 76], dehazing [8, 19, 41, 57, 69, 71, 72], desnowing [29, 44, 73] and raindrop removal [37, 40, 66].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various CNN-based methods for image restoration tasks. No verifiable resources are identified.",
      "processing_time": 58.42325472831726,
      "citing_paper_id": "244714491",
      "cited_paper_id": 17115407
    },
    {
      "context_text": "Recently, Convolutional Neural Networks (CNNs) based solutions have been explored extensively for deraining [11, 37, 53, 56, 61, 63, 69, 70, 76], dehazing [8, 19, 41, 57, 69, 71, 72], desnowing [29, 44, 73] and raindrop removal [37, 40, 66].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various CNN-based methods for image restoration tasks. No verifiable resources are identified.",
      "processing_time": 58.42325472831726,
      "citing_paper_id": "244714491",
      "cited_paper_id": 22540825
    },
    {
      "context_text": "Recently, Convolutional Neural Networks (CNNs) based solutions have been explored extensively for deraining [11, 37, 53, 56, 61, 63, 69, 70, 76], dehazing [8, 19, 41, 57, 69, 71, 72], desnowing [29, 44, 73] and raindrop removal [37, 40, 66].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various CNN-based methods for image restoration tasks. No verifiable resources are identified.",
      "processing_time": 58.42325472831726,
      "citing_paper_id": "244714491",
      "cited_paper_id": 73439498
    },
    {
      "context_text": "Recently, Convolutional Neural Networks (CNNs) based solutions have been explored extensively for deraining [11, 37, 53, 56, 61, 63, 69, 70, 76], dehazing [8, 19, 41, 57, 69, 71, 72], desnowing [29, 44, 73] and raindrop removal [37, 40, 66].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various CNN-based methods for image restoration tasks. No verifiable resources are identified.",
      "processing_time": 58.42325472831726,
      "citing_paper_id": "244714491",
      "cited_paper_id": 227255179
    },
    {
      "context_text": "Recently, Convolutional Neural Networks (CNNs) based solutions have been explored extensively for deraining [11, 37, 53, 56, 61, 63, 69, 70, 76], dehazing [8, 19, 41, 57, 69, 71, 72], desnowing [29, 44, 73] and raindrop removal [37, 40, 66].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various CNN-based methods for image restoration tasks. No verifiable resources are identified.",
      "processing_time": 58.42325472831726,
      "citing_paper_id": "244714491",
      "cited_paper_id": 233997468
    },
    {
      "context_text": "The computation in the FFN block can be summarized as follows: FFNi(Xi) =MLP (GELU(DWC(MLP (Xi)))) + Xi, where X refers to self-attention features, DWC is depthwise convolution [7], GELU is Gaussian error linear units [14], MLP is multi-layer perceptron, i indicates the stage.",
      "catation_intent": "none",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only methods and components of a neural network architecture.",
      "processing_time": 57.815332651138306,
      "citing_paper_id": "244714491",
      "cited_paper_id": 2375110
    },
    {
      "context_text": "The computation in the FFN block can be summarized as follows: FFNi(Xi) =MLP (GELU(DWC(MLP (Xi)))) + Xi, where X refers to self-attention features, DWC is depthwise convolution [7], GELU is Gaussian error linear units [14], MLP is multi-layer perceptron, i indicates the stage.",
      "catation_intent": "none",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only methods and components of a neural network architecture.",
      "processing_time": 57.815332651138306,
      "citing_paper_id": "244714491",
      "cited_paper_id": 125617073
    },
    {
      "context_text": "Zhang and Patel [68] proposed a pyramid CNN network for image dehazing.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions a method (pyramid CNN network) for image dehazing but does not reference any specific dataset.",
      "processing_time": 58.10319805145264,
      "citing_paper_id": "244714491",
      "cited_paper_id": 4054776
    },
    {
      "context_text": "For rain drop removal, we compare the performance with state-ofthe-art methods like Attention GAN [37], Quan et al.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only state-of-the-art methods for raindrop removal. No verifiable datasets are referenced.",
      "processing_time": 58.11976170539856,
      "citing_paper_id": "244714491",
      "cited_paper_id": 4539586
    },
    {
      "context_text": "All of these models are trained on All-Weather and tested on the Raindrop test dataset.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "All-Weather",
        "Raindrop test dataset"
      ],
      "dataset_descriptions": {
        "All-Weather": "Used for training models to handle various weather conditions, focusing on robustness and generalization in image restoration tasks.",
        "Raindrop test dataset": "Used for evaluating the performance of trained models specifically on raindrop removal, assessing accuracy and effectiveness in real-world scenarios."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions 'All-Weather' and 'Raindrop test dataset', which are specific datasets used for training and testing models in image restoration, particularly for raindrop removal.",
      "processing_time": 61.35992693901062,
      "citing_paper_id": "244714491",
      "cited_paper_id": 4539586
    },
    {
      "context_text": "Rain drop [37] is modelled as I = (1−M) B + R, (1) where I is the degraded image, M is the mask, B is the background and R is the raindrop residual.",
      "catation_intent": [],
      "resource_type": [],
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.3,
      "reasoning": "JSON decode error: Extra data: line 9 column 1 (char 355), response: ```json\n{\n    \"reasoning\": \"The citation does not mention any specific dataset names, only a model o",
      "processing_time": 58.45858407020569,
      "citing_paper_id": "244714491",
      "cited_paper_id": 4539586
    },
    {
      "context_text": "Zhang et al. [70] proposed using a conditional GAN for image deraining.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (conditional GAN) for image deraining.",
      "processing_time": 57.601635217666626,
      "citing_paper_id": "244714491",
      "cited_paper_id": 4539586
    },
    {
      "context_text": "[37] used an attention GAN to remove raindrop and also introduced a new dataset.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions a new dataset introduced by [37], but does not provide a specific name for the dataset. The title of the cited paper confirms the introduction of a new dataset but does not specify a name.",
      "processing_time": 58.51398730278015,
      "citing_paper_id": "244714491",
      "cited_paper_id": 4539586
    },
    {
      "context_text": "Qian et al. [37] used an attention GAN to remove raindrop and also introduced a new dataset.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "new dataset"
      ],
      "dataset_descriptions": {
        "new dataset": "Introduced for raindrop removal, used to evaluate the proposed method in the context of all-in-one image restoration."
      },
      "confidence_score": 0.8,
      "reasoning": "The context mentions a new dataset introduced by Qian et al. for raindrop removal, which is relevant to image restoration.",
      "processing_time": 58.99296760559082,
      "citing_paper_id": "244714491",
      "cited_paper_id": 4539586
    },
    {
      "context_text": "For rain drop removal, we compare the performance with state-of-the-art methods like Attention GAN [37], Quan et al. [40], and complementary cascaded network (CCN) [39].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only models and methods used for raindrop removal. No verifiable resources are identified.",
      "processing_time": 57.84872126579285,
      "citing_paper_id": "244714491",
      "cited_paper_id": 4539586
    },
    {
      "context_text": "For rain drop removal, we compare the performance with state-of-the-art methods like Attention GAN [37], Quan et al. [40], and complementary cascaded network (CCN) [39].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only models and methods used for raindrop removal. No verifiable resources are identified.",
      "processing_time": 57.84872126579285,
      "citing_paper_id": "244714491",
      "cited_paper_id": 235702784
    },
    {
      "context_text": "For rain+fog removal, we compare with HRGAN [21], Details-Net [11], Recurrent squeeze-and-excitation context aggregation Net (RESCAN) [25], and Multi-Stage Progressive Restoration Network (MPRNet) [67].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only models and methods used for rain and fog removal. No verifiable resources are identified.",
      "processing_time": 57.73527407646179,
      "citing_paper_id": "244714491",
      "cited_paper_id": 4539586
    },
    {
      "context_text": "For rain+fog removal, we compare with HRGAN [21], Details-Net [11], Recurrent squeeze-and-excitation context aggregation Net (RESCAN) [25], and Multi-Stage Progressive Restoration Network (MPRNet) [67].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only models and methods used for rain and fog removal. No verifiable resources are identified.",
      "processing_time": 57.73527407646179,
      "citing_paper_id": "244714491",
      "cited_paper_id": 17115407
    },
    {
      "context_text": "The training data consists of 9,000 images sampled from Snow100K [29], 1,069 images from Raindrop [37] and 9,000 images of Outdoor-Rain [21].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Snow100K",
        "Raindrop",
        "Outdoor-Rain"
      ],
      "dataset_descriptions": {
        "Snow100K": "Used for training the model to handle snow removal, providing a large set of images with varying snow conditions.",
        "Raindrop": "Used for training the model to remove raindrops from images, focusing on realistic raindrop patterns and their effects on image quality.",
        "Outdoor-Rain": "Used for training the model to handle outdoor rain scenarios, providing diverse real-world rain conditions."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions specific datasets used for training, which are relevant to the topic of image restoration.",
      "processing_time": 61.61790609359741,
      "citing_paper_id": "244714491",
      "cited_paper_id": 4539586
    },
    {
      "context_text": "We use the Test1 dataset [21, 23], the RainDrop test dataset [37] and the Snow100k-L test set [29] for testing our method.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "DIV2K",
        "Rain100L",
        "Snow100k-L"
      ],
      "dataset_descriptions": {
        "DIV2K": "Used for testing the method's performance on high-resolution images, focusing on image restoration quality and robustness.",
        "Rain100L": "Used to evaluate the method's effectiveness in removing raindrops from images, assessing the clarity and detail recovery.",
        "Snow100k-L": "Used to test the method's ability to remove snow from images, evaluating the restoration of visual details and scene coherence."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions three specific datasets used for testing the method: Test1, RainDrop test dataset, and Snow100k-L test set. These datasets are clearly identified and used for evaluating the performance of the method.",
      "processing_time": 65.94182753562927,
      "citing_paper_id": "244714491",
      "cited_paper_id": 4539586
    },
    {
      "context_text": "We use the Test1 dataset [21, 23], the RainDrop test dataset [37] and the Snow100k-L test set [29] for testing our method.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "DIV2K",
        "Rain100L",
        "Snow100k-L"
      ],
      "dataset_descriptions": {
        "DIV2K": "Used for testing the method's performance on high-resolution images, focusing on image restoration quality and robustness.",
        "Rain100L": "Used to evaluate the method's effectiveness in removing raindrops from images, assessing the clarity and detail recovery.",
        "Snow100k-L": "Used to test the method's ability to remove snow from images, evaluating the restoration of visual details and scene coherence."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions three specific datasets used for testing the method: Test1, RainDrop test dataset, and Snow100k-L test set. These datasets are clearly identified and used for evaluating the performance of the method.",
      "processing_time": 65.94182753562927,
      "citing_paper_id": "244714491",
      "cited_paper_id": 219978541
    },
    {
      "context_text": "[43] proposed pre-processing a hazy image to generate multiple inputs thus introducing color distortions to perform dehazing.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for pre-processing hazy images.",
      "processing_time": 57.088200092315674,
      "citing_paper_id": "244714491",
      "cited_paper_id": 4563057
    },
    {
      "context_text": "We use an Adam optimizer [17] and a learning rate of 0.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (Adam optimizer).",
      "processing_time": 56.830564975738525,
      "citing_paper_id": "244714491",
      "cited_paper_id": 6628106
    },
    {
      "context_text": "[70] proposed using a conditional GAN for image deraining.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (conditional GAN) for image deraining.",
      "processing_time": 57.171382188797,
      "citing_paper_id": "244714491",
      "cited_paper_id": 11922819
    },
    {
      "context_text": "Early methods for weather removal involve modelling priors for weather conditions using empirical observations [13, 45, 46].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only methods for weather removal. The cited papers do not provide additional context to identify datasets.",
      "processing_time": 57.55128502845764,
      "citing_paper_id": "244714491",
      "cited_paper_id": 13133466
    },
    {
      "context_text": "Early methods for weather removal involve modelling priors for weather conditions using empirical observations [13, 45, 46].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only methods for weather removal. The cited papers do not provide additional context to identify datasets.",
      "processing_time": 57.55128502845764,
      "citing_paper_id": "244714491",
      "cited_paper_id": 49333383
    },
    {
      "context_text": "In the original transformer decoder [50], an autoregressive decoder is used to predict the output sequence one element at a time.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a method (transformer decoder).",
      "processing_time": 56.93796992301941,
      "citing_paper_id": "244714491",
      "cited_paper_id": 13756489
    },
    {
      "context_text": "These transformer blocks are similar to encoder-decoder transformer blocks [50].",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (transformer blocks).",
      "processing_time": 56.99418020248413,
      "citing_paper_id": "244714491",
      "cited_paper_id": 13756489
    },
    {
      "context_text": "5830 DetailsNet + Dehaze (DRF) [11] CVPR 2017 15.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for removing rain from images using a deep detail network.",
      "processing_time": 57.48964309692383,
      "citing_paper_id": "244714491",
      "cited_paper_id": 17115407
    },
    {
      "context_text": "For example, while using Details-Net and RESCAN for deraining, we apply Multi-scale boosted de-hazing network (MSBDN) [8] for dehazing.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only models and methods. There are no clear identifiers for datasets.",
      "processing_time": 57.268178939819336,
      "citing_paper_id": "244714491",
      "cited_paper_id": 17115407
    },
    {
      "context_text": "Task Specific DetailsNet + Dehaze (DHF) [11] CVPR 2017 13.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (DHF) and a conference (CVPR 2017).",
      "processing_time": 57.47647500038147,
      "citing_paper_id": "244714491",
      "cited_paper_id": 17115407
    },
    {
      "context_text": "Task Specific DetailsNet [11] CVPR 2017 19.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (DetailsNet) for removing rain from images.",
      "processing_time": 57.21262884140015,
      "citing_paper_id": "244714491",
      "cited_paper_id": 17115407
    },
    {
      "context_text": "This drastically affects the performance of many computer vision algorithms like detection, segmentation and depth estimation [3, 5, 41, 52, 59] which are important parts of autonomous navigation and surveillance systems [28,34–36].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general computer vision algorithms and their applications. No verifiable resources are identified.",
      "processing_time": 57.32057332992554,
      "citing_paper_id": "244714491",
      "cited_paper_id": 52211898
    },
    {
      "context_text": "This drastically affects the performance of many computer vision algorithms like detection, segmentation and depth estimation [3, 5, 41, 52, 59] which are important parts of autonomous navigation and surveillance systems [28,34–36].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general computer vision algorithms and their applications. No verifiable resources are identified.",
      "processing_time": 57.32057332992554,
      "citing_paper_id": "244714491",
      "cited_paper_id": 57759394
    },
    {
      "context_text": "[61] used a recurrent network to decompose rain layers to different layers of various streak types to remove the rain.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific dataset names, only a method for rain removal. No verifiable dataset is referenced.",
      "processing_time": 56.94811677932739,
      "citing_paper_id": "244714491",
      "cited_paper_id": 73439498
    },
    {
      "context_text": "Recently, Convolutional Neural Networks (CNNs) based solutions have been explored extensively for deraining [11, 36, 52, 55, 60, 62, 68, 69, 75], dehazing [8, 19, 40, 56, 68, 70, 71], desnowing [29, 43, 72] and raindrop removal [36, 39, 65].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to various papers exploring CNN-based solutions for image restoration tasks.",
      "processing_time": 57.16285586357117,
      "citing_paper_id": "244714491",
      "cited_paper_id": 195657934
    },
    {
      "context_text": "[20] proposed a stacked dense network for snow removal.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for snow removal.",
      "processing_time": 56.716206789016724,
      "citing_paper_id": "244714491",
      "cited_paper_id": 202143808
    },
    {
      "context_text": "This drastically affects the performance of many computer vision algorithms like detection, segmentation and depth estimation [3, 5, 41, 52, 59] which are important parts of autonomous navigation and surveillance systems [28, 34–36].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general computer vision algorithms and their applications. There are no clear identifiers for datasets.",
      "processing_time": 57.16079616546631,
      "citing_paper_id": "244714491",
      "cited_paper_id": 219629285
    },
    {
      "context_text": "Multi Task All-in-One [23] CVPR 2020 24.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method or approach. The title of the cited paper also does not indicate the use of a specific dataset.",
      "processing_time": 57.43272018432617,
      "citing_paper_id": "244714491",
      "cited_paper_id": 219978541
    },
    {
      "context_text": "All-in-One Weather Removal: All-in-One Network [23] was proposed to handle multiple weather degradations us-ing a single network.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (All-in-One Network) for handling multiple weather degradations.",
      "processing_time": 57.047951221466064,
      "citing_paper_id": "244714491",
      "cited_paper_id": 219978541
    },
    {
      "context_text": "All-in-One method [23] generalizes the adverse weather removal problem as B = D(Ep(Ip)), (4) where E corresponds to the encoder and D corresponds to the decoder.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for bad weather removal using architectural search.",
      "processing_time": 56.7404351234436,
      "citing_paper_id": "244714491",
      "cited_paper_id": 219978541
    },
    {
      "context_text": "(b) All-in-One Network [23] proposes a framework with separate encoders for each task but a generic decoder.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method or framework. The context is about the architecture of the network, not the data used.",
      "processing_time": 57.11688780784607,
      "citing_paper_id": "244714491",
      "cited_paper_id": 219978541
    },
    {
      "context_text": "In this work, we follow a similar formulation of all adverse weather removal as where T corresponds to TransWeather which consists of a weather agnostic encoder and decoder network unlike All-in-One Network.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a method (TransWeather) and a comparison to another method (All-in-One Network).",
      "processing_time": 56.88908863067627,
      "citing_paper_id": "244714491",
      "cited_paper_id": 219978541
    },
    {
      "context_text": "[23] proposed an All-in-One bad weather removal network which was the first work to propose an algorithm that takes in an image degraded by any weather condition as input and predicts the clean image.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method or model. The focus is on the algorithm and its capability to handle various weather conditions.",
      "processing_time": 56.623924255371094,
      "citing_paper_id": "244714491",
      "cited_paper_id": 219978541
    },
    {
      "context_text": "We train our network on a combination of images degraded from a variety of adverse weather conditions similar to All-in-One Network [23].",
      "catation_intent": "reusable resource",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions training on images degraded from various weather conditions but does not specify a named dataset. The reference to 'All-in-One Network' is a method, not a dataset.",
      "processing_time": 56.74875259399414,
      "citing_paper_id": "244714491",
      "cited_paper_id": 219978541
    },
    {
      "context_text": "14 Task Specific SOTA All-in-One [23] TransWeather (Ours)",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method or model called 'TransWeather'. No verifiable datasets are referenced.",
      "processing_time": 56.44246554374695,
      "citing_paper_id": "244714491",
      "cited_paper_id": 219978541
    },
    {
      "context_text": "To the best of our knowledge, no other methods apart from Allin-One network [23] have been proposed for a generic adverse weather removal in the literature.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (Allin-One network) for bad weather removal. No datasets are referenced for training, evaluation, or any other use.",
      "processing_time": 56.74738669395447,
      "citing_paper_id": "244714491",
      "cited_paper_id": 219978541
    },
    {
      "context_text": "Multi Task All-in-One [23] CVPR 2020 28.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method or approach. The title of the cited paper also does not indicate the use of a specific dataset.",
      "processing_time": 56.4951376914978,
      "citing_paper_id": "244714491",
      "cited_paper_id": 219978541
    },
    {
      "context_text": "TransWeather has 31 M parameters which are less than that of All-in-One Network which has 44 M parameters.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only model parameter counts. No verifiable resources are identified.",
      "processing_time": 55.996360063552856,
      "citing_paper_id": "244714491",
      "cited_paper_id": 219978541
    },
    {
      "context_text": "All-in-One Weather Removal: All-in-One Network [23] was proposed to handle multiple weather degradations using a single network.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (All-in-One Network) for handling multiple weather degradations.",
      "processing_time": 56.10663652420044,
      "citing_paper_id": "244714491",
      "cited_paper_id": 219978541
    },
    {
      "context_text": "[6] proposed JSTASR in which a size and transparency aware method was proposed to remove snow.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for snow removal.",
      "processing_time": 55.66666007041931,
      "citing_paper_id": "244714491",
      "cited_paper_id": 226308542
    },
    {
      "context_text": "Chen et al. [6] proposed JSTASR in which a size and transparency aware method was proposed to remove snow.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for snow removal.",
      "processing_time": 55.51077198982239,
      "citing_paper_id": "244714491",
      "cited_paper_id": 226308542
    },
    {
      "context_text": "For snow removal, we compare with Desnow-Net [29], JSTASR [6] and Deep Dense Multi-Scale Network (DDMSNet) [73].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context mentions methods/models (Desnow-Net, JSTASR, DDMSNet) but does not reference any specific datasets. The context is focused on comparing different algorithms for snow removal.",
      "processing_time": 56.38851499557495,
      "citing_paper_id": "244714491",
      "cited_paper_id": 226308542
    },
    {
      "context_text": "For snow removal, we compare with Desnow-Net [29], JSTASR [6] and Deep Dense Multi-Scale Network (DDMSNet) [73].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context mentions methods/models (Desnow-Net, JSTASR, DDMSNet) but does not reference any specific datasets. The context is focused on comparing different algorithms for snow removal.",
      "processing_time": 56.38851499557495,
      "citing_paper_id": "244714491",
      "cited_paper_id": 233997468
    },
    {
      "context_text": "Transformers in low-level vision: Since the introduction of Vision Transformer (ViT) [9] for visual recognition, transformers have been widely adopted for various computer vision tasks [12, 31, 49, 60, 75].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only the adoption of transformers in various computer vision tasks. No verifiable resources are identified.",
      "processing_time": 55.837756872177124,
      "citing_paper_id": "244714491",
      "cited_paper_id": 229924195
    },
    {
      "context_text": "Transformers in low-level vision: Since the introduction of Vision Transformer (ViT) [9] for visual recognition, transformers have been widely adopted for various computer vision tasks [12, 31, 49, 60, 75].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only the adoption of transformers in various computer vision tasks. No verifiable resources are identified.",
      "processing_time": 55.837756872177124,
      "citing_paper_id": "244714491",
      "cited_paper_id": 231986084
    },
    {
      "context_text": "Transformers in low-level vision: Since the introduction of Vision Transformer (ViT) [9] for visual recognition, transformers have been widely adopted for various computer vision tasks [12, 31, 49, 60, 75].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only the adoption of transformers in various computer vision tasks. No verifiable resources are identified.",
      "processing_time": 55.837756872177124,
      "citing_paper_id": "244714491",
      "cited_paper_id": 235624247
    },
    {
      "context_text": "The FFN block used here has a slight variation from ViT as we introduce using depth-wise convolution to MLP inspired from [26, 58, 59].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a methodological variation in the FFN block of a vision transformer.",
      "processing_time": 55.66249442100525,
      "citing_paper_id": "244714491",
      "cited_paper_id": 232417787
    },
    {
      "context_text": "[72] proposed a hierarchial density aware network for image dehazing.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for image dehazing.",
      "processing_time": 55.2156240940094,
      "citing_paper_id": "244714491",
      "cited_paper_id": 233997468
    },
    {
      "context_text": "It can be noted that some recent methods like CCN and DDMSNet when ﬁne-tuned on the individual datasets outperform All-in-One.",
      "catation_intent": "findings",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only methods (CCN and DDMSNet). The citation is discussing the performance of these methods on unspecified datasets.",
      "processing_time": 55.86921548843384,
      "citing_paper_id": "244714491",
      "cited_paper_id": 233997468
    },
    {
      "context_text": "It can be noted that some recent methods like CCN and DDMSNet when ﬁne-tuned on the individual datasets outperform All-in-One.",
      "catation_intent": "findings",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only methods (CCN and DDMSNet). The citation is discussing the performance of these methods on unspecified datasets.",
      "processing_time": 55.86921548843384,
      "citing_paper_id": "244714491",
      "cited_paper_id": 235702784
    },
    {
      "context_text": "Recently, DDMSNet [72] proposed a deep dense multiscale network using semantic and geometric priors for snow removal.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (DDMSNet) for snow removal. No dataset names are provided in the context.",
      "processing_time": 55.78445339202881,
      "citing_paper_id": "244714491",
      "cited_paper_id": 233997468
    },
    {
      "context_text": "[40], and complementary cascaded network (CCN) [39].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods or models. There are no clear identifiers for datasets in the provided context.",
      "processing_time": 55.54916262626648,
      "citing_paper_id": "244714491",
      "cited_paper_id": 235702784
    },
    {
      "context_text": "[39] used a complementary cascaded network to remove rain streaks and raindrops in a unified framework.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for removing rain streaks and raindrops.",
      "processing_time": 55.10517501831055,
      "citing_paper_id": "244714491",
      "cited_paper_id": 235702784
    },
    {
      "context_text": "In ETDNet [38], an efficient transformer block to extract features in a coarse to fine way for image deraining was proposed.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (ETDNet) for image deraining.",
      "processing_time": 55.37092638015747,
      "citing_paper_id": "244714491",
      "cited_paper_id": 237403103
    },
    {
      "context_text": "Transformer-based methods have also been explored for weather removal tasks achieving better performance than CNNs [38, 48, 74].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and performance comparisons.",
      "processing_time": 54.90593409538269,
      "citing_paper_id": "244714491",
      "cited_paper_id": 237403103
    },
    {
      "context_text": "have also been explored for weather removal tasks achieving better performance than CNNs [38, 48, 74].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models and methods. The context is about comparing performance of different models for weather removal tasks.",
      "processing_time": 55.26967215538025,
      "citing_paper_id": "244714491",
      "cited_paper_id": 237403103
    },
    {
      "context_text": "Recently, Convolutional Neural Networks (CNNs) based solutions have been explored extensively for deraining [11, 37, 53, 56, 61, 63, 69, 70, 76], de-hazing [8,19,41,57,69,71,72], desnowing [29,44,73] and raindrop removal [37, 40, 66].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various CNN-based solutions for image restoration tasks. No verifiable resources are identified.",
      "processing_time": 55.231606245040894,
      "citing_paper_id": "244714491",
      "cited_paper_id": 237403103
    },
    {
      "context_text": "In ETDNet [38], an efﬁcient transformer block to extract features in a coarse to ﬁne way for image deraining was proposed.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions ETDNet, which is a method for image deraining, not a dataset. No specific datasets are mentioned in the citation.",
      "processing_time": 55.13686728477478,
      "citing_paper_id": "244714491",
      "cited_paper_id": 237403103
    },
    {
      "context_text": "Transformers are good at extracting rich global information when compared to CNNs [9].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a comparison between transformers and CNNs.",
      "processing_time": 54.50935912132263,
      "citing_paper_id": "244714491",
      "cited_paper_id": 237403103
    },
    {
      "context_text": "In order to reduce the impact of bad weather such as rain and haze on outdoor computer vision systems, image rain removal and haze removal have become important topics in the field of computer vision [1-4].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general topics within computer vision. No dataset names are present in the text.",
      "processing_time": 54.85362410545349,
      "citing_paper_id": "268569639",
      "cited_paper_id": 132226104
    }
  ],
  "filtering_stats": {
    "original_papers_count": 80,
    "filtered_papers_count": 79,
    "filtered_percentage": "98.8%"
  },
  "extraction_stats": {
    "unique_contexts_processed": 2405,
    "total_citation_instances": 4008,
    "successful_extractions": 561,
    "failed_extractions": 3447,
    "total_processing_time": 168.73819375038147
  }
}