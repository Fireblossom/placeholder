{
  "summary": {
    "total_unique_datasets": 210,
    "total_dataset_mentions": 772,
    "unique_dataset_names": 210,
    "extraction_successful": 1542,
    "extraction_failed": 9106,
    "unique_contexts_processed": 7105,
    "total_citation_instances": 10648,
    "total_processing_time": 465.3751790523529
  },
  "datasets_sorted_by_citation_count": [
    {
      "cited_paper_id": "244346144",
      "citation_count": 0,
      "total_dataset_mentions": 46,
      "unique_datasets": [
        "GoPro"
      ],
      "dataset_details": [
        {
          "dataset_name": "GoPro",
          "dataset_description": "Used for low-light enhancement tasks, offering a dataset to test and improve the performance of algorithms in low-light conditions. | Used for deblurring tasks, providing a benchmark for evaluating image restoration algorithms in motion blur scenarios.",
          "citing_paper_id": "271874811",
          "cited_paper_id": 52008443,
          "context_text": "Deblurring and low-light enhancement tasks use the GoPro [55] and LOL-v1 [56] datasets respectively.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions two specific datasets, GoPro and LOL-v1, which are used for deblurring and low-light enhancement tasks, respectively.",
          "citing_paper_doi": "10.48550/arXiv.2408.08091",
          "cited_paper_doi": null,
          "citing_paper_url": "https://www.semanticscholar.org/paper/99facb268ce89958bcd541076286efac30652b46",
          "cited_paper_url": "https://www.semanticscholar.org/paper/c7798bbbcd2e85593a2b2d37b92dabac63420cd8",
          "citing_paper_year": 2024,
          "cited_paper_year": 2018
        },
        {
          "dataset_name": "GoPro",
          "dataset_description": "Used to evaluate and compare the performance of the proposed deblurring method against state-of-the-art techniques, focusing on image quality and restoration accuracy. | Used to assess the effectiveness of the proposed method in real-world scenarios, providing a realistic testbed for image deblurring techniques. | Used to train the deblurring generator, focusing on real-world blur scenarios to improve deblurring performance. | Used to train and evaluate the method for dynamic scene deblurring, focusing on real-world blur scenarios to enhance practical applicability. | Used to train the deblurring generator, focusing on dynamic scene deblurring using a multi-scale convolutional neural network. | Used to train a deep multi-scale convolutional neural network for dynamic scene deblurring, focusing on reblurring and deblurring processes. | Used to evaluate the performance of MPRNet and the proposed method on dynamic scene deblurring, focusing on quantitative metrics and visual quality. | Used to train and evaluate the method for dynamic scene deblurring, focusing on improving image quality in motion-blurred scenes. | Used to train the deblurring generator, focusing on real-world blur scenarios to enhance the model's generalization capabilities. | Used to train the encoder, providing high-quality, sharp images for the deblurring process to improve overall restoration performance. | Used to evaluate the performance of the joint reblurring and deblurring framework, focusing on dynamic scene deblurring using a widely recognized benchmark.",
          "citing_paper_id": "251468222",
          "cited_paper_id": 8671030,
          "context_text": "We first train the framework of reblurring and deblurring on GoPro dataset [24].",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions the GoPro dataset, which is a specific, verifiable dataset used for training a deblurring framework.",
          "citing_paper_doi": "10.48550/arXiv.2208.05244",
          "cited_paper_doi": "10.1109/CVPR.2017.35",
          "citing_paper_url": "https://www.semanticscholar.org/paper/9f40a0f0879bba64853395ef2d0250ae9e545c3c",
          "cited_paper_url": "https://www.semanticscholar.org/paper/65b16da51891a6b98140d425804c8a0fd0299219",
          "citing_paper_year": 2022,
          "cited_paper_year": 2016
        },
        {
          "dataset_name": "GoPro",
          "dataset_description": "Used to evaluate deblurring performance on synthetic dynamic scenes, focusing on the effectiveness of the proposed method compared to state-of-the-art techniques. | Used to assess deblurring performance on real-world images, specifically comparing the proposed method against existing state-of-the-art techniques in challenging conditions. | Used to assess deblurring performance on synthetic images, specifically comparing the proposed method against existing state-of-the-art approaches. | Used to evaluate deblurring performance on real-world images, focusing on the robustness and effectiveness of the proposed method in practical scenarios. | Used to train models for motion deblurring, focusing on dynamic scenes and employing a deep multi-scale convolutional neural network.",
          "citing_paper_id": "264289165",
          "cited_paper_id": 8671030,
          "context_text": "For deblurring, we use the motion deblurring dataset GoPro [34] to train the models.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions the GoPro dataset, which is a specific, verifiable dataset used for training models in the context of motion deblurring.",
          "citing_paper_doi": "10.48550/arXiv.2310.11881",
          "cited_paper_doi": "10.1109/CVPR.2017.35",
          "citing_paper_url": "https://www.semanticscholar.org/paper/a9a441bbbbf0c1e826e36cc864c728d3516a3608",
          "cited_paper_url": "https://www.semanticscholar.org/paper/65b16da51891a6b98140d425804c8a0fd0299219",
          "citing_paper_year": 2023,
          "cited_paper_year": 2016
        },
        {
          "dataset_name": "GoPro",
          "dataset_description": "Used for training a deep multi-scale convolutional neural network for dynamic scene deblurring, focusing on synthetic motion blur. | Used for evaluating the performance of the deblurring method on real-world images with natural motion blur.",
          "citing_paper_id": "272444309",
          "cited_paper_id": 8671030,
          "context_text": "For single image deblurring, we utilize the GoPro [45] dataset for training and evaluate our method on the RealBlur [52] dataset.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions two specific datasets, GoPro and RealBlur, which are used for training and evaluation in the context of single image deblurring.",
          "citing_paper_doi": "10.1145/3694973",
          "cited_paper_doi": "10.1109/CVPR.2017.35",
          "citing_paper_url": "https://www.semanticscholar.org/paper/2893ee935e0c5113a844cfa54f6e79dfda827bba",
          "cited_paper_url": "https://www.semanticscholar.org/paper/65b16da51891a6b98140d425804c8a0fd0299219",
          "citing_paper_year": 2024,
          "cited_paper_year": 2016
        },
        {
          "dataset_name": "GoPro",
          "dataset_description": "Used to evaluate single image motion deblurring on real-world images, assessing the method's performance in practical scenarios. | Used to evaluate the performance of image restoration methods, specifically measuring PSNR improvement in motion deblurring tasks. | Used to evaluate single image motion deblurring on synthetic images, focusing on the effectiveness of the method in restoring degraded images. | Used to evaluate all-in-one image restoration methods, focusing on the performance of deblurring algorithms on synthetic images. | Used to assess the performance of single image motion deblurring algorithms on real-world images, providing a realistic benchmark for deblurring techniques. | Used to evaluate single image motion deblurring on synthetic images, focusing on the effectiveness of the proposed method in removing blur. | Used to evaluate single image motion deblurring algorithms on synthetic data, focusing on the effectiveness of the proposed method in removing blur.",
          "citing_paper_id": "257255385",
          "cited_paper_id": 201624746,
          "context_text": "2 shows the experimental results for single image motion deblurring on synthetic datasets (GoPro [58], HIDE [66]) and real dataset (RealBlur-R [64]), respectively.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions specific datasets used for evaluating single image motion deblurring, which are relevant to the topic of image restoration.",
          "citing_paper_doi": "10.1109/CVPR52729.2023.01753",
          "cited_paper_doi": "10.1109/ICCV.2019.00567",
          "citing_paper_url": "https://www.semanticscholar.org/paper/2920b885278a6973a306b57201e3fc3273b71132",
          "cited_paper_url": "https://www.semanticscholar.org/paper/20a4b3353579525f0b76ec42e17a2284b4453f9a",
          "citing_paper_year": 2023,
          "cited_paper_year": 2019
        },
        {
          "dataset_name": "GoPro",
          "dataset_description": "Used to evaluate single image motion deblurring methods, focusing on the effectiveness of the proposed approach in a controlled synthetic environment. | Used to evaluate all-in-one image restoration methods, specifically assessing performance on real-world blurred images using PSNR and SSIM metrics. | Used to evaluate single-image motion deblurring algorithms, focusing on real-world blur scenarios to benchmark deblurring performance. | Used to evaluate deblurring algorithms, focusing on real-world blur scenarios and performance metrics such as PSNR and SSIM. | Used to assess the performance of single image motion deblurring algorithms on real-world images, providing a realistic benchmark for deblurring techniques. | Used to evaluate all-in-one image restoration methods, focusing on the performance of deblurring algorithms on synthetic images. | Used to evaluate single image motion deblurring algorithms on real-world data, assessing the practical performance and robustness of the proposed method. | Used to evaluate single image motion deblurring algorithms on synthetic data, focusing on the effectiveness of the proposed method in removing blur.",
          "citing_paper_id": "257255385",
          "cited_paper_id": 222104551,
          "context_text": "3 shows the experimental results for single image motion deblurring on synthetic datasets (GoPro [51], HIDE [59]) and real dataset (RealBlur-R [57]), respectively.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions specific datasets used for evaluating single image motion deblurring algorithms, including GoPro, HIDE, and RealBlur-R. These datasets are clearly identified and used for benchmarking.",
          "citing_paper_doi": "10.1109/CVPR52729.2023.01753",
          "cited_paper_doi": "10.22677/THESIS.200000333386",
          "citing_paper_url": "https://www.semanticscholar.org/paper/2920b885278a6973a306b57201e3fc3273b71132",
          "cited_paper_url": "https://www.semanticscholar.org/paper/2d6c14023087b5d5bd90a88da13e0fa765418d84",
          "citing_paper_year": 2023,
          "cited_paper_year": 2020
        },
        {
          "dataset_name": "GoPro",
          "dataset_description": "Used for image deraining, focusing on removing rain streaks from images to improve visual quality and clarity. | Used for deraining, containing a small set of high-quality rainy images for fine-tuning and validation. | Used for denoising, offering a challenging set of images with varying noise levels for performance evaluation. | Used for dehazing, containing a variety of hazy images to train and evaluate dehazing algorithms. | Used for denoising tasks, evaluating the effectiveness of methods in removing noise while preserving image details. | Used for evaluating rain removal techniques, assessing performance in reducing rain streaks and improving image clarity. | Used for image denoising, specifically to test the ability of the method to reduce noise while preserving important image details. | Used for dehazing experiments, focusing on outdoor scenes to evaluate the performance of dehazing algorithms. | Used to assess image restoration performance, particularly in urban scenes, emphasizing PSNR gains. | Used for image deblurring, specifically to evaluate the effectiveness of the proposed method in restoring sharp images from blurred ones. | Used for deblurring, containing a large set of blurred and sharp image pairs for training and testing. | Used for denoising experiments, providing a benchmark for evaluating image restoration algorithms on natural images. | Used to evaluate dehazing algorithms, providing synthetic and real-world hazy images for benchmarking. | Employed for denoising performance evaluation, containing a diverse set of natural images with added noise. | Used for evaluating single-image dehazing methods, assessing the ability to improve visibility in hazy conditions. | Used for evaluating image restoration methods, focusing on deblurring and stabilization of video frames. | Utilized for assessing rain removal techniques, specifically designed to simulate and remove rain streaks from images. | Used for deraining, offering a moderate-sized dataset of rainy images for model testing. | Used for image dehazing, aimed at enhancing visibility by removing atmospheric haze effects from images. | Used to evaluate image restoration methods, focusing on PSNR improvements in natural images with human-segmented ground truth. | Used for dehazing experiments, providing a comprehensive dataset of hazy images for training and evaluation. | Used for evaluating image restoration methods, focusing on deblurring and enhancing visual quality in natural images. | Used for deraining, providing a diverse set of rainy images for robustness testing. | Used for denoising experiments, specifically focusing on a smaller subset of images for evaluation. | Used for denoising, providing a well-known dataset of clean and noisy images for benchmarking. | Used for deraining, providing a large set of images with rain streaks for training and evaluation.",
          "citing_paper_id": "272680196",
          "cited_paper_id": 64193,
          "context_text": "GoPro [44] Rain100L [45] BSD68 [46] SOTS [47] Average Methods PSNR ↑ SSIM ↑ PSNR ↑ SSIM ↑ PSNR ↑ SSIM ↑ PSNR ↑ SSIM ↑ PSNR ↑ SSIM ↑ HINet [59] 30.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The citation mentions several datasets by name, which are commonly used in image restoration tasks. These datasets are likely used for training and evaluation of image restoration methods.",
          "citing_paper_doi": null,
          "cited_paper_doi": "10.1109/ICCV.2001.937655",
          "citing_paper_url": "https://www.semanticscholar.org/paper/e9ef4cfc510d53d0be69835a34a9195027e4cc51",
          "cited_paper_url": "https://www.semanticscholar.org/paper/9a1ed876196ec9733acb1daa6d65e35ff0414291",
          "citing_paper_year": 2023,
          "cited_paper_year": 2001
        },
        {
          "dataset_name": "GoPro",
          "dataset_description": "Used for evaluating the model on 2,025 samples, assessing the effectiveness of deblurring methods. | Used for training the model on 2,103 image pairs, focusing on image deblurring techniques. | Used to evaluate the performance of motion deblurring techniques, focusing on real-world video sequences with human subjects. | Used to evaluate image deblurring performance, focusing on motion blur correction in real-world scenarios. | Used for evaluating the model on 2,025 test samples, assessing the effectiveness of the deblurring methods. | Used for training the model on 2,103 image pairs for image deblurring, focusing on improving the clarity of motion-blurred images. | Used to assess the effectiveness of image restoration methods, particularly in handling diverse and challenging image degradation scenarios. | Used to assess image deblurring effectiveness, particularly in handling various types of motion blur.",
          "citing_paper_id": "248426764",
          "cited_paper_id": 201624746,
          "context_text": "For image deblurring, similar to [36, 55, 63, 79, 80], we train our model with 2, 103 image pairs from GoPro [47] dataset and evaluate each method on the test sets from GoPro and HIDE [58], which contain 1, 111 and 2, 025 samples, respectively.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions specific datasets used for training and evaluation in the context of image deblurring. GoPro and HIDE are clearly identified and used for training and testing, respectively.",
          "citing_paper_doi": "10.1109/CVPR52688.2022.01688",
          "cited_paper_doi": "10.1109/ICCV.2019.00567",
          "citing_paper_url": "https://www.semanticscholar.org/paper/22755044094d1eff9d2b61bf2861649446d31bf9",
          "cited_paper_url": "https://www.semanticscholar.org/paper/20a4b3353579525f0b76ec42e17a2284b4453f9a",
          "citing_paper_year": 2022,
          "cited_paper_year": 2019
        },
        {
          "dataset_name": "GoPro",
          "dataset_description": "Used for deblurring images, focusing on improving image clarity and sharpness using deep learning techniques. | Used for low-light enhancement, specifically to improve the quality of images captured in low-light conditions using deep retinex decomposition.",
          "citing_paper_id": "270045495",
          "cited_paper_id": 52008443,
          "context_text": "For deblurring and low-light enhancement, we employ the GoPro [30] and the LOL-v1 [46] dataset, respectively.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions two specific datasets, GoPro and LOL-v1, which are used for deblurring and low-light enhancement, respectively.",
          "citing_paper_doi": "10.48550/arXiv.2405.15475",
          "cited_paper_doi": null,
          "citing_paper_url": "https://www.semanticscholar.org/paper/1dc809107ceb27fc74071934878fe2dc9292e57d",
          "cited_paper_url": "https://www.semanticscholar.org/paper/c7798bbbcd2e85593a2b2d37b92dabac63420cd8",
          "citing_paper_year": 2024,
          "cited_paper_year": 2018
        },
        {
          "dataset_name": "GoPro",
          "dataset_description": "Used to evaluate and compare the performance of the proposed method against Restormer and NAFNet, focusing on high-resolution image restoration tasks.",
          "citing_paper_id": "270045495",
          "cited_paper_id": 244346144,
          "context_text": "Notably, our method surpasses Restormer [53] and NAFNet [3] on the GoPro test set by 2 .",
          "confidence_score": 1.0,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions the GoPro test set, which is a specific dataset used for evaluating image restoration methods. The dataset is used to compare the performance of the current method against Restormer and NAFNet.",
          "citing_paper_doi": "10.48550/arXiv.2405.15475",
          "cited_paper_doi": "10.1109/CVPR52688.2022.00564",
          "citing_paper_url": "https://www.semanticscholar.org/paper/1dc809107ceb27fc74071934878fe2dc9292e57d",
          "cited_paper_url": "https://www.semanticscholar.org/paper/1e88d5afe19aea324d33541f60a90b7036894c32",
          "citing_paper_year": 2024,
          "cited_paper_year": 2021
        },
        {
          "dataset_name": "GoPro",
          "dataset_description": "Used to evaluate DeblurGAN-v2 in a single-frame setting, treating all frames as individual images, to assess deblurring performance in hand-held camera videos. | Used to evaluate image restoration techniques, focusing on motion blur correction in 600 images. | Used to train deblurring models, selecting every tenth frame to diversify the training data and prevent overfitting. | Used to collect real-world videos for video deblurring, specifically capturing footage at 240 fps using various devices like iPhone 6s, GoPro Hero 4, and Nexus 5x. | Used to train deblurring models, selecting every second frame to diversify the training data and prevent overfitting.",
          "citing_paper_id": "199543931",
          "cited_paper_id": 5872410,
          "context_text": "We take 600 images from GoPRO, and 600 images from DVD, both with motion blurs already (same as above).",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions two specific datasets, GoPRO and DVD, which are used for image restoration experiments involving motion blur.",
          "citing_paper_doi": "10.1109/ICCV.2019.00897",
          "cited_paper_doi": "10.1109/CVPR.2017.33",
          "citing_paper_url": "https://www.semanticscholar.org/paper/81c1dc00b32d12edfab00656593f5db56cfa79e0",
          "cited_paper_url": "https://www.semanticscholar.org/paper/f1f35dd6d024db12f6ae5d0854d8d6589af5d40c",
          "citing_paper_year": 2019,
          "cited_paper_year": 2017
        },
        {
          "dataset_name": "GoPro",
          "dataset_description": "Used to evaluate the SSIM-FLOPs trade-off in generative adversarial networks, focusing on performance metrics in image restoration.",
          "citing_paper_id": "199543931",
          "cited_paper_id": 12803511,
          "context_text": "Speciﬁcally, generative adversarial networks (GANs) [9] The SSIM-FLOPs trade-off plot on the GoPRO dataset.",
          "confidence_score": 1.0,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions the GoPRO dataset, which is a specific dataset used for image restoration tasks. The dataset is used to evaluate the SSIM-FLOPs trade-off in the context of GANs.",
          "citing_paper_doi": "10.1109/ICCV.2019.00897",
          "cited_paper_doi": null,
          "citing_paper_url": "https://www.semanticscholar.org/paper/81c1dc00b32d12edfab00656593f5db56cfa79e0",
          "cited_paper_url": "https://www.semanticscholar.org/paper/353ecf7b66b3e9ff5e9f41145a147e899a2eea5c",
          "citing_paper_year": 2019,
          "cited_paper_year": 2014
        },
        {
          "dataset_name": "GoPro",
          "dataset_description": "Used to evaluate image restoration models, specifically comparing performance against state-of-the-art methods like FFTformer, Restormer, NAFNet, HINet, and Uformer.",
          "citing_paper_id": "272680196",
          "cited_paper_id": 235358213,
          "context_text": "While our approach is no match for the best approach FFTformer [4] on the GoPro dataset, it\n10\noutperforms Restormer [34], NAFNet [20], HINet [59] and Uformer [35].",
          "confidence_score": 1.0,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions the GoPro dataset, which is a specific dataset used for image restoration tasks. No other datasets are mentioned.",
          "citing_paper_doi": null,
          "cited_paper_doi": "10.1109/CVPR52688.2022.01716",
          "citing_paper_url": "https://www.semanticscholar.org/paper/e9ef4cfc510d53d0be69835a34a9195027e4cc51",
          "cited_paper_url": "https://www.semanticscholar.org/paper/2835951fabf12804e17d5a525b2be2bee70e7910",
          "citing_paper_year": 2023,
          "cited_paper_year": 2021
        },
        {
          "dataset_name": "GoPro",
          "dataset_description": "Used to evaluate the performance of Restormer on high-resolution image restoration, focusing on quantitative metrics and visual quality. | Used to evaluate image restoration models, specifically comparing performance against state-of-the-art methods like FFTformer, Restormer, NAFNet, HINet, and Uformer. | Used to evaluate the performance of CAPTNet in image restoration, specifically comparing it to FFTformer and Restormer. The dataset focuses on high-resolution image restoration tasks.",
          "citing_paper_id": "272680196",
          "cited_paper_id": 244346144,
          "context_text": "While our approach is no match for the best approach FFTformer [4] on the GoPro dataset, it\n10\noutperforms Restormer [34], NAFNet [20], HINet [59] and Uformer [35].",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions the GoPro dataset, which is a specific dataset used for image restoration tasks. No other datasets are mentioned.",
          "citing_paper_doi": null,
          "cited_paper_doi": "10.1109/CVPR52688.2022.00564",
          "citing_paper_url": "https://www.semanticscholar.org/paper/e9ef4cfc510d53d0be69835a34a9195027e4cc51",
          "cited_paper_url": "https://www.semanticscholar.org/paper/1e88d5afe19aea324d33541f60a90b7036894c32",
          "citing_paper_year": 2023,
          "cited_paper_year": 2021
        },
        {
          "dataset_name": "GoPro",
          "dataset_description": "Used to evaluate the performance of the proposed image restoration approach, specifically comparing it to the FFTformer method in the context of high-quality image deblurring. | Used to evaluate image restoration models, specifically comparing performance against state-of-the-art methods like FFTformer, Restormer, NAFNet, HINet, and Uformer. | Used to evaluate the performance of CAPTNet in image restoration, specifically comparing it to FFTformer and Restormer. The dataset focuses on high-resolution image restoration tasks.",
          "citing_paper_id": "272680196",
          "cited_paper_id": 253761139,
          "context_text": "While our approach is no match for the best approach FFTformer [4] on the GoPro dataset, it\n10\noutperforms Restormer [34], NAFNet [20], HINet [59] and Uformer [35].",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions the GoPro dataset, which is a specific dataset used for image restoration tasks. No other datasets are mentioned.",
          "citing_paper_doi": null,
          "cited_paper_doi": "10.1109/CVPR52729.2023.00570",
          "citing_paper_url": "https://www.semanticscholar.org/paper/e9ef4cfc510d53d0be69835a34a9195027e4cc51",
          "cited_paper_url": "https://www.semanticscholar.org/paper/8934907fd212d6c8b1206c5e4a7f3f37c96be15f",
          "citing_paper_year": 2023,
          "cited_paper_year": 2022
        },
        {
          "dataset_name": "GoPro",
          "dataset_description": "Used to train an image deblurring model for 1000 epochs, focusing on improving image restoration techniques.",
          "citing_paper_id": "268030812",
          "cited_paper_id": 259298517,
          "context_text": "For ablation experiments, following [84, 20], we train the image deblurring model on GoPro dataset [65] for 1000 epochs only and set the number of Transformer in each CGT is 1.",
          "confidence_score": 1.0,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions the GoPro dataset, which is a specific, verifiable dataset used for training an image deblurring model.",
          "citing_paper_doi": null,
          "cited_paper_doi": null,
          "citing_paper_url": "https://www.semanticscholar.org/paper/caabf4abb96ee83f650aa7b707bd9a99007b3e74",
          "cited_paper_url": "https://www.semanticscholar.org/paper/6af728164e4424218967fd356b3448f5037acd88",
          "citing_paper_year": 2023,
          "cited_paper_year": 2023
        },
        {
          "dataset_name": "GoPro",
          "dataset_description": "Used to retrain and evaluate the NAFNet model with the proposed model contrastive paradigm, focusing on improving image restoration performance. | Used to train and evaluate NAFNet for image deblurring, focusing on improving the quality of deblurred images through supervised learning.",
          "citing_paper_id": "265999804",
          "cited_paper_id": 248085491,
          "context_text": "We take NAFNet (Chen et al. 2022a) as the benchmark and retrain it with the proposed model contrastive paradigm on GoPro dataset.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions the GoPro dataset, which is a specific dataset used for training and evaluating image restoration models.",
          "citing_paper_doi": "10.48550/arXiv.2309.06023",
          "cited_paper_doi": "10.48550/arXiv.2204.04676",
          "citing_paper_url": "https://www.semanticscholar.org/paper/8ce208dfd1259d087e0195f5677cbf2865411ba8",
          "cited_paper_url": "https://www.semanticscholar.org/paper/7d4c2c8407e0caf2f907df9954b056a42a92fd13",
          "citing_paper_year": 2023,
          "cited_paper_year": 2022
        },
        {
          "dataset_name": "GoPro",
          "dataset_description": "Used for low-light image enhancement, specifically addressing the challenge of improving visibility in underexposed images. | Used for low-light image enhancement, specifically to train and evaluate models on improving image quality in low-light conditions using deep retinex decomposition. | Used for motion deblurring, specifically to train and evaluate models on dynamic scene deblurring tasks using a multi-scale convolutional neural network. | Used for dynamic scene deblurring tasks, focusing on improving image clarity in motion-blurred images using a deep multi-scale convolutional neural network.",
          "citing_paper_id": "278740510",
          "cited_paper_id": 8671030,
          "context_text": "Deblur-ring and low-light enhancement tasks utilize the GoPro [34] and LOL-v1 [34] datasets, respectively.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions two specific datasets, GoPro and LOL-v1, which are used for deblurring and low-light enhancement tasks, respectively.",
          "citing_paper_doi": "10.48550/arXiv.2505.12630",
          "cited_paper_doi": "10.1109/CVPR.2017.35",
          "citing_paper_url": "https://www.semanticscholar.org/paper/18b73e3db636e6ded820bc0b5b5ad13993e287bc",
          "cited_paper_url": "https://www.semanticscholar.org/paper/65b16da51891a6b98140d425804c8a0fd0299219",
          "citing_paper_year": 2025,
          "cited_paper_year": 2016
        },
        {
          "dataset_name": "GoPro",
          "dataset_description": "Used for motion deblurring, specifically to train and evaluate models on dynamic scene deblurring tasks using a multi-scale convolutional neural network. | Used for low-light image enhancement, specifically to train and evaluate models on improving image quality in low-light conditions using deep retinex decomposition.",
          "citing_paper_id": "278740510",
          "cited_paper_id": 52008443,
          "context_text": "These encompass datasets from the previously mentioned three-task scenario, alongside additional datasets: GoPro [34] for motion deblurring and LOL [50] for low-light image enhancement.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions two specific datasets, GoPro and LOL, which are used for motion deblurring and low-light image enhancement, respectively. These datasets are clearly identified and their usage is described.",
          "citing_paper_doi": "10.48550/arXiv.2505.12630",
          "cited_paper_doi": null,
          "citing_paper_url": "https://www.semanticscholar.org/paper/18b73e3db636e6ded820bc0b5b5ad13993e287bc",
          "cited_paper_url": "https://www.semanticscholar.org/paper/c7798bbbcd2e85593a2b2d37b92dabac63420cd8",
          "citing_paper_year": 2025,
          "cited_paper_year": 2018
        },
        {
          "dataset_name": "GoPro",
          "dataset_description": "Used for training in low-light enhancement tasks, offering a challenging dataset to improve restoration performance. | Used for training in image deblurring tasks, providing a challenging dataset to improve restoration performance.",
          "citing_paper_id": "266149517",
          "cited_paper_id": 248085491,
          "context_text": "In addition, we include two challenging tasks of image deblurring and low-light enhancement and use the GoPro [41] and LOL dataset [59] for training, as previous research [8, 69, 70].",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions the GoPro and LOL datasets, which are used for training in the context of image deblurring and low-light enhancement tasks.",
          "citing_paper_doi": "10.48550/arXiv.2312.05038",
          "cited_paper_doi": "10.48550/arXiv.2204.04676",
          "citing_paper_url": "https://www.semanticscholar.org/paper/38496bf5e8fd6f16ddf36578586b08a8225a4aa2",
          "cited_paper_url": "https://www.semanticscholar.org/paper/7d4c2c8407e0caf2f907df9954b056a42a92fd13",
          "citing_paper_year": 2023,
          "cited_paper_year": 2022
        },
        {
          "dataset_name": "GoPro",
          "dataset_description": "Used for training in low-light enhancement tasks, offering a challenging dataset to improve restoration performance. | Used for training in image deblurring tasks, providing a challenging dataset to improve restoration performance.",
          "citing_paper_id": "266149517",
          "cited_paper_id": 260680793,
          "context_text": "In addition, we include two challenging tasks of image deblurring and low-light enhancement and use the GoPro [41] and LOL dataset [59] for training, as previous research [8, 69, 70].",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions the GoPro and LOL datasets, which are used for training in the context of image deblurring and low-light enhancement tasks.",
          "citing_paper_doi": "10.48550/arXiv.2312.05038",
          "cited_paper_doi": "10.1145/3581783.3611825",
          "citing_paper_url": "https://www.semanticscholar.org/paper/38496bf5e8fd6f16ddf36578586b08a8225a4aa2",
          "cited_paper_url": "https://www.semanticscholar.org/paper/b764f1db97572275dfd7b7f106b190d06a7c9fa0",
          "citing_paper_year": 2023,
          "cited_paper_year": 2023
        },
        {
          "dataset_name": "GoPro",
          "dataset_description": "Used for image motion deblurring comparisons, focusing on dynamic scene deblurring using a deep multi-scale convolutional neural network. | Used to evaluate deblurring results of MPRNet, specifically comparing different inference schemes on dynamic scenes. | Used to evaluate image deblurring models, focusing on dynamic scene deblurring performance and quality. | Used for image motion deblurring comparisons, emphasizing human-aware motion deblurring techniques. | Used to evaluate image deblurring models, focusing on human-aware motion deblurring performance and quality. | Used for video deblurring comparisons, focusing on dynamic scene deblurring using a deep multi-scale convolutional neural network. | Used to evaluate single-image motion deblurring methods, specifically comparing the performance of TLC against HINet, MPRNet, and Restormer using PSNR as the metric. | Used to generate example images of block boundary artifacts for visual inspection, focusing on dynamic scene deblurring. | Used to evaluate the distribution of mean statistics between training and inference phases, focusing on dynamic scene deblurring. | Used to evaluate deblurring performance of MPRNet, focusing on dynamic scene deblurring using patch-based inference. | Used to train models for dynamic scene deblurring, focusing on improving image quality in motion-blurred scenes using a deep multi-scale convolutional neural network.",
          "citing_paper_id": "250451699",
          "cited_paper_id": 8671030,
          "context_text": "HINet [7], MPRNet [55], and Restormer [53]) and evaluate them on test set of GoPro [28] and HIDE [36] dataset.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions the GoPro and HIDE datasets, which are used for evaluating image restoration models. These datasets are specific and relevant to the research topic.",
          "citing_paper_doi": "10.1007/978-3-031-20071-7_4",
          "cited_paper_doi": "10.1109/CVPR.2017.35",
          "citing_paper_url": "https://www.semanticscholar.org/paper/6f1cd5055f85ed15e7b61f599946a934c25d39b6",
          "cited_paper_url": "https://www.semanticscholar.org/paper/65b16da51891a6b98140d425804c8a0fd0299219",
          "citing_paper_year": 2021,
          "cited_paper_year": 2016
        },
        {
          "dataset_name": "GoPro",
          "dataset_description": "Used for image motion deblurring comparisons, focusing on dynamic scene deblurring using a deep multi-scale convolutional neural network. | Used to assess the effectiveness of image restoration models, particularly in handling diverse image degradation scenarios. | Used to evaluate image deblurring models, focusing on dynamic scene deblurring performance and quality. | Used for image motion deblurring comparisons, emphasizing human-aware motion deblurring techniques. | Used to evaluate image deblurring models, focusing on human-aware motion deblurring performance and quality. | Used to evaluate the performance of image restoration models, focusing on motion deblurring in high-resolution images. | Used for image motion deblurring comparisons, specifically evaluating the effectiveness of the Test-time Local Converter in reducing train-test inconsistency.",
          "citing_paper_id": "250451699",
          "cited_paper_id": 201624746,
          "context_text": "HINet [7], MPRNet [55], and Restormer [53]) and evaluate them on test set of GoPro [28] and HIDE [36] dataset.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions the GoPro and HIDE datasets, which are used for evaluating image restoration models. These datasets are specific and relevant to the research topic.",
          "citing_paper_doi": "10.1007/978-3-031-20071-7_4",
          "cited_paper_doi": "10.1109/ICCV.2019.00567",
          "citing_paper_url": "https://www.semanticscholar.org/paper/6f1cd5055f85ed15e7b61f599946a934c25d39b6",
          "cited_paper_url": "https://www.semanticscholar.org/paper/20a4b3353579525f0b76ec42e17a2284b4453f9a",
          "citing_paper_year": 2021,
          "cited_paper_year": 2019
        },
        {
          "dataset_name": "GoPro",
          "dataset_description": "Used to evaluate different video deblurring algorithms, focusing on performance and quality improvements in the context of state-of-the-art methods. | Used to evaluate different video deblurring algorithms, focusing on performance and quality improvements in deblurring techniques.",
          "citing_paper_id": "250451699",
          "cited_paper_id": 245117567,
          "context_text": "We apply our TLC to state-of-the-art video deblurring method (i.e., RNN-MBP [68]) and evaluate different video deblurring algorithms on GoPro datasets.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions the GoPro datasets, which are used for evaluating video deblurring algorithms. The dataset is clearly named and relevant to the research topic of image restoration.",
          "citing_paper_doi": "10.1007/978-3-031-20071-7_4",
          "cited_paper_doi": "10.1609/aaai.v36i3.20272",
          "citing_paper_url": "https://www.semanticscholar.org/paper/6f1cd5055f85ed15e7b61f599946a934c25d39b6",
          "cited_paper_url": "https://www.semanticscholar.org/paper/26a3f9072e995a9dcc39b0bcb2c6ce080d99b9ef",
          "citing_paper_year": 2021,
          "cited_paper_year": 2021
        },
        {
          "dataset_name": "GoPro",
          "dataset_description": "Applied to benchmark deblurring methods, containing diverse real-world images with various types of blur to evaluate algorithm robustness. | Used for evaluating deblurring algorithms, providing real-world motion blur scenarios to test restoration performance. | Employed to train and evaluate deblurring algorithms, featuring real-world blurred images with corresponding ground truth for performance assessment. | Utilized for video deblurring, offering high-resolution video sequences with realistic motion blur to assess restoration quality.",
          "citing_paper_id": "253510104",
          "cited_paper_id": 222104551,
          "context_text": ", GoPro [25], REDS [24], HIDE [36], RealBlur [32], etc.",
          "confidence_score": 0.9,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions multiple datasets by name, which are likely used for image restoration tasks. The cited paper title confirms 'RealBlur' is a dataset.",
          "citing_paper_doi": "10.48550/arXiv.2211.07317",
          "cited_paper_doi": "10.22677/THESIS.200000333386",
          "citing_paper_url": "https://www.semanticscholar.org/paper/e032d45a762cca82a9cfb446e469b13c03ce0690",
          "cited_paper_url": "https://www.semanticscholar.org/paper/2d6c14023087b5d5bd90a88da13e0fa765418d84",
          "citing_paper_year": 2022,
          "cited_paper_year": 2020
        },
        {
          "dataset_name": "GoPro",
          "dataset_description": "Used for testing the trained model, assessing its effectiveness on real-world blurred images. | Used for evaluating the proposed method, focusing on high-resolution image restoration and deblurring performance. | Used for training and validating a motion deblurring model, focusing on human-aware motion deblurring techniques. | Used for evaluating the proposed method on real-world datasets, focusing on realistic image restoration challenges. | Used for training the proposed image restoration method, focusing on motion deblurring and high-resolution image restoration. | Used for testing the trained model, evaluating its performance on a diverse set of images with varying blur conditions.",
          "citing_paper_id": "266690738",
          "cited_paper_id": 201624746,
          "context_text": "Following [15, 114], we train the proposed method on GoPro training data and evaluate our method on GoPro [68], HIDE [86], and real-world datasets (RealBlur-R [81] and RealBlur-J [81]).",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions specific datasets used for training and evaluation in the context of image restoration. These datasets are clearly identified and used for evaluating the proposed method.",
          "citing_paper_doi": "10.1109/CVPR52733.2024.00277",
          "cited_paper_doi": "10.1109/ICCV.2019.00567",
          "citing_paper_url": "https://www.semanticscholar.org/paper/c6495e41d3974476e0be9d5137c3dda51125bcec",
          "cited_paper_url": "https://www.semanticscholar.org/paper/20a4b3353579525f0b76ec42e17a2284b4453f9a",
          "citing_paper_year": 2023,
          "cited_paper_year": 2019
        },
        {
          "dataset_name": "GoPro",
          "dataset_description": "Used for evaluating the proposed method, focusing on high-resolution image restoration and deblurring performance. | Used for evaluating the proposed method on real-world datasets, focusing on realistic image restoration challenges. | Used for training and evaluating the proposed method on single-image defocus deblurring and dual-pixel defocus deblurring tasks. | Used for training the proposed image restoration method, focusing on motion deblurring and high-resolution image restoration.",
          "citing_paper_id": "266690738",
          "cited_paper_id": 244346144,
          "context_text": "Following [15, 114], we train the proposed method on GoPro training data and evaluate our method on GoPro [68], HIDE [86], and real-world datasets (RealBlur-R [81] and RealBlur-J [81]).",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions specific datasets used for training and evaluation in the context of image restoration. These datasets are clearly identified and used for evaluating the proposed method.",
          "citing_paper_doi": "10.1109/CVPR52733.2024.00277",
          "cited_paper_doi": "10.1109/CVPR52688.2022.00564",
          "citing_paper_url": "https://www.semanticscholar.org/paper/c6495e41d3974476e0be9d5137c3dda51125bcec",
          "cited_paper_url": "https://www.semanticscholar.org/paper/1e88d5afe19aea324d33541f60a90b7036894c32",
          "citing_paper_year": 2023,
          "cited_paper_year": 2021
        },
        {
          "dataset_name": "GoPro",
          "dataset_description": "Used for evaluating the proposed method, focusing on high-resolution image restoration and deblurring performance. | Used for evaluating the proposed method on real-world datasets, focusing on realistic image restoration challenges. | Used for training the proposed image restoration method, focusing on motion deblurring and high-resolution image restoration.",
          "citing_paper_id": "266690738",
          "cited_paper_id": 248085491,
          "context_text": "Following [15, 114], we train the proposed method on GoPro training data and evaluate our method on GoPro [68], HIDE [86], and real-world datasets (RealBlur-R [81] and RealBlur-J [81]).",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions specific datasets used for training and evaluation in the context of image restoration. These datasets are clearly identified and used for evaluating the proposed method.",
          "citing_paper_doi": "10.1109/CVPR52733.2024.00277",
          "cited_paper_doi": "10.48550/arXiv.2204.04676",
          "citing_paper_url": "https://www.semanticscholar.org/paper/c6495e41d3974476e0be9d5137c3dda51125bcec",
          "cited_paper_url": "https://www.semanticscholar.org/paper/7d4c2c8407e0caf2f907df9954b056a42a92fd13",
          "citing_paper_year": 2023,
          "cited_paper_year": 2022
        },
        {
          "dataset_name": "GoPro",
          "dataset_description": "Used for motion deblurring experiments, consisting of 2103 images for training and 1111 for testing, focusing on dynamic scene deblurring. | Used for low-light image enhancement, evaluating the effectiveness of the proposed method in improving image quality under low-light conditions. | Used for single-image dehazing, benchmarking the effectiveness of dehazing algorithms in improving image clarity. | Used to compare methods on the motion deblurring task, focusing on performance evaluation in dynamic scenes. | Used for dynamic scene deblurring, evaluating the performance of deep multi-scale convolutional neural networks in restoring blurred images. | Used for motion deblurring, assessing the performance of the method in removing motion blur from images captured in dynamic scenes.",
          "citing_paper_id": "267320695",
          "cited_paper_id": 8671030,
          "context_text": "We use the GoPro dataset for motion deblurring [57] which consist of 2103 images for training, and 1111 for testing.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context clearly mentions the GoPro dataset, which is used for motion deblurring experiments. The dataset is described with specific details about its size and purpose.",
          "citing_paper_doi": "10.1007/978-3-031-72764-1_1",
          "cited_paper_doi": "10.1109/CVPR.2017.35",
          "citing_paper_url": "https://www.semanticscholar.org/paper/03ad1a40a4399c8b77bbeaa389fcd14b10b322c0",
          "cited_paper_url": "https://www.semanticscholar.org/paper/65b16da51891a6b98140d425804c8a0fd0299219",
          "citing_paper_year": 2024,
          "cited_paper_year": 2016
        },
        {
          "dataset_name": "GoPro",
          "dataset_description": "Used to evaluate the performance of MaIR in image restoration, specifically comparing PSNR scores against Restormer. | Used for training high-resolution image restoration models, specifically employing 2,103 blurry-clean image pairs to improve restoration quality.",
          "citing_paper_id": "275133634",
          "cited_paper_id": 244346144,
          "context_text": "In detail, MaIR outperforms Restormer [53] by 0.77dB on the GoPro dataset and by 0.35dB on the HIDE dataset in terms of PSNR.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions two specific datasets, GoPro and HIDE, which are used to evaluate the performance of MaIR compared to Restormer.",
          "citing_paper_doi": "10.48550/arXiv.2412.20066",
          "cited_paper_doi": "10.1109/CVPR52688.2022.00564",
          "citing_paper_url": "https://www.semanticscholar.org/paper/2b78fc275b7fc3a37061620ae155c017ab6c22e1",
          "cited_paper_url": "https://www.semanticscholar.org/paper/1e88d5afe19aea324d33541f60a90b7036894c32",
          "citing_paper_year": 2024,
          "cited_paper_year": 2021
        },
        {
          "dataset_name": "GoPro",
          "dataset_description": "Used for single-image dehazing, benchmarking the effectiveness of dehazing algorithms in improving image clarity. | Used to train models for single-image dehazing, focusing on outdoor scenes with approximately 72,000 training images. | Used to evaluate and compare dehazing methods, focusing on outdoor scenes to assess performance in realistic conditions. | Used for dynamic scene deblurring, evaluating the performance of deep multi-scale convolutional neural networks in restoring blurred images. | Used for dehazing comparisons in all-in-one image restoration methods, focusing on outdoor image quality enhancement.",
          "citing_paper_id": "267320695",
          "cited_paper_id": 39760169,
          "context_text": "Deblurring GoPro [57] Dehazing SOTS [41] Method PSNR/SSIM Method PSNR/SSIM Limitations Our method achieves state-of-the-art results in five tasks, proving the potential of using instructions to guide deep blind restoration models.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions two datasets, GoPro and SOTS, which are used for deblurring and dehazing tasks, respectively. These are specific datasets relevant to image restoration.",
          "citing_paper_doi": "10.1007/978-3-031-72764-1_1",
          "cited_paper_doi": "10.1109/TIP.2018.2867951",
          "citing_paper_url": "https://www.semanticscholar.org/paper/03ad1a40a4399c8b77bbeaa389fcd14b10b322c0",
          "cited_paper_url": "https://www.semanticscholar.org/paper/abf75b81b96ff2cd262086675deba55f907108db",
          "citing_paper_year": 2024,
          "cited_paper_year": 2017
        },
        {
          "dataset_name": "GoPro",
          "dataset_description": "Used to evaluate image restoration methods, specifically comparing performance against state-of-the-art models like FFTformer, Restormer, NAFNet, MSFS-Net, HINet, and Uformer.",
          "citing_paper_id": "261556765",
          "cited_paper_id": 248085491,
          "context_text": "While our approach is no match for the best approach FFTformer [4] on the GoPro dataset, it outperforms Restormer [43], NAFNet [25], MSFS-Net [7], HINet [61] and Uformer [44].",
          "confidence_score": 1.0,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions the GoPro dataset, which is a specific dataset used for evaluating image restoration methods. No other datasets are mentioned.",
          "citing_paper_doi": "10.1109/TCSVT.2024.3398810",
          "cited_paper_doi": "10.48550/arXiv.2204.04676",
          "citing_paper_url": "https://www.semanticscholar.org/paper/01a914eadebc6f20e1f436bd9a02381eccf215ae",
          "cited_paper_url": "https://www.semanticscholar.org/paper/7d4c2c8407e0caf2f907df9954b056a42a92fd13",
          "citing_paper_year": 2023,
          "cited_paper_year": 2022
        },
        {
          "dataset_name": "GoPro",
          "dataset_description": "Used for numerical comparisons in dynamic scene deblurring, evaluating the effectiveness of the proposed method against existing techniques. | Used for numerical comparisons in dynamic scene deblurring, focusing on performance evaluation of deblurring algorithms. | Used to evaluate image motion deblurring results, specifically assessing the effectiveness of the deep multi-scale convolutional neural network in dynamic scene deblurring. | Used to train models for 1000 epochs, focusing on dynamic scene deblurring using a deep multi-scale convolutional neural network. | Used to train models for dynamic scene deblurring, focusing on multi-scale convolutional neural networks over 1000 epochs. | Used for numerical comparisons in human-aware motion deblurring, assessing the performance of the proposed method in handling complex motion scenarios. | Used to evaluate motion deblurring methods, focusing on performance metrics and visual quality improvements in dynamic scenes. | Used to evaluate the computational costs of five motion deblurring methods, focusing on performance metrics and efficiency in dynamic scene deblurring. | Used to evaluate image motion deblurring methods, focusing on dynamic scene deblurring performance using a multi-scale convolutional neural network. | Used to sample input images for dynamic scene deblurring experiments, focusing on the effectiveness of the proposed deblurring method.",
          "citing_paper_id": "259298517",
          "cited_paper_id": 8671030,
          "context_text": "The numerical comparisons on the synthetic GoPro (Nah et al., 2017) and HIDE (Shen et al.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions two specific datasets, GoPro and HIDE, which are used for numerical comparisons in the context of dynamic scene deblurring.",
          "citing_paper_doi": null,
          "cited_paper_doi": "10.1109/CVPR.2017.35",
          "citing_paper_url": "https://www.semanticscholar.org/paper/6af728164e4424218967fd356b3448f5037acd88",
          "cited_paper_url": "https://www.semanticscholar.org/paper/65b16da51891a6b98140d425804c8a0fd0299219",
          "citing_paper_year": 2023,
          "cited_paper_year": 2016
        },
        {
          "dataset_name": "GoPro",
          "dataset_description": "Used to train models for image processing, focusing on enhancing image quality and restoration techniques. The dataset is utilized for its diverse and high-resolution content. | Used to train models for dynamic scene deblurring, focusing on multi-scale convolutional neural networks over 1000 epochs. | Used to evaluate the performance of the proposed SFNet model in image restoration, specifically comparing PSNR scores against the MLP model MAXIM-2S.",
          "citing_paper_id": "259298517",
          "cited_paper_id": 245837508,
          "context_text": "Following the recent method (Tu et al., 2022), all models are trained on the GoPro (Nah et al., 2017) dataset for 1000 epochs, and N is set to 7 in Fig.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions the GoPro dataset, which is a specific, verifiable dataset used for training models in image restoration tasks.",
          "citing_paper_doi": null,
          "cited_paper_doi": "10.1109/CVPR52688.2022.00568",
          "citing_paper_url": "https://www.semanticscholar.org/paper/6af728164e4424218967fd356b3448f5037acd88",
          "cited_paper_url": "https://www.semanticscholar.org/paper/fbf68eb0cf8237cef59ebdb301569c79b9676ff9",
          "citing_paper_year": 2023,
          "cited_paper_year": 2022
        },
        {
          "dataset_name": "GoPro",
          "dataset_description": "Used for numerical comparisons in human-aware motion deblurring, assessing the performance of the proposed method in handling complex motion scenarios. | Used for deraining comparisons, evaluating the generalization capability of the proposed method against previous approaches. | Used for numerical comparisons in dynamic scene deblurring, evaluating the effectiveness of the proposed method against existing techniques. | Used to demonstrate stronger generalization capability compared to Stripformer on all metrics.",
          "citing_paper_id": "259298517",
          "cited_paper_id": 201624746,
          "context_text": "The numerical comparisons on the synthetic GoPro (Nah et al., 2017) and HIDE (Shen et al., 2020) (Nah et al., 2017).",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions two specific datasets, 'GoPro' and 'HIDE', which are used for numerical comparisons in the context of image restoration, particularly deblurring.",
          "citing_paper_doi": null,
          "cited_paper_doi": "10.1109/ICCV.2019.00567",
          "citing_paper_url": "https://www.semanticscholar.org/paper/6af728164e4424218967fd356b3448f5037acd88",
          "cited_paper_url": "https://www.semanticscholar.org/paper/20a4b3353579525f0b76ec42e17a2284b4453f9a",
          "citing_paper_year": 2023,
          "cited_paper_year": 2019
        },
        {
          "dataset_name": "GoPro",
          "dataset_description": "Used for qualitative comparisons of motion deblurring methods, focusing on visual quality improvements over previous state-of-the-art techniques. | Used for qualitative comparisons of image deblurring methods, focusing on visual quality improvements over previous state-of-the-art techniques.",
          "citing_paper_id": "231639270",
          "cited_paper_id": 202719293,
          "context_text": "Qualitative Analysis: Figure 10 shows the qualitative comparisons on the GoPro test dataset with previous state-ofthe-art [24, 15, 17], and Figure 11 on the HIDE dataset with [15, 24].",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions two specific datasets, GoPro and HIDE, which are used for qualitative comparisons in image restoration tasks.",
          "citing_paper_doi": "10.1109/TIP.2022.3216216",
          "cited_paper_doi": "10.1609/AAAI.V34I07.6862",
          "citing_paper_url": "https://www.semanticscholar.org/paper/010e74e834c12e840e603b53ad14cbfc454ce7a1",
          "cited_paper_url": "https://www.semanticscholar.org/paper/0d66fe6f57458156bacd3fd9fc6e4c2f67aac298",
          "citing_paper_year": 2021,
          "cited_paper_year": 2019
        },
        {
          "dataset_name": "GoPro",
          "dataset_description": "Used for image deblurring experiments, focusing on enhancing image quality and reducing blur in realistic video sequences. | Used for image deblurring experiments, focusing on improving image clarity and reducing motion blur in high-speed video sequences.",
          "citing_paper_id": "270155352",
          "cited_paper_id": 233476615,
          "context_text": "Image deblurring: We conducted experiments on two benchmark datasets for image deblurring, including GoPro [1] and REDS [67].",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions two specific datasets, GoPro and REDS, which are used for image deblurring experiments.",
          "citing_paper_doi": "10.1109/TMM.2024.3407656",
          "cited_paper_doi": "10.1109/CVPRW53098.2021.00025",
          "citing_paper_url": "https://www.semanticscholar.org/paper/3a80830bf3e31c855afaff17a5b590bc052839c6",
          "cited_paper_url": "https://www.semanticscholar.org/paper/149ed92d2e52acc6645aec45cbda486e071e5fe4",
          "citing_paper_year": 2024,
          "cited_paper_year": 2021
        },
        {
          "dataset_name": "GoPro",
          "dataset_description": "Used for low-light enhancement, focusing on improving image quality in low-light conditions. | Used for deblurring dynamic scenes, employing a deep multi-scale convolutional neural network to improve image clarity.",
          "citing_paper_id": "274305801",
          "cited_paper_id": 8671030,
          "context_text": "For deblurring and low-light enhancement, we employ the GoPro [34] and the LOL-v1 [53] dataset, respectively.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions two specific datasets, GoPro and LOL-v1, which are used for deblurring and low-light enhancement, respectively.",
          "citing_paper_doi": "10.48550/arXiv.2411.18466",
          "cited_paper_doi": "10.1109/CVPR.2017.35",
          "citing_paper_url": "https://www.semanticscholar.org/paper/2850f4c1f43789f0a8a48e7bb83cf2a5e0561e54",
          "cited_paper_url": "https://www.semanticscholar.org/paper/65b16da51891a6b98140d425804c8a0fd0299219",
          "citing_paper_year": 2024,
          "cited_paper_year": 2016
        },
        {
          "dataset_name": "GoPro",
          "dataset_description": "Used to evaluate real-world blur restoration, focusing on challenging JPEG-compressed images with various blur types. | Used to assess deblurring effectiveness on JPEG-compressed images, addressing common artifacts and compression issues. | Used for training and evaluating deblurring models, focusing on high-resolution video frames to improve image clarity. | Used for evaluating deblurring algorithms, focusing on synthetic motion blur in high-resolution images. | Used to evaluate deblurring performance on real-world images, emphasizing realistic blur conditions and challenging scenarios. | Used to assess real-world blur restoration, emphasizing realistic out-of-focus and motion blur effects. | Used to enhance deblurring algorithms, providing a diverse set of real-world blurred images for robustness testing.",
          "citing_paper_id": "259243623",
          "cited_paper_id": null,
          "context_text": "Merged Deblurring [32] GoPro [33] 2103 1111 HIDE [34] - 2025 RealBlur-R [35] - 980 RealBlur-J [35] - 980 Table 1: Summary of the datasets used for ProRes.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions several datasets used for image restoration, specifically deblurring and real-world blur scenarios.",
          "citing_paper_doi": "10.48550/arXiv.2306.13653",
          "cited_paper_doi": null,
          "citing_paper_url": "https://www.semanticscholar.org/paper/fe94fa22e2ffc22d61cff41a96cc2f012ebde3c1",
          "cited_paper_url": null,
          "citing_paper_year": 2023,
          "cited_paper_year": null
        },
        {
          "dataset_name": "GoPro",
          "dataset_description": "Extended to remote sensing image dehazing, achieving state-of-the-art performance in thick haze conditions. | Used to evaluate nighttime defogging techniques, focusing on high-low frequency decomposition and grayscale-color networks. | Used to evaluate the model's performance in image dehazing, comparing PSNR against IRNeXt with minimal additional parameters and FLOPs. | Extended to remote sensing image dehazing, achieving state-of-the-art performance in thin haze conditions. | Extended to nighttime image dehazing, achieving state-of-the-art performance in this specific scenario. | Extended to remote sensing image dehazing, achieving state-of-the-art performance in moderate haze conditions.",
          "citing_paper_id": "270736284",
          "cited_paper_id": 222178070,
          "context_text": "…PSNR over IRNeXt [34] on the GoPro [25] dataset with extra only 0.07M parameters and 0.01G FLOPs. b ) Experiments: Our model is extended to the nighttime (NHR [35], GTA5 [36]) and remote sensing (SateHaze1k-Thin/Moderate/Thick [37]) image dehazing problems and achieves state-of-the-art performance.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions specific datasets used for experiments, including GoPro, NHR, GTA5, and SateHaze1k-Thin/Moderate/Thick. These datasets are used to evaluate the model's performance in various image dehazing scenarios.",
          "citing_paper_doi": "10.1109/TPAMI.2024.3419007",
          "cited_paper_doi": "10.1007/978-3-030-58610-2_28",
          "citing_paper_url": "https://www.semanticscholar.org/paper/804d2302880b91965e84a7c72ac26429d26792a0",
          "cited_paper_url": "https://www.semanticscholar.org/paper/c328db44a5bfd6ea189c5c659a86e8894a275456",
          "citing_paper_year": 2024,
          "cited_paper_year": 2020
        },
        {
          "dataset_name": "GoPro",
          "dataset_description": "Extended to remote sensing image dehazing, achieving state-of-the-art performance in thick haze conditions. | Used to evaluate the model's performance in image dehazing, comparing PSNR against IRNeXt with minimal additional parameters and FLOPs. | Used to evaluate image restoration techniques in remote sensing, focusing on haze removal and image clarity enhancement. | Extended to remote sensing image dehazing, achieving state-of-the-art performance in thin haze conditions. | Extended to nighttime image dehazing, achieving state-of-the-art performance in this specific scenario. | Extended to remote sensing image dehazing, achieving state-of-the-art performance in moderate haze conditions.",
          "citing_paper_id": "270736284",
          "cited_paper_id": null,
          "context_text": "…PSNR over IRNeXt [34] on the GoPro [25] dataset with extra only 0.07M parameters and 0.01G FLOPs. b ) Experiments: Our model is extended to the nighttime (NHR [35], GTA5 [36]) and remote sensing (SateHaze1k-Thin/Moderate/Thick [37]) image dehazing problems and achieves state-of-the-art performance.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions specific datasets used for experiments, including GoPro, NHR, GTA5, and SateHaze1k-Thin/Moderate/Thick. These datasets are used to evaluate the model's performance in various image dehazing scenarios.",
          "citing_paper_doi": "10.1109/TPAMI.2024.3419007",
          "cited_paper_doi": null,
          "citing_paper_url": "https://www.semanticscholar.org/paper/804d2302880b91965e84a7c72ac26429d26792a0",
          "cited_paper_url": null,
          "citing_paper_year": 2024,
          "cited_paper_year": null
        },
        {
          "dataset_name": "GoPro",
          "dataset_description": "Used to present computational costs and compare performance of ConvIR-L with Transformer models Restormer and Stripformer, focusing on efficiency and performance gains.",
          "citing_paper_id": "270736284",
          "cited_paper_id": 248085101,
          "context_text": "…and computational costs on the GoPro [25] dataset are presented in Table X. Compared with Transformer models Restormer [16] and Stripformer [17] that have quadrative complexity, our network, ConvIR-L, is built on the efﬁcient convolutional network and receives remarkable performance gains…",
          "confidence_score": 1.0,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions the GoPro dataset, which is used to present computational costs and compare performance with other models. The dataset is specific and relevant to image restoration.",
          "citing_paper_doi": "10.1109/TPAMI.2024.3419007",
          "cited_paper_doi": "10.48550/arXiv.2204.04627",
          "citing_paper_url": "https://www.semanticscholar.org/paper/804d2302880b91965e84a7c72ac26429d26792a0",
          "cited_paper_url": "https://www.semanticscholar.org/paper/aab4ed4ee6bad74d4c07fa002da476504c62fda9",
          "citing_paper_year": 2024,
          "cited_paper_year": 2022
        },
        {
          "dataset_name": "GoPro",
          "dataset_description": "Used for deraining, specifically targeting the removal of rain streaks from images to improve visual quality. | Used for validating deblurring algorithms, focusing on real-world image degradation and noise. | Used for raindrop removal, focusing on realistic raindrop effects on images. | Used for low-light image enhancement, containing pairs of low-light and normal-light images. | Used for deblurring dynamic scenes, focusing on improving clarity and sharpness in video frames using a deep multi-scale convolutional neural network. | Used for evaluating denoising algorithms, containing 68 color images with varying levels of noise. | Used for haze removal, focusing on synthetic and real-world hazy images. | Used for motion deblurring, containing pairs of blurred and sharp images from dynamic scenes. | Used for rain removal, containing synthetic rain images with ground truth. | Used for low-light enhancement, specifically addressing the challenge of improving visibility in dark images. | Used for derain-drop tasks, focusing on removing raindrops from images to enhance overall image clarity. | Used for dehazing, focusing on removing atmospheric effects to enhance image clarity and color fidelity. | Used for super-resolution tasks, providing high-quality images for validation. | Used for dehazing, specifically for outdoor scenes with varying atmospheric conditions.",
          "citing_paper_id": "264145824",
          "cited_paper_id": 8671030,
          "context_text": "Specifically, we use GoPro [36], LOL [65], RESIDE [28], Rain200L [68], and Raindrop datasets [42] for deblurring, low-light enhancement, dehazing, deraining, and derain-drop tasks respectively.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context explicitly mentions several datasets used for specific image restoration tasks, which align with the research topic of all-in-one image restoration.",
          "citing_paper_doi": "10.48550/arXiv.2310.10123",
          "cited_paper_doi": "10.1109/CVPR.2017.35",
          "citing_paper_url": "https://www.semanticscholar.org/paper/8efc22d25fc33d73aa8de1044e5afe775ed87e31",
          "cited_paper_url": "https://www.semanticscholar.org/paper/65b16da51891a6b98140d425804c8a0fd0299219",
          "citing_paper_year": 2023,
          "cited_paper_year": 2016
        },
        {
          "dataset_name": "GoPro",
          "dataset_description": "Used for deraining, specifically targeting the removal of rain streaks from images to improve visual quality. | Used for validating deblurring algorithms, focusing on real-world image degradation and noise. | Used for raindrop removal, focusing on realistic raindrop effects on images. | Used for low-light image enhancement, containing pairs of low-light and normal-light images. | Used for deblurring dynamic scenes, focusing on improving clarity and sharpness in video frames using a deep multi-scale convolutional neural network. | Used for evaluating denoising algorithms, containing 68 color images with varying levels of noise. | Used for haze removal, focusing on synthetic and real-world hazy images. | Used for motion deblurring, containing pairs of blurred and sharp images from dynamic scenes. | Used for rain removal, containing synthetic rain images with ground truth. | Used for low-light enhancement, specifically addressing the challenge of improving visibility in dark images. | Used for derain-drop tasks, focusing on removing raindrops from images to enhance overall image clarity. | Used for dehazing, focusing on removing atmospheric effects to enhance image clarity and color fidelity. | Used for super-resolution tasks, providing high-quality images for validation. | Used for dehazing, specifically for outdoor scenes with varying atmospheric conditions.",
          "citing_paper_id": "264145824",
          "cited_paper_id": 15443600,
          "context_text": "Specifically, we use GoPro [36], LOL [65], RESIDE [28], Rain200L [68], and Raindrop datasets [42] for deblurring, low-light enhancement, dehazing, deraining, and derain-drop tasks respectively.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context explicitly mentions several datasets used for specific image restoration tasks, which align with the research topic of all-in-one image restoration.",
          "citing_paper_doi": "10.48550/arXiv.2310.10123",
          "cited_paper_doi": "10.1109/CVPR.2017.183",
          "citing_paper_url": "https://www.semanticscholar.org/paper/8efc22d25fc33d73aa8de1044e5afe775ed87e31",
          "cited_paper_url": "https://www.semanticscholar.org/paper/6de27f256e97c014a95706c72af7fa651e8fa34a",
          "citing_paper_year": 2023,
          "cited_paper_year": 2016
        },
        {
          "dataset_name": "GoPro",
          "dataset_description": "Used for training and testing image deblurring models, consisting of 2,103 pairs of blurred and sharp images for training and 1,111 pairs for testing.",
          "citing_paper_id": "278528939",
          "cited_paper_id": 8671030,
          "context_text": "For image deblurring, we utilize the GoPro [64] dataset, which consists of 2,103 pairs of blurred and sharp images for training and 1,111 pairs for testing.",
          "confidence_score": 1.0,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context clearly mentions the GoPro dataset, which is used for training and testing image deblurring models.",
          "citing_paper_doi": "10.1109/TIP.2025.3567205",
          "cited_paper_doi": "10.1109/CVPR.2017.35",
          "citing_paper_url": "https://www.semanticscholar.org/paper/99b442f68dce288fd1f5d30858f7ac8b8d0068e9",
          "cited_paper_url": "https://www.semanticscholar.org/paper/65b16da51891a6b98140d425804c8a0fd0299219",
          "citing_paper_year": 2025,
          "cited_paper_year": 2016
        },
        {
          "dataset_name": "GoPro",
          "dataset_description": "Used for evaluating DiffIR on image motion deblurring, assessing performance on a diverse set of images with varying degrees of blur. | Used for training and evaluating DiffIR on image motion deblurring, focusing on improving image clarity and reducing blur artifacts.",
          "citing_paper_id": "257557425",
          "cited_paper_id": 201624746,
          "context_text": "We train DiffIR on GoPro [41] dataset for image motion deblurring and evaluate DiffIR on two classic benchmarks (GoPro, HIDE [48]).",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions the GoPro and HIDE datasets, which are used for training and evaluation in the field of image motion deblurring.",
          "citing_paper_doi": "10.1109/ICCV51070.2023.01204",
          "cited_paper_doi": "10.1109/ICCV.2019.00567",
          "citing_paper_url": "https://www.semanticscholar.org/paper/67909a17f9c9467de536aa2cf7b0864dc6215e96",
          "cited_paper_url": "https://www.semanticscholar.org/paper/20a4b3353579525f0b76ec42e17a2284b4453f9a",
          "citing_paper_year": 2023,
          "cited_paper_year": 2019
        }
      ]
    },
    {
      "cited_paper_id": "8282555",
      "citation_count": 0,
      "total_dataset_mentions": 43,
      "unique_datasets": [
        "DIV2K"
      ],
      "dataset_details": [
        {
          "dataset_name": "DIV2K",
          "dataset_description": "Used for evaluation, reporting the highest PSNR (dB) values after 50,000 iterations, to assess the performance of the trained model. | Used as the training dataset for single image super-resolution with a scale factor of ×2, focusing on improving PSNR values through iterative training. | Used to train the Deep Texture-Fidelity Network (DTFN) for single image super-resolution, focusing on enhancing texture fidelity and overall image quality.",
          "citing_paper_id": "272552320",
          "cited_paper_id": 484327,
          "context_text": "WeuseDIV2K[40]asthetrainingdatasetwithascale factor of × 2 and report the highest PSNR (dB) values on Set5 [2] after 5 × 10 4 iterations.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions two specific datasets, DIV2K and Set5, which are commonly used in image super-resolution tasks. The usage is clear and relevant to the research topic of all-in-one image restoration.",
          "citing_paper_doi": "10.1109/TPAMI.2024.3457790",
          "cited_paper_doi": "10.1109/CVPRW.2017.149",
          "citing_paper_url": "https://www.semanticscholar.org/paper/e80e3946f27f380e34b4f431d70de6274e74e000",
          "cited_paper_url": "https://www.semanticscholar.org/paper/5054edca22325ddd3507b860f9af4a961baea009",
          "citing_paper_year": 2024,
          "cited_paper_year": 2017
        },
        {
          "dataset_name": "DIV2K",
          "dataset_description": "Used to train the Deep Texture-Fidelity Network (DTFN) for single image super-resolution, focusing on enhancing texture fidelity and overall image quality.",
          "citing_paper_id": "272552320",
          "cited_paper_id": 6540453,
          "context_text": "We follow the previous studies [24], [31], [38], [56] and train our Deep Texture-Fidelity Network (DTFN) on 800 images from the DIV2K dataset [40].",
          "confidence_score": 1.0,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions the use of the DIV2K dataset for training the DTFN model, which is directly relevant to the research topic of image restoration.",
          "citing_paper_doi": "10.1109/TPAMI.2024.3457790",
          "cited_paper_doi": "10.1109/CVPRW.2017.151",
          "citing_paper_url": "https://www.semanticscholar.org/paper/e80e3946f27f380e34b4f431d70de6274e74e000",
          "cited_paper_url": "https://www.semanticscholar.org/paper/7ba5d3808e117e7a68dc40331ce1d483ceeedcb2",
          "citing_paper_year": 2024,
          "cited_paper_year": 2017
        },
        {
          "dataset_name": "DIV2K",
          "dataset_description": "Used to train and evaluate SwinIR for single image super-resolution, focusing on performance across multiple scale factors. | Used for training SwinIR on classical image super-resolution at ×2 scale, focusing on high-quality image restoration. | Used for testing SwinIR's performance on manga images, evaluating its effectiveness in restoring detailed and stylistic content.",
          "citing_paper_id": "237266491",
          "cited_paper_id": 4493958,
          "context_text": "For ablation study, we train SwinIR on DIV2K [1] for classical image SR (×2) and test it on Manga109 [60].",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions two specific datasets, DIV2K and Manga109, which are used for training and testing in the context of image super-resolution.",
          "citing_paper_doi": "10.1109/ICCVW54120.2021.00210",
          "cited_paper_doi": "10.1109/CVPRW.2017.150",
          "citing_paper_url": "https://www.semanticscholar.org/paper/7a9a708ca61c14886aa0dcd6d13dac7879713f5f",
          "cited_paper_url": "https://www.semanticscholar.org/paper/6aeb583c084ad2eb709154ec91f22ca8d6de4425",
          "citing_paper_year": 2021,
          "cited_paper_year": 2017
        },
        {
          "dataset_name": "DIV2K",
          "dataset_description": "Used for training and testing image restoration models, specifically dividing 800 images into 750 for training and 50 for testing.",
          "citing_paper_id": "221173039",
          "cited_paper_id": 4746623,
          "context_text": "Following the experimental settings of works [47,41], we use the DIV2K dataset [2] which contains 800 images (750 images for training and 50 images for testing).",
          "confidence_score": 1.0,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context clearly mentions the use of the DIV2K dataset, which is a well-known dataset in the field of image restoration.",
          "citing_paper_doi": "10.1007/978-3-030-58523-5_36",
          "cited_paper_doi": "10.1109/CVPR.2018.00259",
          "citing_paper_url": "https://www.semanticscholar.org/paper/bd4346fa7145f126d32b19ced1223fcb7e7eef0e",
          "cited_paper_url": "https://www.semanticscholar.org/paper/9fb3707a0f90c6620251d202a972a2c626dce976",
          "citing_paper_year": 2020,
          "cited_paper_year": 2018
        },
        {
          "dataset_name": "DIV2K",
          "dataset_description": "Employed for training in gaussian color image denoising, containing a variety of natural images for comprehensive model evaluation. | Used for training in gaussian color image denoising, providing additional data to enhance model performance. | Utilized for training in gaussian color image denoising, offering a diverse set of images for robust model training. | Used to train classic single image super-resolution models, enhancing the performance of image restoration techniques. | Used to train both classic and lightweight single image super-resolution models, focusing on high-resolution image restoration. | Used for training models in gaussian color image denoising, providing high-resolution images for super-resolution tasks.",
          "citing_paper_id": "267938238",
          "cited_paper_id": 484327,
          "context_text": "We employ DIV2K [64] and Flickr2K [42] to train classic SR models and use DIV2K only to train lightweight SR models.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions two specific datasets, DIV2K and Flickr2K, which are used for training single image super-resolution models. The usage is clearly described, and both datasets are relevant to the topic of image restoration.",
          "citing_paper_doi": "10.48550/arXiv.2402.15648",
          "cited_paper_doi": "10.1109/CVPRW.2017.149",
          "citing_paper_url": "https://www.semanticscholar.org/paper/e730beb44042499763d36214c0498434e470dfd5",
          "cited_paper_url": "https://www.semanticscholar.org/paper/5054edca22325ddd3507b860f9af4a961baea009",
          "citing_paper_year": 2024,
          "cited_paper_year": 2017
        },
        {
          "dataset_name": "DIV2K",
          "dataset_description": "Employed for training in gaussian color image denoising, containing a variety of natural images for comprehensive model evaluation. | Used for training in gaussian color image denoising, providing additional data to enhance model performance. | Utilized for training in gaussian color image denoising, offering a diverse set of images for robust model training. | Used to train classic single image super-resolution models, enhancing the performance of image restoration techniques. | Used to train both classic and lightweight single image super-resolution models, focusing on high-resolution image restoration. | Used for training models in gaussian color image denoising, providing high-resolution images for super-resolution tasks.",
          "citing_paper_id": "267938238",
          "cited_paper_id": 6540453,
          "context_text": "We employ DIV2K [64] and Flickr2K [42] to train classic SR models and use DIV2K only to train lightweight SR models.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions two specific datasets, DIV2K and Flickr2K, which are used for training single image super-resolution models. The usage is clearly described, and both datasets are relevant to the topic of image restoration.",
          "citing_paper_doi": "10.48550/arXiv.2402.15648",
          "cited_paper_doi": "10.1109/CVPRW.2017.151",
          "citing_paper_url": "https://www.semanticscholar.org/paper/e730beb44042499763d36214c0498434e470dfd5",
          "cited_paper_url": "https://www.semanticscholar.org/paper/7ba5d3808e117e7a68dc40331ce1d483ceeedcb2",
          "citing_paper_year": 2024,
          "cited_paper_year": 2017
        },
        {
          "dataset_name": "DIV2K",
          "dataset_description": "Used for training in gaussian denoising, introducing new challenges for image quality assessment models. | Used for training in gaussian denoising, providing high-resolution images for super-resolution tasks. | Used for training in gaussian denoising, offering a diverse set of images for image quality assessment. | Used for training in gaussian denoising, containing images for benchmarking image restoration algorithms.",
          "citing_paper_id": "270155352",
          "cited_paper_id": 4493958,
          "context_text": "For gaussian denoising, weusedtrainingdata:DIV2K[83],Flickr2K[84],BSD400[85], andWED[86].",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions specific datasets used for training in the context of gaussian denoising. These datasets are clearly identified and are relevant to the topic of image restoration.",
          "citing_paper_doi": "10.1109/TMM.2024.3407656",
          "cited_paper_doi": "10.1109/CVPRW.2017.150",
          "citing_paper_url": "https://www.semanticscholar.org/paper/3a80830bf3e31c855afaff17a5b590bc052839c6",
          "cited_paper_url": "https://www.semanticscholar.org/paper/6aeb583c084ad2eb709154ec91f22ca8d6de4425",
          "citing_paper_year": 2024,
          "cited_paper_year": 2017
        },
        {
          "dataset_name": "DIV2K",
          "dataset_description": "Used for training and testing image restoration models, specifically evaluating performance on images corrupted by Gaussian blur, noise, and JPEG compression.",
          "citing_paper_id": "221173039",
          "cited_paper_id": 54440425,
          "context_text": "w knowledge while consolidating previously learned knowledge through memory replay, without accessing the old training samples. 4 Experiments 4.1 Datasets Following the experimental settings of works [47,41], we use the DIV2K dataset [2] which contains 800 images (750 images for training and 50 images for testing). The images are corrupted by a sequence of Gaussian blur, Gaussian noise and JPEG compressi",
          "confidence_score": 1.0,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions the use of the DIV2K dataset for image restoration experiments, specifically for training and testing with images corrupted by Gaussian blur, noise, and JPEG compression.",
          "citing_paper_doi": "10.1007/978-3-030-58523-5_36",
          "cited_paper_doi": "10.1109/CVPR.2019.00925",
          "citing_paper_url": "https://www.semanticscholar.org/paper/bd4346fa7145f126d32b19ced1223fcb7e7eef0e",
          "cited_paper_url": "https://www.semanticscholar.org/paper/c4e6b18b5573329a5b62f297f6ab9565f841e11b",
          "citing_paper_year": 2020,
          "cited_paper_year": 2018
        },
        {
          "dataset_name": "DIV2K",
          "dataset_description": "Used for training in gaussian denoising, introducing new challenges for image quality assessment models. | Used for training in gaussian denoising, providing high-resolution images for super-resolution tasks. | Used for training in gaussian denoising, offering a diverse set of images for image quality assessment. | Used for training in gaussian denoising, containing images for benchmarking image restoration algorithms.",
          "citing_paper_id": "270155352",
          "cited_paper_id": 4840263,
          "context_text": "For gaussian denoising, weusedtrainingdata:DIV2K[83],Flickr2K[84],BSD400[85], andWED[86].",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions specific datasets used for training in the context of gaussian denoising. These datasets are clearly identified and are relevant to the topic of image restoration.",
          "citing_paper_doi": "10.1109/TMM.2024.3407656",
          "cited_paper_doi": "10.1109/TIP.2016.2631888",
          "citing_paper_url": "https://www.semanticscholar.org/paper/3a80830bf3e31c855afaff17a5b590bc052839c6",
          "cited_paper_url": "https://www.semanticscholar.org/paper/1e2fe67d117a19a59a4f82e4950499c5ee5a7473",
          "citing_paper_year": 2024,
          "cited_paper_year": 2017
        },
        {
          "dataset_name": "DIV2K",
          "dataset_description": "Used for training in gaussian denoising, introducing new challenges for image quality assessment models. | Used for training in gaussian denoising, providing high-resolution images for super-resolution tasks. | Used for training in gaussian denoising, offering a diverse set of images for image quality assessment. | Used for training in gaussian denoising, containing images for benchmarking image restoration algorithms.",
          "citing_paper_id": "270155352",
          "cited_paper_id": 206764694,
          "context_text": "For gaussian denoising, weusedtrainingdata:DIV2K[83],Flickr2K[84],BSD400[85], andWED[86].",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions specific datasets used for training in the context of gaussian denoising. These datasets are clearly identified and are relevant to the topic of image restoration.",
          "citing_paper_doi": "10.1109/TMM.2024.3407656",
          "cited_paper_doi": "10.1109/TPAMI.2010.161",
          "citing_paper_url": "https://www.semanticscholar.org/paper/3a80830bf3e31c855afaff17a5b590bc052839c6",
          "cited_paper_url": "https://www.semanticscholar.org/paper/0e5a262bf59b68ba8a7a1103d16fa33a9f5ffc28",
          "citing_paper_year": 2024,
          "cited_paper_year": 2011
        },
        {
          "dataset_name": "DIV2K",
          "dataset_description": "Used to train models on high-quality images, focusing on single image super-resolution techniques. Contains 800 images for training and evaluation.",
          "citing_paper_id": "252683961",
          "cited_paper_id": 4493958,
          "context_text": "We train all models on DIV2K (Agustsson & Timofte, 2017), which contains 800 high-quality images.",
          "confidence_score": 1.0,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "DIV2K is a specific dataset used for training models in the context of image restoration, particularly single image super-resolution.",
          "citing_paper_doi": "10.48550/arXiv.2210.00405",
          "cited_paper_doi": "10.1109/CVPRW.2017.150",
          "citing_paper_url": "https://www.semanticscholar.org/paper/a3d9fd2c384e98fb6074a9064562a4e4dd941ed8",
          "cited_paper_url": "https://www.semanticscholar.org/paper/6aeb583c084ad2eb709154ec91f22ca8d6de4425",
          "citing_paper_year": 2022,
          "cited_paper_year": 2017
        },
        {
          "dataset_name": "DIV2K",
          "dataset_description": "Used as a source of clean images for training and validation in image restoration, specifically employing 750 images for training and 50 for validation.",
          "citing_paper_id": "251772829",
          "cited_paper_id": 4493958,
          "context_text": "Following SHDD [21], we use DIV2K [3] as clean images; 750 images from the training dataset are used to create the training set of our dataset and 50 images for the validation set.",
          "confidence_score": 1.0,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "DIV2K is explicitly mentioned as a dataset used for clean images in the context of creating a training and validation set for image restoration.",
          "citing_paper_doi": "10.1109/CVPRW56347.2022.00069",
          "cited_paper_doi": "10.1109/CVPRW.2017.150",
          "citing_paper_url": "https://www.semanticscholar.org/paper/ad7a07d34fc95885301241f5943ffe99ba369206",
          "cited_paper_url": "https://www.semanticscholar.org/paper/6aeb583c084ad2eb709154ec91f22ca8d6de4425",
          "citing_paper_year": 2022,
          "cited_paper_year": 2017
        },
        {
          "dataset_name": "DIV2K",
          "dataset_description": "Used for evaluating face super-resolution, focusing on high-quality facial details and realistic face reconstruction. | Commonly used for evaluating super-resolution, providing a diverse set of natural images for benchmarking. | Commonly used for evaluating super-resolution, providing urban scenes to test the ability to reconstruct complex textures and structures. | Used for evaluating natural image super-resolution, providing a diverse set of images to test the generalizability of SR models. | Used to test supervised diffusion model-based image restoration models, focusing on 4x image super-resolution performance. | Commonly used for evaluating super-resolution, focusing on manga images to test the ability to reconstruct line art and text. | Commonly used for evaluating super-resolution, providing a small but widely recognized set of images for benchmarking. | Commonly used for evaluating super-resolution, offering a slightly larger set of images for more comprehensive testing.",
          "citing_paper_id": "261031011",
          "cited_paper_id": 8282555,
          "context_text": "The experimental re-sults for supervised diffusion model-based IR models on 4x image super-resolution are listed in Table 3, which are tested on the DIV2K [45] and Urban100 [266] datasets.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions two specific datasets, DIV2K and Urban100, used for testing image super-resolution models.",
          "citing_paper_doi": "10.48550/arXiv.2308.09388",
          "cited_paper_doi": "10.1109/CVPR.2015.7299156",
          "citing_paper_url": "https://www.semanticscholar.org/paper/70ba95384664ab7becfd8196d20a46fe66112c91",
          "cited_paper_url": "https://www.semanticscholar.org/paper/2946c4e29b057d3543f09b89f547965895676d19",
          "citing_paper_year": 2023,
          "cited_paper_year": 2015
        },
        {
          "dataset_name": "DIV2K",
          "dataset_description": "Used for training in traditional image super-resolution, specifically focusing on bicubic downsampling.",
          "citing_paper_id": "261031011",
          "cited_paper_id": 201303660,
          "context_text": "For traditional image SR ( i.e. , bicubic downsampling), the standard training data is typically composed of DIV2K [45] and Flick2K [262].",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions two specific datasets, DIV2K and Flick2K, which are used for training in the context of image super-resolution.",
          "citing_paper_doi": "10.48550/arXiv.2308.09388",
          "cited_paper_doi": "10.1109/ICCVW.2019.00478",
          "citing_paper_url": "https://www.semanticscholar.org/paper/70ba95384664ab7becfd8196d20a46fe66112c91",
          "cited_paper_url": "https://www.semanticscholar.org/paper/4cc498bd4bb4eaea28ffdc9e48dcd44b9b15bdf2",
          "citing_paper_year": 2023,
          "cited_paper_year": 2019
        },
        {
          "dataset_name": "DIV2K",
          "dataset_description": "Used to produce hybrid distortion datasets, focusing on generating diverse image degradation scenarios for training and evaluating image restoration models. | Used to evaluate the performance of FDR-Net on hybrid distortions, demonstrating superior results compared to state-of-the-art methods. | Used to compare the performance of FDR-Net with state-of-the-art methods, specifically evaluating image restoration quality and efficiency.",
          "citing_paper_id": "220686623",
          "cited_paper_id": 4746623,
          "context_text": "In this section, we compare the performance of our FDR-Net on the DIV2K dataset with state-of-the-art methods, including RL-Restore [37] and operationwise attention network [28].",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions the DIV2K dataset, which is a well-known dataset in the field of image restoration. The dataset is used to compare the performance of FDR-Net with other state-of-the-art methods.",
          "citing_paper_doi": "10.1007/978-3-030-58526-6_19",
          "cited_paper_doi": "10.1109/CVPR.2018.00259",
          "citing_paper_url": "https://www.semanticscholar.org/paper/78a92b98a173a54ccf3cbc02b9b7fe3384dbef1d",
          "cited_paper_url": "https://www.semanticscholar.org/paper/9fb3707a0f90c6620251d202a972a2c626dce976",
          "citing_paper_year": 2020,
          "cited_paper_year": 2018
        },
        {
          "dataset_name": "DIV2K",
          "dataset_description": "Used to compare the performance of FDR-Net with state-of-the-art methods, specifically evaluating image restoration quality and efficiency.",
          "citing_paper_id": "220686623",
          "cited_paper_id": 54440425,
          "context_text": "In this section, we compare the performance of our FDR-Net on the DIV2K dataset with state-of-the-art methods, including RL-Restore [37] and operationwise attention network [28].",
          "confidence_score": 1.0,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions the DIV2K dataset, which is a well-known dataset in the field of image restoration. The dataset is used to compare the performance of FDR-Net with other state-of-the-art methods.",
          "citing_paper_doi": "10.1007/978-3-030-58526-6_19",
          "cited_paper_doi": "10.1109/CVPR.2019.00925",
          "citing_paper_url": "https://www.semanticscholar.org/paper/78a92b98a173a54ccf3cbc02b9b7fe3384dbef1d",
          "cited_paper_url": "https://www.semanticscholar.org/paper/c4e6b18b5573329a5b62f297f6ab9565f841e11b",
          "citing_paper_year": 2020,
          "cited_paper_year": 2018
        },
        {
          "dataset_name": "DIV2K",
          "dataset_description": "Used for evaluation of image super-resolution methods, consisting of 100 high-resolution images. | Used for evaluation of image super-resolution methods, focusing on urban scenes with complex textures. | Used for evaluation of image super-resolution methods, containing manga images with distinct artistic styles. | Used as training data for image super-resolution, focusing on high-quality images with diverse content. | Used for evaluation of image super-resolution methods, containing a small set of high-quality images. | Used as training data for image super-resolution, providing a large set of high-resolution images. | Used for evaluation of image super-resolution methods, containing a set of diverse images.",
          "citing_paper_id": "252693111",
          "cited_paper_id": 49657846,
          "context_text": "For image SR, following previous works Zhang et al. (2018b); Haris et al. (2018), we use DIV2K Timofte et al. (2017) and Flickr2K Lim et al. (2017) as training data, Set5 Bevilacqua et al. (2012), Set14 Zeyde et al. (2010), B100 Martin et al. (2001), Urban100 Huang et al. (2015), and Manga109…",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions several datasets used for training and evaluation in image super-resolution tasks, which are directly relevant to the topic of all-in-one image restoration.",
          "citing_paper_doi": "10.48550/arXiv.2210.01427",
          "cited_paper_doi": "10.1007/978-3-030-01234-2_18",
          "citing_paper_url": "https://www.semanticscholar.org/paper/a1f7f5597fdbc54e58f1f2a1a640bf355e87a978",
          "cited_paper_url": "https://www.semanticscholar.org/paper/9775f8964a2eea1c9e35a02b1b906487396ea1f5",
          "citing_paper_year": 2022,
          "cited_paper_year": 2018
        },
        {
          "dataset_name": "DIV2K",
          "dataset_description": "Used for training models in image denoising and JPEG CAR, focusing on natural images with various content. | Used for training models in image denoising and JPEG CAR, specifically designed for web images with diverse qualities. | Used for training models in image denoising and JPEG CAR, providing high-resolution images for restoration tasks. | Used for training models in image denoising and JPEG CAR, offering a diverse set of images for robustness.",
          "citing_paper_id": "252693111",
          "cited_paper_id": 4840263,
          "context_text": "For image denoising and JPEG CAR, same as SwinIR Liang et al. (2021), we use training data: DIV2K, Flickr2K, BSD500 Arbelaez et al. (2010), and WED Ma et al. (2016).",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions specific datasets used for training in the context of image denoising and JPEG CAR. These datasets are clearly identified and are relevant to the research topic of image restoration.",
          "citing_paper_doi": "10.48550/arXiv.2210.01427",
          "cited_paper_doi": "10.1109/TIP.2016.2631888",
          "citing_paper_url": "https://www.semanticscholar.org/paper/a1f7f5597fdbc54e58f1f2a1a640bf355e87a978",
          "cited_paper_url": "https://www.semanticscholar.org/paper/1e2fe67d117a19a59a4f82e4950499c5ee5a7473",
          "citing_paper_year": 2022,
          "cited_paper_year": 2017
        },
        {
          "dataset_name": "DIV2K",
          "dataset_description": "Used for image restoration, containing 4744 clean images. Compared with HQ-50K to highlight the larger scale of the latter. | Used for single image super-resolution tasks, including 2,650 high-resolution images. The dataset is designed to address specific degradation types. | Used as a benchmark for comparing the size of the new dataset, highlighting the scale difference in the number of images for single image super-resolution tasks. | Used as a benchmark dataset for single image super-resolution, providing high-quality images for training and evaluation. | Used for image restoration, containing 400 clean images. Compared with HQ-50K to highlight the larger scale of the latter. | Used for rain removal tasks, providing a large dataset of rainy images for training and evaluation. | Used to systematically explore the impact of resolution by resizing images to match the average pixel count of ImageNet images (200,000 pixels). | Used to train and evaluate image restoration models, but noted for limited coverage of artificial images, leading to a performance gap compared to HQ-50K. | Part of the DF2K dataset, used for training and evaluating single image super-resolution models, providing a diverse set of images. | Used for image restoration, containing 2650 clean images. Compared with HQ-50K to highlight the larger scale of the latter. | Used as a large-scale dataset for image restoration, containing diverse images for training and testing models. | Used for image restoration, containing 1212 clean images. Compared with HQ-50K to highlight the larger scale of the latter. | Used to compare the average pixel number of images, highlighting its size relative to other datasets in image restoration and super-resolution tasks. | Used for image restoration, particularly for evaluating models on web images with various degradations. | Used for single image super-resolution tasks, including 800 high-resolution images. The dataset is designed to address specific degradation types. | Part of the DF2K dataset, used for training and evaluating single image super-resolution models, providing high-quality images. | Used as a baseline for single image super-resolution, combining images from DIV2K and Flickr2K to train and evaluate models. | Used to train and evaluate image restoration models, focusing on high-quality images to improve performance on diverse image types. | Used for single image super-resolution, comprising outdoor natural images to train and evaluate restoration models. | Used for image restoration, containing 800 clean images. Compared with HQ-50K to highlight the larger scale of the latter.",
          "citing_paper_id": "259108489",
          "cited_paper_id": 4493958,
          "context_text": "In order to systematically explore the impact of resolution, we resize the images from DIV2K [2] to the same resolution as the average pixel count of images in ImageNet (200,000 pixels).",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions two datasets, DIV2K and ImageNet, which are both used for image resizing experiments to study the impact of resolution.",
          "citing_paper_doi": "10.48550/arXiv.2306.05390",
          "cited_paper_doi": "10.1109/CVPRW.2017.150",
          "citing_paper_url": "https://www.semanticscholar.org/paper/4296bf82e67051e4cba10e510932f55fc12dd1e2",
          "cited_paper_url": "https://www.semanticscholar.org/paper/6aeb583c084ad2eb709154ec91f22ca8d6de4425",
          "citing_paper_year": 2023,
          "cited_paper_year": 2017
        },
        {
          "dataset_name": "DIV2K",
          "dataset_description": "Used for training models in image denoising and JPEG CAR, focusing on natural images with various content. | Used for training models in image denoising and JPEG CAR, specifically designed for web images with diverse qualities. | Used for training models in image denoising and JPEG CAR, providing high-resolution images for restoration tasks. | Used for training models in image denoising and JPEG CAR, offering a diverse set of images for robustness.",
          "citing_paper_id": "252693111",
          "cited_paper_id": 206764694,
          "context_text": "For image denoising and JPEG CAR, same as SwinIR Liang et al. (2021), we use training data: DIV2K, Flickr2K, BSD500 Arbelaez et al. (2010), and WED Ma et al. (2016).",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions specific datasets used for training in the context of image denoising and JPEG CAR. These datasets are clearly identified and are relevant to the research topic of image restoration.",
          "citing_paper_doi": "10.48550/arXiv.2210.01427",
          "cited_paper_doi": "10.1109/TPAMI.2010.161",
          "citing_paper_url": "https://www.semanticscholar.org/paper/a1f7f5597fdbc54e58f1f2a1a640bf355e87a978",
          "cited_paper_url": "https://www.semanticscholar.org/paper/0e5a262bf59b68ba8a7a1103d16fa33a9f5ffc28",
          "citing_paper_year": 2022,
          "cited_paper_year": 2011
        },
        {
          "dataset_name": "DIV2K",
          "dataset_description": "Used for training models for image super-resolution (×2), focusing on enhancing image quality and resolution. | Used for training models in image denoising and JPEG CAR, providing high-resolution images for restoration tasks. | Used as test data for evaluating image super-resolution performance, focusing on a small set of benchmark images. | Used for training models in image denoising and JPEG CAR, specifically designed for web images with diverse qualities. | Used for training models in image denoising and JPEG CAR, focusing on natural images with various content. | Used for training models in image denoising and JPEG CAR, offering a diverse set of images for robustness. | Used as training data for image super-resolution, providing high-quality images for upscaling and enhancement. | Used as test data for evaluating image super-resolution performance, providing a moderate-sized set of benchmark images. | Used as test data for evaluating image super-resolution performance, tailored for manga illustrations. | Used for training models for 2x image superresolution, focusing on high-resolution image generation from low-resolution inputs. | Used as test data for evaluating image super-resolution performance, specifically designed for urban scenes. | Used as training data for image super-resolution, offering a diverse set of images for model training. | Used as test data for evaluating image super-resolution performance, containing a variety of natural images.",
          "citing_paper_id": "252693111",
          "cited_paper_id": 206769988,
          "context_text": "For image denoising and JPEG CAR, same as SwinIR Liang et al. (2021), we use training data: DIV2K, Flickr2K, BSD500 Arbelaez et al. (2010), and WED Ma et al. (2016).",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions specific datasets used for training in the context of image denoising and JPEG CAR. These datasets are clearly identified and are relevant to the research topic of image restoration.",
          "citing_paper_doi": "10.48550/arXiv.2210.01427",
          "cited_paper_doi": "10.1109/ICCV.2013.241",
          "citing_paper_url": "https://www.semanticscholar.org/paper/a1f7f5597fdbc54e58f1f2a1a640bf355e87a978",
          "cited_paper_url": "https://www.semanticscholar.org/paper/f3cb2b0b4c1fc5cd06d20ceabb7ebfccdce90ad8",
          "citing_paper_year": 2022,
          "cited_paper_year": 2013
        },
        {
          "dataset_name": "DIV2K",
          "dataset_description": "Used for training in image restoration, containing images for various restoration tasks including denoising and deblurring. | Used for testing in image restoration, offering a variety of images to assess generalization and robustness. | Used for testing in image restoration, focusing on benchmarking denoising performance. | Used for testing in image restoration, providing a standard set of images for evaluating restoration quality. | Used for training in image restoration, providing high-quality images for super-resolution tasks. | Used for testing in image restoration, specifically for evaluating super-resolution algorithms on urban scenes. | Used for training in image restoration, offering a large set of diverse images for model training. | Used for training in image restoration, providing a diverse set of images for enhancement and restoration.",
          "citing_paper_id": "271543778",
          "cited_paper_id": 4493958,
          "context_text": "We adopt a combined set including 900 images from DIV2K [2], 2650 images from Flickr2K, 400 images from BSD500 [3], and 4744 images from WaterlooED (WED) [38], as the training dataset, and use four datasets, including CBSD68 [39], Urban100 [20], Kodak24 [17], and McMaster [77], as the test dataset.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions several image datasets used for training and testing in the context of image restoration. These datasets are specific and verifiable.",
          "citing_paper_doi": "10.48550/arXiv.2407.20928",
          "cited_paper_doi": "10.1109/CVPRW.2017.150",
          "citing_paper_url": "https://www.semanticscholar.org/paper/261cde5efca4a5c146479bf42fb9438b581cd839",
          "cited_paper_url": "https://www.semanticscholar.org/paper/6aeb583c084ad2eb709154ec91f22ca8d6de4425",
          "citing_paper_year": 2024,
          "cited_paper_year": 2017
        },
        {
          "dataset_name": "DIV2K",
          "dataset_description": "Used for training in image restoration, containing images for various restoration tasks including denoising and deblurring. | Used for testing in image restoration, offering a variety of images to assess generalization and robustness. | Used for testing in image restoration, focusing on benchmarking denoising performance. | Used for testing in image restoration, providing a standard set of images for evaluating restoration quality. | Used for training in image restoration, providing high-quality images for super-resolution tasks. | Used for testing in image restoration, specifically for evaluating super-resolution algorithms on urban scenes. | Used for training in image restoration, offering a large set of diverse images for model training. | Used for training in image restoration, providing a diverse set of images for enhancement and restoration.",
          "citing_paper_id": "271543778",
          "cited_paper_id": 15614128,
          "context_text": "We adopt a combined set including 900 images from DIV2K [2], 2650 images from Flickr2K, 400 images from BSD500 [3], and 4744 images from WaterlooED (WED) [38], as the training dataset, and use four datasets, including CBSD68 [39], Urban100 [20], Kodak24 [17], and McMaster [77], as the test dataset.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions several image datasets used for training and testing in the context of image restoration. These datasets are specific and verifiable.",
          "citing_paper_doi": "10.48550/arXiv.2407.20928",
          "cited_paper_doi": "10.1117/1.3600632",
          "citing_paper_url": "https://www.semanticscholar.org/paper/261cde5efca4a5c146479bf42fb9438b581cd839",
          "cited_paper_url": "https://www.semanticscholar.org/paper/47b9a4dfbff5d2bb8b4f8f6e6fc2294426aab8ac",
          "citing_paper_year": 2024,
          "cited_paper_year": 2011
        },
        {
          "dataset_name": "DIV2K",
          "dataset_description": "Used for training in image restoration, containing images for various restoration tasks including denoising and deblurring. | Used for testing in image restoration, offering a variety of images to assess generalization and robustness. | Used for testing in image restoration, focusing on benchmarking denoising performance. | Used for testing in image restoration, providing a standard set of images for evaluating restoration quality. | Used for training in image restoration, providing high-quality images for super-resolution tasks. | Used for testing in image restoration, specifically for evaluating super-resolution algorithms on urban scenes. | Used for training in image restoration, offering a large set of diverse images for model training. | Used for training in image restoration, providing a diverse set of images for enhancement and restoration.",
          "citing_paper_id": "271543778",
          "cited_paper_id": null,
          "context_text": "We adopt a combined set including 900 images from DIV2K [2], 2650 images from Flickr2K, 400 images from BSD500 [3], and 4744 images from WaterlooED (WED) [38], as the training dataset, and use four datasets, including CBSD68 [39], Urban100 [20], Kodak24 [17], and McMaster [77], as the test dataset.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions several image datasets used for training and testing in the context of image restoration. These datasets are specific and verifiable.",
          "citing_paper_doi": "10.48550/arXiv.2407.20928",
          "cited_paper_doi": null,
          "citing_paper_url": "https://www.semanticscholar.org/paper/261cde5efca4a5c146479bf42fb9438b581cd839",
          "cited_paper_url": null,
          "citing_paper_year": 2024,
          "cited_paper_year": null
        },
        {
          "dataset_name": "DIV2K",
          "dataset_description": "Used to assess image restoration techniques, particularly for super-resolution, on a diverse set of 100 images. | Used to evaluate image restoration methods, focusing on super-resolution performance on a small set of high-quality images. | Used to validate image restoration models, specifically for super-resolution, using 100 high-resolution images. | Used to train and fine-tune models on 800 high-resolution images, focusing on single image super-resolution techniques.",
          "citing_paper_id": "236772838",
          "cited_paper_id": 4493958,
          "context_text": "We train and ﬁne-tune our models on the DIV2K training dataset [2], which consists of 800 high-resolution images.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions the use of the DIV2K training dataset, which is a specific and verifiable dataset used for training and fine-tuning models.",
          "citing_paper_doi": null,
          "cited_paper_doi": "10.1109/CVPRW.2017.150",
          "citing_paper_url": "https://www.semanticscholar.org/paper/dbfdb4235ac0d766b2f1a2dde89de6ed60e86605",
          "cited_paper_url": "https://www.semanticscholar.org/paper/6aeb583c084ad2eb709154ec91f22ca8d6de4425",
          "citing_paper_year": 2021,
          "cited_paper_year": 2017
        },
        {
          "dataset_name": "DIV2K",
          "dataset_description": "Used to evaluate the performance of Swin2SR on restored samples, focusing on single image super-resolution tasks.",
          "citing_paper_id": "259075580",
          "cited_paper_id": 4493958,
          "context_text": "For Swin2SR [13], we use the official DIV2K [36] restored samples provided by the authors.",
          "confidence_score": 1.0,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "DIV2K is a well-known dataset for image super-resolution tasks, which aligns with the context of using restored samples for Swin2SR.",
          "citing_paper_doi": "10.48550/arXiv.2306.02342",
          "cited_paper_doi": "10.1109/CVPRW.2017.150",
          "citing_paper_url": "https://www.semanticscholar.org/paper/e0c1ad4a0335682ccf71426225d80e4aa800e47e",
          "cited_paper_url": "https://www.semanticscholar.org/paper/6aeb583c084ad2eb709154ec91f22ca8d6de4425",
          "citing_paper_year": 2023,
          "cited_paper_year": 2017
        },
        {
          "dataset_name": "DIV2K",
          "dataset_description": "Used for training a model to reduce JPEG compression artifacts, providing a variety of image qualities and challenges. | Used for training a model to reduce JPEG compression artifacts, contributing additional images for comprehensive evaluation. | Used for training a model to reduce JPEG compression artifacts, providing high-quality images for restoration tasks. | Used for training a model to reduce JPEG compression artifacts, offering a diverse set of images for robustness.",
          "citing_paper_id": "254018182",
          "cited_paper_id": 4840263,
          "context_text": "For JPEG compression artifact reduction, training set consists of DIV2K [37], Flickr2K [22], BSD500 [2], and WED [26].",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions specific datasets used for training a model for JPEG compression artifact reduction. These datasets are clearly named and are relevant to the topic of image restoration.",
          "citing_paper_doi": "10.48550/arXiv.2211.13654",
          "cited_paper_doi": "10.1109/TIP.2016.2631888",
          "citing_paper_url": "https://www.semanticscholar.org/paper/1c49e58a935c80c8ec8307e937fc61c8f1f80433",
          "cited_paper_url": "https://www.semanticscholar.org/paper/1e2fe67d117a19a59a4f82e4950499c5ee5a7473",
          "citing_paper_year": 2022,
          "cited_paper_year": 2017
        },
        {
          "dataset_name": "DIV2K",
          "dataset_description": "Used to train COLA-Net for compression artifact reduction tasks, focusing on high-quality images to improve restoration performance. | Used to train COLA-Net for synthetic image denoising and compression artifact reduction, focusing on high-quality images.",
          "citing_paper_id": "232170566",
          "cited_paper_id": 484327,
          "context_text": "For synthetic image denoising and compression artifact reduction tasks, we train COLA-Net with DIV2K [42] dataset, which contains 800 high-quality images.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions the use of the DIV2K dataset for training a network on synthetic image denoising and compression artifact reduction tasks.",
          "citing_paper_doi": "10.1109/TMM.2021.3063916",
          "cited_paper_doi": "10.1109/CVPRW.2017.149",
          "citing_paper_url": "https://www.semanticscholar.org/paper/7d76ae738c806330632255ad2c592525b3d2df22",
          "cited_paper_url": "https://www.semanticscholar.org/paper/5054edca22325ddd3507b860f9af4a961baea009",
          "citing_paper_year": 2021,
          "cited_paper_year": 2017
        },
        {
          "dataset_name": "DIV2K",
          "dataset_description": "Used to compare the proposed dataset in single image super-resolution, combining images from DIV2K and Flickr2K for a larger dataset. | Used to train and validate image restoration models, specifically focusing on high-resolution image generation from low-resolution inputs. | Used to compare the proposed dataset in single image super-resolution, providing a large-scale dataset with diverse categories. | Used to compare the proposed dataset in single image super-resolution, focusing on high-resolution image quality and diversity. | Used to evaluate single image super-resolution techniques, focusing on the impact of down-sampling factors on image content.",
          "citing_paper_id": "260843368",
          "cited_paper_id": 4493958,
          "context_text": "For the older DIV2K dataset [3], the down-sampling factor is manually determined depending on the image content.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions the DIV2K dataset, which is a well-known dataset in the field of image restoration. The dataset is used for evaluating super-resolution techniques.",
          "citing_paper_doi": "10.1109/CVPRW59228.2023.00178",
          "cited_paper_doi": "10.1109/CVPRW.2017.150",
          "citing_paper_url": "https://www.semanticscholar.org/paper/851446ea3c70c918cdf8c62db0dd0b1900806e5f",
          "cited_paper_url": "https://www.semanticscholar.org/paper/6aeb583c084ad2eb709154ec91f22ca8d6de4425",
          "citing_paper_year": 2023,
          "cited_paper_year": 2017
        },
        {
          "dataset_name": "DIV2K",
          "dataset_description": "Used for evaluating image restoration algorithms, particularly for tasks involving image segmentation and denoising. | Used for training and evaluation in image restoration tasks, offering a diverse set of natural images for algorithm testing. | Used for training and evaluation in single image super-resolution tasks, providing high-quality images for benchmarking. | Used for training and evaluating single image super-resolution models, focusing on high-resolution image restoration and quality enhancement.",
          "citing_paper_id": "243938678",
          "cited_paper_id": 4482028,
          "context_text": "Note that this is unlike most prior work [Dong et al. 2015; Liu et al. 2018b], which mainly use small datasets such as DIV2K [Agustsson and Timofte 2017] and BSD500 [Martin et al. 2001] for training and evaluation.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions two specific datasets, DIV2K and BSD500, which are used for training and evaluation in image restoration tasks.",
          "citing_paper_doi": "10.1145/3528233.3530757",
          "cited_paper_doi": null,
          "citing_paper_url": "https://www.semanticscholar.org/paper/37c9c4e7648f639c0b36f150fc6c6c90b3682f4a",
          "cited_paper_url": "https://www.semanticscholar.org/paper/12dd69d51accbda91187f33142d1c0ecd8475666",
          "citing_paper_year": 2021,
          "cited_paper_year": 2017
        },
        {
          "dataset_name": "DIV2K",
          "dataset_description": "Used for training and evaluation in image restoration tasks, offering a diverse set of natural images for algorithm testing. | Used for training and evaluation in single image super-resolution tasks, providing high-quality images for benchmarking.",
          "citing_paper_id": "243938678",
          "cited_paper_id": 4493958,
          "context_text": "Note that this is unlike most prior work [Dong et al. 2015; Liu et al. 2018b], which mainly use small datasets such as DIV2K [Agustsson and Timofte 2017] and BSD500 [Martin et al. 2001] for training and evaluation.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions two specific datasets, DIV2K and BSD500, which are used for training and evaluation in image restoration tasks.",
          "citing_paper_doi": "10.1145/3528233.3530757",
          "cited_paper_doi": "10.1109/CVPRW.2017.150",
          "citing_paper_url": "https://www.semanticscholar.org/paper/37c9c4e7648f639c0b36f150fc6c6c90b3682f4a",
          "cited_paper_url": "https://www.semanticscholar.org/paper/6aeb583c084ad2eb709154ec91f22ca8d6de4425",
          "citing_paper_year": 2021,
          "cited_paper_year": 2017
        },
        {
          "dataset_name": "DIV2K",
          "dataset_description": "Used for training and evaluation in image restoration tasks, offering a diverse set of natural images for algorithm testing. | Used for training and evaluation in single image super-resolution tasks, providing high-quality images for benchmarking.",
          "citing_paper_id": "243938678",
          "cited_paper_id": 5035107,
          "context_text": "Note that this is unlike most prior work [Dong et al. 2015; Liu et al. 2018b], which mainly use small datasets such as DIV2K [Agustsson and Timofte 2017] and BSD500 [Martin et al. 2001] for training and evaluation.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions two specific datasets, DIV2K and BSD500, which are used for training and evaluation in image restoration tasks.",
          "citing_paper_doi": "10.1145/3528233.3530757",
          "cited_paper_doi": "10.1007/978-3-030-01252-6_6",
          "citing_paper_url": "https://www.semanticscholar.org/paper/37c9c4e7648f639c0b36f150fc6c6c90b3682f4a",
          "cited_paper_url": "https://www.semanticscholar.org/paper/2a417a16473e2bcb1c98cd7814bc106760925e60",
          "citing_paper_year": 2021,
          "cited_paper_year": 2018
        },
        {
          "dataset_name": "DIV2K",
          "dataset_description": "Used for training and evaluation in image restoration tasks, offering a diverse set of natural images for algorithm testing. | Used for training and evaluation in single image super-resolution tasks, providing high-quality images for benchmarking.",
          "citing_paper_id": "243938678",
          "cited_paper_id": 29151865,
          "context_text": "Note that this is unlike most prior work [Dong et al. 2015; Liu et al. 2018b], which mainly use small datasets such as DIV2K [Agustsson and Timofte 2017] and BSD500 [Martin et al. 2001] for training and evaluation.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions two specific datasets, DIV2K and BSD500, which are used for training and evaluation in image restoration tasks.",
          "citing_paper_doi": "10.1145/3528233.3530757",
          "cited_paper_doi": "10.1109/CVPRW.2018.00121",
          "citing_paper_url": "https://www.semanticscholar.org/paper/37c9c4e7648f639c0b36f150fc6c6c90b3682f4a",
          "cited_paper_url": "https://www.semanticscholar.org/paper/5813ae0a48ff50d48f406df52ee5b9795c34a8bd",
          "citing_paper_year": 2021,
          "cited_paper_year": 2018
        },
        {
          "dataset_name": "DIV2K",
          "dataset_description": "Used for training MaIR for synthetic noise removal, providing high-quality images for restoration tasks. | Used for training MaIR for synthetic noise removal, providing additional images for robustness in restoration. | Used for training MaIR for synthetic noise removal, offering a variety of image quality challenges. | Used for training MaIR for synthetic noise removal, contributing diverse real-world images to the training set.",
          "citing_paper_id": "275133634",
          "cited_paper_id": 4840263,
          "context_text": "Datasets: For synthetic noise removal, we train MaIR on DFWB, which consists of DIV2K, Flickr2K, Water-loo Exploration Dataset (WED) [34] and BSD400 [35].",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions specific datasets used for training a model for synthetic noise removal. These datasets are clearly identified and are relevant to the topic of image restoration.",
          "citing_paper_doi": "10.48550/arXiv.2412.20066",
          "cited_paper_doi": "10.1109/TIP.2016.2631888",
          "citing_paper_url": "https://www.semanticscholar.org/paper/2b78fc275b7fc3a37061620ae155c017ab6c22e1",
          "cited_paper_url": "https://www.semanticscholar.org/paper/1e2fe67d117a19a59a4f82e4950499c5ee5a7473",
          "citing_paper_year": 2024,
          "cited_paper_year": 2017
        },
        {
          "dataset_name": "DIV2K",
          "dataset_description": "Used to train models for single image super-resolution, focusing on enhancing image quality and resolution using 800 training images.",
          "citing_paper_id": "85501306",
          "cited_paper_id": 484327,
          "context_text": "We use 800 training images in DIV2K (Timofte et al., 2017; Agustsson & Timofte, 2017) to train all of our models.",
          "confidence_score": 1.0,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "DIV2K is a well-known dataset for image super-resolution and restoration tasks, which aligns with the research topic of All-in-One Image Restoration.",
          "citing_paper_doi": null,
          "cited_paper_doi": "10.1109/CVPRW.2017.149",
          "citing_paper_url": "https://www.semanticscholar.org/paper/34439db81b482cd562e1cdba974c70a2b89cd6d4",
          "cited_paper_url": "https://www.semanticscholar.org/paper/5054edca22325ddd3507b860f9af4a961baea009",
          "citing_paper_year": 2019,
          "cited_paper_year": 2017
        },
        {
          "dataset_name": "DIV2K",
          "dataset_description": "Used for training the DASR model, specifically targeting outdoor scenes for image restoration. | Used for training the DASR model, focusing on high-resolution image restoration and enhancement. | Used for training the DASR model, providing a diverse set of images for super-resolution tasks.",
          "citing_paper_id": "247762315",
          "cited_paper_id": 52154773,
          "context_text": "Following previous works [2,29], we employ DIV2K, Flickr2K, and OutdoorSceneTraining datasets for training our DASR model.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions specific datasets used for training a model, which are relevant to the topic of image restoration.",
          "citing_paper_doi": "10.48550/arXiv.2203.14216",
          "cited_paper_doi": "10.1007/978-3-030-11021-5_5",
          "citing_paper_url": "https://www.semanticscholar.org/paper/c89ffcb32a0741a89159aec95ad2e1a6017b4d1a",
          "cited_paper_url": "https://www.semanticscholar.org/paper/1bdd30a8acc75c58a1bdd4daa4545d5f3971a826",
          "citing_paper_year": 2022,
          "cited_paper_year": 2018
        },
        {
          "dataset_name": "DIV2K",
          "dataset_description": "Used for denoising experiments with a noise level of σ = 50, evaluating the effectiveness of the proposed FROT cost in image restoration. | Used for de-raining tasks to validate the transport residual condition module, focusing on removing rain streaks from images. | Used for deraining experiments, testing the ability of the proposed FROT cost to remove rain artifacts from images. | Used for super-resolution tasks to validate the transport residual condition module, focusing on high-resolution image reconstruction. | Used to evaluate deraining performance on real-world images, assessing the effectiveness of GAN-based restoration methods. | Used to evaluate super-resolution performance, focusing on enhancing image resolution using GAN-based methods. | Used to evaluate deraining performance, focusing on synthetic rain removal using GAN-based methods. | Used for super-resolution experiments, assessing the performance of the proposed FROT cost in enhancing image resolution. | Used for denoising tasks to validate the transport residual condition module, focusing on reducing noise in images. | Used for dehazing experiments, evaluating the proposed FROT cost's effectiveness in improving visibility in hazy images.",
          "citing_paper_id": "269605449",
          "cited_paper_id": 4493958,
          "context_text": "We validate the importance of the transport residual condition module on three tasks (SR on DIV2K (Agustsson & Timofte, 2017), De-raining on Rain100L (Fan et al., 2019), and Denoising on Kodak24 (Franzen, 1999)).",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions three specific datasets used for validating the transport residual condition module in image restoration tasks.",
          "citing_paper_doi": "10.48550/arXiv.2405.02843",
          "cited_paper_doi": "10.1109/CVPRW.2017.150",
          "citing_paper_url": "https://www.semanticscholar.org/paper/0c7542272e5edcff29608a0e37f9a3b705b363e7",
          "cited_paper_url": "https://www.semanticscholar.org/paper/6aeb583c084ad2eb709154ec91f22ca8d6de4425",
          "citing_paper_year": 2024,
          "cited_paper_year": 2017
        },
        {
          "dataset_name": "DIV2K",
          "dataset_description": "Used for evaluation of image super-resolution methods, consisting of 100 high-resolution images. | Used for evaluation of image super-resolution methods, focusing on urban scenes with complex textures. | Used for evaluation of image super-resolution methods, containing manga images with distinct artistic styles. | Used as training data for image super-resolution, focusing on high-quality images with diverse content. | Used for evaluation of image super-resolution methods, containing a small set of high-quality images. | Used as training data for image super-resolution, providing a large set of high-resolution images. | Used for evaluation of image super-resolution methods, containing a set of diverse images.",
          "citing_paper_id": "252693111",
          "cited_paper_id": 2141622,
          "context_text": "For image SR, following previous works Zhang et al. (2018b); Haris et al. (2018), we use DIV2K Timofte et al. (2017) and Flickr2K Lim et al. (2017) as training data, Set5 Bevilacqua et al. (2012), Set14 Zeyde et al. (2010), B100 Martin et al. (2001), Urban100 Huang et al. (2015), and Manga109…",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions several datasets used for training and evaluation in image super-resolution tasks, which are directly relevant to the topic of all-in-one image restoration.",
          "citing_paper_doi": "10.48550/arXiv.2210.01427",
          "cited_paper_doi": "10.1109/CVPR.2018.00344",
          "citing_paper_url": "https://www.semanticscholar.org/paper/a1f7f5597fdbc54e58f1f2a1a640bf355e87a978",
          "cited_paper_url": "https://www.semanticscholar.org/paper/b26b4c9da872d3c6122dbf23b9d6e063dc6456b5",
          "citing_paper_year": 2022,
          "cited_paper_year": 2017
        },
        {
          "dataset_name": "DIV2K",
          "dataset_description": "Used alongside DIV2K to enhance the training of the TGSR network, providing additional diverse image pairs for robustness. | Used to train the TGSR network for single image super-resolution, focusing on high-quality image pairs for task grouping. | Used to train the TGSR network, specifically for outdoor scenes, enhancing the network's performance on real-world images.",
          "citing_paper_id": "268030722",
          "cited_paper_id": 4493958,
          "context_text": "We employ DIV2K [1], Flickr2K [1] and OutdoorSceneTraining [40] datasets to implement our task grouping algorithm and train the TGSR network.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions specific datasets used for training a network for image restoration, which aligns with the research topic.",
          "citing_paper_doi": null,
          "cited_paper_doi": "10.1109/CVPRW.2017.150",
          "citing_paper_url": "https://www.semanticscholar.org/paper/791a58a22c56c4e135fa8ef8801b235e3f6a648f",
          "cited_paper_url": "https://www.semanticscholar.org/paper/6aeb583c084ad2eb709154ec91f22ca8d6de4425",
          "citing_paper_year": 2023,
          "cited_paper_year": 2017
        },
        {
          "dataset_name": "DIV2K",
          "dataset_description": "Used for fine-tuning DepictQA on degradation evaluation, providing a diverse set of images for robust model training. | Used for fine-tuning DepictQA on degradation evaluation, focusing on high-resolution image restoration and quality assessment.",
          "citing_paper_id": "273532643",
          "cited_paper_id": 4493958,
          "context_text": "T UNING VLM DIV2K (Agustsson & Timofte, 2017) and Flickr2K (Timofte et al., 2017) datasets are used for fine-tuning DepictQA (You et al., 2024a) on degradation evaluation.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions two specific datasets, DIV2K and Flickr2K, which are used for fine-tuning a model called DepictQA on degradation evaluation.",
          "citing_paper_doi": "10.48550/arXiv.2410.17809",
          "cited_paper_doi": "10.1109/CVPRW.2017.150",
          "citing_paper_url": "https://www.semanticscholar.org/paper/b13aaf8bd57e10823a1e5374750fafd0699ef8f7",
          "cited_paper_url": "https://www.semanticscholar.org/paper/6aeb583c084ad2eb709154ec91f22ca8d6de4425",
          "citing_paper_year": 2024,
          "cited_paper_year": 2017
        },
        {
          "dataset_name": "DIV2K",
          "dataset_description": "Used as a synthetic image denoising dataset, providing high-quality images for training and evaluating denoising algorithms. | Used for training AutoDIR for super-resolution, focusing on enhancing image resolution at x8 and x4 scales. | Used as a synthetic image denoising dataset, complementing DIV2K with additional diverse images for comprehensive model training. | Used as a real image denoising training dataset, focusing on enhancing the model's ability to handle real-world noise patterns.",
          "citing_paper_id": "264145824",
          "cited_paper_id": 6540453,
          "context_text": "For super-resolution, we follow previous practice and train AutoDIR upon DIV2K [2] and Flickr2K [32] training sets, with jointly downsampled images at x8 and x4 scales as inputs.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions specific datasets used for training a model for super-resolution, which are relevant to the topic of image restoration.",
          "citing_paper_doi": "10.48550/arXiv.2310.10123",
          "cited_paper_doi": "10.1109/CVPRW.2017.151",
          "citing_paper_url": "https://www.semanticscholar.org/paper/8efc22d25fc33d73aa8de1044e5afe775ed87e31",
          "cited_paper_url": "https://www.semanticscholar.org/paper/7ba5d3808e117e7a68dc40331ce1d483ceeedcb2",
          "citing_paper_year": 2023,
          "cited_paper_year": 2017
        },
        {
          "dataset_name": "DIV2K",
          "dataset_description": "Used for training image restoration models, focusing on high-resolution image generation and enhancement. | Used for training image restoration models, providing a diverse set of images for general-purpose image quality improvement. | Used for training image restoration models, offering a large number of high-resolution images for super-resolution tasks. | Used for training image restoration models, specifically focusing on face images to improve facial detail and clarity. | Used to evaluate inpainting performance, focusing on high-resolution image restoration tasks.",
          "citing_paper_id": "259137590",
          "cited_paper_id": 4493958,
          "context_text": "Datasets Our training dataset includes images from DIV2K train set [1], Flickr2K [22], DIV8K train set [12], and 10,000 face images from FFHQ [18].",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context explicitly mentions multiple image datasets used for training. These datasets are specific and verifiable, and their usage is clearly described.",
          "citing_paper_doi": "10.1109/ICCV51070.2023.00495",
          "cited_paper_doi": "10.1109/CVPRW.2017.150",
          "citing_paper_url": "https://www.semanticscholar.org/paper/81c739551f9122f5dc5ddf78900c577e716ad49a",
          "cited_paper_url": "https://www.semanticscholar.org/paper/6aeb583c084ad2eb709154ec91f22ca8d6de4425",
          "citing_paper_year": 2023,
          "cited_paper_year": 2017
        },
        {
          "dataset_name": "DIV2K",
          "dataset_description": "Used to select high-quality image patches for single image super-resolution, focusing on enhancing image resolution and quality.",
          "citing_paper_id": "227238849",
          "cited_paper_id": 4493958,
          "context_text": "We select 250 image patches from two high-quality image datasets (DIV2K (Agustsson and Timofte, 2017) and Flickr2K (Timofte et al.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions two specific image datasets, DIV2K and Flickr2K, which are used to select image patches for the research.",
          "citing_paper_doi": null,
          "cited_paper_doi": "10.1109/CVPRW.2017.150",
          "citing_paper_url": "https://www.semanticscholar.org/paper/f942a4a56e6549c83844747ad6c4ae58000b2988",
          "cited_paper_url": "https://www.semanticscholar.org/paper/6aeb583c084ad2eb709154ec91f22ca8d6de4425",
          "citing_paper_year": 2020,
          "cited_paper_year": 2017
        }
      ]
    },
    {
      "cited_paper_id": "4710341",
      "citation_count": 0,
      "total_dataset_mentions": 38,
      "unique_datasets": [
        "Set5"
      ],
      "dataset_details": [
        {
          "dataset_name": "Set5",
          "dataset_description": "Used to evaluate super-resolution performance on high-quality and low-quality query textures in SR (× 4) scenarios, focusing on urban scenes. | Used to evaluate super-resolution performance on high-quality and low-quality query textures in SR (× 4) scenarios, focusing on manga images. | Used to evaluate the DTFN model on single-image super-resolution, focusing on high-quality image restoration and detail enhancement. | Used to evaluate the DTFN model on single-image super-resolution, focusing on high-quality image restoration and detail enhancement in manga images.",
          "citing_paper_id": "272552320",
          "cited_paper_id": 64193,
          "context_text": "We then evaluat our DTFN on ﬁve widely used SISR benchmarks: Set5 [2], Set14 [51], B100 [29], Urban100 [16], and Manga109 [30].",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions five specific benchmarks used for evaluating a DTFN model on single-image super-resolution tasks. These benchmarks are clearly identified and are relevant to the research topic of image restoration.",
          "citing_paper_doi": "10.1109/TPAMI.2024.3457790",
          "cited_paper_doi": "10.1109/ICCV.2001.937655",
          "citing_paper_url": "https://www.semanticscholar.org/paper/e80e3946f27f380e34b4f431d70de6274e74e000",
          "cited_paper_url": "https://www.semanticscholar.org/paper/9a1ed876196ec9733acb1daa6d65e35ff0414291",
          "citing_paper_year": 2024,
          "cited_paper_year": 2001
        },
        {
          "dataset_name": "Set5",
          "dataset_description": "Used for assessing the effectiveness of image restoration techniques, particularly in handling various types of image degradation. | Used to evaluate single image scale-up performance, specifically measuring the average PSNR for image restoration quality. | Used to evaluate single image scale-up performance, focusing on PSNR results and convergence trends. | Used to assess the performance of image restoration methods on manga images, specifically targeting the preservation of line art and color fidelity. | Used to test the ability of image restoration models to handle complex urban scenes, emphasizing detail preservation and texture recovery. | Used for evaluating the generalization capabilities of image restoration algorithms across diverse natural images, focusing on edge and structure preservation. | Used for evaluating single-image super-resolution methods, focusing on high-quality image restoration and upscaling performance.",
          "citing_paper_id": "222141081",
          "cited_paper_id": 2356330,
          "context_text": "Set5 [4], Set14 [34], Urban100 [19], BSD100 [24] and Manga109 [25].",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions specific datasets used for image restoration tasks, which are directly relevant to the research topic.",
          "citing_paper_doi": null,
          "cited_paper_doi": "10.1007/978-3-642-27413-8_47",
          "citing_paper_url": "https://www.semanticscholar.org/paper/c8230ea819dc5774b0fce1766b9a5c094bb1106c",
          "cited_paper_url": "https://www.semanticscholar.org/paper/d1dce9ad419d8c7037980a9ea28506ed0e640bba",
          "citing_paper_year": 2020,
          "cited_paper_year": 2010
        },
        {
          "dataset_name": "Set5",
          "dataset_description": "Used as the training dataset for single image super-resolution with a scale factor of ×2, focusing on improving PSNR values through iterative training. | Used to evaluate the performance of FSRCNN, EDSR, and RCAN models, focusing on image super-resolution quality improvements. | Used to evaluate the DTFN model on single-image super-resolution, focusing on high-quality image restoration and detail enhancement. | Used to evaluate the DTFN model on single-image super-resolution, focusing on high-quality image restoration and detail enhancement in manga images. | Used for evaluation, reporting the highest PSNR (dB) values after 50,000 iterations, to assess the performance of the trained model.",
          "citing_paper_id": "272552320",
          "cited_paper_id": 5250573,
          "context_text": "We then evaluat our DTFN on ﬁve widely used SISR benchmarks: Set5 [2], Set14 [51], B100 [29], Urban100 [16], and Manga109 [30].",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions five specific benchmarks used for evaluating a DTFN model on single-image super-resolution tasks. These benchmarks are clearly identified and are relevant to the research topic of image restoration.",
          "citing_paper_doi": "10.1109/TPAMI.2024.3457790",
          "cited_paper_doi": "10.5244/C.26.135",
          "citing_paper_url": "https://www.semanticscholar.org/paper/e80e3946f27f380e34b4f431d70de6274e74e000",
          "cited_paper_url": "https://www.semanticscholar.org/paper/c7338140c446bb1b8c1a2c24aa380890214dc0d7",
          "citing_paper_year": 2024,
          "cited_paper_year": 2012
        },
        {
          "dataset_name": "Set5",
          "dataset_description": "Used for assessing the effectiveness of image restoration techniques, particularly in handling various types of image degradation. | Used to assess the performance of image restoration methods on manga images, specifically targeting the preservation of line art and color fidelity. | Used to test the ability of image restoration models to handle complex urban scenes, emphasizing detail preservation and texture recovery. | Used for evaluating the generalization capabilities of image restoration algorithms across diverse natural images, focusing on edge and structure preservation. | Used for evaluating single-image super-resolution methods, focusing on high-quality image restoration and upscaling performance.",
          "citing_paper_id": "222141081",
          "cited_paper_id": 5250573,
          "context_text": "Set5 [4], Set14 [34], Urban100 [19], BSD100 [24] and Manga109 [25].",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions specific datasets used for image restoration tasks, which are directly relevant to the research topic.",
          "citing_paper_doi": null,
          "cited_paper_doi": "10.5244/C.26.135",
          "citing_paper_url": "https://www.semanticscholar.org/paper/c8230ea819dc5774b0fce1766b9a5c094bb1106c",
          "cited_paper_url": "https://www.semanticscholar.org/paper/c7338140c446bb1b8c1a2c24aa380890214dc0d7",
          "citing_paper_year": 2020,
          "cited_paper_year": 2012
        },
        {
          "dataset_name": "Set5",
          "dataset_description": "Used to demonstrate the effectiveness of the Texture-Fidelity Strategy (TFS) for super-resolution at a scale of 4x, focusing on texture fidelity in manga images. | Used to demonstrate the effectiveness of the Texture-Fidelity Strategy (TFS) for super-resolution at a scale of 4x, focusing on texture fidelity in urban scenes. | Used to evaluate super-resolution performance on high-quality and low-quality query textures in SR (× 4) scenarios, focusing on urban scenes. | Used to evaluate the DTFN model on single-image super-resolution, focusing on high-quality image restoration and detail enhancement. | Used to evaluate super-resolution performance on high-quality and low-quality query textures in SR (× 4) scenarios, focusing on manga images. | Used to evaluate the DTFN model on single-image super-resolution, focusing on high-quality image restoration and detail enhancement in manga images.",
          "citing_paper_id": "272552320",
          "cited_paper_id": 8887614,
          "context_text": "We then evaluat our DTFN on ﬁve widely used SISR benchmarks: Set5 [2], Set14 [51], B100 [29], Urban100 [16], and Manga109 [30].",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions five specific benchmarks used for evaluating a DTFN model on single-image super-resolution tasks. These benchmarks are clearly identified and are relevant to the research topic of image restoration.",
          "citing_paper_doi": "10.1109/TPAMI.2024.3457790",
          "cited_paper_doi": "10.1007/s11042-016-4020-z",
          "citing_paper_url": "https://www.semanticscholar.org/paper/e80e3946f27f380e34b4f431d70de6274e74e000",
          "cited_paper_url": "https://www.semanticscholar.org/paper/1fef45786e707e6b9b8517b0403e596ecbdea6a5",
          "citing_paper_year": 2024,
          "cited_paper_year": 2015
        },
        {
          "dataset_name": "Set5",
          "dataset_description": "Used to evaluate image restoration performance with a scale factor of ×4, comparing PSNR improvements over SwinIR.",
          "citing_paper_id": "272552320",
          "cited_paper_id": 237266491,
          "context_text": "For example, on Set5, Set14, B100, Urban100 and Manga109 datasets with scale factor × 4 , our DTFN improves the PSNR by 0.13 dB, 0.11 dB, 0.05 dB, 0.31 dB and 0.27 dB respectively over the impressive SwinIR [23].",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions multiple datasets used for evaluating image restoration performance. These datasets are specific and have clear identifiers.",
          "citing_paper_doi": "10.1109/TPAMI.2024.3457790",
          "cited_paper_doi": "10.1109/ICCVW54120.2021.00210",
          "citing_paper_url": "https://www.semanticscholar.org/paper/e80e3946f27f380e34b4f431d70de6274e74e000",
          "cited_paper_url": "https://www.semanticscholar.org/paper/7a9a708ca61c14886aa0dcd6d13dac7879713f5f",
          "citing_paper_year": 2024,
          "cited_paper_year": 2021
        },
        {
          "dataset_name": "Set5",
          "dataset_description": "Used to evaluate image super-resolution methods, focusing on PSNR and SSIM metrics at different scaling factors. | Used to evaluate single image super-resolution methods, focusing on PSNR and SSIM metrics for image quality assessment. | Used for evaluating single image scale-up methods, focusing on super-resolution performance metrics like PSNR and SSIM.",
          "citing_paper_id": "237266491",
          "cited_paper_id": 2356330,
          "context_text": "Method Scale #Params #Mult-Adds Set5 [3] Set14 [87] BSD100 [58] Urban100 [34] Manga109 [60]PSNR SSIM PSNR SSIM PSNR SSIM PSNR SSIM PSNR SSIM CARN [2] ×2 1,592K 222.8G 37.76 0.9590 33.52 0.9166 32.09 0.8978 31.92 0.9256 38.36 0.9765 FALSR-A [12] ×2 1,021K 234.7G 37.82 0.959 33.55 0.9168 32.1 0.8987 31.93 0.9256 - - IMDN [35] ×2 694K 158.8G 38.00 0.9605 33.63 0.9177 32.19 0.8996 32.17 0.9283 38.88 0.9774 LAPAR-A [44] ×2 548K 171.0G 38.01 0.9605 33.62 0.9183 32.19 0.8999 32.10 0.9283 38.67 0.9772 LatticeNet [57] ×2 756K 169.5G 38.15 0.9610 33.78 0.9193 32.25 0.9005 32.43 0.9302 - - SwinIR (Ours) ×2 878K 195.6G 38.14 0.9611 33.86 0.9206 32.31 0.9012 32.76 0.9340 39.12 0.9783 CARN [2] ×3 1,592K 118.8G 34.29 0.9255 30.29 0.8407 29.06 0.8034 28.06 0.8493 33.50 0.9440 IMDN [35] ×3 703K 71.5G 34.36 0.9270 30.32 0.8417 29.09 0.8046 28.17 0.8519 33.61 0.9445 LAPAR-A [44] ×3 544K 114.0G 34.36 0.9267 30.34 0.8421 29.11 0.8054 28.15 0.8523 33.51 0.9441 LatticeNet [57] ×3 765K 76.3G 34.53 0.9281 30.39 0.8424 29.15 0.8059 28.33 0.8538 - - SwinIR (Ours) ×3 886K 87.2G 34.62 0.9289 30.54 0.8463 29.20 0.8082 28.66 0.8624 33.98 0.9478 CARN [2] ×4 1,592K 90.9G 32.13 0.8937 28.60 0.7806 27.58 0.7349 26.07 0.7837 30.47 0.9084 IMDN [35] ×4 715K 40.9G 32.21 0.8948 28.58 0.7811 27.56 0.7353 26.04 0.7838 30.45 0.9075 LAPAR-A [44] ×4 659K 94.0G 32.15 0.8944 28.61 0.7818 27.61 0.7366 26.14 0.7871 30.42 0.9074 LatticeNet [57] ×4 777K 43.6G 32.30 0.8962 28.68 0.7830 27.62 0.7367 26.25 0.7873 - - SwinIR (Ours) ×4 897K 49.6G 32.44 0.8976 28.77 0.7858 27.69 0.7406 26.47 0.7980 30.92 0.9151\nTable 4: Quantitative comparison (average PSNR/SSIM/PSNR-B) with state-of-the-art methods for JPEG compression artifact reduction on benchmark datasets.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions several benchmark datasets used for evaluating image super-resolution methods. These datasets are commonly used in the field of image restoration.",
          "citing_paper_doi": "10.1109/ICCVW54120.2021.00210",
          "cited_paper_doi": "10.1007/978-3-642-27413-8_47",
          "citing_paper_url": "https://www.semanticscholar.org/paper/7a9a708ca61c14886aa0dcd6d13dac7879713f5f",
          "cited_paper_url": "https://www.semanticscholar.org/paper/d1dce9ad419d8c7037980a9ea28506ed0e640bba",
          "citing_paper_year": 2021,
          "cited_paper_year": 2010
        },
        {
          "dataset_name": "Set5",
          "dataset_description": "Used to evaluate image super-resolution methods, focusing on PSNR and SSIM metrics at different scaling factors.",
          "citing_paper_id": "237266491",
          "cited_paper_id": 4710341,
          "context_text": "Method Scale #Params #Mult-Adds Set5 [3] Set14 [87] BSD100 [58] Urban100 [34] Manga109 [60]PSNR SSIM PSNR SSIM PSNR SSIM PSNR SSIM PSNR SSIM CARN [2] ×2 1,592K 222.8G 37.76 0.9590 33.52 0.9166 32.09 0.8978 31.92 0.9256 38.36 0.9765 FALSR-A [12] ×2 1,021K 234.7G 37.82 0.959 33.55 0.9168 32.1 0.8987 31.93 0.9256 - - IMDN [35] ×2 694K 158.8G 38.00 0.9605 33.63 0.9177 32.19 0.8996 32.17 0.9283 38.88 0.9774 LAPAR-A [44] ×2 548K 171.0G 38.01 0.9605 33.62 0.9183 32.19 0.8999 32.10 0.9283 38.67 0.9772 LatticeNet [57] ×2 756K 169.5G 38.15 0.9610 33.78 0.9193 32.25 0.9005 32.43 0.9302 - - SwinIR (Ours) ×2 878K 195.6G 38.14 0.9611 33.86 0.9206 32.31 0.9012 32.76 0.9340 39.12 0.9783 CARN [2] ×3 1,592K 118.8G 34.29 0.9255 30.29 0.8407 29.06 0.8034 28.06 0.8493 33.50 0.9440 IMDN [35] ×3 703K 71.5G 34.36 0.9270 30.32 0.8417 29.09 0.8046 28.17 0.8519 33.61 0.9445 LAPAR-A [44] ×3 544K 114.0G 34.36 0.9267 30.34 0.8421 29.11 0.8054 28.15 0.8523 33.51 0.9441 LatticeNet [57] ×3 765K 76.3G 34.53 0.9281 30.39 0.8424 29.15 0.8059 28.33 0.8538 - - SwinIR (Ours) ×3 886K 87.2G 34.62 0.9289 30.54 0.8463 29.20 0.8082 28.66 0.8624 33.98 0.9478 CARN [2] ×4 1,592K 90.9G 32.13 0.8937 28.60 0.7806 27.58 0.7349 26.07 0.7837 30.47 0.9084 IMDN [35] ×4 715K 40.9G 32.21 0.8948 28.58 0.7811 27.56 0.7353 26.04 0.7838 30.45 0.9075 LAPAR-A [44] ×4 659K 94.0G 32.15 0.8944 28.61 0.7818 27.61 0.7366 26.14 0.7871 30.42 0.9074 LatticeNet [57] ×4 777K 43.6G 32.30 0.8962 28.68 0.7830 27.62 0.7367 26.25 0.7873 - - SwinIR (Ours) ×4 897K 49.6G 32.44 0.8976 28.77 0.7858 27.69 0.7406 26.47 0.7980 30.92 0.9151\nTable 4: Quantitative comparison (average PSNR/SSIM/PSNR-B) with state-of-the-art methods for JPEG compression artifact reduction on benchmark datasets.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions several benchmark datasets used for evaluating image super-resolution methods. These datasets are commonly used in the field of image restoration.",
          "citing_paper_doi": "10.1109/ICCVW54120.2021.00210",
          "cited_paper_doi": "10.1007/978-3-030-01249-6_16",
          "citing_paper_url": "https://www.semanticscholar.org/paper/7a9a708ca61c14886aa0dcd6d13dac7879713f5f",
          "cited_paper_url": "https://www.semanticscholar.org/paper/bef8694328016889a4b87761984046e1d9cf79b9",
          "citing_paper_year": 2021,
          "cited_paper_year": 2018
        },
        {
          "dataset_name": "Set5",
          "dataset_description": "Used to evaluate image super-resolution methods, focusing on PSNR and SSIM metrics at different scaling factors. | Used to evaluate the performance of various image super-resolution methods, focusing on PSNR results and the impact of model complexity. | Used to evaluate super-resolution methods, specifically focusing on the performance of different algorithms in increasing image resolution by a factor of 4. | Used to evaluate single image super-resolution methods, focusing on PSNR and SSIM metrics for image quality assessment. | Used for evaluating single image scale-up methods, focusing on super-resolution performance metrics like PSNR and SSIM.",
          "citing_paper_id": "237266491",
          "cited_paper_id": 5250573,
          "context_text": "Method Scale #Params #Mult-Adds Set5 [3] Set14 [87] BSD100 [58] Urban100 [34] Manga109 [60]PSNR SSIM PSNR SSIM PSNR SSIM PSNR SSIM PSNR SSIM CARN [2] ×2 1,592K 222.8G 37.76 0.9590 33.52 0.9166 32.09 0.8978 31.92 0.9256 38.36 0.9765 FALSR-A [12] ×2 1,021K 234.7G 37.82 0.959 33.55 0.9168 32.1 0.8987 31.93 0.9256 - - IMDN [35] ×2 694K 158.8G 38.00 0.9605 33.63 0.9177 32.19 0.8996 32.17 0.9283 38.88 0.9774 LAPAR-A [44] ×2 548K 171.0G 38.01 0.9605 33.62 0.9183 32.19 0.8999 32.10 0.9283 38.67 0.9772 LatticeNet [57] ×2 756K 169.5G 38.15 0.9610 33.78 0.9193 32.25 0.9005 32.43 0.9302 - - SwinIR (Ours) ×2 878K 195.6G 38.14 0.9611 33.86 0.9206 32.31 0.9012 32.76 0.9340 39.12 0.9783 CARN [2] ×3 1,592K 118.8G 34.29 0.9255 30.29 0.8407 29.06 0.8034 28.06 0.8493 33.50 0.9440 IMDN [35] ×3 703K 71.5G 34.36 0.9270 30.32 0.8417 29.09 0.8046 28.17 0.8519 33.61 0.9445 LAPAR-A [44] ×3 544K 114.0G 34.36 0.9267 30.34 0.8421 29.11 0.8054 28.15 0.8523 33.51 0.9441 LatticeNet [57] ×3 765K 76.3G 34.53 0.9281 30.39 0.8424 29.15 0.8059 28.33 0.8538 - - SwinIR (Ours) ×3 886K 87.2G 34.62 0.9289 30.54 0.8463 29.20 0.8082 28.66 0.8624 33.98 0.9478 CARN [2] ×4 1,592K 90.9G 32.13 0.8937 28.60 0.7806 27.58 0.7349 26.07 0.7837 30.47 0.9084 IMDN [35] ×4 715K 40.9G 32.21 0.8948 28.58 0.7811 27.56 0.7353 26.04 0.7838 30.45 0.9075 LAPAR-A [44] ×4 659K 94.0G 32.15 0.8944 28.61 0.7818 27.61 0.7366 26.14 0.7871 30.42 0.9074 LatticeNet [57] ×4 777K 43.6G 32.30 0.8962 28.68 0.7830 27.62 0.7367 26.25 0.7873 - - SwinIR (Ours) ×4 897K 49.6G 32.44 0.8976 28.77 0.7858 27.69 0.7406 26.47 0.7980 30.92 0.9151\nTable 4: Quantitative comparison (average PSNR/SSIM/PSNR-B) with state-of-the-art methods for JPEG compression artifact reduction on benchmark datasets.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions several benchmark datasets used for evaluating image super-resolution methods. These datasets are commonly used in the field of image restoration.",
          "citing_paper_doi": "10.1109/ICCVW54120.2021.00210",
          "cited_paper_doi": "10.5244/C.26.135",
          "citing_paper_url": "https://www.semanticscholar.org/paper/7a9a708ca61c14886aa0dcd6d13dac7879713f5f",
          "cited_paper_url": "https://www.semanticscholar.org/paper/c7338140c446bb1b8c1a2c24aa380890214dc0d7",
          "citing_paper_year": 2021,
          "cited_paper_year": 2012
        },
        {
          "dataset_name": "Set5",
          "dataset_description": "Used to evaluate image super-resolution methods, focusing on PSNR and SSIM metrics at different scaling factors.",
          "citing_paper_id": "237266491",
          "cited_paper_id": 202889117,
          "context_text": "Method Scale #Params #Mult-Adds Set5 [3] Set14 [87] BSD100 [58] Urban100 [34] Manga109 [60]PSNR SSIM PSNR SSIM PSNR SSIM PSNR SSIM PSNR SSIM CARN [2] ×2 1,592K 222.8G 37.76 0.9590 33.52 0.9166 32.09 0.8978 31.92 0.9256 38.36 0.9765 FALSR-A [12] ×2 1,021K 234.7G 37.82 0.959 33.55 0.9168 32.1 0.8987 31.93 0.9256 - - IMDN [35] ×2 694K 158.8G 38.00 0.9605 33.63 0.9177 32.19 0.8996 32.17 0.9283 38.88 0.9774 LAPAR-A [44] ×2 548K 171.0G 38.01 0.9605 33.62 0.9183 32.19 0.8999 32.10 0.9283 38.67 0.9772 LatticeNet [57] ×2 756K 169.5G 38.15 0.9610 33.78 0.9193 32.25 0.9005 32.43 0.9302 - - SwinIR (Ours) ×2 878K 195.6G 38.14 0.9611 33.86 0.9206 32.31 0.9012 32.76 0.9340 39.12 0.9783 CARN [2] ×3 1,592K 118.8G 34.29 0.9255 30.29 0.8407 29.06 0.8034 28.06 0.8493 33.50 0.9440 IMDN [35] ×3 703K 71.5G 34.36 0.9270 30.32 0.8417 29.09 0.8046 28.17 0.8519 33.61 0.9445 LAPAR-A [44] ×3 544K 114.0G 34.36 0.9267 30.34 0.8421 29.11 0.8054 28.15 0.8523 33.51 0.9441 LatticeNet [57] ×3 765K 76.3G 34.53 0.9281 30.39 0.8424 29.15 0.8059 28.33 0.8538 - - SwinIR (Ours) ×3 886K 87.2G 34.62 0.9289 30.54 0.8463 29.20 0.8082 28.66 0.8624 33.98 0.9478 CARN [2] ×4 1,592K 90.9G 32.13 0.8937 28.60 0.7806 27.58 0.7349 26.07 0.7837 30.47 0.9084 IMDN [35] ×4 715K 40.9G 32.21 0.8948 28.58 0.7811 27.56 0.7353 26.04 0.7838 30.45 0.9075 LAPAR-A [44] ×4 659K 94.0G 32.15 0.8944 28.61 0.7818 27.61 0.7366 26.14 0.7871 30.42 0.9074 LatticeNet [57] ×4 777K 43.6G 32.30 0.8962 28.68 0.7830 27.62 0.7367 26.25 0.7873 - - SwinIR (Ours) ×4 897K 49.6G 32.44 0.8976 28.77 0.7858 27.69 0.7406 26.47 0.7980 30.92 0.9151\nTable 4: Quantitative comparison (average PSNR/SSIM/PSNR-B) with state-of-the-art methods for JPEG compression artifact reduction on benchmark datasets.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions several benchmark datasets used for evaluating image super-resolution methods. These datasets are commonly used in the field of image restoration.",
          "citing_paper_doi": "10.1109/ICCVW54120.2021.00210",
          "cited_paper_doi": "10.1145/3343031.3351084",
          "citing_paper_url": "https://www.semanticscholar.org/paper/7a9a708ca61c14886aa0dcd6d13dac7879713f5f",
          "cited_paper_url": "https://www.semanticscholar.org/paper/ce105188e4e1fac292769306c9800949720fc59c",
          "citing_paper_year": 2021,
          "cited_paper_year": 2019
        },
        {
          "dataset_name": "Set5",
          "dataset_description": "Used to evaluate image super-resolution methods, focusing on PSNR and SSIM metrics at different scaling factors.",
          "citing_paper_id": "237266491",
          "cited_paper_id": 227275677,
          "context_text": "Method Scale #Params #Mult-Adds Set5 [3] Set14 [87] BSD100 [58] Urban100 [34] Manga109 [60]PSNR SSIM PSNR SSIM PSNR SSIM PSNR SSIM PSNR SSIM CARN [2] ×2 1,592K 222.8G 37.76 0.9590 33.52 0.9166 32.09 0.8978 31.92 0.9256 38.36 0.9765 FALSR-A [12] ×2 1,021K 234.7G 37.82 0.959 33.55 0.9168 32.1 0.8987 31.93 0.9256 - - IMDN [35] ×2 694K 158.8G 38.00 0.9605 33.63 0.9177 32.19 0.8996 32.17 0.9283 38.88 0.9774 LAPAR-A [44] ×2 548K 171.0G 38.01 0.9605 33.62 0.9183 32.19 0.8999 32.10 0.9283 38.67 0.9772 LatticeNet [57] ×2 756K 169.5G 38.15 0.9610 33.78 0.9193 32.25 0.9005 32.43 0.9302 - - SwinIR (Ours) ×2 878K 195.6G 38.14 0.9611 33.86 0.9206 32.31 0.9012 32.76 0.9340 39.12 0.9783 CARN [2] ×3 1,592K 118.8G 34.29 0.9255 30.29 0.8407 29.06 0.8034 28.06 0.8493 33.50 0.9440 IMDN [35] ×3 703K 71.5G 34.36 0.9270 30.32 0.8417 29.09 0.8046 28.17 0.8519 33.61 0.9445 LAPAR-A [44] ×3 544K 114.0G 34.36 0.9267 30.34 0.8421 29.11 0.8054 28.15 0.8523 33.51 0.9441 LatticeNet [57] ×3 765K 76.3G 34.53 0.9281 30.39 0.8424 29.15 0.8059 28.33 0.8538 - - SwinIR (Ours) ×3 886K 87.2G 34.62 0.9289 30.54 0.8463 29.20 0.8082 28.66 0.8624 33.98 0.9478 CARN [2] ×4 1,592K 90.9G 32.13 0.8937 28.60 0.7806 27.58 0.7349 26.07 0.7837 30.47 0.9084 IMDN [35] ×4 715K 40.9G 32.21 0.8948 28.58 0.7811 27.56 0.7353 26.04 0.7838 30.45 0.9075 LAPAR-A [44] ×4 659K 94.0G 32.15 0.8944 28.61 0.7818 27.61 0.7366 26.14 0.7871 30.42 0.9074 LatticeNet [57] ×4 777K 43.6G 32.30 0.8962 28.68 0.7830 27.62 0.7367 26.25 0.7873 - - SwinIR (Ours) ×4 897K 49.6G 32.44 0.8976 28.77 0.7858 27.69 0.7406 26.47 0.7980 30.92 0.9151\nTable 4: Quantitative comparison (average PSNR/SSIM/PSNR-B) with state-of-the-art methods for JPEG compression artifact reduction on benchmark datasets.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions several benchmark datasets used for evaluating image super-resolution methods. These datasets are commonly used in the field of image restoration.",
          "citing_paper_doi": "10.1109/ICCVW54120.2021.00210",
          "cited_paper_doi": null,
          "citing_paper_url": "https://www.semanticscholar.org/paper/7a9a708ca61c14886aa0dcd6d13dac7879713f5f",
          "cited_paper_url": "https://www.semanticscholar.org/paper/0b78d81b735fd5449d04c561d9898472e33d7e08",
          "citing_paper_year": 2021,
          "cited_paper_year": 2021
        },
        {
          "dataset_name": "Set5",
          "dataset_description": "Used to evaluate the performance of the proposed method, specifically comparing the impact of placing AdaFM layers before and after activation in image super-resolution tasks. | Used to evaluate super-resolution models, focusing on RGB channel training and y-channel PSNR calculation on the test set. | Used to evaluate the performance of models with AdaFM layers, specifically comparing the impact of layer placement on image super-resolution quality. | Used to evaluate super-resolution models, focusing on RGB channel training and y-channel PSNR calculation. | Used to evaluate the performance of the proposed single-image super-resolution method, focusing on convergence and quantitative results.",
          "citing_paper_id": "119308964",
          "cited_paper_id": 5250573,
          "context_text": "The convergence curves on Set5 are plotted in Figure 5 , and the quantitative results are presented in Table 2.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions 'Set5', which is a known dataset in image processing and computer vision, particularly for super-resolution tasks.",
          "citing_paper_doi": "10.1109/CVPR.2019.01131",
          "cited_paper_doi": "10.5244/C.26.135",
          "citing_paper_url": "https://www.semanticscholar.org/paper/e6ab7b7fee9a25342ef1cac082c77a7f1021a982",
          "cited_paper_url": "https://www.semanticscholar.org/paper/c7338140c446bb1b8c1a2c24aa380890214dc0d7",
          "citing_paper_year": 2019,
          "cited_paper_year": 2012
        },
        {
          "dataset_name": "Set5",
          "dataset_description": "Used to evaluate single image super-resolution methods, specifically comparing VDSR, EDSR, RDN, and OISR models at 4x scaling factor. | Used to evaluate the performance of various image super-resolution methods, focusing on PSNR results and the impact of model complexity.",
          "citing_paper_id": "237266491",
          "cited_paper_id": 6540453,
          "context_text": "0.2 0.4 0.6 0.8 1.0 1.2 Number of Parameters 1e8\n32.45\n32.50\n32.55\n32.60\n32.65\n32.70\nPS NR\n(d B)\nEDSR (CVPR2017)\nRNAN (ICLR2019)\nOISR (CVPR2019)\nRDN (CVPR2018)\nRCAN (ECCV2018)\nIGNN (NeurIPS2020)\nHAN (ECCV2020)\nNLSA (CVPR2021)\nIPT (CVPR2021)\nSwinIR (ours)\nFigure 1: PSNR results v.s the total number of parameters of different methods for image SR (×4) on Set5 [3].\nods [73, 14, 28], they generally suffer from two basic problems that stem from the basic building block, i.e., the convolution layer.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions 'Set5' as a dataset used for evaluating image super-resolution methods. No other specific datasets are mentioned.",
          "citing_paper_doi": "10.1109/ICCVW54120.2021.00210",
          "cited_paper_doi": "10.1109/CVPRW.2017.151",
          "citing_paper_url": "https://www.semanticscholar.org/paper/7a9a708ca61c14886aa0dcd6d13dac7879713f5f",
          "cited_paper_url": "https://www.semanticscholar.org/paper/7ba5d3808e117e7a68dc40331ce1d483ceeedcb2",
          "citing_paper_year": 2021,
          "cited_paper_year": 2017
        },
        {
          "dataset_name": "Set5",
          "dataset_description": "Used to evaluate image restoration models, specifically comparing performance metrics across various noise levels and models. | Used to evaluate the performance of various image super-resolution methods, focusing on PSNR results and the impact of model complexity. | Used to test image restoration methods at JPEG quality factors 10, 20, 30, and 40, focusing on performance evaluation under compression.",
          "citing_paper_id": "237266491",
          "cited_paper_id": 57189262,
          "context_text": "0.2 0.4 0.6 0.8 1.0 1.2 Number of Parameters 1e8\n32.45\n32.50\n32.55\n32.60\n32.65\n32.70\nPS NR\n(d B)\nEDSR (CVPR2017)\nRNAN (ICLR2019)\nOISR (CVPR2019)\nRDN (CVPR2018)\nRCAN (ECCV2018)\nIGNN (NeurIPS2020)\nHAN (ECCV2020)\nNLSA (CVPR2021)\nIPT (CVPR2021)\nSwinIR (ours)\nFigure 1: PSNR results v.s the total number of parameters of different methods for image SR (×4) on Set5 [3].\nods [73, 14, 28], they generally suffer from two basic problems that stem from the basic building block, i.e., the convolution layer.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions 'Set5' as a dataset used for evaluating image super-resolution methods. No other specific datasets are mentioned.",
          "citing_paper_doi": "10.1109/ICCVW54120.2021.00210",
          "cited_paper_doi": "10.1109/TPAMI.2020.2968521",
          "citing_paper_url": "https://www.semanticscholar.org/paper/7a9a708ca61c14886aa0dcd6d13dac7879713f5f",
          "cited_paper_url": "https://www.semanticscholar.org/paper/7a4c5dc91af23805f4f6ecea30ca6e5d085e2108",
          "citing_paper_year": 2021,
          "cited_paper_year": 2018
        },
        {
          "dataset_name": "Set5",
          "dataset_description": "Used to evaluate single image super-resolution methods, specifically comparing VDSR, EDSR, RDN, and OISR models at 4x scaling factor. | Used to evaluate the performance of various image super-resolution methods, focusing on PSNR results and the impact of model complexity.",
          "citing_paper_id": "237266491",
          "cited_paper_id": 192571596,
          "context_text": "0.2 0.4 0.6 0.8 1.0 1.2 Number of Parameters 1e8\n32.45\n32.50\n32.55\n32.60\n32.65\n32.70\nPS NR\n(d B)\nEDSR (CVPR2017)\nRNAN (ICLR2019)\nOISR (CVPR2019)\nRDN (CVPR2018)\nRCAN (ECCV2018)\nIGNN (NeurIPS2020)\nHAN (ECCV2020)\nNLSA (CVPR2021)\nIPT (CVPR2021)\nSwinIR (ours)\nFigure 1: PSNR results v.s the total number of parameters of different methods for image SR (×4) on Set5 [3].\nods [73, 14, 28], they generally suffer from two basic problems that stem from the basic building block, i.e., the convolution layer.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions 'Set5' as a dataset used for evaluating image super-resolution methods. No other specific datasets are mentioned.",
          "citing_paper_doi": "10.1109/ICCVW54120.2021.00210",
          "cited_paper_doi": "10.1109/CVPR.2019.00183",
          "citing_paper_url": "https://www.semanticscholar.org/paper/7a9a708ca61c14886aa0dcd6d13dac7879713f5f",
          "cited_paper_url": "https://www.semanticscholar.org/paper/5ae1d33e36111eda24842bc3fa61ed1319c94da4",
          "citing_paper_year": 2021,
          "cited_paper_year": 2019
        },
        {
          "dataset_name": "Set5",
          "dataset_description": "Used to evaluate the performance of various image super-resolution methods, focusing on PSNR results and the impact of model complexity.",
          "citing_paper_id": "237266491",
          "cited_paper_id": 221187023,
          "context_text": "0.2 0.4 0.6 0.8 1.0 1.2 Number of Parameters 1e8\n32.45\n32.50\n32.55\n32.60\n32.65\n32.70\nPS NR\n(d B)\nEDSR (CVPR2017)\nRNAN (ICLR2019)\nOISR (CVPR2019)\nRDN (CVPR2018)\nRCAN (ECCV2018)\nIGNN (NeurIPS2020)\nHAN (ECCV2020)\nNLSA (CVPR2021)\nIPT (CVPR2021)\nSwinIR (ours)\nFigure 1: PSNR results v.s the total number of parameters of different methods for image SR (×4) on Set5 [3].\nods [73, 14, 28], they generally suffer from two basic problems that stem from the basic building block, i.e., the convolution layer.",
          "confidence_score": 1.0,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions 'Set5' as a dataset used for evaluating image super-resolution methods. No other specific datasets are mentioned.",
          "citing_paper_doi": "10.1109/ICCVW54120.2021.00210",
          "cited_paper_doi": "10.1007/978-3-030-58610-2_12",
          "citing_paper_url": "https://www.semanticscholar.org/paper/7a9a708ca61c14886aa0dcd6d13dac7879713f5f",
          "cited_paper_url": "https://www.semanticscholar.org/paper/737e2998f1db3b2ad8aaa0390ac439402ce1fb23",
          "citing_paper_year": 2021,
          "cited_paper_year": 2020
        },
        {
          "dataset_name": "Set5",
          "dataset_description": "Used to evaluate the performance of various image super-resolution methods, focusing on PSNR results and the impact of model complexity.",
          "citing_paper_id": "237266491",
          "cited_paper_id": 235641598,
          "context_text": "0.2 0.4 0.6 0.8 1.0 1.2 Number of Parameters 1e8\n32.45\n32.50\n32.55\n32.60\n32.65\n32.70\nPS NR\n(d B)\nEDSR (CVPR2017)\nRNAN (ICLR2019)\nOISR (CVPR2019)\nRDN (CVPR2018)\nRCAN (ECCV2018)\nIGNN (NeurIPS2020)\nHAN (ECCV2020)\nNLSA (CVPR2021)\nIPT (CVPR2021)\nSwinIR (ours)\nFigure 1: PSNR results v.s the total number of parameters of different methods for image SR (×4) on Set5 [3].\nods [73, 14, 28], they generally suffer from two basic problems that stem from the basic building block, i.e., the convolution layer.",
          "confidence_score": 1.0,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions 'Set5' as a dataset used for evaluating image super-resolution methods. No other specific datasets are mentioned.",
          "citing_paper_doi": "10.1109/ICCVW54120.2021.00210",
          "cited_paper_doi": "10.1109/CVPR46437.2021.00352",
          "citing_paper_url": "https://www.semanticscholar.org/paper/7a9a708ca61c14886aa0dcd6d13dac7879713f5f",
          "cited_paper_url": "https://www.semanticscholar.org/paper/a37e55b6bb39b50a31ac47100fb2f7ce10cc725b",
          "citing_paper_year": 2021,
          "cited_paper_year": 2021
        },
        {
          "dataset_name": "Set5",
          "dataset_description": "Used to evaluate the effectiveness of super-resolution methods, focusing on low-complexity single-image super-resolution techniques.",
          "citing_paper_id": "267938238",
          "cited_paper_id": 5250573,
          "context_text": "Moreover, we use Set5 [5], Set14 [74], B100 [50], Urban100 [29], and Manga109 [51] to evaluate the effectiveness of different SR methods.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The citation mentions specific datasets used for evaluating super-resolution methods, which are directly relevant to image restoration.",
          "citing_paper_doi": "10.48550/arXiv.2402.15648",
          "cited_paper_doi": "10.5244/C.26.135",
          "citing_paper_url": "https://www.semanticscholar.org/paper/e730beb44042499763d36214c0498434e470dfd5",
          "cited_paper_url": "https://www.semanticscholar.org/paper/c7338140c446bb1b8c1a2c24aa380890214dc0d7",
          "citing_paper_year": 2024,
          "cited_paper_year": 2012
        },
        {
          "dataset_name": "Set5",
          "dataset_description": "Used to test × 4 super-resolution robustness, covering a wide range of natural scenes and structures. | Used to evaluate × 4 super-resolution on urban scenes, focusing on architectural details and textures. | Used to assess × 4 super-resolution on manga images, emphasizing line art and stylistic elements. | Used to assess × 4 super-resolution accuracy, emphasizing fine details and texture in diverse natural images. | Used to evaluate × 4 super-resolution performance, focusing on natural image quality and detail preservation.",
          "citing_paper_id": "264289165",
          "cited_paper_id": 2356330,
          "context_text": "In Table 3, we present the quantitative results of × 4 SR on five benchmark datasets: Set5 [2], Set14 [51], BSD100 [31], Urban100 [19] and Manga109 [33].",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions five benchmark datasets used for evaluating × 4 super-resolution. These datasets are specific and have clear identifiers.",
          "citing_paper_doi": "10.48550/arXiv.2310.11881",
          "cited_paper_doi": "10.1007/978-3-642-27413-8_47",
          "citing_paper_url": "https://www.semanticscholar.org/paper/a9a441bbbbf0c1e826e36cc864c728d3516a3608",
          "cited_paper_url": "https://www.semanticscholar.org/paper/d1dce9ad419d8c7037980a9ea28506ed0e640bba",
          "citing_paper_year": 2023,
          "cited_paper_year": 2010
        },
        {
          "dataset_name": "Set5",
          "dataset_description": "Used to test × 4 super-resolution robustness, covering a wide range of natural scenes and structures. | Used to evaluate × 4 super-resolution on urban scenes, focusing on architectural details and textures. | Used to assess × 4 super-resolution on manga images, emphasizing line art and stylistic elements. | Used to assess × 4 super-resolution accuracy, emphasizing fine details and texture in diverse natural images. | Used to evaluate × 4 super-resolution performance, focusing on natural image quality and detail preservation.",
          "citing_paper_id": "264289165",
          "cited_paper_id": 8887614,
          "context_text": "In Table 3, we present the quantitative results of × 4 SR on five benchmark datasets: Set5 [2], Set14 [51], BSD100 [31], Urban100 [19] and Manga109 [33].",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions five benchmark datasets used for evaluating × 4 super-resolution. These datasets are specific and have clear identifiers.",
          "citing_paper_doi": "10.48550/arXiv.2310.11881",
          "cited_paper_doi": "10.1007/s11042-016-4020-z",
          "citing_paper_url": "https://www.semanticscholar.org/paper/a9a441bbbbf0c1e826e36cc864c728d3516a3608",
          "cited_paper_url": "https://www.semanticscholar.org/paper/1fef45786e707e6b9b8517b0403e596ecbdea6a5",
          "citing_paper_year": 2023,
          "cited_paper_year": 2015
        },
        {
          "dataset_name": "Set5",
          "dataset_description": "Used to evaluate single image scale-up methods, focusing on super-resolution performance using sparse representations. | Used to assess the effectiveness of single image scale-up techniques, particularly in enhancing image quality through sparse representations. | Used for evaluating single image scale-up methods, focusing on sparse-representation techniques.",
          "citing_paper_id": "252683961",
          "cited_paper_id": 2356330,
          "context_text": "In addition, we use Set5 (Bevilacqua et al., 2012), Set14 (Zeyde et al., 2010), B100 (Martin et al., 2001), Urban100 (Huang et al., 2015), and Manga109 (Matsui et al., 2017) for evaluation.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The citation mentions multiple datasets used for evaluation in the context of image restoration. Each dataset is referenced by a specific name and year, indicating they are verifiable resources.",
          "citing_paper_doi": "10.48550/arXiv.2210.00405",
          "cited_paper_doi": "10.1007/978-3-642-27413-8_47",
          "citing_paper_url": "https://www.semanticscholar.org/paper/a3d9fd2c384e98fb6074a9064562a4e4dd941ed8",
          "cited_paper_url": "https://www.semanticscholar.org/paper/d1dce9ad419d8c7037980a9ea28506ed0e640bba",
          "citing_paper_year": 2022,
          "cited_paper_year": 2010
        },
        {
          "dataset_name": "Set5",
          "dataset_description": "Used to assess the performance of image restoration methods, focusing on specific aspects of the research (e.g., image restoration quality). | Used to evaluate single-image super-resolution methods, focusing on low-complexity nonnegative neighbor embedding techniques. | Used to test the effectiveness of image restoration algorithms, particularly in preserving structural similarity and peak signal-to-noise ratio. | Used to evaluate the performance of image restoration techniques on urban scenes, emphasizing high-resolution detail recovery. | Used to assess the quality of image restoration methods on manga images, focusing on preserving artistic details and visual clarity.",
          "citing_paper_id": "257255385",
          "cited_paper_id": 5250573,
          "context_text": "Method Scale # Params [M] Set5 [3] Set14 [88] BSD100 [55] Urban100 [30] Manga109 [56] PSNR↑ SSIM↑ PSNR↑ SSIM↑ PSNR↑ SSIM↑ PSNR↑ SSIM↑ PSNR↑ SSIM↑",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The citation mentions several datasets used for evaluating image restoration methods. These datasets are specific and have clear identifiers.",
          "citing_paper_doi": "10.1109/CVPR52729.2023.01753",
          "cited_paper_doi": "10.5244/C.26.135",
          "citing_paper_url": "https://www.semanticscholar.org/paper/2920b885278a6973a306b57201e3fc3273b71132",
          "cited_paper_url": "https://www.semanticscholar.org/paper/c7338140c446bb1b8c1a2c24aa380890214dc0d7",
          "citing_paper_year": 2023,
          "cited_paper_year": 2012
        },
        {
          "dataset_name": "Set5",
          "dataset_description": "Used for evaluating super-resolution algorithms, emphasizing the restoration of fine details and textures in images. | Used to test the robustness of image restoration models on urban scenes, focusing on architectural and man-made structures. | Used for assessing the effectiveness of image restoration techniques, particularly in handling diverse and complex image structures. | Used to evaluate single image scale-up methods, focusing on sparse-representation techniques for image super-resolution. | Used for testing single image scale-up and super-resolution methods, focusing on high-quality image restoration performance.",
          "citing_paper_id": "8550762",
          "cited_paper_id": 2356330,
          "context_text": "For testing, four benchmark datasets, Set5 [1], Set14 [39],",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The citation mentions 'Set5' and 'Set14' as benchmark datasets, which are commonly used in image restoration and super-resolution tasks.",
          "citing_paper_doi": "10.1109/ICCV.2017.486",
          "cited_paper_doi": "10.1007/978-3-642-27413-8_47",
          "citing_paper_url": "https://www.semanticscholar.org/paper/5fdd40601a2ccdaa2d2ade27872bd8b3f43b2c1c",
          "cited_paper_url": "https://www.semanticscholar.org/paper/d1dce9ad419d8c7037980a9ea28506ed0e640bba",
          "citing_paper_year": 2017,
          "cited_paper_year": 2010
        },
        {
          "dataset_name": "Set5",
          "dataset_description": "Used to assess the performance of super-resolution techniques on urban scenes, emphasizing architectural details and high-frequency content. | Used for evaluating super-resolution algorithms, emphasizing the restoration of fine details and textures in images. | Used for training in single image super-resolution, providing a subset of high-resolution images for model training. | Used for training in single image super-resolution, providing a larger set of 200 images for model training. | Used to evaluate single image super-resolution methods, focusing on diverse real-world images with complex textures and structures. | Used for assessing the effectiveness of image restoration techniques, particularly in handling diverse and complex image structures. | Used to test the robustness of image restoration models on urban scenes, focusing on architectural and man-made structures. | Used to evaluate the performance of single image super-resolution networks, focusing on scale factor ×3. The dataset consists of five high-resolution images commonly used for benchmarking. | Used for testing single image scale-up and super-resolution methods, focusing on high-quality image restoration performance.",
          "citing_paper_id": "8550762",
          "cited_paper_id": 8282555,
          "context_text": "For testing, four benchmark datasets, Set5 [1], Set14 [39],\nBSD100 [28] and Urban100 [15] are used.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions four specific benchmark datasets used for testing in the research. These datasets are commonly used in image restoration and super-resolution tasks.",
          "citing_paper_doi": "10.1109/ICCV.2017.486",
          "cited_paper_doi": "10.1109/CVPR.2015.7299156",
          "citing_paper_url": "https://www.semanticscholar.org/paper/5fdd40601a2ccdaa2d2ade27872bd8b3f43b2c1c",
          "cited_paper_url": "https://www.semanticscholar.org/paper/2946c4e29b057d3543f09b89f547965895676d19",
          "citing_paper_year": 2017,
          "cited_paper_year": 2015
        },
        {
          "dataset_name": "Set5",
          "dataset_description": "Used for evaluation of image super-resolution methods, focusing on urban scenes with complex textures. | Used for evaluation of image super-resolution methods, containing manga images with distinct artistic styles. | Used for evaluation of image super-resolution methods, consisting of 100 high-resolution images. | Used as training data for image super-resolution, focusing on high-quality images with diverse content. | Used for evaluation of image super-resolution methods, containing a small set of high-quality images. | Used as training data for image super-resolution, providing a large set of high-resolution images. | Used for evaluation of image super-resolution methods, containing a set of diverse images.",
          "citing_paper_id": "252693111",
          "cited_paper_id": 10514149,
          "context_text": "For image SR, following previous works Zhang et al. (2018b); Haris et al. (2018), we use DIV2K Timofte et al. (2017) and Flickr2K Lim et al. (2017) as training data, Set5 Bevilacqua et al. (2012), Set14 Zeyde et al. (2010), B100 Martin et al. (2001), Urban100 Huang et al. (2015), and Manga109…",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions several datasets used for training and evaluation in image super-resolution tasks, which are directly relevant to the topic of all-in-one image restoration.",
          "citing_paper_doi": "10.48550/arXiv.2210.01427",
          "cited_paper_doi": "10.1109/TIP.2018.2839891",
          "citing_paper_url": "https://www.semanticscholar.org/paper/a1f7f5597fdbc54e58f1f2a1a640bf355e87a978",
          "cited_paper_url": "https://www.semanticscholar.org/paper/e248ac3596d48ce338244624c2fd194dc0651bc6",
          "citing_paper_year": 2022,
          "cited_paper_year": 2017
        },
        {
          "dataset_name": "Set5",
          "dataset_description": "Used to evaluate image restoration methods, focusing on super-resolution performance using PSNR and SSIM metrics.",
          "citing_paper_id": "252693111",
          "cited_paper_id": 6628106,
          "context_text": "Method Scale Set5 Set14 B100 Urban100 Manga109 PSNR SSIM PSNR SSIM PSNR SSIM PSNR SSIM PSNR SSIM EDSR Lim et al. (2017) ×2 38.11 0.9602 33.92 0.9195 32.32 0.9013 32.93 0.9351 39.10 0.9773 RCAN Zhang et al. (2018b) ×2 38.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The citation span mentions several dataset names (Set5, Set14, B100, Urban100, Manga109) which are used for evaluating image restoration methods. These datasets are specific and relevant to the topic of all-in-one image restoration.",
          "citing_paper_doi": "10.48550/arXiv.2210.01427",
          "cited_paper_doi": null,
          "citing_paper_url": "https://www.semanticscholar.org/paper/a1f7f5597fdbc54e58f1f2a1a640bf355e87a978",
          "cited_paper_url": "https://www.semanticscholar.org/paper/a6cb366736791bcccc5c8639de5a8f9636bf87e8",
          "citing_paper_year": 2022,
          "cited_paper_year": 2014
        },
        {
          "dataset_name": "Set5",
          "dataset_description": "Used to evaluate image restoration methods, focusing on super-resolution performance using PSNR and SSIM metrics.",
          "citing_paper_id": "252693111",
          "cited_paper_id": 49212467,
          "context_text": "Method Scale Set5 Set14 B100 Urban100 Manga109 PSNR SSIM PSNR SSIM PSNR SSIM PSNR SSIM PSNR SSIM EDSR Lim et al. (2017) ×2 38.",
          "confidence_score": 0.8,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions several dataset names, which are likely used for evaluating image restoration methods. However, the context does not specify the exact usage or research questions.",
          "citing_paper_doi": "10.48550/arXiv.2210.01427",
          "cited_paper_doi": "10.1109/WACV45572.2020.9093586",
          "citing_paper_url": "https://www.semanticscholar.org/paper/a1f7f5597fdbc54e58f1f2a1a640bf355e87a978",
          "cited_paper_url": "https://www.semanticscholar.org/paper/ec4ce5210a75e6a15f48dc0988f12c4f2d2ce394",
          "citing_paper_year": 2022,
          "cited_paper_year": 2018
        },
        {
          "dataset_name": "Set5",
          "dataset_description": "Used to evaluate the performance of MWCNN in image super-resolution, comparing PSNR gains against VDSR. | Used to assess the effectiveness of image restoration techniques, particularly in super-resolution tasks. | Used to evaluate super-resolution performance at a scale factor of 4, comparing MWCNN's PSNR values against SRResNet. | Used to evaluate MWCNN's single image super-resolution performance, focusing on high-quality image restoration and detail preservation. | Used to test the robustness of image restoration algorithms, emphasizing detail preservation and artifact reduction. | Used to evaluate the performance of image restoration methods on urban scenes, focusing on texture and structure recovery. | Used to evaluate super-resolution performance at a scale factor of 4, comparing MWCNN's PSNR values against other methods. | Used to evaluate single image scale-up methods, focusing on super-resolution performance and visual quality.",
          "citing_paper_id": "29151865",
          "cited_paper_id": 5250573,
          "context_text": "We test MWCNN on four datasets, i.e., Set5 [7], Set14 [56], BSD100 [38], and Urban100 [23], because they are widely adopted to\nevaluate SISR performance.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions four specific datasets used to evaluate the performance of MWCNN for single image super-resolution. These datasets are widely adopted for this purpose.",
          "citing_paper_doi": "10.1109/CVPRW.2018.00121",
          "cited_paper_doi": "10.5244/C.26.135",
          "citing_paper_url": "https://www.semanticscholar.org/paper/5813ae0a48ff50d48f406df52ee5b9795c34a8bd",
          "cited_paper_url": "https://www.semanticscholar.org/paper/c7338140c446bb1b8c1a2c24aa380890214dc0d7",
          "citing_paper_year": 2018,
          "cited_paper_year": 2012
        },
        {
          "dataset_name": "Set5",
          "dataset_description": "Used for evaluating single-image super-resolution methods, focusing on low-complexity algorithms and nonnegative neighbor embedding.",
          "citing_paper_id": "275133634",
          "cited_paper_id": 5250573,
          "context_text": "For evaluation, we employ the following five datasets as test sets, i.e. , Set5 [3], Set14 [54], B100 [35], Urban100 [20] and Manga109 [36].",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context explicitly mentions five datasets used for evaluation in the research on image restoration. These datasets are specific and relevant to the topic.",
          "citing_paper_doi": "10.48550/arXiv.2412.20066",
          "cited_paper_doi": "10.5244/C.26.135",
          "citing_paper_url": "https://www.semanticscholar.org/paper/2b78fc275b7fc3a37061620ae155c017ab6c22e1",
          "cited_paper_url": "https://www.semanticscholar.org/paper/c7338140c446bb1b8c1a2c24aa380890214dc0d7",
          "citing_paper_year": 2024,
          "cited_paper_year": 2012
        },
        {
          "dataset_name": "Set5",
          "dataset_description": "Used to evaluate RNAN+ in image restoration, specifically for super-resolution tasks on urban scenes with complex textures. | Used to test the performance of RNAN+ in image restoration, emphasizing super-resolution on a larger set of natural images. | Used for color and gray-scale image denoising, offering a diverse set of urban scenes for testing image restoration methods. | Used to evaluate the performance of RNAN+ in image restoration, focusing on super-resolution tasks with a small set of high-quality images. | Used to assess the effectiveness of RNAN+ in image restoration, particularly for super-resolution tasks with a diverse set of images. | Used for color and gray-scale image denoising, serving as a standard dataset for assessing image restoration performance. | Used to assess the performance of RNAN+ in image restoration, focusing on super-resolution tasks with manga images, highlighting its ability to handle line art and text. | Used for color and gray-scale image denoising, providing a benchmark for evaluating image restoration techniques.",
          "citing_paper_id": "85501306",
          "cited_paper_id": 64193,
          "context_text": "As shown in Table 6, our RNAN+ achieves the second best performance among benchmark datasets: Set5 (Bevilacqua et al., 2012), Set14 (Zeyde et al., 2010), B100 (Martin et al., 2001), Urban100 (Huang et al., 2015), and Manga109 (Matsui et al., 2017).",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions several benchmark datasets used to evaluate the performance of RNAN+. These datasets are specific and have clear identifiers.",
          "citing_paper_doi": null,
          "cited_paper_doi": "10.1109/ICCV.2001.937655",
          "citing_paper_url": "https://www.semanticscholar.org/paper/34439db81b482cd562e1cdba974c70a2b89cd6d4",
          "cited_paper_url": "https://www.semanticscholar.org/paper/9a1ed876196ec9733acb1daa6d65e35ff0414291",
          "citing_paper_year": 2019,
          "cited_paper_year": 2001
        },
        {
          "dataset_name": "Set5",
          "dataset_description": "Used to evaluate RNAN+ in image restoration, specifically for super-resolution tasks on urban scenes with complex textures. | Used to test the performance of RNAN+ in image restoration, emphasizing super-resolution on a larger set of natural images. | Used to evaluate the performance of RNAN+ in image restoration, focusing on super-resolution tasks with a small set of high-quality images. | Used to assess the effectiveness of RNAN+ in image restoration, particularly for super-resolution tasks with a diverse set of images. | Used to assess the performance of RNAN+ in image restoration, focusing on super-resolution tasks with manga images, highlighting its ability to handle line art and text.",
          "citing_paper_id": "85501306",
          "cited_paper_id": 5250573,
          "context_text": "As shown in Table 6, our RNAN+ achieves the second best performance among benchmark datasets: Set5 (Bevilacqua et al., 2012), Set14 (Zeyde et al., 2010), B100 (Martin et al., 2001), Urban100 (Huang et al., 2015), and Manga109 (Matsui et al., 2017).",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions several benchmark datasets used to evaluate the performance of RNAN+. These datasets are specific and have clear identifiers.",
          "citing_paper_doi": null,
          "cited_paper_doi": "10.5244/C.26.135",
          "citing_paper_url": "https://www.semanticscholar.org/paper/34439db81b482cd562e1cdba974c70a2b89cd6d4",
          "cited_paper_url": "https://www.semanticscholar.org/paper/c7338140c446bb1b8c1a2c24aa380890214dc0d7",
          "citing_paper_year": 2019,
          "cited_paper_year": 2012
        },
        {
          "dataset_name": "Set5",
          "dataset_description": "Used to evaluate RNAN+ in image restoration, specifically for super-resolution tasks on urban scenes with complex textures. | Used to test the performance of RNAN+ in image restoration, emphasizing super-resolution on a larger set of natural images. | Used to evaluate the performance of RNAN+ in image restoration, focusing on super-resolution tasks with a small set of high-quality images. | Used to assess the effectiveness of RNAN+ in image restoration, particularly for super-resolution tasks with a diverse set of images. | Used to assess the performance of RNAN+ in image restoration, focusing on super-resolution tasks with manga images, highlighting its ability to handle line art and text.",
          "citing_paper_id": "85501306",
          "cited_paper_id": 8887614,
          "context_text": "As shown in Table 6, our RNAN+ achieves the second best performance among benchmark datasets: Set5 (Bevilacqua et al., 2012), Set14 (Zeyde et al., 2010), B100 (Martin et al., 2001), Urban100 (Huang et al., 2015), and Manga109 (Matsui et al., 2017).",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions several benchmark datasets used to evaluate the performance of RNAN+. These datasets are specific and have clear identifiers.",
          "citing_paper_doi": null,
          "cited_paper_doi": "10.1007/s11042-016-4020-z",
          "citing_paper_url": "https://www.semanticscholar.org/paper/34439db81b482cd562e1cdba974c70a2b89cd6d4",
          "cited_paper_url": "https://www.semanticscholar.org/paper/1fef45786e707e6b9b8517b0403e596ecbdea6a5",
          "citing_paper_year": 2019,
          "cited_paper_year": 2015
        },
        {
          "dataset_name": "Set5",
          "dataset_description": "Used for training a model for image de-noising tasks, providing a larger set of diverse images. | Used to evaluate image super-resolution methods, focusing on high-quality image reconstruction from low-resolution inputs. | Used for training a model for image de-noising tasks, providing a small set of high-quality images.",
          "citing_paper_id": "996788",
          "cited_paper_id": 9971732,
          "context_text": "We adopt the four testing datasets (i.e., Set5 and Set14, BSD100 and Urban100 [40]) used in [35].",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The citation mentions specific datasets used for testing image super-resolution methods, which are directly relevant to the topic of all-in-one image restoration.",
          "citing_paper_doi": "10.1109/TIP.2017.2662206",
          "cited_paper_doi": "10.1109/CVPR.2016.182",
          "citing_paper_url": "https://www.semanticscholar.org/paper/0c00a328fa7cd56ee60338c54e89bd48310db80b",
          "cited_paper_url": "https://www.semanticscholar.org/paper/b5f3e5d2912bedbcd9458952d664b08db6aed962",
          "citing_paper_year": 2016,
          "cited_paper_year": 2015
        },
        {
          "dataset_name": "Set5",
          "dataset_description": "Used to evaluate bicubic degradation in image restoration, focusing on average PSNR and SSIM metrics.",
          "citing_paper_id": "2141622",
          "cited_paper_id": 2356330,
          "context_text": "Average PSNR and SSIM results for bicubic degradation on datasets Set5 [3], Set14 [54], BSD100 [33] and Urban100 [19].",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions specific datasets used for evaluating image restoration methods, which are directly relevant to the research topic.",
          "citing_paper_doi": "10.1109/CVPR.2018.00344",
          "cited_paper_doi": "10.1007/978-3-642-27413-8_47",
          "citing_paper_url": "https://www.semanticscholar.org/paper/b26b4c9da872d3c6122dbf23b9d6e063dc6456b5",
          "cited_paper_url": "https://www.semanticscholar.org/paper/d1dce9ad419d8c7037980a9ea28506ed0e640bba",
          "citing_paper_year": 2017,
          "cited_paper_year": 2010
        },
        {
          "dataset_name": "Set5",
          "dataset_description": "Used to evaluate different image restoration methods under various degradations, focusing on quantitative performance metrics. | Used to evaluate bicubic degradation in image restoration, focusing on average PSNR and SSIM metrics.",
          "citing_paper_id": "2141622",
          "cited_paper_id": 5250573,
          "context_text": "Average PSNR and SSIM results for bicubic degradation on datasets Set5 [3], Set14 [54], BSD100 [33] and Urban100 [19].",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions specific datasets used for evaluating image restoration methods, which are directly relevant to the research topic.",
          "citing_paper_doi": "10.1109/CVPR.2018.00344",
          "cited_paper_doi": "10.5244/C.26.135",
          "citing_paper_url": "https://www.semanticscholar.org/paper/b26b4c9da872d3c6122dbf23b9d6e063dc6456b5",
          "cited_paper_url": "https://www.semanticscholar.org/paper/c7338140c446bb1b8c1a2c24aa380890214dc0d7",
          "citing_paper_year": 2017,
          "cited_paper_year": 2012
        },
        {
          "dataset_name": "Set5",
          "dataset_description": "Used for evaluating image restoration methods, specifically comparing various super-resolution techniques. The dataset consists of 14 high-resolution images commonly used in the literature. | Used to evaluate image super-resolution methods, focusing on upscaling performance and accuracy using various deep learning and traditional techniques.",
          "citing_paper_id": "15799108",
          "cited_paper_id": 7036556,
          "context_text": "Set5 Bicubic K-SVD [58] ANR [54] SR-CNN [19] RFL [53] TNRD 5 7 7",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions 'Set5' which is a known dataset in the field of image super-resolution. However, the other terms ('Bicubic', 'K-SVD', 'ANR', 'SR-CNN', 'RFL', 'TNRD') are methods or algorithms, not datasets.",
          "citing_paper_doi": "10.1109/TPAMI.2016.2596743",
          "cited_paper_doi": "10.1109/CVPR.2015.7299003",
          "citing_paper_url": "https://www.semanticscholar.org/paper/eb04068416ade86de63cf9d9939e14d0bc9b96f9",
          "cited_paper_url": "https://www.semanticscholar.org/paper/6d0096f31fca8948b255c72a683fe951954ef334",
          "citing_paper_year": 2015,
          "cited_paper_year": 2015
        },
        {
          "dataset_name": "Set5",
          "dataset_description": "Used for evaluating image restoration methods, specifically comparing various super-resolution techniques. The dataset consists of 14 high-resolution images commonly used in the literature. | Used to evaluate image super-resolution methods, focusing on upscaling performance and accuracy using various deep learning and traditional techniques.",
          "citing_paper_id": "15799108",
          "cited_paper_id": 18874645,
          "context_text": "Set5 Bicubic K-SVD [58] ANR [54] SR-CNN [19] RFL [53] TNRD 5 7 7",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions 'Set5' which is a known dataset in the field of image super-resolution. However, the other terms ('Bicubic', 'K-SVD', 'ANR', 'SR-CNN', 'RFL', 'TNRD') are methods or algorithms, not datasets.",
          "citing_paper_doi": "10.1109/TPAMI.2016.2596743",
          "cited_paper_doi": "10.1007/978-3-319-10593-2_13",
          "citing_paper_url": "https://www.semanticscholar.org/paper/eb04068416ade86de63cf9d9939e14d0bc9b96f9",
          "cited_paper_url": "https://www.semanticscholar.org/paper/0504945cc2d03550fecb6ff02e637f9421107c25",
          "citing_paper_year": 2015,
          "cited_paper_year": 2014
        },
        {
          "dataset_name": "Set5",
          "dataset_description": "Used to evaluate GAN-based super-resolution methods, focusing on high-quality image restoration performance.",
          "citing_paper_id": "257557425",
          "cited_paper_id": 215785896,
          "context_text": "We evaluate our DiffIRS2 and other SOTA GAN-based SR methods on five benchmarks (Set5 [3], Set14 [77], General100 [16], Urban100 [25], and DIV2K100 [1]) using LPIPS [85] and DISTS [13].",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions five benchmarks used for evaluating image restoration methods. These benchmarks are likely datasets used for testing and validation.",
          "citing_paper_doi": "10.1109/ICCV51070.2023.01204",
          "cited_paper_doi": "10.1109/TPAMI.2020.3045810",
          "citing_paper_url": "https://www.semanticscholar.org/paper/67909a17f9c9467de536aa2cf7b0864dc6215e96",
          "cited_paper_url": "https://www.semanticscholar.org/paper/f0f6998fb5a8f88c003860ca06f6ecaf6b5feec3",
          "citing_paper_year": 2023,
          "cited_paper_year": 2020
        }
      ]
    },
    {
      "cited_paper_id": "73439498",
      "citation_count": 0,
      "total_dataset_mentions": 37,
      "unique_datasets": [
        "BSD400"
      ],
      "dataset_details": [
        {
          "dataset_name": "BSD400",
          "dataset_description": "Used to evaluate image restoration methods, focusing on general image quality and restoration accuracy under various degradations. | Used to evaluate the effectiveness of image restoration models in removing rain streaks. It consists of 100 pairs of rainy and clean images. | Used to evaluate the robustness of image dehazing models. It includes synthetic outdoor scenes with varying levels of haze. | Used to test restoration performance on urban scenes, focusing on architectural details and texture recovery under multiple degradations. | Used to evaluate the effectiveness of restoration methods in synthetic outdoor scenes with haze, focusing on realistic haze removal and image clarity. | Used to assess the performance of restoration methods in removing haze from outdoor scenes, focusing on color fidelity and depth perception. | Used to assess the effectiveness of restoration methods on images with water effects, emphasizing visual clarity and detail preservation. | Used to evaluate image restoration models, focusing on rain, haze, and noise corruption. The dataset provides high-quality images for benchmarking. | Used to test image restoration algorithms, particularly for urban scenes with rain, haze, and noise. It includes high-resolution images of cityscapes. | Used to assess the performance of image dehazing algorithms. It contains outdoor scenes with synthetic haze for benchmarking. | Used to assess the performance of image restoration models under various corruptions, including rain, haze, and noise. It contains diverse real-world images. | Used to evaluate the ability of restoration methods to remove rain streaks from images, focusing on preserving scene details and reducing artifacts.",
          "citing_paper_id": "273502246",
          "cited_paper_id": 250551851,
          "context_text": "…MiOIR [121] rain-haze-noise[63] 3 Rain, Haze, Noise-σ 15 , Noise-σ 25 , Noise-σ 50 BSD400, WED, Urban100, Rain100L, RESIDE-OTS, RESIDE-SOTS AirNet [63], PromptIR [65],PIP [72], Textpromp-tIR [94], NDR [96], InstructIR [62], AdaIR [78], U-WADN [186], DyNet [185], DaAIR [187], AnyIR [188], MEASNet…",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions several datasets and methods used for image restoration tasks involving rain, haze, and noise. The datasets are clearly identified and used for evaluation.",
          "citing_paper_doi": "10.48550/arXiv.2410.15067",
          "cited_paper_doi": "10.1109/CVPR52688.2022.01693",
          "citing_paper_url": "https://www.semanticscholar.org/paper/f5ff17162a4936db3f64ed7bafae13abcfa63a27",
          "cited_paper_url": "https://www.semanticscholar.org/paper/c85f67a49cfdc29a77bd741a59e739cdc814edde",
          "citing_paper_year": 2024,
          "cited_paper_year": 2022
        },
        {
          "dataset_name": "BSD400",
          "dataset_description": "Used to assess image restoration techniques, emphasizing urban scenes with complex degradations. | Used to evaluate the performance of image restoration methods in enhancing low-light images, focusing on improving visibility and detail in dark conditions. | Used to test image restoration algorithms on high-resolution urban scenes, evaluating their ability to handle complex textures and structures. | Used to assess the effectiveness of image restoration techniques in removing haze, emphasizing synthetic outdoor scenes with varying levels of haze. | Used to assess deblurring techniques, featuring real-world motion blur. | Used to assess the effectiveness of image restoration techniques in handling real-world degradations, emphasizing practical application scenarios. | Used to evaluate haze removal algorithms, offering synthetic outdoor scenes with realistic haze. | Used to evaluate the performance of image restoration methods in removing rain streaks from images, focusing on realistic rain patterns. | Used to evaluate the performance of image restoration methods in removing haze from real-world outdoor scenes, focusing on natural and challenging conditions. | Used to evaluate rain removal algorithms, focusing on heavy rain conditions. | Used to evaluate low-light image enhancement, focusing on improving visibility in dark conditions. | Used to evaluate image restoration methods under various degradations, focusing on benchmarking performance across diverse image content. | Used to evaluate image restoration methods, focusing on various degradations including rain, haze, noise, blur, and low-light conditions. | Used to test haze removal methods, providing outdoor synthetic images with varying levels of haze. | Used to test image restoration algorithms, particularly for evaluating performance under multiple degradations. | Used to test the ability of image restoration algorithms to handle motion blur, focusing on high-speed camera footage with realistic blur effects.",
          "citing_paper_id": "273502246",
          "cited_paper_id": 260085391,
          "context_text": "…rain-haze-noise-blur-dark[66] 5 Rain, Haze, Noise-σ 25 , Blur, Low-Light BSD400, WED, Urban100, Rain100L, RESIDE-OTS, RESIDE-SOTS, Gopro, LOL IDR [66], AdaIR [78], InstructIR [62], PIP [72], DaAIR [187], AnyIR [188], MEASNet [110], HAIR [189], Perceive-IR [111] rain-noise-blur-dark[61] 4 Rain,…",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions multiple datasets used for image restoration, which are relevant to the research topic of all-in-one image restoration.",
          "citing_paper_doi": "10.48550/arXiv.2410.15067",
          "cited_paper_doi": "10.1109/CVPR52729.2023.00564",
          "citing_paper_url": "https://www.semanticscholar.org/paper/f5ff17162a4936db3f64ed7bafae13abcfa63a27",
          "cited_paper_url": "https://www.semanticscholar.org/paper/6eb039fa12dc81be85c24a10dda5522c629b0b12",
          "citing_paper_year": 2024,
          "cited_paper_year": 2023
        },
        {
          "dataset_name": "BSD400",
          "dataset_description": "Used to test the effectiveness of image restoration models in removing rain streaks from images. | Used to evaluate the performance of image restoration models on synthetic hazy images. | Used to assess the performance of image restoration models on watermarked and encrypted degraded images. | Used to evaluate super-resolution and denoising capabilities on high-resolution urban scenes. | Used to evaluate image restoration models, focusing on denoising and deblurring tasks with a diverse set of images. | Used to assess the effectiveness of image restoration models on real-world hazy images.",
          "citing_paper_id": "273502246",
          "cited_paper_id": 264305756,
          "context_text": "…15 , Noise-σ 25 , Noise-σ 50 BSD400, WED, Urban100, Rain100L, RESIDE-OTS, RESIDE-SOTS AirNet [63], PromptIR [65],PIP [72], Textpromp-tIR [94], NDR [96], InstructIR [62], AdaIR [78], U-WADN [186], DyNet [185], DaAIR [187], AnyIR [188], MEASNet [110], HAIR [189], Perceive-IR [111]…",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The citation mentions several datasets and methods, but only datasets are considered. The datasets are used for evaluating image restoration models under various degradation conditions.",
          "citing_paper_doi": "10.48550/arXiv.2410.15067",
          "cited_paper_doi": "10.1109/TIP.2024.3456583",
          "citing_paper_url": "https://www.semanticscholar.org/paper/f5ff17162a4936db3f64ed7bafae13abcfa63a27",
          "cited_paper_url": "https://www.semanticscholar.org/paper/a9eb8bd7057c82150b7d4cafb073ea4f2cb93636",
          "citing_paper_year": 2024,
          "cited_paper_year": 2023
        },
        {
          "dataset_name": "BSD400",
          "dataset_description": "Used to evaluate rain removal and haze reduction, focusing on natural appearance and detail preservation. | Used to test deblurring algorithms, focusing on sharpness and motion artifact reduction. | Used to evaluate the effectiveness of rain removal techniques, focusing on naturalness and artifact reduction. | Used to evaluate low-light image enhancement, focusing on brightness and noise reduction. | Used to evaluate snow removal methods, focusing on natural appearance and detail preservation. | Used to test image restoration algorithms in urban scenes, focusing on architectural details and texture recovery. | Used to assess noise reduction in images, focusing on preserving fine details and reducing artifacts. | Used to test all-weather image restoration, focusing on robustness and quality across diverse conditions. | Used to test video dehazing and deraining, focusing on temporal consistency and visual quality. | Used to assess multi-weather condition restoration, focusing on comprehensive performance across various degradations. | Used to assess haze removal methods, focusing on color fidelity and contrast enhancement. | Used to evaluate synthetic outdoor scenes for haze removal, emphasizing realistic results and visual quality. | Used to evaluate snow removal techniques, focusing on naturalness and artifact reduction. | Used to assess the performance of image restoration models in removing water effects, emphasizing clarity and detail preservation. | Used to evaluate image restoration methods under various degradation conditions, focusing on robustness and quality of restored images.",
          "citing_paper_id": "273502246",
          "cited_paper_id": 265609570,
          "context_text": "Meanwhile, MiOIR [121] rain-haze-noise[63] 3 Rain, Haze, Noise-σ 15 , Noise-σ 25 , Noise-σ 50 BSD400, WED, Urban100, Rain100L, RESIDE-OTS, RESIDE-SOTS AirNet [63], PromptIR [65],PIP [72], Textpromp-tIR [94], NDR [96], InstructIR [62], AdaIR [78], U-WADN [186], DyNet [185], DaAIR [187], AnyIR [188], MEASNet [110], HAIR [189], Perceive-IR [111] rain-haze-noise-blur-dark[66] 5 Rain, Haze, Noise-σ 25 , Blur, Low-Light BSD400, WED, Urban100, Rain100L, RESIDE-OTS, RESIDE-SOTS, Gopro, LOL IDR [66], AdaIR [78], InstructIR [62], PIP [72], DaAIR [187], AnyIR [188], MEASNet [110], HAIR [189], Perceive-IR [111] rain-noise-blur-dark[61] 4 Rain, Noise, Blur, Low-light SIDD, merged deraining, merged debluring,LOL ProRes [61] rain-haze-snow[67] 3 Rain, Haze, Snow SPA+,REVIDE,RealSnow WGWS-Net [67], TKMANet [100], Art [135] rain-haze-snow[64] 3 Raindrop, Rain+Fog, Snow All-weather (Outdoor-Rain,Snow100k,Raindrop) Transweather [64], WeatherDiff [69], AWRCP [68], AirNet [56] rain-haze-snow[173] 3 Rain, Haze, Snow WeatherStream (176, 100 training images and 11, 400 testing images) LDR [74] SimMIM [180] introduced a generalized masked image modeling approach based on the Swin-ViT [182] architecture.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions several datasets used for evaluating image restoration methods under various conditions such as rain, haze, noise, blur, and low-light. These datasets are specifically named and are relevant to the topic of all-in-one image restoration.",
          "citing_paper_doi": "10.48550/arXiv.2410.15067",
          "cited_paper_doi": "10.48550/arXiv.2312.01677",
          "citing_paper_url": "https://www.semanticscholar.org/paper/f5ff17162a4936db3f64ed7bafae13abcfa63a27",
          "cited_paper_url": "https://www.semanticscholar.org/paper/c2fbbe80f7374a3aade7dcc1c6617f91818ab729",
          "citing_paper_year": 2024,
          "cited_paper_year": 2023
        },
        {
          "dataset_name": "BSD400",
          "dataset_description": "Used to test rain removal algorithms, focusing on heavy rain conditions. | Used to assess the performance of image restoration models on watermarked and encrypted images, emphasizing robustness. | Used to assess deblurring performance, focusing on motion blur in video frames. | Used to test dehazing models on synthetic outdoor scenes, focusing on the realism and effectiveness of haze removal. | Used to evaluate super-resolution and deblurring algorithms, focusing on urban scenes. | Used to evaluate dehazing algorithms, focusing on outdoor real-world images. | Used to evaluate dehazing algorithms, focusing on outdoor synthetic images. | Used to evaluate super-resolution and denoising models on urban scenes, highlighting fine details and textures. | Used to evaluate dehazing models on outdoor scenes, assessing the ability to restore visibility in hazy conditions. | Used to evaluate image restoration techniques, focusing on deblurring and denoising performance. | Used to test rain removal algorithms, focusing on heavy rain conditions and their impact on image quality. | Used to assess image restoration quality, particularly for wide exposure differences. | Used to evaluate low-light image enhancement, focusing on improving visibility in dark conditions. | Used to evaluate image restoration models, focusing on denoising and deblurring tasks with a diverse set of images. | Used to test denoising algorithms, focusing on realistic noise patterns in images.",
          "citing_paper_id": "273502246",
          "cited_paper_id": 266149517,
          "context_text": "…Haze, Noise-σ 25 , Blur, Low-Light BSD400, WED, Urban100, Rain100L, RESIDE-OTS, RESIDE-SOTS, Gopro, LOL IDR [66], AdaIR [78], InstructIR [62], PIP [72], DaAIR [187], AnyIR [188], MEASNet [110], HAIR [189], Perceive-IR [111] rain-noise-blur-dark[61] 4 Rain, Noise, Blur, Low-light SIDD, merged…",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions several datasets and methods related to image restoration. Only datasets are included, and methods are excluded.",
          "citing_paper_doi": "10.48550/arXiv.2410.15067",
          "cited_paper_doi": "10.48550/arXiv.2312.05038",
          "citing_paper_url": "https://www.semanticscholar.org/paper/f5ff17162a4936db3f64ed7bafae13abcfa63a27",
          "cited_paper_url": "https://www.semanticscholar.org/paper/38496bf5e8fd6f16ddf36578586b08a8225a4aa2",
          "citing_paper_year": 2024,
          "cited_paper_year": 2023
        },
        {
          "dataset_name": "BSD400",
          "dataset_description": "Used for single-task image denoising, generating training images by adding Gaussian noise at levels σ ∈ {15, 25, 50}.",
          "citing_paper_id": "271874811",
          "cited_paper_id": 4840263,
          "context_text": "For single-task image denoising, we use the BSD400 [49] and WED [50] datasets, adding Gaussian noise at levels σ ∈ { 15 , 25 , 50 } to generate training images.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions two specific datasets, BSD400 and WED, which are used for generating training images by adding Gaussian noise for single-task image denoising.",
          "citing_paper_doi": "10.48550/arXiv.2408.08091",
          "cited_paper_doi": "10.1109/TIP.2016.2631888",
          "citing_paper_url": "https://www.semanticscholar.org/paper/99facb268ce89958bcd541076286efac30652b46",
          "cited_paper_url": "https://www.semanticscholar.org/paper/1e2fe67d117a19a59a4f82e4950499c5ee5a7473",
          "citing_paper_year": 2024,
          "cited_paper_year": 2017
        },
        {
          "dataset_name": "BSD400",
          "dataset_description": "Used for low-light enhancement, focusing on improving visibility and color accuracy in low-light conditions. | Used for denoising, focusing on enhancing image quality by reducing noise while preserving important features. | Used for deblurring, aimed at restoring sharpness and clarity to blurred images, particularly in motion scenarios. | Used for dehazing, improving visibility in hazy images, focusing on atmospheric scattering effects. | Used for denoising, specifically addressing high-frequency noise, enhancing image clarity. | Used for dehazing, addressing the removal of atmospheric haze to enhance image clarity and detail. | Used for deraining, focusing on removing rain streaks from images to improve visual quality and clarity. | Used for deraining, enhancing images degraded by rain, focusing on realistic rain streak removal. | Used for denoising, specifically targeting the reduction of noise in images to improve overall quality. | Used for low-light enhancement, improving visibility in dark images, focusing on brightness and contrast adjustment. | Used for denoising, reducing noise in images, focusing on preserving fine details and textures. | Used for deblurring, correcting motion blur in images, focusing on sharpness and detail recovery.",
          "citing_paper_id": "260085391",
          "cited_paper_id": 4840263,
          "context_text": "including Rain200L [53] for deraining, RESIDE [27] for dehazing, BSD400 [35] and WED [33] for denoising, GoPro [39] for deblurring and LOL [7] for low-light enhancement.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The citation mentions several datasets used for various image restoration tasks, which align with the research topic of all-in-one image restoration.",
          "citing_paper_doi": "10.1109/CVPR52729.2023.00564",
          "cited_paper_doi": "10.1109/TIP.2016.2631888",
          "citing_paper_url": "https://www.semanticscholar.org/paper/6eb039fa12dc81be85c24a10dda5522c629b0b12",
          "cited_paper_url": "https://www.semanticscholar.org/paper/1e2fe67d117a19a59a4f82e4950499c5ee5a7473",
          "citing_paper_year": 2023,
          "cited_paper_year": 2017
        },
        {
          "dataset_name": "BSD400",
          "dataset_description": "Used for evaluating image restoration methods, providing a small but diverse set of images to assess performance. | Used as training data for image restoration, focusing on natural images segmented by humans to evaluate segmentation algorithms and ecological statistics. | Used for evaluating image restoration methods, offering a larger set of images to validate the effectiveness of the restoration techniques.",
          "citing_paper_id": "248426764",
          "cited_paper_id": 64193,
          "context_text": "For this application, we choose the widely used BSD400 dataset [43] as the training data and evaluate each method on Set11 [35] and BSD68 [43] test sets.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions specific datasets used for training and evaluation in image restoration. BSD400, Set11, and BSD68 are all well-known datasets in the field.",
          "citing_paper_doi": "10.1109/CVPR52688.2022.01688",
          "cited_paper_doi": "10.1109/ICCV.2001.937655",
          "citing_paper_url": "https://www.semanticscholar.org/paper/22755044094d1eff9d2b61bf2861649446d31bf9",
          "cited_paper_url": "https://www.semanticscholar.org/paper/9a1ed876196ec9733acb1daa6d65e35ff0414291",
          "citing_paper_year": 2022,
          "cited_paper_year": 2001
        },
        {
          "dataset_name": "BSD400",
          "dataset_description": "Used for image denoising in a single-task setting, combined with BSD400 and augmented with Gaussian noise at levels σ ∈ [15, 25, 50] to create noisy images. | Used for image denoising in a single-task setting, combined with WED and augmented with Gaussian noise at levels σ ∈ [15, 25, 50] to create noisy images.",
          "citing_paper_id": "270045495",
          "cited_paper_id": 247411392,
          "context_text": "For All-in-One and single-task settings, we follow existing work [22, 31] and include following datasets: For image denoising in single task setting, we combine the BSD400 [1] and WED [27] datasets, adding Gaussian noise at levels σ ∈ [15 , 25 , 50] to create noisy images.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions two specific datasets, BSD400 and WED, which are used for image denoising in a single-task setting. These datasets are combined and Gaussian noise is added to create noisy images.",
          "citing_paper_doi": "10.48550/arXiv.2405.15475",
          "cited_paper_doi": "10.48550/arXiv.2203.06074",
          "citing_paper_url": "https://www.semanticscholar.org/paper/1dc809107ceb27fc74071934878fe2dc9292e57d",
          "cited_paper_url": "https://www.semanticscholar.org/paper/03244185e343d6cc2397269c510f33433a7df502",
          "citing_paper_year": 2024,
          "cited_paper_year": 2022
        },
        {
          "dataset_name": "BSD400",
          "dataset_description": "Used for testing image denoising models, focusing on urban scenes to assess performance on specific image types. | Used for denoising evaluations, providing a benchmark for assessing image restoration algorithms on web images. | Used for denoising, evaluating the effectiveness of the proposed method in removing noise from natural images. | Used for training image denoising models, complementing BSD400 with additional images and annotations. | Used for testing image denoising models, offering a smaller set of high-quality images for evaluation. | Used for training image denoising models, providing a diverse set of natural images with human-segmented annotations. | Used for dehazing, evaluating the method's effectiveness in enhancing visibility in hazy conditions. | Used to create a degraded version with spatially variant noises, focusing on image restoration techniques and evaluating their performance. | Used to evaluate denoising methods, focusing on performance metrics under the one-by-one setting. | Used to provide a smaller set of natural images for evaluation in image restoration tasks, focusing on the robustness and performance of restoration algorithms on a diverse set of images. | Used for denoising, testing the method's ability to handle various types of noise in diverse images. | Used for denoising evaluations, providing a benchmark for assessing image restoration algorithms on natural images. | Used for dehazing evaluations, providing a benchmark for assessing image restoration algorithms on hazy images. | Used for denoising, focusing on urban scenes to evaluate the method's performance in complex environments. | Used for denoising, assessing the performance of the method on a smaller, curated set of images. | Used to conduct an ablation study on the effectiveness of the network structure, specifically evaluating the impact of removing the DCN and SFT layers. | Used to provide a large set of clean natural images for training and evaluation in image restoration tasks, focusing on the quality and diversity of natural scenes. | Used for deraining evaluations, providing a benchmark for assessing image restoration algorithms on rainy images. | Used for deraining, assessing the method's capability to remove rain streaks from images. | Used for denoising evaluations, providing a benchmark for assessing image restoration algorithms on urban scenes.",
          "citing_paper_id": "250551851",
          "cited_paper_id": 64193,
          "context_text": "For image denoising, we use the combination of BSD400 and WED as training set, and that of BSD68 and Urban100 as testing sets like [55].",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions specific datasets used for training and testing in the context of image denoising. These datasets are clearly identified and used for the purpose of training and evaluation.",
          "citing_paper_doi": "10.1109/CVPR52688.2022.01693",
          "cited_paper_doi": "10.1109/ICCV.2001.937655",
          "citing_paper_url": "https://www.semanticscholar.org/paper/c85f67a49cfdc29a77bd741a59e739cdc814edde",
          "cited_paper_url": "https://www.semanticscholar.org/paper/9a1ed876196ec9733acb1daa6d65e35ff0414291",
          "citing_paper_year": 2022,
          "cited_paper_year": 2001
        },
        {
          "dataset_name": "BSD400",
          "dataset_description": "Used for image deraining experiments, consisting of 200 rainy-clean training pairs and 100 testing image pairs. | Used to evaluate image deraining performance, focusing on quantitative results and comparing different deraining methods. | Used for denoising, focusing on urban scenes to evaluate the method's performance in complex environments. | Used for denoising, assessing the performance of the method on a smaller, curated set of images. | Used for denoising, testing the method's ability to handle various types of noise in diverse images. | Used for denoising evaluations, providing a benchmark for assessing image restoration algorithms on web images. | Used for deraining evaluations, providing a benchmark for assessing image restoration algorithms on rainy images. | Used to evaluate state-of-the-art derain methods, focusing on performance metrics and visual quality improvements in rain removal from images. | Used for denoising, evaluating the effectiveness of the proposed method in removing noise from natural images. | Used for denoising evaluations, providing a benchmark for assessing image restoration algorithms on natural images. | Used for dehazing evaluations, providing a benchmark for assessing image restoration algorithms on hazy images. | Used for deraining, assessing the method's capability to remove rain streaks from images. | Used for dehazing, evaluating the method's effectiveness in enhancing visibility in hazy conditions. | Used for denoising evaluations, providing a benchmark for assessing image restoration algorithms on urban scenes.",
          "citing_paper_id": "250551851",
          "cited_paper_id": 73439498,
          "context_text": ", BSD400, BSD68 [31], WED [30], and Urban100 for denoising; Rain100L [49] for deraining; and RESIDE [22] for dehazing.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions several datasets used for specific image restoration tasks such as denoising, deraining, and dehazing. These datasets are clearly identified and used for evaluating the performance of the proposed methods.",
          "citing_paper_doi": "10.1109/CVPR52688.2022.01693",
          "cited_paper_doi": "10.1109/TPAMI.2019.2895793",
          "citing_paper_url": "https://www.semanticscholar.org/paper/c85f67a49cfdc29a77bd741a59e739cdc814edde",
          "cited_paper_url": "https://www.semanticscholar.org/paper/79890a7d61b082947e1300f60231336a53cc285c",
          "citing_paper_year": 2022,
          "cited_paper_year": 2020
        },
        {
          "dataset_name": "BSD400",
          "dataset_description": "Used for denoising, with 100 images for testing at a resampling ratio of × 3. | Used for deraining, with 200 images for training and 100 images for testing at a resampling ratio of × 120. | Used for denoising by adding Gaussian noise levels σ ∈ {15, 25, 50} to train models for image restoration. | Used for dehazing to train models for improving visibility in hazy images. | Used for denoising, with 68 images for testing at a resampling ratio of × 3. | Used for denoising, with 400 image pairs for training and 68 images for testing at a resampling ratio of × 3. | Used for dehazing, with 72135 images for training and 500 images for testing at a resampling ratio of × 1. | Used for deraining to train models for removing rain streaks from images.",
          "citing_paper_id": "266149517",
          "cited_paper_id": 4840263,
          "context_text": "The training datasets include BSD400 [3] and WED [38] for denoising by adding Gaussian noise levels σ ∈ { 15 , 25 , 50 } , Rain100L [64] for deraining, and SOTS [28] for dehazing.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions specific datasets used for training in image restoration tasks, including denoising, deraining, and dehazing.",
          "citing_paper_doi": "10.48550/arXiv.2312.05038",
          "cited_paper_doi": "10.1109/TIP.2016.2631888",
          "citing_paper_url": "https://www.semanticscholar.org/paper/38496bf5e8fd6f16ddf36578586b08a8225a4aa2",
          "cited_paper_url": "https://www.semanticscholar.org/paper/1e2fe67d117a19a59a4f82e4950499c5ee5a7473",
          "citing_paper_year": 2023,
          "cited_paper_year": 2017
        },
        {
          "dataset_name": "BSD400",
          "dataset_description": "Used for denoising, with 100 images for testing at a resampling ratio of × 3. | Used for deraining, with 200 images for training and 100 images for testing at a resampling ratio of × 120. | Used for denoising by adding Gaussian noise levels σ ∈ {15, 25, 50} to train models for image restoration. | Used for dehazing to train models for improving visibility in hazy images. | Used for denoising, with 68 images for testing at a resampling ratio of × 3. | Used for denoising, with 400 image pairs for training and 68 images for testing at a resampling ratio of × 3. | Used for dehazing, with 72135 images for training and 500 images for testing at a resampling ratio of × 1. | Used for deraining to train models for removing rain streaks from images.",
          "citing_paper_id": "266149517",
          "cited_paper_id": 206764694,
          "context_text": "The training datasets include BSD400 [3] and WED [38] for denoising by adding Gaussian noise levels σ ∈ { 15 , 25 , 50 } , Rain100L [64] for deraining, and SOTS [28] for dehazing.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions specific datasets used for training in image restoration tasks, including denoising, deraining, and dehazing.",
          "citing_paper_doi": "10.48550/arXiv.2312.05038",
          "cited_paper_doi": "10.1109/TPAMI.2010.161",
          "citing_paper_url": "https://www.semanticscholar.org/paper/38496bf5e8fd6f16ddf36578586b08a8225a4aa2",
          "cited_paper_url": "https://www.semanticscholar.org/paper/0e5a262bf59b68ba8a7a1103d16fa33a9f5ffc28",
          "citing_paper_year": 2023,
          "cited_paper_year": 2011
        },
        {
          "dataset_name": "BSD400",
          "dataset_description": "Used for training and testing image denoising, with 400 and 4744 pairs for training and 68 images for testing. | Used for training and testing image denoising, with 400 and 4744 pairs for training and 100 images for testing. | Used for deraining, with 200 images for training and 100 images for testing, focusing on removing rain streaks from images. | Used for denoising by adding Gaussian noise levels σ ∈ {15, 25, 50} to train models for image restoration. | Used for dehazing, with 72135 images for training and 500 images for testing, focusing on improving visibility in hazy conditions. | Used for training and testing image deblurring, with a large dataset for training and testing. | Used for training and testing image dehazing, with 72135 images for training and 500 images for testing. | Used for deblurring, with a large dataset for training, focusing on restoring sharpness to blurred images. | Used for dehazing to train models for improving visibility in hazy images. | Used for denoising, with 68 images for testing, focusing on image quality improvement. | Used for denoising, with 400 and 4744 pairs for training and 68 images for testing, focusing on image quality improvement. | Used for denoising, with 100 images for testing, focusing on image quality improvement. | Used for training and testing image deraining, with 200 images for training and 100 images for testing. | Used to evaluate the performance of PIP on heavy rain degradation, specifically assessing the method's ability to handle unseen noise levels and heavy rain conditions. | Used for deraining to train models for removing rain streaks from images.",
          "citing_paper_id": "266149517",
          "cited_paper_id": 219530930,
          "context_text": "The training datasets include BSD400 [3] and WED [38] for denoising by adding Gaussian noise levels σ ∈ { 15 , 25 , 50 } , Rain100L [64] for deraining, and SOTS [28] for dehazing.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions specific datasets used for training in image restoration tasks, including denoising, deraining, and dehazing.",
          "citing_paper_doi": "10.48550/arXiv.2312.05038",
          "cited_paper_doi": "10.1109/cvpr42600.2020.00583",
          "citing_paper_url": "https://www.semanticscholar.org/paper/38496bf5e8fd6f16ddf36578586b08a8225a4aa2",
          "cited_paper_url": "https://www.semanticscholar.org/paper/c209d9c0d49b2377860acad2acbcc13523a40b7f",
          "citing_paper_year": 2023,
          "cited_paper_year": 2020
        },
        {
          "dataset_name": "BSD400",
          "dataset_description": "Used for image denoising, specifically by adding Gaussian noise at levels σ ∈ 15, 25, 50 to create noisy images for training.",
          "citing_paper_id": "268856875",
          "cited_paper_id": 4840263,
          "context_text": "Specifically, for image denoising, we combine the BSD400 [5] (400 training images) and WED [29] (4,744 images) datasets, adding Gaussian noise at levels σ ∈ 15 , 25 , 50 to create noisy images.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions two specific datasets, BSD400 and WED, which are used for image denoising by adding Gaussian noise at specific levels.",
          "citing_paper_doi": "10.48550/arXiv.2404.02154",
          "cited_paper_doi": "10.1109/TIP.2016.2631888",
          "citing_paper_url": "https://www.semanticscholar.org/paper/1d99d28c3cf5a21671a9cf5de8b51009c72d4624",
          "cited_paper_url": "https://www.semanticscholar.org/paper/1e2fe67d117a19a59a4f82e4950499c5ee5a7473",
          "citing_paper_year": 2024,
          "cited_paper_year": 2017
        },
        {
          "dataset_name": "BSD400",
          "dataset_description": "Used for training in the single-task setting of image denoising, providing a diverse set of images to improve model robustness. | Used for evaluating image quality assessment models, providing a diverse set of images to test model performance. | Used for training in the single-task setting of image denoising, offering additional challenges and variations to enhance model performance. | Used for training image restoration models, focusing on improving image quality and restoration accuracy.",
          "citing_paper_id": "259224666",
          "cited_paper_id": 4840263,
          "context_text": "For image denoising in the single-task setting, we use a combined set of BSD400 [1] and WED [40] datasets for training.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions two specific datasets, BSD400 and WED, which are used for training in the image denoising task. Both names are specific and plausible, fitting the criteria for inclusion.",
          "citing_paper_doi": "10.48550/arXiv.2306.13090",
          "cited_paper_doi": "10.1109/TIP.2016.2631888",
          "citing_paper_url": "https://www.semanticscholar.org/paper/c17f50017272c908cebdf2181675b7c6406b7218",
          "cited_paper_url": "https://www.semanticscholar.org/paper/1e2fe67d117a19a59a4f82e4950499c5ee5a7473",
          "citing_paper_year": 2023,
          "cited_paper_year": 2017
        },
        {
          "dataset_name": "BSD400",
          "dataset_description": "Used for image restoration, including 100 urban scene images to evaluate restoration techniques. | Used for testing denoising models, evaluating performance on a standardized set of images. | Used for image quality assessment, containing 4,744 images to test and validate image restoration models. | Used for training denoising models, offering web images to enhance real-world applicability. | Used for image restoration, comprising 68 clean natural images to assess restoration performance. | Used for image restoration, consisting of 400 clean natural images to evaluate restoration algorithms. | Used for testing denoising models, assessing performance on high-resolution urban scenes. | Used for training denoising models, providing a diverse set of images to improve generalization.",
          "citing_paper_id": "264305756",
          "cited_paper_id": 4840263,
          "context_text": "For denoising, we take the widely-used BSD400 [61] and WEB [70] datasets as training sets, and the BSD68 [61] and Urban100 [62] datasets as testing sets.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions specific datasets used for training and testing in the context of image denoising. These datasets are clearly identified and their usage is described.",
          "citing_paper_doi": "10.1109/TIP.2024.3456583",
          "cited_paper_doi": "10.1109/TIP.2016.2631888",
          "citing_paper_url": "https://www.semanticscholar.org/paper/a9eb8bd7057c82150b7d4cafb073ea4f2cb93636",
          "cited_paper_url": "https://www.semanticscholar.org/paper/1e2fe67d117a19a59a4f82e4950499c5ee5a7473",
          "citing_paper_year": 2023,
          "cited_paper_year": 2017
        },
        {
          "dataset_name": "BSD400",
          "dataset_description": "Used for color image denoising, offering a diverse set of images to test and validate denoising techniques. | Used for color image denoising, providing a benchmark for evaluating denoising algorithms in the context of image restoration. | Used for image deraining, specifically to assess the effectiveness of deraining methods in removing rain streaks from images. | Used for image dehazing, to evaluate the performance of dehazing algorithms in improving visibility in hazy images.",
          "citing_paper_id": "266690738",
          "cited_paper_id": 195787503,
          "context_text": "Following [73], we adopt BSD400 [6] and WED [64] for color image denoising, Rain100L [28] for image deraining, and SOTS [50] for image dehazing.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The citation mentions specific datasets used for different tasks in image restoration, which aligns with the research topic of All-in-One Image Restoration.",
          "citing_paper_doi": "10.1109/CVPR52733.2024.00277",
          "cited_paper_doi": "10.1109/TPAMI.2019.2925793",
          "citing_paper_url": "https://www.semanticscholar.org/paper/c6495e41d3974476e0be9d5137c3dda51125bcec",
          "cited_paper_url": "https://www.semanticscholar.org/paper/40762c365a3ef8dac29d499426669d97fea4f0d2",
          "citing_paper_year": 2023,
          "cited_paper_year": 2019
        },
        {
          "dataset_name": "BSD400",
          "dataset_description": "Used for color image denoising, offering a diverse set of images to test and validate denoising techniques. | Used for color image denoising, providing a benchmark for evaluating denoising algorithms in the context of image restoration. | Used for image deraining, specifically to assess the effectiveness of deraining methods in removing rain streaks from images. | Used for image dehazing, to evaluate the performance of dehazing algorithms in improving visibility in hazy images.",
          "citing_paper_id": "266690738",
          "cited_paper_id": 259224666,
          "context_text": "Following [73], we adopt BSD400 [6] and WED [64] for color image denoising, Rain100L [28] for image deraining, and SOTS [50] for image dehazing.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The citation mentions specific datasets used for different tasks in image restoration, which aligns with the research topic of All-in-One Image Restoration.",
          "citing_paper_doi": "10.1109/CVPR52733.2024.00277",
          "cited_paper_doi": "10.48550/arXiv.2306.13090",
          "citing_paper_url": "https://www.semanticscholar.org/paper/c6495e41d3974476e0be9d5137c3dda51125bcec",
          "cited_paper_url": "https://www.semanticscholar.org/paper/c17f50017272c908cebdf2181675b7c6406b7218",
          "citing_paper_year": 2023,
          "cited_paper_year": 2023
        },
        {
          "dataset_name": "BSD400",
          "dataset_description": "Used for training image restoration models, focusing on contour detection and hierarchical segmentation. | Used for training image restoration models, providing diverse images for enhancing model generalization.",
          "citing_paper_id": "267320695",
          "cited_paper_id": 206764694,
          "context_text": "We use a combination of BSD400 [2] and WED [50] datasets for training.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions two specific datasets, BSD400 and WED, which are used for training in the research.",
          "citing_paper_doi": "10.1007/978-3-031-72764-1_1",
          "cited_paper_doi": "10.1109/TPAMI.2010.161",
          "citing_paper_url": "https://www.semanticscholar.org/paper/03ad1a40a4399c8b77bbeaa389fcd14b10b322c0",
          "cited_paper_url": "https://www.semanticscholar.org/paper/0e5a262bf59b68ba8a7a1103d16fa33a9f5ffc28",
          "citing_paper_year": 2024,
          "cited_paper_year": 2011
        },
        {
          "dataset_name": "BSD400",
          "dataset_description": "Used for training image denoising models, focusing on improving denoising performance through a diverse set of images. | Used for training image denoising models, providing a variety of watermarked and enhanced images to improve robustness.",
          "citing_paper_id": "271274210",
          "cited_paper_id": 4840263,
          "context_text": "Specifically, for image denoising, we employ BSD400 [76] and WED [77] for training.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions two specific datasets, BSD400 and WED, which are used for training in the context of image denoising.",
          "citing_paper_doi": "10.1109/TCSVT.2024.3429557",
          "cited_paper_doi": "10.1109/TIP.2016.2631888",
          "citing_paper_url": "https://www.semanticscholar.org/paper/1091511c40eebb0f8e964246c91e91a85480dbf9",
          "cited_paper_url": "https://www.semanticscholar.org/paper/1e2fe67d117a19a59a4f82e4950499c5ee5a7473",
          "citing_paper_year": 2024,
          "cited_paper_year": 2017
        },
        {
          "dataset_name": "BSD400",
          "dataset_description": "Used for image denoising, containing images with varying noise levels to test denoising performance. | Used for image denoising, offering a smaller but focused set of images for benchmarking. | Used for image deraining, providing synthetic rain images to assess deraining algorithms. | Used for training image de-noising models, offering additional challenges and variations in image quality. | Used for image dehazing, containing hazy images to evaluate dehazing techniques. | Used for training image de-noising models, providing a large set of images for algorithm development. | Used as the testing set for evaluating image de-noising models, containing 68 ground truth images for performance assessment. | Used for image denoising, providing a diverse set of images to evaluate denoising algorithms.",
          "citing_paper_id": "267199899",
          "cited_paper_id": 4840263,
          "context_text": "Consequently, our experimentation necessitates the utilization of five corresponding datasets, which comprise BSD400, BSD68 [22], WED [21] for the purpose of image denoising, Rain100L [30] for image deraining, and RESIDE [17] for image dehazing.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context clearly mentions five datasets used for specific image restoration tasks, which align with the research topic of all-in-one image restoration.",
          "citing_paper_doi": "10.48550/arXiv.2401.13221",
          "cited_paper_doi": "10.1109/TIP.2016.2631888",
          "citing_paper_url": "https://www.semanticscholar.org/paper/a41198991cb411e9dbcbedf637b974bb0c326e45",
          "cited_paper_url": "https://www.semanticscholar.org/paper/1e2fe67d117a19a59a4f82e4950499c5ee5a7473",
          "citing_paper_year": 2024,
          "cited_paper_year": 2017
        },
        {
          "dataset_name": "BSD400",
          "dataset_description": "Used for image denoising, containing images with varying noise levels to test denoising performance. | Used for image denoising, offering a smaller but focused set of images for benchmarking. | Used for image deraining, providing synthetic rain images to assess deraining algorithms. | Used for image dehazing, containing hazy images to evaluate dehazing techniques. | Used for training and testing image deraining models, consisting of 200 rainy-clean training pairs and 100 testing image pairs. | Used for image denoising, providing a diverse set of images to evaluate denoising algorithms.",
          "citing_paper_id": "267199899",
          "cited_paper_id": 73439498,
          "context_text": "Consequently, our experimentation necessitates the utilization of five corresponding datasets, which comprise BSD400, BSD68 [22], WED [21] for the purpose of image denoising, Rain100L [30] for image deraining, and RESIDE [17] for image dehazing.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context clearly mentions five datasets used for specific image restoration tasks, which align with the research topic of all-in-one image restoration.",
          "citing_paper_doi": "10.48550/arXiv.2401.13221",
          "cited_paper_doi": "10.1109/TPAMI.2019.2895793",
          "citing_paper_url": "https://www.semanticscholar.org/paper/a41198991cb411e9dbcbedf637b974bb0c326e45",
          "cited_paper_url": "https://www.semanticscholar.org/paper/79890a7d61b082947e1300f60231336a53cc285c",
          "citing_paper_year": 2024,
          "cited_paper_year": 2020
        },
        {
          "dataset_name": "BSD400",
          "dataset_description": "Used for training on deraining tasks. This dataset focuses on removing rain streaks from images, enhancing the clarity of rainy scenes. | Used for training on Gaussian denoising with noise levels of 15, 25, and 50. This dataset helps in evaluating the model's ability to handle various noise intensities. | Used for training on dehazing tasks. This dataset is designed to improve image quality by reducing haze and increasing visibility in hazy conditions. | Used for training on Gaussian denoising with noise levels of 15, 25, and 50. This dataset complements BSD400 in assessing the model's denoising capabilities.",
          "citing_paper_id": "273323238",
          "cited_paper_id": 4840263,
          "context_text": "The training set of UIRD-12 closely follows previous All-in-One works [37, 52]: BSD400 [1] and WED [46] datasets for training on Gaussian denoising ( σ = { 15 , 25 , 50 }); Rain100L dataset [68] for derain; SOTS dataset [36] for dehaze.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions specific datasets used for training in the All-in-One Image Restoration domain, including BSD400, WED, Rain100L, and SOTS. These datasets are clearly identified and their usage is described.",
          "citing_paper_doi": "10.48550/arXiv.2410.08688",
          "cited_paper_doi": "10.1109/TIP.2016.2631888",
          "citing_paper_url": "https://www.semanticscholar.org/paper/3b58b1884ffa794d8095cbf98ebe012c0735a61e",
          "cited_paper_url": "https://www.semanticscholar.org/paper/1e2fe67d117a19a59a4f82e4950499c5ee5a7473",
          "citing_paper_year": 2024,
          "cited_paper_year": 2017
        },
        {
          "dataset_name": "BSD400",
          "dataset_description": "Used for training on dehazing tasks. This dataset is designed to improve image quality by reducing haze and increasing visibility in hazy conditions. | Used for training on Gaussian denoising with noise levels of 15, 25, and 50. This dataset helps in evaluating the model's ability to handle various noise intensities. | Used for training on Gaussian denoising with noise levels of 15, 25, and 50. This dataset complements BSD400 in assessing the model's denoising capabilities. | Used for training on deraining tasks. This dataset focuses on removing rain streaks from images, enhancing the clarity of rainy scenes. | Used to test and synthesize image degradations, focusing on benchmarking single-image dehazing and other restoration tasks.",
          "citing_paper_id": "273323238",
          "cited_paper_id": 39760169,
          "context_text": "The training set of UIRD-12 closely follows previous All-in-One works [37, 52]: BSD400 [1] and WED [46] datasets for training on Gaussian denoising ( σ = { 15 , 25 , 50 }); Rain100L dataset [68] for derain; SOTS dataset [36] for dehaze.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions specific datasets used for training in the All-in-One Image Restoration domain, including BSD400, WED, Rain100L, and SOTS. These datasets are clearly identified and their usage is described.",
          "citing_paper_doi": "10.48550/arXiv.2410.08688",
          "cited_paper_doi": "10.1109/TIP.2018.2867951",
          "citing_paper_url": "https://www.semanticscholar.org/paper/3b58b1884ffa794d8095cbf98ebe012c0735a61e",
          "cited_paper_url": "https://www.semanticscholar.org/paper/abf75b81b96ff2cd262086675deba55f907108db",
          "citing_paper_year": 2024,
          "cited_paper_year": 2017
        },
        {
          "dataset_name": "BSD400",
          "dataset_description": "Used for training on deraining tasks. This dataset focuses on removing rain streaks from images, enhancing the clarity of rainy scenes. | Used for training on Gaussian denoising with noise levels of 15, 25, and 50. This dataset helps in evaluating the model's ability to handle various noise intensities. | Used for training on dehazing tasks. This dataset is designed to improve image quality by reducing haze and increasing visibility in hazy conditions. | Used for training on Gaussian denoising with noise levels of 15, 25, and 50. This dataset complements BSD400 in assessing the model's denoising capabilities.",
          "citing_paper_id": "273323238",
          "cited_paper_id": 206764694,
          "context_text": "The training set of UIRD-12 closely follows previous All-in-One works [37, 52]: BSD400 [1] and WED [46] datasets for training on Gaussian denoising ( σ = { 15 , 25 , 50 }); Rain100L dataset [68] for derain; SOTS dataset [36] for dehaze.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions specific datasets used for training in the All-in-One Image Restoration domain, including BSD400, WED, Rain100L, and SOTS. These datasets are clearly identified and their usage is described.",
          "citing_paper_doi": "10.48550/arXiv.2410.08688",
          "cited_paper_doi": "10.1109/TPAMI.2010.161",
          "citing_paper_url": "https://www.semanticscholar.org/paper/3b58b1884ffa794d8095cbf98ebe012c0735a61e",
          "cited_paper_url": "https://www.semanticscholar.org/paper/0e5a262bf59b68ba8a7a1103d16fa33a9f5ffc28",
          "citing_paper_year": 2024,
          "cited_paper_year": 2011
        },
        {
          "dataset_name": "BSD400",
          "dataset_description": "Used for training on dehazing tasks. This dataset is designed to improve image quality by reducing haze and increasing visibility in hazy conditions. | Used for training on Gaussian denoising with noise levels of 15, 25, and 50. This dataset helps in evaluating the model's ability to handle various noise intensities. | Used for training on Gaussian denoising with noise levels of 15, 25, and 50. This dataset complements BSD400 in assessing the model's denoising capabilities. | Used for training on deraining tasks. This dataset focuses on removing rain streaks from images, enhancing the clarity of rainy scenes. | Used to evaluate various One-to-Many and One-to-Composite image restoration methods, focusing on performance across different types of image degradation.",
          "citing_paper_id": "273323238",
          "cited_paper_id": 250551851,
          "context_text": "The training set of UIRD-12 closely follows previous All-in-One works [37, 52]: BSD400 [1] and WED [46] datasets for training on Gaussian denoising ( σ = { 15 , 25 , 50 }); Rain100L dataset [68] for derain; SOTS dataset [36] for dehaze.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions specific datasets used for training in the All-in-One Image Restoration domain, including BSD400, WED, Rain100L, and SOTS. These datasets are clearly identified and their usage is described.",
          "citing_paper_doi": "10.48550/arXiv.2410.08688",
          "cited_paper_doi": "10.1109/CVPR52688.2022.01693",
          "citing_paper_url": "https://www.semanticscholar.org/paper/3b58b1884ffa794d8095cbf98ebe012c0735a61e",
          "cited_paper_url": "https://www.semanticscholar.org/paper/c85f67a49cfdc29a77bd741a59e739cdc814edde",
          "citing_paper_year": 2024,
          "cited_paper_year": 2022
        },
        {
          "dataset_name": "BSD400",
          "dataset_description": "Used for training on dehazing tasks. This dataset is designed to improve image quality by reducing haze and increasing visibility in hazy conditions. | Used for training on Gaussian denoising with noise levels of 15, 25, and 50. This dataset helps in evaluating the model's ability to handle various noise intensities. | Used for training on Gaussian denoising with noise levels of 15, 25, and 50. This dataset complements BSD400 in assessing the model's denoising capabilities. | Used for training on deraining tasks. This dataset focuses on removing rain streaks from images, enhancing the clarity of rainy scenes. | Used to evaluate various One-to-Many and One-to-Composite image restoration methods, focusing on performance across different types of image degradation.",
          "citing_paper_id": "273323238",
          "cited_paper_id": 268064460,
          "context_text": "The training set of UIRD-12 closely follows previous All-in-One works [37, 52]: BSD400 [1] and WED [46] datasets for training on Gaussian denoising ( σ = { 15 , 25 , 50 }); Rain100L dataset [68] for derain; SOTS dataset [36] for dehaze.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions specific datasets used for training in the All-in-One Image Restoration domain, including BSD400, WED, Rain100L, and SOTS. These datasets are clearly identified and their usage is described.",
          "citing_paper_doi": "10.48550/arXiv.2410.08688",
          "cited_paper_doi": null,
          "citing_paper_url": "https://www.semanticscholar.org/paper/3b58b1884ffa794d8095cbf98ebe012c0735a61e",
          "cited_paper_url": "https://www.semanticscholar.org/paper/20d4e85814f64977590bc9276eec84203f0fcf9b",
          "citing_paper_year": 2024,
          "cited_paper_year": 2023
        },
        {
          "dataset_name": "BSD400",
          "dataset_description": "Used for image denoising by adding Gaussian noise at levels σ ∈ [15, 25, 50] to create noisy images, enhancing the robustness of the restoration model.",
          "citing_paper_id": "274305801",
          "cited_paper_id": 4840263,
          "context_text": "For all-in-one, we follow existing work [24, 36] and include following datasets: For image denoising, we combine the BSD400 [2] and WED [31] datasets, adding Gaussian noise at levels σ ∈ [15 , 25 , 50] to create noisy images.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions two specific datasets, BSD400 and WED, which are used for image denoising by adding Gaussian noise at specified levels.",
          "citing_paper_doi": "10.48550/arXiv.2411.18466",
          "cited_paper_doi": "10.1109/TIP.2016.2631888",
          "citing_paper_url": "https://www.semanticscholar.org/paper/2850f4c1f43789f0a8a48e7bb83cf2a5e0561e54",
          "cited_paper_url": "https://www.semanticscholar.org/paper/1e2fe67d117a19a59a4f82e4950499c5ee5a7473",
          "citing_paper_year": 2024,
          "cited_paper_year": 2017
        },
        {
          "dataset_name": "BSD400",
          "dataset_description": "Used for image denoising by adding Gaussian noise at levels σ ∈ [15, 25, 50] to create noisy images, enhancing the robustness of the restoration model.",
          "citing_paper_id": "274305801",
          "cited_paper_id": 55358798,
          "context_text": "For all-in-one, we follow existing work [24, 36] and include following datasets: For image denoising, we combine the BSD400 [2] and WED [31] datasets, adding Gaussian noise at levels σ ∈ [15 , 25 , 50] to create noisy images.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions two specific datasets, BSD400 and WED, which are used for image denoising by adding Gaussian noise at specified levels.",
          "citing_paper_doi": "10.48550/arXiv.2411.18466",
          "cited_paper_doi": null,
          "citing_paper_url": "https://www.semanticscholar.org/paper/2850f4c1f43789f0a8a48e7bb83cf2a5e0561e54",
          "cited_paper_url": "https://www.semanticscholar.org/paper/ab17b1ba06c179395645f8fef7fd0e2982d684d4",
          "citing_paper_year": 2024,
          "cited_paper_year": 2013
        },
        {
          "dataset_name": "BSD400",
          "dataset_description": "Used for image denoising by adding Gaussian noise at levels σ ∈ [15, 25, 50] to create noisy images, enhancing the robustness of the restoration model.",
          "citing_paper_id": "274305801",
          "cited_paper_id": 206764694,
          "context_text": "For all-in-one, we follow existing work [24, 36] and include following datasets: For image denoising, we combine the BSD400 [2] and WED [31] datasets, adding Gaussian noise at levels σ ∈ [15 , 25 , 50] to create noisy images.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions two specific datasets, BSD400 and WED, which are used for image denoising by adding Gaussian noise at specified levels.",
          "citing_paper_doi": "10.48550/arXiv.2411.18466",
          "cited_paper_doi": "10.1109/TPAMI.2010.161",
          "citing_paper_url": "https://www.semanticscholar.org/paper/2850f4c1f43789f0a8a48e7bb83cf2a5e0561e54",
          "cited_paper_url": "https://www.semanticscholar.org/paper/0e5a262bf59b68ba8a7a1103d16fa33a9f5ffc28",
          "citing_paper_year": 2024,
          "cited_paper_year": 2011
        },
        {
          "dataset_name": "BSD400",
          "dataset_description": "Used for image denoising by adding Gaussian noise at levels σ ∈ [15, 25, 50] to create noisy images, enhancing the robustness of the restoration model.",
          "citing_paper_id": "274305801",
          "cited_paper_id": 250551851,
          "context_text": "For all-in-one, we follow existing work [24, 36] and include following datasets: For image denoising, we combine the BSD400 [2] and WED [31] datasets, adding Gaussian noise at levels σ ∈ [15 , 25 , 50] to create noisy images.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions two specific datasets, BSD400 and WED, which are used for image denoising by adding Gaussian noise at specified levels.",
          "citing_paper_doi": "10.48550/arXiv.2411.18466",
          "cited_paper_doi": "10.1109/CVPR52688.2022.01693",
          "citing_paper_url": "https://www.semanticscholar.org/paper/2850f4c1f43789f0a8a48e7bb83cf2a5e0561e54",
          "cited_paper_url": "https://www.semanticscholar.org/paper/c85f67a49cfdc29a77bd741a59e739cdc814edde",
          "citing_paper_year": 2024,
          "cited_paper_year": 2022
        },
        {
          "dataset_name": "BSD400",
          "dataset_description": "Used for image denoising by adding Gaussian noise at levels σ ∈ [15, 25, 50] to create noisy images, enhancing the robustness of the restoration model.",
          "citing_paper_id": "274305801",
          "cited_paper_id": 268030812,
          "context_text": "For all-in-one, we follow existing work [24, 36] and include following datasets: For image denoising, we combine the BSD400 [2] and WED [31] datasets, adding Gaussian noise at levels σ ∈ [15 , 25 , 50] to create noisy images.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions two specific datasets, BSD400 and WED, which are used for image denoising by adding Gaussian noise at specified levels.",
          "citing_paper_doi": "10.48550/arXiv.2411.18466",
          "cited_paper_doi": null,
          "citing_paper_url": "https://www.semanticscholar.org/paper/2850f4c1f43789f0a8a48e7bb83cf2a5e0561e54",
          "cited_paper_url": "https://www.semanticscholar.org/paper/caabf4abb96ee83f650aa7b707bd9a99007b3e74",
          "citing_paper_year": 2024,
          "cited_paper_year": 2023
        },
        {
          "dataset_name": "BSD400",
          "dataset_description": "Used for training a single-task image denoising model, providing 400 training images to enhance model performance. | Used for training a single-task image denoising model, contributing 4,744 images to improve model robustness. | Used to generate noisy images for image restoration experiments by adding Gaussian noise with varying levels (σ ∈ {15, 25, 50}).",
          "citing_paper_id": "268553835",
          "cited_paper_id": 4840263,
          "context_text": "Starting from these clean images of BSD400 [2] and WED [40], we generate their corresponding noisy versions by adding Gaussian noise with varying levels ( σ ∈ { 15 , 25 , 50 } ).",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions BSD400 and WED as datasets used to generate noisy images for image restoration experiments.",
          "citing_paper_doi": "10.48550/arXiv.2403.14614",
          "cited_paper_doi": "10.1109/TIP.2016.2631888",
          "citing_paper_url": "https://www.semanticscholar.org/paper/8cf850abfec7ce894b498ef082282382e8d3374c",
          "cited_paper_url": "https://www.semanticscholar.org/paper/1e2fe67d117a19a59a4f82e4950499c5ee5a7473",
          "citing_paper_year": 2024,
          "cited_paper_year": 2017
        },
        {
          "dataset_name": "BSD400",
          "dataset_description": "Used to evaluate super-resolution and deblurring techniques on urban scenes. | Used to test rain removal algorithms, focusing on light rain conditions. | Used to evaluate haze removal techniques, focusing on outdoor real-world scenes. | Used to evaluate low-light image enhancement methods, focusing on improving visibility in dark conditions. | Used to test deblurring algorithms, particularly for motion blur in video frames. | Used to test image restoration algorithms, emphasizing urban scenes with rain, noise, blur, and low-light degradations. | Used to assess image restoration performance, particularly under rain, noise, blur, and low-light degradations. | Used to assess image restoration quality, particularly for wide exposure differences. | Used to assess haze removal methods, specifically for outdoor synthetic scenes. | Used to assess dehazing effectiveness on outdoor real-world images. | Used to evaluate image restoration methods, focusing on degradations such as rain, haze, noise, blur, and low-light conditions. | Used to evaluate dehazing performance on outdoor synthetic images. | Used to evaluate image denoising performance, focusing on noise reduction at σ 50. | Used to evaluate rain removal techniques, focusing on heavy rain conditions in images.",
          "citing_paper_id": "273502246",
          "cited_paper_id": 267320695,
          "context_text": "…5 Rain, Haze, Noise-σ 25 , Blur, Low-Light BSD400, WED, Urban100, Rain100L, RESIDE-OTS, RESIDE-SOTS, Gopro, LOL IDR [66], AdaIR [78], InstructIR [62], PIP [72], DaAIR [187], AnyIR [188], MEASNet [110], HAIR [189], Perceive-IR [111] rain-noise-blur-dark[61] 4 Rain, Noise, Blur, Low-light SIDD,…",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions multiple datasets used for image restoration tasks, including BSD400, WED, Urban100, Rain100L, RESIDE-OTS, RESIDE-SOTS, Gopro, and LOL. These datasets are specifically used for evaluating image restoration methods under various degradations.",
          "citing_paper_doi": "10.48550/arXiv.2410.15067",
          "cited_paper_doi": "10.1007/978-3-031-72764-1_1",
          "citing_paper_url": "https://www.semanticscholar.org/paper/f5ff17162a4936db3f64ed7bafae13abcfa63a27",
          "cited_paper_url": "https://www.semanticscholar.org/paper/03ad1a40a4399c8b77bbeaa389fcd14b10b322c0",
          "citing_paper_year": 2024,
          "cited_paper_year": 2024
        },
        {
          "dataset_name": "BSD400",
          "dataset_description": "Used for evaluating image restoration models, focusing on blur degradation. The dataset provides high-quality images for benchmarking. | Used for evaluating image restoration models, focusing on rain removal. The dataset includes images with varying levels of rain intensity. | Used for evaluating image restoration models, focusing on dehazing. The dataset provides outdoor scenes with real-world haze. | Used for evaluating image restoration models, focusing on motion deblurring. The dataset includes sharp and blurred image pairs. | Used for evaluating image restoration models, focusing on denoising. The dataset includes raw and noisy image pairs. | Used for evaluating image restoration models, focusing on urban scenes. The dataset provides high-resolution images of urban environments. | Used for evaluating image restoration models, focusing on various degradations. The dataset includes images with multiple types of distortions. | Used to evaluate image restoration models, focusing on various degradations including rain, haze, noise, blur, and low-light conditions. | Used for evaluating image restoration models, focusing on dehazing. The dataset provides outdoor scenes with synthetic haze. | Used for evaluating image restoration models, focusing on low-light enhancement. The dataset provides low-light and normal-light image pairs.",
          "citing_paper_id": "273502246",
          "cited_paper_id": 270045495,
          "context_text": "…RESIDE-SOTS AirNet [63], PromptIR [65],PIP [72], Textpromp-tIR [94], NDR [96], InstructIR [62], AdaIR [78], U-WADN [186], DyNet [185], DaAIR [187], AnyIR [188], MEASNet [110], HAIR [189], Perceive-IR [111] rain-haze-noise-blur-dark[66] 5 Rain, Haze, Noise-σ 25 , Blur, Low-Light BSD400, WED,…",
          "confidence_score": 0.9,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions several datasets and methods, but only 'BSD400' and 'WED' are specific datasets. Other items are either methods or unclear references.",
          "citing_paper_doi": "10.48550/arXiv.2410.15067",
          "cited_paper_doi": "10.48550/arXiv.2405.15475",
          "citing_paper_url": "https://www.semanticscholar.org/paper/f5ff17162a4936db3f64ed7bafae13abcfa63a27",
          "cited_paper_url": "https://www.semanticscholar.org/paper/1dc809107ceb27fc74071934878fe2dc9292e57d",
          "citing_paper_year": 2024,
          "cited_paper_year": 2024
        },
        {
          "dataset_name": "BSD400",
          "dataset_description": "Used for training image denoising models, providing 4,744 high-quality images to enhance model performance. | Used for training image denoising models, providing 400 high-quality images to enhance model performance.",
          "citing_paper_id": "278528939",
          "cited_paper_id": 4840263,
          "context_text": "For image denoising, we combine the BSD400 [51] and WED [52] datasets for training, which include 400 and 4,744 high-quality images, respectively.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions two specific datasets, BSD400 and WED, which are used for training in the context of image denoising.",
          "citing_paper_doi": "10.1109/TIP.2025.3567205",
          "cited_paper_doi": "10.1109/TIP.2016.2631888",
          "citing_paper_url": "https://www.semanticscholar.org/paper/99b442f68dce288fd1f5d30858f7ac8b8d0068e9",
          "cited_paper_url": "https://www.semanticscholar.org/paper/1e2fe67d117a19a59a4f82e4950499c5ee5a7473",
          "citing_paper_year": 2025,
          "cited_paper_year": 2017
        }
      ]
    },
    {
      "cited_paper_id": "234357997",
      "citation_count": 0,
      "total_dataset_mentions": 27,
      "unique_datasets": [
        "ImageNet"
      ],
      "dataset_details": [
        {
          "dataset_name": "ImageNet",
          "dataset_description": "Used to train DiffBIR at 512x512 resolution for image restoration, focusing on the quality and effectiveness of the restoration process.",
          "citing_paper_id": "261276317",
          "cited_paper_id": 57246310,
          "context_text": "We train DiffBIR on the ImageNet [11] dataset at 512× 512 resolution for BIR.",
          "confidence_score": 1.0,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions the ImageNet dataset, which is a well-known image dataset used for training and evaluation in computer vision tasks. The dataset is specifically used for training DiffBIR at a high resolution for image restoration.",
          "citing_paper_doi": "10.48550/arXiv.2308.15070",
          "cited_paper_doi": "10.1109/CVPR.2009.5206848",
          "citing_paper_url": "https://www.semanticscholar.org/paper/858f0643110ccccb6a9103747f2169fecfb98668",
          "cited_paper_url": "https://www.semanticscholar.org/paper/d2c733e34d48784a37d717fe43d9e93277a8c53e",
          "citing_paper_year": 2023,
          "cited_paper_year": 2009
        },
        {
          "dataset_name": "ImageNet",
          "dataset_description": "Used for pretraining the image processing transformer model, focusing on improving performance in image restoration tasks.",
          "citing_paper_id": "268819753",
          "cited_paper_id": 227239228,
          "context_text": "IPT [9] has pioneered the application of the standard vision transformer architecture to image restoration, which achieves outstanding performance with pretrained on Ima-geNet dataset.",
          "confidence_score": 1.0,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions the ImageNet dataset, which is used for pretraining the model in the context of image restoration.",
          "citing_paper_doi": "10.48550/arXiv.2404.00633",
          "cited_paper_doi": "10.1109/CVPR46437.2021.01212",
          "citing_paper_url": "https://www.semanticscholar.org/paper/979e606d3f09415be6d520213eabf9bfacd9f8dd",
          "cited_paper_url": "https://www.semanticscholar.org/paper/6f6f73e69ee0d9d5d7d088bb882db1851d98175a",
          "citing_paper_year": 2024,
          "cited_paper_year": 2020
        },
        {
          "dataset_name": "ImageNet",
          "dataset_description": "Used to train image restoration networks, focusing on improving the quality of restored images through large-scale pre-training.",
          "citing_paper_id": "260843368",
          "cited_paper_id": 227239228,
          "context_text": "Recently, ImageNet [15] has also been used to train image restoration networks [11, 41].",
          "confidence_score": 1.0,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions ImageNet as a dataset used for training image restoration networks, which is directly relevant to the topic of all-in-one image restoration.",
          "citing_paper_doi": "10.1109/CVPRW59228.2023.00178",
          "cited_paper_doi": "10.1109/CVPR46437.2021.01212",
          "citing_paper_url": "https://www.semanticscholar.org/paper/851446ea3c70c918cdf8c62db0dd0b1900806e5f",
          "cited_paper_url": "https://www.semanticscholar.org/paper/6f6f73e69ee0d9d5d7d088bb882db1851d98175a",
          "citing_paper_year": 2023,
          "cited_paper_year": 2020
        },
        {
          "dataset_name": "ImageNet",
          "dataset_description": "Used to validate the performance of DDRM on a subset of 1000 validation samples, focusing on computational complexity and image restoration quality. | Used to compute the transport operator with two disjoint sets of 10 randomly picked images from the train split, focusing on image restoration techniques. | Used to report model performance on 50,000 validation samples, focusing on image restoration quality and comparing results across different models.",
          "citing_paper_id": "259075580",
          "cited_paper_id": 57246310,
          "context_text": "Hence, for all models except DDRM [3] and Swin2SR [13], we report the performance on the 50,000 validation samples of ImageNet [29] following [7, 35].",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions 'ImageNet' as a dataset used for reporting performance on 50,000 validation samples. The cited paper title confirms it is a dataset.",
          "citing_paper_doi": "10.48550/arXiv.2306.02342",
          "cited_paper_doi": "10.1109/CVPR.2009.5206848",
          "citing_paper_url": "https://www.semanticscholar.org/paper/e0c1ad4a0335682ccf71426225d80e4aa800e47e",
          "cited_paper_url": "https://www.semanticscholar.org/paper/d2c733e34d48784a37d717fe43d9e93277a8c53e",
          "citing_paper_year": 2023,
          "cited_paper_year": 2009
        },
        {
          "dataset_name": "ImageNet",
          "dataset_description": "Used to report model performance on 50,000 validation samples, focusing on image restoration quality and comparing results across different models.",
          "citing_paper_id": "259075580",
          "cited_paper_id": 233241040,
          "context_text": "Hence, for all models except DDRM [3] and Swin2SR [13], we report the performance on the 50,000 validation samples of ImageNet [29] following [7, 35].",
          "confidence_score": 1.0,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions 'ImageNet' as a dataset used for reporting performance on 50,000 validation samples. The cited paper title confirms it is a dataset.",
          "citing_paper_doi": "10.48550/arXiv.2306.02342",
          "cited_paper_doi": "10.1109/TPAMI.2022.3204461",
          "citing_paper_url": "https://www.semanticscholar.org/paper/e0c1ad4a0335682ccf71426225d80e4aa800e47e",
          "cited_paper_url": "https://www.semanticscholar.org/paper/bc7e6165b00f0c39d40ca2c7a4eb33fcc0e3200d",
          "citing_paper_year": 2023,
          "cited_paper_year": 2021
        },
        {
          "dataset_name": "ImageNet",
          "dataset_description": "Used to validate the performance of DDRM on a subset of 1000 validation samples, focusing on computational complexity and image restoration quality. | Used to report model performance on 50,000 validation samples, focusing on image restoration quality and comparing results across different models.",
          "citing_paper_id": "259075580",
          "cited_paper_id": 246411364,
          "context_text": "Hence, for all models except DDRM [3] and Swin2SR [13], we report the performance on the 50,000 validation samples of ImageNet [29] following [7, 35].",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions 'ImageNet' as a dataset used for reporting performance on 50,000 validation samples. The cited paper title confirms it is a dataset.",
          "citing_paper_doi": "10.48550/arXiv.2306.02342",
          "cited_paper_doi": null,
          "citing_paper_url": "https://www.semanticscholar.org/paper/e0c1ad4a0335682ccf71426225d80e4aa800e47e",
          "cited_paper_url": "https://www.semanticscholar.org/paper/3d3c5fcbc40aadccceda58d3d9c5cd00588ea0b7",
          "citing_paper_year": 2023,
          "cited_paper_year": 2022
        },
        {
          "dataset_name": "ImageNet",
          "dataset_description": "Used for large-scale pretraining in image restoration tasks, but requires further fine-tuning on specific high-quality restoration datasets.",
          "citing_paper_id": "259108489",
          "cited_paper_id": 227239228,
          "context_text": "Despite some existing restoration works [7, 29] utilizing ImageNet [21] for large-scale pretraining, they still require further fine-tuning on specific high-quality restoration datasets.",
          "confidence_score": 1.0,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions ImageNet as a dataset used for large-scale pretraining in image restoration works. No other specific datasets are mentioned.",
          "citing_paper_doi": "10.48550/arXiv.2306.05390",
          "cited_paper_doi": "10.1109/CVPR46437.2021.01212",
          "citing_paper_url": "https://www.semanticscholar.org/paper/4296bf82e67051e4cba10e510932f55fc12dd1e2",
          "cited_paper_url": "https://www.semanticscholar.org/paper/6f6f73e69ee0d9d5d7d088bb882db1851d98175a",
          "citing_paper_year": 2023,
          "cited_paper_year": 2020
        },
        {
          "dataset_name": "ImageNet",
          "dataset_description": "Used for large-scale pretraining in image restoration tasks, but requires further fine-tuning on specific high-quality restoration datasets.",
          "citing_paper_id": "259108489",
          "cited_paper_id": 263861862,
          "context_text": "Despite some existing restoration works [7, 29] utilizing ImageNet [21] for large-scale pretraining, they still require further fine-tuning on specific high-quality restoration datasets.",
          "confidence_score": 1.0,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions ImageNet as a dataset used for large-scale pretraining in image restoration works. No other specific datasets are mentioned.",
          "citing_paper_doi": "10.48550/arXiv.2306.05390",
          "cited_paper_doi": null,
          "citing_paper_url": "https://www.semanticscholar.org/paper/4296bf82e67051e4cba10e510932f55fc12dd1e2",
          "cited_paper_url": "https://www.semanticscholar.org/paper/f00050543bf3326722c0e926c64e249dfb8d7136",
          "citing_paper_year": 2023,
          "cited_paper_year": 2021
        },
        {
          "dataset_name": "ImageNet",
          "dataset_description": "Used to evaluate the empirical effectiveness of GDP compared to unsupervised methods, focusing on consistency and FID scores in image restoration.",
          "citing_paper_id": "257921922",
          "cited_paper_id": 8758543,
          "context_text": "We demonstrate the empirical effectiveness of GDP by comparing it with various competitive unsupervised methods under the linear or multi-linear inverse problem on ImageNet [14], LSUN [89], and CelebA [30] datasets in terms of consistency and FID.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions three specific datasets: ImageNet, LSUN, and CelebA. These are well-known image datasets used for evaluating image restoration and generation methods.",
          "citing_paper_doi": "10.1109/CVPR52729.2023.00958",
          "cited_paper_doi": null,
          "citing_paper_url": "https://www.semanticscholar.org/paper/5549dc3ceff07561d9fb59610c0f78c71617901a",
          "cited_paper_url": "https://www.semanticscholar.org/paper/9179e740dad4ca4c183f7677b854e5b15f9a122f",
          "citing_paper_year": 2023,
          "cited_paper_year": 2016
        },
        {
          "dataset_name": "ImageNet",
          "dataset_description": "Used to pre-train a DDPM model for producing diverse and high-fidelity outputs in an unsupervised manner for unified image restoration and enhancement. | Used to evaluate the empirical effectiveness of GDP compared to unsupervised methods, focusing on consistency and FID scores in image restoration.",
          "citing_paper_id": "257921922",
          "cited_paper_id": 234357997,
          "context_text": "We demonstrate the empirical effectiveness of GDP by comparing it with various competitive unsupervised methods under the linear or multi-linear inverse problem on ImageNet [14], LSUN [89], and CelebA [30] datasets in terms of consistency and FID.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions three specific datasets: ImageNet, LSUN, and CelebA. These are well-known image datasets used for evaluating image restoration and generation methods.",
          "citing_paper_doi": "10.1109/CVPR52729.2023.00958",
          "cited_paper_doi": null,
          "citing_paper_url": "https://www.semanticscholar.org/paper/5549dc3ceff07561d9fb59610c0f78c71617901a",
          "cited_paper_url": "https://www.semanticscholar.org/paper/64ea8f180d0682e6c18d1eb688afdb2027c02794",
          "citing_paper_year": 2023,
          "cited_paper_year": 2021
        },
        {
          "dataset_name": "ImageNet",
          "dataset_description": "Used to evaluate the empirical effectiveness of GDP compared to unsupervised methods, focusing on consistency and FID scores in image restoration.",
          "citing_paper_id": "257921922",
          "cited_paper_id": 235619773,
          "context_text": "We demonstrate the empirical effectiveness of GDP by comparing it with various competitive unsupervised methods under the linear or multi-linear inverse problem on ImageNet [14], LSUN [89], and CelebA [30] datasets in terms of consistency and FID.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions three specific datasets: ImageNet, LSUN, and CelebA. These are well-known image datasets used for evaluating image restoration and generation methods.",
          "citing_paper_doi": "10.1109/CVPR52729.2023.00958",
          "cited_paper_doi": null,
          "citing_paper_url": "https://www.semanticscholar.org/paper/5549dc3ceff07561d9fb59610c0f78c71617901a",
          "cited_paper_url": "https://www.semanticscholar.org/paper/0f183bcfe65781c06b1a48a6f56e0f3c63e8e4a4",
          "citing_paper_year": 2023,
          "cited_paper_year": 2021
        },
        {
          "dataset_name": "ImageNet",
          "dataset_description": "Used for training image restoration models, focusing on common objects in context. The dataset helps in evaluating performance on high-quality images, highlighting issues with low-quality inputs. | Utilized to enrich the representation space with contextual information about common objects, improving the model's performance in image understanding tasks. | Used to provide a rich representation space for semantic information, enhancing the model's ability to understand visual content.",
          "citing_paper_id": "260810536",
          "cited_paper_id": 2930547,
          "context_text": ", ImageNet [64], CoCo [47]) to provide a representation space rich in semantic information.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions ImageNet and CoCo as sources of semantic information for representation space. These are well-known image datasets.",
          "citing_paper_doi": "10.1109/CVPR52729.2023.01351",
          "cited_paper_doi": "10.1007/s11263-015-0816-y",
          "citing_paper_url": "https://www.semanticscholar.org/paper/c2e9a21f9f0fee0c8b18a3eb275298a406737f14",
          "cited_paper_url": "https://www.semanticscholar.org/paper/e74f9b7f8eec6ba4704c206b93bc8079af3da4bd",
          "citing_paper_year": 2023,
          "cited_paper_year": 2014
        },
        {
          "dataset_name": "ImageNet",
          "dataset_description": "Used for training image restoration models, focusing on common objects in context. The dataset helps in evaluating performance on high-quality images, highlighting issues with low-quality inputs. | Utilized to enrich the representation space with contextual information about common objects, improving the model's performance in image understanding tasks. | Used to provide a rich representation space for semantic information, enhancing the model's ability to understand visual content.",
          "citing_paper_id": "260810536",
          "cited_paper_id": 14113767,
          "context_text": ", ImageNet [64], CoCo [47]) to provide a representation space rich in semantic information.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions ImageNet and CoCo as sources of semantic information for representation space. These are well-known image datasets.",
          "citing_paper_doi": "10.1109/CVPR52729.2023.01351",
          "cited_paper_doi": "10.1007/978-3-319-10602-1_48",
          "citing_paper_url": "https://www.semanticscholar.org/paper/c2e9a21f9f0fee0c8b18a3eb275298a406737f14",
          "cited_paper_url": "https://www.semanticscholar.org/paper/71b7178df5d2b112d07e45038cb5637208659ff7",
          "citing_paper_year": 2023,
          "cited_paper_year": 2014
        },
        {
          "dataset_name": "ImageNet",
          "dataset_description": "Used for pre-training diffusion models to serve as the backbone for image restoration tasks, focusing on leveraging large-scale image data for improved model performance.",
          "citing_paper_id": "274437167",
          "cited_paper_id": 234357997,
          "context_text": "Some prior image restoration models [13, 18, 35, 58, 69] use pre-trained diffusion [10] on ImageNet [9] as their back-bone.",
          "confidence_score": 1.0,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions 'ImageNet' as a dataset used for pre-training diffusion models in the context of image restoration. No other specific datasets are mentioned.",
          "citing_paper_doi": "10.48550/arXiv.2412.00878",
          "cited_paper_doi": null,
          "citing_paper_url": "https://www.semanticscholar.org/paper/5ca2a7d55aac655fa060bb1383b281d5ea2d102e",
          "cited_paper_url": "https://www.semanticscholar.org/paper/64ea8f180d0682e6c18d1eb688afdb2027c02794",
          "citing_paper_year": 2024,
          "cited_paper_year": 2021
        },
        {
          "dataset_name": "ImageNet",
          "dataset_description": "Used for pre-training diffusion models to serve as the backbone for image restoration tasks, focusing on leveraging large-scale image data for improved model performance.",
          "citing_paper_id": "274437167",
          "cited_paper_id": 246411364,
          "context_text": "Some prior image restoration models [13, 18, 35, 58, 69] use pre-trained diffusion [10] on ImageNet [9] as their back-bone.",
          "confidence_score": 1.0,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions 'ImageNet' as a dataset used for pre-training diffusion models in the context of image restoration. No other specific datasets are mentioned.",
          "citing_paper_doi": "10.48550/arXiv.2412.00878",
          "cited_paper_doi": null,
          "citing_paper_url": "https://www.semanticscholar.org/paper/5ca2a7d55aac655fa060bb1383b281d5ea2d102e",
          "cited_paper_url": "https://www.semanticscholar.org/paper/3d3c5fcbc40aadccceda58d3d9c5cd00588ea0b7",
          "citing_paper_year": 2024,
          "cited_paper_year": 2022
        },
        {
          "dataset_name": "ImageNet",
          "dataset_description": "Used for pre-training diffusion models to serve as the backbone for image restoration tasks, focusing on leveraging large-scale image data for improved model performance.",
          "citing_paper_id": "274437167",
          "cited_paper_id": 254125609,
          "context_text": "Some prior image restoration models [13, 18, 35, 58, 69] use pre-trained diffusion [10] on ImageNet [9] as their back-bone.",
          "confidence_score": 1.0,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions 'ImageNet' as a dataset used for pre-training diffusion models in the context of image restoration. No other specific datasets are mentioned.",
          "citing_paper_doi": "10.48550/arXiv.2412.00878",
          "cited_paper_doi": "10.48550/arXiv.2212.00490",
          "citing_paper_url": "https://www.semanticscholar.org/paper/5ca2a7d55aac655fa060bb1383b281d5ea2d102e",
          "cited_paper_url": "https://www.semanticscholar.org/paper/3a75ed3e9e81c9db573ef73d20e2c66c12aaedf8",
          "citing_paper_year": 2024,
          "cited_paper_year": 2022
        },
        {
          "dataset_name": "ImageNet",
          "dataset_description": "Used for pre-training diffusion models to serve as the backbone for image restoration tasks, focusing on leveraging large-scale image data for improved model performance.",
          "citing_paper_id": "274437167",
          "cited_paper_id": 257921922,
          "context_text": "Some prior image restoration models [13, 18, 35, 58, 69] use pre-trained diffusion [10] on ImageNet [9] as their back-bone.",
          "confidence_score": 1.0,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions 'ImageNet' as a dataset used for pre-training diffusion models in the context of image restoration. No other specific datasets are mentioned.",
          "citing_paper_doi": "10.48550/arXiv.2412.00878",
          "cited_paper_doi": "10.1109/CVPR52729.2023.00958",
          "citing_paper_url": "https://www.semanticscholar.org/paper/5ca2a7d55aac655fa060bb1383b281d5ea2d102e",
          "cited_paper_url": "https://www.semanticscholar.org/paper/5549dc3ceff07561d9fb59610c0f78c71617901a",
          "citing_paper_year": 2024,
          "cited_paper_year": 2023
        },
        {
          "dataset_name": "ImageNet",
          "dataset_description": "Used for pre-training diffusion models to serve as the backbone for image restoration tasks, focusing on leveraging large-scale image data for improved model performance.",
          "citing_paper_id": "274437167",
          "cited_paper_id": 258947068,
          "context_text": "Some prior image restoration models [13, 18, 35, 58, 69] use pre-trained diffusion [10] on ImageNet [9] as their back-bone.",
          "confidence_score": 1.0,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions 'ImageNet' as a dataset used for pre-training diffusion models in the context of image restoration. No other specific datasets are mentioned.",
          "citing_paper_doi": "10.48550/arXiv.2412.00878",
          "cited_paper_doi": "10.48550/arXiv.2305.16965",
          "citing_paper_url": "https://www.semanticscholar.org/paper/5ca2a7d55aac655fa060bb1383b281d5ea2d102e",
          "cited_paper_url": "https://www.semanticscholar.org/paper/939e96c890a0b43ef88a72182389e52420c91934",
          "citing_paper_year": 2024,
          "cited_paper_year": 2023
        },
        {
          "dataset_name": "ImageNet",
          "dataset_description": "Used to collect high-quality images for image restoration experiments, focusing on diverse and large-scale image data.",
          "citing_paper_id": "274437167",
          "cited_paper_id": 257952310,
          "context_text": "We first collect HQ images from [51], ImageNet [9], and SAM [21].",
          "confidence_score": 0.8,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions 'HQ images' from three sources, but only ImageNet is a clear, verifiable dataset. SAM likely refers to a method or tool, not a dataset.",
          "citing_paper_doi": "10.48550/arXiv.2412.00878",
          "cited_paper_doi": "10.1109/ICCV51070.2023.00371",
          "citing_paper_url": "https://www.semanticscholar.org/paper/5ca2a7d55aac655fa060bb1383b281d5ea2d102e",
          "cited_paper_url": "https://www.semanticscholar.org/paper/7470a1702c8c86e6f28d32cfa315381150102f5b",
          "citing_paper_year": 2024,
          "cited_paper_year": 2023
        },
        {
          "dataset_name": "ImageNet",
          "dataset_description": "Used to collect high-quality images for image restoration experiments, focusing on diverse and large-scale image data.",
          "citing_paper_id": "274437167",
          "cited_paper_id": null,
          "context_text": "We first collect HQ images from [51], ImageNet [9], and SAM [21].",
          "confidence_score": 0.8,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions 'HQ images' from three sources, but only ImageNet is a clear, verifiable dataset. SAM likely refers to a method or tool, not a dataset.",
          "citing_paper_doi": "10.48550/arXiv.2412.00878",
          "cited_paper_doi": null,
          "citing_paper_url": "https://www.semanticscholar.org/paper/5ca2a7d55aac655fa060bb1383b281d5ea2d102e",
          "cited_paper_url": null,
          "citing_paper_year": 2024,
          "cited_paper_year": null
        },
        {
          "dataset_name": "ImageNet",
          "dataset_description": "Used to evaluate colorization performance, specifically comparing different methods including Palette (I), Palette (I+P), Regression, ColTran, and PixColor.",
          "citing_paper_id": "243938678",
          "cited_paper_id": 25005910,
          "context_text": "Interestingly Palette (I) performs slightly better than Palette (I+P) on ImageNet indicating that augmentation with Places2 images during training doesn’t\nFool Rate % on Set-I (3 sec display) Fool Rate % on Set-II (3 sec display)\n0 10 20 30 40 50 60 70 Palette (ours)\nRegression\nColTran\nPixColor\n49.1%\n39.2%\n34.8%\n27.5%\n0 10 20 30 40 50 60 70 Palette (ours)\nRegression\nColTran\nPixColor\n46.5%\n39.7%\n38.3%\n34.3%\nFool Rate % on Set-I (5 sec display) Fool Rate % on Set-II (5 sec display)\n0 10 20 30 40 50 60 70 Palette (ours)\nRegression\nColTran\nPixColor\n47.0%\n37.1%\n32.6%\n22.4%\n0 10 20 30 40 50 60 70 Palette (ours)\nRegression\nColTran\nPixColor\n44.8%\n40.9%\n37.2%\n29.7%\nFigure C.1: Human evaluation results on ImageNet colorization.",
          "confidence_score": 1.0,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions ImageNet, which is a well-known dataset used for image classification and other computer vision tasks. It is used here for evaluating colorization performance.",
          "citing_paper_doi": "10.1145/3528233.3530757",
          "cited_paper_doi": "10.5244/C.31.112",
          "citing_paper_url": "https://www.semanticscholar.org/paper/37c9c4e7648f639c0b36f150fc6c6c90b3682f4a",
          "cited_paper_url": "https://www.semanticscholar.org/paper/1a3ec0662929d2b9eb9a555c64cf16a846f4fb9c",
          "citing_paper_year": 2021,
          "cited_paper_year": 2017
        },
        {
          "dataset_name": "ImageNet",
          "dataset_description": "Used to evaluate inpainting, uncropping, and JPEG restoration methods, leveraging its scale, diversity, and public availability. | Used to evaluate inpainting, uncropping, and JPEG restoration, focusing on sample quality scores for various methods. | Used to evaluate inpainting, uncropping, and JPEG restoration, focusing on sample quality scores for several baselines.",
          "citing_paper_id": "243938678",
          "cited_paper_id": 57246310,
          "context_text": "Finally, we advocate a standardized evaluation protocol for inpainting, uncropping, and JPEG restoration based on ImageNet [Deng et al., 2009] and report sample quality scores for several",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions ImageNet as a dataset used for evaluating inpainting, uncropping, and JPEG restoration tasks.",
          "citing_paper_doi": "10.1145/3528233.3530757",
          "cited_paper_doi": "10.1109/CVPR.2009.5206848",
          "citing_paper_url": "https://www.semanticscholar.org/paper/37c9c4e7648f639c0b36f150fc6c6c90b3682f4a",
          "cited_paper_url": "https://www.semanticscholar.org/paper/d2c733e34d48784a37d717fe43d9e93277a8c53e",
          "citing_paper_year": 2021,
          "cited_paper_year": 2009
        },
        {
          "dataset_name": "ImageNet",
          "dataset_description": "Used as a benchmark dataset for single image super-resolution, containing a small set of high-resolution images. | Used as a benchmark dataset for single image super-resolution, containing a diverse set of urban scene images. | Used as a benchmark dataset for single image super-resolution, containing a moderate set of high-resolution images. | Used to select high-quality images for single image super-resolution, providing a large-scale hierarchical image database.",
          "citing_paper_id": "268364340",
          "cited_paper_id": 2356330,
          "context_text": "It should be noted that we select the HQimagesfromImageNet[91]insteadoftheprevailingdatasets in SR, such as Set5 [95], Set14 [96], and Urban100 [97].",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions specific datasets used for single image super-resolution, including ImageNet, Set5, Set14, and Urban100. These are all well-known datasets in the field of image restoration.",
          "citing_paper_doi": "10.1109/TPAMI.2024.3461721",
          "cited_paper_doi": "10.1007/978-3-642-27413-8_47",
          "citing_paper_url": "https://www.semanticscholar.org/paper/decac28a79f9cbb5be2658f35a6cdb5bc705c2bc",
          "cited_paper_url": "https://www.semanticscholar.org/paper/d1dce9ad419d8c7037980a9ea28506ed0e640bba",
          "citing_paper_year": 2024,
          "cited_paper_year": 2010
        },
        {
          "dataset_name": "ImageNet",
          "dataset_description": "Used as a benchmark dataset for single image super-resolution, containing a small set of high-resolution images. | Used as a benchmark dataset for single image super-resolution, containing a diverse set of urban scene images. | Used as a benchmark dataset for single image super-resolution, containing a moderate set of high-resolution images. | Used to select high-quality images for single image super-resolution, providing a large-scale hierarchical image database.",
          "citing_paper_id": "268364340",
          "cited_paper_id": 8282555,
          "context_text": "It should be noted that we select the HQimagesfromImageNet[91]insteadoftheprevailingdatasets in SR, such as Set5 [95], Set14 [96], and Urban100 [97].",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions specific datasets used for single image super-resolution, including ImageNet, Set5, Set14, and Urban100. These are all well-known datasets in the field of image restoration.",
          "citing_paper_doi": "10.1109/TPAMI.2024.3461721",
          "cited_paper_doi": "10.1109/CVPR.2015.7299156",
          "citing_paper_url": "https://www.semanticscholar.org/paper/decac28a79f9cbb5be2658f35a6cdb5bc705c2bc",
          "cited_paper_url": "https://www.semanticscholar.org/paper/2946c4e29b057d3543f09b89f547965895676d19",
          "citing_paper_year": 2024,
          "cited_paper_year": 2015
        },
        {
          "dataset_name": "ImageNet",
          "dataset_description": "Used to construct a test dataset of 2,000 facial images for evaluation, focusing on the quality of facial image restoration. | Used to construct a test dataset of 2,000 natural images for evaluation, facilitating assessment of image restoration performance on diverse content. | Used as a benchmark dataset for single image super-resolution, containing a small set of high-resolution images. | Used as a benchmark dataset for single image super-resolution, containing a diverse set of urban scene images. | Used as a benchmark dataset for single image super-resolution, containing a moderate set of high-resolution images. | Used to train models on high-resolution images, specifically for image restoration tasks, by randomly cropping 256x256 patches from the training set. | Used to create a synthetic test dataset by randomly selecting 3000 images from the validation set, denoted as ImageNet-Test. | Used to select high-quality images for single image super-resolution, providing a large-scale hierarchical image database.",
          "citing_paper_id": "268364340",
          "cited_paper_id": 57246310,
          "context_text": "It should be noted that we select the HQimagesfromImageNet[91]insteadoftheprevailingdatasets in SR, such as Set5 [95], Set14 [96], and Urban100 [97].",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions specific datasets used for single image super-resolution, including ImageNet, Set5, Set14, and Urban100. These are all well-known datasets in the field of image restoration.",
          "citing_paper_doi": "10.1109/TPAMI.2024.3461721",
          "cited_paper_doi": "10.1109/CVPR.2009.5206848",
          "citing_paper_url": "https://www.semanticscholar.org/paper/decac28a79f9cbb5be2658f35a6cdb5bc705c2bc",
          "cited_paper_url": "https://www.semanticscholar.org/paper/d2c733e34d48784a37d717fe43d9e93277a8c53e",
          "citing_paper_year": 2024,
          "cited_paper_year": 2009
        },
        {
          "dataset_name": "ImageNet",
          "dataset_description": "Used to pretrain the backbone of the restoration model, enabling it to handle various low-level vision tasks through multiple heads and tails.",
          "citing_paper_id": "259459919",
          "cited_paper_id": 227239228,
          "context_text": "For image restoration, IPT [45] pretrains the backbone of restoration model on ImageNet, equipped with multiple heads and tails for coping with different low-level vision tasks respectively.",
          "confidence_score": 1.0,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions ImageNet as a dataset used for pretraining the backbone of a restoration model. No other datasets are mentioned.",
          "citing_paper_doi": "10.1109/TCSVT.2023.3286405",
          "cited_paper_doi": "10.1109/CVPR46437.2021.01212",
          "citing_paper_url": "https://www.semanticscholar.org/paper/fca6cb29677c6fe278290ee1814d9bc31c155a65",
          "cited_paper_url": "https://www.semanticscholar.org/paper/6f6f73e69ee0d9d5d7d088bb882db1851d98175a",
          "citing_paper_year": 2024,
          "cited_paper_year": 2020
        },
        {
          "dataset_name": "ImageNet",
          "dataset_description": "Used to generate low-resolution, noised, and rainy images for pre-training the IPT model, focusing on image restoration techniques.",
          "citing_paper_id": "247940251",
          "cited_paper_id": 227239228,
          "context_text": "The authors augment ImageNet images to low-resolution, noised, and rainy images via corresponding filters and then pre-train the IPTwith each set.",
          "confidence_score": 1.0,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions the augmentation of ImageNet images, which is a well-known dataset. No other specific datasets are mentioned.",
          "citing_paper_doi": "10.1016/j.image.2022.116701",
          "cited_paper_doi": "10.1109/CVPR46437.2021.01212",
          "citing_paper_url": "https://www.semanticscholar.org/paper/a52f1afdd11d9a902729ecb1e5be852470756711",
          "cited_paper_url": "https://www.semanticscholar.org/paper/6f6f73e69ee0d9d5d7d088bb882db1851d98175a",
          "citing_paper_year": 2020,
          "cited_paper_year": 2020
        }
      ]
    },
    {
      "cited_paper_id": "201624746",
      "citation_count": 0,
      "total_dataset_mentions": 23,
      "unique_datasets": [
        "SIDD"
      ],
      "dataset_details": [
        {
          "dataset_name": "SIDD",
          "dataset_description": "Used for real image denoising, focusing on noise characteristics in smartphone cameras, employing a high-quality dataset for training and evaluation. | Used for denoising experiments with real-world datasets, evaluating denoising algorithms on smartphone camera images. | Used for denoising experiments with real-world datasets, evaluating denoising algorithms on diverse real-world images. | Used for denoising experiments with synthetic benchmark datasets, evaluating denoising algorithms under controlled conditions. | Used for real image denoising, providing a benchmark for evaluating denoising algorithms on real photographs, emphasizing practical performance.",
          "citing_paper_id": "244346144",
          "cited_paper_id": 9715523,
          "context_text": "Real image denoising on SIDD [1] and DND [60] datasets.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions two specific datasets, SIDD and DND, which are used for real image denoising. These datasets are clearly identified and are relevant to the research topic of all-in-one image restoration.",
          "citing_paper_doi": "10.1109/CVPR52688.2022.00564",
          "cited_paper_doi": "10.1109/CVPR.2017.294",
          "citing_paper_url": "https://www.semanticscholar.org/paper/1e88d5afe19aea324d33541f60a90b7036894c32",
          "cited_paper_url": "https://www.semanticscholar.org/paper/d00d9e427d1ceb81164ab6487462127c337b1b0a",
          "citing_paper_year": 2021,
          "cited_paper_year": 2017
        },
        {
          "dataset_name": "SIDD",
          "dataset_description": "Used for low-light image restoration, enhancing images with low illumination levels. | Used for low-light image enhancement, assessing the effectiveness of restoration techniques in improving visibility. | Used for evaluating image restoration under noise conditions, focusing on denoising performance in low-light scenarios. | Used for snow removal, focusing on all-weather image restoration. | Used for merged deraining and deblurring tasks, focusing on rain, noise, and blur in low-light conditions. | Used for snow removal, focusing on large-scale evaluation of restoration algorithms in snowy conditions. | Used for raindrop removal, evaluating the performance of restoration techniques in handling raindrop artifacts. | Used for deblurring and deraining, testing the ability of models to handle multiple adverse weather conditions simultaneously. | Used for snow removal, assessing the effectiveness of restoration techniques in eliminating snow artifacts from images. | Used for rain removal, evaluating the performance of restoration algorithms in reducing rain artifacts. | Used for rain and haze removal, focusing on all-weather image restoration.",
          "citing_paper_id": "273502246",
          "cited_paper_id": 259144110,
          "context_text": "…Perceive-IR [111] rain-noise-blur-dark[61] 4 Rain, Noise, Blur, Low-light SIDD, merged deraining, merged debluring,LOL ProRes [61] rain-haze-snow[67] 3 Rain, Haze, Snow SPA+,REVIDE,RealSnow WGWS-Net [67], TKMANet [100], Art [135] rain-haze-snow[64] 3 Raindrop, Rain+Fog, Snow All-weather…",
          "confidence_score": 0.9,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions several datasets used for image restoration under various weather conditions, including rain, noise, blur, and low-light scenarios.",
          "citing_paper_doi": "10.48550/arXiv.2410.15067",
          "cited_paper_doi": "10.1109/CVPR52729.2023.02083",
          "citing_paper_url": "https://www.semanticscholar.org/paper/f5ff17162a4936db3f64ed7bafae13abcfa63a27",
          "cited_paper_url": "https://www.semanticscholar.org/paper/25ac8b26688c0a2349c72337737e0252a0f8051e",
          "citing_paper_year": 2024,
          "cited_paper_year": 2023
        },
        {
          "dataset_name": "SIDD",
          "dataset_description": "Used for evaluating the model on 1,000 (512×512) images, assessing denoising effectiveness on diverse real photographs. | Used for deblurring, providing realistic blur effects to test and validate deblurring algorithms. | Used for evaluating the denoising model, providing a benchmark for comparing performance against other methods. | Used to evaluate the denoising performance of the model on real photographs, containing 50 test images without corresponding ground truth. | Used for training and evaluating the denoising model, focusing on real-world noise characteristics in photographs. | Used to evaluate the denoising performance of the model, focusing on synthetic images with known noise characteristics. The dataset contains 320 training and 40 test images. | Used for motion deblurring, focusing on human-aware motion deblurring techniques to improve image clarity. | Used for deraining, combining multiple datasets to comprehensively evaluate deraining methods. | Used for benchmarking denoising algorithms with real photographs, focusing on performance evaluation and comparison. | Used for training and evaluating the model on high-resolution images, focusing on real-world denoising performance. | Used to benchmark denoising algorithms with real photographs, focusing on performance metrics such as PSNR and SSIM. | Used for denoising real photographs, assessing the performance of denoising methods on diverse noise patterns. | Used for deblurring, specifically addressing challenging deblurring scenarios to enhance image quality. | Used for denoising real photographs, evaluating the effectiveness of denoising algorithms in practical scenarios.",
          "citing_paper_id": "245837508",
          "cited_paper_id": 9715523,
          "context_text": "We used SIDD [2] and DND [70] for denoising, GoPro [60], HIDE [79], and RealBlur [77] for debluring, a combined dataset Rain13k used in [108] for deraining.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions several datasets used for specific image restoration tasks, which align with the research topic of all-in-one image restoration.",
          "citing_paper_doi": "10.1109/CVPR52688.2022.00568",
          "cited_paper_doi": "10.1109/CVPR.2017.294",
          "citing_paper_url": "https://www.semanticscholar.org/paper/fbf68eb0cf8237cef59ebdb301569c79b9676ff9",
          "cited_paper_url": "https://www.semanticscholar.org/paper/d00d9e427d1ceb81164ab6487462127c337b1b0a",
          "citing_paper_year": 2022,
          "cited_paper_year": 2017
        },
        {
          "dataset_name": "SIDD",
          "dataset_description": "Used to evaluate the model's performance on 1,111 image pairs, assessing its generalization ability after training. | Applied to assess deblurring performance on real-world images, focusing on challenging scenarios with varying blur types. | Used for visual comparisons of motion deblurring models, focusing on real-world motion blur in diverse scenes. | Used for motion deblurring, focusing on human-aware motion deblurring techniques to improve image clarity. | Used for deraining, combining multiple datasets to comprehensively evaluate deraining methods. | Applied to assess deblurring algorithms, emphasizing the removal of motion blur in high-resolution images. | Used for deblurring, specifically addressing challenging deblurring scenarios to enhance image quality. | Used for visual comparisons in image deblurring, specifically evaluating the performance of DMPHN and other methods. The dataset provides a benchmark for assessing deblurring algorithms. | Used for denoising real photographs, evaluating the effectiveness of denoising algorithms in practical scenarios. | Used for deblurring, providing realistic blur effects to test and validate deblurring algorithms. | Used to test deblurring algorithms on real-world images, emphasizing robustness across different blur conditions. | Employed for video deblurring, offering a large-scale dataset with high-resolution video sequences for training and evaluation. | Used for denoising real photographs, assessing the performance of denoising methods on diverse noise patterns. | Used for quantitative comparison of deblurring methods, focusing on synthetic blur scenarios to evaluate performance metrics. | Used to evaluate motion deblurring methods, focusing on human-aware deblurring performance metrics such as PSNR and SSIM. | Utilized to test deblurring methods, specifically addressing realistic motion blur in challenging conditions. | Used for visual comparisons of motion deblurring models, focusing on real-world motion blur with ground truth images. | Used to demonstrate motion deblurring techniques, focusing on human-aware motion deblurring in real-world scenarios. | Used for training and evaluating the model on motion deblurring tasks, focusing on improving image clarity and reducing blur. | Used for evaluating the model's performance on motion deblurring, specifically assessing its ability to handle diverse real-world scenarios. | Utilized for evaluating deblurring algorithms, containing real-world blurred images with ground truth sharp images. | Used to test the model's generalization on 2,025 images, providing additional validation beyond the training domain. | Used to evaluate image restoration methods, providing a diverse set of images with various types of degradation. | Used for motion deblurring experiments, providing a large set of sharp and blurred image pairs to train and evaluate deblurring models. | Used to assess the model's performance on 980 paired images of camera JPEG output and RAW images, testing real-world applicability. | Used for visual comparisons of motion deblurring models, focusing on synthetic motion blur in high-resolution images. | Employed to evaluate deblurring models, concentrating on long-exposure and motion-induced blur in video sequences. | Used to evaluate motion deblurring techniques, focusing on visual quality and restoration performance in real-world scenarios.",
          "citing_paper_id": "245837508",
          "cited_paper_id": 201624746,
          "context_text": "We used SIDD [2] and DND [70] for denoising, GoPro [60], HIDE [79], and RealBlur [77] for debluring, a combined dataset Rain13k used in [108] for deraining.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions several datasets used for specific image restoration tasks, which align with the research topic of all-in-one image restoration.",
          "citing_paper_doi": "10.1109/CVPR52688.2022.00568",
          "cited_paper_doi": "10.1109/ICCV.2019.00567",
          "citing_paper_url": "https://www.semanticscholar.org/paper/fbf68eb0cf8237cef59ebdb301569c79b9676ff9",
          "cited_paper_url": "https://www.semanticscholar.org/paper/20a4b3353579525f0b76ec42e17a2284b4453f9a",
          "citing_paper_year": 2022,
          "cited_paper_year": 2019
        },
        {
          "dataset_name": "SIDD",
          "dataset_description": "Used to reimplement and evaluate the DMPHN(1-2-4-8) model for dynamic scene deblurring, focusing on performance metrics and visual quality. | Used to compare models with and without HIN for deblurring, focusing on performance improvements in dynamic scene deblurring. | Used to evaluate the 3× speedup of the proposed method in dynamic scene deblurring, focusing on performance metrics. | Used to test the effectiveness of HIN in video deblurring, showcasing its performance in restoring video sequences. | Used for image deblurring experiments, focusing on dynamic scene deblurring using a deep multi-scale convolutional neural network. | Used for training models on image deblurring, specifically targeting dynamic scene deblurring. | Used for training models on image deraining, consisting of 13,712 clean-rain image pairs. | Used for training models on image denoising, focusing on high-quality denoising for smartphone cameras. | Used to train and evaluate an image deblurring model, focusing on dynamic scenes with jpeg compression artifacts, employing a deep multi-scale convolutional neural network. | Used to evaluate the effectiveness of HIN in denoising smartphone camera images, demonstrating its performance in a specific image restoration task. | Used to assess the effectiveness of HIN in deblurring dynamic scenes, highlighting its capability in handling motion blur. | Used for deblurring comparisons, focusing on dynamic scene deblurring using a deep multi-scale convolutional neural network.",
          "citing_paper_id": "234482841",
          "cited_paper_id": 8671030,
          "context_text": "Datasets As in [56], we train our models on SIDD [1] for image denoising, GoPro [31] for image deblurring, and 13,712 clean-rain image pairs (for simplicity, denoted as Rain13k in the following) gathered from [10, 25, 49, 60, 61] for image deraining.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions three specific datasets used for training models for different image restoration tasks: SIDD for denoising, GoPro for deblurring, and Rain13k for deraining.",
          "citing_paper_doi": "10.1109/CVPRW53098.2021.00027",
          "cited_paper_doi": "10.1109/CVPR.2017.35",
          "citing_paper_url": "https://www.semanticscholar.org/paper/9b8cbe9a636b00b3543d8e1f98c24de12d6c3d17",
          "cited_paper_url": "https://www.semanticscholar.org/paper/65b16da51891a6b98140d425804c8a0fd0299219",
          "citing_paper_year": 2021,
          "cited_paper_year": 2016
        },
        {
          "dataset_name": "SIDD",
          "dataset_description": "Used for training models on image deraining, consisting of 13,712 clean-rain image pairs. | Used for training models on image denoising, focusing on high-quality denoising for smartphone cameras. | Used for training models on image deblurring, specifically targeting dynamic scene deblurring.",
          "citing_paper_id": "234482841",
          "cited_paper_id": 9007541,
          "context_text": "Datasets As in [56], we train our models on SIDD [1] for image denoising, GoPro [31] for image deblurring, and 13,712 clean-rain image pairs (for simplicity, denoted as Rain13k in the following) gathered from [10, 25, 49, 60, 61] for image deraining.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions three specific datasets used for training models for different image restoration tasks: SIDD for denoising, GoPro for deblurring, and Rain13k for deraining.",
          "citing_paper_doi": "10.1109/CVPRW53098.2021.00027",
          "cited_paper_doi": "10.1109/CVPR.2016.299",
          "citing_paper_url": "https://www.semanticscholar.org/paper/9b8cbe9a636b00b3543d8e1f98c24de12d6c3d17",
          "cited_paper_url": "https://www.semanticscholar.org/paper/7c17025c540b88df14da35229618b5e896ab9528",
          "citing_paper_year": 2021,
          "cited_paper_year": 2016
        },
        {
          "dataset_name": "SIDD",
          "dataset_description": "Used to train PRMID and CycleISP models for image denoising, focusing on enhancing image quality from smartphone cameras. | Used for training and testing an image denoising model, focusing on high-resolution images and patches from smartphone cameras. | Used to test the effectiveness of HIN in video deblurring, showcasing its performance in restoring video sequences. | Used for denoising comparisons in image restoration, specifically evaluating the performance of denoising algorithms on smartphone camera images. | Used for training models on image deraining, consisting of 13,712 clean-rain image pairs. | Used to assess the effectiveness of HIN in deblurring dynamic scenes, highlighting its capability in handling motion blur. | Used for training models on image denoising, focusing on high-quality denoising for smartphone cameras. | Used to evaluate the effectiveness of HIN in denoising smartphone camera images, demonstrating its performance in a specific image restoration task. | Used to compare models with and without HIN for denoising, focusing on the performance improvement in smartphone camera images. | Used to evaluate the performance of the proposed image restoration method, specifically focusing on denoising capabilities in smartphone camera images. | Used for training models on image deblurring, specifically targeting dynamic scene deblurring.",
          "citing_paper_id": "234482841",
          "cited_paper_id": 52059988,
          "context_text": "Datasets As in [56], we train our models on SIDD [1] for image denoising, GoPro [31] for image deblurring, and 13,712 clean-rain image pairs (for simplicity, denoted as Rain13k in the following) gathered from [10, 25, 49, 60, 61] for image deraining.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions three specific datasets used for training models for different image restoration tasks: SIDD for denoising, GoPro for deblurring, and Rain13k for deraining.",
          "citing_paper_doi": "10.1109/CVPRW53098.2021.00027",
          "cited_paper_doi": "10.1109/CVPR.2018.00182",
          "citing_paper_url": "https://www.semanticscholar.org/paper/9b8cbe9a636b00b3543d8e1f98c24de12d6c3d17",
          "cited_paper_url": "https://www.semanticscholar.org/paper/ebf35073e122782f685a0d6c231622412f28a53b",
          "citing_paper_year": 2021,
          "cited_paper_year": 2018
        },
        {
          "dataset_name": "SIDD",
          "dataset_description": "Used for training models on image deraining, consisting of 13,712 clean-rain image pairs. | Used for training models on image denoising, focusing on high-quality denoising for smartphone cameras. | Used for training models on image deblurring, specifically targeting dynamic scene deblurring.",
          "citing_paper_id": "234482841",
          "cited_paper_id": 231802205,
          "context_text": "Datasets As in [56], we train our models on SIDD [1] for image denoising, GoPro [31] for image deblurring, and 13,712 clean-rain image pairs (for simplicity, denoted as Rain13k in the following) gathered from [10, 25, 49, 60, 61] for image deraining.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions three specific datasets used for training models for different image restoration tasks: SIDD for denoising, GoPro for deblurring, and Rain13k for deraining.",
          "citing_paper_doi": "10.1109/CVPRW53098.2021.00027",
          "cited_paper_doi": "10.1109/CVPR46437.2021.01458",
          "citing_paper_url": "https://www.semanticscholar.org/paper/9b8cbe9a636b00b3543d8e1f98c24de12d6c3d17",
          "cited_paper_url": "https://www.semanticscholar.org/paper/92d50602db5746f03b91562e2cc8a98bec584e9b",
          "citing_paper_year": 2021,
          "cited_paper_year": 2021
        },
        {
          "dataset_name": "SIDD",
          "dataset_description": "Used for training the ART model on high-resolution images for denoising, focusing on improving performance in real image denoising tasks. | Used for training the ART model on high-resolution images from smartphone cameras, focusing on denoising performance. | Used for testing ART, comparing its denoising performance against other state-of-the-art methods. | Used for training ART on high-resolution images from smartphone cameras, focusing on denoising performance. | Used for training and evaluating an image denoising model, focusing on high-resolution images from smartphone cameras. | Used for testing and validating the performance of the ART model, comparing it with state-of-the-art methods in real image denoising. | Used for testing the ART model, evaluating its denoising capabilities against other state-of-the-art methods. | Used for testing the image denoising model, providing a benchmark for quantitative comparisons. | Used to train ART on high-resolution images, focusing on denoising for smartphone cameras, enhancing image quality through machine learning techniques.",
          "citing_paper_id": "252693111",
          "cited_paper_id": 52059988,
          "context_text": "We train ART on 320 high-resolution images from SIDD Abdelhamed et al. (2018) datasets. The testing datasets include SIDD test set and DND dataset Plotz & Roth (2017). Quantitative Comparisons.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions specific datasets used for training and testing an image restoration model. SIDD and DND are clearly identified and used for denoising experiments.",
          "citing_paper_doi": "10.48550/arXiv.2210.01427",
          "cited_paper_doi": "10.1109/CVPR.2018.00182",
          "citing_paper_url": "https://www.semanticscholar.org/paper/a1f7f5597fdbc54e58f1f2a1a640bf355e87a978",
          "cited_paper_url": "https://www.semanticscholar.org/paper/ebf35073e122782f685a0d6c231622412f28a53b",
          "citing_paper_year": 2022,
          "cited_paper_year": 2018
        },
        {
          "dataset_name": "SIDD",
          "dataset_description": "Used to benchmark denoising algorithms, providing a challenging set of real-world noisy images for evaluation. | Used to test denoising algorithms on real photographs, evaluating performance on a diverse set of images with known noise characteristics. | Used to benchmark denoising algorithms, specifically comparing performance against other methods in real-world scenarios.",
          "citing_paper_id": "254018182",
          "cited_paper_id": 9715523,
          "context_text": "And we have two testing datasets: SIDD validation set [1] and DND [33].",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions two specific datasets used for testing: SIDD validation set and DND. These are clearly identified and relevant to the topic of image restoration.",
          "citing_paper_doi": "10.48550/arXiv.2211.13654",
          "cited_paper_doi": "10.1109/CVPR.2017.294",
          "citing_paper_url": "https://www.semanticscholar.org/paper/1c49e58a935c80c8ec8307e937fc61c8f1f80433",
          "cited_paper_url": "https://www.semanticscholar.org/paper/d00d9e427d1ceb81164ab6487462127c337b1b0a",
          "citing_paper_year": 2022,
          "cited_paper_year": 2017
        },
        {
          "dataset_name": "SIDD",
          "dataset_description": "Used to conduct experiments on denoising noisy images captured with smartphones, focusing on improving image quality through restoration techniques.",
          "citing_paper_id": "264305756",
          "cited_paper_id": 52059988,
          "context_text": "For noisy images, we conduct experiments using the SIDD [74] dataset, which contains 200 images captured with smartphones.",
          "confidence_score": 1.0,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context clearly mentions the use of the SIDD dataset for conducting experiments on noisy images captured with smartphones.",
          "citing_paper_doi": "10.1109/TIP.2024.3456583",
          "cited_paper_doi": "10.1109/CVPR.2018.00182",
          "citing_paper_url": "https://www.semanticscholar.org/paper/a9eb8bd7057c82150b7d4cafb073ea4f2cb93636",
          "cited_paper_url": "https://www.semanticscholar.org/paper/ebf35073e122782f685a0d6c231622412f28a53b",
          "citing_paper_year": 2023,
          "cited_paper_year": 2018
        },
        {
          "dataset_name": "SIDD",
          "dataset_description": "Used to evaluate the performance of the proposed method, specifically comparing PSNR improvements over CycleISP. The dataset provides real-world noisy images for benchmarking. | Used to evaluate the performance of the proposed method, specifically comparing the improvement in PSNR over CycleISP. The dataset focuses on real-world image denoising.",
          "citing_paper_id": "212725053",
          "cited_paper_id": 212737191,
          "context_text": "32 dB over CycleISP [38] on SIDD and 0 .",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions 'SIDD' which is a known dataset in the field of image restoration. The citation is used to compare performance metrics.",
          "citing_paper_doi": "10.1109/TPAMI.2022.3167175",
          "cited_paper_doi": "10.1109/CVPR42600.2020.00277",
          "citing_paper_url": "https://www.semanticscholar.org/paper/bb3c793a5a6d3be2454d22a76ada5b6a94ec42f5",
          "cited_paper_url": "https://www.semanticscholar.org/paper/51352023ed22068fbfaa7693fc37123c0a5fce9f",
          "citing_paper_year": 2022,
          "cited_paper_year": 2020
        },
        {
          "dataset_name": "SIDD",
          "dataset_description": "Used for denoising comparisons, evaluating various methods including deep CNNs, BM3D, and the proposed MIRNet. Focuses on image restoration performance metrics such as PSNR and SSIM. | Used to evaluate denoising performance, comparing MIRNet against other methods like BM3D, NC, DnCNN, CBDNet, VDN, and RIDNet. The dataset provides noisy images for benchmarking.",
          "citing_paper_id": "263784964",
          "cited_paper_id": 996788,
          "context_text": "Enriched Features for Real Image Restoration and Enhancement 9 Table 1: Denoising comparisons on the SIDD dataset [1]. Method DnCNN MLP GLIDE TNRD FoE BM3D WNNM NLM KSVD EPLL CBDNet RIDNet VDN MIRNet [120] [11] [96] [20] [85] [21] [38] [10] [2] [128] [39] [5] [118] (Ours) PSNR &quot; 23.66 24.71 24.71 24.73 25.58 25.65 25.78 26.76 26.88 27.11 30.78 38.71 39.28 39.72 SSIM &quot; 0.583 0.641 0.774 0.643",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions the SIDD dataset, which is used for denoising comparisons. The dataset is clearly identified and used in the research context.",
          "citing_paper_doi": "10.1007/978-3-030-58595-2_30",
          "cited_paper_doi": "10.1109/TIP.2017.2662206",
          "citing_paper_url": "https://www.semanticscholar.org/paper/26f27bd5892e869b54c69d140b20c33638f26c57",
          "cited_paper_url": "https://www.semanticscholar.org/paper/0c00a328fa7cd56ee60338c54e89bd48310db80b",
          "citing_paper_year": 2020,
          "cited_paper_year": 2016
        },
        {
          "dataset_name": "SIDD",
          "dataset_description": "Used for denoising comparisons, evaluating various methods including the proposed MIRNet. The dataset provides a benchmark for image restoration performance.",
          "citing_paper_id": "263784964",
          "cited_paper_id": 7477309,
          "context_text": "ion and Enhancement 9 Table 1: Denoising comparisons on the SIDD dataset [1]. Method DnCNN MLP GLIDE TNRD FoE BM3D WNNM NLM KSVD EPLL CBDNet RIDNet VDN MIRNet [120] [11] [96] [20] [85] [21] [38] [10] [2] [128] [39] [5] [118] (Ours) PSNR &quot; 23.66 24.71 24.71 24.73 25.58 25.65 25.78 26.76 26.88 27.11 30.78 38.71 39.28 39.72 SSIM &quot; 0.583 0.641 0.774 0.643 0.792 0.685 0.809 0.699 0.842 0.870 0.7",
          "confidence_score": 1.0,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions the SIDD dataset, which is used for denoising comparisons. The dataset is clearly identified and used in the research.",
          "citing_paper_doi": "10.1007/978-3-030-58595-2_30",
          "cited_paper_doi": "10.1109/TSP.2006.881199",
          "citing_paper_url": "https://www.semanticscholar.org/paper/26f27bd5892e869b54c69d140b20c33638f26c57",
          "cited_paper_url": "https://www.semanticscholar.org/paper/83b522f4bfa5db7f7d34f839475af7d078107634",
          "citing_paper_year": 2020,
          "cited_paper_year": 2006
        },
        {
          "dataset_name": "SIDD",
          "dataset_description": "Used to evaluate the performance of the proposed image restoration algorithm, comparing it against CycleISP in terms of PSNR improvement.",
          "citing_paper_id": "270155352",
          "cited_paper_id": 212737191,
          "context_text": "Speciﬁcally, our algorithm achieves a 0.48 dB performance improvement over CycleISP [97] in terms of SIDD compared to the most recent methods.",
          "confidence_score": 0.7,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions 'SIDD' which is likely a dataset name, but does not provide enough information about its usage or specific characteristics. The citation intent is to compare performance against another method.",
          "citing_paper_doi": "10.1109/TMM.2024.3407656",
          "cited_paper_doi": "10.1109/CVPR42600.2020.00277",
          "citing_paper_url": "https://www.semanticscholar.org/paper/3a80830bf3e31c855afaff17a5b590bc052839c6",
          "cited_paper_url": "https://www.semanticscholar.org/paper/51352023ed22068fbfaa7693fc37123c0a5fce9f",
          "citing_paper_year": 2024,
          "cited_paper_year": 2020
        },
        {
          "dataset_name": "SIDD",
          "dataset_description": "Used to evaluate image denoising performance, specifically comparing the proposed MAXIM-3S with other state-of-the-art methods like VDN, DANet, MIRNet, CycleISP, and MPRNet.",
          "citing_paper_id": "245837508",
          "cited_paper_id": 201667906,
          "context_text": "Visual examples for image denoising on SIDD [2] among VDN [104], DANet [105], MIRNet [107], CycleISP [106], MPRNet [108], and the proposed MAXIM-3S.",
          "confidence_score": 1.0,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions 'SIDD' which is a known dataset for image denoising. The other names mentioned are models or methods, not datasets.",
          "citing_paper_doi": "10.1109/CVPR52688.2022.00568",
          "cited_paper_doi": null,
          "citing_paper_url": "https://www.semanticscholar.org/paper/fbf68eb0cf8237cef59ebdb301569c79b9676ff9",
          "cited_paper_url": "https://www.semanticscholar.org/paper/696205a3c87bd0cb4bb8d8cacc437ebcfcdb07e3",
          "citing_paper_year": 2022,
          "cited_paper_year": 2019
        },
        {
          "dataset_name": "SIDD",
          "dataset_description": "Used to evaluate image denoising performance, specifically comparing the proposed MAXIM-3S with other state-of-the-art methods like VDN, DANet, MIRNet, CycleISP, and MPRNet.",
          "citing_paper_id": "245837508",
          "cited_paper_id": 212737191,
          "context_text": "Visual examples for image denoising on SIDD [2] among VDN [104], DANet [105], MIRNet [107], CycleISP [106], MPRNet [108], and the proposed MAXIM-3S.",
          "confidence_score": 1.0,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions 'SIDD' which is a known dataset for image denoising. The other names mentioned are models or methods, not datasets.",
          "citing_paper_doi": "10.1109/CVPR52688.2022.00568",
          "cited_paper_doi": "10.1109/CVPR42600.2020.00277",
          "citing_paper_url": "https://www.semanticscholar.org/paper/fbf68eb0cf8237cef59ebdb301569c79b9676ff9",
          "cited_paper_url": "https://www.semanticscholar.org/paper/51352023ed22068fbfaa7693fc37123c0a5fce9f",
          "citing_paper_year": 2022,
          "cited_paper_year": 2020
        },
        {
          "dataset_name": "SIDD",
          "dataset_description": "Used to evaluate image denoising performance, specifically comparing the proposed MAXIM-3S with other state-of-the-art methods like VDN, DANet, MIRNet, CycleISP, and MPRNet.",
          "citing_paper_id": "245837508",
          "cited_paper_id": 220495900,
          "context_text": "Visual examples for image denoising on SIDD [2] among VDN [104], DANet [105], MIRNet [107], CycleISP [106], MPRNet [108], and the proposed MAXIM-3S.",
          "confidence_score": 1.0,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions 'SIDD' which is a known dataset for image denoising. The other names mentioned are models or methods, not datasets.",
          "citing_paper_doi": "10.1109/CVPR52688.2022.00568",
          "cited_paper_doi": "10.1007/978-3-030-58607-2_3",
          "citing_paper_url": "https://www.semanticscholar.org/paper/fbf68eb0cf8237cef59ebdb301569c79b9676ff9",
          "cited_paper_url": "https://www.semanticscholar.org/paper/65025bb97b0c23d8aac779fe06a545f5d919133a",
          "citing_paper_year": 2022,
          "cited_paper_year": 2020
        },
        {
          "dataset_name": "SIDD",
          "dataset_description": "Used to conduct a pilot study exploring the MAXIM families of image restoration models, specifically evaluating MAXIM-FFT, -MLP, -gMLP, and -SA variants.",
          "citing_paper_id": "245837508",
          "cited_paper_id": 225039882,
          "context_text": "We conducted a pilot study using MAXIM-1S and -2S on SIDD [2] to explore the MAXIM families: MAXIM-FFT, -MLP, -gMLP (modeled in this paper), -SA, where we use the Fourier Transform filter [43, 74], spatial MLP [85], gMLP [53], and self-attention [23] on spatial axes using the same multiaxis approach (Fig.",
          "confidence_score": 1.0,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions the use of SIDD for a pilot study, which is a specific dataset used in image restoration research. No other datasets are explicitly named.",
          "citing_paper_doi": "10.1109/CVPR52688.2022.00568",
          "cited_paper_doi": null,
          "citing_paper_url": "https://www.semanticscholar.org/paper/fbf68eb0cf8237cef59ebdb301569c79b9676ff9",
          "cited_paper_url": "https://www.semanticscholar.org/paper/268d347e8a55b5eb82fb5e7d2f800e33c75ab18a",
          "citing_paper_year": 2022,
          "cited_paper_year": 2020
        },
        {
          "dataset_name": "SIDD",
          "dataset_description": "Used to conduct a pilot study exploring the MAXIM families of image restoration models, specifically evaluating MAXIM-FFT, -MLP, -gMLP, and -SA variants.",
          "citing_paper_id": "245837508",
          "cited_paper_id": 233714958,
          "context_text": "We conducted a pilot study using MAXIM-1S and -2S on SIDD [2] to explore the MAXIM families: MAXIM-FFT, -MLP, -gMLP (modeled in this paper), -SA, where we use the Fourier Transform filter [43, 74], spatial MLP [85], gMLP [53], and self-attention [23] on spatial axes using the same multiaxis approach (Fig.",
          "confidence_score": 1.0,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions the use of SIDD for a pilot study, which is a specific dataset used in image restoration research. No other datasets are explicitly named.",
          "citing_paper_doi": "10.1109/CVPR52688.2022.00568",
          "cited_paper_doi": null,
          "citing_paper_url": "https://www.semanticscholar.org/paper/fbf68eb0cf8237cef59ebdb301569c79b9676ff9",
          "cited_paper_url": "https://www.semanticscholar.org/paper/67571d29190faea9fbd104acd16274f8c4edf254",
          "citing_paper_year": 2022,
          "cited_paper_year": 2021
        },
        {
          "dataset_name": "SIDD",
          "dataset_description": "Used to conduct a pilot study exploring the MAXIM families of image restoration models, specifically evaluating MAXIM-FFT, -MLP, -gMLP, and -SA variants.",
          "citing_paper_id": "245837508",
          "cited_paper_id": 234336004,
          "context_text": "We conducted a pilot study using MAXIM-1S and -2S on SIDD [2] to explore the MAXIM families: MAXIM-FFT, -MLP, -gMLP (modeled in this paper), -SA, where we use the Fourier Transform filter [43, 74], spatial MLP [85], gMLP [53], and self-attention [23] on spatial axes using the same multiaxis approach (Fig.",
          "confidence_score": 1.0,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions the use of SIDD for a pilot study, which is a specific dataset used in image restoration research. No other datasets are explicitly named.",
          "citing_paper_doi": "10.1109/CVPR52688.2022.00568",
          "cited_paper_doi": "10.18653/v1/2022.naacl-main.319",
          "citing_paper_url": "https://www.semanticscholar.org/paper/fbf68eb0cf8237cef59ebdb301569c79b9676ff9",
          "cited_paper_url": "https://www.semanticscholar.org/paper/1f133158a8973fb33fea188f20517cd7e69bfe7f",
          "citing_paper_year": 2022,
          "cited_paper_year": 2021
        },
        {
          "dataset_name": "SIDD",
          "dataset_description": "Used to conduct a pilot study exploring the MAXIM families of image restoration models, specifically evaluating MAXIM-FFT, -MLP, -gMLP, and -SA variants.",
          "citing_paper_id": "245837508",
          "cited_paper_id": 234742218,
          "context_text": "We conducted a pilot study using MAXIM-1S and -2S on SIDD [2] to explore the MAXIM families: MAXIM-FFT, -MLP, -gMLP (modeled in this paper), -SA, where we use the Fourier Transform filter [43, 74], spatial MLP [85], gMLP [53], and self-attention [23] on spatial axes using the same multiaxis approach (Fig.",
          "confidence_score": 1.0,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions the use of SIDD for a pilot study, which is a specific dataset used in image restoration research. No other datasets are explicitly named.",
          "citing_paper_doi": "10.1109/CVPR52688.2022.00568",
          "cited_paper_doi": null,
          "citing_paper_url": "https://www.semanticscholar.org/paper/fbf68eb0cf8237cef59ebdb301569c79b9676ff9",
          "cited_paper_url": "https://www.semanticscholar.org/paper/e3a3e85c5a32af29e13b3561f6cf070de70651de",
          "citing_paper_year": 2022,
          "cited_paper_year": 2021
        },
        {
          "dataset_name": "SIDD",
          "dataset_description": "Used for validating deblurring algorithms, focusing on real-world image degradation and noise. | Used for raindrop removal, focusing on realistic raindrop effects on images. | Used for low-light image enhancement, containing pairs of low-light and normal-light images. | Used for evaluating denoising algorithms, containing 68 color images with varying levels of noise. | Used for haze removal, focusing on synthetic and real-world hazy images. | Used for motion deblurring, containing pairs of blurred and sharp images from dynamic scenes. | Used for rain removal, containing synthetic rain images with ground truth. | Used for super-resolution tasks, providing high-quality images for validation. | Used for dehazing, specifically for outdoor scenes with varying atmospheric conditions.",
          "citing_paper_id": "264145824",
          "cited_paper_id": 201624746,
          "context_text": "These include the SIDD validation set [1], CBSD68 [33], DIV2K validation set [2], GoPro test set [36], HIDE [53], LOL [65], SOTS-Outdoor [28], Rain100 [68], and Raindrop datasets [42], each corresponding to their respective tasks.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context lists multiple datasets used for various image restoration tasks, which align with the research topic of all-in-one image restoration.",
          "citing_paper_doi": "10.48550/arXiv.2310.10123",
          "cited_paper_doi": "10.1109/ICCV.2019.00567",
          "citing_paper_url": "https://www.semanticscholar.org/paper/8efc22d25fc33d73aa8de1044e5afe775ed87e31",
          "cited_paper_url": "https://www.semanticscholar.org/paper/20a4b3353579525f0b76ec42e17a2284b4453f9a",
          "citing_paper_year": 2023,
          "cited_paper_year": 2019
        }
      ]
    },
    {
      "cited_paper_id": "8282555",
      "citation_count": 0,
      "total_dataset_mentions": 19,
      "unique_datasets": [
        "BSD68"
      ],
      "dataset_details": [
        {
          "dataset_name": "BSD68",
          "dataset_description": "Used for denoising and deblurring experiments, providing a diverse set of natural images with ground truth annotations. | Used for deblurring tasks, containing real-world motion-blurred images paired with sharp reference images. | Used to test image denoising algorithms, focusing on performance evaluation and benchmarking. | Used for evaluating image restoration methods, focusing on high-resolution urban scenes with detailed textures. | Used for denoising and deblurring, featuring a large collection of images with various noise levels and blur types.",
          "citing_paper_id": "270869793",
          "cited_paper_id": 64193,
          "context_text": "Benchmarks and evaluation metric: For the image denoising task, we conduct testing on the BSD68 [36] dataset.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions the BSD68 dataset, which is a specific, verifiable dataset used for image denoising tasks.",
          "citing_paper_doi": "10.48550/arXiv.2407.00676",
          "cited_paper_doi": "10.1109/ICCV.2001.937655",
          "citing_paper_url": "https://www.semanticscholar.org/paper/ca95c6148941dea8e67f05146dded1f43e6b8f8a",
          "cited_paper_url": "https://www.semanticscholar.org/paper/9a1ed876196ec9733acb1daa6d65e35ff0414291",
          "citing_paper_year": 2024,
          "cited_paper_year": 2001
        },
        {
          "dataset_name": "BSD68",
          "dataset_description": "Applied for video deblurring, testing the effectiveness of motion blur removal in video sequences. | Used to assess super-resolution methods, focusing on high-resolution reconstruction of urban scenes. | Used for evaluating image deraining algorithms, focusing on synthetic rain streaks on images. | Used to assess image restoration techniques in urban scenes, emphasizing visual fidelity and detail preservation. | Used to evaluate image denoising performance at various noise levels, focusing on restoration quality metrics. | Used for low-light image enhancement, evaluating the ability to improve visibility in dark conditions. | Used to test image restoration algorithms on high-quality images, evaluating their effectiveness across different noise levels. | Utilized for outdoor scene dehazing, assessing the performance of haze removal techniques. | Employed for image denoising, specifically evaluating noise reduction in grayscale images. | Utilized for general image quality assessment, evaluating various restoration techniques on a diverse set of images.",
          "citing_paper_id": "260085391",
          "cited_paper_id": null,
          "context_text": "BSD68 [35] Urban100 [18] Kodak24 [12] Method σ=15 σ=25 σ=50 σ=15 σ=25 σ=50 σ=15 σ=25 σ=50",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The citation mentions specific datasets used for image restoration experiments, which are relevant to the research topic.",
          "citing_paper_doi": "10.1109/CVPR52729.2023.00564",
          "cited_paper_doi": null,
          "citing_paper_url": "https://www.semanticscholar.org/paper/6eb039fa12dc81be85c24a10dda5522c629b0b12",
          "cited_paper_url": null,
          "citing_paper_year": 2023,
          "cited_paper_year": null
        },
        {
          "dataset_name": "BSD68",
          "dataset_description": "Used to test image restoration methods, focusing on natural images with human-segmented annotations. | Used to evaluate image restoration techniques, specifically targeting urban scenes with complex structures.",
          "citing_paper_id": "270045495",
          "cited_paper_id": 64193,
          "context_text": "Testing is conducted on the BSD68 [28] and Urban100 [16] datasets.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions two specific datasets, BSD68 and Urban100, which are used for testing image restoration methods.",
          "citing_paper_doi": "10.48550/arXiv.2405.15475",
          "cited_paper_doi": "10.1109/ICCV.2001.937655",
          "citing_paper_url": "https://www.semanticscholar.org/paper/1dc809107ceb27fc74071934878fe2dc9292e57d",
          "cited_paper_url": "https://www.semanticscholar.org/paper/9a1ed876196ec9733acb1daa6d65e35ff0414291",
          "citing_paper_year": 2024,
          "cited_paper_year": 2001
        },
        {
          "dataset_name": "BSD68",
          "dataset_description": "Used to evaluate denoising methods, focusing on image quality and restoration performance in a controlled benchmark setting. | Used to assess deraining techniques, emphasizing the removal of rain streaks and preservation of image details in urban scenes.",
          "citing_paper_id": "265609570",
          "cited_paper_id": 996788,
          "context_text": "BSD68 [41] Urban100 [5] setting, we integrate specific networks designed for particular tasks, such as denoising [8, 24, 27, 28, 66] methods and deraining [12, 13, 25, 46, 50] schemes, in addition to the aforementioned approaches.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions BSD68 and Urban100, which are known datasets in image restoration. These datasets are used for evaluating denoising and deraining methods.",
          "citing_paper_doi": "10.48550/arXiv.2312.01677",
          "cited_paper_doi": "10.1109/TIP.2017.2662206",
          "citing_paper_url": "https://www.semanticscholar.org/paper/c2fbbe80f7374a3aade7dcc1c6617f91818ab729",
          "cited_paper_url": "https://www.semanticscholar.org/paper/0c00a328fa7cd56ee60338c54e89bd48310db80b",
          "citing_paper_year": 2023,
          "cited_paper_year": 2016
        },
        {
          "dataset_name": "BSD68",
          "dataset_description": "Used to evaluate denoising methods, focusing on image quality and restoration performance in a controlled benchmark setting. | Used to assess deraining techniques, emphasizing the removal of rain streaks and preservation of image details in urban scenes.",
          "citing_paper_id": "265609570",
          "cited_paper_id": 1887989,
          "context_text": "BSD68 [41] Urban100 [5] setting, we integrate specific networks designed for particular tasks, such as denoising [8, 24, 27, 28, 66] methods and deraining [12, 13, 25, 46, 50] schemes, in addition to the aforementioned approaches.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions BSD68 and Urban100, which are known datasets in image restoration. These datasets are used for evaluating denoising and deraining methods.",
          "citing_paper_doi": "10.48550/arXiv.2312.01677",
          "cited_paper_doi": "10.1109/ICIP.2007.4378954",
          "citing_paper_url": "https://www.semanticscholar.org/paper/c2fbbe80f7374a3aade7dcc1c6617f91818ab729",
          "cited_paper_url": "https://www.semanticscholar.org/paper/ea9f1c9fc983d6f34d4c8fd1d03e7f7ad365030d",
          "citing_paper_year": 2023,
          "cited_paper_year": 2007
        },
        {
          "dataset_name": "BSD68",
          "dataset_description": "Used to evaluate denoising methods, focusing on image quality and restoration performance in a controlled benchmark setting. | Used to assess deraining techniques, emphasizing the removal of rain streaks and preservation of image details in urban scenes.",
          "citing_paper_id": "265609570",
          "cited_paper_id": 3406592,
          "context_text": "BSD68 [41] Urban100 [5] setting, we integrate specific networks designed for particular tasks, such as denoising [8, 24, 27, 28, 66] methods and deraining [12, 13, 25, 46, 50] schemes, in addition to the aforementioned approaches.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions BSD68 and Urban100, which are known datasets in image restoration. These datasets are used for evaluating denoising and deraining methods.",
          "citing_paper_doi": "10.48550/arXiv.2312.01677",
          "cited_paper_doi": "10.1109/CVPR.2018.00079",
          "citing_paper_url": "https://www.semanticscholar.org/paper/c2fbbe80f7374a3aade7dcc1c6617f91818ab729",
          "cited_paper_url": "https://www.semanticscholar.org/paper/34aaa735409b666b8dc678c23acb600b1d87913b",
          "citing_paper_year": 2023,
          "cited_paper_year": 2018
        },
        {
          "dataset_name": "BSD68",
          "dataset_description": "Used to evaluate denoising methods, focusing on image quality and restoration performance in a controlled benchmark setting. | Used to assess deraining techniques, emphasizing the removal of rain streaks and preservation of image details in urban scenes.",
          "citing_paper_id": "265609570",
          "cited_paper_id": 10514149,
          "context_text": "BSD68 [41] Urban100 [5] setting, we integrate specific networks designed for particular tasks, such as denoising [8, 24, 27, 28, 66] methods and deraining [12, 13, 25, 46, 50] schemes, in addition to the aforementioned approaches.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions BSD68 and Urban100, which are known datasets in image restoration. These datasets are used for evaluating denoising and deraining methods.",
          "citing_paper_doi": "10.48550/arXiv.2312.01677",
          "cited_paper_doi": "10.1109/TIP.2018.2839891",
          "citing_paper_url": "https://www.semanticscholar.org/paper/c2fbbe80f7374a3aade7dcc1c6617f91818ab729",
          "cited_paper_url": "https://www.semanticscholar.org/paper/e248ac3596d48ce338244624c2fd194dc0651bc6",
          "citing_paper_year": 2023,
          "cited_paper_year": 2017
        },
        {
          "dataset_name": "BSD68",
          "dataset_description": "Used to evaluate denoising methods, focusing on image quality and restoration performance in a controlled benchmark setting. | Used to assess deraining techniques, emphasizing the removal of rain streaks and preservation of image details in urban scenes.",
          "citing_paper_id": "265609570",
          "cited_paper_id": 203042751,
          "context_text": "BSD68 [41] Urban100 [5] setting, we integrate specific networks designed for particular tasks, such as denoising [8, 24, 27, 28, 66] methods and deraining [12, 13, 25, 46, 50] schemes, in addition to the aforementioned approaches.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions BSD68 and Urban100, which are known datasets in image restoration. These datasets are used for evaluating denoising and deraining methods.",
          "citing_paper_doi": "10.48550/arXiv.2312.01677",
          "cited_paper_doi": "10.1016/j.neunet.2019.08.022",
          "citing_paper_url": "https://www.semanticscholar.org/paper/c2fbbe80f7374a3aade7dcc1c6617f91818ab729",
          "cited_paper_url": "https://www.semanticscholar.org/paper/b876422091c91ef739c958737f3f600a62ec9723",
          "citing_paper_year": 2023,
          "cited_paper_year": 2020
        },
        {
          "dataset_name": "BSD68",
          "dataset_description": "Used as test data for image denoising, assessing the effectiveness of restoration techniques on high-quality images. | Used as test data for image denoising, evaluating the performance of restoration methods on a diverse set of images. | Used as test data for image denoising, providing a benchmark for evaluating the quality of restored images. | Used as test data for image denoising, focusing on urban scenes to assess the robustness of restoration algorithms. | Used to evaluate the performance of the proposed ART method against other image restoration techniques, focusing on image quality metrics.",
          "citing_paper_id": "252693111",
          "cited_paper_id": null,
          "context_text": "We use BSD68 Martin et al. (2001), Kodak24 Franzen (1999), McMaster Zhang et al. (2011), and Urban100 as test data of image denoising.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions specific datasets used for testing image denoising, which are relevant to the topic of All-in-One Image Restoration.",
          "citing_paper_doi": "10.48550/arXiv.2210.01427",
          "cited_paper_doi": null,
          "citing_paper_url": "https://www.semanticscholar.org/paper/a1f7f5597fdbc54e58f1f2a1a640bf355e87a978",
          "cited_paper_url": null,
          "citing_paper_year": 2022,
          "cited_paper_year": null
        },
        {
          "dataset_name": "BSD68",
          "dataset_description": "Used to test image restoration methods, focusing on urban scene images and evaluating super-resolution techniques. | Used to test image restoration methods, focusing on natural image denoising and evaluating algorithm performance. | Used for comparative analysis of image denoising methods, focusing on evaluating the performance of all-in-one restoration techniques. | Used for denoising comparisons in a single-task setting, focusing on image restoration performance and algorithm evaluation.",
          "citing_paper_id": "268856875",
          "cited_paper_id": 64193,
          "context_text": "S2: Comparative analysis of image denoising by all-in-one methods on the BSD68 dataset [30].",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions the BSD68 dataset, which is a specific dataset used for image denoising evaluation. The dataset is clearly identified and used for comparative analysis.",
          "citing_paper_doi": "10.48550/arXiv.2404.02154",
          "cited_paper_doi": "10.1109/ICCV.2001.937655",
          "citing_paper_url": "https://www.semanticscholar.org/paper/1d99d28c3cf5a21671a9cf5de8b51009c72d4624",
          "cited_paper_url": "https://www.semanticscholar.org/paper/9a1ed876196ec9733acb1daa6d65e35ff0414291",
          "citing_paper_year": 2024,
          "cited_paper_year": 2001
        },
        {
          "dataset_name": "BSD68",
          "dataset_description": "Used to test image restoration methods, focusing on urban scene images and evaluating super-resolution techniques. | Used to evaluate the performance of the proposed DyNet-L method at a noise level of σ = 50, focusing on image restoration quality. | Used to test image restoration methods, focusing on natural image denoising and evaluating algorithm performance. | Used for denoising comparisons in a single-task setting, focusing on image restoration performance and algorithm evaluation.",
          "citing_paper_id": "268856875",
          "cited_paper_id": 8282555,
          "context_text": "Table 4: Denoising comparisons in the single-task setting on BSD68 [30] and Ur-ban100 [20] datasets.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions two specific datasets, BSD68 and Urban100, which are used for denoising comparisons in a single-task setting.",
          "citing_paper_doi": "10.48550/arXiv.2404.02154",
          "cited_paper_doi": "10.1109/CVPR.2015.7299156",
          "citing_paper_url": "https://www.semanticscholar.org/paper/1d99d28c3cf5a21671a9cf5de8b51009c72d4624",
          "cited_paper_url": "https://www.semanticscholar.org/paper/2946c4e29b057d3543f09b89f547965895676d19",
          "citing_paper_year": 2024,
          "cited_paper_year": 2015
        },
        {
          "dataset_name": "BSD68",
          "dataset_description": "Used to test image restoration methods, focusing on natural images with human-segmented annotations. | Used to evaluate image restoration techniques, specifically targeting urban scenes with complex structures.",
          "citing_paper_id": "259224666",
          "cited_paper_id": 64193,
          "context_text": "Testing is performed on BSD68 [41] and Urban100 [22] datasets.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions two specific datasets, BSD68 and Urban100, which are used for testing image restoration methods.",
          "citing_paper_doi": "10.48550/arXiv.2306.13090",
          "cited_paper_doi": "10.1109/ICCV.2001.937655",
          "citing_paper_url": "https://www.semanticscholar.org/paper/c17f50017272c908cebdf2181675b7c6406b7218",
          "cited_paper_url": "https://www.semanticscholar.org/paper/9a1ed876196ec9733acb1daa6d65e35ff0414291",
          "citing_paper_year": 2023,
          "cited_paper_year": 2001
        },
        {
          "dataset_name": "BSD68",
          "dataset_description": "Used to assess image restoration quality in urban scenes, evaluating denoising effectiveness at different noise levels (σ = 15, 25, 50) with a grouping constraint in luminance-chrominance space. | Used to evaluate image denoising performance at various noise levels (σ = 15, 25, 50), focusing on color image restoration using sparse 3D collaborative filtering.",
          "citing_paper_id": "259224666",
          "cited_paper_id": 1887989,
          "context_text": "BSD68 [41] Urban100 [22] Method σ = 15 σ = 25 σ = 50 σ = 15 σ = 25 σ = 50 CBM3D [11] 33.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions BSD68 and Urban100, which are known image datasets used for image restoration tasks. The citation intent is to reference reusable resources, and the resource type is a dataset.",
          "citing_paper_doi": "10.48550/arXiv.2306.13090",
          "cited_paper_doi": "10.1109/ICIP.2007.4378954",
          "citing_paper_url": "https://www.semanticscholar.org/paper/c17f50017272c908cebdf2181675b7c6406b7218",
          "cited_paper_url": "https://www.semanticscholar.org/paper/ea9f1c9fc983d6f34d4c8fd1d03e7f7ad365030d",
          "citing_paper_year": 2023,
          "cited_paper_year": 2007
        },
        {
          "dataset_name": "BSD68",
          "dataset_description": "Used to evaluate image restoration methods, focusing on denoising performance and visual quality. | Employed to test image restoration models, focusing on high-resolution urban scene images and their structural integrity. | Utilized for evaluating image restoration algorithms, particularly for assessing texture and edge preservation. | Used to assess image restoration techniques, emphasizing color fidelity and detail preservation.",
          "citing_paper_id": "275133634",
          "cited_paper_id": 15614128,
          "context_text": "For evaluation, we utilize BSD68 [35], Kodak24, McMaster [62], and Urban100 as test set.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions specific datasets used for evaluation in the research on image restoration. These datasets are commonly used for testing image restoration algorithms.",
          "citing_paper_doi": "10.48550/arXiv.2412.20066",
          "cited_paper_doi": "10.1117/1.3600632",
          "citing_paper_url": "https://www.semanticscholar.org/paper/2b78fc275b7fc3a37061620ae155c017ab6c22e1",
          "cited_paper_url": "https://www.semanticscholar.org/paper/47b9a4dfbff5d2bb8b4f8f6e6fc2294426aab8ac",
          "citing_paper_year": 2024,
          "cited_paper_year": 2011
        },
        {
          "dataset_name": "BSD68",
          "dataset_description": "Used to evaluate trained models in image restoration, focusing on urban scenes with high-resolution images. | Used to evaluate trained models in image restoration, focusing on natural images with human segmentation annotations. | Used as a baseline for comparison in image restoration evaluation, highlighting the performance differences with Urban100. | Used to evaluate image restoration performance, focusing on higher resolution and quality images, demonstrating significant performance gains.",
          "citing_paper_id": "271274210",
          "cited_paper_id": 64193,
          "context_text": "The trained models are evaluated on BSD68 [74] and Urban100 [78].",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions two specific datasets, BSD68 and Urban100, which are used for evaluating trained models in the context of image restoration.",
          "citing_paper_doi": "10.1109/TCSVT.2024.3429557",
          "cited_paper_doi": "10.1109/ICCV.2001.937655",
          "citing_paper_url": "https://www.semanticscholar.org/paper/1091511c40eebb0f8e964246c91e91a85480dbf9",
          "cited_paper_url": "https://www.semanticscholar.org/paper/9a1ed876196ec9733acb1daa6d65e35ff0414291",
          "citing_paper_year": 2024,
          "cited_paper_year": 2001
        },
        {
          "dataset_name": "BSD68",
          "dataset_description": "Used for testing image restoration methods, specifically focusing on denoising performance on natural images.",
          "citing_paper_id": "274305801",
          "cited_paper_id": 64193,
          "context_text": "Testing is conducted on the BSD68 [32].",
          "confidence_score": 1.0,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "BSD68 is a well-known dataset in image processing and computer vision, specifically for image denoising tasks. The context indicates it is used for testing.",
          "citing_paper_doi": "10.48550/arXiv.2411.18466",
          "cited_paper_doi": "10.1109/ICCV.2001.937655",
          "citing_paper_url": "https://www.semanticscholar.org/paper/2850f4c1f43789f0a8a48e7bb83cf2a5e0561e54",
          "cited_paper_url": "https://www.semanticscholar.org/paper/9a1ed876196ec9733acb1daa6d65e35ff0414291",
          "citing_paper_year": 2024,
          "cited_paper_year": 2001
        },
        {
          "dataset_name": "BSD68",
          "dataset_description": "Used for evaluating image demosaicing, emphasizing the quality of restored images and the effectiveness of the restoration algorithm. | Used for evaluating image demosaicing performance, focusing on color accuracy and detail preservation in natural images. | Used for evaluating image demosaicing, specifically assessing the restoration of high-frequency details in urban scenes. | Used to assess denoising effectiveness on a diverse set of natural images, emphasizing robustness across different image types. | Used for evaluating image demosaicing, focusing on the restoration of detailed textures and edges in diverse image content. | Used to test denoising algorithms on high-resolution urban scenes, highlighting detail preservation and noise reduction. | Used to evaluate image denoising performance, focusing on natural image quality and visual fidelity. | Used for qualitative evaluation of image restoration techniques, focusing on urban scene images and their restoration quality. | Used for qualitative evaluation of image restoration techniques, focusing on natural image quality and restoration accuracy.",
          "citing_paper_id": "260748050",
          "cited_paper_id": 64193,
          "context_text": "We further present qualitative evaluations on BSD68 and Urban100.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions two specific datasets, BSD68 and Urban100, which are commonly used in image restoration and segmentation tasks.",
          "citing_paper_doi": "10.1007/s11263-023-01843-5",
          "cited_paper_doi": "10.1109/ICCV.2001.937655",
          "citing_paper_url": "https://www.semanticscholar.org/paper/e98cc6c252a7d69898730df17449fc1a09d1a1ae",
          "cited_paper_url": "https://www.semanticscholar.org/paper/9a1ed876196ec9733acb1daa6d65e35ff0414291",
          "citing_paper_year": 2023,
          "cited_paper_year": 2001
        },
        {
          "dataset_name": "BSD68",
          "dataset_description": "Used for evaluating denoising algorithms, focusing on high-resolution urban scenes with diverse textures and structures. | Used to obtain blurred images, focusing on deblurring techniques to restore sharpness and clarity. | Used to obtain clean images for benchmarking, focusing on denoising and restoration performance evaluation. | Used to obtain rainy images, focusing on rain removal to enhance image quality and detail. | Used to obtain low-light images for evaluation, focusing on enhancing visibility and reducing noise in dark conditions. | Used to evaluate image denoising performance, focusing on urban scenes with complex textures and structures. | Used to assess denoising effectiveness in the single-task setting, comprising a diverse set of natural images. | Used to assess image denoising quality, featuring a diverse set of natural images with high detail. | Used to evaluate denoising performance in the single-task setting, focusing on high-resolution urban images. | Used to test image denoising effectiveness, consisting of grayscale images with various degradation levels. | Used to obtain synthetic and real-world hazy images, focusing on dehazing techniques to improve clarity and color accuracy. | Used for evaluating denoising algorithms, focusing on natural images with human-segmented ground truth.",
          "citing_paper_id": "268553835",
          "cited_paper_id": 64193,
          "context_text": "Denoising task evaluation is performed on the BSD68 [41] and Urban100 [25] datasets.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions two specific datasets, BSD68 and Urban100, which are used for denoising task evaluation. These datasets are clearly identified and relevant to the research topic of image restoration.",
          "citing_paper_doi": "10.48550/arXiv.2403.14614",
          "cited_paper_doi": "10.1109/ICCV.2001.937655",
          "citing_paper_url": "https://www.semanticscholar.org/paper/8cf850abfec7ce894b498ef082282382e8d3374c",
          "cited_paper_url": "https://www.semanticscholar.org/paper/9a1ed876196ec9733acb1daa6d65e35ff0414291",
          "citing_paper_year": 2024,
          "cited_paper_year": 2001
        },
        {
          "dataset_name": "BSD68",
          "dataset_description": "Used to assess image restoration performance, specifically on urban scenes with complex structures and textures. | Used to evaluate image restoration methods, focusing on natural images with human-segmented ground truth.",
          "citing_paper_id": "278528939",
          "cited_paper_id": 64193,
          "context_text": "Evaluation is performed on the BSD68 [53] and Urban100 [54] datasets.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions two specific datasets, BSD68 and Urban100, which are used for evaluation in the research. These datasets are relevant to image restoration.",
          "citing_paper_doi": "10.1109/TIP.2025.3567205",
          "cited_paper_doi": "10.1109/ICCV.2001.937655",
          "citing_paper_url": "https://www.semanticscholar.org/paper/99b442f68dce288fd1f5d30858f7ac8b8d0068e9",
          "cited_paper_url": "https://www.semanticscholar.org/paper/9a1ed876196ec9733acb1daa6d65e35ff0414291",
          "citing_paper_year": 2025,
          "cited_paper_year": 2001
        }
      ]
    },
    {
      "cited_paper_id": "52285314",
      "citation_count": 0,
      "total_dataset_mentions": 17,
      "unique_datasets": [
        "Rain100L"
      ],
      "dataset_details": [
        {
          "dataset_name": "Rain100L",
          "dataset_description": "Used for denoising, combined with BSD400 to enhance image quality by reducing noise. | Used for low-light enhancement, improving the visibility and quality of images captured in low-light conditions. | Used for deblurring, focusing on restoring sharpness in blurred images using deep learning techniques. | Used for denoising, combined with WED to reduce noise in images while preserving edges and details. | Used for deraining, focusing on removing rain streaks from images using deep learning methods. | Used for dehazing, specifically employing the Indoor Training Set (ITS) to improve visibility in hazy images.",
          "citing_paper_id": "273643057",
          "cited_paper_id": 15443600,
          "context_text": "Specifically, we employ Rain100L [35] for deraining, the Indoor Training Set (ITS) from the RESIDE [16] dataset for dehazing, a combination of BSD400 [2] and WED [23] for denoising, GoPro [24] for deblurring, and LOL [33] for low-light enhancement.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The citation clearly mentions specific datasets used for various image restoration tasks, which align with the research topic of all-in-one image restoration.",
          "citing_paper_doi": "10.1145/3664647.3680762",
          "cited_paper_doi": "10.1109/CVPR.2017.183",
          "citing_paper_url": "https://www.semanticscholar.org/paper/f292c903c1571ba0ee97aacdba29b7008b2fe501",
          "cited_paper_url": "https://www.semanticscholar.org/paper/6de27f256e97c014a95706c72af7fa651e8fa34a",
          "citing_paper_year": 2024,
          "cited_paper_year": 2016
        },
        {
          "dataset_name": "Rain100L",
          "dataset_description": "Used for denoising, combined with BSD400 to enhance image quality by reducing noise. | Used for low-light enhancement, improving the visibility and quality of images captured in low-light conditions. | Used for deblurring, focusing on restoring sharpness in blurred images using deep learning techniques. | Used for denoising, combined with WED to reduce noise in images while preserving edges and details. | Used for deraining, focusing on removing rain streaks from images using deep learning methods. | Used for dehazing, specifically employing the Indoor Training Set (ITS) to improve visibility in hazy images.",
          "citing_paper_id": "273643057",
          "cited_paper_id": 206764694,
          "context_text": "Specifically, we employ Rain100L [35] for deraining, the Indoor Training Set (ITS) from the RESIDE [16] dataset for dehazing, a combination of BSD400 [2] and WED [23] for denoising, GoPro [24] for deblurring, and LOL [33] for low-light enhancement.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The citation clearly mentions specific datasets used for various image restoration tasks, which align with the research topic of all-in-one image restoration.",
          "citing_paper_doi": "10.1145/3664647.3680762",
          "cited_paper_doi": "10.1109/TPAMI.2010.161",
          "citing_paper_url": "https://www.semanticscholar.org/paper/f292c903c1571ba0ee97aacdba29b7008b2fe501",
          "cited_paper_url": "https://www.semanticscholar.org/paper/0e5a262bf59b68ba8a7a1103d16fa33a9f5ffc28",
          "citing_paper_year": 2024,
          "cited_paper_year": 2011
        },
        {
          "dataset_name": "Rain100L",
          "dataset_description": "Used for the image deraining task, containing 100 pairs of original and rainy images to train and evaluate deraining algorithms.",
          "citing_paper_id": "270869793",
          "cited_paper_id": 219530930,
          "context_text": "For the image deraining task, we employ the Rain100L [59] dataset, which contains 100 pairs of original images and their corresponding rainy images.",
          "confidence_score": 1.0,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context clearly mentions the use of the Rain100L dataset for the image deraining task, providing a specific dataset name and its purpose.",
          "citing_paper_doi": "10.48550/arXiv.2407.00676",
          "cited_paper_doi": "10.1109/cvpr42600.2020.00583",
          "citing_paper_url": "https://www.semanticscholar.org/paper/ca95c6148941dea8e67f05146dded1f43e6b8f8a",
          "cited_paper_url": "https://www.semanticscholar.org/paper/c209d9c0d49b2377860acad2acbcc13523a40b7f",
          "citing_paper_year": 2024,
          "cited_paper_year": 2020
        },
        {
          "dataset_name": "Rain100L",
          "dataset_description": "Used to train and evaluate the AirNet model, focusing on overfitting issues in image de-raining. The dataset contains synthetic rain images for testing and validating the model's performance.",
          "citing_paper_id": "266643865",
          "cited_paper_id": 49333383,
          "context_text": "Although the AirNet introduces the attention mechanisms and contrastive learning, the overﬁt-ting knowledge on the dataset Rain100 L makes it intractable [12], (c) HL [66], (d) TSDNet [28], (e) TransWeather [20] and (f) AiOENet, respectively.",
          "confidence_score": 1.0,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions 'Rain100 L' as a dataset used for training and evaluating the AirNet model, which is relevant to image restoration.",
          "citing_paper_doi": "10.1109/TIV.2023.3347952",
          "cited_paper_doi": "10.18535/ijecs/v5i1.12",
          "citing_paper_url": "https://www.semanticscholar.org/paper/11c52757c7dd3d50fc62fccc7971c6d3894e28d0",
          "cited_paper_url": "https://www.semanticscholar.org/paper/fb7621c9bf7acdca786ea4619dd0f296994196c9",
          "citing_paper_year": 2024,
          "cited_paper_year": 2016
        },
        {
          "dataset_name": "Rain100L",
          "dataset_description": "Used to train and evaluate the AirNet model, focusing on overfitting issues in image de-raining. The dataset contains synthetic rain images for testing and validating the model's performance.",
          "citing_paper_id": "266643865",
          "cited_paper_id": 254737528,
          "context_text": "Although the AirNet introduces the attention mechanisms and contrastive learning, the overﬁt-ting knowledge on the dataset Rain100 L makes it intractable [12], (c) HL [66], (d) TSDNet [28], (e) TransWeather [20] and (f) AiOENet, respectively.",
          "confidence_score": 1.0,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions 'Rain100 L' as a dataset used for training and evaluating the AirNet model, which is relevant to image restoration.",
          "citing_paper_doi": "10.1109/TIV.2023.3347952",
          "cited_paper_doi": "10.1109/TII.2022.3170594",
          "citing_paper_url": "https://www.semanticscholar.org/paper/11c52757c7dd3d50fc62fccc7971c6d3894e28d0",
          "cited_paper_url": "https://www.semanticscholar.org/paper/80deea8b3581b3c394bf5398d7da958d720cc3a9",
          "citing_paper_year": 2024,
          "cited_paper_year": 2023
        },
        {
          "dataset_name": "Rain100L",
          "dataset_description": "Used for low-light enhancement evaluation, specifically to improve image quality in low-light conditions. | Used to assess deraining performance on real-world images, emphasizing diverse and challenging scenarios. | Used for deraining evaluation, focusing on removing rain streaks from images to enhance clarity. | Used for visual comparisons of dehazing results, focusing on ship detection in maritime scenes with precise annotations. | Used to quantitatively assess deraining method performance, focusing on synthetic rain images with ground truth. | Used for desnowing evaluation, specifically to remove snow from images to improve visual quality. | Used for visual comparisons of desnowing results, though the specific content and characteristics are not detailed. | Used to train and evaluate ship detection models, focusing on global image features and deraining performance using a large-scale annotated dataset. | Used to evaluate image restoration techniques, containing a diverse set of images with specific degradation types. | Used for visual comparisons of deraining results, focusing on ship detection in synthetic images. | Used for dehazing evaluation, focusing on improving visibility in hazy images through image restoration techniques. | Used to quantitatively and qualitatively evaluate dehazing results, focusing on image quality and performance metrics. | Used to evaluate low-light enhancement methods, focusing on improving image quality in low-light conditions. | Used to evaluate dehazing results on ship images, assessing the effectiveness of dehazing algorithms in maritime contexts. | Used for visual comparisons of low-light enhancement results, focusing on ship detection in synthetic images. | Used for visual comparisons of dehazing results, though specific details about the dataset content are not provided in the context. | Used to evaluate deraining methods on ship images, providing precise annotations for ship detection in rainy conditions. | Used for visual comparisons of low-light enhancement results, though specific details about the dataset are not provided. | Used to evaluate low-light enhancement methods, focusing on specific challenges in low-light scenarios. | Used to evaluate image restoration methods, focusing on outdoor scene images with various degradations. | Used to explore global image features and assess deraining performance, providing a dataset for evaluating the stability of the DualGCN model. | Used for visual comparisons of deraining results, focusing on synthetic image restoration.",
          "citing_paper_id": "266643865",
          "cited_paper_id": 52285314,
          "context_text": "3) Image Deraining: In our numerical deraining experiments, the datasets Rain100L [62], Seaships [64], and SMD [65] are employed to quantitatively assess the performance of different deraining methods.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions three specific datasets used for deraining experiments. The datasets are clearly named and their usage is described.",
          "citing_paper_doi": "10.1109/TIV.2023.3347952",
          "cited_paper_doi": "10.1109/TMM.2018.2865686",
          "citing_paper_url": "https://www.semanticscholar.org/paper/11c52757c7dd3d50fc62fccc7971c6d3894e28d0",
          "cited_paper_url": "https://www.semanticscholar.org/paper/dcb6f06631021811091ce691592b12a237c12907",
          "citing_paper_year": 2024,
          "cited_paper_year": 2018
        },
        {
          "dataset_name": "Rain100L",
          "dataset_description": "Used for low-light enhancement evaluation, specifically to improve image quality in low-light conditions. | Used to assess deraining performance on real-world images, emphasizing diverse and challenging scenarios. | Used for deraining evaluation, focusing on removing rain streaks from images to enhance clarity. | Used for visual comparisons of dehazing results, focusing on ship detection in maritime scenes with precise annotations. | Used to quantitatively assess deraining method performance, focusing on synthetic rain images with ground truth. | Used for desnowing evaluation, specifically to remove snow from images to improve visual quality. | Used for visual comparisons of desnowing results, though the specific content and characteristics are not detailed. | Used to evaluate image restoration techniques, containing a diverse set of images with specific degradation types. | Used for visual comparisons of deraining results, focusing on ship detection in synthetic images. | Used for dehazing evaluation, focusing on improving visibility in hazy images through image restoration techniques. | Used to quantitatively and qualitatively evaluate dehazing results, focusing on image quality and performance metrics. | Used to evaluate low-light enhancement methods, focusing on improving image quality in low-light conditions. | Used to evaluate dehazing results on ship images, assessing the effectiveness of dehazing algorithms in maritime contexts. | Used for visual comparisons of low-light enhancement results, focusing on ship detection in synthetic images. | Used for visual comparisons of dehazing results, though specific details about the dataset content are not provided in the context. | Used to evaluate deraining methods on ship images, providing precise annotations for ship detection in rainy conditions. | Used for visual comparisons of low-light enhancement results, though specific details about the dataset are not provided. | Used to evaluate low-light enhancement methods, focusing on specific challenges in low-light scenarios. | Used to evaluate image restoration methods, focusing on outdoor scene images with various degradations. | Used to explore global image features and assess deraining performance, providing a dataset for evaluating the stability of the DualGCN model. | Used for visual comparisons of deraining results, focusing on synthetic image restoration.",
          "citing_paper_id": "266643865",
          "cited_paper_id": null,
          "context_text": "3) Image Deraining: In our numerical deraining experiments, the datasets Rain100L [62], Seaships [64], and SMD [65] are employed to quantitatively assess the performance of different deraining methods.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions three specific datasets used for deraining experiments. The datasets are clearly named and their usage is described.",
          "citing_paper_doi": "10.1109/TIV.2023.3347952",
          "cited_paper_doi": null,
          "citing_paper_url": "https://www.semanticscholar.org/paper/11c52757c7dd3d50fc62fccc7971c6d3894e28d0",
          "cited_paper_url": null,
          "citing_paper_year": 2024,
          "cited_paper_year": null
        },
        {
          "dataset_name": "Rain100L",
          "dataset_description": "Used for single-task deraining experiments, focusing on texture preservation and rain removal effectiveness in images.",
          "citing_paper_id": "270045495",
          "cited_paper_id": 219530930,
          "context_text": "For single-task deraining, we use Rain100L [48].",
          "confidence_score": 1.0,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "Rain100L is a specific dataset used for image deraining, which is relevant to the research topic of all-in-one image restoration.",
          "citing_paper_doi": "10.48550/arXiv.2405.15475",
          "cited_paper_doi": "10.1109/cvpr42600.2020.00583",
          "citing_paper_url": "https://www.semanticscholar.org/paper/1dc809107ceb27fc74071934878fe2dc9292e57d",
          "cited_paper_url": "https://www.semanticscholar.org/paper/c209d9c0d49b2377860acad2acbcc13523a40b7f",
          "citing_paper_year": 2024,
          "cited_paper_year": 2020
        },
        {
          "dataset_name": "Rain100L",
          "dataset_description": "Used for deraining, specifically evaluating the performance of the proposed method on 200 training and 100 testing clean-rainy image pairs.",
          "citing_paper_id": "268856875",
          "cited_paper_id": 219530930,
          "context_text": "For deraining, we use Rain100L [50] with 200 training and 100 testing clean-rainy image pairs.",
          "confidence_score": 1.0,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "Rain100L is a specific dataset used for deraining tasks, which is directly relevant to the research topic of all-in-one image restoration.",
          "citing_paper_doi": "10.48550/arXiv.2404.02154",
          "cited_paper_doi": "10.1109/cvpr42600.2020.00583",
          "citing_paper_url": "https://www.semanticscholar.org/paper/1d99d28c3cf5a21671a9cf5de8b51009c72d4624",
          "cited_paper_url": "https://www.semanticscholar.org/paper/c209d9c0d49b2377860acad2acbcc13523a40b7f",
          "citing_paper_year": 2024,
          "cited_paper_year": 2020
        },
        {
          "dataset_name": "Rain100L",
          "dataset_description": "Used to evaluate deraining performance in the single-task setting, focusing on image restoration quality and effectiveness of the proposed method.",
          "citing_paper_id": "259224666",
          "cited_paper_id": 195489762,
          "context_text": "Method DehazeNet[4] MSCNN[46] AODNet[30] EPDN[44] FDGAN[14] AirNet[29] Restormer[68] Table 3: Deraining results in the single-task setting on Rain100L [16].",
          "confidence_score": 1.0,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions 'Rain100L' as a dataset used for deraining results. It is a specific dataset with a clear name and version tag.",
          "citing_paper_doi": "10.48550/arXiv.2306.13090",
          "cited_paper_doi": "10.1109/CVPR.2019.00835",
          "citing_paper_url": "https://www.semanticscholar.org/paper/c17f50017272c908cebdf2181675b7c6406b7218",
          "cited_paper_url": "https://www.semanticscholar.org/paper/0089d5b7e386ae6fccef17fb3b7df5c33b9588a4",
          "citing_paper_year": 2023,
          "cited_paper_year": 2019
        },
        {
          "dataset_name": "Rain100L",
          "dataset_description": "Used for training and testing single-task image deraining models, consisting of 200 clean-rainy image pairs for training and 100 pairs for testing. | Used to conduct ablation experiments for the deraining task, focusing on evaluating the effectiveness of the proposed method in removing rain from images.",
          "citing_paper_id": "259224666",
          "cited_paper_id": 219530930,
          "context_text": "We carry out this ablation experiment on Rain100L [65] for deraining task.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "Rain100L is a specific dataset used for the deraining task, which is relevant to the research topic of image restoration.",
          "citing_paper_doi": "10.48550/arXiv.2306.13090",
          "cited_paper_doi": "10.1109/cvpr42600.2020.00583",
          "citing_paper_url": "https://www.semanticscholar.org/paper/c17f50017272c908cebdf2181675b7c6406b7218",
          "cited_paper_url": "https://www.semanticscholar.org/paper/c209d9c0d49b2377860acad2acbcc13523a40b7f",
          "citing_paper_year": 2023,
          "cited_paper_year": 2020
        },
        {
          "dataset_name": "Rain100L",
          "dataset_description": "Used for evaluating the model's performance on light rain conditions, focusing on image quality and restoration accuracy. | Used for evaluating the model's performance on a large-scale test set, focusing on scalability and consistency. | Used for evaluating the model's performance on heavy rain conditions, focusing on image quality and restoration accuracy. | Used for evaluating the model's performance on a diverse set of test images, focusing on generalization and robustness.",
          "citing_paper_id": "261556765",
          "cited_paper_id": 3406592,
          "context_text": "For image deraining, we train our model on 13,712 clean-rain image pairs gathered from multiple datasets [27], [51], [52], [53], and perform evaluation on Rain100L [27], Rain100H [27], Test100 [53] and Test1200 [54].",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions specific datasets used for training and evaluation in the context of image deraining. These datasets are clearly identified and used for evaluating the performance of the model.",
          "citing_paper_doi": "10.1109/TCSVT.2024.3398810",
          "cited_paper_doi": "10.1109/CVPR.2018.00079",
          "citing_paper_url": "https://www.semanticscholar.org/paper/01a914eadebc6f20e1f436bd9a02381eccf215ae",
          "cited_paper_url": "https://www.semanticscholar.org/paper/34aaa735409b666b8dc678c23acb600b1d87913b",
          "citing_paper_year": 2023,
          "cited_paper_year": 2018
        },
        {
          "dataset_name": "Rain100L",
          "dataset_description": "Used for evaluating the model's performance on light rain conditions, focusing on image quality and restoration accuracy. | Used for dehazing, focusing on improving visibility in hazy images using a conditional generative adversarial network. | Used for deraining, focusing on removing rain streaks from images using a conditional generative adversarial network. | Used for evaluating the model's performance on a diverse set of test images, focusing on generalization and robustness. | Used for evaluating the model's performance on a large-scale test set, focusing on scalability and consistency. | Used for deblurring, focusing on sharpening blurred images using a conditional generative adversarial network. | Used for denoising, focusing on reducing noise in images using a conditional generative adversarial network. | Used for evaluating the model's performance on heavy rain conditions, focusing on image quality and restoration accuracy.",
          "citing_paper_id": "261556765",
          "cited_paper_id": 11922819,
          "context_text": "For image deraining, we train our model on 13,712 clean-rain image pairs gathered from multiple datasets [27], [51], [52], [53], and perform evaluation on Rain100L [27], Rain100H [27], Test100 [53] and Test1200 [54].",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions specific datasets used for training and evaluation in the context of image deraining. These datasets are clearly identified and used for evaluating the performance of the model.",
          "citing_paper_doi": "10.1109/TCSVT.2024.3398810",
          "cited_paper_doi": "10.1109/TCSVT.2019.2920407",
          "citing_paper_url": "https://www.semanticscholar.org/paper/01a914eadebc6f20e1f436bd9a02381eccf215ae",
          "cited_paper_url": "https://www.semanticscholar.org/paper/920f0c070701caabf023c600f3e310f1906ca818",
          "citing_paper_year": 2023,
          "cited_paper_year": 2017
        },
        {
          "dataset_name": "Rain100L",
          "dataset_description": "Used to compare image restoration methods for deraining, focusing on the effectiveness of different deraining techniques in the context of all-in-one image restoration.",
          "citing_paper_id": "267320695",
          "cited_paper_id": 195787503,
          "context_text": "In Figure 12, we compare with image restoration methods for deraining on Rain100L [21].",
          "confidence_score": 1.0,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions 'Rain100L' as a dataset used for comparing image restoration methods for deraining. The dataset name is specific and relevant to the research topic.",
          "citing_paper_doi": "10.1007/978-3-031-72764-1_1",
          "cited_paper_doi": "10.1109/TPAMI.2019.2925793",
          "citing_paper_url": "https://www.semanticscholar.org/paper/03ad1a40a4399c8b77bbeaa389fcd14b10b322c0",
          "cited_paper_url": "https://www.semanticscholar.org/paper/40762c365a3ef8dac29d499426669d97fea4f0d2",
          "citing_paper_year": 2024,
          "cited_paper_year": 2019
        },
        {
          "dataset_name": "Rain100L",
          "dataset_description": "Used to evaluate and compare the performance of MPRNet, NAFNet, and Restormer with and without the proposed TRC module in image restoration tasks.",
          "citing_paper_id": "269605449",
          "cited_paper_id": 231802205,
          "context_text": "We have now included a comparison on Rain100L dataset (Fan et al., 2019) between the MPRNet (Zamir et al., 2021), NAFNet (Chu et al., 2022), Restormer (Zamir et al., 2022) methods and the corresponding methods with the proposed TRC module.",
          "confidence_score": 1.0,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions the Rain100L dataset, which is a specific dataset used for evaluating image restoration methods. The dataset is used to compare the performance of different models, including MPRNet, NAFNet, and Restormer, with and without the proposed TRC module.",
          "citing_paper_doi": "10.48550/arXiv.2405.02843",
          "cited_paper_doi": "10.1109/CVPR46437.2021.01458",
          "citing_paper_url": "https://www.semanticscholar.org/paper/0c7542272e5edcff29608a0e37f9a3b705b363e7",
          "cited_paper_url": "https://www.semanticscholar.org/paper/92d50602db5746f03b91562e2cc8a98bec584e9b",
          "citing_paper_year": 2024,
          "cited_paper_year": 2021
        },
        {
          "dataset_name": "Rain100L",
          "dataset_description": "Used to evaluate and compare the performance of MPRNet, NAFNet, and Restormer with and without the proposed TRC module in image restoration tasks.",
          "citing_paper_id": "269605449",
          "cited_paper_id": 248239613,
          "context_text": "We have now included a comparison on Rain100L dataset (Fan et al., 2019) between the MPRNet (Zamir et al., 2021), NAFNet (Chu et al., 2022), Restormer (Zamir et al., 2022) methods and the corresponding methods with the proposed TRC module.",
          "confidence_score": 1.0,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions the Rain100L dataset, which is a specific dataset used for evaluating image restoration methods. The dataset is used to compare the performance of different models, including MPRNet, NAFNet, and Restormer, with and without the proposed TRC module.",
          "citing_paper_doi": "10.48550/arXiv.2405.02843",
          "cited_paper_doi": "10.1109/CVPRW56347.2022.00130",
          "citing_paper_url": "https://www.semanticscholar.org/paper/0c7542272e5edcff29608a0e37f9a3b705b363e7",
          "cited_paper_url": "https://www.semanticscholar.org/paper/b83ccbb93ca610612a88bc237444a79c7edf4db2",
          "citing_paper_year": 2024,
          "cited_paper_year": 2022
        },
        {
          "dataset_name": "Rain100L",
          "dataset_description": "Used to evaluate image deraining performance, comparing various models including RESCAN, PreNet, MSPFN, MPRNet, HINet, and MAXIM-2S.",
          "citing_paper_id": "245837508",
          "cited_paper_id": 59316941,
          "context_text": "Visual examples for image deraining on Rain100L [102] among RESCAN [48], PreNet [75], MSPFN [33], MPRNet [108], HINet [15], and our MAXIM-2S model.",
          "confidence_score": 1.0,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "Rain100L is mentioned as a specific dataset used for visual examples in the context of image deraining. The citation is clearly using this dataset for evaluation.",
          "citing_paper_doi": "10.1109/CVPR52688.2022.00568",
          "cited_paper_doi": "10.1109/CVPR.2019.00406",
          "citing_paper_url": "https://www.semanticscholar.org/paper/fbf68eb0cf8237cef59ebdb301569c79b9676ff9",
          "cited_paper_url": "https://www.semanticscholar.org/paper/6d411f15a0d17aba58006aa7d2eaebc66a0ca8c8",
          "citing_paper_year": 2022,
          "cited_paper_year": 2019
        }
      ]
    },
    {
      "cited_paper_id": "39760169",
      "citation_count": 0,
      "total_dataset_mentions": 17,
      "unique_datasets": [
        "RESIDE"
      ],
      "dataset_details": [
        {
          "dataset_name": "RESIDE",
          "dataset_description": "Used for dehazing in single-image rain removal, leveraging paired images to train and evaluate dehazing models. | Used for low-light image enhancement, offering paired low-light and normal-light images to train and test LLIE methods. | Used for motion deblurring, containing paired sharp and blurred images from dynamic scenes to train and evaluate deblurring models. | Used for deraining, providing a large dataset of paired rainy and clean images to train and test deraining algorithms.",
          "citing_paper_id": "272987034",
          "cited_paper_id": 1372266,
          "context_text": "For high-cost tasks that degradations are difficult to synthesize, we leverage existing paired datasets, including RE-SIDE [20] for dehazing, Rain13k [9, 25, 27, 35, 54] for deraining, GoPro [40] for motion deblurring, and LOL-v2 [55] for low-light image enhancement (LLIE).",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions specific datasets used for various image restoration tasks, which are directly relevant to the research topic of all-in-one image restoration.",
          "citing_paper_doi": "10.48550/arXiv.2409.19403",
          "cited_paper_doi": "10.1109/TIP.2017.2691802",
          "citing_paper_url": "https://www.semanticscholar.org/paper/cc6a57aa1b8496efd3b9001226fd09b8d121f397",
          "cited_paper_url": "https://www.semanticscholar.org/paper/93c49234abcf6ab8ac82140ae3ce5af797e9132b",
          "citing_paper_year": 2024,
          "cited_paper_year": 2016
        },
        {
          "dataset_name": "RESIDE",
          "dataset_description": "Used for dehazing in single-image rain removal, leveraging paired images to train and evaluate dehazing models. | Used for low-light image enhancement, offering paired low-light and normal-light images to train and test LLIE methods. | Used for motion deblurring, containing paired sharp and blurred images from dynamic scenes to train and evaluate deblurring models. | Used for deraining, providing a large dataset of paired rainy and clean images to train and test deraining algorithms.",
          "citing_paper_id": "272987034",
          "cited_paper_id": 8671030,
          "context_text": "For high-cost tasks that degradations are difficult to synthesize, we leverage existing paired datasets, including RE-SIDE [20] for dehazing, Rain13k [9, 25, 27, 35, 54] for deraining, GoPro [40] for motion deblurring, and LOL-v2 [55] for low-light image enhancement (LLIE).",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions specific datasets used for various image restoration tasks, which are directly relevant to the research topic of all-in-one image restoration.",
          "citing_paper_doi": "10.48550/arXiv.2409.19403",
          "cited_paper_doi": "10.1109/CVPR.2017.35",
          "citing_paper_url": "https://www.semanticscholar.org/paper/cc6a57aa1b8496efd3b9001226fd09b8d121f397",
          "cited_paper_url": "https://www.semanticscholar.org/paper/65b16da51891a6b98140d425804c8a0fd0299219",
          "citing_paper_year": 2024,
          "cited_paper_year": 2016
        },
        {
          "dataset_name": "RESIDE",
          "dataset_description": "Used for evaluating dehazing algorithms, focusing on outdoor scenes to assess performance in removing atmospheric haze. | Used for evaluating deraining algorithms, combining multiple test sets to assess performance in removing rain streaks from images. | Used for motion deblurring, containing paired sharp and blurred images from dynamic scenes to train and evaluate deblurring models. | Used for evaluating kernel estimation in image restoration, focusing on the accuracy of kernel prediction for various degradation types. | Used for evaluating denoising algorithms, focusing on reducing noise while preserving image details. | Used for dehazing in single-image rain removal, leveraging paired images to train and evaluate dehazing models. | Used for evaluating low-light enhancement algorithms, focusing on improving visibility in low-light conditions. | Used for deraining, providing a large dataset of paired rainy and clean images to train and test deraining algorithms. | Used for low-light image enhancement, offering paired low-light and normal-light images to train and test LLIE methods. | Used for evaluating motion deblurring algorithms, focusing on real-world motion blur to assess the clarity and sharpness of restored images.",
          "citing_paper_id": "272987034",
          "cited_paper_id": 39760169,
          "context_text": "For high-cost tasks that degradations are difficult to synthesize, we leverage existing paired datasets, including RE-SIDE [20] for dehazing, Rain13k [9, 25, 27, 35, 54] for deraining, GoPro [40] for motion deblurring, and LOL-v2 [55] for low-light image enhancement (LLIE).",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions specific datasets used for various image restoration tasks, which are directly relevant to the research topic of all-in-one image restoration.",
          "citing_paper_doi": "10.48550/arXiv.2409.19403",
          "cited_paper_doi": "10.1109/TIP.2018.2867951",
          "citing_paper_url": "https://www.semanticscholar.org/paper/cc6a57aa1b8496efd3b9001226fd09b8d121f397",
          "cited_paper_url": "https://www.semanticscholar.org/paper/abf75b81b96ff2cd262086675deba55f907108db",
          "citing_paper_year": 2024,
          "cited_paper_year": 2017
        },
        {
          "dataset_name": "RESIDE",
          "dataset_description": "Used for dehazing in single-image rain removal, leveraging paired images to train and evaluate dehazing models. | Used for low-light image enhancement, offering paired low-light and normal-light images to train and test LLIE methods. | Used for motion deblurring, containing paired sharp and blurred images from dynamic scenes to train and evaluate deblurring models. | Used for deraining, providing a large dataset of paired rainy and clean images to train and test deraining algorithms.",
          "citing_paper_id": "272987034",
          "cited_paper_id": 209376714,
          "context_text": "For high-cost tasks that degradations are difficult to synthesize, we leverage existing paired datasets, including RE-SIDE [20] for dehazing, Rain13k [9, 25, 27, 35, 54] for deraining, GoPro [40] for motion deblurring, and LOL-v2 [55] for low-light image enhancement (LLIE).",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions specific datasets used for various image restoration tasks, which are directly relevant to the research topic of all-in-one image restoration.",
          "citing_paper_doi": "10.48550/arXiv.2409.19403",
          "cited_paper_doi": "10.1109/tpami.2020.2995190",
          "citing_paper_url": "https://www.semanticscholar.org/paper/cc6a57aa1b8496efd3b9001226fd09b8d121f397",
          "cited_paper_url": "https://www.semanticscholar.org/paper/505a27b7ffac313dd910d584ffa0eb416fc59ba2",
          "citing_paper_year": 2024,
          "cited_paper_year": 2019
        },
        {
          "dataset_name": "RESIDE",
          "dataset_description": "Used as a benchmark for single-image de-hazing, evaluating the performance of de-hazing algorithms on various real-world and synthetic hazy images.",
          "citing_paper_id": "268513542",
          "cited_paper_id": 39760169,
          "context_text": "We use RESIDE [22] datasets as the de-hazing benchmark.",
          "confidence_score": 1.0,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "RESIDE is mentioned as a dataset used for de-hazing benchmarking, which is directly relevant to the topic of image restoration.",
          "citing_paper_doi": "10.1109/CVPR52733.2024.02404",
          "cited_paper_doi": "10.1109/TIP.2018.2867951",
          "citing_paper_url": "https://www.semanticscholar.org/paper/1ff44c75c361d2ed28f868361a29437197b0270c",
          "cited_paper_url": "https://www.semanticscholar.org/paper/abf75b81b96ff2cd262086675deba55f907108db",
          "citing_paper_year": 2024,
          "cited_paper_year": 2017
        },
        {
          "dataset_name": "RESIDE",
          "dataset_description": "Used for training and testing single-image dehazing models, focusing on improving image clarity and quality in hazy conditions.",
          "citing_paper_id": "275133634",
          "cited_paper_id": 39760169,
          "context_text": "Datasets: Following existing works [47], we employ RESIDE dataset [24] for training and testing.",
          "confidence_score": 1.0,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions the use of the RESIDE dataset for training and testing, which is relevant to the topic of image restoration.",
          "citing_paper_doi": "10.48550/arXiv.2412.20066",
          "cited_paper_doi": "10.1109/TIP.2018.2867951",
          "citing_paper_url": "https://www.semanticscholar.org/paper/2b78fc275b7fc3a37061620ae155c017ab6c22e1",
          "cited_paper_url": "https://www.semanticscholar.org/paper/abf75b81b96ff2cd262086675deba55f907108db",
          "citing_paper_year": 2024,
          "cited_paper_year": 2017
        },
        {
          "dataset_name": "RESIDE",
          "dataset_description": "Used for testing the performance of dehazing models, evaluating their effectiveness in enhancing outdoor scenes with varying levels of haze. | Used for training models in image dehazing, focusing on improving visibility and quality of hazy images through deep learning techniques.",
          "citing_paper_id": "271274210",
          "cited_paper_id": 49864080,
          "context_text": "For image dehazing, RESIDE-β [3] and SOTS-Outdoor [79] are used for training and testing, respectively.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions two specific datasets, RESIDE-β and SOTS-Outdoor, which are used for training and testing in the context of image dehazing.",
          "citing_paper_doi": "10.1109/TCSVT.2024.3429557",
          "cited_paper_doi": "10.1007/978-3-030-01234-2_16",
          "citing_paper_url": "https://www.semanticscholar.org/paper/1091511c40eebb0f8e964246c91e91a85480dbf9",
          "cited_paper_url": "https://www.semanticscholar.org/paper/c2039c76ef7dc86af9e758d7fee3058d9bfbd550",
          "citing_paper_year": 2024,
          "cited_paper_year": 2018
        },
        {
          "dataset_name": "RESIDE",
          "dataset_description": "Used for dehazing experiments, providing nighttime real-world hazy images for evaluation. | Used for dehazing experiments, providing remote sensing hazy images for evaluation. | Used for dehazing experiments, providing a benchmark with dense-haze and haze-free images for evaluation. | Used for dehazing experiments, providing a synthetic dataset of hazy images for training and evaluation. | Used for dehazing experiments, providing synthetic nighttime hazy images for evaluation. | Used for dehazing experiments, providing real hazy and haze-free outdoor images for evaluation. | Used for dehazing experiments, providing real-world hazy images for evaluation.",
          "citing_paper_id": "271274210",
          "cited_paper_id": 102350705,
          "context_text": "We conduct dehazing experiments on four kinds of datasets: a daytime synthetic dataset (RESIDE [3]), four daytime real-world datasets (Dense-Haze [44], NH-HAZE [45], O-Haze [46], and I-Haze [47]), two nighttime datasets (NHR [56], GTA5 [25]), and a remote sensing dataset (SateHaze1k [57]).",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions several datasets used for dehazing experiments, including both synthetic and real-world datasets. These datasets are clearly identified and used for evaluating dehazing algorithms.",
          "citing_paper_doi": "10.1109/TCSVT.2024.3429557",
          "cited_paper_doi": "10.1109/ICIP.2019.8803046",
          "citing_paper_url": "https://www.semanticscholar.org/paper/1091511c40eebb0f8e964246c91e91a85480dbf9",
          "cited_paper_url": "https://www.semanticscholar.org/paper/a3be7b43518abc11a074e6c15d1b5c443e53737c",
          "citing_paper_year": 2024,
          "cited_paper_year": 2019
        },
        {
          "dataset_name": "RESIDE",
          "dataset_description": "Used for dehazing experiments, providing nighttime real-world hazy images for evaluation. | Used for dehazing experiments, providing remote sensing hazy images for evaluation. | Used for dehazing experiments, providing a benchmark with dense-haze and haze-free images for evaluation. | Used for dehazing experiments, providing a synthetic dataset of hazy images for training and evaluation. | Used for dehazing experiments, providing synthetic nighttime hazy images for evaluation. | Used for dehazing experiments, providing real hazy and haze-free outdoor images for evaluation. | Used for dehazing experiments, providing real-world hazy images for evaluation.",
          "citing_paper_id": "271274210",
          "cited_paper_id": 206598041,
          "context_text": "We conduct dehazing experiments on four kinds of datasets: a daytime synthetic dataset (RESIDE [3]), four daytime real-world datasets (Dense-Haze [44], NH-HAZE [45], O-Haze [46], and I-Haze [47]), two nighttime datasets (NHR [56], GTA5 [25]), and a remote sensing dataset (SateHaze1k [57]).",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions several datasets used for dehazing experiments, including both synthetic and real-world datasets. These datasets are clearly identified and used for evaluating dehazing algorithms.",
          "citing_paper_doi": "10.1109/TCSVT.2024.3429557",
          "cited_paper_doi": "10.1109/CVPRW.2018.00119",
          "citing_paper_url": "https://www.semanticscholar.org/paper/1091511c40eebb0f8e964246c91e91a85480dbf9",
          "cited_paper_url": "https://www.semanticscholar.org/paper/fb07f990b8247b46fec6dcd2a065a5f7e553db39",
          "citing_paper_year": 2024,
          "cited_paper_year": 2018
        },
        {
          "dataset_name": "RESIDE",
          "dataset_description": "Used for dehazing experiments, providing nighttime real-world hazy images for evaluation. | Used for dehazing experiments, providing remote sensing hazy images for evaluation. | Used to apply the model for remote sensing dehazing, specifically leveraging SAR image prior in conditional GANs to improve optical imagery quality. | Used for dehazing experiments, providing a benchmark with dense-haze and haze-free images for evaluation. | Used for dehazing experiments, providing a synthetic dataset of hazy images for training and evaluation. | Used for dehazing experiments, providing synthetic nighttime hazy images for evaluation. | Used for dehazing experiments, providing real hazy and haze-free outdoor images for evaluation. | Used to evaluate the effectiveness of the Asimage dehazing model in remote sensing applications, focusing on the performance of dehazing algorithms on satellite imagery. | Used for dehazing experiments, providing real-world hazy images for evaluation.",
          "citing_paper_id": "271274210",
          "cited_paper_id": 211732223,
          "context_text": "We conduct dehazing experiments on four kinds of datasets: a daytime synthetic dataset (RESIDE [3]), four daytime real-world datasets (Dense-Haze [44], NH-HAZE [45], O-Haze [46], and I-Haze [47]), two nighttime datasets (NHR [56], GTA5 [25]), and a remote sensing dataset (SateHaze1k [57]).",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions several datasets used for dehazing experiments, including both synthetic and real-world datasets. These datasets are clearly identified and used for evaluating dehazing algorithms.",
          "citing_paper_doi": "10.1109/TCSVT.2024.3429557",
          "cited_paper_doi": "10.1109/WACV45572.2020.9093471",
          "citing_paper_url": "https://www.semanticscholar.org/paper/1091511c40eebb0f8e964246c91e91a85480dbf9",
          "cited_paper_url": "https://www.semanticscholar.org/paper/a119ceb12cece4ac652d3becba91fdba94237c27",
          "citing_paper_year": 2024,
          "cited_paper_year": 2020
        },
        {
          "dataset_name": "RESIDE",
          "dataset_description": "Used for training image dehazing models, focusing on the ITS training set to improve dehazing performance. | Used for testing the trained dehazing models, evaluating their performance on indoor scenes.",
          "citing_paper_id": "264146360",
          "cited_paper_id": 39760169,
          "context_text": "For image dehazing, we utilize the ITS training set of RESIDE dataset (Li et al. 2018) for training and the SOTS-indoor dataset for testing.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions two specific datasets used for training and testing in the context of image dehazing.",
          "citing_paper_doi": "10.48550/arXiv.2310.10513",
          "cited_paper_doi": "10.1109/TIP.2018.2867951",
          "citing_paper_url": "https://www.semanticscholar.org/paper/836906c334159d46a691dfc5466c33caa3d22f65",
          "cited_paper_url": "https://www.semanticscholar.org/paper/abf75b81b96ff2cd262086675deba55f907108db",
          "citing_paper_year": 2023,
          "cited_paper_year": 2017
        },
        {
          "dataset_name": "RESIDE",
          "dataset_description": "Used to conduct experiments on image dehazing, evaluating the performance of dehazing algorithms on a benchmark dataset. | Used to evaluate single-image dehazing models, specifically focusing on indoor scenes to benchmark performance.",
          "citing_paper_id": "259459919",
          "cited_paper_id": 39760169,
          "context_text": "3) Image Dehazing: We conduct experiments on a popular benchmark dataset named RESIDE [67] for image dehazing.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions the use of the RESIDE dataset for image dehazing experiments, which is a specific and verifiable dataset.",
          "citing_paper_doi": "10.1109/TCSVT.2023.3286405",
          "cited_paper_doi": "10.1109/TIP.2018.2867951",
          "citing_paper_url": "https://www.semanticscholar.org/paper/fca6cb29677c6fe278290ee1814d9bc31c155a65",
          "cited_paper_url": "https://www.semanticscholar.org/paper/abf75b81b96ff2cd262086675deba55f907108db",
          "citing_paper_year": 2024,
          "cited_paper_year": 2017
        },
        {
          "dataset_name": "RESIDE",
          "dataset_description": "This dataset 'RESIDE' was mentioned in the citation context but no detailed description was generated. | This dataset 'Haze4K' was mentioned in the citation context but no detailed description was generated.",
          "citing_paper_id": "258959359",
          "cited_paper_id": 199528450,
          "context_text": "Following [36,46,56,57], we conduct our experiments on RESIDE [29] and Haze4K [38] datasets.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context explicitly mentions two datasets, RESIDE and Haze4K, which are used for conducting experiments in image dehazing.",
          "citing_paper_doi": "10.48550/arXiv.2305.17863",
          "cited_paper_doi": "10.1109/ICCV.2019.00741",
          "citing_paper_url": "https://www.semanticscholar.org/paper/7d8cfd80c9bc7bbfd52b4ee9c9bbc5b7bfd9cc21",
          "cited_paper_url": "https://www.semanticscholar.org/paper/56ac346b77730828f5fc3ee3cf1f47d62a1d3343",
          "citing_paper_year": 2023,
          "cited_paper_year": 2019
        },
        {
          "dataset_name": "RESIDE",
          "dataset_description": "This dataset 'RESIDE' was mentioned in the citation context but no detailed description was generated. | This dataset 'Haze4K' was mentioned in the citation context but no detailed description was generated.",
          "citing_paper_id": "258959359",
          "cited_paper_id": 245837508,
          "context_text": "Following [36,46,56,57], we conduct our experiments on RESIDE [29] and Haze4K [38] datasets.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context explicitly mentions two datasets, RESIDE and Haze4K, which are used for conducting experiments in image dehazing.",
          "citing_paper_doi": "10.48550/arXiv.2305.17863",
          "cited_paper_doi": "10.1109/CVPR52688.2022.00568",
          "citing_paper_url": "https://www.semanticscholar.org/paper/7d8cfd80c9bc7bbfd52b4ee9c9bbc5b7bfd9cc21",
          "cited_paper_url": "https://www.semanticscholar.org/paper/fbf68eb0cf8237cef59ebdb301569c79b9676ff9",
          "citing_paper_year": 2023,
          "cited_paper_year": 2022
        },
        {
          "dataset_name": "RESIDE",
          "dataset_description": "This dataset 'RESIDE' was mentioned in the citation context but no detailed description was generated. | This dataset 'Haze4K' was mentioned in the citation context but no detailed description was generated. | Used for evaluating image restoration methods, focusing on both indoor and outdoor hazy scenes to improve dehazing performance. | Used to train and evaluate image dehazing models, providing a large set of real-world hazy images and their corresponding clear versions.",
          "citing_paper_id": "258959359",
          "cited_paper_id": 236950592,
          "context_text": "Following [46,58,68,70], we conduct our experiments on RESIDE [37] and Haze4K [48] datasets.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The citation mentions two specific datasets, RESIDE and Haze4K, which are used for conducting experiments in image dehazing.",
          "citing_paper_doi": "10.48550/arXiv.2305.17863",
          "cited_paper_doi": "10.1145/3474085.3475331",
          "citing_paper_url": "https://www.semanticscholar.org/paper/7d8cfd80c9bc7bbfd52b4ee9c9bbc5b7bfd9cc21",
          "cited_paper_url": "https://www.semanticscholar.org/paper/f9295ac4abc2235414b9f34752f1cf745dbfaa9a",
          "citing_paper_year": 2023,
          "cited_paper_year": 2021
        },
        {
          "dataset_name": "RESIDE",
          "dataset_description": "Applied to assess rain removal techniques, focusing on the effectiveness of methods in restoring images degraded by rain. | Utilized to test cloud shadow removal, specifically evaluating the ability of algorithms to restore images affected by cloud shadows. | Used to evaluate single-image dehazing methods, providing a benchmark for comparing performance across various dehazing algorithms.",
          "citing_paper_id": "250581068",
          "cited_paper_id": 39760169,
          "context_text": "Various adverse weather datasets are applied including \"RESIDE\" [59], \"Rain 1400\" [2], and \"CSD\" [12].",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The citation mentions specific datasets used for evaluating image restoration techniques under adverse weather conditions.",
          "citing_paper_doi": "10.1109/CVPR52688.2022.01713",
          "cited_paper_doi": "10.1109/TIP.2018.2867951",
          "citing_paper_url": "https://www.semanticscholar.org/paper/50eb3c6c167d3aa1a06cb4f451c05729c9f58a99",
          "cited_paper_url": "https://www.semanticscholar.org/paper/abf75b81b96ff2cd262086675deba55f907108db",
          "citing_paper_year": 2022,
          "cited_paper_year": 2017
        },
        {
          "dataset_name": "RESIDE",
          "dataset_description": "Used for training and evaluating image dehazing models, comprising the Outdoor Training Set (OTS) for training and the Synthetic Outdoor Testing Set (SOTS) for evaluation.",
          "citing_paper_id": "278528939",
          "cited_paper_id": 39760169,
          "context_text": "For image dehazing, the RESIDE [56] dataset is employed, comprising the Outdoor Training Set (OTS) with 8,990 hazy-clean image pairs for training, and the Synthetic Outdoor Testing Set (SOTS) with 500 pairs for evaluation.",
          "confidence_score": 1.0,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context clearly mentions the RESIDE dataset, which is used for training and evaluating image dehazing models. The dataset is described with specific subsets and their purposes.",
          "citing_paper_doi": "10.1109/TIP.2025.3567205",
          "cited_paper_doi": "10.1109/TIP.2018.2867951",
          "citing_paper_url": "https://www.semanticscholar.org/paper/99b442f68dce288fd1f5d30858f7ac8b8d0068e9",
          "cited_paper_url": "https://www.semanticscholar.org/paper/abf75b81b96ff2cd262086675deba55f907108db",
          "citing_paper_year": 2025,
          "cited_paper_year": 2017
        }
      ]
    },
    {
      "cited_paper_id": "8282555",
      "citation_count": 0,
      "total_dataset_mentions": 15,
      "unique_datasets": [
        "CBSD68"
      ],
      "dataset_details": [
        {
          "dataset_name": "CBSD68",
          "dataset_description": "Used for denoising experiments with synthetic benchmark datasets, evaluating denoising algorithms under controlled conditions. | Used for denoising experiments with real-world datasets, evaluating denoising algorithms on smartphone camera images. | Used to evaluate image restoration methods, focusing on performance at different noise levels (σ=15, σ=25, σ=50). | Used for denoising experiments with real-world datasets, evaluating denoising algorithms on diverse real-world images.",
          "citing_paper_id": "244346144",
          "cited_paper_id": 15614128,
          "context_text": "CBSD68 [52] Kodak24 [20] McMaster [104] Urban100 [29] Method σ=15 σ=25 σ=50 σ=15 σ=25 σ=50 σ=15 σ=25 σ=50 σ=15 σ=25 σ=50",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The citation mentions several datasets by name, which are likely used for evaluating image restoration methods. The context suggests these datasets are used for benchmarking performance at different noise levels.",
          "citing_paper_doi": "10.1109/CVPR52688.2022.00564",
          "cited_paper_doi": "10.1117/1.3600632",
          "citing_paper_url": "https://www.semanticscholar.org/paper/1e88d5afe19aea324d33541f60a90b7036894c32",
          "cited_paper_url": "https://www.semanticscholar.org/paper/47b9a4dfbff5d2bb8b4f8f6e6fc2294426aab8ac",
          "citing_paper_year": 2021,
          "cited_paper_year": 2011
        },
        {
          "dataset_name": "CBSD68",
          "dataset_description": "Used for evaluating DeJPEG performance, focusing on artifact removal and quality enhancement in JPEG-compressed images. | Used for evaluating denoising performance, focusing on noise reduction in images with a variety of textures and details. | Used for evaluating super-resolution performance, focusing on upscaling low-resolution images to high resolution with detailed reconstruction.",
          "citing_paper_id": "257219872",
          "cited_paper_id": 9247572,
          "context_text": "For evaluation, we use the CBSD68 dataset [17] for denoising, the LIVE1 [18] for DeJPEG and the Set5 [19] for super-resolution.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The citation mentions three specific datasets used for evaluating different aspects of image restoration: CBSD68 for denoising, LIVE1 for DeJPEG, and Set5 for super-resolution.",
          "citing_paper_doi": "10.1109/ICASSP49357.2023.10095537",
          "cited_paper_doi": "10.1109/TIP.2006.881959",
          "citing_paper_url": "https://www.semanticscholar.org/paper/17150a9a7f62c2de5ccd39db3a9ce631e3e63e1e",
          "cited_paper_url": "https://www.semanticscholar.org/paper/1c79a3b52bf92fbc01de64057a264de9e94a1025",
          "citing_paper_year": 2022,
          "cited_paper_year": 2006
        },
        {
          "dataset_name": "CBSD68",
          "dataset_description": "Used for visual comparison of color image denoising methods, focusing on noise level 50. The dataset provides human-segmented natural images for evaluating denoising algorithms.",
          "citing_paper_id": "237266491",
          "cited_paper_id": 64193,
          "context_text": "Figure 7: Visual comparison of color image denoising (noise level 50) methods on image “163085” from CBSD68 [59].",
          "confidence_score": 1.0,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions 'CBSD68' which is a specific dataset used for image denoising experiments. The dataset is referenced for visual comparison of denoising methods.",
          "citing_paper_doi": "10.1109/ICCVW54120.2021.00210",
          "cited_paper_doi": "10.1109/ICCV.2001.937655",
          "citing_paper_url": "https://www.semanticscholar.org/paper/7a9a708ca61c14886aa0dcd6d13dac7879713f5f",
          "cited_paper_url": "https://www.semanticscholar.org/paper/9a1ed876196ec9733acb1daa6d65e35ff0414291",
          "citing_paper_year": 2021,
          "cited_paper_year": 2001
        },
        {
          "dataset_name": "CBSD68",
          "dataset_description": "Used as test data for image denoising, evaluating the performance of denoising algorithms on a standardized set of images. | Used as test data for image denoising, assessing the effectiveness of denoising techniques on high-quality images. | Used as test data for image denoising, providing a diverse set of images to validate denoising algorithms.",
          "citing_paper_id": "270155352",
          "cited_paper_id": null,
          "context_text": "WeuseCBSD68[68],Kodak[69],andMcMas-ter[70]astestdataofimagedenoising.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The citation mentions specific image datasets used for testing image denoising methods.",
          "citing_paper_doi": "10.1109/TMM.2024.3407656",
          "cited_paper_doi": null,
          "citing_paper_url": "https://www.semanticscholar.org/paper/3a80830bf3e31c855afaff17a5b590bc052839c6",
          "cited_paper_url": null,
          "citing_paper_year": 2024,
          "cited_paper_year": null
        },
        {
          "dataset_name": "CBSD68",
          "dataset_description": "Used to test super-resolution and denoising algorithms, featuring high-resolution urban scenes with complex textures. | Used to evaluate image denoising performance, focusing on color images with real-world noise characteristics. | Used to assess denoising performance, providing a set of color images with real-world noise for benchmarking. | Used to evaluate image restoration methods, focusing on color images with ground truth for quantitative assessment. | Employed to evaluate denoising algorithms, consisting of grayscale images with varying levels of noise. | Applied to assess image quality in restoration tasks, providing a diverse set of natural images for benchmarking.",
          "citing_paper_id": "257255385",
          "cited_paper_id": 996788,
          "context_text": "Method # Params [M] Color Grayscale CBSD68 [49] Kodak24 [16] McMaster [83] Urban100 [28] Set12 [82] BSD68 [49] Urban100 [28]",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The citation mentions several datasets used for evaluation in image denoising and restoration tasks. These datasets are specific and have clear identifiers.",
          "citing_paper_doi": "10.1109/CVPR52729.2023.01753",
          "cited_paper_doi": "10.1109/TIP.2017.2662206",
          "citing_paper_url": "https://www.semanticscholar.org/paper/2920b885278a6973a306b57201e3fc3273b71132",
          "cited_paper_url": "https://www.semanticscholar.org/paper/0c00a328fa7cd56ee60338c54e89bd48310db80b",
          "citing_paper_year": 2023,
          "cited_paper_year": 2016
        },
        {
          "dataset_name": "CBSD68",
          "dataset_description": "Used for training in image restoration, containing images for various restoration tasks including denoising and deblurring. | Used for testing in image restoration, offering a variety of images to assess generalization and robustness. | Used for testing in image restoration, focusing on benchmarking denoising performance. | Used for testing in image restoration, providing a standard set of images for evaluating restoration quality. | Used for training in image restoration, providing high-quality images for super-resolution tasks. | Used for testing in image restoration, specifically for evaluating super-resolution algorithms on urban scenes. | Used to evaluate the performance of UniProcessor and six competing models on 30 severe degradations, focusing on image restoration quality. | Used for training in image restoration, offering a large set of diverse images for model training. | Used for training in image restoration, providing a diverse set of images for enhancement and restoration.",
          "citing_paper_id": "271543778",
          "cited_paper_id": 64193,
          "context_text": "Table 1 quantitatively demonstrates the performance results of our UniProcessor and six competing models for processing 30 severe degradations on the CBSD68 dataset [39].",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions the CBSD68 dataset, which is a specific dataset used for image restoration tasks. The dataset is used to evaluate the performance of the UniProcessor and competing models.",
          "citing_paper_doi": "10.48550/arXiv.2407.20928",
          "cited_paper_doi": "10.1109/ICCV.2001.937655",
          "citing_paper_url": "https://www.semanticscholar.org/paper/261cde5efca4a5c146479bf42fb9438b581cd839",
          "cited_paper_url": "https://www.semanticscholar.org/paper/9a1ed876196ec9733acb1daa6d65e35ff0414291",
          "citing_paper_year": 2024,
          "cited_paper_year": 2001
        },
        {
          "dataset_name": "CBSD68",
          "dataset_description": "Used to test image restoration methods, focusing on benchmarking denoising performance on a set of 68 color images. | Used to evaluate image restoration techniques, particularly for super-resolution tasks on urban scenes.",
          "citing_paper_id": "278740510",
          "cited_paper_id": 64193,
          "context_text": "Testing is conducted on the CBSD68 [32] and Urban100 [14] datasets.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The citation mentions two specific datasets, CBSD68 and Urban100, which are used for testing image restoration methods.",
          "citing_paper_doi": "10.48550/arXiv.2505.12630",
          "cited_paper_doi": "10.1109/ICCV.2001.937655",
          "citing_paper_url": "https://www.semanticscholar.org/paper/18b73e3db636e6ded820bc0b5b5ad13993e287bc",
          "cited_paper_url": "https://www.semanticscholar.org/paper/9a1ed876196ec9733acb1daa6d65e35ff0414291",
          "citing_paper_year": 2025,
          "cited_paper_year": 2001
        },
        {
          "dataset_name": "CBSD68",
          "dataset_description": "Used to test image restoration methods, focusing on benchmarking denoising performance on a set of 68 color images. | Used to evaluate image restoration techniques, particularly for super-resolution tasks on urban scenes.",
          "citing_paper_id": "278740510",
          "cited_paper_id": 8282555,
          "context_text": "Testing is conducted on the CBSD68 [32] and Urban100 [14] datasets.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The citation mentions two specific datasets, CBSD68 and Urban100, which are used for testing image restoration methods.",
          "citing_paper_doi": "10.48550/arXiv.2505.12630",
          "cited_paper_doi": "10.1109/CVPR.2015.7299156",
          "citing_paper_url": "https://www.semanticscholar.org/paper/18b73e3db636e6ded820bc0b5b5ad13993e287bc",
          "cited_paper_url": "https://www.semanticscholar.org/paper/2946c4e29b057d3543f09b89f547965895676d19",
          "citing_paper_year": 2025,
          "cited_paper_year": 2015
        },
        {
          "dataset_name": "CBSD68",
          "dataset_description": "Used to evaluate the performance of DRUNet, comparing its PSNR gains against BM3D, DnCNN, IRCNN, and FFDNet. | Used to evaluate PSNR improvements in image restoration models, specifically focusing on the performance gains across different image domains. | Used to evaluate image restoration performance at noise levels 15, 25, and 50, focusing on denoising effectiveness and quality. | Used to test the effectiveness of image restoration algorithms, particularly at various noise levels. | Used to assess the performance of image restoration techniques, specifically for noise levels 15, 25, and 50. | Used to evaluate color image denoising methods at noise levels 15, 25, and 50, focusing on benchmarking performance. | Used to evaluate image restoration methods across different noise levels, focusing on average PSNR results. | Used to evaluate the performance of image restoration methods, specifically comparing PSNR gains across different noise levels. | Used to evaluate PSNR results of image restoration methods at noise levels 15, 25, and 50, focusing on performance comparison. | Used to evaluate image restoration methods, specifically comparing bicubic and classical degradation, focusing on color images. | Used to evaluate PSNR results in image restoration, specifically comparing performance across different domains.",
          "citing_paper_id": "221377171",
          "cited_paper_id": 13058320,
          "context_text": "Table 2 reports the color image denoising results of different methods for noise levels 15, 25 and 50 on CBSD68 [3], [44], [49], Kodak24 [71] and McMaster [72] datasets.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions specific datasets used for evaluating color image denoising methods at various noise levels.",
          "citing_paper_doi": "10.1109/TPAMI.2021.3088914",
          "cited_paper_doi": "10.1007/s11263-008-0197-6",
          "citing_paper_url": "https://www.semanticscholar.org/paper/d467d747dde65862d9640fe49f69d6b6d46d69ea",
          "cited_paper_url": "https://www.semanticscholar.org/paper/dfcae80f4d34ac09ba8063c5cfb5be954d0bf5f1",
          "citing_paper_year": 2020,
          "cited_paper_year": 2009
        },
        {
          "dataset_name": "CBSD68",
          "dataset_description": "Used to evaluate color image denoising methods at noise levels 15, 25, and 50, focusing on benchmarking performance.",
          "citing_paper_id": "221377171",
          "cited_paper_id": null,
          "context_text": "Table 2 reports the color image denoising results of different methods for noise levels 15, 25 and 50 on CBSD68 [3], [44], [49], Kodak24 [71] and McMaster [72] datasets.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions specific datasets used for evaluating color image denoising methods at various noise levels.",
          "citing_paper_doi": "10.1109/TPAMI.2021.3088914",
          "cited_paper_doi": null,
          "citing_paper_url": "https://www.semanticscholar.org/paper/d467d747dde65862d9640fe49f69d6b6d46d69ea",
          "cited_paper_url": null,
          "citing_paper_year": 2020,
          "cited_paper_year": null
        },
        {
          "dataset_name": "CBSD68",
          "dataset_description": "Used to evaluate image restoration methods at noise levels 15, 25, and 50, focusing on PSNR performance metrics.",
          "citing_paper_id": "221377171",
          "cited_paper_id": 85501306,
          "context_text": "TABLE 2 Average PSNR(dB) Results of Different Methods for Noise Levels 15, 25 and 50 on CBSD68 [3], [44], [49], Kodak24, and McMaster Datasets\nThe best and second best results are highlighted in red and blue colors, respectively.\nof 0.15dB over RNAN, which further demonstrates the flexibility and effectiveness of the proposed DRUNet.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The citation mentions specific datasets used for evaluating image restoration methods, including noise levels and performance metrics.",
          "citing_paper_doi": "10.1109/TPAMI.2021.3088914",
          "cited_paper_doi": null,
          "citing_paper_url": "https://www.semanticscholar.org/paper/d467d747dde65862d9640fe49f69d6b6d46d69ea",
          "cited_paper_url": "https://www.semanticscholar.org/paper/34439db81b482cd562e1cdba974c70a2b89cd6d4",
          "citing_paper_year": 2020,
          "cited_paper_year": 2019
        },
        {
          "dataset_name": "CBSD68",
          "dataset_description": "Used to evaluate image restoration methods, specifically denoising and regularization, using 68 RGB images. | Used to evaluate image restoration methods, specifically denoising and regularization, using 11 grayscale images.",
          "citing_paper_id": "209323842",
          "cited_paper_id": 207758856,
          "context_text": "We evaluate on the standard dataset by Heide et al. (2015), consisting of 11 grayscale images, and the CBSD68 dataset by (Roth and Black, 2009) consisting of 68 RGB images.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions two specific datasets used for evaluation: the dataset by Heide et al. (2015) and the CBSD68 dataset. Both are clearly identified and used for image restoration evaluation.",
          "citing_paper_doi": "10.1007/s11263-021-01572-7",
          "cited_paper_doi": null,
          "citing_paper_url": "https://www.semanticscholar.org/paper/594adc4a13d5be3545de97bd94fc5b268bca0f90",
          "cited_paper_url": "https://www.semanticscholar.org/paper/dda678ae5abcd3b522ff195705a80ecb3f58f043",
          "citing_paper_year": 2021,
          "cited_paper_year": 2019
        },
        {
          "dataset_name": "CBSD68",
          "dataset_description": "Used to evaluate image restoration methods, specifically focusing on performance at a noise level of σ = 50. The dataset consists of human-segmented natural images. | Used to evaluate RCOT under noise levels σ ∈ {25, 50}, focusing on image restoration performance.",
          "citing_paper_id": "269605449",
          "cited_paper_id": 64193,
          "context_text": "Table 6 reports the results on CBSD68 (Martin et al., 2001) with noise level σ = 50 .",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "CBSD68 is a specific dataset used for image restoration evaluation, as indicated by the context and the cited paper title.",
          "citing_paper_doi": "10.48550/arXiv.2405.02843",
          "cited_paper_doi": "10.1109/ICCV.2001.937655",
          "citing_paper_url": "https://www.semanticscholar.org/paper/0c7542272e5edcff29608a0e37f9a3b705b363e7",
          "cited_paper_url": "https://www.semanticscholar.org/paper/9a1ed876196ec9733acb1daa6d65e35ff0414291",
          "citing_paper_year": 2024,
          "cited_paper_year": 2001
        },
        {
          "dataset_name": "CBSD68",
          "dataset_description": "Used to select ground truth images for image restoration experiments, including a variety of natural and synthetic images. | Used to select ground truth images for image restoration experiments, focusing on diverse content and quality. | Used to select ground truth images for image restoration experiments, emphasizing urban scenes and architectural details. | Used to select ground truth images for image restoration experiments, providing high-quality reference images. | Used to evaluate performance on image restoration tasks, focusing on the first six types of distortions.",
          "citing_paper_id": "276960871",
          "cited_paper_id": null,
          "context_text": "Following [13], we adopt the combination of CBSD68 [41], Urban100 [23], Kodak24 [17], and McMaster [85] to evaluate the performance on first six distortions.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The citation mentions specific datasets used for evaluating performance on image restoration tasks, which aligns with the research topic.",
          "citing_paper_doi": "10.48550/arXiv.2503.10120",
          "cited_paper_doi": null,
          "citing_paper_url": "https://www.semanticscholar.org/paper/8d5b315df8c45238becce7237c1512145d4bb36d",
          "cited_paper_url": null,
          "citing_paper_year": 2025,
          "cited_paper_year": null
        },
        {
          "dataset_name": "CBSD68",
          "dataset_description": "Used to evaluate denoising performance at σ = 50, focusing on image restoration quality and comparing results with HQ-50K.",
          "citing_paper_id": "259108489",
          "cited_paper_id": 1151947,
          "context_text": "As shown, our HQ-50K outperforms the other two datasets across all degradation levels, indicating its su-Figure [14] and on Denoise of σ = 50 on CBSD68 [36] perior quality.",
          "confidence_score": 1.0,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions 'CBSD68' as a dataset used for evaluation, specifically for denoising experiments.",
          "citing_paper_doi": "10.48550/arXiv.2306.05390",
          "cited_paper_doi": "10.1109/TIP.2007.891788",
          "citing_paper_url": "https://www.semanticscholar.org/paper/4296bf82e67051e4cba10e510932f55fc12dd1e2",
          "cited_paper_url": "https://www.semanticscholar.org/paper/60e7d2cf0aa8088a9ba7a01b0aad19a61e441e0c",
          "citing_paper_year": 2023,
          "cited_paper_year": 2007
        }
      ]
    },
    {
      "cited_paper_id": "8978557",
      "citation_count": 0,
      "total_dataset_mentions": 14,
      "unique_datasets": [
        "LOL dataset"
      ],
      "dataset_details": [
        {
          "dataset_name": "LOL dataset",
          "dataset_description": "Used to evaluate the proposed method for low-light image enhancement, focusing on improving visibility and color fidelity. | Employed to evaluate the method's ability to enhance multi-exposure images, ensuring dynamic range improvement. | Used to evaluate low-light image enhancement methods, focusing on improving visibility and color fidelity in dark images. | Applied to assess the effectiveness of the method in enhancing local image details and contrast. | Used to evaluate power-constrained contrast enhancement for emissive displays, focusing on histogram equalization techniques despite the lack of reference images. | Used for quantitative comparison of various image restoration methods, focusing on metrics such as PSNR, SSIM, LOE, LOEref, and NIQE. | Applied to assess low-light image enhancement techniques, emphasizing brightness and contrast improvements in dimly lit scenes. | Employed to evaluate multi-exposure fusion techniques, combining multiple exposures to enhance dynamic range and detail. | Utilized to test non-uniform illumination correction, specifically addressing uneven lighting issues in images. | Utilized to test the performance of the method in noise reduction and preserving image structures.",
          "citing_paper_id": "150373946",
          "cited_paper_id": 8978557,
          "context_text": "We evaluate our method on widely-adopted datasets, including LOL [30], LIME [16], NPE [28], and MEF [7].",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions specific datasets used for evaluating the method, which are relevant to image restoration.",
          "citing_paper_doi": "10.1145/3343031.3350926",
          "cited_paper_doi": null,
          "citing_paper_url": "https://www.semanticscholar.org/paper/28168e2c182e5456ad4712dae479dd44423b2ed6",
          "cited_paper_url": "https://www.semanticscholar.org/paper/a4a188c5d9c4383c558fbcda4f5da4658aeda407",
          "citing_paper_year": 2019,
          "cited_paper_year": 2014
        },
        {
          "dataset_name": "LOL dataset",
          "dataset_description": "Utilized to test non-uniform illumination correction, specifically addressing uneven lighting issues in images. | Used to evaluate low-light image enhancement methods, focusing on improving visibility and color fidelity in dark images. | Applied to assess low-light image enhancement techniques, emphasizing brightness and contrast improvements in dimly lit scenes. | Employed to evaluate multi-exposure fusion techniques, combining multiple exposures to enhance dynamic range and detail.",
          "citing_paper_id": "150373946",
          "cited_paper_id": 52008443,
          "context_text": "We evaluate our method on widely-adopted datasets, including LOL [21], LIME [11], NPE [9], and MEF [30].",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions specific datasets used for evaluation, which are relevant to the topic of image restoration.",
          "citing_paper_doi": "10.1145/3343031.3350926",
          "cited_paper_doi": null,
          "citing_paper_url": "https://www.semanticscholar.org/paper/28168e2c182e5456ad4712dae479dd44423b2ed6",
          "cited_paper_url": "https://www.semanticscholar.org/paper/c7798bbbcd2e85593a2b2d37b92dabac63420cd8",
          "citing_paper_year": 2019,
          "cited_paper_year": 2018
        },
        {
          "dataset_name": "LOL dataset",
          "dataset_description": "Used to quantitatively compare low-light image enhancement techniques, including supervised, unsupervised, and zero-shot methods, focusing on performance metrics.",
          "citing_paper_id": "235691711",
          "cited_paper_id": 4095486,
          "context_text": "Table 3 presents quantitative performance comparison of low-light image enhancement on the standard LOL dataset [66] using our approach, and the techniques DRD [66], LightenNet [41], PR [62], LIME [24], SRIE [18], RRM [44], LECARM [54], ALSM [64], and ZeroDCE [23], which are segregated into supervised, unsupervised and zero-shot categories.",
          "confidence_score": 1.0,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions the LOL dataset, which is a specific dataset used for low-light image enhancement. The dataset is used to compare the performance of various techniques.",
          "citing_paper_doi": "10.1109/CVPR46437.2021.01594",
          "cited_paper_doi": "10.1109/TIP.2018.2810539",
          "citing_paper_url": "https://www.semanticscholar.org/paper/68f21cbd2d5001528c07b16d27b8d4de5d6544e2",
          "cited_paper_url": "https://www.semanticscholar.org/paper/ead9732d17af17a89243f840643563b3bdf14525",
          "citing_paper_year": 2021,
          "cited_paper_year": 2018
        },
        {
          "dataset_name": "LOL dataset",
          "dataset_description": "Used to quantitatively compare low-light image enhancement techniques, including supervised, unsupervised, and zero-shot methods, focusing on performance metrics.",
          "citing_paper_id": "235691711",
          "cited_paper_id": 195188053,
          "context_text": "Table 3 presents quantitative performance comparison of low-light image enhancement on the standard LOL dataset [66] using our approach, and the techniques DRD [66], LightenNet [41], PR [62], LIME [24], SRIE [18], RRM [44], LECARM [54], ALSM [64], and ZeroDCE [23], which are segregated into supervised, unsupervised and zero-shot categories.",
          "confidence_score": 1.0,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions the LOL dataset, which is a specific dataset used for low-light image enhancement. The dataset is used to compare the performance of various techniques.",
          "citing_paper_doi": "10.1109/CVPR46437.2021.01594",
          "cited_paper_doi": "10.1109/TIP.2019.2922106",
          "citing_paper_url": "https://www.semanticscholar.org/paper/68f21cbd2d5001528c07b16d27b8d4de5d6544e2",
          "cited_paper_url": "https://www.semanticscholar.org/paper/c70c29456ed9bb20423bfcbba7e4c2a0dd85f400",
          "citing_paper_year": 2021,
          "cited_paper_year": 2019
        },
        {
          "dataset_name": "LOL dataset",
          "dataset_description": "Used to quantitatively compare low-light image enhancement techniques, including supervised, unsupervised, and zero-shot methods, focusing on performance metrics.",
          "citing_paper_id": "235691711",
          "cited_paper_id": 52008443,
          "context_text": "Table 3 presents quantitative performance comparison of low-light image enhancement on the standard LOL dataset [66] using our approach, and the techniques DRD [66], LightenNet [41], PR [62], LIME [24], SRIE [18], RRM [44], LECARM [54], ALSM [64], and ZeroDCE [23], which are segregated into supervised, unsupervised and zero-shot categories.",
          "confidence_score": 1.0,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions the LOL dataset, which is a specific dataset used for low-light image enhancement. The dataset is used to compare the performance of various techniques.",
          "citing_paper_doi": "10.1109/CVPR46437.2021.01594",
          "cited_paper_doi": null,
          "citing_paper_url": "https://www.semanticscholar.org/paper/68f21cbd2d5001528c07b16d27b8d4de5d6544e2",
          "cited_paper_url": "https://www.semanticscholar.org/paper/c7798bbbcd2e85593a2b2d37b92dabac63420cd8",
          "citing_paper_year": 2021,
          "cited_paper_year": 2018
        },
        {
          "dataset_name": "LOL dataset",
          "dataset_description": "Used to quantitatively compare low-light image enhancement techniques, including supervised, unsupervised, and zero-shot methods, focusing on performance metrics.",
          "citing_paper_id": "235691711",
          "cited_paper_id": 204836935,
          "context_text": "Table 3 presents quantitative performance comparison of low-light image enhancement on the standard LOL dataset [66] using our approach, and the techniques DRD [66], LightenNet [41], PR [62], LIME [24], SRIE [18], RRM [44], LECARM [54], ALSM [64], and ZeroDCE [23], which are segregated into supervised, unsupervised and zero-shot categories.",
          "confidence_score": 1.0,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions the LOL dataset, which is a specific dataset used for low-light image enhancement. The dataset is used to compare the performance of various techniques.",
          "citing_paper_doi": "10.1109/CVPR46437.2021.01594",
          "cited_paper_doi": "10.1145/3343031.3350983",
          "citing_paper_url": "https://www.semanticscholar.org/paper/68f21cbd2d5001528c07b16d27b8d4de5d6544e2",
          "cited_paper_url": "https://www.semanticscholar.org/paper/dd475e308290b3ee46dcc0d9ebaa7dcfb7b0fff9",
          "citing_paper_year": 2021,
          "cited_paper_year": 2019
        },
        {
          "dataset_name": "LOL dataset",
          "dataset_description": "Used as a benchmark for low-light image enhancement, consisting of 485 training pairs and 15 testing pairs with diverse indoor and outdoor scenes.",
          "citing_paper_id": "268513542",
          "cited_paper_id": 52008443,
          "context_text": "We use the famous LOL [61] datasets as the benchmark with 485 training pairs and 15 testing pairs, which consist of a large number of indoor and outdoor scenes with different levels of light and noise.",
          "confidence_score": 1.0,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions the 'LOL' dataset, which is used as a benchmark for low-light image enhancement. It specifies the number of training and testing pairs and the types of scenes included.",
          "citing_paper_doi": "10.1109/CVPR52733.2024.02404",
          "cited_paper_doi": null,
          "citing_paper_url": "https://www.semanticscholar.org/paper/1ff44c75c361d2ed28f868361a29437197b0270c",
          "cited_paper_url": "https://www.semanticscholar.org/paper/c7798bbbcd2e85593a2b2d37b92dabac63420cd8",
          "citing_paper_year": 2024,
          "cited_paper_year": 2018
        },
        {
          "dataset_name": "LOL dataset",
          "dataset_description": "Used for Low-Light Enhancement (LLE), focusing on improving image quality in low-light conditions through specific enhancement techniques. | Used for retouching, Low-Light Filtering (LLF), and Multi-Task Modeling (MTM), focusing on expert-level image adjustments and multi-task learning approaches. | Used to generate on-the-fly data pairs for training the first eight types of image restoration models, focusing on diverse image content and quality.",
          "citing_paper_id": "271891992",
          "cited_paper_id": null,
          "context_text": "The LOL dataset [43] is used for LLE, and expert-C retouched images of the Adobe-MIT Fivek dataset [5] are used for retouching, LLF, and MTM.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions two specific datasets, LOL and Adobe-MIT Fivek, which are used for different aspects of image restoration and enhancement.",
          "citing_paper_doi": "10.1145/3664647.3681621",
          "cited_paper_doi": null,
          "citing_paper_url": "https://www.semanticscholar.org/paper/ca2e8f7b9e902aa917904cb4b75f8dc0845edbcd",
          "cited_paper_url": null,
          "citing_paper_year": 2024,
          "cited_paper_year": null
        },
        {
          "dataset_name": "LOL dataset",
          "dataset_description": "Used to evaluate low-light image enhancement techniques, focusing on improving visual quality and detail preservation in dark images. | Used to evaluate the performance of GDP on low-light video enhancement, assessing the effectiveness in enhancing video content under low-light conditions. | Used to challenge the robustness of GDP on extremely low-light conditions, specifically designed to test the limits of low-light enhancement algorithms. | Used for quantitative comparison in the image enlightening task, specifically for mobile phone images in low-light conditions. | Utilized to test mobile phone camera performance in low-light scenarios, evaluating real-world applicability and user experience. | Used to test the capability of GDP on low-light image enhancement, focusing on improving visual quality in low-light conditions. | Used to evaluate low-light image and video enhancement techniques, focusing on performance under various lighting conditions using deep learning methods. | Used for quantitative comparison in the image enlightening task, focusing on video enhancement in low-light conditions. | Applied to assess video enhancement methods, emphasizing temporal consistency and noise reduction in low-light conditions. | Used for low-light image and video enhancement, specifically to train and evaluate deep learning models for improving image quality in low-light conditions. | Used for quantitative comparison in the image enlightening task, evaluating performance on low-light images. | Used to evaluate GDP on low-light enhancement and HDR recovery, demonstrating superior qualitative and quantitative performance compared to zero-shot baselines. | Used to evaluate HDR recovery methods, specifically comparing HDR-GDP-x0 with state-of-the-art HDR techniques on 100 randomly selected test scenes. | Used to train GDP, showing its effectiveness on images outside its training distribution, particularly in low-light enhancement and HDR recovery.",
          "citing_paper_id": "257921922",
          "cited_paper_id": 235390405,
          "context_text": "Qualitative results of low-light enhancement on the LOL [88], VE-LOL [47], and LoLi-Phone [41] datasets.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The citation mentions three specific datasets used for low-light image enhancement, which are relevant to the topic of all-in-one image restoration.",
          "citing_paper_doi": "10.1109/CVPR52729.2023.00958",
          "cited_paper_doi": "10.1109/TPAMI.2021.3126387",
          "citing_paper_url": "https://www.semanticscholar.org/paper/5549dc3ceff07561d9fb59610c0f78c71617901a",
          "cited_paper_url": "https://www.semanticscholar.org/paper/22731d2a9fdb26684f04ad3864b21bf83dd5c03e",
          "citing_paper_year": 2023,
          "cited_paper_year": 2021
        },
        {
          "dataset_name": "LOL dataset",
          "dataset_description": "Used for visual comparison of low-light enhancement approaches, specifically evaluating the performance of different methods including MIRNet. The dataset provides ground-truth images for benchmarking. | Used to assess the performance of the MIRNet algorithm for image enhancement, reporting PSNR/SSIM values and demonstrating significant improvements over previous methods. | Created for low-light image enhancement, providing 485 training images and 15 testing images, each consisting of a low-light input image and its corresponding well-exposed reference. | Used to evaluate low-light image enhancement methods, specifically comparing the performance of various algorithms including the proposed method. The dataset is crucial for benchmarking and advancing the state-of-the-art in low-light image enhancement. | Used for training and testing image super-resolution models at scale factors 2, 3, and 4. Provides 183, 234, and 178 training image pairs, respectively, and 30 test images for each scale factor. | Used to evaluate the effectiveness of the MIRNet algorithm for low-light image enhancement, comparing PSNR/SSIM values against other techniques.",
          "citing_paper_id": "263784964",
          "cited_paper_id": 52008443,
          "context_text": "we demonstrate the eectiveness of our algorithm by evaluating it for the image enhancement task. We report PSNR/SSIM values of our method and several other techniques in Table5and Table6for the LoL [107] and MITAdobe FiveK [12] datasets, respectively. It can be seen that our MIRNet achieves signicant improvements over previous approaches. Notably, when compared to the recent best methods, MIRNet obt",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions two specific datasets, LoL and MITAdobe FiveK, which are used to evaluate the performance of the MIRNet algorithm for image enhancement.",
          "citing_paper_doi": "10.1007/978-3-030-58595-2_30",
          "cited_paper_doi": null,
          "citing_paper_url": "https://www.semanticscholar.org/paper/26f27bd5892e869b54c69d140b20c33638f26c57",
          "cited_paper_url": "https://www.semanticscholar.org/paper/c7798bbbcd2e85593a2b2d37b92dabac63420cd8",
          "citing_paper_year": 2020,
          "cited_paper_year": 2018
        },
        {
          "dataset_name": "LOL dataset",
          "dataset_description": "Used to demonstrate visual results of low-light image enhancement, focusing on the effectiveness of the proposed method in improving image quality.",
          "citing_paper_id": "254535902",
          "cited_paper_id": 250601992,
          "context_text": "Figures D & E illustrate some visual results on LOL [30] dataset.",
          "confidence_score": 1.0,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The citation mentions the use of the LOL dataset for visual results, which is relevant to image restoration.",
          "citing_paper_doi": "10.1109/CVPR52729.2023.01350",
          "cited_paper_doi": "10.1109/CVPR52688.2022.00581",
          "citing_paper_url": "https://www.semanticscholar.org/paper/afd674940791fb2daeecb0801a5068d80d9f77a7",
          "cited_paper_url": "https://www.semanticscholar.org/paper/87509a64ad473cfa78bd83c8fd06d86207b0951c",
          "citing_paper_year": 2022,
          "cited_paper_year": 2022
        },
        {
          "dataset_name": "LOL dataset",
          "dataset_description": "Used for low-light image enhancement, evaluating the effectiveness of the proposed method in improving image quality under low-light conditions. | Used to demonstrate the effectiveness of the proposed method in low-light image enhancement, specifically showing results on images from this dataset. | Used for low-light image enhancement, specifically adopting the official split of 485 training images and 15 testing images to evaluate the model's performance. | Used for motion deblurring, assessing the performance of the method in removing motion blur from images captured in dynamic scenes.",
          "citing_paper_id": "267320695",
          "cited_paper_id": 52008443,
          "context_text": "Additional high-resolution qualitative results using the LOL [83] dataset (low-light image enhancement), and the GoPro [57] dataset (motion deblurring).",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The citation mentions two specific datasets, LOL and GoPro, which are used for low-light image enhancement and motion deblurring, respectively. These datasets are directly relevant to the research topic of all-in-one image restoration.",
          "citing_paper_doi": "10.1007/978-3-031-72764-1_1",
          "cited_paper_doi": null,
          "citing_paper_url": "https://www.semanticscholar.org/paper/03ad1a40a4399c8b77bbeaa389fcd14b10b322c0",
          "cited_paper_url": "https://www.semanticscholar.org/paper/c7798bbbcd2e85593a2b2d37b92dabac63420cd8",
          "citing_paper_year": 2024,
          "cited_paper_year": 2018
        },
        {
          "dataset_name": "LOL dataset",
          "dataset_description": "Used for training and testing low-light image enhancement models, focusing on improving image quality under low-light conditions using deep retinex decomposition.",
          "citing_paper_id": "264146360",
          "cited_paper_id": 52008443,
          "context_text": "We adopt the commonly-used LOL dataset (Wei et al. 2018) for training and testing.",
          "confidence_score": 1.0,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The citation mentions the LOL dataset, which is a specific dataset used for low-light image enhancement. It is clearly used for training and testing in the context of the research.",
          "citing_paper_doi": "10.48550/arXiv.2310.10513",
          "cited_paper_doi": null,
          "citing_paper_url": "https://www.semanticscholar.org/paper/836906c334159d46a691dfc5466c33caa3d22f65",
          "cited_paper_url": "https://www.semanticscholar.org/paper/c7798bbbcd2e85593a2b2d37b92dabac63420cd8",
          "citing_paper_year": 2023,
          "cited_paper_year": 2018
        },
        {
          "dataset_name": "LOL dataset",
          "dataset_description": "Used to evaluate low-light image enhancement methods, focusing on PSNR and SSIM metrics to assess restoration quality. | Used for low-light image enhancement, specifically to train and evaluate models on low-light images, focusing on improving visual quality and detail preservation.",
          "citing_paper_id": "270155352",
          "cited_paper_id": 231641545,
          "context_text": "Low-light image enhancement: We used LOL datasets for low-light enhancement, which have two versions: v1 [75] and v2 [76].",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions the use of LOL datasets for low-light image enhancement, specifying two versions: v1 and v2. These datasets are clearly identified and used for training and evaluation.",
          "citing_paper_doi": "10.1109/TMM.2024.3407656",
          "cited_paper_doi": "10.1109/TIP.2021.3050850",
          "citing_paper_url": "https://www.semanticscholar.org/paper/3a80830bf3e31c855afaff17a5b590bc052839c6",
          "cited_paper_url": "https://www.semanticscholar.org/paper/d1550954e9793d61736d519d0e0fa2085d354d79",
          "citing_paper_year": 2024,
          "cited_paper_year": 2021
        }
      ]
    },
    {
      "cited_paper_id": "57189262",
      "citation_count": 0,
      "total_dataset_mentions": 14,
      "unique_datasets": [
        "LIVE1"
      ],
      "dataset_details": [
        {
          "dataset_name": "LIVE1",
          "dataset_description": "Used to evaluate the quality of interpolated images through the PSNR distance metric, focusing on high-quality image restoration. | Used for evaluating image quality assessment algorithms, focusing on full-reference methods and their statistical performance. | Used to evaluate denoising performance, comparing PSNR values across different noise levels and methods. | Used to evaluate image quality assessment algorithms by calculating PSNR values and selecting the optimal coefficient for the best performance. | Used to evaluate super resolution performance, comparing PSNR values across different scaling factors and methods. | Used to evaluate image quality assessment algorithms, specifically testing the effects of JPEG degradation at q30, focusing on over-sharpening and over-smoothing issues.",
          "citing_paper_id": "119308964",
          "cited_paper_id": 9247572,
          "context_text": "To verify whether the interpolated image is of high quality, we use the PSNR distance on LIVE1 test set as the evaluation metric.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions the LIVE1 test set, which is a specific dataset used for evaluating image quality assessment algorithms.",
          "citing_paper_doi": "10.1109/CVPR.2019.01131",
          "cited_paper_doi": "10.1109/TIP.2006.881959",
          "citing_paper_url": "https://www.semanticscholar.org/paper/e6ab7b7fee9a25342ef1cac082c77a7f1021a982",
          "cited_paper_url": "https://www.semanticscholar.org/paper/1c79a3b52bf92fbc01de64057a264de9e94a1025",
          "citing_paper_year": 2019,
          "cited_paper_year": 2006
        },
        {
          "dataset_name": "LIVE1",
          "dataset_description": "Used to evaluate the performance of image restoration methods, focusing on high-quality denoising and deblocking of grayscale and color images. | Used to test image restoration methods, emphasizing common image degradation scenarios and restoration performance. | Used to evaluate image restoration algorithms, focusing on perceptual quality assessment and distortion types.",
          "citing_paper_id": "252683961",
          "cited_paper_id": null,
          "context_text": "We take the widely used LIVE1 (Sheikh, 2005) and Classic5 (Foi et al., 2007) as test sets to evaluate the performance of each method.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions two specific datasets, LIVE1 and Classic5, which are used as test sets to evaluate the performance of image restoration methods.",
          "citing_paper_doi": "10.48550/arXiv.2210.00405",
          "cited_paper_doi": null,
          "citing_paper_url": "https://www.semanticscholar.org/paper/a3d9fd2c384e98fb6074a9064562a4e4dd941ed8",
          "cited_paper_url": null,
          "citing_paper_year": 2022,
          "cited_paper_year": null
        },
        {
          "dataset_name": "LIVE1",
          "dataset_description": "Used to evaluate the performance of R 2 Net in reducing compression artifacts, specifically measuring improvements over previous methods across all compression factors.",
          "citing_paper_id": "222103869",
          "cited_paper_id": 9569924,
          "context_text": "Similarly, the overall improvement on the LIVE1 dataset for R 2 Net is 0.79 (over [68]) and 0.5 dB (over [12]) for all compression factors.",
          "confidence_score": 1.0,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions the LIVE1 dataset, which is a specific image quality assessment dataset. It is used to evaluate the performance of R 2 Net in reducing compression artifacts.",
          "citing_paper_doi": "10.1109/TNNLS.2021.3131739",
          "cited_paper_doi": "10.1109/ICCV.2015.73",
          "citing_paper_url": "https://www.semanticscholar.org/paper/3d18527d24ff382df0b7fb155d4ce456569d704f",
          "cited_paper_url": "https://www.semanticscholar.org/paper/09642681d46282e76fd9d1336001ef6473b72ec8",
          "citing_paper_year": 2020,
          "cited_paper_year": 2015
        },
        {
          "dataset_name": "LIVE1",
          "dataset_description": "Used to evaluate image restoration methods, specifically comparing performance on high-quality JPEG images with quality factor 10. | Used for visual comparison of JPEG compression artifacts reduction, specifically evaluating the performance of various image restoration methods including CAT (ours).",
          "citing_paper_id": "254018182",
          "cited_paper_id": 57189262,
          "context_text": "LIVE1: carnivaldolls HQ JPEG (q=10) RNAN [52] RDN [53] SwinIR [21] CAT (ours) Figure 6: Visual comparison about JPEG compression artifacts reduction (q=10).",
          "confidence_score": 0.9,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions 'LIVE1' which appears to be a dataset used for visual comparison of image restoration methods. No other specific datasets are mentioned.",
          "citing_paper_doi": "10.48550/arXiv.2211.13654",
          "cited_paper_doi": "10.1109/TPAMI.2020.2968521",
          "citing_paper_url": "https://www.semanticscholar.org/paper/1c49e58a935c80c8ec8307e937fc61c8f1f80433",
          "cited_paper_url": "https://www.semanticscholar.org/paper/7a4c5dc91af23805f4f6ecea30ca6e5d085e2108",
          "citing_paper_year": 2022,
          "cited_paper_year": 2018
        },
        {
          "dataset_name": "LIVE1",
          "dataset_description": "Used to evaluate the performance of image restoration methods, focusing on high-quality denoising and deblocking of grayscale and color images. | Used to assess the effectiveness of image restoration techniques, specifically in denoising and deblocking applications.",
          "citing_paper_id": "232170566",
          "cited_paper_id": 1151947,
          "context_text": "We evaluate the performance on LIVE1 [49] and Classic5 [46] test sets, and the quantitative results are presented in Table IV.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions two specific test sets, LIVE1 and Classic5, which are commonly used in image restoration and quality assessment.",
          "citing_paper_doi": "10.1109/TMM.2021.3063916",
          "cited_paper_doi": "10.1109/TIP.2007.891788",
          "citing_paper_url": "https://www.semanticscholar.org/paper/7d76ae738c806330632255ad2c592525b3d2df22",
          "cited_paper_url": "https://www.semanticscholar.org/paper/60e7d2cf0aa8088a9ba7a01b0aad19a61e441e0c",
          "citing_paper_year": 2021,
          "cited_paper_year": 2007
        },
        {
          "dataset_name": "LIVE1",
          "dataset_description": "Used to evaluate the performance of image restoration methods, focusing on high-quality denoising and deblocking of grayscale and color images. | Used to assess the effectiveness of image restoration techniques, specifically in denoising and deblocking applications.",
          "citing_paper_id": "232170566",
          "cited_paper_id": null,
          "context_text": "We evaluate the performance on LIVE1 [49] and Classic5 [46] test sets, and the quantitative results are presented in Table IV.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions two specific test sets, LIVE1 and Classic5, which are commonly used in image restoration and quality assessment.",
          "citing_paper_doi": "10.1109/TMM.2021.3063916",
          "cited_paper_doi": null,
          "citing_paper_url": "https://www.semanticscholar.org/paper/7d76ae738c806330632255ad2c592525b3d2df22",
          "cited_paper_url": null,
          "citing_paper_year": 2021,
          "cited_paper_year": null
        },
        {
          "dataset_name": "LIVE1",
          "dataset_description": "Used to assess image quality and restoration, reporting PSNR/SSIM values in the context of ARCNN. Focuses on error visibility and structural similarity. | Used to evaluate the performance of the RNAN model, specifically measuring PSNR and SSIM values across various JPEG qualities.",
          "citing_paper_id": "85501306",
          "cited_paper_id": 207761262,
          "context_text": "We use the same datasets LIVE1 (Sheikh et al., 2005) and Classic5 (Foi et al., 2007) in ARCNN and report PSNR/SSIM values in Table 5.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions two specific datasets, LIVE1 and Classic5, which are used for image quality assessment and restoration. These datasets are clearly identified and used in the research.",
          "citing_paper_doi": null,
          "cited_paper_doi": "10.1109/TIP.2003.819861",
          "citing_paper_url": "https://www.semanticscholar.org/paper/34439db81b482cd562e1cdba974c70a2b89cd6d4",
          "cited_paper_url": "https://www.semanticscholar.org/paper/eae2e0fa72e898c289365c0af16daf57a7a6cf40",
          "citing_paper_year": 2019,
          "cited_paper_year": 2004
        },
        {
          "dataset_name": "LIVE1",
          "dataset_description": "Used to assess image quality and restoration, reporting PSNR/SSIM values in the context of ARCNN. Focuses on error visibility and structural similarity. | Used to evaluate the performance of the RNAN model, specifically measuring PSNR and SSIM values across various JPEG qualities.",
          "citing_paper_id": "85501306",
          "cited_paper_id": null,
          "context_text": "We use the same datasets LIVE1 (Sheikh et al., 2005) and Classic5 (Foi et al., 2007) in ARCNN and report PSNR/SSIM values in Table 5.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions two specific datasets, LIVE1 and Classic5, which are used for image quality assessment and restoration. These datasets are clearly identified and used in the research.",
          "citing_paper_doi": null,
          "cited_paper_doi": null,
          "citing_paper_url": "https://www.semanticscholar.org/paper/34439db81b482cd562e1cdba974c70a2b89cd6d4",
          "cited_paper_url": null,
          "citing_paper_year": 2019,
          "cited_paper_year": null
        },
        {
          "dataset_name": "LIVE1",
          "dataset_description": "Utilized to test image compression and restoration algorithms, specifically evaluating color consistency and texture preservation. | Used to evaluate prompt-learning-based methods on high-resolution images, focusing on single image super-resolution performance. | Used to evaluate the performance of PromptCIR on image restoration, focusing on quality factors [10,20,30,40]. | Used for evaluating image compression quality, focusing on perceptual metrics and visual fidelity in restored images. | Applied to assess edge preservation in image restoration, emphasizing segmentation accuracy and boundary detection.",
          "citing_paper_id": "269430366",
          "cited_paper_id": null,
          "context_text": "Following previous works [8, 14, 58], we evaluate our PromptCIR on LIVE1 [45] and ICB [44] with quality factors [10,20,30,40].",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions two specific datasets, LIVE1 and ICB, which are used for evaluating the performance of the PromptCIR model. These datasets are relevant to image restoration and quality assessment.",
          "citing_paper_doi": "10.1109/CVPRW63382.2024.00645",
          "cited_paper_doi": null,
          "citing_paper_url": "https://www.semanticscholar.org/paper/a9a10ec89f604effce447efb7ac12127ca38b29c",
          "cited_paper_url": null,
          "citing_paper_year": 2024,
          "cited_paper_year": null
        },
        {
          "dataset_name": "LIVE1",
          "dataset_description": "Used to evaluate the performance of PromptCIR on image restoration, focusing on quality factors [10,20,30,40].",
          "citing_paper_id": "269430366",
          "cited_paper_id": 6540453,
          "context_text": "Following previous works [8, 14, 58], we evaluate our PromptCIR on LIVE1 [45] and ICB [44] with quality factors [10,20,30,40].",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions two specific datasets, LIVE1 and ICB, which are used for evaluating the performance of the PromptCIR model. These datasets are relevant to image restoration and quality assessment.",
          "citing_paper_doi": "10.1109/CVPRW63382.2024.00645",
          "cited_paper_doi": "10.1109/CVPRW.2017.151",
          "citing_paper_url": "https://www.semanticscholar.org/paper/a9a10ec89f604effce447efb7ac12127ca38b29c",
          "cited_paper_url": "https://www.semanticscholar.org/paper/7ba5d3808e117e7a68dc40331ce1d483ceeedcb2",
          "citing_paper_year": 2024,
          "cited_paper_year": 2017
        },
        {
          "dataset_name": "LIVE1",
          "dataset_description": "Used to evaluate the performance of PromptCIR on image restoration, focusing on quality factors [10,20,30,40].",
          "citing_paper_id": "269430366",
          "cited_paper_id": 246240170,
          "context_text": "Following previous works [8, 14, 58], we evaluate our PromptCIR on LIVE1 [45] and ICB [44] with quality factors [10,20,30,40].",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions two specific datasets, LIVE1 and ICB, which are used for evaluating the performance of the PromptCIR model. These datasets are relevant to image restoration and quality assessment.",
          "citing_paper_doi": "10.1109/CVPRW63382.2024.00645",
          "cited_paper_doi": "10.1109/TPAMI.2023.3282631",
          "citing_paper_url": "https://www.semanticscholar.org/paper/a9a10ec89f604effce447efb7ac12127ca38b29c",
          "cited_paper_url": "https://www.semanticscholar.org/paper/f4b11a696aa5a03fed1bfc47e65fdb7eb0e529c1",
          "citing_paper_year": 2024,
          "cited_paper_year": 2022
        },
        {
          "dataset_name": "LIVE1",
          "dataset_description": "Used to evaluate the performance of PromptCIR on image restoration, focusing on quality factors [10,20,30,40].",
          "citing_paper_id": "269430366",
          "cited_paper_id": 253121295,
          "context_text": "Following previous works [8, 14, 58], we evaluate our PromptCIR on LIVE1 [45] and ICB [44] with quality factors [10,20,30,40].",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions two specific datasets, LIVE1 and ICB, which are used for evaluating the performance of the PromptCIR model. These datasets are relevant to image restoration and quality assessment.",
          "citing_paper_doi": "10.1109/CVPRW63382.2024.00645",
          "cited_paper_doi": "10.1007/978-3-031-19790-1_37",
          "citing_paper_url": "https://www.semanticscholar.org/paper/a9a10ec89f604effce447efb7ac12127ca38b29c",
          "cited_paper_url": "https://www.semanticscholar.org/paper/052339265dea6fa2750271954f011e4ced0505f1",
          "citing_paper_year": 2024,
          "cited_paper_year": 2022
        },
        {
          "dataset_name": "LIVE1",
          "dataset_description": "Used to evaluate image restoration methods, specifically comparing performance gains over existing models like RNAN and DnCNN. | Used to evaluate the performance of PANet with pyramid attention, focusing on image quality across various distortion levels. | Used to evaluate image restoration methods, focusing on high-quality denoising and deblocking of grayscale and color images.",
          "citing_paper_id": "260748050",
          "cited_paper_id": 1151947,
          "context_text": "We present results on LIVE1 Sheikh et al. (2005) and Classic5 Foi et al. (2007), following the same settings in RNAN.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions two specific datasets, LIVE1 and Classic5, which are used for evaluating image restoration methods.",
          "citing_paper_doi": "10.1007/s11263-023-01843-5",
          "cited_paper_doi": "10.1109/TIP.2007.891788",
          "citing_paper_url": "https://www.semanticscholar.org/paper/e98cc6c252a7d69898730df17449fc1a09d1a1ae",
          "cited_paper_url": "https://www.semanticscholar.org/paper/60e7d2cf0aa8088a9ba7a01b0aad19a61e441e0c",
          "citing_paper_year": 2023,
          "cited_paper_year": 2007
        },
        {
          "dataset_name": "LIVE1",
          "dataset_description": "Used to evaluate the performance of PANet with pyramid attention, focusing on image quality across various distortion levels. | Used to evaluate image restoration methods, focusing on high-quality denoising and deblocking of grayscale and color images.",
          "citing_paper_id": "260748050",
          "cited_paper_id": null,
          "context_text": "We present results on LIVE1 Sheikh et al. (2005) and Classic5 Foi et al. (2007), following the same settings in RNAN.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions two specific datasets, LIVE1 and Classic5, which are used for evaluating image restoration methods.",
          "citing_paper_doi": "10.1007/s11263-023-01843-5",
          "cited_paper_doi": null,
          "citing_paper_url": "https://www.semanticscholar.org/paper/e98cc6c252a7d69898730df17449fc1a09d1a1ae",
          "cited_paper_url": null,
          "citing_paper_year": 2023,
          "cited_paper_year": null
        }
      ]
    },
    {
      "cited_paper_id": "8282555",
      "citation_count": 0,
      "total_dataset_mentions": 12,
      "unique_datasets": [
        "BSD100"
      ],
      "dataset_details": [
        {
          "dataset_name": "BSD100",
          "dataset_description": "Used for deblurring tasks, featuring real motion-blurred images to test deblurring algorithms. | Used for rain removal tasks, providing synthetic rain images to train and evaluate de-raining models. | Used for super-resolution tasks, featuring urban scenes to assess restoration quality in complex environments. | Used for super-resolution tasks, providing high-quality images for training and evaluation. | Used for rain removal tasks, containing real outdoor rain images to test de-raining performance. | Used for rain removal tasks, providing a mix of synthetic and real rain images for comprehensive evaluation. | Used for rain removal tasks, offering synthetic and real rain images to evaluate de-raining algorithms. | Used to evaluate image denoising models, focusing on performance in natural image denoising tasks. | Used to evaluate image deblurring models, focusing on performance in handling motion blur in video frames. | Used for deblurring tasks, containing a diverse set of real and synthetic blurred images for robust evaluation. | Used to evaluate image restoration models, focusing on rain removal performance in synthetic rain images. | Used for super-resolution tasks, containing manga images to test restoration algorithms on line art and text.",
          "citing_paper_id": "273502246",
          "cited_paper_id": 64193,
          "context_text": "With the rapid development of AiOIR, numerous researchers collected a series of datasets tailored for various IR tasks, e.g. , BSD100 [26], Manga109 [27] and Urban100 [28] for SR, RainDrop [29], Outdoor-Rain [30], SPA [31] and Rain-100H [32] for draining, GoPro [33] and HIDE [34] for deblurring,…",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions several datasets specifically designed for image restoration tasks, which align with the research topic of All-in-One Image Restoration.",
          "citing_paper_doi": "10.48550/arXiv.2410.15067",
          "cited_paper_doi": "10.1109/ICCV.2001.937655",
          "citing_paper_url": "https://www.semanticscholar.org/paper/f5ff17162a4936db3f64ed7bafae13abcfa63a27",
          "cited_paper_url": "https://www.semanticscholar.org/paper/9a1ed876196ec9733acb1daa6d65e35ff0414291",
          "citing_paper_year": 2024,
          "cited_paper_year": 2001
        },
        {
          "dataset_name": "BSD100",
          "dataset_description": "Used for deblurring tasks, providing sharp and blurred image pairs to train deblurring models. | Used for deblurring tasks, containing high-resolution images with motion blur to enhance deblurring quality. | Used for deraining tasks, providing synthetic rain images to remove rain streaks. | Used for super-resolution tasks, featuring urban scenes to improve resolution and texture. | Used for super-resolution tasks, providing high-quality images for training and evaluation. | Used for low-light image enhancement, providing low-light images to improve brightness and contrast. | Used to evaluate image denoising models, focusing on performance in natural image denoising tasks. | Used for deraining tasks, containing real outdoor rain images to improve image clarity. | Used for deraining tasks, featuring high-quality rain images to improve deraining accuracy. | Used to evaluate image deblurring models, focusing on performance in handling motion blur in video frames. | Used to evaluate image restoration models, focusing on rain removal performance in synthetic rain images. | Used for super-resolution tasks, focusing on manga images to enhance detail and clarity. | Used for deraining tasks, offering a diverse set of rain images to enhance deraining performance.",
          "citing_paper_id": "273502246",
          "cited_paper_id": 15443600,
          "context_text": "…researchers collected a series of datasets tailored for various IR tasks, e.g. , BSD100 [26], Manga109 [27] and Urban100 [28] for SR, RainDrop [29], Outdoor-Rain [30], SPA [31] and Rain-100H [32] for draining, GoPro [33] and HIDE [34] for deblurring, LOL [35] for low-light image enhancement, etc .",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions several datasets used for various image restoration tasks, which align with the research topic of all-in-one image restoration.",
          "citing_paper_doi": "10.48550/arXiv.2410.15067",
          "cited_paper_doi": "10.1109/CVPR.2017.183",
          "citing_paper_url": "https://www.semanticscholar.org/paper/f5ff17162a4936db3f64ed7bafae13abcfa63a27",
          "cited_paper_url": "https://www.semanticscholar.org/paper/6de27f256e97c014a95706c72af7fa651e8fa34a",
          "citing_paper_year": 2024,
          "cited_paper_year": 2016
        },
        {
          "dataset_name": "BSD100",
          "dataset_description": "Used for super-resolution tasks, featuring urban scenes to improve resolution and texture. | Used for low-light image enhancement, containing low-light and normal-light image pairs to improve low-light visibility. | Used for super-resolution tasks, providing high-quality images for training and evaluation. | Used for deblurring tasks, providing a large dataset of blurred and clear images for deblurring research. | Used for rain removal tasks, providing images with raindrops to train and test rain removal algorithms. | Used for rain removal tasks, consisting of 100 pairs of rainy and clean images for training and testing. | Used for deblurring tasks, featuring sharp and blurred image pairs to train and evaluate deblurring models. | Used for rain removal tasks, containing outdoor scenes with rain to evaluate rain removal performance. | Used for rain removal tasks, offering synthetic and real rain images to assess rain removal effectiveness. | Used for super-resolution tasks, focusing on manga images to enhance detail and clarity.",
          "citing_paper_id": "273502246",
          "cited_paper_id": 4539586,
          "context_text": "…researchers collected a series of datasets tailored for various IR tasks, e.g. , BSD100 [26], Manga109 [27] and Urban100 [28] for SR, RainDrop [29], Outdoor-Rain [30], SPA [31] and Rain-100H [32] for draining, GoPro [33] and HIDE [34] for deblurring, LOL [35] for low-light image enhancement,…",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions several datasets used for specific image restoration tasks, which align with the research topic of all-in-one image restoration.",
          "citing_paper_doi": "10.48550/arXiv.2410.15067",
          "cited_paper_doi": "10.1109/CVPR.2018.00263",
          "citing_paper_url": "https://www.semanticscholar.org/paper/f5ff17162a4936db3f64ed7bafae13abcfa63a27",
          "cited_paper_url": "https://www.semanticscholar.org/paper/34e38559df559bfedfc3e1edc2b120ab2b5d444e",
          "citing_paper_year": 2024,
          "cited_paper_year": 2017
        },
        {
          "dataset_name": "BSD100",
          "dataset_description": "Used for deblurring tasks, providing a diverse set of images with varying degrees of blur. | Used for super-resolution tasks, providing high-quality images for training and evaluation. | Used for rain removal tasks, containing synthetic and real rain images to enhance model robustness. | Used for rain removal tasks, providing images with raindrops to train models for better visibility. | Used for low-light enhancement tasks, containing low-light and normal-light image pairs to improve visibility. | Used for rain removal tasks, offering a large set of rainy images for comprehensive evaluation. | Used for deblurring tasks, featuring sharp and blurred image pairs to train models for motion blur correction. | Used for rain removal tasks, focusing on outdoor scenes with rain to improve image clarity. | Used for super-resolution tasks, featuring urban scenes to improve resolution in complex environments. | Used for super-resolution tasks, focusing on manga images to enhance detail and clarity.",
          "citing_paper_id": "273502246",
          "cited_paper_id": 8282555,
          "context_text": "…of AiOIR, numerous researchers collected a series of datasets tailored for various IR tasks, e.g. , BSD100 [26], Manga109 [27] and Urban100 [28] for SR, RainDrop [29], Outdoor-Rain [30], SPA [31] and Rain-100H [32] for draining, GoPro [33] and HIDE [34] for deblurring, LOL [35] for…",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions several datasets used for specific image restoration tasks, which align with the topic of All-in-One Image Restoration.",
          "citing_paper_doi": "10.48550/arXiv.2410.15067",
          "cited_paper_doi": "10.1109/CVPR.2015.7299156",
          "citing_paper_url": "https://www.semanticscholar.org/paper/f5ff17162a4936db3f64ed7bafae13abcfa63a27",
          "cited_paper_url": "https://www.semanticscholar.org/paper/2946c4e29b057d3543f09b89f547965895676d19",
          "citing_paper_year": 2024,
          "cited_paper_year": 2015
        },
        {
          "dataset_name": "BSD100",
          "dataset_description": "Used for deblurring tasks, providing sharp and blurred image pairs to train deblurring models. | Used for deblurring tasks, containing high-resolution images with motion blur to enhance deblurring quality. | Used for deraining tasks, providing synthetic rain images to remove rain streaks. | Used for super-resolution tasks, featuring urban scenes to improve resolution and texture. | Used for super-resolution tasks, providing high-quality images for training and evaluation. | Used for low-light image enhancement, providing low-light images to improve brightness and contrast. | Used for deraining tasks, containing real outdoor rain images to improve image clarity. | Used for deraining tasks, featuring high-quality rain images to improve deraining accuracy. | Used for super-resolution tasks, focusing on manga images to enhance detail and clarity. | Used for deraining tasks, offering a diverse set of rain images to enhance deraining performance.",
          "citing_paper_id": "273502246",
          "cited_paper_id": 91184545,
          "context_text": "…researchers collected a series of datasets tailored for various IR tasks, e.g. , BSD100 [26], Manga109 [27] and Urban100 [28] for SR, RainDrop [29], Outdoor-Rain [30], SPA [31] and Rain-100H [32] for draining, GoPro [33] and HIDE [34] for deblurring, LOL [35] for low-light image enhancement, etc .",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions several datasets used for various image restoration tasks, which align with the research topic of all-in-one image restoration.",
          "citing_paper_doi": "10.48550/arXiv.2410.15067",
          "cited_paper_doi": "10.1109/CVPR.2019.01255",
          "citing_paper_url": "https://www.semanticscholar.org/paper/f5ff17162a4936db3f64ed7bafae13abcfa63a27",
          "cited_paper_url": "https://www.semanticscholar.org/paper/d3fa312b1f12ee60391705d3cac34cbcad42db14",
          "citing_paper_year": 2024,
          "cited_paper_year": 2019
        },
        {
          "dataset_name": "BSD100",
          "dataset_description": "Used to test × 4 super-resolution robustness, covering a wide range of natural scenes and structures. | Used to evaluate × 4 super-resolution on urban scenes, focusing on architectural details and textures. | Used to assess × 4 super-resolution on manga images, emphasizing line art and stylistic elements. | Used to assess × 4 super-resolution accuracy, emphasizing fine details and texture in diverse natural images. | Used to evaluate × 4 super-resolution performance, focusing on natural image quality and detail preservation.",
          "citing_paper_id": "264289165",
          "cited_paper_id": 64193,
          "context_text": "In Table 3, we present the quantitative results of × 4 SR on five benchmark datasets: Set5 [2], Set14 [51], BSD100 [31], Urban100 [19] and Manga109 [33].",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions five benchmark datasets used for evaluating × 4 super-resolution. These datasets are specific and have clear identifiers.",
          "citing_paper_doi": "10.48550/arXiv.2310.11881",
          "cited_paper_doi": "10.1109/ICCV.2001.937655",
          "citing_paper_url": "https://www.semanticscholar.org/paper/a9a441bbbbf0c1e826e36cc864c728d3516a3608",
          "cited_paper_url": "https://www.semanticscholar.org/paper/9a1ed876196ec9733acb1daa6d65e35ff0414291",
          "citing_paper_year": 2023,
          "cited_paper_year": 2001
        },
        {
          "dataset_name": "BSD100",
          "dataset_description": "Used to evaluate image restoration methods, specifically focusing on denoising and deblocking performance on a set of high-quality images.",
          "citing_paper_id": "209389923",
          "cited_paper_id": 1151947,
          "context_text": "The maximum iteration number is [46], PC-LRM [74], ANCE [75], D IC TV [53], BM3D [30], WNNM [5], CONCOLOR [48], SSR-QC [49], LER A G [76] AND RRC ON THE BSD 100 [70] A PPENDIX A P ROOF OF THE T HEOREM 2 Proof.",
          "confidence_score": 1.0,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions 'BSD 100' which is a well-known dataset in image processing. No other specific datasets are mentioned.",
          "citing_paper_doi": "10.1109/TIP.2019.2958309",
          "cited_paper_doi": "10.1109/TIP.2007.891788",
          "citing_paper_url": "https://www.semanticscholar.org/paper/ce8aa84e1e7930b071f242017fa17cae5727e1e9",
          "cited_paper_url": "https://www.semanticscholar.org/paper/60e7d2cf0aa8088a9ba7a01b0aad19a61e441e0c",
          "citing_paper_year": 2018,
          "cited_paper_year": 2007
        },
        {
          "dataset_name": "BSD100",
          "dataset_description": "Used to evaluate image restoration methods, specifically focusing on denoising and deblocking performance on a set of high-quality images.",
          "citing_paper_id": "209389923",
          "cited_paper_id": 7166471,
          "context_text": "The maximum iteration number is [46], PC-LRM [74], ANCE [75], D IC TV [53], BM3D [30], WNNM [5], CONCOLOR [48], SSR-QC [49], LER A G [76] AND RRC ON THE BSD 100 [70] A PPENDIX A P ROOF OF THE T HEOREM 2 Proof.",
          "confidence_score": 1.0,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions 'BSD 100' which is a well-known dataset in image processing. No other specific datasets are mentioned.",
          "citing_paper_doi": "10.1109/TIP.2019.2958309",
          "cited_paper_doi": "10.1109/DCC.2013.95",
          "citing_paper_url": "https://www.semanticscholar.org/paper/ce8aa84e1e7930b071f242017fa17cae5727e1e9",
          "cited_paper_url": "https://www.semanticscholar.org/paper/f8bad37efd8f63c7968cc51ef44273734bbcd976",
          "citing_paper_year": 2018,
          "cited_paper_year": 2013
        },
        {
          "dataset_name": "BSD100",
          "dataset_description": "Used to evaluate image restoration methods, specifically focusing on denoising and deblocking performance on a set of high-quality images.",
          "citing_paper_id": "209389923",
          "cited_paper_id": 11580231,
          "context_text": "The maximum iteration number is [46], PC-LRM [74], ANCE [75], D IC TV [53], BM3D [30], WNNM [5], CONCOLOR [48], SSR-QC [49], LER A G [76] AND RRC ON THE BSD 100 [70] A PPENDIX A P ROOF OF THE T HEOREM 2 Proof.",
          "confidence_score": 1.0,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions 'BSD 100' which is a well-known dataset in image processing. No other specific datasets are mentioned.",
          "citing_paper_doi": "10.1109/TIP.2019.2958309",
          "cited_paper_doi": "10.1109/TIP.2016.2627807",
          "citing_paper_url": "https://www.semanticscholar.org/paper/ce8aa84e1e7930b071f242017fa17cae5727e1e9",
          "cited_paper_url": "https://www.semanticscholar.org/paper/015017ba46c513b5432879464f5e89d18f36bc2b",
          "citing_paper_year": 2018,
          "cited_paper_year": 2016
        },
        {
          "dataset_name": "BSD100",
          "dataset_description": "Used to evaluate image restoration methods, specifically focusing on denoising and deblocking performance on a set of high-quality images. | Used to verify the proposed algorithm for restoring JPEG-compressed images, focusing on image quality and restoration accuracy. | Used to evaluate the RRC method on 200 test images, focusing on contour detection and image segmentation performance.",
          "citing_paper_id": "209389923",
          "cited_paper_id": 206764694,
          "context_text": "The maximum iteration number is [46], PC-LRM [74], ANCE [75], D IC TV [53], BM3D [30], WNNM [5], CONCOLOR [48], SSR-QC [49], LER A G [76] AND RRC ON THE BSD 100 [70] A PPENDIX A P ROOF OF THE T HEOREM 2 Proof.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions 'BSD 100' which is a well-known dataset in image processing. No other specific datasets are mentioned.",
          "citing_paper_doi": "10.1109/TIP.2019.2958309",
          "cited_paper_doi": "10.1109/TPAMI.2010.161",
          "citing_paper_url": "https://www.semanticscholar.org/paper/ce8aa84e1e7930b071f242017fa17cae5727e1e9",
          "cited_paper_url": "https://www.semanticscholar.org/paper/0e5a262bf59b68ba8a7a1103d16fa33a9f5ffc28",
          "citing_paper_year": 2018,
          "cited_paper_year": 2011
        },
        {
          "dataset_name": "BSD100",
          "dataset_description": "Used to evaluate image restoration methods, focusing on average PSNR and SSIM metrics across various restoration techniques.",
          "citing_paper_id": "209389923",
          "cited_paper_id": 1475121,
          "context_text": "AVERAGE PSNR (dB) (TOP ENTRY IN EACH CELL) AND SSIM (BOTTOM ENTRY) BY JPEG, SA-DCT [46], PC-LRM [74], ANCE [75], DICTV [53], BM3D [30], WNNM [5], CONCOLOR [48], SSR-QC [49], LERAG [76] AND RRC ON THE BSD 100 [70], CLASSIC5 AND LIVE1 [77] DATASET",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The citation mentions specific datasets used for evaluating image restoration methods. These datasets are clearly identified and are relevant to the research topic.",
          "citing_paper_doi": "10.1109/TIP.2019.2958309",
          "cited_paper_doi": "10.1109/TIP.2007.901238",
          "citing_paper_url": "https://www.semanticscholar.org/paper/ce8aa84e1e7930b071f242017fa17cae5727e1e9",
          "cited_paper_url": "https://www.semanticscholar.org/paper/51d267b782e7caf2b6bc7240b1a5f48044ffe115",
          "citing_paper_year": 2018,
          "cited_paper_year": 2007
        },
        {
          "dataset_name": "BSD100",
          "dataset_description": "Used to verify the proposed algorithm for restoring JPEG-compressed images, focusing on image quality and restoration accuracy.",
          "citing_paper_id": "209389923",
          "cited_paper_id": 9247572,
          "context_text": "In this subsection, we verify the proposed algorithm to restore JPEG-compressed images on three widely used dataset, including BSD 100 [70] 2 , Classic5 and LIVE1 [77].",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions three specific datasets used for verifying the proposed algorithm for restoring JPEG-compressed images. These datasets are BSD 100, Classic5, and LIVE1.",
          "citing_paper_doi": "10.1109/TIP.2019.2958309",
          "cited_paper_doi": "10.1109/TIP.2006.881959",
          "citing_paper_url": "https://www.semanticscholar.org/paper/ce8aa84e1e7930b071f242017fa17cae5727e1e9",
          "cited_paper_url": "https://www.semanticscholar.org/paper/1c79a3b52bf92fbc01de64057a264de9e94a1025",
          "citing_paper_year": 2018,
          "cited_paper_year": 2006
        }
      ]
    },
    {
      "cited_paper_id": "52008443",
      "citation_count": 0,
      "total_dataset_mentions": 12,
      "unique_datasets": [
        "Rain100H"
      ],
      "dataset_details": [
        {
          "dataset_name": "Rain100H",
          "dataset_description": "Used to assess rain removal effectiveness on low-resolution images, emphasizing PSNR and SSIM scores. | Utilized for comprehensive testing of rain removal algorithms, providing a large set of images for evaluation. | Employed to validate rain removal techniques on a moderate-sized dataset, measuring PSNR and SSIM values. | Used to evaluate rain removal performance on high-resolution images, focusing on PSNR and SSIM metrics.",
          "citing_paper_id": "244346144",
          "cited_paper_id": 17115407,
          "context_text": "Test100 [97] Rain100H [86] Rain100L [86] Test2800 [22] Test1200 [96] Average Method PSNR ↑ SSIM ↑ PSNR ↑ SSIM ↑ PSNR ↑ SSIM ↑ PSNR ↑ SSIM ↑ PSNR ↑ SSIM ↑ PSNR ↑ SSIM ↑",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions several datasets that are used for testing and evaluating image restoration methods, particularly for rain removal.",
          "citing_paper_doi": "10.1109/CVPR52688.2022.00564",
          "cited_paper_doi": "10.1109/CVPR.2017.186",
          "citing_paper_url": "https://www.semanticscholar.org/paper/1e88d5afe19aea324d33541f60a90b7036894c32",
          "cited_paper_url": "https://www.semanticscholar.org/paper/29ad37a306732676a41f25e05960ff0adbca631e",
          "citing_paper_year": 2021,
          "cited_paper_year": 2017
        },
        {
          "dataset_name": "Rain100H",
          "dataset_description": "Used to train and evaluate rain removal algorithms, focusing on heavy rain streaks in images to improve restoration quality. | Used to evaluate heavy rain streak removal techniques, focusing on the effectiveness of the proposed method in enhancing image quality. | Used to evaluate the performance of different model structures for rain removal, specifically measuring PSNR and SSIM values.",
          "citing_paper_id": "232222608",
          "cited_paper_id": 15443600,
          "context_text": "[14] build a dataset of heavy rain streaks, named as Rain100H.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions the creation of a dataset named Rain100H, which is relevant to the topic of image restoration, particularly for rain removal.",
          "citing_paper_doi": "10.1109/TIP.2021.3108019",
          "cited_paper_doi": "10.1109/CVPR.2017.183",
          "citing_paper_url": "https://www.semanticscholar.org/paper/0cd840ae923f2bc837d5744d0f0eb074e9f359e3",
          "cited_paper_url": "https://www.semanticscholar.org/paper/6de27f256e97c014a95706c72af7fa651e8fa34a",
          "citing_paper_year": 2021,
          "cited_paper_year": 2016
        },
        {
          "dataset_name": "Rain100H",
          "dataset_description": "Used to evaluate deraining methods, focusing on low-resolution images with synthetic rain. Used for training and evaluation of image restoration models, specifically for deraining tasks. | Used to test rain removal methods, focusing on high-resolution images, evaluating performance on various restoration tasks in image restoration. | Used to evaluate image restoration performance, specifically for rain removal, focusing on low-resolution images with synthetic rain. | Used to evaluate the model's performance on image restoration, specifically addressing the restoration of images with raindrops. | Used to evaluate deraining methods, containing a large set of test images with synthetic rain. Used for comprehensive evaluation of image restoration models, specifically for deraining tasks. | Used to evaluate image restoration performance, specifically for rain removal, focusing on a large set of 2800 images. | Used to evaluate image restoration performance, specifically for rain removal, focusing on images generated by an adversarial generative network. | Used to train the model on removing rain streaks from images, focusing on the degradation patterns and detail preservation. | Used to evaluate image restoration performance, specifically for rain removal, focusing on high-resolution images with synthetic rain. | Used to evaluate image restoration performance, specifically for rain removal, focusing on a large set of 1200 images. | Used to evaluate image restoration methods, focusing on a moderate set of test images, assessing performance across multiple metrics. | Used to evaluate image restoration methods, focusing on a large set of test images, assessing performance across multiple metrics. | Used to evaluate deraining methods, containing a moderate set of test images with synthetic rain. Used for intermediate evaluation of image restoration models, specifically for deraining tasks. | Used to evaluate image restoration performance, specifically for rain removal, focusing on a diverse set of 100 images. | Used to test rain removal methods, focusing on low-resolution images, evaluating performance on various restoration tasks. | Used to evaluate deraining methods, focusing on high-resolution images with synthetic rain. Used for training and evaluation of image restoration models, specifically for deraining tasks.",
          "citing_paper_id": "237213661",
          "cited_paper_id": 17115407,
          "context_text": "Test100 [25] Rain100H [22] Rain100L [22] Test2800 [4] Test1200 [24] Average\nMethods PSNR ↑ SSIM ↑ PSNR ↑ SSIM ↑ PSNR ↑ SSIM ↑ PSNR ↑ SSIM ↑ PSNR ↑ SSIM ↑ PSNR ↑ SSIM ↑ MSPFN [9] 27.50 0.876 28.66 0.860 32.40 0.933 32.82 0.930 32.39 0.916 30.75 0.903 SPAIR (Single) 30.35 0.909 30.95 0.892 36.93 0.969 33.34 0.936 33.04 0.922 32.91 0.926 OWAN [17] (Joint) 23.85 0.810 24.46 0.724 28.54 0.878 30.40 0.891 30.09 0.872 27.47 0.835 SPAIR (Joint) 30.33 0.909 30.81 0.892 36.39 0.964 33.34 0.936 33.10 0.925 32.79 0.925\nalong rain direction.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions several datasets that are used for evaluating image restoration methods, particularly for deraining. These datasets are specific and have clear identifiers.",
          "citing_paper_doi": "10.1109/ICCV48922.2021.00231",
          "cited_paper_doi": "10.1109/CVPR.2017.186",
          "citing_paper_url": "https://www.semanticscholar.org/paper/992e98e919ccbc960ea5cfa836cf718aeae895b6",
          "cited_paper_url": "https://www.semanticscholar.org/paper/29ad37a306732676a41f25e05960ff0adbca631e",
          "citing_paper_year": 2021,
          "cited_paper_year": 2017
        },
        {
          "dataset_name": "Rain100H",
          "dataset_description": "Used to evaluate deraining methods, focusing on low-resolution images with synthetic rain. Used for training and evaluation of image restoration models, specifically for deraining tasks. | Used to evaluate deraining methods, containing a moderate set of test images with synthetic rain. Used for intermediate evaluation of image restoration models, specifically for deraining tasks. | Used to evaluate deraining methods, containing a large set of test images with synthetic rain. Used for comprehensive evaluation of image restoration models, specifically for deraining tasks. | Used to evaluate deraining methods, focusing on high-resolution images with synthetic rain. Used for training and evaluation of image restoration models, specifically for deraining tasks.",
          "citing_paper_id": "237213661",
          "cited_paper_id": 54440425,
          "context_text": "Test100 [25] Rain100H [22] Rain100L [22] Test2800 [4] Test1200 [24] Average\nMethods PSNR ↑ SSIM ↑ PSNR ↑ SSIM ↑ PSNR ↑ SSIM ↑ PSNR ↑ SSIM ↑ PSNR ↑ SSIM ↑ PSNR ↑ SSIM ↑ MSPFN [9] 27.50 0.876 28.66 0.860 32.40 0.933 32.82 0.930 32.39 0.916 30.75 0.903 SPAIR (Single) 30.35 0.909 30.95 0.892 36.93 0.969 33.34 0.936 33.04 0.922 32.91 0.926 OWAN [17] (Joint) 23.85 0.810 24.46 0.724 28.54 0.878 30.40 0.891 30.09 0.872 27.47 0.835 SPAIR (Joint) 30.33 0.909 30.81 0.892 36.39 0.964 33.34 0.936 33.10 0.925 32.79 0.925\nalong rain direction.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions several datasets that are used for evaluating image restoration methods, particularly for deraining. These datasets are specific and have clear identifiers.",
          "citing_paper_doi": "10.1109/ICCV48922.2021.00231",
          "cited_paper_doi": "10.1109/CVPR.2019.00925",
          "citing_paper_url": "https://www.semanticscholar.org/paper/992e98e919ccbc960ea5cfa836cf718aeae895b6",
          "cited_paper_url": "https://www.semanticscholar.org/paper/c4e6b18b5573329a5b62f297f6ab9565f841e11b",
          "citing_paper_year": 2021,
          "cited_paper_year": 2018
        },
        {
          "dataset_name": "Rain100H",
          "dataset_description": "Used to evaluate deraining methods, focusing on low-resolution images with synthetic rain. Used for training and evaluation of image restoration models, specifically for deraining tasks. | Used for training SPAIR in deraining tasks, specifically evaluating performance in Table 1 of the main paper. | Used to evaluate deraining methods, containing a large set of test images with synthetic rain. Used for comprehensive evaluation of image restoration models, specifically for deraining tasks. | Used for training SPAIR in image restoration, specifically assessing performance in Table 3 of the main paper. | Used to evaluate deraining methods, containing a moderate set of test images with synthetic rain. Used for intermediate evaluation of image restoration models, specifically for deraining tasks. | Used to evaluate deraining methods, focusing on high-resolution images with synthetic rain. Used for training and evaluation of image restoration models, specifically for deraining tasks.",
          "citing_paper_id": "237213661",
          "cited_paper_id": 214623217,
          "context_text": "Test100 [25] Rain100H [22] Rain100L [22] Test2800 [4] Test1200 [24] Average\nMethods PSNR ↑ SSIM ↑ PSNR ↑ SSIM ↑ PSNR ↑ SSIM ↑ PSNR ↑ SSIM ↑ PSNR ↑ SSIM ↑ PSNR ↑ SSIM ↑ MSPFN [9] 27.50 0.876 28.66 0.860 32.40 0.933 32.82 0.930 32.39 0.916 30.75 0.903 SPAIR (Single) 30.35 0.909 30.95 0.892 36.93 0.969 33.34 0.936 33.04 0.922 32.91 0.926 OWAN [17] (Joint) 23.85 0.810 24.46 0.724 28.54 0.878 30.40 0.891 30.09 0.872 27.47 0.835 SPAIR (Joint) 30.33 0.909 30.81 0.892 36.39 0.964 33.34 0.936 33.10 0.925 32.79 0.925\nalong rain direction.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions several datasets that are used for evaluating image restoration methods, particularly for deraining. These datasets are specific and have clear identifiers.",
          "citing_paper_doi": "10.1109/ICCV48922.2021.00231",
          "cited_paper_doi": "10.1109/CVPR42600.2020.00837",
          "citing_paper_url": "https://www.semanticscholar.org/paper/992e98e919ccbc960ea5cfa836cf718aeae895b6",
          "cited_paper_url": "https://www.semanticscholar.org/paper/1234b5451347ce6fe3bae9c34e67edbacaf73b52",
          "citing_paper_year": 2021,
          "cited_paper_year": 2020
        },
        {
          "dataset_name": "Rain100H",
          "dataset_description": "Used to test rain removal algorithms, focusing on low-resolution images with synthetic rain. The dataset evaluates the performance of image restoration methods on less complex images. | Used to evaluate the performance of image restoration models on a moderate-sized set of images. The dataset provides a balanced assessment of the methods' effectiveness. | Used to evaluate real image denoising performance of various models, including DGUNet, VDNet, GDANet, AINDNet, MIRNet, DeamNet, and MPRNet, focusing on parameter efficiency. | Used to evaluate the generalization of image restoration models on a large set of diverse images. The dataset assesses the robustness and adaptability of the methods. | Used to test rain removal algorithms, focusing on high-resolution images with synthetic rain. The dataset evaluates the performance of image restoration methods under challenging conditions.",
          "citing_paper_id": "248426764",
          "cited_paper_id": 231802205,
          "context_text": "Method Test100 [79] Rain100H [71] Rain100L [71] Test2800 [79] Test1200 [79] Average PSNR↑ SSIM↑ PSNR↑ SSIM↑ PSNR↑ SSIM↑ PSNR↑ SSIM↑ PSNR↑ SSIM↑ PSNR↑ SSIM↑",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions several datasets that are used for testing image restoration methods. These datasets are specific and have clear identifiers.",
          "citing_paper_doi": "10.1109/CVPR52688.2022.01688",
          "cited_paper_doi": "10.1109/CVPR46437.2021.01458",
          "citing_paper_url": "https://www.semanticscholar.org/paper/22755044094d1eff9d2b61bf2861649446d31bf9",
          "cited_paper_url": "https://www.semanticscholar.org/paper/92d50602db5746f03b91562e2cc8a98bec584e9b",
          "citing_paper_year": 2022,
          "cited_paper_year": 2021
        },
        {
          "dataset_name": "Rain100H",
          "dataset_description": "Used to evaluate image de-raining methods, focusing on low-resolution images with synthetic rain. The dataset assesses the performance of models in removing rain streaks while preserving image details. | Used for evaluating the model's performance on light rain conditions, focusing on image quality and restoration accuracy. | Used for evaluating de-raining models, containing a smaller set of synthetic rainy images. | Used to evaluate image de-raining methods, focusing on high-resolution images with synthetic rain. The dataset assesses the performance of models in removing rain streaks while preserving image details. | Used for training and evaluating deblurring models, providing a large set of real-world blurry images. | Used for evaluating de-raining models, containing 100 low-intensity rainy images. | Used for evaluating de-raining models, containing 100 high-intensity rainy images. | Used for evaluating the model's performance on a diverse set of test images, focusing on generalization and robustness. | Used for evaluating the model's performance on a large-scale test set, focusing on scalability and consistency. | Used for training and evaluating de-raining models, providing a large set of synthetic rainy images. | Used for evaluating de-raining models, consisting of 100 test images with varying rain intensities. | Used for training and evaluating deblurring models, offering a diverse set of real-world blurry images. | Used for training and evaluating de-raining models, offering a moderate-sized set of synthetic rainy images. | Used for evaluating the model's performance on heavy rain conditions, focusing on image quality and restoration accuracy.",
          "citing_paper_id": "272680196",
          "cited_paper_id": 3406592,
          "context_text": "Test100 [52] Test1200 [53] Rain100H [45] Rain100L [45] Average Methods PSNR ↑ SSIM ↑ PSNR ↑ SSIM ↑ PSNR ↑ SSIM ↑ PSNR ↑ SSIM ↑ PSNR SSIM",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions specific datasets used for image de-raining experiments, which are relevant to the topic of image restoration.",
          "citing_paper_doi": null,
          "cited_paper_doi": "10.1109/CVPR.2018.00079",
          "citing_paper_url": "https://www.semanticscholar.org/paper/e9ef4cfc510d53d0be69835a34a9195027e4cc51",
          "cited_paper_url": "https://www.semanticscholar.org/paper/34aaa735409b666b8dc678c23acb600b1d87913b",
          "citing_paper_year": 2023,
          "cited_paper_year": 2018
        },
        {
          "dataset_name": "Rain100H",
          "dataset_description": "Used to demonstrate stronger generalization capability compared to Stripformer on all metrics. | Used to evaluate the performance of the proposed SFNet model in image restoration, specifically comparing PSNR scores against the MLP model MAXIM-2S. | Used for deraining comparisons, containing 1200 real-world rainy images to evaluate model scalability. | Used for deraining comparisons, focusing on low-quality synthetic rain images to assess model robustness. | Used to demonstrate stronger generalization capability, containing diverse real-world rainy images to test model adaptability. | Used for deraining comparisons, evaluating the generalization capability of the proposed method against previous approaches. | Used for deraining comparisons, containing 2800 real-world rainy images to assess model consistency. | Used for deraining comparisons, focusing on high-quality synthetic rain images to evaluate model performance. | Used for deraining comparisons, containing 100 real-world rainy images to test model generalization.",
          "citing_paper_id": "259298517",
          "cited_paper_id": 11922819,
          "context_text": "Table 7: Deraining comparisons with previous methods on five deraining datasets: Rain100H (Yang et al., 2017), Rain100L (Yang et al., 2017), Test100 (Zhang et al., 2019a), Test1200 (Zhang & Patel, 2018) and Test2800 (Fu et al., 2017). demonstrates stronger generalization capability to HIDE dataset…",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions specific datasets used for deraining comparisons, which are relevant to the topic of image restoration.",
          "citing_paper_doi": null,
          "cited_paper_doi": "10.1109/TCSVT.2019.2920407",
          "citing_paper_url": "https://www.semanticscholar.org/paper/6af728164e4424218967fd356b3448f5037acd88",
          "cited_paper_url": "https://www.semanticscholar.org/paper/920f0c070701caabf023c600f3e310f1906ca818",
          "citing_paper_year": 2023,
          "cited_paper_year": 2017
        },
        {
          "dataset_name": "Rain100H",
          "dataset_description": "Used to demonstrate stronger generalization capability compared to Stripformer on all metrics. | Used to evaluate the performance of the proposed SFNet model in image restoration, specifically comparing PSNR scores against the MLP model MAXIM-2S. | Used for deraining comparisons, containing 1200 real-world rainy images to evaluate model scalability. | Used for deraining comparisons, focusing on low-quality synthetic rain images to assess model robustness. | Used to demonstrate stronger generalization capability, containing diverse real-world rainy images to test model adaptability. | Used for deraining comparisons, evaluating the generalization capability of the proposed method against previous approaches. | Used for deraining comparisons, containing 2800 real-world rainy images to assess model consistency. | Used for deraining comparisons, focusing on high-quality synthetic rain images to evaluate model performance. | Used for deraining comparisons, containing 100 real-world rainy images to test model generalization.",
          "citing_paper_id": "259298517",
          "cited_paper_id": 102351795,
          "context_text": "Table 7: Deraining comparisons with previous methods on five deraining datasets: Rain100H (Yang et al., 2017), Rain100L (Yang et al., 2017), Test100 (Zhang et al., 2019a), Test1200 (Zhang & Patel, 2018) and Test2800 (Fu et al., 2017). demonstrates stronger generalization capability to HIDE dataset…",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions specific datasets used for deraining comparisons, which are relevant to the topic of image restoration.",
          "citing_paper_doi": null,
          "cited_paper_doi": "10.1109/CVPR.2019.00613",
          "citing_paper_url": "https://www.semanticscholar.org/paper/6af728164e4424218967fd356b3448f5037acd88",
          "cited_paper_url": "https://www.semanticscholar.org/paper/a5c1156c2c8185df4581cf139df05aab66b3bb22",
          "citing_paper_year": 2023,
          "cited_paper_year": 2019
        },
        {
          "dataset_name": "Rain100H",
          "dataset_description": "Used to demonstrate stronger generalization capability compared to Stripformer on all metrics. | Used for deraining comparisons, evaluating the generalization capability of the proposed method against previous approaches.",
          "citing_paper_id": "259298517",
          "cited_paper_id": 202781591,
          "context_text": "Table 7: Deraining comparisons with previous methods on five deraining datasets: Rain100H (Yang et al., 2017), Rain100L (Yang et al., 2017), Test100 (Zhang et al., 2019a), Test1200 (Zhang & Patel, 2018) and Test2800 (Fu et al., 2017). demonstrates stronger generalization capability to HIDE dataset than Stripformer on all metrics.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions specific datasets used for deraining comparisons, which are relevant to the topic of image restoration.",
          "citing_paper_doi": null,
          "cited_paper_doi": "10.1109/ICIP.2019.8803391",
          "citing_paper_url": "https://www.semanticscholar.org/paper/6af728164e4424218967fd356b3448f5037acd88",
          "cited_paper_url": "https://www.semanticscholar.org/paper/27364a5aea5e8202584f4f378a22d9f3bac27568",
          "citing_paper_year": 2023,
          "cited_paper_year": 2019
        },
        {
          "dataset_name": "Rain100H",
          "dataset_description": "Used to train and evaluate raindrop removal, focusing on realistic raindrop effects in outdoor scenes. | Used to train and evaluate low-light enhancement, focusing on underexposed images. | Used to test dehazing algorithms, providing a large set of images with diverse atmospheric conditions. | Used to train and evaluate rainstreak removal, focusing on synthetic rain streaks in high-resolution images. | Used to train and evaluate haze removal, focusing on diverse real-world hazy images. | Used to test image restoration methods, focusing on low-light enhancement with added noise or JPEG artifacts. | Used to test image restoration methods, focusing on dehazing from images with added noise or JPEG artifacts. | Used to assess rain removal effectiveness, specifically on real-world images with varying rain intensities. | Used to test image restoration methods, focusing on rain removal from images with added noise or JPEG artifacts. | Used to evaluate performance on rain removal, focusing on high-resolution images with synthetic rain. | Used to evaluate low-light enhancement, featuring images with significant illumination variations.",
          "citing_paper_id": "276960871",
          "cited_paper_id": 52008443,
          "context_text": "Following [13], we use 100 images from Rain100H [75], 58 images from RainDrop [49], 1000 images from RESIDE-6k [50], and 15 images from LOL [68] to evaluate the performance on last four distortions, respectively.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions specific image datasets used for evaluating performance on image restoration tasks, which align with the research topic of all-in-one image restoration.",
          "citing_paper_doi": "10.48550/arXiv.2503.10120",
          "cited_paper_doi": null,
          "citing_paper_url": "https://www.semanticscholar.org/paper/8d5b315df8c45238becce7237c1512145d4bb36d",
          "cited_paper_url": "https://www.semanticscholar.org/paper/c7798bbbcd2e85593a2b2d37b92dabac63420cd8",
          "citing_paper_year": 2025,
          "cited_paper_year": 2018
        },
        {
          "dataset_name": "Rain100H",
          "dataset_description": "Used to train and evaluate raindrop removal, focusing on realistic raindrop effects in outdoor scenes. | Used to train and evaluate low-light enhancement, focusing on underexposed images. | Used to test dehazing algorithms, providing a large set of images with diverse atmospheric conditions. | Used to train and evaluate rainstreak removal, focusing on synthetic rain streaks in high-resolution images. | Used to train and evaluate haze removal, focusing on diverse real-world hazy images. | Used to test image restoration methods, focusing on low-light enhancement with added noise or JPEG artifacts. | Used to test image restoration methods, focusing on dehazing from images with added noise or JPEG artifacts. | Used to assess rain removal effectiveness, specifically on real-world images with varying rain intensities. | Used to test image restoration methods, focusing on rain removal from images with added noise or JPEG artifacts. | Used to evaluate performance on rain removal, focusing on high-resolution images with synthetic rain. | Used to evaluate low-light enhancement, featuring images with significant illumination variations.",
          "citing_paper_id": "276960871",
          "cited_paper_id": 208138077,
          "context_text": "Following [13], we use 100 images from Rain100H [75], 58 images from RainDrop [49], 1000 images from RESIDE-6k [50], and 15 images from LOL [68] to evaluate the performance on last four distortions, respectively.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions specific image datasets used for evaluating performance on image restoration tasks, which align with the research topic of all-in-one image restoration.",
          "citing_paper_doi": "10.48550/arXiv.2503.10120",
          "cited_paper_doi": "10.1609/AAAI.V34I07.6865",
          "citing_paper_url": "https://www.semanticscholar.org/paper/8d5b315df8c45238becce7237c1512145d4bb36d",
          "cited_paper_url": "https://www.semanticscholar.org/paper/22418b7b7d5d6d18b81f232f60c22a15d1c8a38a",
          "citing_paper_year": 2025,
          "cited_paper_year": 2019
        }
      ]
    },
    {
      "cited_paper_id": "39760169",
      "citation_count": 0,
      "total_dataset_mentions": 12,
      "unique_datasets": [
        "Rain14000"
      ],
      "dataset_details": [
        {
          "dataset_name": "Rain14000",
          "dataset_description": "Used to validate deraining techniques on a large-scale dataset, assessing performance across diverse rain conditions and image complexities. | Used for evaluating the deraining model, providing a larger set of 1200 test images to validate performance. | Used to benchmark deraining methods on an extensive dataset, ensuring comprehensive evaluation of model accuracy and efficiency. | Used for evaluating the deraining model, providing a comprehensive set of 2800 test images to validate performance. | Used to test deraining algorithms on a moderate-sized dataset, evaluating the generalization and robustness of the models. | Used for evaluating the deraining model, focusing on high-quality images to assess performance. | Used to evaluate deraining performance on high-resolution images, focusing on detailed rain removal and image quality enhancement. | Used for training the deraining model, providing a large set of rainy images to improve generalization. | Used to assess deraining effectiveness on low-resolution images, emphasizing the removal of light rain streaks and noise reduction. | Used for evaluating the deraining model, providing a set of 100 test images to validate performance. | Used for evaluating the deraining model, focusing on low-quality images to assess robustness.",
          "citing_paper_id": "270155352",
          "cited_paper_id": 15443600,
          "context_text": "Image deraining: We used Rain14000 dataset [71] for training, while evaluated on Rain100H [73], Rain100L [73], Test100 [72], Test1200 [74], and Test2800 [71].",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions specific datasets used for training and evaluation in the image deraining task. These datasets are clearly identified and used in the research.",
          "citing_paper_doi": "10.1109/TMM.2024.3407656",
          "cited_paper_doi": "10.1109/CVPR.2017.183",
          "citing_paper_url": "https://www.semanticscholar.org/paper/3a80830bf3e31c855afaff17a5b590bc052839c6",
          "cited_paper_url": "https://www.semanticscholar.org/paper/6de27f256e97c014a95706c72af7fa651e8fa34a",
          "citing_paper_year": 2024,
          "cited_paper_year": 2016
        },
        {
          "dataset_name": "Rain14000",
          "dataset_description": "Used for testing deraining models, featuring low-resolution rainy images and their clean versions. | Used for training and testing image enhancement models, featuring retouched images and their original versions. | Used to evaluate the performance of rain removal methods on a smaller, curated set of images, emphasizing accuracy and visual quality. | Used to evaluate the method's performance in removing rain from low-resolution images, focusing on PSNR and SSIM metrics. | Used for training and testing dehazing models, providing outdoor scenes with haze and their clear versions. | Used to test the method's ability to remove rain from a smaller set of images, evaluating PSNR and SSIM metrics. | Used to assess the method's effectiveness in removing rain from high-resolution images, emphasizing PSNR and SSIM metrics. | Used to further evaluate the scalability and performance of the rain removal method on an extensive dataset of 2800 images. | Used to assess rain removal effectiveness on low-resolution images, emphasizing noise reduction and clarity enhancement. | Used for training and testing deraining models, providing a large set of rainy images with corresponding clean images. | Used to test rain removal methods, focusing on high-rain density images to assess effectiveness and detail preservation. | Used for testing deraining models, providing a set of rainy images and their clean counterparts. | Used to evaluate the scalability and generalization of rain removal methods on a larger, diverse set of images. | Used for training deraining models, containing synthetic rainy images and their clean counterparts. | Used to evaluate the method's performance on a larger set of images, focusing on PSNR and SSIM metrics. | Used to test the generalization of the rain removal model on a diverse set of 100 images, evaluating overall performance. | Used for training deraining models, containing a small set of rainy images and their clean versions. | Used to test rain removal methods, focusing on low-rain density images to evaluate performance and robustness. | Used to evaluate rain removal performance on high-resolution images, focusing on detail preservation and artifact reduction. | Used for testing deraining models, featuring high-resolution rainy images and their clean versions. | Used for training and testing dehazing models, providing indoor scenes with haze and their clear versions. | Used to assess the method's effectiveness in outdoor settings, emphasizing PSNR and SSIM metrics. | Used for training and testing low-light enhancement models, providing low-light images and their well-lit counterparts. | Used to test the robustness and consistency of rain removal methods across a very large dataset, ensuring wide applicability. | Used for training and testing deraining models, including images with raindrops and their clean versions. | Used to validate the robustness of the rain removal algorithm on a larger dataset of 1200 images, assessing consistency and reliability. | Used to evaluate the method's performance in indoor settings, focusing on PSNR and SSIM metrics. | Used for training deraining models, consisting of synthetic rainy images and their clean versions. | Used to test the method's robustness on a very large set of images, assessing PSNR and SSIM metrics.",
          "citing_paper_id": "245837508",
          "cited_paper_id": 17115407,
          "context_text": "Deraining Rain14000 [26] 11200 2800 Test2800 Rain1800 [102] 1800 0 Rain800 [112] 700 98 Test100 Rain100H [102] 0 100 Rain100H Rain100L [102] 0 100 Rain100L Rain1200 [111] 0 1200 Test1200 Rain12 [50] 12 0 Raindrop [71] 861 58 Raindrop-A Raindrop [71] 0 239 Raindrop-B Dehazing RESIDE-ITS [45] 13990 500 SOTS-Indoor RESIDE-OTS [45] 313950 500 SOTS-Outdoor Enhancement MIT-Adobe FiveK [7] 4500 500 FiveK (Retouching) LOL [96] 485 15 LOL",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context lists several datasets used for training and testing various image restoration tasks, including deraining, dehazing, and low-light enhancement.",
          "citing_paper_doi": "10.1109/CVPR52688.2022.00568",
          "cited_paper_doi": "10.1109/CVPR.2017.186",
          "citing_paper_url": "https://www.semanticscholar.org/paper/fbf68eb0cf8237cef59ebdb301569c79b9676ff9",
          "cited_paper_url": "https://www.semanticscholar.org/paper/29ad37a306732676a41f25e05960ff0adbca631e",
          "citing_paper_year": 2022,
          "cited_paper_year": 2017
        },
        {
          "dataset_name": "Rain14000",
          "dataset_description": "Used for testing deraining models, featuring low-resolution rainy images and their clean versions. | Used for training and testing image enhancement models, featuring retouched images and their original versions. | Used to evaluate single-image dehazing methods, specifically measuring performance on indoor and outdoor sets using PSNR. | Used to evaluate single-image dehazing methods, focusing on indoor scenes to assess performance under controlled lighting conditions. | Used for training and testing dehazing models, providing outdoor scenes with haze and their clear versions. | Used for dehazing, evaluating the effectiveness of dehazing algorithms on synthetic and real-world hazy images. | Evaluated for image enhancement, focusing on professional-level adjustments to improve image quality. | Used for training and testing deraining models, providing a large set of rainy images with corresponding clean images. | Consists of 313,950 hazy images synthesized from 8,970 haze-free outdoor scenes, used for training and evaluating dehazing algorithms. | Used to evaluate the performance of the proposed model on indoor and outdoor dehazing tasks, specifically measuring PSNR improvements over the previous best model. | Evaluated for low-light enhancement, assessing the performance of algorithms in improving visibility in dark conditions. | Used for testing deraining models, providing a set of rainy images and their clean counterparts. | Used for training a dehazing model, focusing on outdoor scenes to enhance image restoration and visibility. | Used to train and evaluate single-image dehazing models, specifically with 500 indoor images for ITS-trained and 500 outdoor images for OTS-trained models. | Used for training deraining models, containing synthetic rainy images and their clean counterparts. | Used to provide dehazing comparisons in both indoor and outdoor settings, evaluating the effectiveness of image restoration techniques. | Used for training deraining models, containing a small set of rainy images and their clean versions. | Used to benchmark single-image dehazing methods, evaluating performance on a variety of hazy images with known ground truth. | Used for testing deraining models, featuring high-resolution rainy images and their clean versions. | Used for training and testing dehazing models, providing indoor scenes with haze and their clear versions. | Contains 13,990 hazy images generated from 1399 clean indoor scenes, used for training dehazing models. | Used for training and testing low-light enhancement models, providing low-light images and their well-lit counterparts. | Used for training and testing deraining models, including images with raindrops and their clean versions. | Used to benchmark single-image dehazing methods, containing both indoor and outdoor hazy images synthesized from clean scenes. | Used to evaluate and compare the performance of various image dehazing methods, including GCANet, GridDehaze, DuRN, MSBDN, FFA-Net, and MAXIM-2S, focusing on visual quality and effectiveness. | Used to evaluate single-image dehazing methods, focusing on outdoor scenes to assess performance under varying environmental conditions. | Used for training deraining models, consisting of synthetic rainy images and their clean versions. | Used for training a dehazing model, focusing on indoor scenes to improve image clarity and visual quality.",
          "citing_paper_id": "245837508",
          "cited_paper_id": 39760169,
          "context_text": "Deraining Rain14000 [26] 11200 2800 Test2800 Rain1800 [102] 1800 0 Rain800 [112] 700 98 Test100 Rain100H [102] 0 100 Rain100H Rain100L [102] 0 100 Rain100L Rain1200 [111] 0 1200 Test1200 Rain12 [50] 12 0 Raindrop [71] 861 58 Raindrop-A Raindrop [71] 0 239 Raindrop-B Dehazing RESIDE-ITS [45] 13990 500 SOTS-Indoor RESIDE-OTS [45] 313950 500 SOTS-Outdoor Enhancement MIT-Adobe FiveK [7] 4500 500 FiveK (Retouching) LOL [96] 485 15 LOL",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context lists several datasets used for training and testing various image restoration tasks, including deraining, dehazing, and low-light enhancement.",
          "citing_paper_doi": "10.1109/CVPR52688.2022.00568",
          "cited_paper_doi": "10.1109/TIP.2018.2867951",
          "citing_paper_url": "https://www.semanticscholar.org/paper/fbf68eb0cf8237cef59ebdb301569c79b9676ff9",
          "cited_paper_url": "https://www.semanticscholar.org/paper/abf75b81b96ff2cd262086675deba55f907108db",
          "citing_paper_year": 2022,
          "cited_paper_year": 2017
        },
        {
          "dataset_name": "Rain14000",
          "dataset_description": "Used for low-light image enhancement, providing 500 pairs of images to train and evaluate models on improving visibility in dark conditions. | Used for testing deraining models, featuring low-resolution rainy images and their clean versions. | Used for training and testing image enhancement models, featuring retouched images and their original versions. | Used to assess low-light image enhancement methods, specifically improving brightness and detail in dark images. | Used to evaluate the model's performance in low-light image enhancement, specifically measuring SSIM values. | Used for training and testing dehazing models, providing outdoor scenes with haze and their clear versions. | Used to compare low-light image enhancement methods, focusing on improving visibility and detail in dark images. | Used for evaluating image restoration methods, focusing on color and tone adjustments in professional photography. | Used for dehazing, evaluating the effectiveness of dehazing algorithms on synthetic and real-world hazy images. | Evaluated for image enhancement, focusing on professional-level adjustments to improve image quality. | Used to demonstrate image restoration techniques, specifically for color correction and enhancement in professional photography. | Used to evaluate the model's performance in image restoration, specifically measuring PSNR values. | Used to evaluate image enhancement techniques, focusing on color and exposure adjustments in professional photographs. | Used for training and testing deraining models, providing a large set of rainy images with corresponding clean images. | Evaluated for low-light enhancement, assessing the performance of algorithms in improving visibility in dark conditions. | Used for testing deraining models, providing a set of rainy images and their clean counterparts. | Used for training deraining models, containing synthetic rainy images and their clean counterparts. | Used for training deraining models, containing a small set of rainy images and their clean versions. | Used for testing deraining models, featuring high-resolution rainy images and their clean versions. | Used for training and testing dehazing models, providing indoor scenes with haze and their clear versions. | Used for training and testing low-light enhancement models, providing low-light images and their well-lit counterparts. | Used for training and testing deraining models, including images with raindrops and their clean versions. | Used to evaluate low-light enhancement techniques, specifically focusing on the performance of deep retinex decomposition methods. | Used for low-light image enhancement, evaluating methods on improving visibility and quality in dark images. | Used for training deraining models, consisting of synthetic rainy images and their clean versions.",
          "citing_paper_id": "245837508",
          "cited_paper_id": 52008443,
          "context_text": "Deraining Rain14000 [26] 11200 2800 Test2800 Rain1800 [102] 1800 0 Rain800 [112] 700 98 Test100 Rain100H [102] 0 100 Rain100H Rain100L [102] 0 100 Rain100L Rain1200 [111] 0 1200 Test1200 Rain12 [50] 12 0 Raindrop [71] 861 58 Raindrop-A Raindrop [71] 0 239 Raindrop-B Dehazing RESIDE-ITS [45] 13990 500 SOTS-Indoor RESIDE-OTS [45] 313950 500 SOTS-Outdoor Enhancement MIT-Adobe FiveK [7] 4500 500 FiveK (Retouching) LOL [96] 485 15 LOL",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context lists several datasets used for training and testing various image restoration tasks, including deraining, dehazing, and low-light enhancement.",
          "citing_paper_doi": "10.1109/CVPR52688.2022.00568",
          "cited_paper_doi": null,
          "citing_paper_url": "https://www.semanticscholar.org/paper/fbf68eb0cf8237cef59ebdb301569c79b9676ff9",
          "cited_paper_url": "https://www.semanticscholar.org/paper/c7798bbbcd2e85593a2b2d37b92dabac63420cd8",
          "citing_paper_year": 2022,
          "cited_paper_year": 2018
        },
        {
          "dataset_name": "Rain14000",
          "dataset_description": "Used to train the model for rain streak removal, focusing on image pairs to enhance the model's ability to handle rain streaks in images. | Used to train the model for rain streak removal, providing 700 clean-rain image pairs to further refine the model's performance. | Used to train the model for rain streak removal, providing 1,800 clean-rain image pairs to improve the model's robustness. | Used to train the model for rain streak removal, providing 11,200 clean-rain image pairs to enhance the model's ability to handle diverse rain conditions. | Used to train the model for rain streak removal, providing additional image pairs to ensure comprehensive training.",
          "citing_paper_id": "248426764",
          "cited_paper_id": 9007541,
          "context_text": "Specifically, we use 11,200 clean-rain image pairs in Rain14000 [20], 1,800 image pairs in Rain1800 [71], 700 image pairs in Rain800 [82] image pairs in Rain12 [39] to train our model.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions specific datasets used for training a model for rain streak removal, which are clearly identified and relevant to the topic of image restoration.",
          "citing_paper_doi": "10.1109/CVPR52688.2022.01688",
          "cited_paper_doi": "10.1109/CVPR.2016.299",
          "citing_paper_url": "https://www.semanticscholar.org/paper/22755044094d1eff9d2b61bf2861649446d31bf9",
          "cited_paper_url": "https://www.semanticscholar.org/paper/7c17025c540b88df14da35229618b5e896ab9528",
          "citing_paper_year": 2022,
          "cited_paper_year": 2016
        },
        {
          "dataset_name": "Rain14000",
          "dataset_description": "Used for evaluating rain removal methods, focusing on synthetic rain with low density on clean images. | Used for evaluating rain removal methods, focusing on synthetic rain with high density on clean images. | Used for evaluating image restoration methods, focusing on diverse real-world images with various degradations. | Used to test rain removal algorithms, focusing on high-resolution images with synthetic rain. The dataset evaluates the performance of image restoration methods under challenging conditions. | Used to test rain removal algorithms, focusing on low-resolution images with synthetic rain. The dataset evaluates the performance of image restoration methods on less complex images. | Used to evaluate the performance of image restoration models on a moderate-sized set of images. The dataset provides a balanced assessment of the methods' effectiveness. | Used to evaluate the generalization of image restoration models on a large set of diverse images. The dataset assesses the robustness and adaptability of the methods. | Used to train and evaluate rain removal models, providing 11,200 clean-rain image pairs for robustness testing. | Used to train and evaluate rain removal models, providing 1,800 clean-rain image pairs for performance validation. | Used for evaluating image restoration methods, focusing on a large set of images with realistic degradations. | Used for evaluating image restoration methods, focusing on a smaller set of high-quality images with specific degradations. | Used to train and evaluate rain removal models, providing 700 clean-rain image pairs for additional testing. | Used to evaluate image deraining methods, focusing on the performance of deraining algorithms on synthetic rain images.",
          "citing_paper_id": "248426764",
          "cited_paper_id": 15443600,
          "context_text": "Specifically, we use 11,200 clean-rain image pairs in Rain14000 [20], 1,800 image pairs in Rain1800 [71], 700 image pairs in Rain800 [82] and 12",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions specific datasets used for training and evaluation in the context of rain removal from images.",
          "citing_paper_doi": "10.1109/CVPR52688.2022.01688",
          "cited_paper_doi": "10.1109/CVPR.2017.183",
          "citing_paper_url": "https://www.semanticscholar.org/paper/22755044094d1eff9d2b61bf2861649446d31bf9",
          "cited_paper_url": "https://www.semanticscholar.org/paper/6de27f256e97c014a95706c72af7fa651e8fa34a",
          "citing_paper_year": 2022,
          "cited_paper_year": 2016
        },
        {
          "dataset_name": "Rain14000",
          "dataset_description": "Used for deraining, containing a small set of high-quality rainy images for fine-tuning and validation. | Used for dehazing experiments, focusing on outdoor scenes to evaluate the performance of dehazing algorithms. | Used for deraining, providing a diverse set of rainy images for robustness testing. | Used for deraining, offering a moderate-sized dataset of rainy images for model testing. | Used for denoising, offering a challenging set of images with varying noise levels for performance evaluation. | Used for deblurring, containing a large set of blurred and sharp image pairs for training and testing. | Used for denoising experiments, specifically focusing on a smaller subset of images for evaluation. | Used for denoising experiments, providing a benchmark for evaluating image restoration algorithms on natural images. | Used for dehazing, containing a variety of hazy images to train and evaluate dehazing algorithms. | Used for dehazing experiments, providing a comprehensive dataset of hazy images for training and evaluation. | Used for denoising, providing a well-known dataset of clean and noisy images for benchmarking. | Used for deraining, providing a large set of images with rain streaks for training and evaluation.",
          "citing_paper_id": "272680196",
          "cited_paper_id": 4840263,
          "context_text": "I, including Rain14000 [50], Rain1800 [45], Rain12 [51] and Rain800 [52] for deraining, RESIDE [47] for dehazing, BSD300 [46] and WED [55] for denoising and GoPro [44] for deblurring.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions several datasets used for various image restoration tasks, such as deraining, dehazing, denoising, and deblurring. These datasets are specific and relevant to the topic of all-in-one image restoration.",
          "citing_paper_doi": null,
          "cited_paper_doi": "10.1109/TIP.2016.2631888",
          "citing_paper_url": "https://www.semanticscholar.org/paper/e9ef4cfc510d53d0be69835a34a9195027e4cc51",
          "cited_paper_url": "https://www.semanticscholar.org/paper/1e2fe67d117a19a59a4f82e4950499c5ee5a7473",
          "citing_paper_year": 2023,
          "cited_paper_year": 2017
        },
        {
          "dataset_name": "Rain14000",
          "dataset_description": "Used for deraining, containing a small set of high-quality rainy images for fine-tuning and validation. | Used for evaluating de-raining models, containing a smaller set of synthetic rainy images. | Used for deraining, providing a diverse set of rainy images for robustness testing. | Used for deraining, offering a moderate-sized dataset of rainy images for model testing. | Used for denoising, offering a challenging set of images with varying noise levels for performance evaluation. | Used for evaluating de-raining models, containing 100 low-intensity rainy images. | Used for deblurring, containing a large set of blurred and sharp image pairs for training and testing. | Used for training and evaluating de-raining models, offering a moderate-sized set of synthetic rainy images. | Used for evaluating de-raining models, containing 100 high-intensity rainy images. | Used for training and evaluating deblurring models, providing a large set of real-world blurry images. | Used for training and evaluating de-raining models, providing a large set of synthetic rainy images. | Used for dehazing, containing a variety of hazy images to train and evaluate dehazing algorithms. | Used for evaluating de-raining models, consisting of 100 test images with varying rain intensities. | Used for training and evaluating deblurring models, offering a diverse set of real-world blurry images. | Used for denoising, providing a well-known dataset of clean and noisy images for benchmarking. | Used for deraining, providing a large set of images with rain streaks for training and evaluation.",
          "citing_paper_id": "272680196",
          "cited_paper_id": 9007541,
          "context_text": "I, including Rain14000 [50], Rain1800 [45], Rain12 [51] and Rain800 [52] for deraining, RESIDE [47] for dehazing, BSD300 [46] and WED [55] for denoising and GoPro [44] for deblurring.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions several datasets used for various image restoration tasks, such as deraining, dehazing, denoising, and deblurring. These datasets are specific and relevant to the topic of all-in-one image restoration.",
          "citing_paper_doi": null,
          "cited_paper_doi": "10.1109/CVPR.2016.299",
          "citing_paper_url": "https://www.semanticscholar.org/paper/e9ef4cfc510d53d0be69835a34a9195027e4cc51",
          "cited_paper_url": "https://www.semanticscholar.org/paper/7c17025c540b88df14da35229618b5e896ab9528",
          "citing_paper_year": 2023,
          "cited_paper_year": 2016
        },
        {
          "dataset_name": "Rain14000",
          "dataset_description": "Used for deraining, containing a small set of high-quality rainy images for fine-tuning and validation. | Used for evaluating de-raining models, containing a smaller set of synthetic rainy images. | Used for denoising, offering a challenging set of images with varying noise levels for performance evaluation. | Used for evaluating the model's performance on a diverse set of test images, focusing on generalization and robustness. | Used for evaluating de-raining models, containing 100 high-intensity rainy images. | Used for dehazing, containing a variety of hazy images to train and evaluate dehazing algorithms. | Used for training and evaluating deblurring models, offering a diverse set of real-world blurry images. | Used for evaluating de-raining models, containing 100 low-intensity rainy images. | Used for deblurring, containing a large set of blurred and sharp image pairs for training and testing. | Used for evaluating the model's performance on a large-scale test set, focusing on scalability and consistency. | Used for training and evaluating de-raining models, providing a large set of synthetic rainy images. | Used for denoising, providing a well-known dataset of clean and noisy images for benchmarking. | Used to evaluate image de-raining methods, focusing on low-resolution images with synthetic rain. The dataset assesses the performance of models in removing rain streaks while preserving image details. | Used for evaluating the model's performance on light rain conditions, focusing on image quality and restoration accuracy. | Used for deraining, offering a moderate-sized dataset of rainy images for model testing. | Used to evaluate image de-raining methods, focusing on high-resolution images with synthetic rain. The dataset assesses the performance of models in removing rain streaks while preserving image details. | Used for training and evaluating deblurring models, providing a large set of real-world blurry images. | Used for evaluating the model's performance on heavy rain conditions, focusing on image quality and restoration accuracy. | Used for deraining, providing a diverse set of rainy images for robustness testing. | Used for evaluating de-raining models, consisting of 100 test images with varying rain intensities. | Used for training and evaluating de-raining models, offering a moderate-sized set of synthetic rainy images. | Used for deraining, providing a large set of images with rain streaks for training and evaluation.",
          "citing_paper_id": "272680196",
          "cited_paper_id": 11922819,
          "context_text": "I, including Rain14000 [50], Rain1800 [45], Rain12 [51] and Rain800 [52] for deraining, RESIDE [47] for dehazing, BSD300 [46] and WED [55] for denoising and GoPro [44] for deblurring.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions several datasets used for various image restoration tasks, such as deraining, dehazing, denoising, and deblurring. These datasets are specific and relevant to the topic of all-in-one image restoration.",
          "citing_paper_doi": null,
          "cited_paper_doi": "10.1109/TCSVT.2019.2920407",
          "citing_paper_url": "https://www.semanticscholar.org/paper/e9ef4cfc510d53d0be69835a34a9195027e4cc51",
          "cited_paper_url": "https://www.semanticscholar.org/paper/920f0c070701caabf023c600f3e310f1906ca818",
          "citing_paper_year": 2023,
          "cited_paper_year": 2017
        },
        {
          "dataset_name": "Rain14000",
          "dataset_description": "Used to evaluate rain removal algorithms on low-resolution images with light rain, focusing on noise reduction and clarity enhancement. | Used to benchmark rain removal algorithms on a small, curated dataset with challenging rain scenarios. | Used to train and test rain removal models, offering a large dataset with varied rain conditions and image complexities. | Used to test and validate rain removal techniques, providing a smaller but high-quality dataset with synthetic rain effects. | Used to train and evaluate snow removal algorithms, providing a large dataset with diverse snow conditions and image types. | Used to evaluate the performance of the proposed AIRFormer in image deraining, focusing on the effectiveness of the method in removing rain streaks. | Used to test and validate synthetic snow generation and removal methods, focusing on realistic snow simulation. | Used to evaluate image dehazing and deraining algorithms in indoor scenes, emphasizing the removal of atmospheric disturbances. | Used to assess the performance of rain removal methods on high-resolution images with heavy rain, emphasizing detail preservation. | Used to train and evaluate rain removal algorithms, focusing on diverse rain patterns and intensities in real-world images. | Used to test the robustness of the proposed AIRFormer in heavy rain conditions, integrating physics models and conditional adversarial learning. | Used to test and validate outdoor scene dehazing and deraining techniques, focusing on natural lighting and environmental conditions. | Used to assess the performance of haze and rain removal algorithms in synthetic and real-world images, emphasizing robustness and generalization. | Used to assess the deraining capabilities of the proposed AIRFormer, specifically examining its ability to handle various rain intensities and patterns. | Used to compile a comprehensive all-in-one image restoration dataset, integrating various rain and snow removal datasets for training and evaluation. | Used to develop and test raindrop removal techniques, focusing on the unique challenges posed by raindrops on surfaces.",
          "citing_paper_id": "260271048",
          "cited_paper_id": 11922819,
          "context_text": "We collect an all-in-one image restoration dataset named AIR40K, including Rain14000 [26], Rain800 [79], Rain100H [80], Rain100L [80], Rain1200 [33], Rain12 [81], Raindrop [43], Snow100K [27], CSD [10], ITS [34], OTS [34], HSTS [34].",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions multiple datasets used for image restoration, which are specific and relevant to the research topic.",
          "citing_paper_doi": "10.1109/TCSVT.2023.3299324",
          "cited_paper_doi": "10.1109/TCSVT.2019.2920407",
          "citing_paper_url": "https://www.semanticscholar.org/paper/2a828c28938b196fff7b31e58a08853daaaa8b96",
          "cited_paper_url": "https://www.semanticscholar.org/paper/920f0c070701caabf023c600f3e310f1906ca818",
          "citing_paper_year": 2024,
          "cited_paper_year": 2017
        },
        {
          "dataset_name": "Rain14000",
          "dataset_description": "Used to validate deraining techniques on a large-scale dataset, assessing performance across diverse rain conditions and image complexities. | Used for evaluating the deraining model, providing a larger set of 1200 test images to validate performance. | Used to benchmark deraining methods on an extensive dataset, ensuring comprehensive evaluation of model accuracy and efficiency. | Used for evaluating the deraining model, providing a comprehensive set of 2800 test images to validate performance. | Used to test deraining algorithms on a moderate-sized dataset, evaluating the generalization and robustness of the models. | Used for evaluating the deraining model, focusing on high-quality images to assess performance. | Used to evaluate deraining performance on high-resolution images, focusing on detailed rain removal and image quality enhancement. | Used for training the deraining model, providing a large set of rainy images to improve generalization. | Used to assess deraining effectiveness on low-resolution images, emphasizing the removal of light rain streaks and noise reduction. | Used for evaluating the deraining model, providing a set of 100 test images to validate performance. | Used for evaluating the deraining model, focusing on low-quality images to assess robustness.",
          "citing_paper_id": "270155352",
          "cited_paper_id": 17115407,
          "context_text": "Image deraining: We used Rain14000 dataset [71] for training, while evaluated on Rain100H [73], Rain100L [73], Test100 [72], Test1200 [74], and Test2800 [71].",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions specific datasets used for training and evaluation in the image deraining task. These datasets are clearly identified and used in the research.",
          "citing_paper_doi": "10.1109/TMM.2024.3407656",
          "cited_paper_doi": "10.1109/CVPR.2017.186",
          "citing_paper_url": "https://www.semanticscholar.org/paper/3a80830bf3e31c855afaff17a5b590bc052839c6",
          "cited_paper_url": "https://www.semanticscholar.org/paper/29ad37a306732676a41f25e05960ff0adbca631e",
          "citing_paper_year": 2024,
          "cited_paper_year": 2017
        },
        {
          "dataset_name": "Rain14000",
          "dataset_description": "Used to validate deraining techniques on a large-scale dataset, assessing performance across diverse rain conditions and image complexities. | Used for evaluating the deraining model, providing a larger set of 1200 test images to validate performance. | Used to benchmark deraining methods on an extensive dataset, ensuring comprehensive evaluation of model accuracy and efficiency. | Used for evaluating the deraining model, providing a comprehensive set of 2800 test images to validate performance. | Used to test deraining algorithms on a moderate-sized dataset, evaluating the generalization and robustness of the models. | Used for evaluating the deraining model, focusing on high-quality images to assess performance. | Used to evaluate deraining performance on high-resolution images, focusing on detailed rain removal and image quality enhancement. | Used for training the deraining model, providing a large set of rainy images to improve generalization. | Used to assess deraining effectiveness on low-resolution images, emphasizing the removal of light rain streaks and noise reduction. | Used for evaluating the deraining model, providing a set of 100 test images to validate performance. | Used for evaluating the deraining model, focusing on low-quality images to assess robustness.",
          "citing_paper_id": "270155352",
          "cited_paper_id": 214623217,
          "context_text": "Image deraining: We used Rain14000 dataset [71] for training, while evaluated on Rain100H [73], Rain100L [73], Test100 [72], Test1200 [74], and Test2800 [71].",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions specific datasets used for training and evaluation in the image deraining task. These datasets are clearly identified and used in the research.",
          "citing_paper_doi": "10.1109/TMM.2024.3407656",
          "cited_paper_doi": "10.1109/CVPR42600.2020.00837",
          "citing_paper_url": "https://www.semanticscholar.org/paper/3a80830bf3e31c855afaff17a5b590bc052839c6",
          "cited_paper_url": "https://www.semanticscholar.org/paper/1234b5451347ce6fe3bae9c34e67edbacaf73b52",
          "citing_paper_year": 2024,
          "cited_paper_year": 2020
        }
      ]
    },
    {
      "cited_paper_id": "8282555",
      "citation_count": 0,
      "total_dataset_mentions": 11,
      "unique_datasets": [
        "Urban100"
      ],
      "dataset_details": [
        {
          "dataset_name": "Urban100",
          "dataset_description": "Used to evaluate the performance of the Restormer model at a challenging noise level of 50, focusing on high-resolution image restoration. | Used to test and analyze image restoration performance at a challenging noise level of σ = 50, focusing on super-resolution techniques. | Used for denoising experiments with real-world datasets, evaluating denoising algorithms on smartphone camera images. | Used for denoising experiments with real-world datasets, evaluating denoising algorithms on diverse real-world images. | Used for denoising experiments with synthetic benchmark datasets, evaluating denoising algorithms under controlled conditions.",
          "citing_paper_id": "244346144",
          "cited_paper_id": 8282555,
          "context_text": "Speciﬁcally, for the challenging noise level 50 on high-resolution Urban100 dataset [29], Restormer achieves 0 .",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions the 'Urban100 dataset' which is a specific dataset used for image restoration tasks. The dataset is used to evaluate the performance of the Restormer model at a challenging noise level.",
          "citing_paper_doi": "10.1109/CVPR52688.2022.00564",
          "cited_paper_doi": "10.1109/CVPR.2015.7299156",
          "citing_paper_url": "https://www.semanticscholar.org/paper/1e88d5afe19aea324d33541f60a90b7036894c32",
          "cited_paper_url": "https://www.semanticscholar.org/paper/2946c4e29b057d3543f09b89f547965895676d19",
          "citing_paper_year": 2021,
          "cited_paper_year": 2015
        },
        {
          "dataset_name": "Urban100",
          "dataset_description": "Used for testing real denoising, focusing on real-world noise patterns to assess the robustness of the restoration method. | Used for synthetic image denoising experiments, evaluating the performance of the proposed IPT-V2 method on images with additive white Gaussian noise. | Used for real-world image denoising experiments, evaluating the performance of the proposed IPT-V2 method on images with real-world noise. | Used for testing deraining, focusing on rain-streak removal from images to evaluate the performance in challenging weather conditions. | Used for testing Gaussian denoising, focusing on urban scene images to evaluate restoration quality and effectiveness.",
          "citing_paper_id": "268819753",
          "cited_paper_id": 8282555,
          "context_text": "The results are test on Urban100 [29], DND [60] and Rain100H [82] for Gaussian denoising, real denoising and deraining, respectively.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The citation mentions specific datasets used for testing different aspects of image restoration, which aligns with the research topic.",
          "citing_paper_doi": "10.48550/arXiv.2404.00633",
          "cited_paper_doi": "10.1109/CVPR.2015.7299156",
          "citing_paper_url": "https://www.semanticscholar.org/paper/979e606d3f09415be6d520213eabf9bfacd9f8dd",
          "cited_paper_url": "https://www.semanticscholar.org/paper/2946c4e29b057d3543f09b89f547965895676d19",
          "citing_paper_year": 2024,
          "cited_paper_year": 2015
        },
        {
          "dataset_name": "Urban100",
          "dataset_description": "Used to evaluate image denoising performance, comparing the proposed method against various state-of-the-art models with a noise level of σ=50. | Used to evaluate image super-resolution performance, comparing the proposed method against various state-of-the-art models at a scale factor of ×4.",
          "citing_paper_id": "227239228",
          "cited_paper_id": 10514149,
          "context_text": "…of the proposed method, Urban100( × 4): img 004 HR VDSR[41] EDSR[51] RDN[94] OISR[35] SAN[17] RNAN [93] IGNN BSD68: 163085 GT Noisy ( σ =50) CBM3D [16] TNRD [14] RDN [94] DnCNN [87] MemNet [65] IRCNN [88] FFDNet [89] IPT (ours) we compare our results with various state-of-the-art models.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions several datasets and methods, but only 'Urban100' and 'BSD68' are specific datasets used for evaluation. Other names like 'VDSR', 'EDSR', etc., are models or methods.",
          "citing_paper_doi": "10.1109/CVPR46437.2021.01212",
          "cited_paper_doi": "10.1109/TIP.2018.2839891",
          "citing_paper_url": "https://www.semanticscholar.org/paper/6f6f73e69ee0d9d5d7d088bb882db1851d98175a",
          "cited_paper_url": "https://www.semanticscholar.org/paper/e248ac3596d48ce338244624c2fd194dc0651bc6",
          "citing_paper_year": 2020,
          "cited_paper_year": 2017
        },
        {
          "dataset_name": "Urban100",
          "dataset_description": "Used to evaluate single image super-resolution methods, focusing on urban scenes at a scale factor of 4. The dataset helps assess the quality of upscaled images compared to high-resolution ground truth. | Used to evaluate denoising algorithms, specifically for images with Gaussian noise (σ=50). The dataset helps assess the effectiveness of noise reduction techniques. | Used to evaluate single image super-resolution methods, focusing on urban scenes at 4x scaling. The dataset helps assess the quality and detail recovery of high-resolution images.",
          "citing_paper_id": "227239228",
          "cited_paper_id": 174788791,
          "context_text": "To verify the effectiveness of the proposed method, Urban100( × 4): img 004 HR VDSR[41] EDSR[51] RDN[94] OISR[35] SAN[17] RNAN [93] IGNN BSD68: 163085 GT Noisy ( σ =50) CBM3D [16] TNRD [14] RDN [94] DnCNN [87] MemNet [65] IRCNN [88] FFDNet [89] IPT (ours) we compare our results with various…",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions specific datasets used for comparing the performance of the proposed method against existing methods. These datasets are relevant to image restoration and super-resolution.",
          "citing_paper_doi": "10.1109/CVPR46437.2021.01212",
          "cited_paper_doi": "10.1109/CVPR.2019.01132",
          "citing_paper_url": "https://www.semanticscholar.org/paper/6f6f73e69ee0d9d5d7d088bb882db1851d98175a",
          "cited_paper_url": "https://www.semanticscholar.org/paper/fd2a0a326db4f034fe22340c20b7bacd9a14c3d6",
          "citing_paper_year": 2020,
          "cited_paper_year": 2019
        },
        {
          "dataset_name": "Urban100",
          "dataset_description": "Used for comprehensive image restoration, focusing on combined degradations including rain, haze, noise, blur, and low light. | Used for image restoration, focusing on prompt-based methods for various degradation types. | Used for evaluating image restoration techniques, focusing on urban scenes with high-resolution details. | Used for adaptive image restoration, focusing on dynamic adjustment to different degradation levels. | Used for image restoration, focusing on text prompts to guide restoration processes. | Used for image restoration, particularly for removing atmospheric interference in aerial images. | Used for versatile image restoration, focusing on handling multiple degradations in a single model. | Used for all-in-one image restoration, focusing on unified-width adaptive dynamic networks. | Used for data-driven image restoration, focusing on adaptive methods for various degradations. | Used for hybrid image restoration, combining multiple techniques for comprehensive degradation handling. | Used for dynamic network adaptation in image restoration, focusing on efficient and flexible architectures. | Used to assess rain removal algorithms, specifically targeting light rain conditions in images. | Used for multi-exposure image fusion, focusing on enhancing dynamic range and detail preservation. | Used for noise reduction, specifically targeting non-uniform noise distributions in images. | Used for dehazing, focusing on outdoor real-world images with natural haze conditions. | Used for dehazing, focusing on outdoor synthetic images with varying levels of haze. | Used for image restoration, focusing on instruction-based methods for diverse degradation types. | Used for image restoration, specifically for handling multiple degradations in a single framework. | Used for perceptual image restoration, focusing on improving visual quality and realism.",
          "citing_paper_id": "273502246",
          "cited_paper_id": 267199899,
          "context_text": "…Urban100, Rain100L, RESIDE-OTS, RESIDE-SOTS AirNet [63], PromptIR [65],PIP [72], Textpromp-tIR [94], NDR [96], InstructIR [62], AdaIR [78], U-WADN [186], DyNet [185], DaAIR [187], AnyIR [188], MEASNet [110], HAIR [189], Perceive-IR [111] rain-haze-noise-blur-dark[66] 5 Rain, Haze, Noise-σ 25 ,…",
          "confidence_score": 0.9,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions several datasets and methods, but only multi-word proper nouns and hyphenated names with digits are included as they meet the criteria for specific, verifiable datasets.",
          "citing_paper_doi": "10.48550/arXiv.2410.15067",
          "cited_paper_doi": "10.48550/arXiv.2401.13221",
          "citing_paper_url": "https://www.semanticscholar.org/paper/f5ff17162a4936db3f64ed7bafae13abcfa63a27",
          "cited_paper_url": "https://www.semanticscholar.org/paper/a41198991cb411e9dbcbedf637b974bb0c326e45",
          "citing_paper_year": 2024,
          "cited_paper_year": 2024
        },
        {
          "dataset_name": "Urban100",
          "dataset_description": "Used to assess denoising algorithms, specifically targeting a diverse set of natural images with different noise intensities. | Used to evaluate the performance of image restoration methods, focusing on high-quality denoising and deblocking of grayscale and color images. | Used to test image denoising methods, emphasizing the restoration of a small, well-defined set of images under varying noise conditions. | Used to evaluate the performance of the BBCU model in image restoration, specifically comparing it against other BNN models at a quality factor of 40. | Used to evaluate image denoising performance, focusing on high-quality restoration of urban scenes with various noise levels.",
          "citing_paper_id": "252683961",
          "cited_paper_id": 1151947,
          "context_text": "The standard benchmarks: Urban100 (Huang et al., 2015), BSD68 (Martin et al., 2001), and Set12 (Shan et al., Methods OPs(G) Params(K) Live1 Classic5 q = 10 q = 20 q = 30 q = 40 q = 10 q = 20 q = 30 q = 40 The quantitative results of image denoising are shown in Tab.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions specific datasets used for image denoising, which are relevant to the topic of all-in-one image restoration.",
          "citing_paper_doi": "10.48550/arXiv.2210.00405",
          "cited_paper_doi": "10.1109/TIP.2007.891788",
          "citing_paper_url": "https://www.semanticscholar.org/paper/a3d9fd2c384e98fb6074a9064562a4e4dd941ed8",
          "cited_paper_url": "https://www.semanticscholar.org/paper/60e7d2cf0aa8088a9ba7a01b0aad19a61e441e0c",
          "citing_paper_year": 2022,
          "cited_paper_year": 2007
        },
        {
          "dataset_name": "Urban100",
          "dataset_description": "Used to evaluate image denoising methods, focusing on performance metrics and visual quality improvements. | Used to test image denoising methods, emphasizing the restoration of a small, well-defined set of images under varying noise conditions. | Used to evaluate image denoising performance, focusing on high-quality restoration of urban scenes with various noise levels. | Used to assess denoising algorithms, specifically targeting a diverse set of natural images with different noise intensities.",
          "citing_paper_id": "252683961",
          "cited_paper_id": 198448968,
          "context_text": "The standard benchmarks: Urban100 (Huang et al., 2015), BSD68 (Martin et al., 2001), and Set12 (Shan et al., Methods OPs(G) Params(K) Live1 Classic5 q = 10 q = 20 q = 30 q = 40 q = 10 q = 20 q = 30 q = 40 The quantitative results of image denoising are shown in Tab.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions specific datasets used for image denoising, which are relevant to the topic of all-in-one image restoration.",
          "citing_paper_doi": "10.48550/arXiv.2210.00405",
          "cited_paper_doi": "10.3233/JIFS-190017",
          "citing_paper_url": "https://www.semanticscholar.org/paper/a3d9fd2c384e98fb6074a9064562a4e4dd941ed8",
          "cited_paper_url": "https://www.semanticscholar.org/paper/d8764cb4961b9cfab62ab31b22583ed2fe31f72c",
          "citing_paper_year": 2022,
          "cited_paper_year": 2019
        },
        {
          "dataset_name": "Urban100",
          "dataset_description": "Used for denoising experiments, specifically to evaluate the performance of the proposed method under additive white Gaussian noise conditions. | Used for Gaussian image denoising comparisons, focusing on evaluating different categories of methods on high-resolution images.",
          "citing_paper_id": "250451699",
          "cited_paper_id": 8282555,
          "context_text": "We perform denoising experiments on synthetic benchmark dataset Urban100 [14] generated with additive white Gaussian noise.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions a specific dataset, Urban100, which is used for denoising experiments. The dataset is clearly identified and its usage is described.",
          "citing_paper_doi": "10.1007/978-3-031-20071-7_4",
          "cited_paper_doi": "10.1109/CVPR.2015.7299156",
          "citing_paper_url": "https://www.semanticscholar.org/paper/6f1cd5055f85ed15e7b61f599946a934c25d39b6",
          "cited_paper_url": "https://www.semanticscholar.org/paper/2946c4e29b057d3543f09b89f547965895676d19",
          "citing_paper_year": 2021,
          "cited_paper_year": 2015
        },
        {
          "dataset_name": "Urban100",
          "dataset_description": "Used for general image restoration, focusing on a diverse set of images to test restoration algorithms. | Used for validating image restoration models, focusing on high-quality images with a variety of content and textures. | Used for image super-resolution, focusing on enhancing the quality of urban scenes with high-frequency details.",
          "citing_paper_id": "264146360",
          "cited_paper_id": 8887614,
          "context_text": "2001), Manga109 (Matsui et al. 2017), Urban100 (Huang, Singh, and Ahuja 2015), General100 (Dong, Loy, and Tang 2016), and DIV2K-Valid (Agustsson and Timofte 2017).",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The citation mentions multiple datasets that are commonly used in image restoration and related tasks. These datasets are specific and have clear identifiers.",
          "citing_paper_doi": "10.48550/arXiv.2310.10513",
          "cited_paper_doi": "10.1007/s11042-016-4020-z",
          "citing_paper_url": "https://www.semanticscholar.org/paper/836906c334159d46a691dfc5466c33caa3d22f65",
          "cited_paper_url": "https://www.semanticscholar.org/paper/1fef45786e707e6b9b8517b0403e596ecbdea6a5",
          "citing_paper_year": 2023,
          "cited_paper_year": 2015
        },
        {
          "dataset_name": "Urban100",
          "dataset_description": "Used to assess image denoising quality, featuring a diverse set of natural images with high detail. | Used to test image denoising effectiveness, consisting of grayscale images with various degradation levels. | Used to evaluate image denoising performance, focusing on urban scenes with complex textures and structures.",
          "citing_paper_id": "268553835",
          "cited_paper_id": null,
          "context_text": "Table 6: Image denoising results of directly applying the pre-trained model under the five-degradation setting to the Urban100 [25], Kodak24 [53] and BSD68 [41] datasets.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions three specific datasets used for image denoising experiments. These datasets are commonly used in image processing and computer vision for evaluating restoration algorithms.",
          "citing_paper_doi": "10.48550/arXiv.2403.14614",
          "cited_paper_doi": null,
          "citing_paper_url": "https://www.semanticscholar.org/paper/8cf850abfec7ce894b498ef082282382e8d3374c",
          "cited_paper_url": null,
          "citing_paper_year": 2024,
          "cited_paper_year": null
        },
        {
          "dataset_name": "Urban100",
          "dataset_description": "Used to evaluate image restoration performance at a noise level of σ = 25, comparing AdaIR against IDR in terms of PSNR improvement.",
          "citing_paper_id": "268553835",
          "cited_paper_id": 244346144,
          "context_text": "On Urban100 [25] for the noise level σ = 25 , AdaIR produces a significant performance gain of 0.39 dB PSNR over IDR [76].",
          "confidence_score": 1.0,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions 'Urban100' as a dataset used for evaluating image restoration performance at a specific noise level.",
          "citing_paper_doi": "10.48550/arXiv.2403.14614",
          "cited_paper_doi": "10.1109/CVPR52688.2022.00564",
          "citing_paper_url": "https://www.semanticscholar.org/paper/8cf850abfec7ce894b498ef082282382e8d3374c",
          "cited_paper_url": "https://www.semanticscholar.org/paper/1e88d5afe19aea324d33541f60a90b7036894c32",
          "citing_paper_year": 2024,
          "cited_paper_year": 2021
        }
      ]
    },
    {
      "cited_paper_id": "233476615",
      "citation_count": 0,
      "total_dataset_mentions": 11,
      "unique_datasets": [
        "REDS"
      ],
      "dataset_details": [
        {
          "dataset_name": "REDS",
          "dataset_description": "Used to conduct experiments and evaluate results in image restoration, focusing on the validation set to assess performance.",
          "citing_paper_id": "270155352",
          "cited_paper_id": 245837508,
          "context_text": "Wefurther conduct experiments on the REDS dataset and evaluate the result on the validation set following [4], [80].",
          "confidence_score": 1.0,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions the REDS dataset, which is a specific dataset used for image restoration. The dataset is used for conducting experiments and evaluating results.",
          "citing_paper_doi": "10.1109/TMM.2024.3407656",
          "cited_paper_doi": "10.1109/CVPR52688.2022.00568",
          "citing_paper_url": "https://www.semanticscholar.org/paper/3a80830bf3e31c855afaff17a5b590bc052839c6",
          "cited_paper_url": "https://www.semanticscholar.org/paper/fbf68eb0cf8237cef59ebdb301569c79b9676ff9",
          "citing_paper_year": 2024,
          "cited_paper_year": 2022
        },
        {
          "dataset_name": "REDS",
          "dataset_description": "Used to conduct experiments and evaluate results in image restoration, focusing on the validation set to assess performance.",
          "citing_paper_id": "270155352",
          "cited_paper_id": 234482841,
          "context_text": "Wefurther conduct experiments on the REDS dataset and evaluate the result on the validation set following [4], [80].",
          "confidence_score": 1.0,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions the REDS dataset, which is a specific dataset used for image restoration. The dataset is used for conducting experiments and evaluating results.",
          "citing_paper_doi": "10.1109/TMM.2024.3407656",
          "cited_paper_doi": "10.1109/CVPRW53098.2021.00027",
          "citing_paper_url": "https://www.semanticscholar.org/paper/3a80830bf3e31c855afaff17a5b590bc052839c6",
          "cited_paper_url": "https://www.semanticscholar.org/paper/9b8cbe9a636b00b3543d8e1f98c24de12d6c3d17",
          "citing_paper_year": 2024,
          "cited_paper_year": 2021
        },
        {
          "dataset_name": "REDS",
          "dataset_description": "Utilized to test deblurring methods, specifically addressing realistic motion blur in challenging conditions. | Used to train the model on 24,000 images for the NTIRE 2021 Image Deblurring Challenge, focusing on JPEG artifacts. | Used for motion deblurring experiments, providing a large set of sharp and blurred image pairs to train and evaluate deblurring models. | Used to test deblurring algorithms on real-world images, emphasizing robustness across different blur conditions. | Applied to assess deblurring performance on real-world images, focusing on challenging scenarios with varying blur types. | Used for training a model for 10k epochs, focusing on image deblurring tasks. The dataset provides high-quality video sequences for deblurring research. | Employed for video deblurring, offering a large-scale dataset with high-resolution video sequences for training and evaluation. | Used for visual comparisons of image deblurring results, specifically focusing on JPEG artifacts in the NTIRE 2021 Image Deblurring Challenge Track 2. | Used for quantitative comparisons of MAXIM-3S, HINet, and MPRNet in the NTIRE 2021 Image Deblurring Challenge, focusing on deblurring performance with JPEG artifacts. | Applied to assess deblurring algorithms, emphasizing the removal of motion blur in high-resolution images. | Utilized for evaluating deblurring algorithms, containing real-world blurred images with ground truth sharp images. | Employed to evaluate deblurring models, concentrating on long-exposure and motion-induced blur in video sequences. | Used for assessing raindrop removal effectiveness, specifically targeting single image raindrop removal. | Used for evaluating deblurring performance, focusing on motion blur in video sequences. | Used to evaluate motion deblurring techniques, focusing on visual quality and restoration performance in real-world scenarios.",
          "citing_paper_id": "245837508",
          "cited_paper_id": 233476615,
          "context_text": "Due to space limitations, we detail the outcomes of our experiments on the REDS deblurring [61] and the Raindrop removal task [71] in Appendix.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The citation mentions two specific tasks and datasets: REDS deblurring and Raindrop removal. These are clearly identified as datasets used for experimental outcomes.",
          "citing_paper_doi": "10.1109/CVPR52688.2022.00568",
          "cited_paper_doi": "10.1109/CVPRW53098.2021.00025",
          "citing_paper_url": "https://www.semanticscholar.org/paper/fbf68eb0cf8237cef59ebdb301569c79b9676ff9",
          "cited_paper_url": "https://www.semanticscholar.org/paper/149ed92d2e52acc6645aec45cbda486e071e5fe4",
          "citing_paper_year": 2022,
          "cited_paper_year": 2021
        },
        {
          "dataset_name": "REDS",
          "dataset_description": "Used for training and evaluating motion deblurring models, focusing on improving the representation capability of IR networks for complex degradations.",
          "citing_paper_id": "261031011",
          "cited_paper_id": 8671030,
          "context_text": "…and DID-MDN [50] for draining, REDS [51], and Gopro [52] for motion deblurring, etc. Leveraging these datasets, the majority of recent works [1–3, 7–11, 13, 16, 19, 21–23, 32– 34, 53–55] focused on improving the representation capability of IR networks for complicated degradation through…",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions specific datasets used for training and evaluation in image restoration tasks, particularly for motion deblurring.",
          "citing_paper_doi": "10.48550/arXiv.2308.09388",
          "cited_paper_doi": "10.1109/CVPR.2017.35",
          "citing_paper_url": "https://www.semanticscholar.org/paper/70ba95384664ab7becfd8196d20a46fe66112c91",
          "cited_paper_url": "https://www.semanticscholar.org/paper/65b16da51891a6b98140d425804c8a0fd0299219",
          "citing_paper_year": 2023,
          "cited_paper_year": 2016
        },
        {
          "dataset_name": "REDS",
          "dataset_description": "Used for training and evaluating motion deblurring models, focusing on improving the representation capability of IR networks for complex degradations.",
          "citing_paper_id": "261031011",
          "cited_paper_id": 248366743,
          "context_text": "…and DID-MDN [50] for draining, REDS [51], and Gopro [52] for motion deblurring, etc. Leveraging these datasets, the majority of recent works [1–3, 7–11, 13, 16, 19, 21–23, 32– 34, 53–55] focused on improving the representation capability of IR networks for complicated degradation through…",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions specific datasets used for training and evaluation in image restoration tasks, particularly for motion deblurring.",
          "citing_paper_doi": "10.48550/arXiv.2308.09388",
          "cited_paper_doi": "10.1109/CVPRW56347.2022.00061",
          "citing_paper_url": "https://www.semanticscholar.org/paper/70ba95384664ab7becfd8196d20a46fe66112c91",
          "cited_paper_url": "https://www.semanticscholar.org/paper/916d10920b79cef09b14e86810637ab3c70e5956",
          "citing_paper_year": 2023,
          "cited_paper_year": 2021
        },
        {
          "dataset_name": "REDS",
          "dataset_description": "Used for motion deblurring tasks in image restoration, specifically to improve the representation capability of IR networks for complex degradations. | Used for motion deblurring tasks in image restoration, enhancing the representation capability of IR networks for complex degradations. | Used for draining tasks in image restoration, focusing on improving the representation capability of IR networks for complex degradations.",
          "citing_paper_id": "261031011",
          "cited_paper_id": 18874645,
          "context_text": "…[49] and DID-MDN [50] for draining, REDS [51], and Gopro [52] for motion deblurring, etc. Leveraging these datasets, the majority of recent works [1–3, 7–11, 13, 16, 19, 21–23, 32– 34, 53–55] focused on improving the representation capability of IR networks for complicated degradation through…",
          "confidence_score": 0.9,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions several datasets used for specific tasks in image restoration, such as draining and motion deblurring. These datasets are leveraged to improve the representation capability of image restoration networks.",
          "citing_paper_doi": "10.48550/arXiv.2308.09388",
          "cited_paper_doi": "10.1007/978-3-319-10593-2_13",
          "citing_paper_url": "https://www.semanticscholar.org/paper/70ba95384664ab7becfd8196d20a46fe66112c91",
          "cited_paper_url": "https://www.semanticscholar.org/paper/0504945cc2d03550fecb6ff02e637f9421107c25",
          "citing_paper_year": 2023,
          "cited_paper_year": 2014
        },
        {
          "dataset_name": "REDS",
          "dataset_description": "Used for training and evaluating motion deblurring models, focusing on improving the representation capability of IR networks for complex degradations.",
          "citing_paper_id": "261031011",
          "cited_paper_id": 25616905,
          "context_text": "…DID-MDN [50] for draining, REDS [51], and Gopro [52] for motion deblurring, etc. Leveraging these datasets, the majority of recent works [1–3, 7–11, 13, 16, 19, 21–23, 32– 34, 53–55] focused on improving the representation capability of IR networks for complicated degradation through well-designed…",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions specific datasets used for training and evaluation in image restoration tasks, particularly for motion deblurring.",
          "citing_paper_doi": "10.48550/arXiv.2308.09388",
          "cited_paper_doi": "10.1109/ICCV.2017.509",
          "citing_paper_url": "https://www.semanticscholar.org/paper/70ba95384664ab7becfd8196d20a46fe66112c91",
          "cited_paper_url": "https://www.semanticscholar.org/paper/5e73e9cc8b97ce225217d0ffc37d207f22cbff1a",
          "citing_paper_year": 2023,
          "cited_paper_year": 2017
        },
        {
          "dataset_name": "REDS",
          "dataset_description": "Used for motion deblurring, focusing on improving the representation capability of image restoration networks through well-designed backbones.",
          "citing_paper_id": "261031011",
          "cited_paper_id": 215827845,
          "context_text": "…REDS [51], and Gopro [52] for motion deblurring, etc. Leveraging these datasets, the majority of recent works [1–3, 7–11, 13, 16, 19, 21–23, 32– 34, 53–55] focused on improving the representation capability of IR networks for complicated degradation through well-designed backbones based on…",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions specific datasets used for motion deblurring, which are relevant to the research topic of image restoration. These datasets are used to improve the representation capability of image restoration networks.",
          "citing_paper_doi": "10.48550/arXiv.2308.09388",
          "cited_paper_doi": "10.1007/978-3-030-58598-3_18",
          "citing_paper_url": "https://www.semanticscholar.org/paper/70ba95384664ab7becfd8196d20a46fe66112c91",
          "cited_paper_url": "https://www.semanticscholar.org/paper/80f1da2e64d2c78bc6a7b5131fc6e274eb29924f",
          "citing_paper_year": 2023,
          "cited_paper_year": 2020
        },
        {
          "dataset_name": "REDS",
          "dataset_description": "Used for motion deblurring, focusing on improving the representation capability of image restoration networks through well-designed backbones.",
          "citing_paper_id": "261031011",
          "cited_paper_id": 232352874,
          "context_text": "…REDS [51], and Gopro [52] for motion deblurring, etc. Leveraging these datasets, the majority of recent works [1–3, 7–11, 13, 16, 19, 21–23, 32– 34, 53–55] focused on improving the representation capability of IR networks for complicated degradation through well-designed backbones based on…",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions specific datasets used for motion deblurring, which are relevant to the research topic of image restoration. These datasets are used to improve the representation capability of image restoration networks.",
          "citing_paper_doi": "10.48550/arXiv.2308.09388",
          "cited_paper_doi": "10.1109/ICCV48922.2021.00986",
          "citing_paper_url": "https://www.semanticscholar.org/paper/70ba95384664ab7becfd8196d20a46fe66112c91",
          "cited_paper_url": "https://www.semanticscholar.org/paper/c8b25fab5608c3e033d34b4483ec47e68ba109b7",
          "citing_paper_year": 2023,
          "cited_paper_year": 2021
        },
        {
          "dataset_name": "REDS",
          "dataset_description": "Used for motion deblurring in high-resolution image restoration, focusing on improving the representation capability of IR networks for complex degradations. | Used for motion deblurring in high-resolution image restoration, focusing on enhancing the performance of IR networks under challenging conditions.",
          "citing_paper_id": "261031011",
          "cited_paper_id": 244346144,
          "context_text": "…[50] for draining, REDS [51], and Gopro [52] for motion deblurring, etc. Leveraging these datasets, the majority of recent works [1–3, 7–11, 13, 16, 19, 21–23, 32– 34, 53–55] focused on improving the representation capability of IR networks for complicated degradation through well-designed…",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions specific datasets used for various image restoration tasks, which align with the research topic of all-in-one image restoration.",
          "citing_paper_doi": "10.48550/arXiv.2308.09388",
          "cited_paper_doi": "10.1109/CVPR52688.2022.00564",
          "citing_paper_url": "https://www.semanticscholar.org/paper/70ba95384664ab7becfd8196d20a46fe66112c91",
          "cited_paper_url": "https://www.semanticscholar.org/paper/1e88d5afe19aea324d33541f60a90b7036894c32",
          "citing_paper_year": 2023,
          "cited_paper_year": 2021
        },
        {
          "dataset_name": "REDS",
          "dataset_description": "Used for motion deblurring, focusing on improving the representation capability of image restoration networks through well-designed backbones.",
          "citing_paper_id": "261031011",
          "cited_paper_id": 250562472,
          "context_text": "…draining, REDS [51], and Gopro [52] for motion deblurring, etc. Leveraging these datasets, the majority of recent works [1–3, 7–11, 13, 16, 19, 21–23, 32– 34, 53–55] focused on improving the representation capability of IR networks for complicated degradation through well-designed backbones…",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions specific datasets used for motion deblurring, which are relevant to the topic of image restoration. The datasets are clearly named and used in the research context.",
          "citing_paper_doi": "10.48550/arXiv.2308.09388",
          "cited_paper_doi": "10.1145/3529446.3529455",
          "citing_paper_url": "https://www.semanticscholar.org/paper/70ba95384664ab7becfd8196d20a46fe66112c91",
          "cited_paper_url": "https://www.semanticscholar.org/paper/d84e3f35202dad85cd6bbfc0c80088866afb3061",
          "citing_paper_year": 2023,
          "cited_paper_year": 2022
        }
      ]
    },
    {
      "cited_paper_id": "4766599",
      "citation_count": 0,
      "total_dataset_mentions": 11,
      "unique_datasets": [
        "DPDD"
      ],
      "dataset_details": [
        {
          "dataset_name": "DPDD",
          "dataset_description": "Used to evaluate defocus deblurring performance, focusing on mean absolute error (MAE) and learned perceptual image patch similarity (LPIPS) metrics.",
          "citing_paper_id": "271500033",
          "cited_paper_id": 4766599,
          "context_text": "…peak signal-to-noise ratio (PSNR) and structural similarity index (SSIM) [Wang et al. , 2004] for all datasets, and additionally, adopt the mean absolute error (MAE) and learned perceptual image patch similarity (LPIPS) [Zhang et al. , 2018] for the DPDD [Abuolaim and Brown, 2020] dataset.",
          "confidence_score": 1.0,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions the DPDD dataset, which is used for defocus deblurring. The dataset is specifically referenced in the context of evaluating image restoration methods using various metrics.",
          "citing_paper_doi": "10.24963/ijcai.2024/80",
          "cited_paper_doi": "10.1109/CVPR.2018.00068",
          "citing_paper_url": "https://www.semanticscholar.org/paper/371579255875a7d7726e3402158dd2eb79307f7c",
          "cited_paper_url": "https://www.semanticscholar.org/paper/c468bbde6a22d961829e1970e6ad5795e05418d1",
          "citing_paper_year": 2024,
          "cited_paper_year": 2018
        },
        {
          "dataset_name": "DPDD",
          "dataset_description": "Used to evaluate defocus deblurring performance, focusing on mean absolute error (MAE) and learned perceptual image patch similarity (LPIPS) metrics.",
          "citing_paper_id": "271500033",
          "cited_paper_id": 218470249,
          "context_text": "…peak signal-to-noise ratio (PSNR) and structural similarity index (SSIM) [Wang et al. , 2004] for all datasets, and additionally, adopt the mean absolute error (MAE) and learned perceptual image patch similarity (LPIPS) [Zhang et al. , 2018] for the DPDD [Abuolaim and Brown, 2020] dataset.",
          "confidence_score": 1.0,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions the DPDD dataset, which is used for defocus deblurring. The dataset is specifically referenced in the context of evaluating image restoration methods using various metrics.",
          "citing_paper_doi": "10.24963/ijcai.2024/80",
          "cited_paper_doi": "10.1007/978-3-030-58607-2_7",
          "citing_paper_url": "https://www.semanticscholar.org/paper/371579255875a7d7726e3402158dd2eb79307f7c",
          "cited_paper_url": "https://www.semanticscholar.org/paper/3fbba4f6ec7396721ae52d1ffb6ab67cea83c07b",
          "citing_paper_year": 2024,
          "cited_paper_year": 2020
        },
        {
          "dataset_name": "DPDD",
          "dataset_description": "Used for image dehazing comparisons in nighttime scenes, evaluating various methods including the proposed CSNet. The dataset focuses on nighttime image restoration tasks. | Used to evaluate image restoration methods, specifically focusing on dehazing performance using metrics like MAE and LPIPS.",
          "citing_paper_id": "271500033",
          "cited_paper_id": 17319211,
          "context_text": "We measure the widely-used peak signal-to-noise ratio (PSNR) and structural similarity index (SSIM) [Wang et al. , 2004] for all datasets, and additionally, adopt the mean absolute error (MAE) and learned perceptual image patch similarity (LPIPS) [Zhang et al. , 2018] for the DPDD [Abuolaim and Brown, 2020] dataset.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The citation mentions the DPDD dataset, which is used for evaluating image restoration methods. The other terms mentioned are metrics, not datasets.",
          "citing_paper_doi": "10.24963/ijcai.2024/80",
          "cited_paper_doi": "10.1109/ICCV.2015.34",
          "citing_paper_url": "https://www.semanticscholar.org/paper/371579255875a7d7726e3402158dd2eb79307f7c",
          "cited_paper_url": "https://www.semanticscholar.org/paper/ff62b041e26705dcaab686d98ef09e6102805f11",
          "citing_paper_year": 2024,
          "cited_paper_year": 2015
        },
        {
          "dataset_name": "DPDD",
          "dataset_description": "Used to train networks for defocus deblurring, focusing on dual-pixel data to improve image restoration quality. | Used for evaluating defocus deblurring using dual-pixel data, providing a diverse set of images to test algorithm robustness. | Used for defocus deblurring experiments, providing light field data from a Lytro Illum camera. | Used to train networks for defocus deblurring, capturing sequential images with different aperture sizes to create defocused and all-in-focus image pairs. | Used to capture defocus and all-in-focus pairs for defocus deblurring, enhancing image restoration techniques with dual-pixel data. | Used for defocus deblurring experiments, providing dual-pixel data from a Google Pixel 4 smartphone. | Used for evaluating defocus deblurring using dual-pixel data, similar to DPDD but with a different set of images. | Used for evaluating defocus deblurring methods, focusing on synthetic and real-world images to assess algorithm performance. | Used for evaluating defocus deblurring on real-world images, providing a practical testbed for algorithm validation. | Used to demonstrate cross correlation between defocused and all-in-focus image pairs, focusing on dual-pixel data for defocus deblurring. | Used for defocus deblurring experiments, providing real-world defocused images from a Sony α7R IV camera. | Used for defocus deblurring experiments, providing dual-pixel data from internet sources. | Used for evaluating light field-based defocus deblurring, assessing the effectiveness of algorithms in handling depth-of-field effects. | Used for defocus deblurring experiments, providing images captured with a Canon EOS 5D Mark IV camera. | This dataset 'LFDOF dataset' was mentioned in the citation context but no detailed description was generated.",
          "citing_paper_id": "247922539",
          "cited_paper_id": 218470249,
          "context_text": "Cross correlation between defocused and all-in-focus image pair in DPDD [2] (top) and LFDOF dataset [29] (bottom).",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions two specific datasets, DPDD and LFDOF, which are used to show cross correlation between defocused and all-in-focus image pairs.",
          "citing_paper_doi": "10.1109/CVPR52688.2022.01582",
          "cited_paper_doi": "10.1007/978-3-030-58607-2_7",
          "citing_paper_url": "https://www.semanticscholar.org/paper/60be91a493530eec8cf969bce80f59af0f15b6b4",
          "cited_paper_url": "https://www.semanticscholar.org/paper/3fbba4f6ec7396721ae52d1ffb6ab67cea83c07b",
          "citing_paper_year": 2022,
          "cited_paper_year": 2020
        },
        {
          "dataset_name": "DPDD",
          "dataset_description": "Used to evaluate and compare the performance of various image restoration algorithms, specifically focusing on defocus deblurring techniques. | Used to verify the effectiveness of the proposed network for single-image de-focus deblurring, comparing results with 7 representative algorithms.",
          "citing_paper_id": "268678266",
          "cited_paper_id": 235703231,
          "context_text": "We verify the effectiveness of the proposed network for single-image de-focus deblurring using the widely used DPDD (Abuolaim and Brown 2020) dataset, and compare the results with 7 representative algorithms: DPDNet (Abuolaim and Brown 2020), KPAC (Son et al. 2021), DRBNet (Ruan et al. 2022), IFAN (Lee et al. 2021), MDP (Abuolaim, Afifi, and Brown 2022), Restormer (Zamir et al. 2022), and LaKDNet (Ruan et al. 2023).",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions the DPDD dataset, which is used to verify the effectiveness of the proposed network for single-image de-focus deblurring. The dataset is widely used and is relevant to the research topic.",
          "citing_paper_doi": "10.1609/aaai.v38i2.27907",
          "cited_paper_doi": "10.1109/CVPR46437.2021.00207",
          "citing_paper_url": "https://www.semanticscholar.org/paper/d5a17832f115c16d4097ea39f204917734f9fba0",
          "cited_paper_url": "https://www.semanticscholar.org/paper/fc79ea4a0895078f3958f697c075d347d57b4c8a",
          "citing_paper_year": 2024,
          "cited_paper_year": 2021
        },
        {
          "dataset_name": "DPDD",
          "dataset_description": "Used to evaluate defocus deblurring methods, specifically comparing performance on 37 indoor and 39 outdoor scenes. | Used to evaluate image fidelity scores of different defocus deblurring methods, focusing on performance metrics and comparative analysis. | Used for qualitative evaluation of the TLC method for dual-pixel defocus deblurring, focusing on image restoration quality and performance.",
          "citing_paper_id": "250451699",
          "cited_paper_id": 218470249,
          "context_text": "5: Qualitative evaluation of our TLC for Dual-pixel defocus deblurring on the DPDD dataset [1].",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions the 'DPDD dataset' which is a specific dataset used for evaluating defocus deblurring techniques. The dataset is clearly identified and used for qualitative evaluation.",
          "citing_paper_doi": "10.1007/978-3-031-20071-7_4",
          "cited_paper_doi": "10.1007/978-3-030-58607-2_7",
          "citing_paper_url": "https://www.semanticscholar.org/paper/6f1cd5055f85ed15e7b61f599946a934c25d39b6",
          "cited_paper_url": "https://www.semanticscholar.org/paper/3fbba4f6ec7396721ae52d1ffb6ab67cea83c07b",
          "citing_paper_year": 2021,
          "cited_paper_year": 2020
        },
        {
          "dataset_name": "DPDD",
          "dataset_description": "Used to evaluate the performance of the model in single-image defocus deblurring, specifically comparing PSNR gains and parameter efficiency against the Restormer model. | Used for evaluating image defocus deblurring methods, providing dual-pixel data for consistent performance assessment. | Used to evaluate image defocus deblurring methods, focusing on the relationship between the number of parameters and PSNR. The dataset provides dual-pixel data for training and testing. | Used to verify the effectiveness of the network for single-image defocus deblurring, focusing on the performance of the model in removing defocus blur.",
          "citing_paper_id": "271274210",
          "cited_paper_id": 218470249,
          "context_text": "(b ) The number of parameters vs. PSNR on the DPDD [4] dataset for image defocus deblurring.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The citation mentions the DPDD dataset, which is used for evaluating image defocus deblurring. The dataset is clearly identified and relevant to the research topic.",
          "citing_paper_doi": "10.1109/TCSVT.2024.3429557",
          "cited_paper_doi": "10.1007/978-3-030-58607-2_7",
          "citing_paper_url": "https://www.semanticscholar.org/paper/1091511c40eebb0f8e964246c91e91a85480dbf9",
          "cited_paper_url": "https://www.semanticscholar.org/paper/3fbba4f6ec7396721ae52d1ffb6ab67cea83c07b",
          "citing_paper_year": 2024,
          "cited_paper_year": 2020
        },
        {
          "dataset_name": "DPDD",
          "dataset_description": "Used for evaluating image defocus deblurring methods, providing dual-pixel data for consistent performance assessment. | Used to evaluate the performance of the model in single-image defocus deblurring, specifically comparing PSNR gains and parameter efficiency against the Restormer model.",
          "citing_paper_id": "271274210",
          "cited_paper_id": 244346144,
          "context_text": "For single-image defocus deblurring, our model achieves a performance gain of 0.2 dB PSNR over the strong Trans-former model Restormer [1] in the combined category of the DPDD [4] dataset, while consuming 46% fewer parameters.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions the DPDD dataset, which is used for evaluating the performance of the model in single-image defocus deblurring. The dataset is clearly identified and used for benchmarking.",
          "citing_paper_doi": "10.1109/TCSVT.2024.3429557",
          "cited_paper_doi": "10.1109/CVPR52688.2022.00564",
          "citing_paper_url": "https://www.semanticscholar.org/paper/1091511c40eebb0f8e964246c91e91a85480dbf9",
          "cited_paper_url": "https://www.semanticscholar.org/paper/1e88d5afe19aea324d33541f60a90b7036894c32",
          "citing_paper_year": 2024,
          "cited_paper_year": 2021
        },
        {
          "dataset_name": "DPDD",
          "dataset_description": "Used for evaluating image defocus deblurring methods, providing dual-pixel data for consistent performance assessment.",
          "citing_paper_id": "271274210",
          "cited_paper_id": 247922539,
          "context_text": "3) Image Defocus Deblurring: Consistent with [1] and [9], we utilize DPDD [4] for evaluation.",
          "confidence_score": 1.0,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "DPDD is identified as a dataset used for evaluation in the context of image defocus deblurring. The cited paper 'Defocus Deblurring Using Dual-Pixel Data' confirms DPDD as a dataset.",
          "citing_paper_doi": "10.1109/TCSVT.2024.3429557",
          "cited_paper_doi": "10.1109/CVPR52688.2022.01582",
          "citing_paper_url": "https://www.semanticscholar.org/paper/1091511c40eebb0f8e964246c91e91a85480dbf9",
          "cited_paper_url": "https://www.semanticscholar.org/paper/60be91a493530eec8cf969bce80f59af0f15b6b4",
          "citing_paper_year": 2024,
          "cited_paper_year": 2022
        },
        {
          "dataset_name": "DPDD",
          "dataset_description": "Used for quantitative comparisons in defocus deblurring, specifically evaluating the performance of image restoration methods using dual-pixel data. | Used to evaluate the performance of the network on the defocus deblurring problem, specifically comparing PSNR gains over Restormer. | Used for training, validation, and testing in defocus deblurring, consisting of 350 scenes for training, 74 scenes for validation, and 76 scenes for testing.",
          "citing_paper_id": "265537221",
          "cited_paper_id": 218470249,
          "context_text": "Our network also shows potential on defocus deblurring problem by producing a performance gain of 0.2 dB PSNR over Restormer [61] on the combined category of DPDD [1] dataset.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The citation mentions the DPDD dataset, which is used to evaluate the performance of the network on the defocus deblurring problem.",
          "citing_paper_doi": "10.1109/ICCV51070.2023.01195",
          "cited_paper_doi": "10.1007/978-3-030-58607-2_7",
          "citing_paper_url": "https://www.semanticscholar.org/paper/1e277206e1a59e7e22f1fdcab886d3869cdd0871",
          "cited_paper_url": "https://www.semanticscholar.org/paper/3fbba4f6ec7396721ae52d1ffb6ab67cea83c07b",
          "citing_paper_year": 2023,
          "cited_paper_year": 2020
        },
        {
          "dataset_name": "DPDD",
          "dataset_description": "Used for training, validation, and testing in defocus deblurring, consisting of 350 scenes for training, 74 scenes for validation, and 76 scenes for testing.",
          "citing_paper_id": "265537221",
          "cited_paper_id": 235703231,
          "context_text": "Consistent with previous algorithms [25, 44, 61], we use the DPDD [1] dataset that consists of 350 scenes for training, 74 scenes for validation, and 76 scenes for testing.",
          "confidence_score": 1.0,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context explicitly mentions the DPDD dataset, which is used for training, validation, and testing in the context of defocus deblurring.",
          "citing_paper_doi": "10.1109/ICCV51070.2023.01195",
          "cited_paper_doi": "10.1109/CVPR46437.2021.00207",
          "citing_paper_url": "https://www.semanticscholar.org/paper/1e277206e1a59e7e22f1fdcab886d3869cdd0871",
          "cited_paper_url": "https://www.semanticscholar.org/paper/fc79ea4a0895078f3958f697c075d347d57b4c8a",
          "citing_paper_year": 2023,
          "cited_paper_year": 2021
        }
      ]
    },
    {
      "cited_paper_id": "265537221",
      "citation_count": 0,
      "total_dataset_mentions": 11,
      "unique_datasets": [
        "CSD"
      ],
      "dataset_details": [
        {
          "dataset_name": "CSD",
          "dataset_description": "Used to compare desnowing performance, focusing on snow removal effectiveness and image quality metrics.",
          "citing_paper_id": "270736284",
          "cited_paper_id": 837707,
          "context_text": "2) Image Desnowing: We compare desnowing performance on three widely-adopted datasets: CSD [28], SRRS [86], and Snow100K [44].",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions three specific datasets used for comparing desnowing performance. These datasets are clearly named and relevant to the research topic of image restoration.",
          "citing_paper_doi": "10.1109/TPAMI.2024.3419007",
          "cited_paper_doi": "10.1109/TIP.2018.2806202",
          "citing_paper_url": "https://www.semanticscholar.org/paper/804d2302880b91965e84a7c72ac26429d26792a0",
          "cited_paper_url": "https://www.semanticscholar.org/paper/9e7487d7396acf298f972f0b8ab0397e0d9e7a0f",
          "citing_paper_year": 2024,
          "cited_paper_year": 2017
        },
        {
          "dataset_name": "CSD",
          "dataset_description": "Used to compare desnowing performance, focusing on snow removal effectiveness and image quality metrics.",
          "citing_paper_id": "270736284",
          "cited_paper_id": 226308542,
          "context_text": "2) Image Desnowing: We compare desnowing performance on three widely-adopted datasets: CSD [28], SRRS [86], and Snow100K [44].",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions three specific datasets used for comparing desnowing performance. These datasets are clearly named and relevant to the research topic of image restoration.",
          "citing_paper_doi": "10.1109/TPAMI.2024.3419007",
          "cited_paper_doi": "10.1007/978-3-030-58589-1_45",
          "citing_paper_url": "https://www.semanticscholar.org/paper/804d2302880b91965e84a7c72ac26429d26792a0",
          "cited_paper_url": "https://www.semanticscholar.org/paper/574b568fd5a4a33dcfd47a14b253a4b35acb95ee",
          "citing_paper_year": 2024,
          "cited_paper_year": 2020
        },
        {
          "dataset_name": "CSD",
          "dataset_description": "Used for training and evaluating image desnowing algorithms, providing a large-scale synthetic dataset for benchmarking. | Used for training and evaluating image desnowing algorithms, emphasizing real-world scenarios and robustness. | Used for training and evaluating image desnowing algorithms, focusing on context-aware deep network performance.",
          "citing_paper_id": "261031011",
          "cited_paper_id": 837707,
          "context_text": "Image Desnowing utilizes three datasets, i.e. , CSD [277], Snow100k [278], and SRRS [279].",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions three specific datasets used for image desnowing, which are relevant to the topic of image restoration.",
          "citing_paper_doi": "10.48550/arXiv.2308.09388",
          "cited_paper_doi": "10.1109/TIP.2018.2806202",
          "citing_paper_url": "https://www.semanticscholar.org/paper/70ba95384664ab7becfd8196d20a46fe66112c91",
          "cited_paper_url": "https://www.semanticscholar.org/paper/9e7487d7396acf298f972f0b8ab0397e0d9e7a0f",
          "citing_paper_year": 2023,
          "cited_paper_year": 2017
        },
        {
          "dataset_name": "CSD",
          "dataset_description": "Used to evaluate the performance of the proposed model on the image desnowing task, comparing it against the recent algorithm IRNeXt using PSNR as the metric.",
          "citing_paper_id": "268678266",
          "cited_paper_id": 260927679,
          "context_text": "Furthermore, the proposed model also represents the strong capability on the image desnowing task, outperforming the recent algo-rithm IRNeXt (Cui et al. 2023c) by 0.7 dB PSNR on the CSD (Chen et al. 2021b) dataset.",
          "confidence_score": 1.0,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions the CSD dataset, which is used to evaluate the performance of the proposed model on the image desnowing task.",
          "citing_paper_doi": "10.1609/aaai.v38i2.27907",
          "cited_paper_doi": null,
          "citing_paper_url": "https://www.semanticscholar.org/paper/d5a17832f115c16d4097ea39f204917734f9fba0",
          "cited_paper_url": "https://www.semanticscholar.org/paper/3fb08a1c07eba3461d6ed784b25d952979e90e0d",
          "citing_paper_year": 2024,
          "cited_paper_year": 2023
        },
        {
          "dataset_name": "CSD",
          "dataset_description": "Used to evaluate the performance of the PromptRestorer method in image desnowing, comparing it with existing state-of-the-art methods. | Used to evaluate the performance of the proposed desnowing algorithm, specifically measuring PSNR improvements over state-of-the-art methods.",
          "citing_paper_id": "268030812",
          "cited_paper_id": 244461440,
          "context_text": "05 dB PSNR improvement over the state-of-the-art approach [101] on the CSD benchmark [15].",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The citation mentions the CSD benchmark, which is a specific dataset used for evaluating image restoration methods. The context indicates it is used to measure performance improvements.",
          "citing_paper_doi": null,
          "cited_paper_doi": "10.1109/ICCV48922.2021.00416",
          "citing_paper_url": "https://www.semanticscholar.org/paper/caabf4abb96ee83f650aa7b707bd9a99007b3e74",
          "cited_paper_url": "https://www.semanticscholar.org/paper/51b09bddafff606138fe3e69acb9121f3415aad9",
          "citing_paper_year": 2023,
          "cited_paper_year": 2021
        },
        {
          "dataset_name": "CSD",
          "dataset_description": "Used to evaluate the performance of IRNeXt in image restoration, specifically comparing it to TransWeather. The dataset focuses on images degraded by adverse weather conditions.",
          "citing_paper_id": "260927679",
          "cited_paper_id": 244714491,
          "context_text": "In particular, on the lately proposed CSD dataset, IRNeXt yields a 5.53 dB improvement over TransWeather (Valanarasu et al., 2022).",
          "confidence_score": 1.0,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions the 'CSD dataset' which is a specific dataset used for evaluating image restoration performance. The dataset is used to compare the performance of IRNeXt against TransWeather.",
          "citing_paper_doi": null,
          "cited_paper_doi": "10.1109/CVPR52688.2022.00239",
          "citing_paper_url": "https://www.semanticscholar.org/paper/3fb08a1c07eba3461d6ed784b25d952979e90e0d",
          "cited_paper_url": "https://www.semanticscholar.org/paper/b27d3be4264dcd06f990b44968f4382526f24f1e",
          "citing_paper_year": 2023,
          "cited_paper_year": 2021
        },
        {
          "dataset_name": "CSD",
          "dataset_description": "Used to evaluate the desnowing performance of the proposed model, comparing it with MSP-Former. The dataset focuses on single image desnowing, providing a benchmark for performance measurement.",
          "citing_paper_id": "260927679",
          "cited_paper_id": 250450940,
          "context_text": "Furthermore, compared with MSP-Former (Chen et al., 2022), which is elaborately designed for desnowing, our model shows a significant performance boost of 3.54 dB on the CSD dataset.",
          "confidence_score": 1.0,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions the CSD dataset, which is used to evaluate the performance of the model in desnowing images. The dataset is specific and relevant to the research topic of image restoration.",
          "citing_paper_doi": null,
          "cited_paper_doi": "10.1109/ICASSP49357.2023.10095605",
          "citing_paper_url": "https://www.semanticscholar.org/paper/3fb08a1c07eba3461d6ed784b25d952979e90e0d",
          "cited_paper_url": "https://www.semanticscholar.org/paper/a2c20b0af7ee20727d92e66629183e3e5ae41d99",
          "citing_paper_year": 2023,
          "cited_paper_year": 2022
        },
        {
          "dataset_name": "CSD",
          "dataset_description": "Used to compare the proposed method's performance with existing state-of-the-art methods in image restoration, focusing on quantitative and qualitative evaluations.",
          "citing_paper_id": "259298517",
          "cited_paper_id": 248085491,
          "context_text": "We compare our method on the CSD (Chen et al., 2021b) dataset with existing state-of-the-art methods (Chen et al., 2022a; Valanarasu et al., 2022; Chen et al., 2022c).",
          "confidence_score": 1.0,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions the CSD dataset, which is used to compare the performance of the proposed method against existing state-of-the-art methods in image restoration.",
          "citing_paper_doi": null,
          "cited_paper_doi": "10.48550/arXiv.2204.04676",
          "citing_paper_url": "https://www.semanticscholar.org/paper/6af728164e4424218967fd356b3448f5037acd88",
          "cited_paper_url": "https://www.semanticscholar.org/paper/7d4c2c8407e0caf2f907df9954b056a42a92fd13",
          "citing_paper_year": 2023,
          "cited_paper_year": 2022
        },
        {
          "dataset_name": "CSD",
          "dataset_description": "Used to evaluate the performance of CSNet in image restoration, specifically measuring PSNR improvement over FocalNet. The dataset is relevant for assessing desnowing algorithms. | Used to evaluate the performance of image restoration algorithms, specifically measuring PSNR gain over the FocalNet algorithm.",
          "citing_paper_id": "271500033",
          "cited_paper_id": 244461440,
          "context_text": "The numerical scores on the CSD [Chen et al. , 2021] and Snow100K [Liu et al. , 2018] are presented in PSNR gain over the FocalNet [ Cui et al. , 2023a ] algorithm.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions two datasets, CSD and Snow100K, which are used to evaluate the performance of image restoration algorithms. The datasets are used to measure PSNR gain over the FocalNet algorithm.",
          "citing_paper_doi": "10.24963/ijcai.2024/80",
          "cited_paper_doi": "10.1109/ICCV48922.2021.00416",
          "citing_paper_url": "https://www.semanticscholar.org/paper/371579255875a7d7726e3402158dd2eb79307f7c",
          "cited_paper_url": "https://www.semanticscholar.org/paper/51b09bddafff606138fe3e69acb9121f3415aad9",
          "citing_paper_year": 2024,
          "cited_paper_year": 2021
        },
        {
          "dataset_name": "CSD",
          "dataset_description": "Used to assess the performance of the proposed method in haze removal, demonstrating a 0.10 dB PSNR improvement over FocalNet. | Used for image dehazing comparisons, specifically evaluating nighttime image restoration performance using various methods. | Used to evaluate the effectiveness of the proposed method in removing real-world haze degradations, comparing performance gains in PSNR against FocalNet. | Used to evaluate defocus deblurring performance, comparing PSNR gains between CSNet and FocalNet in the combined category. | Used to evaluate the performance of CSNet in image restoration, specifically measuring PSNR improvement over FocalNet. The dataset is relevant for assessing desnowing algorithms. | Used to evaluate the performance of image restoration algorithms, specifically measuring PSNR gain over the FocalNet algorithm.",
          "citing_paper_id": "271500033",
          "cited_paper_id": 265537221,
          "context_text": "The numerical scores on the CSD [Chen et al. , 2021] and Snow100K [Liu et al. , 2018] are presented in PSNR gain over the FocalNet [ Cui et al. , 2023a ] algorithm.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions two datasets, CSD and Snow100K, which are used to evaluate the performance of image restoration algorithms. The datasets are used to measure PSNR gain over the FocalNet algorithm.",
          "citing_paper_doi": "10.24963/ijcai.2024/80",
          "cited_paper_doi": "10.1109/ICCV51070.2023.01195",
          "citing_paper_url": "https://www.semanticscholar.org/paper/371579255875a7d7726e3402158dd2eb79307f7c",
          "cited_paper_url": "https://www.semanticscholar.org/paper/1e277206e1a59e7e22f1fdcab886d3869cdd0871",
          "citing_paper_year": 2024,
          "cited_paper_year": 2023
        },
        {
          "dataset_name": "CSD",
          "dataset_description": "Used to evaluate the performance of the small model in desnowing images, comparing it to MSP-Former using PSNR as the metric.",
          "citing_paper_id": "270736284",
          "cited_paper_id": 250450940,
          "context_text": "Furthermore,comparedwithMSP-Former[50],whichiselaborately designedfordesnowing,oursmallmodelshowsasigniﬁcant performance boost of 4.68 dB PSNR on the CSD dataset.",
          "confidence_score": 1.0,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions the CSD dataset, which is used to evaluate the performance of the small model compared to MSP-Former in terms of desnowing image restoration.",
          "citing_paper_doi": "10.1109/TPAMI.2024.3419007",
          "cited_paper_doi": "10.1109/ICASSP49357.2023.10095605",
          "citing_paper_url": "https://www.semanticscholar.org/paper/804d2302880b91965e84a7c72ac26429d26792a0",
          "cited_paper_url": "https://www.semanticscholar.org/paper/a2c20b0af7ee20727d92e66629183e3e5ae41d99",
          "citing_paper_year": 2024,
          "cited_paper_year": 2022
        }
      ]
    },
    {
      "cited_paper_id": "219530930",
      "citation_count": 0,
      "total_dataset_mentions": 10,
      "unique_datasets": [
        "SOTS"
      ],
      "dataset_details": [
        {
          "dataset_name": "SOTS",
          "dataset_description": "Used for testing in the image dehazing task, consisting of 500 outdoor images to evaluate dehazing algorithms.",
          "citing_paper_id": "270869793",
          "cited_paper_id": 39760169,
          "context_text": "In the image dehazing task, we utilize the standard outdoor test set of the SOTS [25] dataset, which consists of 500 images for testing.",
          "confidence_score": 1.0,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions the SOTS dataset, which is used for testing in the image dehazing task. The dataset is clearly identified and its use is specific to the research context.",
          "citing_paper_doi": "10.48550/arXiv.2407.00676",
          "cited_paper_doi": "10.1109/TIP.2018.2867951",
          "citing_paper_url": "https://www.semanticscholar.org/paper/ca95c6148941dea8e67f05146dded1f43e6b8f8a",
          "cited_paper_url": "https://www.semanticscholar.org/paper/abf75b81b96ff2cd262086675deba55f907108db",
          "citing_paper_year": 2024,
          "cited_paper_year": 2017
        },
        {
          "dataset_name": "SOTS",
          "dataset_description": "Used for dehazing experiments, specifically evaluating single-image dehazing methods and their performance on synthetic and real-world images.",
          "citing_paper_id": "271874811",
          "cited_paper_id": 39760169,
          "context_text": "Dehazing experiments utilize the SOTS dataset [54].",
          "confidence_score": 1.0,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context clearly mentions the use of the SOTS dataset for dehazing experiments, which is directly relevant to the topic of image restoration.",
          "citing_paper_doi": "10.48550/arXiv.2408.08091",
          "cited_paper_doi": "10.1109/TIP.2018.2867951",
          "citing_paper_url": "https://www.semanticscholar.org/paper/99facb268ce89958bcd541076286efac30652b46",
          "cited_paper_url": "https://www.semanticscholar.org/paper/abf75b81b96ff2cd262086675deba55f907108db",
          "citing_paper_year": 2024,
          "cited_paper_year": 2017
        },
        {
          "dataset_name": "SOTS",
          "dataset_description": "Employed for image deraining, focusing on the effectiveness of deraining methods on synthetic rain images with varying intensities. | Used for image dehazing, specifically evaluating the performance of dehazing algorithms on synthetic and real-world hazy images.",
          "citing_paper_id": "278740510",
          "cited_paper_id": 219530930,
          "context_text": "For image dehazing, we use the SOTS [20] dataset, while Rain100L [53] is employed for image deraining.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions two specific datasets, SOTS and Rain100L, which are used for image dehazing and deraining respectively.",
          "citing_paper_doi": "10.48550/arXiv.2505.12630",
          "cited_paper_doi": "10.1109/cvpr42600.2020.00583",
          "citing_paper_url": "https://www.semanticscholar.org/paper/18b73e3db636e6ded820bc0b5b5ad13993e287bc",
          "cited_paper_url": "https://www.semanticscholar.org/paper/c209d9c0d49b2377860acad2acbcc13523a40b7f",
          "citing_paper_year": 2025,
          "cited_paper_year": 2020
        },
        {
          "dataset_name": "SOTS",
          "dataset_description": "Used to train a model for realistic single image dehazing, focusing on indoor scenes to improve clarity and visual quality. | Used for testing single-image dehazing algorithms, containing 500 indoor and outdoor hazy images to evaluate restoration performance.",
          "citing_paper_id": "235689508",
          "cited_paper_id": 39760169,
          "context_text": "For testing, the synthetic objective testing set (SOTS) in RESIDE is used, which contains 500 indoor and outdoor hazy images.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions a specific dataset, SOTS, which is part of the RESIDE collection. It is used for testing dehazing algorithms.",
          "citing_paper_doi": "10.1117/1.JEI.30.3.033026",
          "cited_paper_doi": "10.1109/TIP.2018.2867951",
          "citing_paper_url": "https://www.semanticscholar.org/paper/92f6d5a2caf91b168afd2f87de17c65393301246",
          "cited_paper_url": "https://www.semanticscholar.org/paper/abf75b81b96ff2cd262086675deba55f907108db",
          "citing_paper_year": 2021,
          "cited_paper_year": 2017
        },
        {
          "dataset_name": "SOTS",
          "dataset_description": "Used to compare the performance of all-in-one image dehazing methods, focusing on the effectiveness of different approaches in removing haze from images. | Used to evaluate dehazing algorithms in a single-task setting, focusing on image quality and restoration performance. | Used to compare the performance of all-in-one image dehazing methods, focusing on the effectiveness of different techniques in removing haze from images. | Used for benchmarking single-image dehazing methods, featuring 72,135 training and 500 testing images to evaluate dehazing algorithms.",
          "citing_paper_id": "268856875",
          "cited_paper_id": 39760169,
          "context_text": "Table 2: Dehazing results in the single-task setting on the SOTS benchmark dataset [25].",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions the SOTS benchmark dataset, which is a specific dataset used for evaluating dehazing algorithms.",
          "citing_paper_doi": "10.48550/arXiv.2404.02154",
          "cited_paper_doi": "10.1109/TIP.2018.2867951",
          "citing_paper_url": "https://www.semanticscholar.org/paper/1d99d28c3cf5a21671a9cf5de8b51009c72d4624",
          "cited_paper_url": "https://www.semanticscholar.org/paper/abf75b81b96ff2cd262086675deba55f907108db",
          "citing_paper_year": 2024,
          "cited_paper_year": 2017
        },
        {
          "dataset_name": "SOTS",
          "dataset_description": "Used to test the performance of dehazing models, providing real-world hazy and haze-free outdoor images for evaluation. | Used to train dehazing models, focusing on synthetic hazy images to improve model performance.",
          "citing_paper_id": "269605449",
          "cited_paper_id": 206598041,
          "context_text": "Then we train the dehazing models on the synthetic SOTS dataset and then test their performances on the real hazy dataset O-HAZE (Ancuti et al., 2018).",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions two specific datasets, SOTS and O-HAZE, which are used for training and testing dehazing models, respectively.",
          "citing_paper_doi": "10.48550/arXiv.2405.02843",
          "cited_paper_doi": "10.1109/CVPRW.2018.00119",
          "citing_paper_url": "https://www.semanticscholar.org/paper/0c7542272e5edcff29608a0e37f9a3b705b363e7",
          "cited_paper_url": "https://www.semanticscholar.org/paper/fb07f990b8247b46fec6dcd2a065a5f7e553db39",
          "citing_paper_year": 2024,
          "cited_paper_year": 2018
        },
        {
          "dataset_name": "SOTS",
          "dataset_description": "Used to evaluate the performance of the dehazing model, specifically comparing PSNR scores against a Transformer-based method with similar computational overhead. | Used for testing the proposed method in single-image dehazing experiments, evaluating performance and robustness. | Used to benchmark image dehazing algorithms, specifically comparing CSNet with state-of-the-art methods. The dataset evaluates performance in removing haze from single images. | Used to assess dehazing performance on real-world dense haze conditions, focusing on image restoration quality. | Used to test dehazing algorithms on real-world natural hazy images, emphasizing realistic scenarios. | Used for training the proposed method in single-image dehazing experiments, focusing on improving image quality and clarity. | Used to evaluate dehazing methods on synthetic daytime scenes, providing a controlled environment for benchmarking. | Used to evaluate dehazing techniques on real-world outdoor hazy images, assessing their effectiveness in diverse conditions.",
          "citing_paper_id": "271500033",
          "cited_paper_id": 39760169,
          "context_text": "For daytime scenes, the numerical results on a synthetic (SOTS [Li et al. , 2018]) and three real-world datasets, i.e., Dense-Haze [Ancuti et al. , 2019], NH-HAZE [Ancuti et al. , 2020], and O-HAZE [An-cuti et al. , 2018], are presented in Table 1.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions specific datasets used for evaluating dehazing methods in both synthetic and real-world scenarios.",
          "citing_paper_doi": "10.24963/ijcai.2024/80",
          "cited_paper_doi": "10.1109/TIP.2018.2867951",
          "citing_paper_url": "https://www.semanticscholar.org/paper/371579255875a7d7726e3402158dd2eb79307f7c",
          "cited_paper_url": "https://www.semanticscholar.org/paper/abf75b81b96ff2cd262086675deba55f907108db",
          "citing_paper_year": 2024,
          "cited_paper_year": 2017
        },
        {
          "dataset_name": "SOTS",
          "dataset_description": "Used to evaluate the performance of the dehazing model, specifically comparing PSNR scores against a Transformer-based method with similar computational overhead. | Used to evaluate the performance of CSNet against MB-TaylorFormer-B in image dehazing, focusing on PSNR improvement with similar computational overhead.",
          "citing_paper_id": "271500033",
          "cited_paper_id": 261242635,
          "context_text": "For dehazing, our model outperforms the recent Transformer-based method, MB-TaylorFormer-B [Qiu et al. , 2023] by 0.63 dB PSNR on the widely-used SOTS [Li et al. , 2018] dataset with sim-9 ilar computation overhead, as illustrated in Figure 1.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions the SOTS dataset, which is a widely-used dataset for single-image dehazing. The dataset is used to evaluate the performance of the model in terms of PSNR.",
          "citing_paper_doi": "10.24963/ijcai.2024/80",
          "cited_paper_doi": "10.1109/ICCV51070.2023.01176",
          "citing_paper_url": "https://www.semanticscholar.org/paper/371579255875a7d7726e3402158dd2eb79307f7c",
          "cited_paper_url": "https://www.semanticscholar.org/paper/2579d8b45e51513c58c311adfbb7face54db12e0",
          "citing_paper_year": 2024,
          "cited_paper_year": 2023
        },
        {
          "dataset_name": "SOTS",
          "dataset_description": "Used to benchmark single-image dehazing methods, evaluating performance on synthetic and real-world hazy images.",
          "citing_paper_id": "274305801",
          "cited_paper_id": 39760169,
          "context_text": "The dehazing task utilises the SOTS [23] dataset.",
          "confidence_score": 1.0,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions the SOTS dataset, which is a specific dataset used for the dehazing task. The dataset is clearly identified and relevant to the research topic.",
          "citing_paper_doi": "10.48550/arXiv.2411.18466",
          "cited_paper_doi": "10.1109/TIP.2018.2867951",
          "citing_paper_url": "https://www.semanticscholar.org/paper/2850f4c1f43789f0a8a48e7bb83cf2a5e0561e54",
          "cited_paper_url": "https://www.semanticscholar.org/paper/abf75b81b96ff2cd262086675deba55f907108db",
          "citing_paper_year": 2024,
          "cited_paper_year": 2017
        },
        {
          "dataset_name": "SOTS",
          "dataset_description": "Used to benchmark single-image dehazing performance, comparing ConvIR-B with MB-TaylorFormer-L in terms of PSNR and computational complexity. | Used to evaluate the performance of ConvIR-B in dehazing outdoor scenes, comparing it to DeHamer with a focus on PSNR and parameter efficiency. | Used for visual comparisons in single-image dehazing, providing high-resolution images to evaluate dehazing performance. | Used for visual comparisons in single-image dehazing, focusing on indoor scenes to evaluate dehazing algorithms. | Used to evaluate the performance of the image dehazing model, focusing on outdoor scenes, comparing PSNR and efficiency against the state-of-the-art method. | Used to test image defocus deblurring methods, analyzing the balance between PSNR and model size. | Used to evaluate the performance of ConvIR-B in dehazing indoor scenes, comparing it to DeHamer with a focus on PSNR and parameter efficiency. | Used for visual comparisons in single-image dehazing, focusing on outdoor scenes to evaluate dehazing algorithms. | Used to evaluate the performance of the image dehazing model, focusing on indoor scenes, comparing PSNR and efficiency against the state-of-the-art method. | Used to evaluate image desnowing algorithms, examining the relationship between PSNR and model parameters. | Used to benchmark single-image dehazing methods, comparing performance metrics of the proposed method against state-of-the-art algorithms. | Used to assess image motion deblurring techniques, specifically comparing PSNR against computational complexity (FLOPs). | Used to evaluate image dehazing methods, focusing on the trade-off between PSNR and computational efficiency (FLOPs).",
          "citing_paper_id": "270736284",
          "cited_paper_id": 39760169,
          "context_text": "For this problem, we ﬁrst compare our methods with state-of-the-art algorithms on the SOTS [27] dataset in Table I.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions the SOTS dataset, which is a specific dataset used for benchmarking single-image dehazing methods.",
          "citing_paper_doi": "10.1109/TPAMI.2024.3419007",
          "cited_paper_doi": "10.1109/TIP.2018.2867951",
          "citing_paper_url": "https://www.semanticscholar.org/paper/804d2302880b91965e84a7c72ac26429d26792a0",
          "cited_paper_url": "https://www.semanticscholar.org/paper/abf75b81b96ff2cd262086675deba55f907108db",
          "citing_paper_year": 2024,
          "cited_paper_year": 2017
        }
      ]
    },
    {
      "cited_paper_id": "39760169",
      "citation_count": 0,
      "total_dataset_mentions": 9,
      "unique_datasets": [
        "Snow100K"
      ],
      "dataset_details": [
        {
          "dataset_name": "Snow100K",
          "dataset_description": "Used to train the 'AllWeather' dataset for raindrop removal, evaluating the effectiveness of the model in handling rain-related degradations. | Used to train the 'AllWeather' dataset for snow removal, focusing on context-aware deep network performance under snowy conditions. | Used to train the 'AllWeather' dataset for outdoor rain scenarios, assessing the model's ability to restore images degraded by rain.",
          "citing_paper_id": "273643057",
          "cited_paper_id": 837707,
          "context_text": "‘rain-haze-snow’setting :Following[31],thetrainingdataset, termed “AllWeather”, includes images from Snow100K [22], Raindrop [28], and Outdoor-Rain [17], encompassing a variety of weather-related degradations.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions specific datasets used for training in the 'rain-haze-snow' setting, which are relevant to the topic of image restoration.",
          "citing_paper_doi": "10.1145/3664647.3680762",
          "cited_paper_doi": "10.1109/TIP.2018.2806202",
          "citing_paper_url": "https://www.semanticscholar.org/paper/f292c903c1571ba0ee97aacdba29b7008b2fe501",
          "cited_paper_url": "https://www.semanticscholar.org/paper/9e7487d7396acf298f972f0b8ab0397e0d9e7a0f",
          "citing_paper_year": 2024,
          "cited_paper_year": 2017
        },
        {
          "dataset_name": "Snow100K",
          "dataset_description": "Used to assess the effectiveness of the model in removing snow from real-world images, emphasizing realistic snow removal scenarios. | Used to test the desnowing model's performance, emphasizing the removal of snow from complex scenes. | Used to evaluate the model's performance on image desnowing, focusing on removing snow from synthetic images with varying levels of snow intensity. | Used to evaluate the model's performance in handling complex snow scenarios, demonstrating the model's effectiveness in addressing intricate snow degradations. | Used to assess the effectiveness of the desnowing model, specifically targeting real-world snow removal scenarios. | Used to evaluate the proposed desnowing model, focusing on removing snow from images using a large-scale synthetic dataset. | Used to test the model's ability to handle diverse snow conditions, including both synthetic and real images, ensuring robust desnowing performance.",
          "citing_paper_id": "271274210",
          "cited_paper_id": 837707,
          "context_text": "2) Image Desnowing Results: We evaluate the proposed model on three widely used datasets for image desnowing, including Snow100K [6], SRRS [58], and CSD [21].",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions three specific datasets used for evaluating an image desnowing model. These datasets are clearly named and relevant to the research topic of all-in-one image restoration.",
          "citing_paper_doi": "10.1109/TCSVT.2024.3429557",
          "cited_paper_doi": "10.1109/TIP.2018.2806202",
          "citing_paper_url": "https://www.semanticscholar.org/paper/1091511c40eebb0f8e964246c91e91a85480dbf9",
          "cited_paper_url": "https://www.semanticscholar.org/paper/9e7487d7396acf298f972f0b8ab0397e0d9e7a0f",
          "citing_paper_year": 2024,
          "cited_paper_year": 2017
        },
        {
          "dataset_name": "Snow100K",
          "dataset_description": "Used to assess the effectiveness of the model in removing snow from real-world images, emphasizing realistic snow removal scenarios. | Used to test the desnowing model's performance, emphasizing the removal of snow from complex scenes. | Used to evaluate the model's performance on image desnowing, focusing on removing snow from synthetic images with varying levels of snow intensity. | Used to evaluate the model's performance in handling complex snow scenarios, demonstrating the model's effectiveness in addressing intricate snow degradations. | Used to assess the effectiveness of the desnowing model, specifically targeting real-world snow removal scenarios. | Used to evaluate the proposed desnowing model, focusing on removing snow from images using a large-scale synthetic dataset. | Used to test the model's ability to handle diverse snow conditions, including both synthetic and real images, ensuring robust desnowing performance.",
          "citing_paper_id": "271274210",
          "cited_paper_id": 226308542,
          "context_text": "2) Image Desnowing Results: We evaluate the proposed model on three widely used datasets for image desnowing, including Snow100K [6], SRRS [58], and CSD [21].",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions three specific datasets used for evaluating an image desnowing model. These datasets are clearly named and relevant to the research topic of all-in-one image restoration.",
          "citing_paper_doi": "10.1109/TCSVT.2024.3429557",
          "cited_paper_doi": "10.1007/978-3-030-58589-1_45",
          "citing_paper_url": "https://www.semanticscholar.org/paper/1091511c40eebb0f8e964246c91e91a85480dbf9",
          "cited_paper_url": "https://www.semanticscholar.org/paper/574b568fd5a4a33dcfd47a14b253a4b35acb95ee",
          "citing_paper_year": 2024,
          "cited_paper_year": 2020
        },
        {
          "dataset_name": "Snow100K",
          "dataset_description": "Used to assess the effectiveness of the model in removing snow from real-world images, emphasizing realistic snow removal scenarios. | Used to test the desnowing model's performance, emphasizing the removal of snow from complex scenes. | Used to evaluate the model's performance on image desnowing, focusing on removing snow from synthetic images with varying levels of snow intensity. | Used to evaluate the performance of the image desnowing task, specifically comparing the proposed model against IRNeXt using PSNR as the metric. | Used to assess the effectiveness of the desnowing model, specifically targeting real-world snow removal scenarios. | Used to evaluate the proposed desnowing model, focusing on removing snow from images using a large-scale synthetic dataset. | Used to test the model's ability to handle diverse snow conditions, including both synthetic and real images, ensuring robust desnowing performance.",
          "citing_paper_id": "271274210",
          "cited_paper_id": 244461440,
          "context_text": "2) Image Desnowing Results: We evaluate the proposed model on three widely used datasets for image desnowing, including Snow100K [6], SRRS [58], and CSD [21].",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions three specific datasets used for evaluating an image desnowing model. These datasets are clearly named and relevant to the research topic of all-in-one image restoration.",
          "citing_paper_doi": "10.1109/TCSVT.2024.3429557",
          "cited_paper_doi": "10.1109/ICCV48922.2021.00416",
          "citing_paper_url": "https://www.semanticscholar.org/paper/1091511c40eebb0f8e964246c91e91a85480dbf9",
          "cited_paper_url": "https://www.semanticscholar.org/paper/51b09bddafff606138fe3e69acb9121f3415aad9",
          "citing_paper_year": 2024,
          "cited_paper_year": 2021
        },
        {
          "dataset_name": "Snow100K",
          "dataset_description": "Used for desnowing benchmarking, comprising 50,000 training images, 50,000 testing images, and 1,329 real-world snow images to evaluate desnowing algorithms.",
          "citing_paper_id": "268513542",
          "cited_paper_id": 837707,
          "context_text": "The Snow100K [30] is used for desnowing benchmark with 50000 training data, 50000 testing data and 1329 real-world snow images.",
          "confidence_score": 1.0,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions a specific dataset, Snow100K, which is used for desnowing benchmarking. The dataset is described with a clear structure including training, testing, and real-world image counts.",
          "citing_paper_doi": "10.1109/CVPR52733.2024.02404",
          "cited_paper_doi": "10.1109/TIP.2018.2806202",
          "citing_paper_url": "https://www.semanticscholar.org/paper/1ff44c75c361d2ed28f868361a29437197b0270c",
          "cited_paper_url": "https://www.semanticscholar.org/paper/9e7487d7396acf298f972f0b8ab0397e0d9e7a0f",
          "citing_paper_year": 2024,
          "cited_paper_year": 2017
        },
        {
          "dataset_name": "Snow100K",
          "dataset_description": "This dataset 'RainDrop dataset' was mentioned in the citation context but no detailed description was generated. | Used to train models for image restoration under snowy conditions, contributing 9,000 images to the mixed dataset. | Used to train models for image restoration under rainy conditions, contributing 1,069 images to the mixed dataset. | This dataset 'Outdoor-Rain100' was mentioned in the citation context but no detailed description was generated. | Used for training and evaluating snow removal algorithms, focusing on synthetic snow images with varying intensities and complexities. | This dataset 'All-in-One' was mentioned in the citation context but no detailed description was generated.",
          "citing_paper_id": "258959359",
          "cited_paper_id": 250551851,
          "context_text": "Following the previous works [27,58,44], we use a mixed dataset called Allweather, in which the training set contains 9, 000 images sampled from Snow100K [39], 1, 069 images from Raindrop [45], and 9, 000 images of Outdoor-Rain [31].",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions specific datasets used for training in the All-In-One Image Restoration task, including Snow100K, Raindrop, and Outdoor-Rain.",
          "citing_paper_doi": "10.48550/arXiv.2305.17863",
          "cited_paper_doi": "10.1109/CVPR52688.2022.01693",
          "citing_paper_url": "https://www.semanticscholar.org/paper/7d8cfd80c9bc7bbfd52b4ee9c9bbc5b7bfd9cc21",
          "cited_paper_url": "https://www.semanticscholar.org/paper/c85f67a49cfdc29a77bd741a59e739cdc814edde",
          "citing_paper_year": 2023,
          "cited_paper_year": 2022
        },
        {
          "dataset_name": "Snow100K",
          "dataset_description": "This dataset 'RainDrop dataset' was mentioned in the citation context but no detailed description was generated. | Used to train models for image restoration under snowy conditions, contributing 9,000 images to the mixed dataset. | Used to train models for image restoration under rainy conditions, contributing 1,069 images to the mixed dataset. | This dataset 'Outdoor-Rain100' was mentioned in the citation context but no detailed description was generated. | This dataset 'All-in-One' was mentioned in the citation context but no detailed description was generated.",
          "citing_paper_id": "258959359",
          "cited_paper_id": 251197000,
          "context_text": "Following the previous works [27,58,44], we use a mixed dataset called Allweather, in which the training set contains 9, 000 images sampled from Snow100K [39], 1, 069 images from Raindrop [45], and 9, 000 images of Outdoor-Rain [31].",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions specific datasets used for training in the All-In-One Image Restoration task, including Snow100K, Raindrop, and Outdoor-Rain.",
          "citing_paper_doi": "10.48550/arXiv.2305.17863",
          "cited_paper_doi": "10.1109/TPAMI.2023.3238179",
          "citing_paper_url": "https://www.semanticscholar.org/paper/7d8cfd80c9bc7bbfd52b4ee9c9bbc5b7bfd9cc21",
          "cited_paper_url": "https://www.semanticscholar.org/paper/69beb616ffdc42afe86b7487c5db82b6d88638b8",
          "citing_paper_year": 2023,
          "cited_paper_year": 2022
        },
        {
          "dataset_name": "Snow100K",
          "dataset_description": "Used to train the method for rain removal, focusing on synthetic rain generation and its impact on image restoration. | Used to test desnowing algorithms, specifically evaluating performance on 50,000 paired images in the test split. | Used to train the method for snow removal, focusing on context-aware deep network performance in synthetic snowy conditions. | Used to train a deep network for snow removal, focusing on synthetic snow images and their corresponding clean images to improve image restoration techniques. | Used to train the method for dehazing, focusing on benchmarking single-image dehazing techniques and their effectiveness in various weather conditions.",
          "citing_paper_id": "272367682",
          "cited_paper_id": 837707,
          "context_text": "For desnowing, we use the test split of Snow100k dataset containing 50 , 000 paired images.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions the Snow100k dataset, which is a specific dataset used for testing desnowing algorithms.",
          "citing_paper_doi": "10.48550/arXiv.2409.00263",
          "cited_paper_doi": "10.1109/TIP.2018.2806202",
          "citing_paper_url": "https://www.semanticscholar.org/paper/1f036467c75a1ba623b528d70254d4110ad9906b",
          "cited_paper_url": "https://www.semanticscholar.org/paper/9e7487d7396acf298f972f0b8ab0397e0d9e7a0f",
          "citing_paper_year": 2024,
          "cited_paper_year": 2017
        },
        {
          "dataset_name": "Snow100K",
          "dataset_description": "Used to train the method for dehazing, focusing on benchmarking single-image dehazing techniques and their effectiveness in various weather conditions. | Used for de-hazing experiments, specifically evaluating models on 500 paired images to assess performance in synthetic outdoor conditions. | Used to train the method for rain removal, focusing on synthetic rain generation and its impact on image restoration. | Used to train the method for snow removal, focusing on context-aware deep network performance in synthetic snowy conditions.",
          "citing_paper_id": "272367682",
          "cited_paper_id": 39760169,
          "context_text": "We use the Snow100k (Liu et al. 2018), synthetic rain (Zamir et al. 2021) datasets (SRD) and RESIDE (Li et al. 2019) to train our method for all-weather restoration.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions specific datasets used for training a method for all-weather restoration, which aligns with the research topic of All-in-One Image Restoration.",
          "citing_paper_doi": "10.48550/arXiv.2409.00263",
          "cited_paper_doi": "10.1109/TIP.2018.2867951",
          "citing_paper_url": "https://www.semanticscholar.org/paper/1f036467c75a1ba623b528d70254d4110ad9906b",
          "cited_paper_url": "https://www.semanticscholar.org/paper/abf75b81b96ff2cd262086675deba55f907108db",
          "citing_paper_year": 2024,
          "cited_paper_year": 2017
        }
      ]
    },
    {
      "cited_paper_id": "232352612",
      "citation_count": 0,
      "total_dataset_mentions": 8,
      "unique_datasets": [
        "WeatherStream"
      ],
      "dataset_details": [
        {
          "dataset_name": "WeatherStream",
          "dataset_description": "Used to train deweathering models, supporting the re-use of pretrained models for downstream tasks such as detection, segmentation, depth prediction, and completion.",
          "citing_paper_id": "259340208",
          "cited_paper_id": 3429309,
          "context_text": "Deweathering models trained on WeatherStream may also support the re-use of pretrained models for downstream tasks such as detection [39, 50], segmentation [5, 36, 52], depth prediction [1, 49, 65], and completion [23, 66].",
          "confidence_score": 1.0,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions 'WeatherStream' as a dataset used for training deweathering models, which supports the re-use of pretrained models for various downstream tasks. No other specific datasets are mentioned.",
          "citing_paper_doi": "10.1109/CVPR52729.2023.01297",
          "cited_paper_doi": "10.1109/TPAMI.2017.2699184",
          "citing_paper_url": "https://www.semanticscholar.org/paper/1b2ec6027043f5696df8c44a8225e9caf0b7bf6d",
          "cited_paper_url": "https://www.semanticscholar.org/paper/cab372bc3824780cce20d9dd1c22d4df39ed081a",
          "citing_paper_year": 2023,
          "cited_paper_year": 2016
        },
        {
          "dataset_name": "WeatherStream",
          "dataset_description": "Used to train deweathering models, supporting the re-use of pretrained models for downstream tasks such as detection, segmentation, depth prediction, and completion.",
          "citing_paper_id": "259340208",
          "cited_paper_id": 218870245,
          "context_text": "Deweathering models trained on WeatherStream may also support the re-use of pretrained models for downstream tasks such as detection [39, 50], segmentation [5, 36, 52], depth prediction [1, 49, 65], and completion [23, 66].",
          "confidence_score": 1.0,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions 'WeatherStream' as a dataset used for training deweathering models, which supports the re-use of pretrained models for various downstream tasks. No other specific datasets are mentioned.",
          "citing_paper_doi": "10.1109/CVPR52729.2023.01297",
          "cited_paper_doi": "10.1007/978-3-030-58586-0_33",
          "citing_paper_url": "https://www.semanticscholar.org/paper/1b2ec6027043f5696df8c44a8225e9caf0b7bf6d",
          "cited_paper_url": "https://www.semanticscholar.org/paper/85fa5c4ee3e4122e2fa95f0b86e32e3eb14375aa",
          "citing_paper_year": 2023,
          "cited_paper_year": 2020
        },
        {
          "dataset_name": "WeatherStream",
          "dataset_description": "Used to train deweathering models, supporting the re-use of pretrained models for downstream tasks such as detection, segmentation, depth prediction, and completion.",
          "citing_paper_id": "259340208",
          "cited_paper_id": 232352612,
          "context_text": "Deweathering models trained on WeatherStream may also support the re-use of pretrained models for downstream tasks such as detection [39, 50], segmentation [5, 36, 52], depth prediction [1, 49, 65], and completion [23, 66].",
          "confidence_score": 1.0,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions 'WeatherStream' as a dataset used for training deweathering models, which supports the re-use of pretrained models for various downstream tasks. No other specific datasets are mentioned.",
          "citing_paper_doi": "10.1109/CVPR52729.2023.01297",
          "cited_paper_doi": "10.1109/ICCV48922.2021.01196",
          "citing_paper_url": "https://www.semanticscholar.org/paper/1b2ec6027043f5696df8c44a8225e9caf0b7bf6d",
          "cited_paper_url": "https://www.semanticscholar.org/paper/8e33914d6051dd031a5e096962b9398fc1d16067",
          "citing_paper_year": 2023,
          "cited_paper_year": 2021
        },
        {
          "dataset_name": "WeatherStream",
          "dataset_description": "Used to train deweathering models, supporting the re-use of pretrained models for downstream tasks such as detection, segmentation, depth prediction, and completion.",
          "citing_paper_id": "259340208",
          "cited_paper_id": 237278155,
          "context_text": "Deweathering models trained on WeatherStream may also support the re-use of pretrained models for downstream tasks such as detection [39, 50], segmentation [5, 36, 52], depth prediction [1, 49, 65], and completion [23, 66].",
          "confidence_score": 1.0,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions 'WeatherStream' as a dataset used for training deweathering models, which supports the re-use of pretrained models for various downstream tasks. No other specific datasets are mentioned.",
          "citing_paper_doi": "10.1109/CVPR52729.2023.01297",
          "cited_paper_doi": "10.1109/ICCV48922.2021.01251",
          "citing_paper_url": "https://www.semanticscholar.org/paper/1b2ec6027043f5696df8c44a8225e9caf0b7bf6d",
          "cited_paper_url": "https://www.semanticscholar.org/paper/f8f4e64cf86f39b03f2b3d5c6739c92af55997f7",
          "citing_paper_year": 2023,
          "cited_paper_year": 2021
        },
        {
          "dataset_name": "WeatherStream",
          "dataset_description": "Used to train deweathering models for image restoration, supporting re-use in downstream vision tasks such as depth completion, stereo, optical flow, object detection, segmentation, and monocular depth prediction.",
          "citing_paper_id": "259340208",
          "cited_paper_id": 202565789,
          "context_text": "Nonetheless, deweathering models trained on WeatherStream are not only suited for appealing to aesthetics via image restoration, but may also support the re-use of pretrained models for downstream vision tasks such as depth completion [15,25,28,29,36,46–49,53,56], stereo [4,7,11,50,55], optical flow [1,20–22,37,38], object detection [16, 19,26,33], segmentation [8–10,23,24,34,44,54] and monocular depth prediction [2,5,13,14,17,18,23,30–32,40,42,45,51,52].",
          "confidence_score": 1.0,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions 'WeatherStream' as a dataset used for training deweathering models, which supports image restoration and re-use for various downstream vision tasks. No other specific datasets are mentioned.",
          "citing_paper_doi": "10.1109/CVPR52729.2023.01297",
          "cited_paper_doi": "10.1109/ICCV.2019.00448",
          "citing_paper_url": "https://www.semanticscholar.org/paper/1b2ec6027043f5696df8c44a8225e9caf0b7bf6d",
          "cited_paper_url": "https://www.semanticscholar.org/paper/67fc98119edf2debefd3a533a374429b1836aa10",
          "citing_paper_year": 2023,
          "cited_paper_year": 2019
        },
        {
          "dataset_name": "WeatherStream",
          "dataset_description": "Used to train deweathering models for image restoration, supporting re-use in downstream vision tasks such as depth completion, stereo, optical flow, object detection, segmentation, and monocular depth prediction.",
          "citing_paper_id": "259340208",
          "cited_paper_id": 206594738,
          "context_text": "Nonetheless, deweathering models trained on WeatherStream are not only suited for appealing to aesthetics via image restoration, but may also support the re-use of pretrained models for downstream vision tasks such as depth completion [15,25,28,29,36,46–49,53,56], stereo [4,7,11,50,55], optical flow [1,20–22,37,38], object detection [16, 19,26,33], segmentation [8–10,23,24,34,44,54] and monocular depth prediction [2,5,13,14,17,18,23,30–32,40,42,45,51,52].",
          "confidence_score": 1.0,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions 'WeatherStream' as a dataset used for training deweathering models, which supports image restoration and re-use for various downstream vision tasks. No other specific datasets are mentioned.",
          "citing_paper_doi": "10.1109/CVPR52729.2023.01297",
          "cited_paper_doi": "10.1109/CVPR.2016.91",
          "citing_paper_url": "https://www.semanticscholar.org/paper/1b2ec6027043f5696df8c44a8225e9caf0b7bf6d",
          "cited_paper_url": "https://www.semanticscholar.org/paper/f8e79ac0ea341056ef20f2616628b3e964764cfd",
          "citing_paper_year": 2023,
          "cited_paper_year": 2015
        },
        {
          "dataset_name": "WeatherStream",
          "dataset_description": "Used to train deweathering models for image restoration, supporting re-use in downstream vision tasks such as depth completion, stereo, optical flow, object detection, segmentation, and monocular depth prediction.",
          "citing_paper_id": "259340208",
          "cited_paper_id": 238198098,
          "context_text": "Nonetheless, deweathering models trained on WeatherStream are not only suited for appealing to aesthetics via image restoration, but may also support the re-use of pretrained models for downstream vision tasks such as depth completion [15,25,28,29,36,46–49,53,56], stereo [4,7,11,50,55], optical flow [1,20–22,37,38], object detection [16, 19,26,33], segmentation [8–10,23,24,34,44,54] and monocular depth prediction [2,5,13,14,17,18,23,30–32,40,42,45,51,52].",
          "confidence_score": 1.0,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions 'WeatherStream' as a dataset used for training deweathering models, which supports image restoration and re-use for various downstream vision tasks. No other specific datasets are mentioned.",
          "citing_paper_doi": "10.1109/CVPR52729.2023.01297",
          "cited_paper_doi": "10.1109/ICCV48922.2021.00351",
          "citing_paper_url": "https://www.semanticscholar.org/paper/1b2ec6027043f5696df8c44a8225e9caf0b7bf6d",
          "cited_paper_url": "https://www.semanticscholar.org/paper/a1826bf2590cd9c67c97b76a3d378e61ed7fcbf9",
          "citing_paper_year": 2023,
          "cited_paper_year": 2021
        },
        {
          "dataset_name": "WeatherStream",
          "dataset_description": "Used to train models on real-world frames containing rain-fog degradations, enhancing the robustness of image restoration algorithms. | Used to train models on images impaired by various degradations, improving the generalization of image restoration techniques. | Used to test and compare MWFormer-real and TransWeather-real models, focusing on realistic weather degradation scenarios in single images.",
          "citing_paper_id": "274281468",
          "cited_paper_id": 259340208,
          "context_text": "RainDrop [31] Outdoor-Rain [50] Snow100K [51] WeatherStream [54] CSD [55] TransWeather-real [ We also compared MWFormer-real and TransWeather-real on the two more realistic testsets: WeatherStream [54] and CSD [55] testsets.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions several datasets used for testing and comparing models in the domain of image restoration, particularly for weather-related degradations.",
          "citing_paper_doi": "10.1109/TIP.2024.3501855",
          "cited_paper_doi": "10.1109/CVPR52729.2023.01297",
          "citing_paper_url": "https://www.semanticscholar.org/paper/d39a9d1e085cc1d74fa7b9ed27e13c6356647de2",
          "cited_paper_url": "https://www.semanticscholar.org/paper/1b2ec6027043f5696df8c44a8225e9caf0b7bf6d",
          "citing_paper_year": 2024,
          "cited_paper_year": 2023
        }
      ]
    },
    {
      "cited_paper_id": "5250573",
      "citation_count": 0,
      "total_dataset_mentions": 7,
      "unique_datasets": [
        "Set14"
      ],
      "dataset_details": [
        {
          "dataset_name": "Set14",
          "dataset_description": "Used for testing image restoration models, assessing performance on a slightly larger set of images. | Used for testing image restoration models, evaluating performance on a small, well-curated set of images. | Used for testing image restoration models, particularly for urban scenes, evaluating performance on architectural details and textures. | Used for testing image restoration, particularly for urban scenes, to evaluate the effectiveness of restoration techniques in complex environments. | Used for training in image restoration, offering a diverse set of images for various restoration tasks. | Used to evaluate image restoration models, focusing on a diverse set of real-world images with varying complexities. | Used for training image restoration models, providing high-quality images for upscaling and denoising tasks. | Used for evaluating image restoration algorithms, specifically for testing generalization and robustness in diverse real-world scenarios. | Used for training image restoration models, offering a diverse set of images for various restoration tasks. | Used for training in image restoration, providing high-quality images for upscaling and denoising tasks. | Used for testing image restoration algorithms, focusing on small-scale images to evaluate performance. | Used for testing image restoration algorithms, providing a moderate set of images to assess quality and accuracy. | Used to test the super-resolution method, evaluating performance on synthesized degraded low-resolution images using isotropic Gaussian kernels.",
          "citing_paper_id": "235755417",
          "cited_paper_id": 2356330,
          "context_text": "The testing data includes two classic datasets widely used in SR task, namely Set14 [77] and BSD100 [78], and HR images are blurred and downsampled using eight selected isotropic Gaussian kernels for each scale factor to synthesize degraded LR.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions two specific datasets, Set14 and BSD100, which are used for testing in the super-resolution task. The datasets are used to evaluate the performance of the method on synthesized degraded low-resolution images.",
          "citing_paper_doi": "10.1109/TPAMI.2022.3203009",
          "cited_paper_doi": "10.1007/978-3-642-27413-8_47",
          "citing_paper_url": "https://www.semanticscholar.org/paper/0b8a64c8145d95e0fc0bd8f6eb54b5da7aca4fb7",
          "cited_paper_url": "https://www.semanticscholar.org/paper/d1dce9ad419d8c7037980a9ea28506ed0e640bba",
          "citing_paper_year": 2021,
          "cited_paper_year": 2010
        },
        {
          "dataset_name": "Set14",
          "dataset_description": "Used to evaluate single image super-resolution results, specifically focusing on the 'barbara' image with an upscaling factor of ×4. | Used to evaluate super-resolution performance at a scale factor of 4, comparing MWCNN's PSNR values against other methods. | Used to evaluate the performance of MWCNN in image super-resolution, comparing PSNR gains against VDSR. | Used to assess the effectiveness of image restoration techniques, particularly in super-resolution tasks. | Used to evaluate super-resolution performance at a scale factor of 4, comparing MWCNN's PSNR values against SRResNet. | Used to evaluate MWCNN's single image super-resolution performance, focusing on high-quality image restoration and detail preservation. | Used to test the robustness of image restoration algorithms, emphasizing detail preservation and artifact reduction. | Used to evaluate the performance of image restoration methods on urban scenes, focusing on texture and structure recovery. | Used for visual comparisons of competing image restoration methods, focusing on the 'Barbara' image to evaluate performance and quality. | Used to evaluate single image scale-up methods, focusing on super-resolution performance and visual quality.",
          "citing_paper_id": "29151865",
          "cited_paper_id": 2356330,
          "context_text": "Single image super-resolution results of “barbara” (Set14) with upscaling factor ×4.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions 'Set14', which is a known dataset used for image super-resolution tasks. The dataset is used to evaluate single image super-resolution results.",
          "citing_paper_doi": "10.1109/CVPRW.2018.00121",
          "cited_paper_doi": "10.1007/978-3-642-27413-8_47",
          "citing_paper_url": "https://www.semanticscholar.org/paper/5813ae0a48ff50d48f406df52ee5b9795c34a8bd",
          "cited_paper_url": "https://www.semanticscholar.org/paper/d1dce9ad419d8c7037980a9ea28506ed0e640bba",
          "citing_paper_year": 2018,
          "cited_paper_year": 2010
        },
        {
          "dataset_name": "Set14",
          "dataset_description": "Used to assess low-complexity single-image super-resolution techniques, emphasizing nonnegative neighbor embedding for image enhancement. | Used to evaluate single image scale-up methods, focusing on image quality and restoration performance using sparse representations.",
          "citing_paper_id": "209323842",
          "cited_paper_id": 2356330,
          "context_text": "We report on the standard Set14 dataset by Zeyde et al. (2010) and Set5 by Bevilacqua et al. (2012).",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The citation mentions two specific datasets, Set14 and Set5, which are commonly used in image restoration and super-resolution tasks.",
          "citing_paper_doi": "10.1007/s11263-021-01572-7",
          "cited_paper_doi": "10.1007/978-3-642-27413-8_47",
          "citing_paper_url": "https://www.semanticscholar.org/paper/594adc4a13d5be3545de97bd94fc5b268bca0f90",
          "cited_paper_url": "https://www.semanticscholar.org/paper/d1dce9ad419d8c7037980a9ea28506ed0e640bba",
          "citing_paper_year": 2021,
          "cited_paper_year": 2010
        },
        {
          "dataset_name": "Set14",
          "dataset_description": "Used to assess low-complexity single-image super-resolution techniques, emphasizing nonnegative neighbor embedding for image enhancement. | Used to evaluate single image scale-up methods, focusing on image quality and restoration performance using sparse representations.",
          "citing_paper_id": "209323842",
          "cited_paper_id": 5250573,
          "context_text": "We report on the standard Set14 dataset by Zeyde et al. (2010) and Set5 by Bevilacqua et al. (2012).",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The citation mentions two specific datasets, Set14 and Set5, which are commonly used in image restoration and super-resolution tasks.",
          "citing_paper_doi": "10.1007/s11263-021-01572-7",
          "cited_paper_doi": "10.5244/C.26.135",
          "citing_paper_url": "https://www.semanticscholar.org/paper/594adc4a13d5be3545de97bd94fc5b268bca0f90",
          "cited_paper_url": "https://www.semanticscholar.org/paper/c7338140c446bb1b8c1a2c24aa380890214dc0d7",
          "citing_paper_year": 2021,
          "cited_paper_year": 2012
        },
        {
          "dataset_name": "Set14",
          "dataset_description": "Used for evaluating image restoration and segmentation algorithms, providing a diverse set of natural images with ground truth annotations. | Used for evaluating image restoration methods on manga images, focusing on preserving line art and color fidelity. | Used for evaluating image restoration methods, focusing on super-resolution performance on a small set of high-quality images.",
          "citing_paper_id": "264146360",
          "cited_paper_id": 64193,
          "context_text": "2012), Set14 (Zeyde, Elad, and Protter 2010), BSDS100 (Martin et al. 2001), Manga109 (Matsui et al.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The citation mentions several datasets by name, which are commonly used in image restoration and super-resolution tasks.",
          "citing_paper_doi": "10.48550/arXiv.2310.10513",
          "cited_paper_doi": "10.1109/ICCV.2001.937655",
          "citing_paper_url": "https://www.semanticscholar.org/paper/836906c334159d46a691dfc5466c33caa3d22f65",
          "cited_paper_url": "https://www.semanticscholar.org/paper/9a1ed876196ec9733acb1daa6d65e35ff0414291",
          "citing_paper_year": 2023,
          "cited_paper_year": 2001
        },
        {
          "dataset_name": "Set14",
          "dataset_description": "Used to evaluate the model on various unsatisfactory degradation tasks, focusing on image restoration performance and quality.",
          "citing_paper_id": "268030722",
          "cited_paper_id": 2356330,
          "context_text": "After that, we evaluate the model on Set14 [49] for each unsatisfactory degradation task.",
          "confidence_score": 1.0,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "Set14 is a well-known dataset in image processing, particularly for super-resolution tasks. The context indicates it is used for evaluating the model on various degradation tasks.",
          "citing_paper_doi": null,
          "cited_paper_doi": "10.1007/978-3-642-27413-8_47",
          "citing_paper_url": "https://www.semanticscholar.org/paper/791a58a22c56c4e135fa8ef8801b235e3f6a648f",
          "cited_paper_url": "https://www.semanticscholar.org/paper/d1dce9ad419d8c7037980a9ea28506ed0e640bba",
          "citing_paper_year": 2023,
          "cited_paper_year": 2010
        },
        {
          "dataset_name": "Set14",
          "dataset_description": "Used to evaluate image restoration methods, focusing on PSNR and LPIPS metrics for assessing image quality and perceptual similarity. | Used to assess image restoration performance in urban scenes, evaluating both PSNR and LPIPS metrics. | Used to evaluate GAN-based super-resolution methods, focusing on performance metrics like LPIPS and PSNR. | Used to evaluate general-purpose image restoration, assessing PSNR and LPIPS for a diverse set of images. | Used to evaluate high-resolution image restoration, focusing on PSNR and LPIPS to measure restoration quality and perceptual fidelity. | Used to test image restoration methods on manga images, evaluating PSNR and LPIPS to measure restoration accuracy and visual quality.",
          "citing_paper_id": "257557425",
          "cited_paper_id": 2356330,
          "context_text": "Method Set14 [77] Urban100 [25] Manga109 [43] General100 [16] DIV2K100 [1] PSNR ↑ LPIPS ↓ PSNR ↑ LPIPS ↓ PSNR ↑ LPIPS ↓ PSNR ↑ LPIPS ↓ PSNR ↑ LPIPS ↓",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The citation mentions several datasets used for evaluating image restoration methods, which are relevant to the 'All-in-One Image Restoration' topic.",
          "citing_paper_doi": "10.1109/ICCV51070.2023.01204",
          "cited_paper_doi": "10.1007/978-3-642-27413-8_47",
          "citing_paper_url": "https://www.semanticscholar.org/paper/67909a17f9c9467de536aa2cf7b0864dc6215e96",
          "cited_paper_url": "https://www.semanticscholar.org/paper/d1dce9ad419d8c7037980a9ea28506ed0e640bba",
          "citing_paper_year": 2023,
          "cited_paper_year": 2010
        }
      ]
    },
    {
      "cited_paper_id": "198185751",
      "citation_count": 0,
      "total_dataset_mentions": 7,
      "unique_datasets": [
        "HIDE"
      ],
      "dataset_details": [
        {
          "dataset_name": "HIDE",
          "dataset_description": "Used for zero-shot generalization to validate the model's performance on real-world motion blur with 980 image pairs. | Used for zero-shot generalization to validate the model's performance on human-aware motion deblurring with 2025 image pairs.",
          "citing_paper_id": "268513542",
          "cited_paper_id": 201624746,
          "context_text": "To further validate the power of our model, we perform zero-shot generalization on HIDE [47], RealBlur-J [42] and RealBlur-R [42] with 2025, 980, 980 pairs respectively.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The citation mentions specific datasets used for zero-shot generalization, which are relevant to the image restoration topic.",
          "citing_paper_doi": "10.1109/CVPR52733.2024.02404",
          "cited_paper_doi": "10.1109/ICCV.2019.00567",
          "citing_paper_url": "https://www.semanticscholar.org/paper/1ff44c75c361d2ed28f868361a29437197b0270c",
          "cited_paper_url": "https://www.semanticscholar.org/paper/20a4b3353579525f0b76ec42e17a2284b4453f9a",
          "citing_paper_year": 2024,
          "cited_paper_year": 2019
        },
        {
          "dataset_name": "HIDE",
          "dataset_description": "Used to assess deblurring performance on synthetic images, emphasizing diverse blur types and conditions. | Used to test deblurring effectiveness on real-world images, specifically those with realistic motion blur. | Used to evaluate deblurring algorithms on synthetic images, focusing on motion blur in high-speed scenes. | Used to evaluate deblurring quality on real-world images, particularly those with challenging lighting and noise. | Used to evaluate the performance of the PromptRestorer method in motion deblurring, advancing state-of-the-art approaches by comparing against existing methods. | Used to assess the effectiveness of the PromptRestorer method in handling various image degradation issues, demonstrating superior performance over current approaches.",
          "citing_paper_id": "268030812",
          "cited_paper_id": 201624746,
          "context_text": "We evaluate deblurring results on both synthetic datasets (GoPro [65], HIDE [76]) and real-world datasets (RealBlur-R [73], RealBlur-J [73]).",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions specific datasets used for evaluating deblurring results, which are relevant to the topic of image restoration.",
          "citing_paper_doi": null,
          "cited_paper_doi": "10.1109/ICCV.2019.00567",
          "citing_paper_url": "https://www.semanticscholar.org/paper/caabf4abb96ee83f650aa7b707bd9a99007b3e74",
          "cited_paper_url": "https://www.semanticscholar.org/paper/20a4b3353579525f0b76ec42e17a2284b4453f9a",
          "citing_paper_year": 2023,
          "cited_paper_year": 2019
        },
        {
          "dataset_name": "HIDE",
          "dataset_description": "Used to evaluate seven recent deblurring methods, focusing on their performance and availability. The dataset provides a benchmark for comparing deblurring techniques.",
          "citing_paper_id": "231639270",
          "cited_paper_id": 198185751,
          "context_text": "For HIDE [18] dataset, we chose seven recent deblurring [9, 20, 18, 3, 24, 15, 19] methods depending on compared methods’ availability.",
          "confidence_score": 1.0,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions the HIDE dataset and its use in evaluating deblurring methods. The dataset is clearly identified and used for a specific purpose.",
          "citing_paper_doi": "10.1109/TIP.2022.3216216",
          "cited_paper_doi": "10.1109/CVPR.2019.00397",
          "citing_paper_url": "https://www.semanticscholar.org/paper/010e74e834c12e840e603b53ad14cbfc454ce7a1",
          "cited_paper_url": "https://www.semanticscholar.org/paper/a58468dace1b8654f4ab5a92411ad3c76998c280",
          "citing_paper_year": 2021,
          "cited_paper_year": 2019
        },
        {
          "dataset_name": "HIDE",
          "dataset_description": "Used to evaluate seven recent deblurring methods, focusing on their performance and availability. The dataset provides a benchmark for comparing deblurring techniques.",
          "citing_paper_id": "231639270",
          "cited_paper_id": 199543931,
          "context_text": "For HIDE [18] dataset, we chose seven recent deblurring [9, 20, 18, 3, 24, 15, 19] methods depending on compared methods’ availability.",
          "confidence_score": 1.0,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions the HIDE dataset and its use in evaluating deblurring methods. The dataset is clearly identified and used for a specific purpose.",
          "citing_paper_doi": "10.1109/TIP.2022.3216216",
          "cited_paper_doi": "10.1109/ICCV.2019.00897",
          "citing_paper_url": "https://www.semanticscholar.org/paper/010e74e834c12e840e603b53ad14cbfc454ce7a1",
          "cited_paper_url": "https://www.semanticscholar.org/paper/81c1dc00b32d12edfab00656593f5db56cfa79e0",
          "citing_paper_year": 2021,
          "cited_paper_year": 2019
        },
        {
          "dataset_name": "HIDE",
          "dataset_description": "Used for qualitative comparisons of deblurring results, focusing on testing images to evaluate the effectiveness of the proposed method. | Used for qualitative comparisons of motion deblurring methods, focusing on visual quality improvements over previous state-of-the-art techniques. | Used to evaluate BANet on image deblurring, specifically with 3214 pairs of blurred and sharp images, 2103 for training and the rest for testing. | Used to evaluate seven recent deblurring methods, focusing on their performance and availability. The dataset provides a benchmark for comparing deblurring techniques. | Used for qualitative comparisons of image deblurring methods, focusing on visual quality improvements over previous state-of-the-art techniques. | Used to test BANet on image deblurring, containing 2025 pairs of HD images, all for testing.",
          "citing_paper_id": "231639270",
          "cited_paper_id": 201624746,
          "context_text": "For HIDE [18] dataset, we chose seven recent deblurring [9, 20, 18, 3, 24, 15, 19] methods depending on compared methods’ availability.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions the HIDE dataset and its use in evaluating deblurring methods. The dataset is clearly identified and used for a specific purpose.",
          "citing_paper_doi": "10.1109/TIP.2022.3216216",
          "cited_paper_doi": "10.1109/ICCV.2019.00567",
          "citing_paper_url": "https://www.semanticscholar.org/paper/010e74e834c12e840e603b53ad14cbfc454ce7a1",
          "cited_paper_url": "https://www.semanticscholar.org/paper/20a4b3353579525f0b76ec42e17a2284b4453f9a",
          "citing_paper_year": 2021,
          "cited_paper_year": 2019
        },
        {
          "dataset_name": "HIDE",
          "dataset_description": "Used for qualitative comparisons of motion deblurring methods, focusing on visual quality improvements over previous state-of-the-art techniques. | Used to evaluate seven recent deblurring methods, focusing on their performance and availability. The dataset provides a benchmark for comparing deblurring techniques. | Used for qualitative comparisons of image deblurring methods, focusing on visual quality improvements over previous state-of-the-art techniques.",
          "citing_paper_id": "231639270",
          "cited_paper_id": 208138230,
          "context_text": "For HIDE [18] dataset, we chose seven recent deblurring [9, 20, 18, 3, 24, 15, 19] methods depending on compared methods’ availability.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions the HIDE dataset and its use in evaluating deblurring methods. The dataset is clearly identified and used for a specific purpose.",
          "citing_paper_doi": "10.1109/TIP.2022.3216216",
          "cited_paper_doi": "10.1007/978-3-030-58539-6_20",
          "citing_paper_url": "https://www.semanticscholar.org/paper/010e74e834c12e840e603b53ad14cbfc454ce7a1",
          "cited_paper_url": "https://www.semanticscholar.org/paper/20eb22846f41c99566046304dbb40e1515e3dd1a",
          "citing_paper_year": 2021,
          "cited_paper_year": 2019
        },
        {
          "dataset_name": "HIDE",
          "dataset_description": "Used for visual comparisons in image deblurring, specifically evaluating the performance of DMPHN and other methods. The dataset provides a benchmark for assessing deblurring algorithms.",
          "citing_paper_id": "245837508",
          "cited_paper_id": 102351795,
          "context_text": "Visual comparisons for image deblurring on HIDE [79] among DMPHN [110], Suin et al.",
          "confidence_score": 0.9,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions 'HIDE' which is likely a dataset used for visual comparisons in image deblurring. The context does not provide enough information to determine if other terms are datasets.",
          "citing_paper_doi": "10.1109/CVPR52688.2022.00568",
          "cited_paper_doi": "10.1109/CVPR.2019.00613",
          "citing_paper_url": "https://www.semanticscholar.org/paper/fbf68eb0cf8237cef59ebdb301569c79b9676ff9",
          "cited_paper_url": "https://www.semanticscholar.org/paper/a5c1156c2c8185df4581cf139df05aab66b3bb22",
          "citing_paper_year": 2022,
          "cited_paper_year": 2019
        }
      ]
    },
    {
      "cited_paper_id": "197545095",
      "citation_count": 0,
      "total_dataset_mentions": 7,
      "unique_datasets": [
        "UIEB"
      ],
      "dataset_details": [
        {
          "dataset_name": "UIEB",
          "dataset_description": "Utilized for training and evaluating underwater image enhancement algorithms, focusing on improving visual quality and clarity in underwater environments.",
          "citing_paper_id": "271891992",
          "cited_paper_id": 58014237,
          "context_text": "The UIEB dataset [28] is utilized for the two underwater image enhancement tasks.",
          "confidence_score": 1.0,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context clearly mentions the UIEB dataset, which is a specific benchmark dataset for underwater image enhancement. The dataset is used for training and evaluation in the context of underwater image enhancement tasks.",
          "citing_paper_doi": "10.1145/3664647.3681621",
          "cited_paper_doi": "10.1109/TIP.2019.2955241",
          "citing_paper_url": "https://www.semanticscholar.org/paper/ca2e8f7b9e902aa917904cb4b75f8dc0845edbcd",
          "cited_paper_url": "https://www.semanticscholar.org/paper/9496c94a6630a165a24db4e32f0b3f12a6616307",
          "citing_paper_year": 2024,
          "cited_paper_year": 2019
        },
        {
          "dataset_name": "UIEB",
          "dataset_description": "Used to evaluate underwater image enhancement techniques, containing 950 underwater images with 850 synthetic reference images enhanced using methods like Retinex, UDCP, Red Channel, and MSCNN.",
          "citing_paper_id": "253708090",
          "cited_paper_id": 926505,
          "context_text": "Recall that UIEB dataset [35] contains 950 underwater images, where 850 of them have their synthetic reference image enhanced by implementing 12 different techniques such as Retinex [62], UDCP [63], Red Channel [64] and MSCNN [65] amongst others.",
          "confidence_score": 1.0,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions the UIEB dataset, which is a specific dataset containing underwater images and their synthetic reference images enhanced using various techniques.",
          "citing_paper_doi": "10.48550/arXiv.2211.10026",
          "cited_paper_doi": null,
          "citing_paper_url": "https://www.semanticscholar.org/paper/29412f7b8cff20da3221937e066619aaa5f78655",
          "cited_paper_url": "https://www.semanticscholar.org/paper/ae963f2c3769a5246314395a2eaf6fd63dabd5b8",
          "citing_paper_year": 2022,
          "cited_paper_year": 2016
        },
        {
          "dataset_name": "UIEB",
          "dataset_description": "Used to evaluate underwater image enhancement methods, specifically comparing Euclidean distance metrics across various techniques including Retinex, CLAHE, FUnIE-GAN, Water-Net, UGAN, and DGD-cGAN. | Used to evaluate image restoration methods, specifically comparing performance on underwater image de-watering. The dataset provides a benchmark for assessing the effectiveness of various restoration techniques. | Used to evaluate underwater image enhancement methods, comparing results against ground truth images using various restoration techniques. | Used to evaluate underwater image enhancement methods, comparing results from various models including Retinex, CLAHE, FUnIE-GAN, Water-Net, UGAN, and DGD-cGAN. | Used to evaluate underwater image enhancement methods, providing high-resolution images for benchmarking various restoration techniques. | Used to evaluate underwater image enhancement techniques, containing 950 underwater images with 850 synthetic reference images enhanced using methods like Retinex, UDCP, Red Channel, and MSCNN.",
          "citing_paper_id": "253708090",
          "cited_paper_id": 8430295,
          "context_text": "Recall that UIEB dataset [35] contains 950 underwater images, where 850 of them have their synthetic reference image enhanced by implementing 12 different techniques such as Retinex [62], UDCP [63], Red Channel [64] and MSCNN [65] amongst others.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions the UIEB dataset, which is a specific dataset containing underwater images and their synthetic reference images enhanced using various techniques.",
          "citing_paper_doi": "10.48550/arXiv.2211.10026",
          "cited_paper_doi": "10.1109/83.597272",
          "citing_paper_url": "https://www.semanticscholar.org/paper/29412f7b8cff20da3221937e066619aaa5f78655",
          "cited_paper_url": "https://www.semanticscholar.org/paper/114ef0ee39bfe835f7df778f36a8ad60571f5449",
          "citing_paper_year": 2022,
          "cited_paper_year": 1997
        },
        {
          "dataset_name": "UIEB",
          "dataset_description": "Used to evaluate underwater image enhancement techniques, containing 950 underwater images with 850 synthetic reference images enhanced using methods like Retinex, UDCP, Red Channel, and MSCNN.",
          "citing_paper_id": "253708090",
          "cited_paper_id": 17763780,
          "context_text": "Recall that UIEB dataset [35] contains 950 underwater images, where 850 of them have their synthetic reference image enhanced by implementing 12 different techniques such as Retinex [62], UDCP [63], Red Channel [64] and MSCNN [65] amongst others.",
          "confidence_score": 1.0,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions the UIEB dataset, which is a specific dataset containing underwater images and their synthetic reference images enhanced using various techniques.",
          "citing_paper_doi": "10.48550/arXiv.2211.10026",
          "cited_paper_doi": "10.1007/978-3-319-46475-6_10",
          "citing_paper_url": "https://www.semanticscholar.org/paper/29412f7b8cff20da3221937e066619aaa5f78655",
          "cited_paper_url": "https://www.semanticscholar.org/paper/392e419efe21b6e2cc1125f27482d22bf3c677be",
          "citing_paper_year": 2022,
          "cited_paper_year": 2016
        },
        {
          "dataset_name": "UIEB",
          "dataset_description": "Used to evaluate underwater image enhancement techniques, containing 950 underwater images with 850 synthetic reference images enhanced using methods like Retinex, UDCP, Red Channel, and MSCNN.",
          "citing_paper_id": "253708090",
          "cited_paper_id": 26960375,
          "context_text": "Recall that UIEB dataset [35] contains 950 underwater images, where 850 of them have their synthetic reference image enhanced by implementing 12 different techniques such as Retinex [62], UDCP [63], Red Channel [64] and MSCNN [65] amongst others.",
          "confidence_score": 1.0,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions the UIEB dataset, which is a specific dataset containing underwater images and their synthetic reference images enhanced using various techniques.",
          "citing_paper_doi": "10.48550/arXiv.2211.10026",
          "cited_paper_doi": "10.1016/J.JVCIR.2014.11.006",
          "citing_paper_url": "https://www.semanticscholar.org/paper/29412f7b8cff20da3221937e066619aaa5f78655",
          "cited_paper_url": "https://www.semanticscholar.org/paper/fb393e362434480d47959d146487761be667d4a2",
          "citing_paper_year": 2022,
          "cited_paper_year": 2015
        },
        {
          "dataset_name": "UIEB",
          "dataset_description": "A subset of ImageNet is used for testing, providing a diverse range of images to validate generalization of underwater image enhancement techniques. | Used to evaluate underwater image enhancement methods, specifically comparing Euclidean distance metrics across various techniques including Retinex, CLAHE, FUnIE-GAN, Water-Net, UGAN, and DGD-cGAN. | Used to test the removal of water effects from underwater images, evaluating the effectiveness of restoration algorithms. | Used to test and evaluate methods for enhancing underwater visual perception, focusing on color correction and contrast improvement. | Used to test the robustness of underwater image enhancement algorithms, particularly in diverse underwater environments. | Used to assess the effectiveness of underwater image enhancement techniques, emphasizing color correction and clarity. | Used to test and evaluate underwater image enhancement algorithms, focusing on improving visual perception in underwater environments. | Used as a general-purpose image dataset to test the robustness and generalization of image restoration methods. | Used to evaluate underwater image enhancement methods, comparing results from various models including Retinex, CLAHE, FUnIE-GAN, Water-Net, UGAN, and DGD-cGAN. | Used to test underwater image enhancement methods, focusing on visual quality and restoration accuracy. | Used to evaluate underwater image enhancement methods, comparing results against ground truth images using various restoration techniques. | Used for testing underwater image enhancement algorithms, focusing on turbidity effects and visual clarity improvements. | Used to evaluate underwater image enhancement techniques, containing 950 underwater images with 850 synthetic reference images enhanced using methods like Retinex, UDCP, Red Channel, and MSCNN. | Used to evaluate underwater image enhancement techniques, assessing color correction and clarity improvements. | Used to train a pre-trained network for underwater image enhancement, focusing on paired images to improve restoration quality. | Used to evaluate underwater image enhancement methods, focusing on visual quality and restoration performance. | Used for benchmarking underwater image enhancement techniques, evaluating performance across various underwater conditions and image qualities. | A subset of ImageNet is used for testing, providing a diverse range of images to validate the generalizability of the image restoration methods. | Used to evaluate underwater image enhancement methods, providing high-resolution images for benchmarking various restoration techniques. | Used for testing underwater image restoration methods, focusing on turbidity effects and visual quality improvements.",
          "citing_paper_id": "253708090",
          "cited_paper_id": 58014237,
          "context_text": "Recall that UIEB dataset [35] contains 950 underwater images, where 850 of them have their synthetic reference image enhanced by implementing 12 different techniques such as Retinex [62], UDCP [63], Red Channel [64] and MSCNN [65] amongst others.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions the UIEB dataset, which is a specific dataset containing underwater images and their synthetic reference images enhanced using various techniques.",
          "citing_paper_doi": "10.48550/arXiv.2211.10026",
          "cited_paper_doi": "10.1109/TIP.2019.2955241",
          "citing_paper_url": "https://www.semanticscholar.org/paper/29412f7b8cff20da3221937e066619aaa5f78655",
          "cited_paper_url": "https://www.semanticscholar.org/paper/9496c94a6630a165a24db4e32f0b3f12a6616307",
          "citing_paper_year": 2022,
          "cited_paper_year": 2019
        },
        {
          "dataset_name": "UIEB",
          "dataset_description": "Used to provide reference images for comparison in underwater image enhancement, focusing on visual quality and restoration effectiveness.",
          "citing_paper_id": "275789940",
          "cited_paper_id": 197545095,
          "context_text": "Reference images from the UIEB dataset are provided alongside each sample for comparison. heritage and landscapes [5].",
          "confidence_score": 1.0,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions the UIEB dataset, which is a specific dataset used for underwater image enhancement. It is used for providing reference images for comparison.",
          "citing_paper_doi": "10.48550/arXiv.2501.12981",
          "cited_paper_doi": "10.1016/j.image.2020.115978",
          "citing_paper_url": "https://www.semanticscholar.org/paper/298b051872e5b7cf945ba4bf7bbfb9d83c9f738f",
          "cited_paper_url": "https://www.semanticscholar.org/paper/9f64b5c28465fc716ed838b4456589506af67e6d",
          "citing_paper_year": 2025,
          "cited_paper_year": 2019
        }
      ]
    },
    {
      "cited_paper_id": "235702920",
      "citation_count": 0,
      "total_dataset_mentions": 6,
      "unique_datasets": [
        "Rain200L"
      ],
      "dataset_details": [
        {
          "dataset_name": "Rain200L",
          "dataset_description": "Used for training and evaluating raindrop removal algorithms, focusing on realistic raindrop effects and image restoration. | Used for deblurring, assessing methods to sharpen and restore clarity to blurred images. | Used for deraindrop, testing algorithms to eliminate raindrops from images captured through wet surfaces. | Used for training and evaluating rain removal algorithms, focusing on light rain conditions and image clarity. | Used to evaluate the denoising performance of TAPE-Net on smartphone camera images. | Used to evaluate de-raining algorithms, focusing on realistic raindrop patterns and their impact on image quality. | Used for training and evaluating rain removal algorithms, focusing on heavy rain conditions and image clarity. | Used for denoising smartphone camera images, focusing on high-quality noise reduction techniques. | Used to evaluate the performance of the image restoration method, focusing on raindrop removal from images. | Used for deraindrop, testing algorithms to remove raindrops from images captured through glass surfaces. | Used to evaluate the performance of the image restoration method, focusing on rain removal from images with a low level of rain intensity. | Used to evaluate the overall image restoration performance of TAPE-Net, focusing on a variety of image degradation types. | Used for super resolution, testing upscaling techniques to enhance image resolution. | Used to evaluate de-raining techniques, containing synthetic rain overlaid on clean images to simulate heavy rain scenarios. | Used to evaluate the de-raining performance of TAPE-Net, focusing on high-complexity rain patterns. | Used to evaluate the effectiveness of task-agnostic pre-training on image denoising, specifically measuring PSNR improvements without using real noisy images. | Used to evaluate the de-raining performance of TAPE-Net, focusing on raindrop effects. | Used for desnowing, assessing methods to remove snowflakes from images. | Used for shadow removal, evaluating algorithms to eliminate shadows from images. | Used for evaluating denoising algorithms on smartphone camera images, focusing on real-world noise patterns and quality. | Used for demoireing, addressing moiré patterns in images caused by interference between patterns. | Used for demoireing, evaluating techniques to reduce moiré patterns in images. | Used for deraining, assessing performance in removing light rain streaks from images. | Used to evaluate the performance of the image restoration method, focusing on various image restoration tasks as described in the TIP2018 dataset. | Used for super resolution, focusing on upscaling low-resolution images to high-resolution with minimal loss of detail. | Used for desnowing, evaluating techniques to remove snowflakes from images captured in snowy conditions. | Used for task-agnostic pre-training in image denoising, specifically adding synthetic Gaussian noises to ground truth images from smartphone cameras. | Used for evaluating various image restoration techniques, focusing on a diverse set of image degradation types and restoration quality. | Used to benchmark denoising algorithms for smartphone cameras, providing real-world noisy images and their corresponding ground truths. | Used for shadow removal, assessing methods to eliminate shadows from images, enhancing visual clarity. | Used to evaluate the de-raining performance of TAPE-Net, focusing on low-complexity rain patterns. | Used to evaluate the performance of the image restoration method, focusing on rain removal from images with a high level of rain intensity. | Used for deblurring, testing algorithms to reduce motion blur and improve image sharpness. | Used to assess de-raining performance, providing a diverse set of images with varying rain intensities and environmental conditions. | Used for deraining, evaluating methods to remove rain streaks from images under heavy rain conditions. | Used to test de-raining methods, featuring synthetic rain overlaid on clean images to simulate light rain scenarios. | Used to evaluate the performance of the image restoration method, focusing on denoising smartphone camera images. | Used to fine-tune a pre-trained model on real-noise/non-noise image pairs, specifically addressing image denoising for smartphone cameras.",
          "citing_paper_id": "247411392",
          "cited_paper_id": 52059988,
          "context_text": "The TAPE-Net improve the PSNR by 1.45dB, 1.03dB, 0.84dB, 0.49dB and 0.75dB on the Rain200L, Rain200H, Raindrop800, SIDD, and TIP2018 dataset, respectively.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The citation mentions specific datasets used for evaluating the performance of TAPE-Net in image restoration tasks, particularly in denoising and de-raining.",
          "citing_paper_doi": "10.48550/arXiv.2203.06074",
          "cited_paper_doi": "10.1109/CVPR.2018.00182",
          "citing_paper_url": "https://www.semanticscholar.org/paper/03244185e343d6cc2397269c510f33433a7df502",
          "cited_paper_url": "https://www.semanticscholar.org/paper/ebf35073e122782f685a0d6c231622412f28a53b",
          "citing_paper_year": 2022,
          "cited_paper_year": 2018
        },
        {
          "dataset_name": "Rain200L",
          "dataset_description": "Used to evaluate image de-raining methods, containing a large set of synthetic rain images. | Used for training and testing a deep detail network for rain removal, featuring synthetic rain images. | Used to train DGUNet for image de-raining, providing a larger set of diverse rain images to improve generalization. | Used to train models for image de-raining, containing diverse synthetic rain images to test and improve de-raining algorithms. | Used to evaluate image de-raining methods, focusing on low-resolution synthetic rain images. | Used to evaluate image de-raining methods, containing synthetic rain images generated by the DDN model. | Used to train models for image de-raining, focusing on high-resolution synthetic rain images to enhance de-raining quality. | Used for training and evaluating de-raining models, containing diverse synthetic rain scenarios. | Used for evaluating image de-raining methods, focusing on low-resolution rain streaks in synthetic images. | Used to evaluate image de-raining methods, focusing on high-resolution synthetic rain images. | Used to evaluate image de-raining methods, providing clean images for comparison. | Used to train DGUNet for image de-raining, focusing on high-resolution rain images to enhance restoration accuracy. | Used to train DGUNet for image de-raining, focusing on low-resolution rain images to improve restoration quality. | Used for training and testing image de-raining algorithms, providing a larger set of synthetic rain images. | Used to train models for image de-raining, providing a larger set of synthetic rain images to improve generalization. | Used to evaluate the performance of the CoIC method with contrastive loss, specifically assessing its effectiveness in handling complex and accumulated rain. | Used to train DGUNet, focusing on the impact of rain-/detail-aware negative exemplars in low-resolution images. | Used for evaluating image de-raining methods, focusing on high-resolution rain streaks in synthetic images. | Used to evaluate image de-raining methods, focusing on diverse indoor and outdoor scenes with synthetic rain. | Used to plot layer-wise log T lcc ′ for evaluating de-raining performance, focusing on a larger set of rain images. | Used to train DGUNet, focusing on the impact of rain-/detail-aware negative exemplars in a diverse set of images. | Used to evaluate the performance of the CoIC method, specifically measuring PSNR gains in image de-raining tasks involving complex rain scenarios. | Used to train models for real-world deraining, specifically evaluating the performance of CoIC, DGUNet, and IDT to validate the proposed method's superiority. | Used to generate images with diverse and accumulated rain types for image de-raining research, focusing on orientation, intensity, and density variations. | Used to train DGUNet, focusing on the impact of rain-/detail-aware negative exemplars in high-resolution images. | Used to plot layer-wise log T lcc ′ for evaluating de-raining performance, focusing on low-resolution rain images. | Used to plot layer-wise log T lcc ′ for evaluating de-raining performance, focusing on high-resolution rain images. | Used to evaluate the effectiveness of rain-aware negative exemplars in improving image de-raining performance, focusing on the impact of these exemplars on the dataset. | Used to evaluate image de-raining methods, containing real-world rain images with known ground truth. | Used to train models for image de-raining, providing additional synthetic rain images to enhance the robustness of de-raining methods. | Used to train models for image de-raining, focusing on low-resolution synthetic rain images to improve de-raining performance.",
          "citing_paper_id": "269214582",
          "cited_paper_id": 11922819,
          "context_text": "We can easily observe that the real-world rainy image in Figure 8 ( With images from Rain200L, Rain200H, Rain800, DID 1, DID 3, DDN 4, DDN 12, RealInt, and Clean, we can employ the pre-trained feature extractor to obtain their instance-level representations.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions several datasets used for evaluating image de-raining methods. These datasets are specific and relevant to the topic of all-in-one image restoration.",
          "citing_paper_doi": "10.48550/arXiv.2404.12091",
          "cited_paper_doi": "10.1109/TCSVT.2019.2920407",
          "citing_paper_url": "https://www.semanticscholar.org/paper/348009f068853d56cec79e0eecdeb0279d00893a",
          "cited_paper_url": "https://www.semanticscholar.org/paper/920f0c070701caabf023c600f3e310f1906ca818",
          "citing_paper_year": 2024,
          "cited_paper_year": 2017
        },
        {
          "dataset_name": "Rain200L",
          "dataset_description": "Used to evaluate image de-raining methods, containing a large set of synthetic rain images. | Used for training and testing a deep detail network for rain removal, featuring synthetic rain images. | Used to train DGUNet for image de-raining, providing a larger set of diverse rain images to improve generalization. | Used to evaluate image de-raining methods, focusing on low-resolution synthetic rain images. | Used to evaluate image de-raining methods, containing synthetic rain images generated by the DDN model. | Used to evaluate the performance of a method with a contrastive loss, focusing on homogenous rain conditions in image restoration. | Used to evaluate the impact of dataset collision in image de-raining, focusing on low-resolution rain images. | Used for training and evaluating de-raining models, containing diverse synthetic rain scenarios. | Used for training and evaluating rain removal algorithms, containing 1800 image pairs with heavy rain for training and 200 for evaluation. | Used for evaluating image de-raining methods, focusing on low-resolution rain streaks in synthetic images. | Used to evaluate image de-raining methods, focusing on high-resolution synthetic rain images. | Used to evaluate image de-raining methods, providing clean images for comparison. | Used to train DGUNet for image de-raining, focusing on high-resolution rain images to enhance restoration accuracy. | Used to train DGUNet for image de-raining, focusing on low-resolution rain images to improve restoration quality. | Used for training and testing image de-raining algorithms, providing a larger set of synthetic rain images. | Used to train DGUNet, focusing on the impact of rain-/detail-aware negative exemplars in low-resolution images. | Used for evaluating image de-raining methods, focusing on high-resolution rain streaks in synthetic images. | Used to evaluate image de-raining methods, focusing on diverse indoor and outdoor scenes with synthetic rain. | Used to plot layer-wise log T lcc ′ for evaluating de-raining performance, focusing on a larger set of rain images. | Used to evaluate the impact of dataset collision in image de-raining, focusing on high-resolution rain images. | Used to train DGUNet, focusing on the impact of rain-/detail-aware negative exemplars in a diverse set of images. | Used for training and evaluating rain removal algorithms, containing 1800 image pairs with light rain for training and 200 for evaluation. | Used to train DGUNet, focusing on the impact of rain-/detail-aware negative exemplars in high-resolution images. | Used to plot layer-wise log T lcc ′ for evaluating de-raining performance, focusing on low-resolution rain images. | Used to plot layer-wise log T lcc ′ for evaluating de-raining performance, focusing on high-resolution rain images. | Used to evaluate image de-raining methods, containing real-world rain images with known ground truth.",
          "citing_paper_id": "269214582",
          "cited_paper_id": 15443600,
          "context_text": "We can easily observe that the real-world rainy image in Figure 8 ( With images from Rain200L, Rain200H, Rain800, DID 1, DID 3, DDN 4, DDN 12, RealInt, and Clean, we can employ the pre-trained feature extractor to obtain their instance-level representations.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions several datasets used for evaluating image de-raining methods. These datasets are specific and relevant to the topic of all-in-one image restoration.",
          "citing_paper_doi": "10.48550/arXiv.2404.12091",
          "cited_paper_doi": "10.1109/CVPR.2017.183",
          "citing_paper_url": "https://www.semanticscholar.org/paper/348009f068853d56cec79e0eecdeb0279d00893a",
          "cited_paper_url": "https://www.semanticscholar.org/paper/6de27f256e97c014a95706c72af7fa651e8fa34a",
          "citing_paper_year": 2024,
          "cited_paper_year": 2016
        },
        {
          "dataset_name": "Rain200L",
          "dataset_description": "Used for training and testing image de-raining algorithms, providing a larger set of synthetic rain images. | Used to train models for image de-raining, providing a larger set of synthetic rain images to improve generalization. | Used to train models for image de-raining, focusing on high-resolution synthetic rain images to enhance de-raining quality. | Used for training and testing a deep detail network for rain removal, featuring synthetic rain images. | Used for visualizing rain augmentation effects in image restoration, specifically employing the fourth and twelfth augmentations for light and heavy rain, respectively. | Used to train and test a deep detail network for removing rain from single images, focusing on 14 rain augmentations and 12,600 training pairs. | Used for evaluating image de-raining methods, focusing on high-resolution rain streaks in synthetic images. | Used to train models for image de-raining, containing diverse synthetic rain images to test and improve de-raining algorithms. | Used for training and evaluating de-raining models, containing diverse synthetic rain scenarios. | Used for evaluating image de-raining methods, focusing on low-resolution rain streaks in synthetic images. | Used to train models for image de-raining, providing additional synthetic rain images to enhance the robustness of de-raining methods. | Used to train models for image de-raining, focusing on low-resolution synthetic rain images to improve de-raining performance.",
          "citing_paper_id": "269214582",
          "cited_paper_id": 17115407,
          "context_text": "We conduct extensive experiments utilizing five commonly adopted synthetic datasets: Rain200L & Rain200H (Yang et al., 2017), Rain800 (Zhang et al., 2019), DID-Data (Zhang & Patel, 2018), and DDN-Data (Fu et al., 2017).",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions five specific synthetic datasets used for image de-raining experiments. These datasets are clearly named and relevant to the research topic of all-in-one image restoration.",
          "citing_paper_doi": "10.48550/arXiv.2404.12091",
          "cited_paper_doi": "10.1109/CVPR.2017.186",
          "citing_paper_url": "https://www.semanticscholar.org/paper/348009f068853d56cec79e0eecdeb0279d00893a",
          "cited_paper_url": "https://www.semanticscholar.org/paper/29ad37a306732676a41f25e05960ff0adbca631e",
          "citing_paper_year": 2024,
          "cited_paper_year": 2017
        },
        {
          "dataset_name": "Rain200L",
          "dataset_description": "Used to evaluate the performance of the proposed MSDT method for image deraining, comparing PSNR and SSIM values against other baselines, including the DRSformer. | Used to evaluate the performance of the proposed MSDT model in image deraining, comparing it against the state-of-the-art DRSformer method using PSNR as the metric.",
          "citing_paper_id": "268678353",
          "cited_paper_id": 257636509,
          "context_text": "We can clearly see that our proposed MSDT consistently out-performs all the other baselines in terms of PSNR and SSIM values thanks to our multi-scale architecture, especially exceeding the recent Transformer-based approach DRSformer (Chen et al. 2023a) by 0.52 dB in PSNR on the Rain200L benchmark.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions the Rain200L benchmark, which is a specific dataset used for evaluating image deraining methods. The dataset is used to compare the performance of the proposed method against other baselines.",
          "citing_paper_doi": "10.1609/aaai.v38i2.27865",
          "cited_paper_doi": "10.1109/CVPR52729.2023.00571",
          "citing_paper_url": "https://www.semanticscholar.org/paper/1bc2390c2e32c434043936734c2fb4af5ad800c0",
          "cited_paper_url": "https://www.semanticscholar.org/paper/f647aa3a9e035dc218aa198769660fa696cdf969",
          "citing_paper_year": 2024,
          "cited_paper_year": 2023
        },
        {
          "dataset_name": "Rain200L",
          "dataset_description": "Used to train and evaluate image denoising methods, focusing on diverse image degradation types. | Used to train and evaluate image denoising methods, focusing on real-world noise patterns. | Used to train and evaluate rain removal methods, focusing on high-resolution images with synthetic rain. | Used to train and evaluate rain removal methods, focusing on low-resolution images with synthetic rain.",
          "citing_paper_id": "266690738",
          "cited_paper_id": 235702920,
          "context_text": "Following [17], we train and evaluate the proposed method separately on Rain200L [104], Rain200H [104], DID-Data [115], and DDN-Data [30] datasets.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions specific datasets used for training and evaluating the proposed method in the domain of image restoration, particularly rain removal and denoising.",
          "citing_paper_doi": "10.1109/CVPR52733.2024.00277",
          "cited_paper_doi": "10.1109/CVPR46437.2021.00849",
          "citing_paper_url": "https://www.semanticscholar.org/paper/c6495e41d3974476e0be9d5137c3dda51125bcec",
          "cited_paper_url": "https://www.semanticscholar.org/paper/9ce43f7cd16b53781d2c18f7354a89b10cc2f140",
          "citing_paper_year": 2023,
          "cited_paper_year": 2021
        }
      ]
    },
    {
      "cited_paper_id": "244345938",
      "citation_count": 0,
      "total_dataset_mentions": 6,
      "unique_datasets": [
        "SOTS-Indoor"
      ],
      "dataset_details": [
        {
          "dataset_name": "SOTS-Indoor",
          "dataset_description": "Used to evaluate the performance of OKNet against the Fourmer model in image restoration, focusing on PSNR improvement with comparable complexity.",
          "citing_paper_id": "268678266",
          "cited_paper_id": 260957038,
          "context_text": "More concretely, OKNet significantly outperforms the recent Transformer model Fourmer (Zhou et al. 2023) by 3.47 dB PSNR with comparable complexity on the SOTS-Indoor (Li et al. 2018) dataset, as illustrated in Figure 1 (a).",
          "confidence_score": 1.0,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions the SOTS-Indoor dataset, which is used to evaluate the performance of OKNet compared to the Fourmer model. The dataset is specific and relevant to image restoration.",
          "citing_paper_doi": "10.1609/aaai.v38i2.27907",
          "cited_paper_doi": null,
          "citing_paper_url": "https://www.semanticscholar.org/paper/d5a17832f115c16d4097ea39f204917734f9fba0",
          "cited_paper_url": "https://www.semanticscholar.org/paper/4b688c486f2b223212682fd2b3f72cc0caa94b1b",
          "citing_paper_year": 2024,
          "cited_paper_year": 2023
        },
        {
          "dataset_name": "SOTS-Indoor",
          "dataset_description": "Used to evaluate image dehazing methods, comparing the proposed method's PSNR against recent works on indoor scenes.",
          "citing_paper_id": "259298517",
          "cited_paper_id": 244345938,
          "context_text": "Compared to the recent works, DehazeFormer-L (Song et al., 2022) and PMNet (Ye et al., 2022), our method receives 0.19 dB and 2.83 dB higher PSNR on SOTS-Indoor testset, respectively.",
          "confidence_score": 1.0,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions the SOTS-Indoor testset, which is a specific dataset used for evaluating image dehazing methods. The dataset is used to compare the performance of the proposed method against other recent works.",
          "citing_paper_doi": null,
          "cited_paper_doi": null,
          "citing_paper_url": "https://www.semanticscholar.org/paper/6af728164e4424218967fd356b3448f5037acd88",
          "cited_paper_url": "https://www.semanticscholar.org/paper/ae0843970213145ec57b7414624cde1a8ca10e62",
          "citing_paper_year": 2023,
          "cited_paper_year": 2021
        },
        {
          "dataset_name": "SOTS-Indoor",
          "dataset_description": "Used to evaluate the performance of the proposed method against DehazeFormer-L, focusing on PSNR and FLOPs, demonstrating efficiency and effectiveness in single image dehazing.",
          "citing_paper_id": "271274210",
          "cited_paper_id": 248069347,
          "context_text": "Specifically, it outperforms the expensive DehazeFormer-L [34] by 0.74 dB PSNR on the SOTS-Indoor [3] dataset with only 14% FLOPs, as shown in Figure 1 (a).",
          "confidence_score": 1.0,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions the SOTS-Indoor dataset, which is used to evaluate the performance of the proposed method against DehazeFormer-L in terms of PSNR and FLOPs.",
          "citing_paper_doi": "10.1109/TCSVT.2024.3429557",
          "cited_paper_doi": "10.1109/TIP.2023.3256763",
          "citing_paper_url": "https://www.semanticscholar.org/paper/1091511c40eebb0f8e964246c91e91a85480dbf9",
          "cited_paper_url": "https://www.semanticscholar.org/paper/37246e26163cdd0f5ddd1ea47a5a5019dead8abb",
          "citing_paper_year": 2024,
          "cited_paper_year": 2022
        },
        {
          "dataset_name": "SOTS-Indoor",
          "dataset_description": "Used to benchmark dehazing methods on real-world images with dense haze, testing robustness and effectiveness in challenging scenarios. | Used to evaluate image dehazing algorithms, focusing on indoor scenes to assess performance under controlled conditions. | Used for real-world image dehazing experiments, testing the robustness of dehazing techniques on natural hazy conditions. | Used for real-world image dehazing experiments, assessing the effectiveness of dehazing methods on densely hazy images. | Used for synthetic image dehazing experiments, evaluating the performance of dehazing algorithms on indoor scenes. | Used to assess dehazing performance on real-world images with natural haze, emphasizing practical applicability and visual quality. | Used to benchmark single-image dehazing methods, evaluating the performance of PromptRestorer against other state-of-the-art techniques in indoor scenes.",
          "citing_paper_id": "268030812",
          "cited_paper_id": 39760169,
          "context_text": "Table 3: Image dehazing results on SOTS-Indoor [52] and real-world benchmarks Dense-Haze [2] and NH-Haze [3].",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions specific datasets used for image dehazing experiments, which are relevant to the topic of all-in-one image restoration.",
          "citing_paper_doi": null,
          "cited_paper_doi": "10.1109/TIP.2018.2867951",
          "citing_paper_url": "https://www.semanticscholar.org/paper/caabf4abb96ee83f650aa7b707bd9a99007b3e74",
          "cited_paper_url": "https://www.semanticscholar.org/paper/abf75b81b96ff2cd262086675deba55f907108db",
          "citing_paper_year": 2023,
          "cited_paper_year": 2017
        },
        {
          "dataset_name": "SOTS-Indoor",
          "dataset_description": "This dataset 'SOTS-indoor' was mentioned in the citation context but no detailed description was generated. | The dataset is used to evaluate dehazing performance, focusing on high-resolution outdoor scenes with synthetic and real-world hazy images.",
          "citing_paper_id": "258959359",
          "cited_paper_id": 6200260,
          "context_text": "The comparison methods for the image deraining and dehazing task are CycleGAN [80], pix2pix [19], HRGAN [31], PCNet [23], MPRNet [71], RainHazeDiff64 [44], and Table 2 Dehazing results on SOTS-indoor and Haze4K.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions 'SOTS-indoor' and 'Haze4K' as datasets used for evaluating dehazing results. These are specific datasets relevant to the image restoration domain.",
          "citing_paper_doi": "10.48550/arXiv.2305.17863",
          "cited_paper_doi": "10.1109/CVPR.2017.632",
          "citing_paper_url": "https://www.semanticscholar.org/paper/7d8cfd80c9bc7bbfd52b4ee9c9bbc5b7bfd9cc21",
          "cited_paper_url": "https://www.semanticscholar.org/paper/8acbe90d5b852dadea7810345451a99608ee54c7",
          "citing_paper_year": 2023,
          "cited_paper_year": 2016
        },
        {
          "dataset_name": "SOTS-Indoor",
          "dataset_description": "Used to evaluate the proposed method for all-in-one image restoration, specifically focusing on heavy rain conditions and integrating physics models with conditional adversarial learning. | Used to evaluate GridFormer on heavy rain image restoration, focusing on integrating physics models and conditional adversarial learning. | Used for testing image dehazing models, focusing on improving visibility in outdoor scenes affected by rain using a physics model and conditional adversarial learning. | This dataset 'Outdoor-Rain' was mentioned in the citation context but no detailed description was generated. | Used to train models for image restoration under snowy conditions, contributing 9,000 images to the mixed dataset. | This dataset 'RainDrop-Test' was mentioned in the citation context but no detailed description was generated. | This dataset 'Outdoor' was mentioned in the citation context but no detailed description was generated. | Used to train models for image restoration under rainy conditions, contributing 1,069 images to the mixed dataset. | This dataset 'Outdoor-Rain100' was mentioned in the citation context but no detailed description was generated. | The dataset is used to evaluate dehazing performance, focusing on high-resolution outdoor scenes with synthetic and real-world hazy images. | Used to assess GridFormer's performance on synthetic and real indoor hazy images, emphasizing dehazing techniques. | This dataset 'SOTS-indoor' was mentioned in the citation context but no detailed description was generated. | Used for training and evaluating image deraining models, focusing on removing rain from outdoor scenes using a physics model and conditional adversarial learning. | Used to test GridFormer's effectiveness on high-resolution hazy images, specifically for outdoor scenes. | This dataset 'All-in-One' was mentioned in the citation context but no detailed description was generated.",
          "citing_paper_id": "258959359",
          "cited_paper_id": 131773964,
          "context_text": "The comparison methods for the image deraining and dehazing task are CycleGAN [80], pix2pix [19], HRGAN [31], PCNet [23], MPRNet [71], RainHazeDiff64 [44], and Table 2 Dehazing results on SOTS-indoor and Haze4K.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions 'SOTS-indoor' and 'Haze4K' as datasets used for evaluating dehazing results. These are specific datasets relevant to the image restoration domain.",
          "citing_paper_doi": "10.48550/arXiv.2305.17863",
          "cited_paper_doi": "10.1109/CVPR.2019.00173",
          "citing_paper_url": "https://www.semanticscholar.org/paper/7d8cfd80c9bc7bbfd52b4ee9c9bbc5b7bfd9cc21",
          "cited_paper_url": "https://www.semanticscholar.org/paper/b1b7eeac57e1e78e02d9d97eb809d8459bd06050",
          "citing_paper_year": 2023,
          "cited_paper_year": 2019
        }
      ]
    },
    {
      "cited_paper_id": "227143147",
      "citation_count": 0,
      "total_dataset_mentions": 6,
      "unique_datasets": [
        "MIT-Adobe FiveK"
      ],
      "dataset_details": [
        {
          "dataset_name": "MIT-Adobe FiveK",
          "dataset_description": "Used to apply an operator on images retouched by expert C, focusing on image restoration and retouching techniques. | Used to evaluate image retouching techniques, specifically applying an operator on images retouched by expert C to assess restoration quality. | Used to form input-output pairs for training and evaluating image restoration models, focusing on expert-retouched images.",
          "citing_paper_id": "264146360",
          "cited_paper_id": 207778473,
          "context_text": "2022), we apply this operator on images retouched by expert C of the MIT-Adobe FiveK dataset (Bychkovsky et al. 2011).",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions the MIT-Adobe FiveK dataset, which is a specific, verifiable dataset used for image restoration and retouching.",
          "citing_paper_doi": "10.48550/arXiv.2310.10513",
          "cited_paper_doi": null,
          "citing_paper_url": "https://www.semanticscholar.org/paper/836906c334159d46a691dfc5466c33caa3d22f65",
          "cited_paper_url": "https://www.semanticscholar.org/paper/5ee57b18a7be2b671ffc6f5ce5dbbe92112ee0fb",
          "citing_paper_year": 2023,
          "cited_paper_year": 2007
        },
        {
          "dataset_name": "MIT-Adobe FiveK",
          "dataset_description": "Used to evaluate the performance of the photo retouching network, specifically focusing on images retouched by expert C to assess the quality and effectiveness of the proposed method. | Used to apply an operator on images retouched by expert C, focusing on image restoration and retouching techniques.",
          "citing_paper_id": "264146360",
          "cited_paper_id": 233219999,
          "context_text": "Following (Liu et al. 2022), we apply this operator on images retouched by expert C of the MIT-Adobe FiveK dataset (Bychkovsky et al.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions the MIT-Adobe FiveK dataset, which is a specific, verifiable dataset used for image retouching research.",
          "citing_paper_doi": "10.48550/arXiv.2310.10513",
          "cited_paper_doi": "10.1109/TMM.2022.3179904",
          "citing_paper_url": "https://www.semanticscholar.org/paper/836906c334159d46a691dfc5466c33caa3d22f65",
          "cited_paper_url": "https://www.semanticscholar.org/paper/a80dbbb10bf7463353d85ce3e7614e6e84831b5c",
          "citing_paper_year": 2023,
          "cited_paper_year": 2021
        },
        {
          "dataset_name": "MIT-Adobe FiveK",
          "dataset_description": "Used as the source of images for the exposure correction dataset, providing a large set of images for training, validation, and testing. | Used to evaluate the ShadowDiffusion model for exposure correction, focusing on image quality improvements in low-light conditions. | Used to quantitatively compare image restoration results against expert photographers using PSNR and SSIM metrics, following previous work.",
          "citing_paper_id": "254535902",
          "cited_paper_id": 207778473,
          "context_text": "We adopt the PSNR and SSIM metrics for quantitative comparison, where we compare the results against five different expert photographers in the MIT-Adobe FiveK dataset [3] following previous work [2].",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions the MIT-Adobe FiveK dataset, which is a specific, verifiable dataset used for comparing image restoration results against expert photographers.",
          "citing_paper_doi": "10.1109/CVPR52729.2023.01350",
          "cited_paper_doi": null,
          "citing_paper_url": "https://www.semanticscholar.org/paper/afd674940791fb2daeecb0801a5068d80d9f77a7",
          "cited_paper_url": "https://www.semanticscholar.org/paper/5ee57b18a7be2b671ffc6f5ce5dbbe92112ee0fb",
          "citing_paper_year": 2022,
          "cited_paper_year": 2007
        },
        {
          "dataset_name": "MIT-Adobe FiveK",
          "dataset_description": "Used as the source of images for the exposure correction dataset, providing a large set of images for training, validation, and testing. | Used to evaluate the ShadowDiffusion model for exposure correction, focusing on image quality improvements in low-light conditions. | Used to quantitatively compare image restoration results against expert photographers using PSNR and SSIM metrics, following previous work.",
          "citing_paper_id": "254535902",
          "cited_paper_id": 227143147,
          "context_text": "We adopt the PSNR and SSIM metrics for quantitative comparison, where we compare the results against five different expert photographers in the MIT-Adobe FiveK dataset [3] following previous work [2].",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions the MIT-Adobe FiveK dataset, which is a specific, verifiable dataset used for comparing image restoration results against expert photographers.",
          "citing_paper_doi": "10.1109/CVPR52729.2023.01350",
          "cited_paper_doi": "10.1109/CVPR46437.2021.00904",
          "citing_paper_url": "https://www.semanticscholar.org/paper/afd674940791fb2daeecb0801a5068d80d9f77a7",
          "cited_paper_url": "https://www.semanticscholar.org/paper/f0ff96cd085d71289a1de119d39e5ff26e2d7e45",
          "citing_paper_year": 2022,
          "cited_paper_year": 2020
        },
        {
          "dataset_name": "MIT-Adobe FiveK",
          "dataset_description": "Used for visual comparisons of image retouching methods, including CycleGAN, Exposure, DPE, EnlightenGAN, UEGAN, and MAXIM-2S, focusing on image restoration quality.",
          "citing_paper_id": "245837508",
          "cited_paper_id": 24290799,
          "context_text": "Visual comparisons for image retouching on MIT-Adobe FiveK [7] provided by the authors of [63] among CycleGAN [121], Exposure [31], DPE [18], EnlightenGAN [36], UEGAN [63] and MAXIM-2S.",
          "confidence_score": 1.0,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions the MIT-Adobe FiveK dataset, which is a well-known dataset for image retouching. The dataset is used for visual comparisons of different image restoration methods.",
          "citing_paper_doi": "10.1109/CVPR52688.2022.00568",
          "cited_paper_doi": "10.1145/3181974",
          "citing_paper_url": "https://www.semanticscholar.org/paper/fbf68eb0cf8237cef59ebdb301569c79b9676ff9",
          "cited_paper_url": "https://www.semanticscholar.org/paper/18624a7c5b009b831aec15c931b9f2ff01755f44",
          "citing_paper_year": 2022,
          "cited_paper_year": 2017
        },
        {
          "dataset_name": "MIT-Adobe FiveK",
          "dataset_description": "Used for visual comparisons of image retouching methods, including CycleGAN, Exposure, DPE, EnlightenGAN, UEGAN, and MAXIM-2S, focusing on image restoration quality.",
          "citing_paper_id": "245837508",
          "cited_paper_id": 233404466,
          "context_text": "Visual comparisons for image retouching on MIT-Adobe FiveK [7] provided by the authors of [63] among CycleGAN [121], Exposure [31], DPE [18], EnlightenGAN [36], UEGAN [63] and MAXIM-2S.",
          "confidence_score": 1.0,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions the MIT-Adobe FiveK dataset, which is a well-known dataset for image retouching. The dataset is used for visual comparisons of different image restoration methods.",
          "citing_paper_doi": "10.1109/CVPR52688.2022.00568",
          "cited_paper_doi": null,
          "citing_paper_url": "https://www.semanticscholar.org/paper/fbf68eb0cf8237cef59ebdb301569c79b9676ff9",
          "cited_paper_url": "https://www.semanticscholar.org/paper/dd6d044696df5e4353ff7c92b8009e1201c85129",
          "citing_paper_year": 2022,
          "cited_paper_year": 2018
        }
      ]
    },
    {
      "cited_paper_id": "6496323",
      "citation_count": 0,
      "total_dataset_mentions": 6,
      "unique_datasets": [
        "Rain13K"
      ],
      "dataset_details": [
        {
          "dataset_name": "Rain13K",
          "dataset_description": "Used for training in denoising and dejpeg tasks, providing a diverse set of images for robust model development. | Used for image restoration, containing 1212 clean images. Compared with HQ-50K to highlight the larger scale of the latter. | Used for denoising and dejpeg tasks, containing 4,744 images to evaluate image restoration methods. | Used to compare the average pixel number of images, highlighting its size relative to other datasets in image restoration and super-resolution tasks. | Used for image restoration, containing 800 clean images. Compared with HQ-50K to highlight the larger scale of the latter. | Used for image restoration, containing 4744 clean images. Compared with HQ-50K to highlight the larger scale of the latter. | Used as a benchmark dataset for single image super-resolution, providing high-quality images for training and evaluation. | Used for image restoration, particularly for evaluating models on web images with various degradations. | Used for denoising and dejpeg tasks, containing 400 images to evaluate image restoration methods. | Used for image restoration, containing 2650 clean images. Compared with HQ-50K to highlight the larger scale of the latter. | Used in conjunction with BSD400 for training, enhancing the model's ability to handle various image restoration challenges. | Used for image restoration, containing 400 clean images. Compared with HQ-50K to highlight the larger scale of the latter. | Used as a large-scale dataset for image restoration, containing diverse images for training and testing models. | Used for rain removal tasks, providing a large dataset of rainy images for training and evaluation.",
          "citing_paper_id": "259108489",
          "cited_paper_id": 6496323,
          "context_text": "Our HQ-50K contains 50,000 images, while the commonly used restoration dataset BSDS400 [4], DIV2K [2], Flickr2K [32], WED [35], and Rain13K [58] only contains 400, 800, 2650, 4744 and 1212 clean images respectively.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions several datasets used for image restoration, providing specific image counts for each. These datasets are clearly identified and used for comparison with the HQ-50K dataset.",
          "citing_paper_doi": "10.48550/arXiv.2306.05390",
          "cited_paper_doi": "10.7892/BORIS.113226",
          "citing_paper_url": "https://www.semanticscholar.org/paper/4296bf82e67051e4cba10e510932f55fc12dd1e2",
          "cited_paper_url": "https://www.semanticscholar.org/paper/64617d35726c3fac4e3e3af46eb859372acecf06",
          "citing_paper_year": 2023,
          "cited_paper_year": 2017
        },
        {
          "dataset_name": "Rain13K",
          "dataset_description": "Used to train and evaluate the Complex Rain Model, focusing on diverse rain models for image restoration. The dataset provides a comprehensive set of images with various rain conditions.",
          "citing_paper_id": "264146360",
          "cited_paper_id": 231802205,
          "context_text": "The Complex Rain Model utilizes Rain13K dataset (Zamir et al. 2021), including an assortment of diverse rain models.",
          "confidence_score": 1.0,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The citation mentions the Rain13K dataset, which is a specific dataset used for image restoration, particularly for rain removal. The dataset is used to train and evaluate the Complex Rain Model.",
          "citing_paper_doi": "10.48550/arXiv.2310.10513",
          "cited_paper_doi": "10.1109/CVPR46437.2021.01458",
          "citing_paper_url": "https://www.semanticscholar.org/paper/836906c334159d46a691dfc5466c33caa3d22f65",
          "cited_paper_url": "https://www.semanticscholar.org/paper/92d50602db5746f03b91562e2cc8a98bec584e9b",
          "citing_paper_year": 2023,
          "cited_paper_year": 2021
        },
        {
          "dataset_name": "Rain13K",
          "dataset_description": "Constituent part of Rain13K, used for deraining experiments, providing a large set of rainy images for training and evaluation. | Used for deraining experiments, combining multiple datasets to provide a comprehensive set of rainy images for training and evaluation. | Used for large-scale deraining experiments, comprising 13,000 images with a wide range of rain conditions. | Used for robust deraining algorithm testing, featuring 1400 images with realistic rain effects. | Constituent part of Rain13K, used for deraining experiments, providing a diverse set of rainy images for training and evaluation. | Constituent part of Rain13K, used for deraining experiments, providing a moderate set of rainy images for training and evaluation. | Used for training and evaluating deraining models, providing 800 diverse images with varying rain intensities. | Constituent part of Rain13K, used for deraining experiments, providing a small set of rainy images for training and evaluation. | Used for comprehensive evaluation of deraining methods, containing 1200 images with complex rain patterns. | Used for evaluating deraining algorithms, focusing on lightweight rain streak removal with 100 low-resolution images. | Used for evaluating deraining algorithms, focusing on high-resolution images with 100 samples, emphasizing detailed rain streak removal.",
          "citing_paper_id": "259108489",
          "cited_paper_id": 9007541,
          "context_text": "For deraining, we use the Rain13K dataset, which is a combination of Rain14000 [23], Rain1800 [53], Rain800 [60], and Rain12 [30].",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions specific datasets used for deraining, which are clearly identified and combined into a larger dataset called Rain13K.",
          "citing_paper_doi": "10.48550/arXiv.2306.05390",
          "cited_paper_doi": "10.1109/CVPR.2016.299",
          "citing_paper_url": "https://www.semanticscholar.org/paper/4296bf82e67051e4cba10e510932f55fc12dd1e2",
          "cited_paper_url": "https://www.semanticscholar.org/paper/7c17025c540b88df14da35229618b5e896ab9528",
          "citing_paper_year": 2023,
          "cited_paper_year": 2016
        },
        {
          "dataset_name": "Rain13K",
          "dataset_description": "Used to train a model for rain removal in images, specifically utilizing 13,712 paired clean-rain images from multiple datasets. | Used to train deraining models, focusing on synthetic rain removal, employing a multi-stream dense network for density-aware de-raining. | Used to test the performance of trained deraining models on real rainy images, evaluating the effectiveness of the models in practical scenarios.",
          "citing_paper_id": "269605449",
          "cited_paper_id": 3406592,
          "context_text": "Specifically, we train the deraining models on the synthetic Rain13K dataset (Fu et al., 2017; Li et al., 2016; Yang et al., 2017; Zhang & Patel, 2018; Zhang et al., 2019a) and then test their performances on the real rainy dataset SPANet (Wang et al., 2019).",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions two specific datasets, Rain13K and SPANet, which are used for training and testing deraining models, respectively.",
          "citing_paper_doi": "10.48550/arXiv.2405.02843",
          "cited_paper_doi": "10.1109/CVPR.2018.00079",
          "citing_paper_url": "https://www.semanticscholar.org/paper/0c7542272e5edcff29608a0e37f9a3b705b363e7",
          "cited_paper_url": "https://www.semanticscholar.org/paper/34aaa735409b666b8dc678c23acb600b1d87913b",
          "citing_paper_year": 2024,
          "cited_paper_year": 2018
        },
        {
          "dataset_name": "Rain13K",
          "dataset_description": "Used to train a model for rain removal in images, specifically utilizing 13,712 paired clean-rain images from multiple datasets. | Used to train deraining models, focusing on synthetic rain removal, employing a multi-stream dense network for density-aware de-raining. | Used to test the performance of trained deraining models on real rainy images, evaluating the effectiveness of the models in practical scenarios.",
          "citing_paper_id": "269605449",
          "cited_paper_id": 17115407,
          "context_text": "Specifically, we train the deraining models on the synthetic Rain13K dataset (Fu et al., 2017; Li et al., 2016; Yang et al., 2017; Zhang & Patel, 2018; Zhang et al., 2019a) and then test their performances on the real rainy dataset SPANet (Wang et al., 2019).",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions two specific datasets, Rain13K and SPANet, which are used for training and testing deraining models, respectively.",
          "citing_paper_doi": "10.48550/arXiv.2405.02843",
          "cited_paper_doi": "10.1109/CVPR.2017.186",
          "citing_paper_url": "https://www.semanticscholar.org/paper/0c7542272e5edcff29608a0e37f9a3b705b363e7",
          "cited_paper_url": "https://www.semanticscholar.org/paper/29ad37a306732676a41f25e05960ff0adbca631e",
          "citing_paper_year": 2024,
          "cited_paper_year": 2017
        },
        {
          "dataset_name": "Rain13K",
          "dataset_description": "Used to train a model for rain removal in images, specifically utilizing 13,712 paired clean-rain images from multiple datasets. | Used to train deraining models, focusing on synthetic rain removal, employing a multi-stream dense network for density-aware de-raining. | Used to test the performance of trained deraining models on real rainy images, evaluating the effectiveness of the models in practical scenarios.",
          "citing_paper_id": "269605449",
          "cited_paper_id": 201070037,
          "context_text": "Specifically, we train the deraining models on the synthetic Rain13K dataset (Fu et al., 2017; Li et al., 2016; Yang et al., 2017; Zhang & Patel, 2018; Zhang et al., 2019a) and then test their performances on the real rainy dataset SPANet (Wang et al., 2019).",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions two specific datasets, Rain13K and SPANet, which are used for training and testing deraining models, respectively.",
          "citing_paper_doi": "10.48550/arXiv.2405.02843",
          "cited_paper_doi": "10.1109/ICCV.2019.00319",
          "citing_paper_url": "https://www.semanticscholar.org/paper/0c7542272e5edcff29608a0e37f9a3b705b363e7",
          "cited_paper_url": "https://www.semanticscholar.org/paper/1d01c7431458564aa64606b4b6a7f9161ee423b1",
          "citing_paper_year": 2024,
          "cited_paper_year": 2019
        }
      ]
    },
    {
      "cited_paper_id": "244714491",
      "citation_count": 0,
      "total_dataset_mentions": 6,
      "unique_datasets": [
        "All-Weather dataset"
      ],
      "dataset_details": [
        {
          "dataset_name": "All-Weather dataset",
          "dataset_description": "Used to evaluate deweathering and downstream segmentation performance, focusing on various weather conditions. | Used to evaluate single-image rain removal techniques, focusing on the performance metrics such as FLOPs and number of parameters for each baseline. | Used to evaluate the image restoration quality of the proposed MoFME model, demonstrating superior performance with fewer network parameters. | Used to evaluate deweathering and downstream segmentation performance, specifically addressing rain and haze effects in cityscapes. | Used to assess the effectiveness of MoFME and other baselines in rain removal from images, evaluating performance using PSNR and SSIM metrics. | Used to evaluate the performance of MoFME and other baselines on image restoration tasks across various weather conditions, focusing on PSNR and SSIM metrics.",
          "citing_paper_id": "266573162",
          "cited_paper_id": 195654199,
          "context_text": "All-weather (Valanarasu et al. 2022) and Rain/HazeCityscapes (Hu et al. 2019; Sakaridis, Dai, and Van Gool 2018) datasets are used to evaluate deweathering and downstream segmentation.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions two specific datasets used for evaluating deweathering and downstream segmentation.",
          "citing_paper_doi": "10.48550/arXiv.2312.16610",
          "cited_paper_doi": "10.1109/CVPR.2019.00821",
          "citing_paper_url": "https://www.semanticscholar.org/paper/262ce851fa28064fbd8c0910e6f5978a17046469",
          "cited_paper_url": "https://www.semanticscholar.org/paper/eee324d41e696b743f6f2f9ce0a46d1ac19f08c8",
          "citing_paper_year": 2023,
          "cited_paper_year": 2019
        },
        {
          "dataset_name": "All-Weather dataset",
          "dataset_description": "Used to analyze the distribution of L2 norm across image channels, focusing on the impact of weather conditions on image restoration techniques. | Used to partition images into subsets based on degradation severity (slight, moderate, heavy) for evaluating all-in-one bad weather removal techniques.",
          "citing_paper_id": "265609786",
          "cited_paper_id": 219978541,
          "context_text": "(g ) From top to bottom, distribution of L 2 norm for the three channels across all images in the All-weather dataset [21]. severity data?",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions 'All-weather dataset' which is a specific dataset used in the research. The dataset is used to analyze the distribution of L2 norm across image channels, which is relevant to the research on bad weather removal.",
          "citing_paper_doi": "10.1109/CVPR52733.2024.02352",
          "cited_paper_doi": "10.1109/cvpr42600.2020.00324",
          "citing_paper_url": "https://www.semanticscholar.org/paper/57427ef02d111ba8dfe270f0db970f56eeb8f826",
          "cited_paper_url": "https://www.semanticscholar.org/paper/2e89dd08b5ea206128724419e259c8750e8d1381",
          "citing_paper_year": 2023,
          "cited_paper_year": 2020
        },
        {
          "dataset_name": "All-Weather dataset",
          "dataset_description": "Used to train the model for removing snow from images, focusing on realistic snow removal and image enhancement. | Specifically used to train the model for removing raindrops from images, enhancing clarity and visual quality. | Used to train the model for handling rain and fog simultaneously, improving restoration accuracy in complex weather scenarios. | Used to train and evaluate the model for restoring images degraded by various weather conditions, including rain, fog, and snow.",
          "citing_paper_id": "264555119",
          "cited_paper_id": 837707,
          "context_text": "The model is trained and evaluated on All-Weather dataset [36], which contain three kind of weather degradation datasets, i.e. Raindrop dataset [31] for rain-drop, Outdoor-Rain [22] for rain+fog, and Snow100K-L [28] for snow.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions specific datasets used for training and evaluating a model for image restoration under various weather conditions. The datasets are clearly named and their purposes are specified.",
          "citing_paper_doi": "10.1109/ACCESS.2025.3526168",
          "cited_paper_doi": "10.1109/TIP.2018.2806202",
          "citing_paper_url": "https://www.semanticscholar.org/paper/9f0b27769b77d476623f7b8d47816af3bc22d804",
          "cited_paper_url": "https://www.semanticscholar.org/paper/9e7487d7396acf298f972f0b8ab0397e0d9e7a0f",
          "citing_paper_year": 2023,
          "cited_paper_year": 2017
        },
        {
          "dataset_name": "All-Weather dataset",
          "dataset_description": "Used to train the model for removing snow from images, focusing on realistic snow removal and image enhancement. | Specifically used to train the model for removing raindrops from images, enhancing clarity and visual quality. | Used to train the model for handling rain and fog simultaneously, improving restoration accuracy in complex weather scenarios. | Used to train and evaluate the model for restoring images degraded by various weather conditions, including rain, fog, and snow.",
          "citing_paper_id": "264555119",
          "cited_paper_id": 4539586,
          "context_text": "The model is trained and evaluated on All-Weather dataset [36], which contain three kind of weather degradation datasets, i.e. Raindrop dataset [31] for rain-drop, Outdoor-Rain [22] for rain+fog, and Snow100K-L [28] for snow.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions specific datasets used for training and evaluating a model for image restoration under various weather conditions. The datasets are clearly named and their purposes are specified.",
          "citing_paper_doi": "10.1109/ACCESS.2025.3526168",
          "cited_paper_doi": "10.1109/CVPR.2018.00263",
          "citing_paper_url": "https://www.semanticscholar.org/paper/9f0b27769b77d476623f7b8d47816af3bc22d804",
          "cited_paper_url": "https://www.semanticscholar.org/paper/34e38559df559bfedfc3e1edc2b120ab2b5d444e",
          "citing_paper_year": 2023,
          "cited_paper_year": 2017
        },
        {
          "dataset_name": "All-Weather dataset",
          "dataset_description": "Used to train the model for handling rain and fog simultaneously, improving restoration accuracy in complex weather scenarios. | Used to evaluate the proposed method's performance in restoring images degraded by multiple weather conditions, comparing it with state-of-the-art methods both subjectively and objectively. | Specifically used to train the model for removing raindrops from images, enhancing clarity and visual quality. | Used to train and evaluate the model for restoring images degraded by various weather conditions, including rain, fog, and snow. | Used to train the model for removing snow from images, focusing on realistic snow removal and image enhancement.",
          "citing_paper_id": "264555119",
          "cited_paper_id": 244714491,
          "context_text": "The model is trained and evaluated on All-Weather dataset [36], which contain three kind of weather degradation datasets, i.e. Raindrop dataset [31] for rain-drop, Outdoor-Rain [22] for rain+fog, and Snow100K-L [28] for snow.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions specific datasets used for training and evaluating a model for image restoration under various weather conditions. The datasets are clearly named and their purposes are specified.",
          "citing_paper_doi": "10.1109/ACCESS.2025.3526168",
          "cited_paper_doi": "10.1109/CVPR52688.2022.00239",
          "citing_paper_url": "https://www.semanticscholar.org/paper/9f0b27769b77d476623f7b8d47816af3bc22d804",
          "cited_paper_url": "https://www.semanticscholar.org/paper/b27d3be4264dcd06f990b44968f4382526f24f1e",
          "citing_paper_year": 2023,
          "cited_paper_year": 2021
        },
        {
          "dataset_name": "All-Weather dataset",
          "dataset_description": "Used to evaluate the proposed method's performance in restoring images degraded by multiple weather conditions, comparing it with state-of-the-art methods both subjectively and objectively.",
          "citing_paper_id": "264555119",
          "cited_paper_id": 219978541,
          "context_text": "The proposed method can outperform the state-of-the-art methods on All-Weather dataset [23,36] subjectively and objectively, and can restore the combined multiple degradation weather without training these type data, but enjoy less parameters compared with other blind all-in-one image restoration…",
          "confidence_score": 1.0,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions the 'All-Weather dataset' which is a specific dataset used for evaluating image restoration methods under various weather conditions.",
          "citing_paper_doi": "10.1109/ACCESS.2025.3526168",
          "cited_paper_doi": "10.1109/cvpr42600.2020.00324",
          "citing_paper_url": "https://www.semanticscholar.org/paper/9f0b27769b77d476623f7b8d47816af3bc22d804",
          "cited_paper_url": "https://www.semanticscholar.org/paper/2e89dd08b5ea206128724419e259c8750e8d1381",
          "citing_paper_year": 2023,
          "cited_paper_year": 2020
        }
      ]
    },
    {
      "cited_paper_id": "85498726",
      "citation_count": 0,
      "total_dataset_mentions": 6,
      "unique_datasets": [
        "UFO-120"
      ],
      "dataset_details": [
        {
          "dataset_name": "UFO-120",
          "dataset_description": "Used to analyze the distribution of CMI scores for training samples, highlighting the skew towards lower values in distorted samples compared to ground truth.",
          "citing_paper_id": "211020567",
          "cited_paper_id": 3446915,
          "context_text": "Moreover, in Figure 4a, we show the distributions of CMI for training samples of the UFO-120 dataset, which suggests that the distorted samples’ CMI scores are skewed to much lower values compared to the ground truth.",
          "confidence_score": 1.0,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The citation mentions the UFO-120 dataset, which is used to analyze the distribution of CMI scores for training samples, particularly focusing on the difference between distorted and ground truth samples.",
          "citing_paper_doi": "10.15607/rss.2020.xvi.018",
          "cited_paper_doi": "10.2352/ISSN.2470-1173.2017.12.IQSP-226",
          "citing_paper_url": "https://www.semanticscholar.org/paper/26f278d4d3af8ef7e64325caf09308fec618d9f2",
          "cited_paper_url": "https://www.semanticscholar.org/paper/e3f2e337d4470545398cc6753a54c21debf9c37b",
          "citing_paper_year": 2020,
          "cited_paper_year": 2017
        },
        {
          "dataset_name": "UFO-120",
          "dataset_description": "Used to evaluate image enhancement methods, focusing on peak signal-to-noise ratio (PSNR) and structural similarity index (SSIM) metrics. | Used to evaluate image enhancement methods, focusing on PSNR and SSIM metrics.",
          "citing_paper_id": "211020567",
          "cited_paper_id": 5003887,
          "context_text": "Dataset RGHS UCM MS-Fusion MS-Retinex Water-Net UGAN Fusion-GAN FUnIE-GAN Deep SESR\nP SN R UFO-120 20.05± 3.1 20.99± 2.2 21.32± 3.3 21.69± 3.6 22.46± 1.9 23.45± 3.1 24.07± 2.1 25.15± 2.3 27.15± 3.2 EUVP 20.12± 2.9 20.55± 1.8 19.85± 2.4 21.27± 3.1 20.14± 2.3 23.67± 1.5 23.77± 2.4 26.78± 1.1 25.25± 2.1 UImNet 19.98± 1.8 20.48± 2.2 19.59± 3.2 22.63± 2.5 21.02± 1.6 23.88± 2.1 23.12± 1.9 24.68± 2.4 25.52± 2.7\nSS IM UFO-120 0.75± 0.06 0.78± 0.07 0.79± 0.09 0.75± 0.10 0.79± 0.05 0.80± 0.08 0.82± 0.07 0.82± 0.08 0.84± 0.03 EUVP 0.69± 0.11 0.73± 0.14 0.70± 0.05 0.69± 0.15 0.68± 0.18 0.67± 0.11 0.68± 0.05 0.86± 0.05 0.75± 0.07 UImNet 0.61± 0.08 0.67± 0.06 0.64± 0.11 0.74± 0.04 0.71± 0.07 0.79± 0.08 0.75± 0.07 0.77± 0.06 0.81± 0.05\nU IQ M UFO-120 2.36± 0.33 2.41± 0.53 2.76± 0.45 2.69± 0.59 2.83± 0.48 3.04± 0.28 2.98± 0.28 3.09± 0.51 3.13± 0.45 EUVP 2.45± 0.46 2.48± 0.77 2.51± 0.36 2.48± 0.09 2.55± 0.06 2.70± 0.31 2.58± 0.07 2.95± 0.38 2.98± 0.28 UImNet 2.32± 0.48 2.38± 0.42 2.79± 0.55 2.84± 0.37 2.92± 0.35 3.32± 0.55 3.19± 0.27 3.23± 0.32 3.26± 0.36\nit focused on the right foreground regions for contrast improvement.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions specific datasets (UFO-120, EUVP, UImNet) used for evaluating image enhancement methods. These datasets are relevant to the topic of All-in-One Image Restoration.",
          "citing_paper_doi": "10.15607/rss.2020.xvi.018",
          "cited_paper_doi": "10.1007/978-3-319-73603-7_37",
          "citing_paper_url": "https://www.semanticscholar.org/paper/26f278d4d3af8ef7e64325caf09308fec618d9f2",
          "cited_paper_url": "https://www.semanticscholar.org/paper/496df5211612910578f6ec1fea64aed7efcf7206",
          "citing_paper_year": 2020,
          "cited_paper_year": 2018
        },
        {
          "dataset_name": "UFO-120",
          "dataset_description": "Used to evaluate image enhancement methods, focusing on peak signal-to-noise ratio (PSNR) and structural similarity index (SSIM) metrics. | Used to evaluate image enhancement methods, focusing on PSNR and SSIM metrics.",
          "citing_paper_id": "211020567",
          "cited_paper_id": 58014237,
          "context_text": "Dataset RGHS UCM MS-Fusion MS-Retinex Water-Net UGAN Fusion-GAN FUnIE-GAN Deep SESR\nP SN R UFO-120 20.05± 3.1 20.99± 2.2 21.32± 3.3 21.69± 3.6 22.46± 1.9 23.45± 3.1 24.07± 2.1 25.15± 2.3 27.15± 3.2 EUVP 20.12± 2.9 20.55± 1.8 19.85± 2.4 21.27± 3.1 20.14± 2.3 23.67± 1.5 23.77± 2.4 26.78± 1.1 25.25± 2.1 UImNet 19.98± 1.8 20.48± 2.2 19.59± 3.2 22.63± 2.5 21.02± 1.6 23.88± 2.1 23.12± 1.9 24.68± 2.4 25.52± 2.7\nSS IM UFO-120 0.75± 0.06 0.78± 0.07 0.79± 0.09 0.75± 0.10 0.79± 0.05 0.80± 0.08 0.82± 0.07 0.82± 0.08 0.84± 0.03 EUVP 0.69± 0.11 0.73± 0.14 0.70± 0.05 0.69± 0.15 0.68± 0.18 0.67± 0.11 0.68± 0.05 0.86± 0.05 0.75± 0.07 UImNet 0.61± 0.08 0.67± 0.06 0.64± 0.11 0.74± 0.04 0.71± 0.07 0.79± 0.08 0.75± 0.07 0.77± 0.06 0.81± 0.05\nU IQ M UFO-120 2.36± 0.33 2.41± 0.53 2.76± 0.45 2.69± 0.59 2.83± 0.48 3.04± 0.28 2.98± 0.28 3.09± 0.51 3.13± 0.45 EUVP 2.45± 0.46 2.48± 0.77 2.51± 0.36 2.48± 0.09 2.55± 0.06 2.70± 0.31 2.58± 0.07 2.95± 0.38 2.98± 0.28 UImNet 2.32± 0.48 2.38± 0.42 2.79± 0.55 2.84± 0.37 2.92± 0.35 3.32± 0.55 3.19± 0.27 3.23± 0.32 3.26± 0.36\nit focused on the right foreground regions for contrast improvement.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions specific datasets (UFO-120, EUVP, UImNet) used for evaluating image enhancement methods. These datasets are relevant to the topic of All-in-One Image Restoration.",
          "citing_paper_doi": "10.15607/rss.2020.xvi.018",
          "cited_paper_doi": "10.1109/TIP.2019.2955241",
          "citing_paper_url": "https://www.semanticscholar.org/paper/26f278d4d3af8ef7e64325caf09308fec618d9f2",
          "cited_paper_url": "https://www.semanticscholar.org/paper/9496c94a6630a165a24db4e32f0b3f12a6616307",
          "citing_paper_year": 2020,
          "cited_paper_year": 2019
        },
        {
          "dataset_name": "UFO-120",
          "dataset_description": "Used to evaluate image enhancement methods, focusing on peak signal-to-noise ratio (PSNR) and structural similarity index (SSIM) metrics. | Used to evaluate image enhancement methods, focusing on PSNR and SSIM metrics.",
          "citing_paper_id": "211020567",
          "cited_paper_id": 85498726,
          "context_text": "Dataset RGHS UCM MS-Fusion MS-Retinex Water-Net UGAN Fusion-GAN FUnIE-GAN Deep SESR\nP SN R UFO-120 20.05± 3.1 20.99± 2.2 21.32± 3.3 21.69± 3.6 22.46± 1.9 23.45± 3.1 24.07± 2.1 25.15± 2.3 27.15± 3.2 EUVP 20.12± 2.9 20.55± 1.8 19.85± 2.4 21.27± 3.1 20.14± 2.3 23.67± 1.5 23.77± 2.4 26.78± 1.1 25.25± 2.1 UImNet 19.98± 1.8 20.48± 2.2 19.59± 3.2 22.63± 2.5 21.02± 1.6 23.88± 2.1 23.12± 1.9 24.68± 2.4 25.52± 2.7\nSS IM UFO-120 0.75± 0.06 0.78± 0.07 0.79± 0.09 0.75± 0.10 0.79± 0.05 0.80± 0.08 0.82± 0.07 0.82± 0.08 0.84± 0.03 EUVP 0.69± 0.11 0.73± 0.14 0.70± 0.05 0.69± 0.15 0.68± 0.18 0.67± 0.11 0.68± 0.05 0.86± 0.05 0.75± 0.07 UImNet 0.61± 0.08 0.67± 0.06 0.64± 0.11 0.74± 0.04 0.71± 0.07 0.79± 0.08 0.75± 0.07 0.77± 0.06 0.81± 0.05\nU IQ M UFO-120 2.36± 0.33 2.41± 0.53 2.76± 0.45 2.69± 0.59 2.83± 0.48 3.04± 0.28 2.98± 0.28 3.09± 0.51 3.13± 0.45 EUVP 2.45± 0.46 2.48± 0.77 2.51± 0.36 2.48± 0.09 2.55± 0.06 2.70± 0.31 2.58± 0.07 2.95± 0.38 2.98± 0.28 UImNet 2.32± 0.48 2.38± 0.42 2.79± 0.55 2.84± 0.37 2.92± 0.35 3.32± 0.55 3.19± 0.27 3.23± 0.32 3.26± 0.36\nit focused on the right foreground regions for contrast improvement.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions specific datasets (UFO-120, EUVP, UImNet) used for evaluating image enhancement methods. These datasets are relevant to the topic of All-in-One Image Restoration.",
          "citing_paper_doi": "10.15607/rss.2020.xvi.018",
          "cited_paper_doi": "10.1109/LRA.2020.2974710",
          "citing_paper_url": "https://www.semanticscholar.org/paper/26f278d4d3af8ef7e64325caf09308fec618d9f2",
          "cited_paper_url": "https://www.semanticscholar.org/paper/ee56d8dc8d5900716d763d1cc94f3d226d8f9a25",
          "citing_paper_year": 2020,
          "cited_paper_year": 2019
        },
        {
          "dataset_name": "UFO-120",
          "dataset_description": "Used to evaluate image restoration performance, specifically measuring PSNR, SSIM, and UIQM metrics to assess quality improvements. | Used to evaluate the performance of Deep SESR in generating high-quality HR images from distorted and undistorted LR input patches, focusing on PSNR and SSIM scores.",
          "citing_paper_id": "211020567",
          "cited_paper_id": 9506273,
          "context_text": "However, we observe a slight drop in performance, e.g., 1.8%/1.5%/1.8% lower scores for PSNR/SSIM/UIQM on UFO-120 dataset.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The citation mentions the UFO-120 dataset, which is used to evaluate image restoration performance using PSNR, SSIM, and UIQM metrics.",
          "citing_paper_doi": "10.15607/rss.2020.xvi.018",
          "cited_paper_doi": "10.1109/ICPR.2010.579",
          "citing_paper_url": "https://www.semanticscholar.org/paper/26f278d4d3af8ef7e64325caf09308fec618d9f2",
          "cited_paper_url": "https://www.semanticscholar.org/paper/4a9a98cc86b1e07e548b7edee045275a793f6698",
          "citing_paper_year": 2020,
          "cited_paper_year": 2010
        },
        {
          "dataset_name": "UFO-120",
          "dataset_description": "Used to evaluate image restoration performance, specifically measuring PSNR, SSIM, and UIQM metrics to assess quality improvements.",
          "citing_paper_id": "211020567",
          "cited_paper_id": 29860648,
          "context_text": "However, we observe a slight drop in performance, e.g., 1.8%/1.5%/1.8% lower scores for PSNR/SSIM/UIQM on UFO-120 dataset.",
          "confidence_score": 1.0,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The citation mentions the UFO-120 dataset, which is used to evaluate image restoration performance using PSNR, SSIM, and UIQM metrics.",
          "citing_paper_doi": "10.15607/rss.2020.xvi.018",
          "cited_paper_doi": "10.1109/JOE.2015.2469915",
          "citing_paper_url": "https://www.semanticscholar.org/paper/26f278d4d3af8ef7e64325caf09308fec618d9f2",
          "cited_paper_url": "https://www.semanticscholar.org/paper/64a39d6efee4edef7ca5643798b8905965ec45e6",
          "citing_paper_year": 2020,
          "cited_paper_year": 2016
        }
      ]
    },
    {
      "cited_paper_id": "8282555",
      "citation_count": 0,
      "total_dataset_mentions": 5,
      "unique_datasets": [
        "BSD"
      ],
      "dataset_details": [
        {
          "dataset_name": "BSD",
          "dataset_description": "Used for evaluating image restoration methods, focusing on high-resolution urban scenes with detailed textures. | Used for denoising and deblurring, featuring a large collection of images with various noise levels and blur types. | Used for denoising and deblurring experiments, providing a diverse set of natural images with ground truth annotations. | Used for deblurring tasks, containing real-world motion-blurred images paired with sharp reference images.",
          "citing_paper_id": "270869793",
          "cited_paper_id": 8282555,
          "context_text": "Denoising Deblurring Methods BSD [36] Urban100 [21] GoPro [37] HIDE [49] Plain Text Commands.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions several datasets that are commonly used in image restoration tasks, which align with the research topic of all-in-one image restoration.",
          "citing_paper_doi": "10.48550/arXiv.2407.00676",
          "cited_paper_doi": "10.1109/CVPR.2015.7299156",
          "citing_paper_url": "https://www.semanticscholar.org/paper/ca95c6148941dea8e67f05146dded1f43e6b8f8a",
          "cited_paper_url": "https://www.semanticscholar.org/paper/2946c4e29b057d3543f09b89f547965895676d19",
          "citing_paper_year": 2024,
          "cited_paper_year": 2015
        },
        {
          "dataset_name": "BSD",
          "dataset_description": "Used to illustrate the origin of ground truths and the impact on instance-level representations, highlighting potential issues with dataset-level or density-level separability. | Used to illustrate the potential overlap in ground truths with UCID, highlighting issues in dataset-level or density-level embedding space separation. | Used to illustrate the potential overlap in ground truths with BSD, highlighting issues in dataset-level or density-level embedding space separation.",
          "citing_paper_id": "269214582",
          "cited_paper_id": 14301455,
          "context_text": "On the other hand, the ground truths of different datasets may come from the same dataset, e . g ., BSD (Martin et al., 2001) and UCID (Schaefer & Stich, 2003) dataset, hence the detail-aware property of instance-level representations may not result in well-separated dataset-level or density-level…",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions two specific datasets, BSD and UCID, which are used to illustrate a point about the origin of ground truths and the impact on instance-level representations.",
          "citing_paper_doi": "10.48550/arXiv.2404.12091",
          "cited_paper_doi": "10.1117/12.525375",
          "citing_paper_url": "https://www.semanticscholar.org/paper/348009f068853d56cec79e0eecdeb0279d00893a",
          "cited_paper_url": "https://www.semanticscholar.org/paper/015525f864ccaf28efbdaed46029598441121a9e",
          "citing_paper_year": 2024,
          "cited_paper_year": 2003
        },
        {
          "dataset_name": "BSD",
          "dataset_description": "Used to train and evaluate early image restoration methods, focusing on a small set of 91 images to assess performance. | Used to train and evaluate early image restoration methods, focusing on a larger set of 400 images to assess generalization and robustness.",
          "citing_paper_id": "260843368",
          "cited_paper_id": 10349990,
          "context_text": "Early methods uses small datasets such as the 91 images proposed in [82] and the 400 images from BSD dataset [57].",
          "confidence_score": 0.9,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions two specific datasets: '91 images' and 'BSD dataset'. The '91 images' is a small, specific dataset, and 'BSD dataset' is a well-known dataset in image processing.",
          "citing_paper_doi": "10.1109/CVPRW59228.2023.00178",
          "cited_paper_doi": "10.1109/CVPR.2008.4587647",
          "citing_paper_url": "https://www.semanticscholar.org/paper/851446ea3c70c918cdf8c62db0dd0b1900806e5f",
          "cited_paper_url": "https://www.semanticscholar.org/paper/844236f08ea07371c71b7da2d8183fbd4ac5d799",
          "citing_paper_year": 2023,
          "cited_paper_year": 2008
        },
        {
          "dataset_name": "BSD",
          "dataset_description": "Used for dehazing task, providing 72,135 images for training. | Used to train and evaluate the proposed method for rain removal, focusing on synthetic rainstreak degradation in high-resolution images. | Used to train and evaluate the proposed method for rain removal, focusing on synthetic rainstreak degradation with diverse image conditions. | Used to train and evaluate rain removal methods, focusing on low-resolution images with synthetic rain. | Used to train and evaluate the proposed method for rain removal, focusing on synthetic rainstreak degradation in low-resolution images. | Used for denoising training, providing 4,744 images concatenated with BSD to form the training set. | Used to train and evaluate image denoising methods, focusing on diverse image degradation types. | Used for deraining task, providing 200 images for training. | Used to train and evaluate image denoising methods, focusing on real-world noise patterns. | Used to train and evaluate rain removal methods, focusing on high-resolution images with synthetic rain. | Used for denoising training, providing 400 images concatenated with WED to form the training set. | Used to train and evaluate the proposed method for rain removal, focusing on synthetic rainstreak degradation with challenging scenarios.",
          "citing_paper_id": "266690738",
          "cited_paper_id": 15443600,
          "context_text": "In perspective of training data, we adopt concatenation of 400 images from BSD [6] and 4,744 images from WED [64] dataset as denoising training data, 200 images from Rain100L [104] for deraining task, 72,135 images from SOTS for dehazing task.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions specific datasets used for training various image restoration tasks, including denoising, deraining, and dehazing.",
          "citing_paper_doi": "10.1109/CVPR52733.2024.00277",
          "cited_paper_doi": "10.1109/CVPR.2017.183",
          "citing_paper_url": "https://www.semanticscholar.org/paper/c6495e41d3974476e0be9d5137c3dda51125bcec",
          "cited_paper_url": "https://www.semanticscholar.org/paper/6de27f256e97c014a95706c72af7fa651e8fa34a",
          "citing_paper_year": 2023,
          "cited_paper_year": 2016
        },
        {
          "dataset_name": "BSD",
          "dataset_description": "Used for dehazing task, providing 72,135 images for training. | Used for color image denoising, providing a benchmark for evaluating denoising algorithms in the context of image restoration. | Used for denoising training, providing 4,744 images concatenated with BSD to form the training set. | Used for image deraining, specifically to assess the effectiveness of deraining methods in removing rain streaks from images. | Used for deraining task, providing 200 images for training. | Used for color image denoising, offering a diverse set of images to test and validate denoising techniques. | Used for denoising training, providing 400 images concatenated with WED to form the training set. | Used for image dehazing, to evaluate the performance of dehazing algorithms in improving visibility in hazy images.",
          "citing_paper_id": "266690738",
          "cited_paper_id": 206764694,
          "context_text": "In perspective of training data, we adopt concatenation of 400 images from BSD [6] and 4,744 images from WED [64] dataset as denoising training data, 200 images from Rain100L [104] for deraining task, 72,135 images from SOTS for dehazing task.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions specific datasets used for training various image restoration tasks, including denoising, deraining, and dehazing.",
          "citing_paper_doi": "10.1109/CVPR52733.2024.00277",
          "cited_paper_doi": "10.1109/TPAMI.2010.161",
          "citing_paper_url": "https://www.semanticscholar.org/paper/c6495e41d3974476e0be9d5137c3dda51125bcec",
          "cited_paper_url": "https://www.semanticscholar.org/paper/0e5a262bf59b68ba8a7a1103d16fa33a9f5ffc28",
          "citing_paper_year": 2023,
          "cited_paper_year": 2011
        }
      ]
    },
    {
      "cited_paper_id": "3406592",
      "citation_count": 0,
      "total_dataset_mentions": 5,
      "unique_datasets": [
        "Test100"
      ],
      "dataset_details": [
        {
          "dataset_name": "Test100",
          "dataset_description": "Used to test the deraining method, focusing on performance evaluation with a set of 100 heavy rain images. | Used to test the deraining method, focusing on performance evaluation with a set of 2800 images. | Used to test the deraining method, focusing on performance evaluation with a set of 100 images.",
          "citing_paper_id": "268819753",
          "cited_paper_id": 3406592,
          "context_text": "To evaluate the performance of our method on deraining task, we get the training data from multiple datasets [22, 44, 82, 92, 94] and test on Test100 [94], Rain100H [82] and Test2800 [22].",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions specific datasets used for training and testing a deraining method. These datasets are clearly identified and used for evaluating the performance of the method.",
          "citing_paper_doi": "10.48550/arXiv.2404.00633",
          "cited_paper_doi": "10.1109/CVPR.2018.00079",
          "citing_paper_url": "https://www.semanticscholar.org/paper/979e606d3f09415be6d520213eabf9bfacd9f8dd",
          "cited_paper_url": "https://www.semanticscholar.org/paper/34aaa735409b666b8dc678c23acb600b1d87913b",
          "citing_paper_year": 2024,
          "cited_paper_year": 2018
        },
        {
          "dataset_name": "Test100",
          "dataset_description": "Used to test the deraining method, focusing on performance evaluation with a set of 100 heavy rain images. | Used to test the deraining method, focusing on performance evaluation with a set of 2800 images. | Used to test the deraining method, focusing on performance evaluation with a set of 100 images.",
          "citing_paper_id": "268819753",
          "cited_paper_id": 9007541,
          "context_text": "To evaluate the performance of our method on deraining task, we get the training data from multiple datasets [22, 44, 82, 92, 94] and test on Test100 [94], Rain100H [82] and Test2800 [22].",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions specific datasets used for training and testing a deraining method. These datasets are clearly identified and used for evaluating the performance of the method.",
          "citing_paper_doi": "10.48550/arXiv.2404.00633",
          "cited_paper_doi": "10.1109/CVPR.2016.299",
          "citing_paper_url": "https://www.semanticscholar.org/paper/979e606d3f09415be6d520213eabf9bfacd9f8dd",
          "cited_paper_url": "https://www.semanticscholar.org/paper/7c17025c540b88df14da35229618b5e896ab9528",
          "citing_paper_year": 2024,
          "cited_paper_year": 2016
        },
        {
          "dataset_name": "Test100",
          "dataset_description": "Used to test the deraining method, focusing on performance evaluation with a set of 100 heavy rain images. | Used to test the deraining method, focusing on performance evaluation with a set of 2800 images. | Used to test the deraining method, focusing on performance evaluation with a set of 100 images.",
          "citing_paper_id": "268819753",
          "cited_paper_id": 11922819,
          "context_text": "To evaluate the performance of our method on deraining task, we get the training data from multiple datasets [22, 44, 82, 92, 94] and test on Test100 [94], Rain100H [82] and Test2800 [22].",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions specific datasets used for training and testing a deraining method. These datasets are clearly identified and used for evaluating the performance of the method.",
          "citing_paper_doi": "10.48550/arXiv.2404.00633",
          "cited_paper_doi": "10.1109/TCSVT.2019.2920407",
          "citing_paper_url": "https://www.semanticscholar.org/paper/979e606d3f09415be6d520213eabf9bfacd9f8dd",
          "cited_paper_url": "https://www.semanticscholar.org/paper/920f0c070701caabf023c600f3e310f1906ca818",
          "citing_paper_year": 2024,
          "cited_paper_year": 2017
        },
        {
          "dataset_name": "Test100",
          "dataset_description": "Used for deraining comparisons, focusing on high-resolution images and restoration accuracy. | Used to evaluate the de-raining method, focusing on average PSNR for low-quality image restoration. | Used for deraining comparisons, focusing on high-resolution images and restoration performance using a multi-stream dense network. | Used for deraining comparisons, focusing on image quality and restoration performance using a multi-stream dense network. | Used to assess the effectiveness of the de-raining method on high-resolution rainy images, containing 100 test images. | Used to evaluate the performance of the de-raining method, focusing on a set of 100 test images. | Used as a benchmark for initial testing and quick validation of de-raining models, providing a smaller but diverse set of images. | Used to validate de-raining algorithms on a moderate-sized dataset, ensuring consistency and reliability of results. | Used for deraining comparisons, focusing on low-resolution images and restoration effectiveness. | Used for training models on image denoising, focusing on high-quality denoising for smartphone cameras. | Used for deraining comparisons, focusing on a larger set of images and restoration performance using a multi-stream dense network. | Used for deraining comparisons, focusing on a moderate set of images and restoration performance using a multi-stream dense network. | Used to test the de-raining method on a larger set of 2800 images, assessing generalization and robustness. | Used to evaluate the de-raining method on another set of 1200 images, providing additional validation of performance. | Used for deraining comparisons, offering additional test cases for robustness assessment. | Used to evaluate the de-raining method, focusing on average PSNR for high-quality image restoration. | Used for deraining comparisons, focusing on image restoration quality and performance metrics. | Used to test the generalization of de-raining models on a large-scale dataset, evaluating robustness across various rain conditions. | Used for training models on image deraining, consisting of 13,712 clean-rain image pairs. | Used to assess de-raining performance on low-resolution images, emphasizing the impact of density-aware techniques. | Used to evaluate the de-raining method, focusing on average PSNR for image restoration quality. | Used to evaluate de-raining algorithms on high-resolution images, focusing on the effectiveness of multi-stream dense networks. | Used to evaluate the de-raining method, focusing on average PSNR for a larger test set. | Used for deraining comparisons, focusing on low-resolution images and restoration performance using a multi-stream dense network. | Used to evaluate the de-raining method, focusing on average PSNR for another test set. | Used to evaluate the de-raining method on low-resolution rainy images, consisting of 100 test images. | Used for deraining comparisons, providing a larger dataset for comprehensive evaluation. | Used for training models on image deblurring, specifically targeting dynamic scene deblurring.",
          "citing_paper_id": "234482841",
          "cited_paper_id": 3406592,
          "context_text": "Test100 [61] Rain100H [49] Rain100L [49] Test2800 [10] Table 3: Deraining comparisons on Test100 [61], Rain100H [49], Rain100L [49], Test2800 [10] and Test1200 [60].",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The citation mentions multiple datasets used for deraining comparisons. These datasets are specific and have clear identifiers.",
          "citing_paper_doi": "10.1109/CVPRW53098.2021.00027",
          "cited_paper_doi": "10.1109/CVPR.2018.00079",
          "citing_paper_url": "https://www.semanticscholar.org/paper/9b8cbe9a636b00b3543d8e1f98c24de12d6c3d17",
          "cited_paper_url": "https://www.semanticscholar.org/paper/34aaa735409b666b8dc678c23acb600b1d87913b",
          "citing_paper_year": 2021,
          "cited_paper_year": 2018
        },
        {
          "dataset_name": "Test100",
          "dataset_description": "Used to evaluate the performance of image de-raining methods, specifically comparing PSNR improvements against other datasets. The dataset contains images with rain artifacts for restoration evaluation. | Used for visual comparisons in image de-raining, specifically to evaluate the performance of a conditional generative adversarial network. | Used for image deraining comparisons, specifically evaluating the performance of a conditional generative adversarial network on rain removal tasks. | Used to evaluate image de-raining performance, focusing on the effectiveness of the conditional generative adversarial network in removing rain artifacts.",
          "citing_paper_id": "270736284",
          "cited_paper_id": 11922819,
          "context_text": "Image deraining comparisons on the Test100 [89] dataset.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions 'Test100' as a dataset used for image deraining comparisons. The name 'Test100' is specific and plausible, fitting the criteria for inclusion.",
          "citing_paper_doi": "10.1109/TPAMI.2024.3419007",
          "cited_paper_doi": "10.1109/TCSVT.2019.2920407",
          "citing_paper_url": "https://www.semanticscholar.org/paper/804d2302880b91965e84a7c72ac26429d26792a0",
          "cited_paper_url": "https://www.semanticscholar.org/paper/920f0c070701caabf023c600f3e310f1906ca818",
          "citing_paper_year": 2024,
          "cited_paper_year": 2017
        }
      ]
    },
    {
      "cited_paper_id": "221090722",
      "citation_count": 0,
      "total_dataset_mentions": 5,
      "unique_datasets": [
        "NHR"
      ],
      "dataset_details": [
        {
          "dataset_name": "NHR",
          "dataset_description": "Used to assess the performance of the proposed method in haze removal, demonstrating a 0.10 dB PSNR improvement over FocalNet. | Used for image dehazing comparisons in nighttime scenes, evaluating various methods including the proposed CSNet. The dataset focuses on nighttime image restoration tasks. | Used for image dehazing comparisons, evaluating various models including CSNet, FocalNet, and HCD. The dataset focuses on assessing performance metrics such as PSNR and SSIM. | Used to evaluate the effectiveness of the proposed method in removing real-world haze degradations, comparing performance gains in PSNR against FocalNet. | Used to evaluate defocus deblurring performance, comparing PSNR gains between CSNet and FocalNet in the combined category. | Used to evaluate the performance of CSNet in image restoration, specifically measuring PSNR improvement over FocalNet. The dataset is relevant for assessing desnowing algorithms. | Used to evaluate the performance of image restoration algorithms, specifically measuring PSNR gain over the FocalNet algorithm.",
          "citing_paper_id": "271500033",
          "cited_paper_id": 259298517,
          "context_text": "According to the complexity of different tasks, we set N (Figure 3 (b)) to 3 for Methods PSNR ↑ SSIM ↑ NDIM [Zhang et al. , 2014] 14.31 0.526 GS [Li et al. , 2015] 17.32 0.629 MRPF [Zhang et al. , 2017] 16.95 0.667 MRP [Zhang et al. , 2017] 19.93 0.777 OSFD [Zhang et al. , 2020] 21.32 0.804 HCD [Wang et al. , 2024] 23.43 0.953 FocalNet [Cui et al. , 2023a] 25.35 0.969 CSNet (Ours) 26.13 0.971 Table 2: Image dehazing comparisons on the NHR [Zhang et al. , 2020] dataset for nighttime scenes.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions the 'NHR' dataset, which is used for image dehazing comparisons in nighttime scenes. The dataset is referenced in the table comparing different methods.",
          "citing_paper_doi": "10.24963/ijcai.2024/80",
          "cited_paper_doi": null,
          "citing_paper_url": "https://www.semanticscholar.org/paper/371579255875a7d7726e3402158dd2eb79307f7c",
          "cited_paper_url": "https://www.semanticscholar.org/paper/6af728164e4424218967fd356b3448f5037acd88",
          "citing_paper_year": 2024,
          "cited_paper_year": 2023
        },
        {
          "dataset_name": "NHR",
          "dataset_description": "Used to compare the performance of the proposed method against 6 state-of-the-art methods in nighttime image dehazing, focusing on visual quality and quantitative metrics.",
          "citing_paper_id": "268678266",
          "cited_paper_id": 7546676,
          "context_text": "We further present comparisons on the nighttime image dehazing dataset NHR (Zhang et al. 2020) with 6 state-of-the-art methods: GS (Li, Tan, and Brown 2015), MRPF (Zhang et al. 2017), MRP (Zhang et al. 2017), OSFD (Zhang et al. 2020), HCD (Wang et al. 2022a), and FocalNet (Cui et al. 2023a).",
          "confidence_score": 1.0,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions the 'nighttime image dehazing dataset NHR' which is a specific dataset used for evaluating image restoration methods.",
          "citing_paper_doi": "10.1609/aaai.v38i2.27907",
          "cited_paper_doi": "10.1109/CVPR.2017.742",
          "citing_paper_url": "https://www.semanticscholar.org/paper/d5a17832f115c16d4097ea39f204917734f9fba0",
          "cited_paper_url": "https://www.semanticscholar.org/paper/5264b2a9fc945ae0947d29bf19834ec8fdd5c0c1",
          "citing_paper_year": 2024,
          "cited_paper_year": 2017
        },
        {
          "dataset_name": "NHR",
          "dataset_description": "Used to compare the performance of the proposed method against 6 state-of-the-art methods in nighttime image dehazing, focusing on visual quality and quantitative metrics.",
          "citing_paper_id": "268678266",
          "cited_paper_id": 17319211,
          "context_text": "We further present comparisons on the nighttime image dehazing dataset NHR (Zhang et al. 2020) with 6 state-of-the-art methods: GS (Li, Tan, and Brown 2015), MRPF (Zhang et al. 2017), MRP (Zhang et al. 2017), OSFD (Zhang et al. 2020), HCD (Wang et al. 2022a), and FocalNet (Cui et al. 2023a).",
          "confidence_score": 1.0,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions the 'nighttime image dehazing dataset NHR' which is a specific dataset used for evaluating image restoration methods.",
          "citing_paper_doi": "10.1609/aaai.v38i2.27907",
          "cited_paper_doi": "10.1109/ICCV.2015.34",
          "citing_paper_url": "https://www.semanticscholar.org/paper/d5a17832f115c16d4097ea39f204917734f9fba0",
          "cited_paper_url": "https://www.semanticscholar.org/paper/ff62b041e26705dcaab686d98ef09e6102805f11",
          "citing_paper_year": 2024,
          "cited_paper_year": 2015
        },
        {
          "dataset_name": "NHR",
          "dataset_description": "Used to assess the performance of the proposed method in haze removal, demonstrating a 0.10 dB PSNR improvement over FocalNet. | Used for image dehazing comparisons in nighttime scenes, evaluating various methods including the proposed CSNet. The dataset focuses on nighttime image restoration tasks. | Used for image dehazing comparisons, evaluating various models including CSNet, FocalNet, and HCD. The dataset focuses on assessing performance metrics such as PSNR and SSIM. | Used to evaluate the effectiveness of the proposed method in removing real-world haze degradations, comparing performance gains in PSNR against FocalNet. | Used to evaluate defocus deblurring performance, comparing PSNR gains between CSNet and FocalNet in the combined category. | Used to evaluate the performance of CSNet in image restoration, specifically measuring PSNR improvement over FocalNet. The dataset is relevant for assessing desnowing algorithms. | Used to evaluate the performance of image restoration algorithms, specifically measuring PSNR gain over the FocalNet algorithm.",
          "citing_paper_id": "271500033",
          "cited_paper_id": 260862867,
          "context_text": "According to the complexity of different tasks, we set N (Figure 3 (b)) to 3 for Methods PSNR ↑ SSIM ↑ NDIM [Zhang et al. , 2014] 14.31 0.526 GS [Li et al. , 2015] 17.32 0.629 MRPF [Zhang et al. , 2017] 16.95 0.667 MRP [Zhang et al. , 2017] 19.93 0.777 OSFD [Zhang et al. , 2020] 21.32 0.804 HCD [Wang et al. , 2024] 23.43 0.953 FocalNet [Cui et al. , 2023a] 25.35 0.969 CSNet (Ours) 26.13 0.971 Table 2: Image dehazing comparisons on the NHR [Zhang et al. , 2020] dataset for nighttime scenes.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions the 'NHR' dataset, which is used for image dehazing comparisons in nighttime scenes. The dataset is referenced in the table comparing different methods.",
          "citing_paper_doi": "10.24963/ijcai.2024/80",
          "cited_paper_doi": "10.24963/ijcai.2023/72",
          "citing_paper_url": "https://www.semanticscholar.org/paper/371579255875a7d7726e3402158dd2eb79307f7c",
          "cited_paper_url": "https://www.semanticscholar.org/paper/90c6a6d2582f1d76da6768943d150addb2ba7559",
          "citing_paper_year": 2024,
          "cited_paper_year": 2023
        },
        {
          "dataset_name": "NHR",
          "dataset_description": "Used to evaluate nighttime dehazing algorithms, focusing on synthetic benchmark performance and image quality improvements. | Used for image dehazing comparisons, specifically for nighttime scenes, to evaluate the performance of different models.",
          "citing_paper_id": "271500033",
          "cited_paper_id": 221090722,
          "context_text": "In addition, we provide nighttime dehazing results on two datasets, NHR [Zhang et al. , 2020] and GTA5 [Yan et al. , 2020], in Table 2 and Table 3, respectively.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The citation mentions two specific datasets, NHR and GTA5, which are used for evaluating nighttime dehazing results.",
          "citing_paper_doi": "10.24963/ijcai.2024/80",
          "cited_paper_doi": "10.1145/3394171.3413763",
          "citing_paper_url": "https://www.semanticscholar.org/paper/371579255875a7d7726e3402158dd2eb79307f7c",
          "cited_paper_url": "https://www.semanticscholar.org/paper/b4ba7b0c470241adf4cb85da6f141228362e7a7b",
          "citing_paper_year": 2024,
          "cited_paper_year": 2020
        }
      ]
    },
    {
      "cited_paper_id": "236950592",
      "citation_count": 0,
      "total_dataset_mentions": 5,
      "unique_datasets": [
        "Haze-4K"
      ],
      "dataset_details": [
        {
          "dataset_name": "Haze-4K",
          "dataset_description": "Used for comprehensive evaluation of image dehazing techniques, offering a diverse set of synthetic and real-world images to assess generalization and accuracy. | Composed of 33 pairs of outdoor hazy and haze-free images, used to benchmark image dehazing algorithms with dense haze conditions. | Used for benchmarking image dehazing methods, providing dense haze and haze-free images to test algorithm effectiveness in various conditions. | Used to train and evaluate single-image dehazing models, containing 4,000 hazy images to assess dehazing performance. | Collected 443,950 real-world training images and 5,342 testing images, used for comprehensive evaluation of dehazing models in diverse scenarios. | Used for evaluating image dehazing algorithms, focusing on synthetic and real-world hazy images to assess performance and robustness.",
          "citing_paper_id": "261031011",
          "cited_paper_id": 39760169,
          "context_text": "For the image dehazing task, three typical datasets are used for evaluation, including Haze-4K [274], Dense-Haze [275], and RESIDE [276].",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions three specific datasets used for evaluating image dehazing tasks, which are relevant to the topic of All-in-One Image Restoration.",
          "citing_paper_doi": "10.48550/arXiv.2308.09388",
          "cited_paper_doi": "10.1109/TIP.2018.2867951",
          "citing_paper_url": "https://www.semanticscholar.org/paper/70ba95384664ab7becfd8196d20a46fe66112c91",
          "cited_paper_url": "https://www.semanticscholar.org/paper/abf75b81b96ff2cd262086675deba55f907108db",
          "citing_paper_year": 2023,
          "cited_paper_year": 2017
        },
        {
          "dataset_name": "Haze-4K",
          "dataset_description": "Used for comprehensive evaluation of image dehazing techniques, offering a diverse set of synthetic and real-world images to assess generalization and accuracy. | Composed of 33 pairs of outdoor hazy and haze-free images, used to benchmark image dehazing algorithms with dense haze conditions. | Used for benchmarking image dehazing methods, providing dense haze and haze-free images to test algorithm effectiveness in various conditions. | Used to train and evaluate single-image dehazing models, containing 4,000 hazy images to assess dehazing performance. | Collected 443,950 real-world training images and 5,342 testing images, used for comprehensive evaluation of dehazing models in diverse scenarios. | Used for evaluating image dehazing algorithms, focusing on synthetic and real-world hazy images to assess performance and robustness.",
          "citing_paper_id": "261031011",
          "cited_paper_id": 102350705,
          "context_text": "For the image dehazing task, three typical datasets are used for evaluation, including Haze-4K [274], Dense-Haze [275], and RESIDE [276].",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions three specific datasets used for evaluating image dehazing tasks, which are relevant to the topic of All-in-One Image Restoration.",
          "citing_paper_doi": "10.48550/arXiv.2308.09388",
          "cited_paper_doi": "10.1109/ICIP.2019.8803046",
          "citing_paper_url": "https://www.semanticscholar.org/paper/70ba95384664ab7becfd8196d20a46fe66112c91",
          "cited_paper_url": "https://www.semanticscholar.org/paper/a3be7b43518abc11a074e6c15d1b5c443e53737c",
          "citing_paper_year": 2023,
          "cited_paper_year": 2019
        },
        {
          "dataset_name": "Haze-4K",
          "dataset_description": "Used for comprehensive evaluation of image dehazing techniques, offering a diverse set of synthetic and real-world images to assess generalization and accuracy. | Used for evaluating image dehazing algorithms, focusing on synthetic and real-world hazy images to assess performance and robustness. | Used for benchmarking image dehazing methods, providing dense haze and haze-free images to test algorithm effectiveness in various conditions.",
          "citing_paper_id": "261031011",
          "cited_paper_id": 236950592,
          "context_text": "For the image dehazing task, three typical datasets are used for evaluation, including Haze-4K [274], Dense-Haze [275], and RESIDE [276].",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions three specific datasets used for evaluating image dehazing tasks, which are relevant to the topic of All-in-One Image Restoration.",
          "citing_paper_doi": "10.48550/arXiv.2308.09388",
          "cited_paper_doi": "10.1145/3474085.3475331",
          "citing_paper_url": "https://www.semanticscholar.org/paper/70ba95384664ab7becfd8196d20a46fe66112c91",
          "cited_paper_url": "https://www.semanticscholar.org/paper/f9295ac4abc2235414b9f34752f1cf745dbfaa9a",
          "citing_paper_year": 2023,
          "cited_paper_year": 2021
        },
        {
          "dataset_name": "Haze-4K",
          "dataset_description": "Used to evaluate the model on real-world outdoor dehazing tasks, focusing on diverse environmental conditions. | Used to evaluate the model on synthetic and real-world dehazing tasks, focusing on high-resolution images. | Used for visual comparisons in single-image dehazing, providing high-resolution images to evaluate dehazing performance. | Used for visual comparisons in single-image dehazing, focusing on indoor scenes to evaluate dehazing algorithms. | Used for comparing image dehazing performance, focusing on realistic synthetic haze conditions to evaluate restoration quality. | Used to evaluate the model on real-world indoor dehazing tasks, focusing on realistic hazy and haze-free images. | Used for visual comparisons in single-image dehazing, focusing on outdoor scenes to evaluate dehazing algorithms. | Used for image dehazing comparisons, focusing on evaluating the effectiveness of dehazing algorithms on real-world images.",
          "citing_paper_id": "270736284",
          "cited_paper_id": 236950592,
          "context_text": "Furthermore, we evaluate our model on more synthetic and real-world datasets, such as Haze4K [38], O-HAZE [39], and I-Haze [40].",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions three specific datasets used for evaluating the model: Haze4K, O-HAZE, and I-Haze. These datasets are relevant to the topic of image restoration, particularly dehazing.",
          "citing_paper_doi": "10.1109/TPAMI.2024.3419007",
          "cited_paper_doi": "10.1145/3474085.3475331",
          "citing_paper_url": "https://www.semanticscholar.org/paper/804d2302880b91965e84a7c72ac26429d26792a0",
          "cited_paper_url": "https://www.semanticscholar.org/paper/f9295ac4abc2235414b9f34752f1cf745dbfaa9a",
          "citing_paper_year": 2024,
          "cited_paper_year": 2021
        },
        {
          "dataset_name": "Haze-4K",
          "dataset_description": "This dataset 'Haze4K' was mentioned in the citation context but no detailed description was generated.",
          "citing_paper_id": "258959359",
          "cited_paper_id": 244345938,
          "context_text": "For the Haze4K dataset, we follow the previous work [68].",
          "confidence_score": 1.0,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions the Haze4K dataset, which is a specific dataset used for image dehazing. The dataset is referenced for following previous work.",
          "citing_paper_doi": "10.48550/arXiv.2305.17863",
          "cited_paper_doi": null,
          "citing_paper_url": "https://www.semanticscholar.org/paper/7d8cfd80c9bc7bbfd52b4ee9c9bbc5b7bfd9cc21",
          "cited_paper_url": "https://www.semanticscholar.org/paper/ae0843970213145ec57b7414624cde1a8ca10e62",
          "citing_paper_year": 2023,
          "cited_paper_year": 2021
        }
      ]
    },
    {
      "cited_paper_id": "247494896",
      "citation_count": 0,
      "total_dataset_mentions": 5,
      "unique_datasets": [
        "LOL v1"
      ],
      "dataset_details": [
        {
          "dataset_name": "LOL v1",
          "dataset_description": "Used to assess the effectiveness of the proposed method in indoor image restoration, highlighting significant improvements in image quality. | Used to evaluate low-light image enhancement on Sony camera images, focusing on dynamic range and noise reduction. | Used to evaluate the performance of the proposed method in image restoration, specifically focusing on improvements in dynamic scene illumination. | Used for quantitative comparisons in low-light image restoration, focusing on dynamic scenes. | Used to test low-light image enhancement on synthetic images, providing controlled scenarios for evaluation. | Used to evaluate the method on low-light images, focusing on indoor and outdoor scenes to assess restoration quality. | Used to assess low-light image enhancement on mobile device images, emphasizing practical application and user experience. | Used for an ablation study to evaluate the performance of image restoration techniques in outdoor low-light conditions, focusing on dynamic scenes. | Used to evaluate the method on low-light images, focusing on multi-image denoising and deblurring scenarios. | Used to evaluate low-light image enhancement in indoor settings, focusing on common household lighting conditions. | Used for quantitative comparisons in low-light image restoration, focusing on indoor and outdoor scenes. | Used to evaluate visual restoration methods in outdoor environments, focusing on high-quality video data with mechatronic alignment. | Used for quantitative comparisons in low-light image restoration, focusing on specific indoor and outdoor conditions. | Used to test the proposed method on synthetic low-light images, demonstrating substantial gains in restoration quality. | Used to evaluate the method on low-light images, specifically designed for single-image denoising and deblurring. | Used to evaluate visual restoration methods in indoor environments, focusing on high-quality video data with mechatronic alignment. | Used to evaluate low-light image enhancement, focusing on real-world images with ground truth pairs. | Used to assess low-light image enhancement on real-world images, emphasizing realistic lighting conditions. | Used to evaluate the method on low-light images, covering both indoor and outdoor scenes to test robustness.",
          "citing_paper_id": "257496232",
          "cited_paper_id": 244114613,
          "context_text": "We eveluate our method on LOL (v1 [53] and v2 [58]), SID [8], SMID [9], and SDSD [47] (indoor and outdoor).",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions specific datasets used for evaluating the method, which are relevant to image restoration.",
          "citing_paper_doi": "10.1109/ICCV51070.2023.01149",
          "cited_paper_doi": "10.1109/ICCV48922.2021.00956",
          "citing_paper_url": "https://www.semanticscholar.org/paper/42754ad2b7b2193b4d8e3d067808cea8913869c4",
          "cited_paper_url": "https://www.semanticscholar.org/paper/914e29068503742eb2407ec9fd651371dbe72dc0",
          "citing_paper_year": 2023,
          "cited_paper_year": 2021
        },
        {
          "dataset_name": "LOL v1",
          "dataset_description": "Used to evaluate the method on low-light image enhancement, focusing on improving visibility and color accuracy in dark conditions. | Used to evaluate the method on low-light image enhancement, specifically for capturing high-quality images in very low light conditions. | Used to evaluate the method on low-light image enhancement, focusing on dynamic range and noise reduction in low-light scenarios. | Used for quantitative comparisons in low-light image enhancement, focusing on indoor and outdoor scenes. | Used for quantitative comparisons in low-light image enhancement, focusing on motion in the dark. | Used for quantitative comparisons in low-light image enhancement, covering both indoor and outdoor scenarios. | Used to evaluate the method on professional image editing, focusing on color correction and enhancement in a variety of lighting conditions. | Used to evaluate the method on low-light image enhancement, focusing on synthetic and real-world low-light image pairs. | Used for quantitative comparisons in low-light image enhancement, specifically for seeing in the dark.",
          "citing_paper_id": "257496232",
          "cited_paper_id": 4691825,
          "context_text": "We eveluate our method on LOL (v1 [54] and v2 [59]), SID [9], SMID [10], SDSD [48], and FiveK [5] datasets.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions several datasets by name, which are used to evaluate the method for image restoration. These datasets are specific and relevant to the topic of all-in-one image restoration.",
          "citing_paper_doi": "10.1109/ICCV51070.2023.01149",
          "cited_paper_doi": "10.1109/CVPR.2018.00347",
          "citing_paper_url": "https://www.semanticscholar.org/paper/42754ad2b7b2193b4d8e3d067808cea8913869c4",
          "cited_paper_url": "https://www.semanticscholar.org/paper/2286a6ae670644c797ff1e6766b7af7df44a61d5",
          "citing_paper_year": 2023,
          "cited_paper_year": 2018
        },
        {
          "dataset_name": "LOL v1",
          "dataset_description": "Used to evaluate the method on low-light image enhancement, focusing on improving visibility and color accuracy in dark conditions. | Used to evaluate the method on low-light image enhancement, specifically for capturing high-quality images in very low light conditions. | Used to evaluate the method on low-light image enhancement, focusing on dynamic range and noise reduction in low-light scenarios. | Used for quantitative comparisons in low-light image enhancement, focusing on indoor and outdoor scenes. | Used for quantitative comparisons in low-light image enhancement, focusing on motion in the dark. | Used for quantitative comparisons in low-light image enhancement, covering both indoor and outdoor scenarios. | Used to evaluate the method on professional image editing, focusing on color correction and enhancement in a variety of lighting conditions. | Used to assess indoor image restoration quality, emphasizing enhancements in low-light scenarios. | Used to test synthetic low-light image restoration, highlighting significant improvements in PSNR. | Used to evaluate the method on low-light image enhancement, focusing on synthetic and real-world low-light image pairs. | Used to evaluate image restoration performance in low-light conditions, focusing on signal-to-noise ratio improvements. | Used for quantitative comparisons in low-light image enhancement, specifically for seeing in the dark.",
          "citing_paper_id": "257496232",
          "cited_paper_id": 201692604,
          "context_text": "We eveluate our method on LOL (v1 [54] and v2 [59]), SID [9], SMID [10], SDSD [48], and FiveK [5] datasets.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions several datasets by name, which are used to evaluate the method for image restoration. These datasets are specific and relevant to the topic of all-in-one image restoration.",
          "citing_paper_doi": "10.1109/ICCV51070.2023.01149",
          "cited_paper_doi": "10.1109/ICCV.2019.00328",
          "citing_paper_url": "https://www.semanticscholar.org/paper/42754ad2b7b2193b4d8e3d067808cea8913869c4",
          "cited_paper_url": "https://www.semanticscholar.org/paper/c46503888ad5838ec044f9ca0cf8616d94e43c91",
          "citing_paper_year": 2023,
          "cited_paper_year": 2019
        },
        {
          "dataset_name": "LOL v1",
          "dataset_description": "Used to evaluate the method on low-light image enhancement, focusing on improving visibility and color accuracy in dark conditions. | Used to evaluate the method on low-light image enhancement, specifically for capturing high-quality images in very low light conditions. | Used to evaluate the method on low-light image enhancement, focusing on dynamic range and noise reduction in low-light scenarios. | Used to evaluate the method on professional image editing, focusing on color correction and enhancement in a variety of lighting conditions. | Used to evaluate the method on low-light image enhancement, focusing on synthetic and real-world low-light image pairs.",
          "citing_paper_id": "257496232",
          "cited_paper_id": 207778473,
          "context_text": "We eveluate our method on LOL (v1 [54] and v2 [59]), SID [9], SMID [10], SDSD [48], and FiveK [5] datasets.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions several datasets by name, which are used to evaluate the method for image restoration. These datasets are specific and relevant to the topic of all-in-one image restoration.",
          "citing_paper_doi": "10.1109/ICCV51070.2023.01149",
          "cited_paper_doi": null,
          "citing_paper_url": "https://www.semanticscholar.org/paper/42754ad2b7b2193b4d8e3d067808cea8913869c4",
          "cited_paper_url": "https://www.semanticscholar.org/paper/5ee57b18a7be2b671ffc6f5ce5dbbe92112ee0fb",
          "citing_paper_year": 2023,
          "cited_paper_year": 2007
        },
        {
          "dataset_name": "LOL v1",
          "dataset_description": "Used to validate the performance of UniUIR on backlit image enhancement, comparing it against other image enhancement methods. | Used to validate the performance of UniUIR on low-light image enhancement, comparing it against other image enhancement methods. | Used to validate the performance of UniUIR on real-world low-light image enhancement, comparing it against other image enhancement methods. | Used to validate the performance of UniUIR on low-light image enhancement, specifically focusing on Sony camera images.",
          "citing_paper_id": "275789940",
          "cited_paper_id": 247494896,
          "context_text": "…on Other Low-level Vision Tasks: To validate the effectiveness of the proposed UniUIR for low-light and backlit image enhancement tasks, we retrained UniUIR and compared its performance against image enhancement methods on the LOL-v1 [63], LOL-v2-real [64], SID [65], and BAID [66] datasets.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The citation mentions specific datasets used for validating the performance of UniUIR on low-light and backlit image enhancement tasks.",
          "citing_paper_doi": "10.48550/arXiv.2501.12981",
          "cited_paper_doi": "10.1016/j.cviu.2022.103403",
          "citing_paper_url": "https://www.semanticscholar.org/paper/298b051872e5b7cf945ba4bf7bbfb9d83c9f738f",
          "cited_paper_url": "https://www.semanticscholar.org/paper/4b9c0dca4a6dbc201b2ba0842738fd0da257d8b4",
          "citing_paper_year": 2025,
          "cited_paper_year": 2022
        }
      ]
    },
    {
      "cited_paper_id": "10514149",
      "citation_count": 0,
      "total_dataset_mentions": 5,
      "unique_datasets": [
        "Kodak24"
      ],
      "dataset_details": [
        {
          "dataset_name": "Kodak24",
          "dataset_description": "Used to test the performance of image deraining algorithms, assessing the ability to remove rain streaks from images. | Used for denoising experiments with a noise level of σ = 50, evaluating the effectiveness of the proposed FROT cost in image restoration. | Used for de-raining tasks to validate the transport residual condition module, focusing on removing rain streaks from images. | Used for deraining experiments, testing the ability of the proposed FROT cost to remove rain artifacts from images. | Used for super-resolution tasks to validate the transport residual condition module, focusing on high-resolution image reconstruction. | Used to test image denoising performance at a noise level of σ = 25, evaluating the effectiveness of the restoration method. | Used for super-resolution experiments, assessing the performance of the proposed FROT cost in enhancing image resolution. | Used for denoising tasks to validate the transport residual condition module, focusing on reducing noise in images. | Used for dehazing experiments, evaluating the proposed FROT cost's effectiveness in improving visibility in hazy images. | Used to evaluate RCOT under noise levels σ ∈ {25, 50}, focusing on image restoration performance.",
          "citing_paper_id": "269605449",
          "cited_paper_id": null,
          "context_text": "We evaluate RCOT on the Ko-dak24 (Franzen, 1999) and CBSD68 (Martin et al., 2001) datasets under noise levels σ ∈ { 25 , 50 } .",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions two specific datasets, Ko-dak24 and CBSD68, which are used to evaluate the RCOT method under specific noise levels.",
          "citing_paper_doi": "10.48550/arXiv.2405.02843",
          "cited_paper_doi": null,
          "citing_paper_url": "https://www.semanticscholar.org/paper/0c7542272e5edcff29608a0e37f9a3b705b363e7",
          "cited_paper_url": null,
          "citing_paper_year": 2024,
          "cited_paper_year": null
        },
        {
          "dataset_name": "Kodak24",
          "dataset_description": "Used for denoising experiments with a noise level of σ = 50, evaluating the effectiveness of the proposed FROT cost in image restoration. | Used for deraining experiments, testing the ability of the proposed FROT cost to remove rain artifacts from images. | Used for super-resolution experiments, assessing the performance of the proposed FROT cost in enhancing image resolution. | Used to train dehazing models, focusing on synthetic hazy images to improve model performance. | Used to test the performance of dehazing models, providing real-world hazy and haze-free outdoor images for evaluation. | Used for dehazing experiments, evaluating the proposed FROT cost's effectiveness in improving visibility in hazy images.",
          "citing_paper_id": "269605449",
          "cited_paper_id": 39760169,
          "context_text": "We investigate the effect of the proposed FROT cost on four restoration tasks (Denoising on Kodak24 (Franzen, 1999) with noise level σ = 50 , SR on DIV2K (Agustsson & Timofte, 2017), Deraining on Rain100L (Fan et al., 2019), and Dehazing on SOTS (Li et al., 2018a)).",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions specific datasets used for evaluating the proposed method on various image restoration tasks.",
          "citing_paper_doi": "10.48550/arXiv.2405.02843",
          "cited_paper_doi": "10.1109/TIP.2018.2867951",
          "citing_paper_url": "https://www.semanticscholar.org/paper/0c7542272e5edcff29608a0e37f9a3b705b363e7",
          "cited_paper_url": "https://www.semanticscholar.org/paper/abf75b81b96ff2cd262086675deba55f907108db",
          "citing_paper_year": 2024,
          "cited_paper_year": 2017
        },
        {
          "dataset_name": "Kodak24",
          "dataset_description": "Used for evaluating image demosaicing, emphasizing the quality of restored images and the effectiveness of the restoration algorithm. | Used for evaluating image demosaicing performance, focusing on color accuracy and detail preservation in natural images. | Used for evaluating image demosaicing, specifically assessing the restoration of high-frequency details in urban scenes. | Used to assess denoising effectiveness on a diverse set of natural images, emphasizing robustness across different image types. | Used for evaluating image demosaicing, focusing on the restoration of detailed textures and edges in diverse image content. | Used to test denoising algorithms on high-resolution urban scenes, highlighting detail preservation and noise reduction. | Used to evaluate image denoising performance, focusing on natural image quality and visual fidelity.",
          "citing_paper_id": "260748050",
          "cited_paper_id": 1900475,
          "context_text": "Following RNAN Zhang et al. (2019), PANet is evaluated on standard benchmarks for image denoising: Kodak24 (http://r0k.us/graphics/kodak/), BSD68 Martin et al. (2001), and Urban100 Huang et al. (2015).",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions specific datasets used for evaluating image denoising methods. These datasets are clearly named and are relevant to the topic of image restoration.",
          "citing_paper_doi": "10.1007/s11263-023-01843-5",
          "cited_paper_doi": "10.1109/cvpr.2017.300",
          "citing_paper_url": "https://www.semanticscholar.org/paper/e98cc6c252a7d69898730df17449fc1a09d1a1ae",
          "cited_paper_url": "https://www.semanticscholar.org/paper/dc8861b4ab6799be542829ae1ace13f23cf807cd",
          "citing_paper_year": 2023,
          "cited_paper_year": 2017
        },
        {
          "dataset_name": "Kodak24",
          "dataset_description": "Used for evaluating image demosaicing, emphasizing the quality of restored images and the effectiveness of the restoration algorithm. | Used for evaluating image demosaicing performance, focusing on color accuracy and detail preservation in natural images. | Used for evaluating image demosaicing, specifically assessing the restoration of high-frequency details in urban scenes. | Used to assess denoising effectiveness on a diverse set of natural images, emphasizing robustness across different image types. | Used for evaluating image demosaicing, focusing on the restoration of detailed textures and edges in diverse image content. | Used to test denoising algorithms on high-resolution urban scenes, highlighting detail preservation and noise reduction. | Used to evaluate image denoising performance, focusing on natural image quality and visual fidelity.",
          "citing_paper_id": "260748050",
          "cited_paper_id": 10514149,
          "context_text": "Following RNAN Zhang et al. (2019), PANet is evaluated on standard benchmarks for image denoising: Kodak24 (http://r0k.us/graphics/kodak/), BSD68 Martin et al. (2001), and Urban100 Huang et al. (2015).",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions specific datasets used for evaluating image denoising methods. These datasets are clearly named and are relevant to the topic of image restoration.",
          "citing_paper_doi": "10.1007/s11263-023-01843-5",
          "cited_paper_doi": "10.1109/TIP.2018.2839891",
          "citing_paper_url": "https://www.semanticscholar.org/paper/e98cc6c252a7d69898730df17449fc1a09d1a1ae",
          "cited_paper_url": "https://www.semanticscholar.org/paper/e248ac3596d48ce338244624c2fd194dc0651bc6",
          "citing_paper_year": 2023,
          "cited_paper_year": 2017
        },
        {
          "dataset_name": "Kodak24",
          "dataset_description": "Used for evaluating image demosaicing, emphasizing the quality of restored images and the effectiveness of the restoration algorithm. | Used for evaluating image demosaicing performance, focusing on color accuracy and detail preservation in natural images. | Used for evaluating image demosaicing, specifically assessing the restoration of high-frequency details in urban scenes. | Used to assess denoising effectiveness on a diverse set of natural images, emphasizing robustness across different image types. | Used for evaluating image demosaicing, focusing on the restoration of detailed textures and edges in diverse image content. | Used to test denoising algorithms on high-resolution urban scenes, highlighting detail preservation and noise reduction. | Used to evaluate image denoising performance, focusing on natural image quality and visual fidelity.",
          "citing_paper_id": "260748050",
          "cited_paper_id": 85501306,
          "context_text": "Following RNAN Zhang et al. (2019), PANet is evaluated on standard benchmarks for image denoising: Kodak24 (http://r0k.us/graphics/kodak/), BSD68 Martin et al. (2001), and Urban100 Huang et al. (2015).",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions specific datasets used for evaluating image denoising methods. These datasets are clearly named and are relevant to the topic of image restoration.",
          "citing_paper_doi": "10.1007/s11263-023-01843-5",
          "cited_paper_doi": null,
          "citing_paper_url": "https://www.semanticscholar.org/paper/e98cc6c252a7d69898730df17449fc1a09d1a1ae",
          "cited_paper_url": "https://www.semanticscholar.org/paper/34439db81b482cd562e1cdba974c70a2b89cd6d4",
          "citing_paper_year": 2023,
          "cited_paper_year": 2019
        }
      ]
    },
    {
      "cited_paper_id": "90260111",
      "citation_count": 0,
      "total_dataset_mentions": 4,
      "unique_datasets": [
        "RealSRSet"
      ],
      "dataset_details": [
        {
          "dataset_name": "RealSRSet",
          "dataset_description": "Used to evaluate the BFR task, focusing on face recognition in real-world images. | Used to evaluate the BID task, focusing on image denoising in real-world images. | Collected dataset used to evaluate the performance of the DiffBIR model on real-world single image super-resolution, focusing on metric scores. | Used to evaluate the BSR task, focusing on real-world image super-resolution performance. | Used to evaluate the performance of the DiffBIR model on real-world single image super-resolution, focusing on metric scores. | Used to evaluate the BSR task, focusing on synthetic image super-resolution performance. | Used for comparison in a real-world setting, focusing on single image super-resolution, employing the dataset to validate the effectiveness of the proposed method.",
          "citing_paper_id": "261276317",
          "cited_paper_id": 90260111,
          "context_text": "It is observed that our DiffBIR ( s = 0 ) obtains the best scores across all metrics on both the widely used RealSRSet [73] and our collected Real47.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions 'RealSRSet' as a widely used dataset and 'Real47' as a collected dataset. Both are specific datasets used for evaluating image super-resolution models.",
          "citing_paper_doi": "10.48550/arXiv.2308.15070",
          "cited_paper_doi": "10.1109/ICCV.2019.00318",
          "citing_paper_url": "https://www.semanticscholar.org/paper/858f0643110ccccb6a9103747f2169fecfb98668",
          "cited_paper_url": "https://www.semanticscholar.org/paper/a646c1e52c081a266638785134d3e6dc3a3e7068",
          "citing_paper_year": 2023,
          "cited_paper_year": 2019
        },
        {
          "dataset_name": "RealSRSet",
          "dataset_description": "Used to evaluate the BFR task, focusing on face recognition in real-world images. | Used to evaluate the BID task, focusing on image denoising in real-world images. | Collected dataset used to evaluate the performance of the DiffBIR model on real-world single image super-resolution, focusing on metric scores. | Used to evaluate the performance of the DiffBIR model in real-world super-resolution, focusing on kernel estimation and noise injection techniques. | Used to evaluate the BSR task, focusing on real-world image super-resolution performance. | Collected by the authors to evaluate the DiffBIR model, assessing its effectiveness in real-world super-resolution scenarios. | Used to evaluate the performance of the DiffBIR model on real-world single image super-resolution, focusing on metric scores. | Used to evaluate the BSR task, focusing on synthetic image super-resolution performance.",
          "citing_paper_id": "261276317",
          "cited_paper_id": 220246167,
          "context_text": "It is observed that our DiffBIR ( s = 0 ) obtains the best scores across all metrics on both the widely used RealSRSet [73] and our collected Real47.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions 'RealSRSet' as a widely used dataset and 'Real47' as a collected dataset. Both are specific datasets used for evaluating image super-resolution models.",
          "citing_paper_doi": "10.48550/arXiv.2308.15070",
          "cited_paper_doi": "10.1109/CVPRW50498.2020.00241",
          "citing_paper_url": "https://www.semanticscholar.org/paper/858f0643110ccccb6a9103747f2169fecfb98668",
          "cited_paper_url": "https://www.semanticscholar.org/paper/c2aa5ba138e3e57935ca383a12e95f7f3ed2744f",
          "citing_paper_year": 2023,
          "cited_paper_year": 2020
        },
        {
          "dataset_name": "RealSRSet",
          "dataset_description": "Used to evaluate the performance of SwinIR for real-world super-resolution, focusing on low-quality image synthesis and testing the model's ability to enhance images.",
          "citing_paper_id": "237266491",
          "cited_paper_id": 220246167,
          "context_text": "To test the performance of SwinIR for realworld SR, we re-train SwinIR by using the same degradation model as BSRGAN for low-quality image synthesis and test it on the real-world SR benchmark dataset RealSRSet [89].",
          "confidence_score": 1.0,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions 'RealSRSet' as a benchmark dataset used to test the performance of SwinIR for real-world super-resolution. The dataset is clearly identified and used for evaluation.",
          "citing_paper_doi": "10.1109/ICCVW54120.2021.00210",
          "cited_paper_doi": "10.1109/CVPRW50498.2020.00241",
          "citing_paper_url": "https://www.semanticscholar.org/paper/7a9a708ca61c14886aa0dcd6d13dac7879713f5f",
          "cited_paper_url": "https://www.semanticscholar.org/paper/c2aa5ba138e3e57935ca383a12e95f7f3ed2744f",
          "citing_paper_year": 2021,
          "cited_paper_year": 2020
        },
        {
          "dataset_name": "RealSRSet",
          "dataset_description": "Used for visual comparison of real-world image super-resolution methods at ×4 scale, focusing on practical degradation models and deep blind image super-resolution. | Used to evaluate the performance of SwinIR for real-world super-resolution, focusing on low-quality image synthesis and testing the model's ability to enhance images.",
          "citing_paper_id": "237266491",
          "cited_paper_id": 232352764,
          "context_text": "To test the performance of SwinIR for realworld SR, we re-train SwinIR by using the same degradation model as BSRGAN for low-quality image synthesis and test it on the real-world SR benchmark dataset RealSRSet [89].",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions 'RealSRSet' as a benchmark dataset used to test the performance of SwinIR for real-world super-resolution. The dataset is clearly identified and used for evaluation.",
          "citing_paper_doi": "10.1109/ICCVW54120.2021.00210",
          "cited_paper_doi": "10.1109/ICCV48922.2021.00475",
          "citing_paper_url": "https://www.semanticscholar.org/paper/7a9a708ca61c14886aa0dcd6d13dac7879713f5f",
          "cited_paper_url": "https://www.semanticscholar.org/paper/636d288ffb292986ef95158941a711d7cb3693b4",
          "citing_paper_year": 2021,
          "cited_paper_year": 2021
        }
      ]
    },
    {
      "cited_paper_id": "5250573",
      "citation_count": 0,
      "total_dataset_mentions": 4,
      "unique_datasets": [
        "BSDS100"
      ],
      "dataset_details": [
        {
          "dataset_name": "BSDS100",
          "dataset_description": "Used to test the generalization ability of the base network on manga images with blended distortions and varying resolutions, focusing on visual quality and restoration accuracy. | Used to assess image restoration methods, specifically for urban scenes, with moderate-level distortions to evaluate the preservation of fine details and textures. | Used to test the generalization ability of the base network on images with blended distortions and varying resolutions, focusing on edge detection and segmentation performance. | Used for testing super-resolution algorithms, emphasizing the recovery of fine details in small image sets. | Used to evaluate image restoration methods, focusing on moderate-level distortions to assess performance on diverse natural images. | Used for evaluating image restoration methods, focusing on edge detection and segmentation performance. | Used for assessing image restoration in urban scenes, emphasizing the recovery of complex textures and structures. | Used to test the generalization ability of the base network on a small set of high-resolution images with blended distortions and varying resolutions, focusing on super-resolution performance. | Used for assessing image restoration techniques, particularly in handling manga-style images with high detail and contrast. | Used for evaluating image restoration models, focusing on a variety of image types and restoration challenges. | Used to benchmark image restoration algorithms, focusing on small-scale images with moderate distortions to assess upscaling and denoising effectiveness. | Used to test the generalization ability of the base network on urban scene images with blended distortions and varying resolutions, focusing on texture preservation and restoration accuracy. | Used to evaluate image restoration models, emphasizing moderate-level distortions to test performance on a variety of image types and resolutions. | Used to test the generalization ability of the base network on a diverse set of images with blended distortions and varying resolutions, focusing on restoration accuracy and visual quality. | Used to test image restoration techniques, particularly for manga images, with moderate-level distortions to evaluate visual quality and detail preservation.",
          "citing_paper_id": "221173039",
          "cited_paper_id": 2356330,
          "context_text": "Specially, to further verify the generalization ability of our base network on images from different datasets and in different resolutions, we randomly add blended distortions on five public benchmark datasets: BSDS100 [5], MANGA109 [26], SET5 [6], SET14 [48] and URBAN100 [13] and randomly crop the images into 64 × 64, 128 × 128 and 256× 256 respectively, generating the additional testing sets.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions five specific datasets used to test the generalization ability of a base network under various image resolutions and distortions.",
          "citing_paper_doi": "10.1007/978-3-030-58523-5_36",
          "cited_paper_doi": "10.1007/978-3-642-27413-8_47",
          "citing_paper_url": "https://www.semanticscholar.org/paper/bd4346fa7145f126d32b19ced1223fcb7e7eef0e",
          "cited_paper_url": "https://www.semanticscholar.org/paper/d1dce9ad419d8c7037980a9ea28506ed0e640bba",
          "citing_paper_year": 2020,
          "cited_paper_year": 2010
        },
        {
          "dataset_name": "BSDS100",
          "dataset_description": "Used to test the generalization ability of the base network on manga images with blended distortions and varying resolutions, focusing on visual quality and restoration accuracy. | Used to test the generalization ability of the base network on images with blended distortions and varying resolutions, focusing on edge detection and segmentation performance. | Used to test the generalization ability of the base network on a small set of high-resolution images with blended distortions and varying resolutions, focusing on super-resolution performance. | Used to test the generalization ability of the base network on urban scene images with blended distortions and varying resolutions, focusing on texture preservation and restoration accuracy. | Used to test the generalization ability of the base network on a diverse set of images with blended distortions and varying resolutions, focusing on restoration accuracy and visual quality.",
          "citing_paper_id": "221173039",
          "cited_paper_id": 5250573,
          "context_text": "Specially, to further verify the generalization ability of our base network on images from different datasets and in different resolutions, we randomly add blended distortions on five public benchmark datasets: BSDS100 [5], MANGA109 [26], SET5 [6], SET14 [48] and URBAN100 [13] and randomly crop the images into 64 × 64, 128 × 128 and 256× 256 respectively, generating the additional testing sets.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions five specific datasets used to test the generalization ability of a base network under various image resolutions and distortions.",
          "citing_paper_doi": "10.1007/978-3-030-58523-5_36",
          "cited_paper_doi": "10.5244/C.26.135",
          "citing_paper_url": "https://www.semanticscholar.org/paper/bd4346fa7145f126d32b19ced1223fcb7e7eef0e",
          "cited_paper_url": "https://www.semanticscholar.org/paper/c7338140c446bb1b8c1a2c24aa380890214dc0d7",
          "citing_paper_year": 2020,
          "cited_paper_year": 2012
        },
        {
          "dataset_name": "BSDS100",
          "dataset_description": "Used to test the generalization ability of the base network on manga images with blended distortions and varying resolutions, focusing on visual quality and restoration accuracy. | Used to test the generalization ability of the base network on images with blended distortions and varying resolutions, focusing on edge detection and segmentation performance. | Used to test the generalization ability of the base network on a small set of high-resolution images with blended distortions and varying resolutions, focusing on super-resolution performance. | Used to test the generalization ability of the base network on urban scene images with blended distortions and varying resolutions, focusing on texture preservation and restoration accuracy. | Used to test the generalization ability of the base network on a diverse set of images with blended distortions and varying resolutions, focusing on restoration accuracy and visual quality.",
          "citing_paper_id": "221173039",
          "cited_paper_id": 206764694,
          "context_text": "Specially, to further verify the generalization ability of our base network on images from different datasets and in different resolutions, we randomly add blended distortions on five public benchmark datasets: BSDS100 [5], MANGA109 [26], SET5 [6], SET14 [48] and URBAN100 [13] and randomly crop the images into 64 × 64, 128 × 128 and 256× 256 respectively, generating the additional testing sets.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions five specific datasets used to test the generalization ability of a base network under various image resolutions and distortions.",
          "citing_paper_doi": "10.1007/978-3-030-58523-5_36",
          "cited_paper_doi": "10.1109/TPAMI.2010.161",
          "citing_paper_url": "https://www.semanticscholar.org/paper/bd4346fa7145f126d32b19ced1223fcb7e7eef0e",
          "cited_paper_url": "https://www.semanticscholar.org/paper/0e5a262bf59b68ba8a7a1103d16fa33a9f5ffc28",
          "citing_paper_year": 2020,
          "cited_paper_year": 2011
        },
        {
          "dataset_name": "BSDS100",
          "dataset_description": "Used to test the generalization ability of the base network on urban scene images with blended distortions and varying resolutions, focusing on image restoration performance. | Used to test the generalization ability of the base network on images with blended distortions and varying resolutions, focusing on image restoration performance. | Used to test the generalization ability of the base network on manga images with blended distortions and varying resolutions, focusing on image restoration performance.",
          "citing_paper_id": "221173039",
          "cited_paper_id": 8887614,
          "context_text": "the generalization ability of our base network on images from dierent datasets and in dierent resolutions, we randomly add blended distortions on ve public benchmark datasets: BSDS100 [5], MANGA109 [26], SET5 [6], SET14 [48] and URBAN100 [13] and randomly crop the images into 64 64, 128 128 and 256 256 respectively, generating the additional testing sets. The standard deviations of Gaussian blur and",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions five specific datasets used to test the generalization ability of the base network under various distortions and resolutions.",
          "citing_paper_doi": "10.1007/978-3-030-58523-5_36",
          "cited_paper_doi": "10.1007/s11042-016-4020-z",
          "citing_paper_url": "https://www.semanticscholar.org/paper/bd4346fa7145f126d32b19ced1223fcb7e7eef0e",
          "cited_paper_url": "https://www.semanticscholar.org/paper/1fef45786e707e6b9b8517b0403e596ecbdea6a5",
          "citing_paper_year": 2020,
          "cited_paper_year": 2015
        }
      ]
    },
    {
      "cited_paper_id": "215827845",
      "citation_count": 0,
      "total_dataset_mentions": 4,
      "unique_datasets": [
        "Classic5"
      ],
      "dataset_details": [
        {
          "dataset_name": "Classic5",
          "dataset_description": "Used to test image restoration methods at JPEG quality factors 10, 20, 30, and 40, focusing on performance evaluation under compression.",
          "citing_paper_id": "237266491",
          "cited_paper_id": 221377171,
          "context_text": "Following [98, 88], we test different methods on two benchmark datasets (Classic5 [22] and LIVE1 [67]) for JPEG quality factors 10, 20, 30 and 40.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions two specific datasets, Classic5 and LIVE1, which are used to test different methods for image restoration at various JPEG quality factors.",
          "citing_paper_doi": "10.1109/ICCVW54120.2021.00210",
          "cited_paper_doi": "10.1109/TPAMI.2021.3088914",
          "citing_paper_url": "https://www.semanticscholar.org/paper/7a9a708ca61c14886aa0dcd6d13dac7879713f5f",
          "cited_paper_url": "https://www.semanticscholar.org/paper/d467d747dde65862d9640fe49f69d6b6d46d69ea",
          "citing_paper_year": 2021,
          "cited_paper_year": 2020
        },
        {
          "dataset_name": "Classic5",
          "dataset_description": "Used to evaluate image restoration models, specifically comparing performance metrics across various noise levels and models.",
          "citing_paper_id": "237266491",
          "cited_paper_id": 215827845,
          "context_text": "Dataset q ARCNN [17] DnCNN-3 [90] QGAC [20] RNAN [96] RDN [98] DRUNet [88] SwinIR (ours)\nClassic5 [22] 10 29.03/0.7929/28.76 29.40/0.8026/29.13 29.84/0.8370/29.43 29.96/0.8178/29.62 30.00/0.8188/- 30.16/0.8234/29.81 30.27/0.8249/29.95 20 31.15/0.8517/30.59 31.63/0.8610/31.19 31.98/0.8850/31.37 32.11/0.8693/31.57 32.15/0.8699/- 32.39/0.8734/31.80 32.52/0.8748/31.99 30 32.51/0.8806/31.98 32.91/0.8861/32.38 33.22/0.9070/32.42 33.38/0.8924/32.68 33.43/0.8930/- 33.59/0.8949/32.82 33.73/0.8961/33.03 40 33.32/0.8953/32.79 33.77/0.9003/33.20 - 34.27/0.9061/33.4 34.27/0.9061/- 34.41/0.9075/33.51 34.52/0.9082/33.66\nLIVE1 [67] 10 28.96/0.8076/28.77 29.19/0.8123/28.90 29.53/0.8400/29.15 29.63/0.8239/29.25 29.67/0.8247/- 29.79/0.8278/29.48 29.86/0.8287/29.50 20 31.29/0.8733/30.79 31.59/0.8802/31.07 31.86/0.9010/31.27 32.03/0.8877/31.44 32.07/0.8882/- 32.17/0.8899/31.69 32.25/0.8909/31.70 30 32.67/0.9043/32.22 32.98/0.9090/32.34 33.23/0.9250/32.50 33.45/0.9149/32.71 33.51/0.9153/- 33.59/0.9166/32.99 33.69/0.9174/33.01 40 33.63/0.9198/33.14 33.96/0.9247/33.28 - 34.47/0.9299/33.66 34.51/0.9302/- 34.58/0.9312/33.93 34.67/0.9317/33.88\napplications, we further propose a large model and train it on much larger datasets.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions two specific datasets, Classic5 and LIVE1, which are used for evaluating image restoration models. These datasets are clearly identified and used in the research context.",
          "citing_paper_doi": "10.1109/ICCVW54120.2021.00210",
          "cited_paper_doi": "10.1007/978-3-030-58598-3_18",
          "citing_paper_url": "https://www.semanticscholar.org/paper/7a9a708ca61c14886aa0dcd6d13dac7879713f5f",
          "cited_paper_url": "https://www.semanticscholar.org/paper/80f1da2e64d2c78bc6a7b5131fc6e274eb29924f",
          "citing_paper_year": 2021,
          "cited_paper_year": 2020
        },
        {
          "dataset_name": "Classic5",
          "dataset_description": "Used to assess JPEG deblocking effectiveness, emphasizing perceptual quality and distortion metrics in compressed images. | Used to evaluate JPEG deblocking performance, focusing on visual quality and artifact reduction in compressed images.",
          "citing_paper_id": "8550762",
          "cited_paper_id": 996788,
          "context_text": "5 shows the JPEG deblocking results on Classic5 and LIVE1, by citing the results from [40].",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions two specific datasets, Classic5 and LIVE1, which are used for evaluating JPEG deblocking results.",
          "citing_paper_doi": "10.1109/ICCV.2017.486",
          "cited_paper_doi": "10.1109/TIP.2017.2662206",
          "citing_paper_url": "https://www.semanticscholar.org/paper/5fdd40601a2ccdaa2d2ade27872bd8b3f43b2c1c",
          "cited_paper_url": "https://www.semanticscholar.org/paper/0c00a328fa7cd56ee60338c54e89bd48310db80b",
          "citing_paper_year": 2017,
          "cited_paper_year": 2016
        },
        {
          "dataset_name": "Classic5",
          "dataset_description": "Used as a test dataset to evaluate image restoration methods, focusing on compression artifact reduction using deep convolutional networks.",
          "citing_paper_id": "8550762",
          "cited_paper_id": 9569924,
          "context_text": "As in [7], Classic5 and LIVE1 are adopted as the test datasets.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The citation mentions 'Classic5' and 'LIVE1' as test datasets, which are specific and plausible dataset names.",
          "citing_paper_doi": "10.1109/ICCV.2017.486",
          "cited_paper_doi": "10.1109/ICCV.2015.73",
          "citing_paper_url": "https://www.semanticscholar.org/paper/5fdd40601a2ccdaa2d2ade27872bd8b3f43b2c1c",
          "cited_paper_url": "https://www.semanticscholar.org/paper/09642681d46282e76fd9d1336001ef6473b72ec8",
          "citing_paper_year": 2017,
          "cited_paper_year": 2015
        }
      ]
    },
    {
      "cited_paper_id": "3406592",
      "citation_count": 0,
      "total_dataset_mentions": 4,
      "unique_datasets": [
        "Test1"
      ],
      "dataset_details": [
        {
          "dataset_name": "Test1",
          "dataset_description": "Used for training models to restore heavy rain images, integrating physics models and conditional adversarial learning. | Used for quantitative evaluation of the trained model's performance on rain image restoration. | Used to evaluate the performance of image restoration models, specifically comparing qualitative reconstructions of rain-affected images using HRGAN and MPRNet. | Used to evaluate the performance of rain image restoration models, specifically comparing RainHazeDiff64 against HRGAN and MPRNet using visual quality assessments. | Used to simulate rain and fog conditions for image restoration, integrating physics models and conditional adversarial learning to enhance image quality. | Used to evaluate HRGAN for heavy rain image restoration, focusing on generative modeling with GANs and integrating physics models. | Used to evaluate AttentiveGAN for raindrop removal, focusing on generative modeling with GANs and attention mechanisms.",
          "citing_paper_id": "251197000",
          "cited_paper_id": 131773964,
          "context_text": "The Outdoor-Rain training set consists of 9,000 images, and the test set we used, denoted in [14] as Test1, is of size 750 for quantitative evaluations.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions specific datasets used for training and testing in the context of rain image restoration.",
          "citing_paper_doi": "10.1109/TPAMI.2023.3238179",
          "cited_paper_doi": "10.1109/CVPR.2019.00173",
          "citing_paper_url": "https://www.semanticscholar.org/paper/69beb616ffdc42afe86b7487c5db82b6d88638b8",
          "cited_paper_url": "https://www.semanticscholar.org/paper/b1b7eeac57e1e78e02d9d97eb809d8459bd06050",
          "citing_paper_year": 2022,
          "cited_paper_year": 2019
        },
        {
          "dataset_name": "Test1",
          "dataset_description": "Used to evaluate rain removal algorithms on low-resolution images with light rain, focusing on noise reduction and clarity enhancement. | Used to benchmark rain removal algorithms on a small, curated dataset with challenging rain scenarios. | Used to train and test rain removal models, offering a large dataset with varied rain conditions and image complexities. | Used to test and validate rain removal techniques, providing a smaller but high-quality dataset with synthetic rain effects. | Used to train and evaluate snow removal algorithms, providing a large dataset with diverse snow conditions and image types. | Used to evaluate the performance of the proposed AIRFormer in image deraining, focusing on the effectiveness of the method in removing rain streaks. | Used to test and validate synthetic snow generation and removal methods, focusing on realistic snow simulation. | Used to evaluate image dehazing and deraining algorithms in indoor scenes, emphasizing the removal of atmospheric disturbances. | Used to assess the performance of rain removal methods on high-resolution images with heavy rain, emphasizing detail preservation. | Used to train and evaluate rain removal algorithms, focusing on diverse rain patterns and intensities in real-world images. | Used to test the robustness of the proposed AIRFormer in heavy rain conditions, integrating physics models and conditional adversarial learning. | Used to test and validate outdoor scene dehazing and deraining techniques, focusing on natural lighting and environmental conditions. | Used to assess the performance of haze and rain removal algorithms in synthetic and real-world images, emphasizing robustness and generalization. | Used to assess the deraining capabilities of the proposed AIRFormer, specifically examining its ability to handle various rain intensities and patterns. | Used to compile a comprehensive all-in-one image restoration dataset, integrating various rain and snow removal datasets for training and evaluation. | Used to develop and test raindrop removal techniques, focusing on the unique challenges posed by raindrops on surfaces.",
          "citing_paper_id": "260271048",
          "cited_paper_id": 3406592,
          "context_text": "1) Image Deraining: We choose the datasets of Test1 [88], Rain800 [79] and DID-Data [33] to evaluate our proposed AIRFormer.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions three specific datasets used for evaluating the proposed method in image deraining. These datasets are clearly named and relevant to the research topic.",
          "citing_paper_doi": "10.1109/TCSVT.2023.3299324",
          "cited_paper_doi": "10.1109/CVPR.2018.00079",
          "citing_paper_url": "https://www.semanticscholar.org/paper/2a828c28938b196fff7b31e58a08853daaaa8b96",
          "cited_paper_url": "https://www.semanticscholar.org/paper/34aaa735409b666b8dc678c23acb600b1d87913b",
          "citing_paper_year": 2024,
          "cited_paper_year": 2018
        },
        {
          "dataset_name": "Test1",
          "dataset_description": "Used to evaluate the performance of the proposed AIRFormer in image deraining, focusing on the effectiveness of the method in removing rain streaks. | Used to test the robustness of the proposed AIRFormer in heavy rain conditions, integrating physics models and conditional adversarial learning. | Used to assess the deraining capabilities of the proposed AIRFormer, specifically examining its ability to handle various rain intensities and patterns.",
          "citing_paper_id": "260271048",
          "cited_paper_id": 131773964,
          "context_text": "1) Image Deraining: We choose the datasets of Test1 [88], Rain800 [79] and DID-Data [33] to evaluate our proposed AIRFormer.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions three specific datasets used for evaluating the proposed method in image deraining. These datasets are clearly named and relevant to the research topic.",
          "citing_paper_doi": "10.1109/TCSVT.2023.3299324",
          "cited_paper_doi": "10.1109/CVPR.2019.00173",
          "citing_paper_url": "https://www.semanticscholar.org/paper/2a828c28938b196fff7b31e58a08853daaaa8b96",
          "cited_paper_url": "https://www.semanticscholar.org/paper/b1b7eeac57e1e78e02d9d97eb809d8459bd06050",
          "citing_paper_year": 2024,
          "cited_paper_year": 2019
        },
        {
          "dataset_name": "Test1",
          "dataset_description": "Used to evaluate the method's ability to remove snow from images, focusing on the quality and realism of snow removal. | Used to assess the effectiveness of the method in removing raindrops from images, specifically measuring the quality of raindrop removal. | Used to evaluate the performance of the image restoration method, focusing on general image quality and restoration accuracy.",
          "citing_paper_id": "244714491",
          "cited_paper_id": 219978541,
          "context_text": "We use the Test1 dataset [21, 23], the RainDrop test dataset [37] and the Snow100k-L test set [29] for testing our method.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions three specific datasets used for testing the method: Test1, RainDrop test dataset, and Snow100k-L test set. These datasets are clearly identified and used for evaluating the performance of the image restoration method.",
          "citing_paper_doi": "10.1109/CVPR52688.2022.00239",
          "cited_paper_doi": "10.1109/cvpr42600.2020.00324",
          "citing_paper_url": "https://www.semanticscholar.org/paper/b27d3be4264dcd06f990b44968f4382526f24f1e",
          "cited_paper_url": "https://www.semanticscholar.org/paper/2e89dd08b5ea206128724419e259c8750e8d1381",
          "citing_paper_year": 2021,
          "cited_paper_year": 2020
        }
      ]
    },
    {
      "cited_paper_id": "237266491",
      "citation_count": 0,
      "total_dataset_mentions": 4,
      "unique_datasets": [
        "DF2K"
      ],
      "dataset_details": [
        {
          "dataset_name": "DF2K",
          "dataset_description": "Used to train models for image restoration, focusing on enhancing the quality of degraded images using the Swin Transformer method.",
          "citing_paper_id": "264289165",
          "cited_paper_id": 237266491,
          "context_text": "We use the DF2K dataset (the same as SwinIR [25]) to train models.",
          "confidence_score": 1.0,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions the DF2K dataset, which is a specific dataset used for training models in image restoration. The dataset is clearly identified and used in the research context.",
          "citing_paper_doi": "10.48550/arXiv.2310.11881",
          "cited_paper_doi": "10.1109/ICCVW54120.2021.00210",
          "citing_paper_url": "https://www.semanticscholar.org/paper/a9a441bbbbf0c1e826e36cc864c728d3516a3608",
          "cited_paper_url": "https://www.semanticscholar.org/paper/7a9a708ca61c14886aa0dcd6d13dac7879713f5f",
          "citing_paper_year": 2023,
          "cited_paper_year": 2021
        },
        {
          "dataset_name": "DF2K",
          "dataset_description": "Used for training classic image super-resolution models, combining DIV2K and Flickr2K datasets to enhance model performance. | Used as the training set for lightweight image super-resolution, focusing on efficient model training and evaluation. | Part of the DF2K dataset, combined with DIV2K to form a comprehensive training set for classic image super-resolution.",
          "citing_paper_id": "275133634",
          "cited_paper_id": 484327,
          "context_text": "Datasets: Following previous works [18, 27], we employ DF2K (DIV2K [49]+Flickr2K [29]) as the training set for classic image super-resolution, and DIV2K as training set for lightweight image super-resolution.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions specific datasets used for training in the context of image super-resolution, which aligns with the research topic of all-in-one image restoration.",
          "citing_paper_doi": "10.48550/arXiv.2412.20066",
          "cited_paper_doi": "10.1109/CVPRW.2017.149",
          "citing_paper_url": "https://www.semanticscholar.org/paper/2b78fc275b7fc3a37061620ae155c017ab6c22e1",
          "cited_paper_url": "https://www.semanticscholar.org/paper/5054edca22325ddd3507b860f9af4a961baea009",
          "citing_paper_year": 2024,
          "cited_paper_year": 2017
        },
        {
          "dataset_name": "DF2K",
          "dataset_description": "Used for training classic image super-resolution models, combining DIV2K and Flickr2K datasets to enhance model performance. | Used as the training set for lightweight image super-resolution, focusing on efficient model training and evaluation. | Part of the DF2K dataset, combined with DIV2K to form a comprehensive training set for classic image super-resolution.",
          "citing_paper_id": "275133634",
          "cited_paper_id": 237266491,
          "context_text": "Datasets: Following previous works [18, 27], we employ DF2K (DIV2K [49]+Flickr2K [29]) as the training set for classic image super-resolution, and DIV2K as training set for lightweight image super-resolution.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions specific datasets used for training in the context of image super-resolution, which aligns with the research topic of all-in-one image restoration.",
          "citing_paper_doi": "10.48550/arXiv.2412.20066",
          "cited_paper_doi": "10.1109/ICCVW54120.2021.00210",
          "citing_paper_url": "https://www.semanticscholar.org/paper/2b78fc275b7fc3a37061620ae155c017ab6c22e1",
          "cited_paper_url": "https://www.semanticscholar.org/paper/7a9a708ca61c14886aa0dcd6d13dac7879713f5f",
          "citing_paper_year": 2024,
          "cited_paper_year": 2021
        },
        {
          "dataset_name": "DF2K",
          "dataset_description": "Used as a baseline for comparison, a popular smaller dataset for training image restoration networks. | Used to evaluate prompt-learning-based methods on high-resolution images, focusing on single image super-resolution performance. | Used to form a combined training dataset for image restoration, integrating high-resolution images from DIV2K and Flickr2K. | Used as an additional training dataset for image restoration, complementing DF2K with diverse image content. | Provides 2650 high-resolution images to the DF2K dataset, enhancing the diversity of the training set for image restoration. | Used to train image restoration networks, providing a large-scale dataset of 84,991 high-quality images, significantly boosting performance compared to smaller datasets. | Contributes 800 high-resolution images to the DF2K dataset, used for training image restoration models.",
          "citing_paper_id": "269430366",
          "cited_paper_id": 4493958,
          "context_text": "We use DF2K (800 images from DIV2K [1] and 2650 images from Flickr2K [49]) and LS-DIR [27] to form our training datasets.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions specific datasets used for training, which are relevant to the topic of image restoration.",
          "citing_paper_doi": "10.1109/CVPRW63382.2024.00645",
          "cited_paper_doi": "10.1109/CVPRW.2017.150",
          "citing_paper_url": "https://www.semanticscholar.org/paper/a9a10ec89f604effce447efb7ac12127ca38b29c",
          "cited_paper_url": "https://www.semanticscholar.org/paper/6aeb583c084ad2eb709154ec91f22ca8d6de4425",
          "citing_paper_year": 2024,
          "cited_paper_year": 2017
        }
      ]
    },
    {
      "cited_paper_id": "4539586",
      "citation_count": 0,
      "total_dataset_mentions": 4,
      "unique_datasets": [
        "Raindrop dataset"
      ],
      "dataset_details": [
        {
          "dataset_name": "Raindrop dataset",
          "dataset_description": "Used to test various raindrop removal methods, focusing on the effectiveness of different algorithms in image restoration tasks. | Used to train and evaluate raindrop removal algorithms, containing 1,119 pairs of clean images and images with adherent raindrops.",
          "citing_paper_id": "278326960",
          "cited_paper_id": 4539586,
          "context_text": "5 presents the simulation results of various methods tested on the RainDrop dataset [9].",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions the RainDrop dataset, which is a specific dataset used for testing raindrop removal methods. The title confirms it is a dataset.",
          "citing_paper_doi": "10.48550/arXiv.2505.01882",
          "cited_paper_doi": "10.1109/CVPR.2018.00263",
          "citing_paper_url": "https://www.semanticscholar.org/paper/d6c7f3bc3e7b317c652f3c59b55015667ba3f77b",
          "cited_paper_url": "https://www.semanticscholar.org/paper/34e38559df559bfedfc3e1edc2b120ab2b5d444e",
          "citing_paper_year": 2025,
          "cited_paper_year": 2017
        },
        {
          "dataset_name": "Raindrop dataset",
          "dataset_description": "Used to evaluate the performance of different model structures in raindrop removal from images, focusing on PSNR and SSIM metrics.",
          "citing_paper_id": "232222608",
          "cited_paper_id": 4539586,
          "context_text": "TABLE 5 Performance of different model structures on the Raindrop dataset [19] in terms of PSNR and SSIM.",
          "confidence_score": 1.0,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions the 'Raindrop dataset' which is used to evaluate model performance in raindrop removal from images.",
          "citing_paper_doi": "10.1109/TIP.2021.3108019",
          "cited_paper_doi": "10.1109/CVPR.2018.00263",
          "citing_paper_url": "https://www.semanticscholar.org/paper/0cd840ae923f2bc837d5744d0f0eb074e9f359e3",
          "cited_paper_url": "https://www.semanticscholar.org/paper/34e38559df559bfedfc3e1edc2b120ab2b5d444e",
          "citing_paper_year": 2021,
          "cited_paper_year": 2017
        },
        {
          "dataset_name": "Raindrop dataset",
          "dataset_description": "Used for testing deraining models, featuring low-resolution rainy images and their clean versions. | Used for training and testing image enhancement models, featuring retouched images and their original versions. | Used for training and testing dehazing models, providing outdoor scenes with haze and their clear versions. | Used for training and testing deraining models, providing a large set of rainy images with corresponding clean images. | Used for testing deraining models, providing a set of rainy images and their clean counterparts. | Used for training deraining models, containing synthetic rainy images and their clean counterparts. | Used for evaluating deblurring performance, focusing on motion blur in video sequences. | Used for training deraining models, containing a small set of rainy images and their clean versions. | Used for evaluating raindrop removal techniques, specifically comparing deraining methods on images with raindrops, focusing on visual quality and effectiveness. | Used for testing deraining models, featuring high-resolution rainy images and their clean versions. | Used for training and testing dehazing models, providing indoor scenes with haze and their clear versions. | Used for training and testing low-light enhancement models, providing low-light images and their well-lit counterparts. | Used to evaluate the performance of the raindrop removal model, focusing on image quality measured by PSNR. | Used for training and testing deraining models, including images with raindrops and their clean versions. | Used to evaluate raindrop removal performance, focusing on visual quality and effectiveness of the proposed method in real-world scenarios. | Used for assessing raindrop removal effectiveness, specifically targeting single image raindrop removal. | Used to evaluate raindrop removal methods, specifically comparing AGAN, DuRN, Quan, and MAXIM-2S in visual performance. | Used to train and evaluate the MAXIM-2S model for raindrop removal, focusing on image pairs for training and two test sets (A and B) for evaluation. | Used for training deraining models, consisting of synthetic rainy images and their clean versions.",
          "citing_paper_id": "245837508",
          "cited_paper_id": 4539586,
          "context_text": "For the raindrop removal task, we trained MAXIM-2S on 861 pairs of training images in Raindrop dataset [71] for 80k steps with a batch size of 512, and evaluate on testset A (58 images) and testset B (239 images), respectively.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions the 'Raindrop dataset' which is used for training and evaluating a model for raindrop removal. The dataset is clearly identified and used in a specific research context.",
          "citing_paper_doi": "10.1109/CVPR52688.2022.00568",
          "cited_paper_doi": "10.1109/CVPR.2018.00263",
          "citing_paper_url": "https://www.semanticscholar.org/paper/fbf68eb0cf8237cef59ebdb301569c79b9676ff9",
          "cited_paper_url": "https://www.semanticscholar.org/paper/34e38559df559bfedfc3e1edc2b120ab2b5d444e",
          "citing_paper_year": 2022,
          "cited_paper_year": 2017
        },
        {
          "dataset_name": "Raindrop dataset",
          "dataset_description": "Used to train the model for 200 epochs, focusing on high-resolution synthetic rain images for deraining. | Used to train the model for 750 epochs, focusing on real raindrop images for raindrop removal. | Used to evaluate the raindrop removal performance of the proposed method, focusing on the effectiveness of the technique in real-world scenarios. | Used to train the model for 200 epochs, focusing on low-resolution synthetic rain images for deraining. | Used to train the model for 100 epochs, containing real-world cityscape images with synthetic rain for deraining. | Used to train the model for 500 epochs, focusing on removing raindrops from images using a large, high-quality real rain dataset. | Used for evaluating raindrop removal methods from images, focusing on real-world scenarios to test the effectiveness of the proposed algorithm. | Used to train the model for 500 epochs, focusing on low-resolution synthetic rain images for deraining. | Used to train the model for 5 epochs, providing a specialized dataset for single-image deraining. | Used to train the model for 3 epochs, providing a specialized dataset for single-image deraining. | Used to train the model for 500 epochs, focusing on high-resolution synthetic rain images for deraining. | Used to train the model for 500 epochs, providing a large set of synthetic rain images for deraining. | Used to train the model for 200 epochs, providing a large set of synthetic rain images for deraining. | Used to train the model for 100 epochs, offering a diverse set of synthetic rain images for deraining.",
          "citing_paper_id": "247940251",
          "cited_paper_id": 4539586,
          "context_text": "In addition, Raindrop [25] dataset is used to evaluate the raindrop removal performance of the proposed method.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions the 'Raindrop' dataset, which is used to evaluate the performance of the proposed method for raindrop removal. The dataset name is specific and relevant to the research topic.",
          "citing_paper_doi": "10.1016/j.image.2022.116701",
          "cited_paper_doi": "10.1109/CVPR.2018.00263",
          "citing_paper_url": "https://www.semanticscholar.org/paper/a52f1afdd11d9a902729ecb1e5be852470756711",
          "cited_paper_url": "https://www.semanticscholar.org/paper/34e38559df559bfedfc3e1edc2b120ab2b5d444e",
          "citing_paper_year": 2020,
          "cited_paper_year": 2017
        }
      ]
    },
    {
      "cited_paper_id": "218538083",
      "citation_count": 0,
      "total_dataset_mentions": 4,
      "unique_datasets": [
        "RTTS"
      ],
      "dataset_details": [
        {
          "dataset_name": "RTTS",
          "dataset_description": "Utilized for assessing image quality in the absence of reference images, emphasizing spatial quality evaluation in restoration tasks. | Employed to test no-reference image quality assessment techniques, specifically for real-world blurred images in restoration scenarios. | Used for evaluating no-reference image quality assessment methods, focusing on spatial quality metrics in image restoration.",
          "citing_paper_id": "272444309",
          "cited_paper_id": 2927709,
          "context_text": "For unlabeled datasets, including RTTS [32], SPA [63], and RealBlur-J [52], we rely on two renowned no-reference image quality assessment indicators: Blind/Referenceless Image Spatial Quality Evaluator (BRISQUE) [44] and Perception-based Image Quality Evaluator (PIQE) [62].",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions three specific datasets used for unlabeled data in the context of image restoration and quality assessment.",
          "citing_paper_doi": "10.1145/3694973",
          "cited_paper_doi": "10.1109/TIP.2012.2214050",
          "citing_paper_url": "https://www.semanticscholar.org/paper/2893ee935e0c5113a844cfa54f6e79dfda827bba",
          "cited_paper_url": "https://www.semanticscholar.org/paper/a2cad4e4fd946adf6cc87e483b2ba18579de1264",
          "citing_paper_year": 2024,
          "cited_paper_year": 2012
        },
        {
          "dataset_name": "RTTS",
          "dataset_description": "Used to evaluate dehazing performance, focusing on non-homogeneous haze conditions and employing PSNR and SSIM metrics. | Used to evaluate no-reference and full-reference image quality assessment metrics in the context of image restoration, focusing on objective quality evaluation. | Used to test TCDAE on non-homogeneous hazy images, evaluating its capability to handle varying haze densities and distributions. | Used to evaluate TCDAE's performance in image dehazing, focusing on dense haze conditions and comparing PSNR and SSIM scores. | Used to evaluate TCDAE's performance in image dehazing, focusing on non-homogeneous haze conditions and comparing PSNR and SSIM scores. | Used to benchmark image dehazing algorithms, specifically addressing dense haze conditions and providing dense-haze and haze-free images. | Used to assess the effectiveness of TCDAE on dense haze conditions, emphasizing the model's ability to restore clear images. | Used to evaluate dehazing performance, focusing on dense haze conditions and employing PSNR and SSIM metrics. | Used to evaluate no-reference and full-reference image quality assessment metrics in the context of image restoration, focusing on specific image degradation types. | Used to evaluate image dehazing models, providing non-homogeneous hazy and haze-free images for benchmarking. | Used to evaluate image dehazing models, providing dense-haze and haze-free images for benchmarking. | Used to evaluate the visual quality of TCDAE on real hazy images, focusing on dehazing performance and visual clarity. | Used to evaluate no-reference and full-reference image quality assessment indicators, focusing on the performance of dehazing models. | Used to evaluate real-world dehazing performance, focusing on task-driven scenarios and practical applications. | Used to evaluate no-reference and full-reference image quality assessment metrics in the context of image restoration, focusing on real-world blur conditions. | Used to test dehazing algorithms under non-homogeneous haze conditions, providing a diverse set of hazy and haze-free images. | Used to evaluate no-reference and full-reference image quality assessment metrics in the context of image dehazing, focusing on non-homogeneous hazy images. | Used to evaluate no-reference and full-reference image quality assessment metrics in the context of image dehazing, focusing on dense haze conditions.",
          "citing_paper_id": "272444309",
          "cited_paper_id": 102350705,
          "context_text": "We also evaluate the above models on no-reference and full-reference image quality assessment indicators on RTTS [32] Dense-Haze [1], and NH-HAZE [2].",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions three specific datasets used for evaluating image quality assessment indicators in the context of image dehazing.",
          "citing_paper_doi": "10.1145/3694973",
          "cited_paper_doi": "10.1109/ICIP.2019.8803046",
          "citing_paper_url": "https://www.semanticscholar.org/paper/2893ee935e0c5113a844cfa54f6e79dfda827bba",
          "cited_paper_url": "https://www.semanticscholar.org/paper/a3be7b43518abc11a074e6c15d1b5c443e53737c",
          "citing_paper_year": 2024,
          "cited_paper_year": 2019
        },
        {
          "dataset_name": "RTTS",
          "dataset_description": "Used to evaluate dehazing performance, focusing on non-homogeneous haze conditions and employing PSNR and SSIM metrics. | Used to evaluate no-reference and full-reference image quality assessment metrics in the context of image restoration, focusing on objective quality evaluation. | Used to test TCDAE on non-homogeneous hazy images, evaluating its capability to handle varying haze densities and distributions. | Used to evaluate TCDAE's performance in image dehazing, focusing on dense haze conditions and comparing PSNR and SSIM scores. | Used to evaluate TCDAE's performance in image dehazing, focusing on non-homogeneous haze conditions and comparing PSNR and SSIM scores. | Used to benchmark image dehazing algorithms, specifically addressing dense haze conditions and providing dense-haze and haze-free images. | Used to assess the effectiveness of TCDAE on dense haze conditions, emphasizing the model's ability to restore clear images. | Used to evaluate dehazing performance, focusing on dense haze conditions and employing PSNR and SSIM metrics. | Used to evaluate no-reference and full-reference image quality assessment metrics in the context of image restoration, focusing on specific image degradation types. | Used to evaluate image dehazing models, providing non-homogeneous hazy and haze-free images for benchmarking. | Used to evaluate image dehazing models, providing dense-haze and haze-free images for benchmarking. | Used to evaluate the visual quality of TCDAE on real hazy images, focusing on dehazing performance and visual clarity. | Used to evaluate no-reference and full-reference image quality assessment indicators, focusing on the performance of dehazing models. | Used to evaluate real-world dehazing performance, focusing on task-driven scenarios and practical applications. | Used to evaluate no-reference and full-reference image quality assessment metrics in the context of image restoration, focusing on real-world blur conditions. | Used to test dehazing algorithms under non-homogeneous haze conditions, providing a diverse set of hazy and haze-free images. | Used to evaluate no-reference and full-reference image quality assessment metrics in the context of image dehazing, focusing on non-homogeneous hazy images. | Used to evaluate no-reference and full-reference image quality assessment metrics in the context of image dehazing, focusing on dense haze conditions.",
          "citing_paper_id": "272444309",
          "cited_paper_id": 218538083,
          "context_text": "We also evaluate the above models on no-reference and full-reference image quality assessment indicators on RTTS [32] Dense-Haze [1], and NH-HAZE [2].",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions three specific datasets used for evaluating image quality assessment indicators in the context of image dehazing.",
          "citing_paper_doi": "10.1145/3694973",
          "cited_paper_doi": "10.1109/CVPRW50498.2020.00230",
          "citing_paper_url": "https://www.semanticscholar.org/paper/2893ee935e0c5113a844cfa54f6e79dfda827bba",
          "cited_paper_url": "https://www.semanticscholar.org/paper/88e8a624825f0fb457fac26f65d43d9da76beab0",
          "citing_paper_year": 2024,
          "cited_paper_year": 2020
        },
        {
          "dataset_name": "RTTS",
          "dataset_description": "Used to evaluate object detection performance on hazy images, specifically comparing dehazing outcomes from different algorithms.",
          "citing_paper_id": "272444309",
          "cited_paper_id": 218581297,
          "context_text": "Figure 10 presents object detection results on hazy images from RTTS, alongside dehazing outcomes produced by DA-Net [53], PSD [12], and TCDAE.",
          "confidence_score": 0.9,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions 'RTTS' as a source of hazy images for object detection results. The name 'RTTS' appears to be a specific dataset, likely used for evaluating dehazing algorithms.",
          "citing_paper_doi": "10.1145/3694973",
          "cited_paper_doi": "10.1109/cvpr42600.2020.00288",
          "citing_paper_url": "https://www.semanticscholar.org/paper/2893ee935e0c5113a844cfa54f6e79dfda827bba",
          "cited_paper_url": "https://www.semanticscholar.org/paper/7c7b46ac41f12074e5cdd7187efbaa33f8eab6b4",
          "citing_paper_year": 2024,
          "cited_paper_year": 2020
        }
      ]
    },
    {
      "cited_paper_id": "102350705",
      "citation_count": 0,
      "total_dataset_mentions": 4,
      "unique_datasets": [
        "Dense-Haze"
      ],
      "dataset_details": [
        {
          "dataset_name": "Dense-Haze",
          "dataset_description": "Used to evaluate the generalization of the model in non-homogeneous haze conditions, focusing on real-world image dehazing performance. | Used to evaluate the generalization of the model in dense haze conditions, focusing on real-world image dehazing performance.",
          "citing_paper_id": "260957038",
          "cited_paper_id": 218538083,
          "context_text": "In addition, we adopt two real-world datasets: Dense-Haze (Ancuti et al., 2019) and NH-HAZE (Ancuti et al., 2020) to evaluate the generalization.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions two specific datasets, Dense-Haze and NH-HAZE, which are used to evaluate the generalization of the model in real-world scenarios.",
          "citing_paper_doi": null,
          "cited_paper_doi": "10.1109/CVPRW50498.2020.00230",
          "citing_paper_url": "https://www.semanticscholar.org/paper/4b688c486f2b223212682fd2b3f72cc0caa94b1b",
          "cited_paper_url": "https://www.semanticscholar.org/paper/88e8a624825f0fb457fac26f65d43d9da76beab0",
          "citing_paper_year": 2023,
          "cited_paper_year": 2020
        },
        {
          "dataset_name": "Dense-Haze",
          "dataset_description": "Used to validate the performance of the image dehazing approach, focusing on dense haze conditions and comparing results with haze-free images. | Used for synthetic dehazing experiments, evaluating the effectiveness of dehazing algorithms on controlled, simulated hazy images. | Used for real-world dehazing experiments, assessing the performance of dehazing algorithms on actual hazy images with varying densities.",
          "citing_paper_id": "259298517",
          "cited_paper_id": 102350705,
          "context_text": "Additionally, we validate the performance of our approach on the real hazy dataset Dense-Haze (Ancuti et al., 2019).",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions the use of the 'Dense-Haze' dataset for validating the performance of their image dehazing approach.",
          "citing_paper_doi": null,
          "cited_paper_doi": "10.1109/ICIP.2019.8803046",
          "citing_paper_url": "https://www.semanticscholar.org/paper/6af728164e4424218967fd356b3448f5037acd88",
          "cited_paper_url": "https://www.semanticscholar.org/paper/a3be7b43518abc11a074e6c15d1b5c443e53737c",
          "citing_paper_year": 2023,
          "cited_paper_year": 2019
        },
        {
          "dataset_name": "Dense-Haze",
          "dataset_description": "Used to assess dehazing techniques on non-homogeneous hazy and haze-free images, emphasizing variability in haze distribution. | Used for evaluating the trained models on indoor scenes, providing a benchmark for indoor dehazing performance. | Used to benchmark single-image dehazing methods, focusing on performance metrics such as PSNR and FLOPs. | Used to benchmark single-image dehazing methods, focusing on dense-haze and haze-free images to evaluate performance under challenging conditions. | Used to evaluate the performance of the model in single-image dehazing, specifically measuring PSNR improvement over the DeHamer model with significantly fewer parameters. | Used for training the tiny model to evaluate the effectiveness of the proposed modules in single-image dehazing. | Used for training models focused on outdoor scenes, assessing the performance of dehazing algorithms in diverse outdoor conditions. | Used for training models focused on indoor scenes, evaluating the effectiveness of dehazing algorithms in controlled environments. | Used to evaluate the performance of the proposed method in image dehazing, comparing PSNR, parameters, and FLOPs against PMNet. | Used to evaluate dehazing algorithms on real hazy and haze-free outdoor images, providing a realistic testbed for assessing algorithm robustness. | Used for testing the performance of the trained model, specifically to assess the dehazing quality and effectiveness of the modules. | Used for evaluating the trained models on outdoor scenes, serving as a benchmark for outdoor dehazing performance.",
          "citing_paper_id": "265537221",
          "cited_paper_id": 39760169,
          "context_text": "We report the quantitative performance of image dehazing approaches on both syn-13005 thetic [27] and real-world [2, 3, 4] datasets in Table 1.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions synthetic and real-world datasets used for evaluating image dehazing approaches. The cited papers provide specific names for these datasets.",
          "citing_paper_doi": "10.1109/ICCV51070.2023.01195",
          "cited_paper_doi": "10.1109/TIP.2018.2867951",
          "citing_paper_url": "https://www.semanticscholar.org/paper/1e277206e1a59e7e22f1fdcab886d3869cdd0871",
          "cited_paper_url": "https://www.semanticscholar.org/paper/abf75b81b96ff2cd262086675deba55f907108db",
          "citing_paper_year": 2023,
          "cited_paper_year": 2017
        },
        {
          "dataset_name": "Dense-Haze",
          "dataset_description": "Used to assess dehazing techniques on non-homogeneous hazy and haze-free images, emphasizing variability in haze distribution. | Used to evaluate the performance of FocalNet in image dehazing, specifically comparing PSNR and SSIM metrics against PMNet. | Used to assess the model's effectiveness in dehazing non-homogeneous hazy images, emphasizing the ability to handle varying haze densities. | Used to benchmark single-image dehazing methods, focusing on dense-haze and haze-free images to evaluate performance under challenging conditions. | Used to evaluate dehazing algorithms on real hazy and haze-free outdoor images, providing a realistic testbed for assessing algorithm robustness. | Used to test the model's capability in dehazing real outdoor images, ensuring robustness across diverse environmental conditions. | Used to evaluate the model's performance in dehazing dense haze conditions, focusing on the robustness in challenging real-world scenarios.",
          "citing_paper_id": "265537221",
          "cited_paper_id": 206598041,
          "context_text": "We report the quantitative performance of image dehazing approaches on both syn-13005 thetic [27] and real-world [2, 3, 4] datasets in Table 1.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions synthetic and real-world datasets used for evaluating image dehazing approaches. The cited papers provide specific names for these datasets.",
          "citing_paper_doi": "10.1109/ICCV51070.2023.01195",
          "cited_paper_doi": "10.1109/CVPRW.2018.00119",
          "citing_paper_url": "https://www.semanticscholar.org/paper/1e277206e1a59e7e22f1fdcab886d3869cdd0871",
          "cited_paper_url": "https://www.semanticscholar.org/paper/fb07f990b8247b46fec6dcd2a065a5f7e553db39",
          "citing_paper_year": 2023,
          "cited_paper_year": 2018
        }
      ]
    },
    {
      "cited_paper_id": "189762039",
      "citation_count": 0,
      "total_dataset_mentions": 4,
      "unique_datasets": [
        "FFHQ"
      ],
      "dataset_details": [
        {
          "dataset_name": "FFHQ",
          "dataset_description": "Used for evaluating image restoration, particularly for diverse real-world images, to test generalization and robustness. | Used for evaluating image restoration, focusing on specific image degradation types to assess algorithm performance. | Used for evaluating image restoration techniques, focusing on high-resolution face images to assess texture recovery and realism. | Used for evaluating high-resolution image restoration, focusing on diverse content to test scalability and quality. | Used for evaluating curve text detection in wild images, assessing the ability to restore and recognize curved text.",
          "citing_paper_id": "260843368",
          "cited_paper_id": 37734681,
          "context_text": "A couple of other datasets are also often used for image restoration tasks including FFHQ [34], WED [55], OST [76], SCUT-CTW1500 [85], and DIV8K [26].",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions several datasets used for image restoration tasks, which align with the research topic of All-in-One Image Restoration.",
          "citing_paper_doi": "10.1109/CVPRW59228.2023.00178",
          "cited_paper_doi": null,
          "citing_paper_url": "https://www.semanticscholar.org/paper/851446ea3c70c918cdf8c62db0dd0b1900806e5f",
          "cited_paper_url": "https://www.semanticscholar.org/paper/fcdf5055c902ec2d1570e598121f980a71fb90ca",
          "citing_paper_year": 2023,
          "cited_paper_year": 2017
        },
        {
          "dataset_name": "FFHQ",
          "dataset_description": "Used to evaluate IAGAN for image restoration, focusing on high-resolution face images and comparing reconstruction quality. | Used as a benchmark dataset to compare the performance of IAGAN against other generative models, focusing on image restoration and reconstruction quality. | Used to evaluate LaMa for image restoration, focusing on a diverse set of natural images and assessing the method's generalization.",
          "citing_paper_id": "249282628",
          "cited_paper_id": 189762039,
          "context_text": "(a) Measurement, (b) Ground truth, (c) IAGAN [20] for FFHQ, LaMa [43] for ImageNet, (d) DDRM [25], (e) ScoreSDE [41], (f) RePAINT [32], (g) MCG (Ours).",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The citation mentions specific datasets (FFHQ, ImageNet) used for evaluating different methods. No other verifiable resources are mentioned.",
          "citing_paper_doi": "10.48550/arXiv.2206.00941",
          "cited_paper_doi": "10.1609/AAAI.V34I04.5708",
          "citing_paper_url": "https://www.semanticscholar.org/paper/b3f5cf32178bcbed91aa5303b70963c6463f48a2",
          "cited_paper_url": "https://www.semanticscholar.org/paper/944d54502895811d3b2c72d1a1d49a395588f67e",
          "citing_paper_year": 2022,
          "cited_paper_year": 2019
        },
        {
          "dataset_name": "FFHQ",
          "dataset_description": "Used for evaluating general image restoration methods, focusing on large-scale image inpainting and quality assessment. | Used for evaluating face restoration methods, focusing on high-resolution image inpainting and quality assessment.",
          "citing_paper_id": "249282628",
          "cited_paper_id": 237513361,
          "context_text": "[20] for FFHQ, LaMa [43] for ImageNet, (d) DDRM [25], (e) Score-SDE [41], (f) RePAINT [32], (g) MCG (Ours).",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions 'FFHQ' and 'ImageNet', which are known datasets. 'LaMa' is likely a method, not a dataset.",
          "citing_paper_doi": "10.48550/arXiv.2206.00941",
          "cited_paper_doi": "10.1109/WACV51458.2022.00323",
          "citing_paper_url": "https://www.semanticscholar.org/paper/b3f5cf32178bcbed91aa5303b70963c6463f48a2",
          "cited_paper_url": "https://www.semanticscholar.org/paper/fdf7012ebe9d4c4d2d93004613e7a19f69a83a93",
          "citing_paper_year": 2022,
          "cited_paper_year": 2021
        },
        {
          "dataset_name": "FFHQ",
          "dataset_description": "Used for evaluating general image restoration methods, focusing on large-scale image inpainting and quality assessment. | Used for evaluating face restoration methods, focusing on high-resolution image inpainting and quality assessment.",
          "citing_paper_id": "249282628",
          "cited_paper_id": 246240274,
          "context_text": "[20] for FFHQ, LaMa [43] for ImageNet, (d) DDRM [25], (e) Score-SDE [41], (f) RePAINT [32], (g) MCG (Ours).",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions 'FFHQ' and 'ImageNet', which are known datasets. 'LaMa' is likely a method, not a dataset.",
          "citing_paper_doi": "10.48550/arXiv.2206.00941",
          "cited_paper_doi": "10.1109/CVPR52688.2022.01117",
          "citing_paper_url": "https://www.semanticscholar.org/paper/b3f5cf32178bcbed91aa5303b70963c6463f48a2",
          "cited_paper_url": "https://www.semanticscholar.org/paper/1e91fa21b890a8f5d615578f4ddf46c3cb394691",
          "citing_paper_year": 2022,
          "cited_paper_year": 2022
        }
      ]
    },
    {
      "cited_paper_id": "837707",
      "citation_count": 0,
      "total_dataset_mentions": 4,
      "unique_datasets": [
        "SRRS"
      ],
      "dataset_details": [
        {
          "dataset_name": "SRRS",
          "dataset_description": "Used to train and evaluate snow removal algorithms, focusing on context-aware deep network performance in various snow conditions. | Used to assess snow removal techniques, specifically targeting the reduction of snow artifacts and enhancement of image clarity. | Used to evaluate image deraining algorithms, focusing on removing rain streaks and improving visual quality in images.",
          "citing_paper_id": "259298517",
          "cited_paper_id": 837707,
          "context_text": "More results on SRRS (Chen et al., 2020a) and Snow100K (Liu et al., 2018) are provided in Appendix E. Image deraining results.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions two datasets, SRRS and Snow100K, which are used for image deraining results. These datasets are specific and relevant to the research topic of all-in-one image restoration.",
          "citing_paper_doi": null,
          "cited_paper_doi": "10.1109/TIP.2018.2806202",
          "citing_paper_url": "https://www.semanticscholar.org/paper/6af728164e4424218967fd356b3448f5037acd88",
          "cited_paper_url": "https://www.semanticscholar.org/paper/9e7487d7396acf298f972f0b8ab0397e0d9e7a0f",
          "citing_paper_year": 2023,
          "cited_paper_year": 2017
        },
        {
          "dataset_name": "SRRS",
          "dataset_description": "Used to assess snow removal techniques, specifically targeting the reduction of snow artifacts and enhancement of image clarity. | Used to evaluate image deraining algorithms, focusing on removing rain streaks and improving visual quality in images.",
          "citing_paper_id": "259298517",
          "cited_paper_id": 131773660,
          "context_text": "More results on SRRS (Chen et al., 2020a) and Snow100K (Liu et al., 2018) are provided in Appendix E. Image deraining results.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions two datasets, SRRS and Snow100K, which are used for image deraining results. These datasets are specific and relevant to the research topic of all-in-one image restoration.",
          "citing_paper_doi": null,
          "cited_paper_doi": "10.1109/ICCV.2019.00353",
          "citing_paper_url": "https://www.semanticscholar.org/paper/6af728164e4424218967fd356b3448f5037acd88",
          "cited_paper_url": "https://www.semanticscholar.org/paper/0d9eb72ea89bb7bf5720a7b2c2f3f77c26fc67a6",
          "citing_paper_year": 2023,
          "cited_paper_year": 2019
        },
        {
          "dataset_name": "SRRS",
          "dataset_description": "Used to assess snow removal techniques, specifically targeting the reduction of snow artifacts and enhancement of image clarity. | Used to evaluate image deraining algorithms, focusing on removing rain streaks and improving visual quality in images.",
          "citing_paper_id": "259298517",
          "cited_paper_id": 226308542,
          "context_text": "More results on SRRS (Chen et al., 2020a) and Snow100K (Liu et al., 2018) are provided in Appendix E. Image deraining results.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions two datasets, SRRS and Snow100K, which are used for image deraining results. These datasets are specific and relevant to the research topic of all-in-one image restoration.",
          "citing_paper_doi": null,
          "cited_paper_doi": "10.1007/978-3-030-58589-1_45",
          "citing_paper_url": "https://www.semanticscholar.org/paper/6af728164e4424218967fd356b3448f5037acd88",
          "cited_paper_url": "https://www.semanticscholar.org/paper/574b568fd5a4a33dcfd47a14b253a4b35acb95ee",
          "citing_paper_year": 2023,
          "cited_paper_year": 2020
        },
        {
          "dataset_name": "SRRS",
          "dataset_description": "Used to train and evaluate desnowing models, focusing on size and transparency-aware snow removal using modified partial convolution and veiling effect removal. | Used to evaluate desnowing algorithms, focusing on size and transparency-aware snow removal using modified partial convolution and veiling effect removal. | Used to train and evaluate desnowing models, providing a benchmark for comparing different desnowing algorithms and their performance. | Used to train and evaluate desnowing models, offering a large-scale dataset for robust testing and validation of desnowing techniques. | Used to evaluate the performance of FocalNet in image restoration, specifically measuring significant PSNR gains over NAFNet. | Used to evaluate the performance of FocalNet in image restoration, specifically measuring PSNR gains over NAFNet.",
          "citing_paper_id": "265537221",
          "cited_paper_id": 226308542,
          "context_text": "The numerical results on three desnowing datasets, i.e., SRRS [9], CSD [10], and Snow100K [37], are reported in Table 3.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions three specific datasets used for evaluating desnowing algorithms. These datasets are clearly named and relevant to the research topic of image restoration.",
          "citing_paper_doi": "10.1109/ICCV51070.2023.01195",
          "cited_paper_doi": "10.1007/978-3-030-58589-1_45",
          "citing_paper_url": "https://www.semanticscholar.org/paper/1e277206e1a59e7e22f1fdcab886d3869cdd0871",
          "cited_paper_url": "https://www.semanticscholar.org/paper/574b568fd5a4a33dcfd47a14b253a4b35acb95ee",
          "citing_paper_year": 2023,
          "cited_paper_year": 2020
        }
      ]
    },
    {
      "cited_paper_id": "199528450",
      "citation_count": 0,
      "total_dataset_mentions": 4,
      "unique_datasets": [
        "SOTS indoor testset"
      ],
      "dataset_details": [
        {
          "dataset_name": "SOTS indoor testset",
          "dataset_description": "Used to evaluate and compare the performance of various image dehazing methods, including GCANet, GridDehaze, DuRN, MSBDN, FFA-Net, and MAXIM-2S, focusing on visual quality and effectiveness.",
          "citing_paper_id": "245837508",
          "cited_paper_id": 53755281,
          "context_text": "Visual comparisons for image dehazing on SOTS indoor testset [45] among GCANet [13], GridDehaze [54], DuRN [55], MSBDN [22], FFA-Net [72], and our MAXIM-2S.",
          "confidence_score": 1.0,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions 'SOTS indoor testset' which is a specific dataset used for evaluating image dehazing methods. The dataset is used to compare the performance of various dehazing networks.",
          "citing_paper_doi": "10.1109/CVPR52688.2022.00568",
          "cited_paper_doi": "10.1109/WACV.2019.00151",
          "citing_paper_url": "https://www.semanticscholar.org/paper/fbf68eb0cf8237cef59ebdb301569c79b9676ff9",
          "cited_paper_url": "https://www.semanticscholar.org/paper/04ad242b95b2d3127680e790b82030ba9b9d2cce",
          "citing_paper_year": 2022,
          "cited_paper_year": 2018
        },
        {
          "dataset_name": "SOTS indoor testset",
          "dataset_description": "Used to evaluate and compare the performance of various image dehazing methods, including GCANet, GridDehaze, DuRN, MSBDN, FFA-Net, and MAXIM-2S, focusing on visual quality and effectiveness.",
          "citing_paper_id": "245837508",
          "cited_paper_id": 199528450,
          "context_text": "Visual comparisons for image dehazing on SOTS indoor testset [45] among GCANet [13], GridDehaze [54], DuRN [55], MSBDN [22], FFA-Net [72], and our MAXIM-2S.",
          "confidence_score": 1.0,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions 'SOTS indoor testset' which is a specific dataset used for evaluating image dehazing methods. The dataset is used to compare the performance of various dehazing networks.",
          "citing_paper_doi": "10.1109/CVPR52688.2022.00568",
          "cited_paper_doi": "10.1109/ICCV.2019.00741",
          "citing_paper_url": "https://www.semanticscholar.org/paper/fbf68eb0cf8237cef59ebdb301569c79b9676ff9",
          "cited_paper_url": "https://www.semanticscholar.org/paper/56ac346b77730828f5fc3ee3cf1f47d62a1d3343",
          "citing_paper_year": 2022,
          "cited_paper_year": 2019
        },
        {
          "dataset_name": "SOTS indoor testset",
          "dataset_description": "Used to evaluate and compare the performance of various image dehazing methods, including GCANet, GridDehaze, DuRN, MSBDN, FFA-Net, and MAXIM-2S, focusing on visual quality and effectiveness.",
          "citing_paper_id": "245837508",
          "cited_paper_id": 208138077,
          "context_text": "Visual comparisons for image dehazing on SOTS indoor testset [45] among GCANet [13], GridDehaze [54], DuRN [55], MSBDN [22], FFA-Net [72], and our MAXIM-2S.",
          "confidence_score": 1.0,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions 'SOTS indoor testset' which is a specific dataset used for evaluating image dehazing methods. The dataset is used to compare the performance of various dehazing networks.",
          "citing_paper_doi": "10.1109/CVPR52688.2022.00568",
          "cited_paper_doi": "10.1609/AAAI.V34I07.6865",
          "citing_paper_url": "https://www.semanticscholar.org/paper/fbf68eb0cf8237cef59ebdb301569c79b9676ff9",
          "cited_paper_url": "https://www.semanticscholar.org/paper/22418b7b7d5d6d18b81f232f60c22a15d1c8a38a",
          "citing_paper_year": 2022,
          "cited_paper_year": 2019
        },
        {
          "dataset_name": "SOTS indoor testset",
          "dataset_description": "Used to evaluate and compare the performance of various image dehazing methods, including GCANet, GridDehaze, DuRN, MSBDN, FFA-Net, and MAXIM-2S, focusing on visual quality and effectiveness.",
          "citing_paper_id": "245837508",
          "cited_paper_id": 216562731,
          "context_text": "Visual comparisons for image dehazing on SOTS indoor testset [45] among GCANet [13], GridDehaze [54], DuRN [55], MSBDN [22], FFA-Net [72], and our MAXIM-2S.",
          "confidence_score": 1.0,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions 'SOTS indoor testset' which is a specific dataset used for evaluating image dehazing methods. The dataset is used to compare the performance of various dehazing networks.",
          "citing_paper_doi": "10.1109/CVPR52688.2022.00568",
          "cited_paper_doi": "10.1109/cvpr42600.2020.00223",
          "citing_paper_url": "https://www.semanticscholar.org/paper/fbf68eb0cf8237cef59ebdb301569c79b9676ff9",
          "cited_paper_url": "https://www.semanticscholar.org/paper/57f2019325ce70212dd56d59b413564f98e00664",
          "citing_paper_year": 2022,
          "cited_paper_year": 2020
        }
      ]
    },
    {
      "cited_paper_id": "5778488",
      "citation_count": 0,
      "total_dataset_mentions": 4,
      "unique_datasets": [
        "LIME"
      ],
      "dataset_details": [
        {
          "dataset_name": "LIME",
          "dataset_description": "Used to evaluate illumination compensation algorithms, focusing on the method's effectiveness without ground truth. | Used to test dynamic range compression methods, emphasizing color fidelity and detail retention. | Used to test the method on various visual effects, assessing performance in the absence of ground truth. | Used to evaluate various image enhancement techniques, focusing on visual quality and user perception. | Used to test the method on low-light image enhancement, focusing on illumination map estimation without ground truth. | Used to evaluate low-light image enhancement techniques, focusing on illumination map estimation and perceptual quality. | Used to evaluate multi-exposure image fusion techniques, focusing on contrast and detail preservation. | Used to assess non-parametric exposure correction methods, emphasizing visual quality and naturalness. | Used to evaluate the method on noise reduction and enhancement, specifically in the absence of ground truth. | Used to assess multi-exposure image fusion quality, testing the method's performance without ground truth.",
          "citing_paper_id": "257496232",
          "cited_paper_id": 4828378,
          "context_text": "In addition to the above eight benchmarks, we test our method on five datasets: LIME [18], NPE [50], MEF [36], DICM [28], and VV [47] that have no ground truth.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions five datasets by name, which are used to test the method in the absence of ground truth. These datasets are specific and relevant to image restoration and enhancement.",
          "citing_paper_doi": "10.1109/ICCV51070.2023.01149",
          "cited_paper_doi": "10.1109/TIP.2015.2442920",
          "citing_paper_url": "https://www.semanticscholar.org/paper/42754ad2b7b2193b4d8e3d067808cea8913869c4",
          "cited_paper_url": "https://www.semanticscholar.org/paper/19986f7af0086f0e329779f3497e97440765d170",
          "citing_paper_year": 2023,
          "cited_paper_year": 2015
        },
        {
          "dataset_name": "LIME",
          "dataset_description": "Used to evaluate illumination compensation algorithms, focusing on the method's effectiveness without ground truth. | Used to test dynamic range compression methods, emphasizing color fidelity and detail retention. | Used to test the method on various visual effects, assessing performance in the absence of ground truth. | Used to evaluate various image enhancement techniques, focusing on visual quality and user perception. | Used to test the method on low-light image enhancement, focusing on illumination map estimation without ground truth. | Used to evaluate low-light image enhancement techniques, focusing on illumination map estimation and perceptual quality. | Used to evaluate multi-exposure image fusion techniques, focusing on contrast and detail preservation. | Used to assess non-parametric exposure correction methods, emphasizing visual quality and naturalness. | Used to evaluate the method on noise reduction and enhancement, specifically in the absence of ground truth. | Used to assess multi-exposure image fusion quality, testing the method's performance without ground truth.",
          "citing_paper_id": "257496232",
          "cited_paper_id": 5041994,
          "context_text": "In addition to the above eight benchmarks, we test our method on five datasets: LIME [18], NPE [50], MEF [36], DICM [28], and VV [47] that have no ground truth.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions five datasets by name, which are used to test the method in the absence of ground truth. These datasets are specific and relevant to image restoration and enhancement.",
          "citing_paper_doi": "10.1109/ICCV51070.2023.01149",
          "cited_paper_doi": "10.1007/s11042-017-4783-x",
          "citing_paper_url": "https://www.semanticscholar.org/paper/42754ad2b7b2193b4d8e3d067808cea8913869c4",
          "cited_paper_url": "https://www.semanticscholar.org/paper/e4e8f0d68b4957e651c6e4d926c7ddfc7725c049",
          "citing_paper_year": 2023,
          "cited_paper_year": 2018
        },
        {
          "dataset_name": "LIME",
          "dataset_description": "Used to evaluate illumination compensation algorithms, focusing on the method's effectiveness without ground truth. | Used to test dynamic range compression methods, emphasizing color fidelity and detail retention. | Used to test the method on various visual effects, assessing performance in the absence of ground truth. | Used to evaluate various image enhancement techniques, focusing on visual quality and user perception. | Used to test the method on low-light image enhancement, focusing on illumination map estimation without ground truth. | Used to evaluate low-light image enhancement techniques, focusing on illumination map estimation and perceptual quality. | Used to evaluate multi-exposure image fusion techniques, focusing on contrast and detail preservation. | Used to assess non-parametric exposure correction methods, emphasizing visual quality and naturalness. | Used to evaluate the method on noise reduction and enhancement, specifically in the absence of ground truth. | Used to assess multi-exposure image fusion quality, testing the method's performance without ground truth.",
          "citing_paper_id": "257496232",
          "cited_paper_id": 5778488,
          "context_text": "In addition to the above eight benchmarks, we test our method on five datasets: LIME [18], NPE [50], MEF [36], DICM [28], and VV [47] that have no ground truth.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions five datasets by name, which are used to test the method in the absence of ground truth. These datasets are specific and relevant to image restoration and enhancement.",
          "citing_paper_doi": "10.1109/ICCV51070.2023.01149",
          "cited_paper_doi": "10.1109/TIP.2016.2639450",
          "citing_paper_url": "https://www.semanticscholar.org/paper/42754ad2b7b2193b4d8e3d067808cea8913869c4",
          "cited_paper_url": "https://www.semanticscholar.org/paper/aea5b4139f1a72e52d799906a782ba94b2687ee7",
          "citing_paper_year": 2023,
          "cited_paper_year": 2017
        },
        {
          "dataset_name": "LIME",
          "dataset_description": "Used to evaluate illumination compensation algorithms, focusing on the method's effectiveness without ground truth. | Used to test dynamic range compression methods, emphasizing color fidelity and detail retention. | Used to test the method on various visual effects, assessing performance in the absence of ground truth. | Used to evaluate various image enhancement techniques, focusing on visual quality and user perception. | Used to test the method on low-light image enhancement, focusing on illumination map estimation without ground truth. | Used to evaluate low-light image enhancement techniques, focusing on illumination map estimation and perceptual quality. | Used to evaluate multi-exposure image fusion techniques, focusing on contrast and detail preservation. | Used to assess non-parametric exposure correction methods, emphasizing visual quality and naturalness. | Used to evaluate the method on noise reduction and enhancement, specifically in the absence of ground truth. | Used to assess multi-exposure image fusion quality, testing the method's performance without ground truth.",
          "citing_paper_id": "257496232",
          "cited_paper_id": 18235798,
          "context_text": "In addition to the above eight benchmarks, we test our method on five datasets: LIME [18], NPE [50], MEF [36], DICM [28], and VV [47] that have no ground truth.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions five datasets by name, which are used to test the method in the absence of ground truth. These datasets are specific and relevant to image restoration and enhancement.",
          "citing_paper_doi": "10.1109/ICCV51070.2023.01149",
          "cited_paper_doi": "10.1109/TIP.2013.2284059",
          "citing_paper_url": "https://www.semanticscholar.org/paper/42754ad2b7b2193b4d8e3d067808cea8913869c4",
          "cited_paper_url": "https://www.semanticscholar.org/paper/f048dce88f1de442a4f7aede4f5724dcf75965e2",
          "citing_paper_year": 2023,
          "cited_paper_year": 2013
        }
      ]
    },
    {
      "cited_paper_id": "996788",
      "citation_count": 0,
      "total_dataset_mentions": 3,
      "unique_datasets": [
        "Set12"
      ],
      "dataset_details": [
        {
          "dataset_name": "Set12",
          "dataset_description": "Used to evaluate image restoration methods, focusing on performance at different noise levels (σ=15, σ=25, σ=50). | Used to evaluate image restoration methods, focusing on performance metrics at different noise levels (σ=15, σ=25, σ=50). | Used for denoising experiments with real-world datasets, evaluating denoising algorithms on smartphone camera images. | Used for denoising experiments with real-world datasets, evaluating denoising algorithms on diverse real-world images. | Used for denoising experiments with synthetic benchmark datasets, evaluating denoising algorithms under controlled conditions.",
          "citing_paper_id": "244346144",
          "cited_paper_id": 64193,
          "context_text": "Set12 [101] BSD68 [52] Urban100 [29] Method σ=15 σ=25 σ=50 σ=15 σ=25 σ=50 σ=15 σ=25 σ=50",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The citation mentions specific datasets used for evaluation in image restoration experiments. These datasets are likely collections of images used to test the performance of different methods.",
          "citing_paper_doi": "10.1109/CVPR52688.2022.00564",
          "cited_paper_doi": "10.1109/ICCV.2001.937655",
          "citing_paper_url": "https://www.semanticscholar.org/paper/1e88d5afe19aea324d33541f60a90b7036894c32",
          "cited_paper_url": "https://www.semanticscholar.org/paper/9a1ed876196ec9733acb1daa6d65e35ff0414291",
          "citing_paper_year": 2021,
          "cited_paper_year": 2001
        },
        {
          "dataset_name": "Set12",
          "dataset_description": "Used to evaluate synthetic image denoising, featuring high-resolution urban scenes to assess performance on complex, real-world images. | Used to evaluate synthetic image denoising, providing a larger set of diverse images to test robustness and generalization of denoising algorithms. | Used to evaluate image restoration algorithms, focusing on a small set of high-quality images to assess performance in detail. | Used to evaluate image restoration algorithms, featuring a variety of urban scenes to test performance on complex, real-world images. | Used to evaluate image restoration algorithms, providing a larger set of diverse images to test generalization and robustness. | Used to evaluate synthetic image denoising, focusing on a small set of high-quality images to assess denoising performance.",
          "citing_paper_id": "232170566",
          "cited_paper_id": 64193,
          "context_text": "2) Synthetic Image Denoising: For this application, three standard benchmark datasets, i.e., Set12, BSD68 [48], and Urban100 [41] are evaluated.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions three specific datasets used for evaluating synthetic image denoising. These datasets are clearly named and are relevant to the topic of image restoration.",
          "citing_paper_doi": "10.1109/TMM.2021.3063916",
          "cited_paper_doi": "10.1109/ICCV.2001.937655",
          "citing_paper_url": "https://www.semanticscholar.org/paper/7d76ae738c806330632255ad2c592525b3d2df22",
          "cited_paper_url": "https://www.semanticscholar.org/paper/9a1ed876196ec9733acb1daa6d65e35ff0414291",
          "citing_paper_year": 2021,
          "cited_paper_year": 2001
        },
        {
          "dataset_name": "Set12",
          "dataset_description": "Used to test the method's ability to denoise high-resolution urban scene images, highlighting its performance on complex textures and structures. | Used to evaluate the denoising performance of the proposed method on a small set of grayscale images, focusing on visual quality and quantitative metrics. | Used to assess the denoising effectiveness on a larger set of grayscale images, emphasizing robustness and generalization across diverse image content.",
          "citing_paper_id": "266690738",
          "cited_paper_id": 996788,
          "context_text": "For grayscale image denoising, we evaluate our method on Set12 [118], BSD68 [66], and Ur-ban100 [40].",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The citation mentions specific datasets used for evaluating the denoising method. These datasets are well-known in the image processing community and are used for benchmarking.",
          "citing_paper_doi": "10.1109/CVPR52733.2024.00277",
          "cited_paper_doi": "10.1109/TIP.2017.2662206",
          "citing_paper_url": "https://www.semanticscholar.org/paper/c6495e41d3974476e0be9d5137c3dda51125bcec",
          "cited_paper_url": "https://www.semanticscholar.org/paper/0c00a328fa7cd56ee60338c54e89bd48310db80b",
          "citing_paper_year": 2023,
          "cited_paper_year": 2016
        }
      ]
    },
    {
      "cited_paper_id": "64193",
      "citation_count": 0,
      "total_dataset_mentions": 3,
      "unique_datasets": [
        "Manga109"
      ],
      "dataset_details": [
        {
          "dataset_name": "Manga109",
          "dataset_description": "Used for assessing the effectiveness of image restoration techniques, particularly in handling various types of image degradation. | Used to assess the performance of image restoration methods on manga images, specifically targeting the preservation of line art and color fidelity. | Used to test the ability of image restoration models to handle complex urban scenes, emphasizing detail preservation and texture recovery. | Used for evaluating the generalization capabilities of image restoration algorithms across diverse natural images, focusing on edge and structure preservation. | Used for evaluating single-image super-resolution methods, focusing on high-quality image restoration and upscaling performance.",
          "citing_paper_id": "222141081",
          "cited_paper_id": 64193,
          "context_text": "Set5 [4], Set14 [34], Urban100 [19], BSD100 [24] and Manga109 [25].",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions specific datasets used for image restoration tasks, which are directly relevant to the research topic.",
          "citing_paper_doi": null,
          "cited_paper_doi": "10.1109/ICCV.2001.937655",
          "citing_paper_url": "https://www.semanticscholar.org/paper/c8230ea819dc5774b0fce1766b9a5c094bb1106c",
          "cited_paper_url": "https://www.semanticscholar.org/paper/9a1ed876196ec9733acb1daa6d65e35ff0414291",
          "citing_paper_year": 2020,
          "cited_paper_year": 2001
        },
        {
          "dataset_name": "Manga109",
          "dataset_description": "Used to evaluate × 4 scale image super-resolution models, specifically measuring PSNR improvements of 0.16 dB and 0.12 dB in retrained models. | Used to evaluate image deraining performance, specifically measuring PSNR gains of retrained IDT model. | Used to evaluate ×4 scale image super-resolution performance, specifically measuring PSNR improvements of retrained models.",
          "citing_paper_id": "265999804",
          "cited_paper_id": 6540453,
          "context_text": "More specifically, based on lightweight EDSR (Lim et al. 2017) and SwinIR (Liang et al. 2021), our retrained models show superior performance on the Manga109 test dataset for × 4 scale image super resolution, boasting improvements of 0.16 dB and 0.12 dB in terms of PSNR. Retrained IDT (Xiao et al.…",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions the Manga109 test dataset, which is a specific dataset used for evaluating image super-resolution models.",
          "citing_paper_doi": "10.48550/arXiv.2309.06023",
          "cited_paper_doi": "10.1109/CVPRW.2017.151",
          "citing_paper_url": "https://www.semanticscholar.org/paper/8ce208dfd1259d087e0195f5677cbf2865411ba8",
          "cited_paper_url": "https://www.semanticscholar.org/paper/7ba5d3808e117e7a68dc40331ce1d483ceeedcb2",
          "citing_paper_year": 2023,
          "cited_paper_year": 2017
        },
        {
          "dataset_name": "Manga109",
          "dataset_description": "Used for evaluating rain removal from images, focusing on deraining performance using PSNR and SSIM metrics. | Used for evaluating manga image restoration, focusing on dejpeging performance using PSNR and SSIM metrics. | Used to test image super-resolution at a scale factor of ×2, evaluating the performance of the proposed method on manga images. | Used for evaluating image restoration on classic images, focusing on dejpeging performance using PSNR metric. | Used to assess de-raining effectiveness, containing synthetic rain images with ground truth for quantitative evaluation. | Used to evaluate denoising performance, consisting of 68 color images with varying noise levels. | Used to test super-resolution methods, comprising 5 classic images for benchmarking upscaling quality. | Used to evaluate super-resolution performance, focusing on image quality and restoration accuracy. | Used to assess super-resolution performance on manga images, emphasizing detail preservation and visual fidelity. | Used to evaluate de-raining performance, focusing on manga images with metadata for diverse scenarios. | Used for evaluating image denoising, focusing on noise reduction performance using PSNR and SSIM metrics.",
          "citing_paper_id": "259108489",
          "cited_paper_id": 7994542,
          "context_text": "H-MoE S-MoE Params(M) FLOPs(G) SR ×2 Derain Denoise 15 Dejpeg 40 Manga109 [17] Rain100L [52] CBSD68 [36] Classic5 [14] PSNR SSIM PSNR SSIM PSNR PSNR",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions several datasets used for evaluating image restoration tasks, including deraining, denoising, and dejpeging. 'Manga109', 'Rain100L', 'CBSD68', and 'Classic5' are explicitly named and are known datasets in the field.",
          "citing_paper_doi": "10.48550/arXiv.2306.05390",
          "cited_paper_doi": "10.1145/3011549.3011551",
          "citing_paper_url": "https://www.semanticscholar.org/paper/4296bf82e67051e4cba10e510932f55fc12dd1e2",
          "cited_paper_url": "https://www.semanticscholar.org/paper/48320a4be9cc741fdb28ad72f359c449e41309cc",
          "citing_paper_year": 2023,
          "cited_paper_year": 2016
        }
      ]
    },
    {
      "cited_paper_id": "837707",
      "citation_count": 0,
      "total_dataset_mentions": 3,
      "unique_datasets": [
        "Snow100K-L"
      ],
      "dataset_details": [
        {
          "dataset_name": "Snow100K-L",
          "dataset_description": "Used to train a model for snow removal, providing synthetic snow images to enhance the robustness of the restoration algorithm. | Used to train a model for heavy rain and haze removal, providing images with rain streaks and haze to address complex weather conditions. | Used to assess raindrop removal, focusing on the method's ability to restore images degraded by raindrops. | Used to test snow removal performance, evaluating the effectiveness of the restoration method on snowy images. | Used to evaluate general image restoration, providing a diverse set of images for comprehensive testing. | Used to train a model for raindrop removal, providing real raindrop images to improve the model's performance on rainy conditions.",
          "citing_paper_id": "259164829",
          "cited_paper_id": 837707,
          "context_text": "Accordingly, the test dataset consists of 16,801 images from the Snow100K-L test set [9], 58 images from the RainDrop test set [6] and 750 images from the Test1 dataset [29].",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions three specific datasets used for testing image restoration methods, which are directly relevant to the 'All-in-One Image Restoration' topic.",
          "citing_paper_doi": "10.1109/TIP.2024.3368961",
          "cited_paper_doi": "10.1109/TIP.2018.2806202",
          "citing_paper_url": "https://www.semanticscholar.org/paper/98f33bb7102c5c5cc9fac3335e97a0b1b6cfbb1a",
          "cited_paper_url": "https://www.semanticscholar.org/paper/9e7487d7396acf298f972f0b8ab0397e0d9e7a0f",
          "citing_paper_year": 2023,
          "cited_paper_year": 2017
        },
        {
          "dataset_name": "Snow100K-L",
          "dataset_description": "Used to train a model for snow removal, providing synthetic snow images to enhance the robustness of the restoration algorithm. | Used to train a model for heavy rain and haze removal, providing images with rain streaks and haze to address complex weather conditions. | Used to assess raindrop removal, focusing on the method's ability to restore images degraded by raindrops. | Used to test snow removal performance, evaluating the effectiveness of the restoration method on snowy images. | Used to evaluate general image restoration, providing a diverse set of images for comprehensive testing. | Used to train a model for raindrop removal, providing real raindrop images to improve the model's performance on rainy conditions.",
          "citing_paper_id": "259164829",
          "cited_paper_id": 131773964,
          "context_text": "Accordingly, the test dataset consists of 16,801 images from the Snow100K-L test set [9], 58 images from the RainDrop test set [6] and 750 images from the Test1 dataset [29].",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions three specific datasets used for testing image restoration methods, which are directly relevant to the 'All-in-One Image Restoration' topic.",
          "citing_paper_doi": "10.1109/TIP.2024.3368961",
          "cited_paper_doi": "10.1109/CVPR.2019.00173",
          "citing_paper_url": "https://www.semanticscholar.org/paper/98f33bb7102c5c5cc9fac3335e97a0b1b6cfbb1a",
          "cited_paper_url": "https://www.semanticscholar.org/paper/b1b7eeac57e1e78e02d9d97eb809d8459bd06050",
          "citing_paper_year": 2023,
          "cited_paper_year": 2019
        },
        {
          "dataset_name": "Snow100K-L",
          "dataset_description": "Used to assess raindrop removal, focusing on the method's ability to restore images degraded by raindrops. | Used to evaluate general image restoration, providing a diverse set of images for comprehensive testing. | Used to test snow removal performance, evaluating the effectiveness of the restoration method on snowy images.",
          "citing_paper_id": "259164829",
          "cited_paper_id": 244346144,
          "context_text": "Accordingly, the test dataset consists of 16,801 images from the Snow100K-L test set [9], 58 images from the RainDrop test set [6] and 750 images from the Test1 dataset [29].",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions three specific datasets used for testing image restoration methods, which are directly relevant to the 'All-in-One Image Restoration' topic.",
          "citing_paper_doi": "10.1109/TIP.2024.3368961",
          "cited_paper_doi": "10.1109/CVPR52688.2022.00564",
          "citing_paper_url": "https://www.semanticscholar.org/paper/98f33bb7102c5c5cc9fac3335e97a0b1b6cfbb1a",
          "cited_paper_url": "https://www.semanticscholar.org/paper/1e88d5afe19aea324d33541f60a90b7036894c32",
          "citing_paper_year": 2023,
          "cited_paper_year": 2021
        }
      ]
    },
    {
      "cited_paper_id": "91184545",
      "citation_count": 0,
      "total_dataset_mentions": 3,
      "unique_datasets": [
        "SPA+"
      ],
      "dataset_details": [
        {
          "dataset_name": "SPA+",
          "dataset_description": "Used for image desnowing, evaluating the method's performance in removing snow artifacts from images. | Used for image dehazing, testing the method's capability to restore images degraded by haze. | Used for image deraining, assessing the method's effectiveness in removing rain artifacts from images.",
          "citing_paper_id": "273643057",
          "cited_paper_id": 235719890,
          "context_text": "This comprehensive dataset comprises the SPA+ [42] dataset for image deraining, RealSnow [42] for image desnowing, and REVIDE [41] for image dehazing, allowing us to assess the effectiveness of our method across diverse real-world degradation scenarios. iv.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions three specific datasets used for evaluating the effectiveness of the method across various image restoration tasks.",
          "citing_paper_doi": "10.1145/3664647.3680762",
          "cited_paper_doi": "10.1109/CVPR46437.2021.00912",
          "citing_paper_url": "https://www.semanticscholar.org/paper/f292c903c1571ba0ee97aacdba29b7008b2fe501",
          "cited_paper_url": "https://www.semanticscholar.org/paper/cf29f0e9f66174ad274c6f6ef587c2f24c162358",
          "citing_paper_year": 2024,
          "cited_paper_year": 2021
        },
        {
          "dataset_name": "SPA+",
          "dataset_description": "Used to enhance and expand the existing SPA dataset for single-image deraining, focusing on improving rain removal performance through spatial attention mechanisms. | Served as the foundation for constructing the SPA+ dataset, providing initial data for single-image deraining experiments.",
          "citing_paper_id": "259144110",
          "cited_paper_id": 91184545,
          "context_text": "For deraining, we construct the SPA+ dataset based on the previous SPA [73] dataset.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions the construction of the SPA+ dataset based on the previous SPA dataset. Both are specific datasets used for deraining tasks.",
          "citing_paper_doi": "10.1109/CVPR52729.2023.02083",
          "cited_paper_doi": "10.1109/CVPR.2019.01255",
          "citing_paper_url": "https://www.semanticscholar.org/paper/25ac8b26688c0a2349c72337737e0252a0f8051e",
          "cited_paper_url": "https://www.semanticscholar.org/paper/d3fa312b1f12ee60391705d3cac34cbcad42db14",
          "citing_paper_year": 2023,
          "cited_paper_year": 2019
        },
        {
          "dataset_name": "SPA+",
          "dataset_description": "Specific subset of RESIDE used for training, containing outdoor hazy images to improve dehazing performance. | Used for evaluating no-reference image quality assessment methods, focusing on spatial quality metrics in image restoration. | Used to evaluate image dehazing models, providing dense-haze and haze-free images for benchmarking. | Used to evaluate the visual quality of TCDAE on real hazy images, focusing on dehazing performance and visual clarity. | Used to evaluate no-reference and full-reference image quality assessment metrics in the context of image dehazing, focusing on non-homogeneous hazy images. | Used to evaluate no-reference and full-reference image quality assessment metrics in the context of image restoration, focusing on objective quality evaluation. | Used to assess the effectiveness of TCDAE on dense haze conditions, emphasizing the model's ability to restore clear images. | Used to evaluate image dehazing models, providing non-homogeneous hazy and haze-free images for benchmarking. | Used to evaluate the performance of TCDAE in image restoration, specifically focusing on object detection tasks and comparing it with state-of-the-art methods. | Used for quantitative evaluation of image restoration methods, specifically applying no-reference image quality assessment indicators (FADE, BRISQUE, and PIQE) to measure performance. | Utilized for assessing image quality in the absence of reference images, emphasizing spatial quality evaluation in restoration tasks. | Subset of RESIDE used for training, consisting of unannotated real-world hazy images to enhance model robustness. | Used to evaluate no-reference and full-reference image quality assessment indicators, focusing on the performance of dehazing models. | Used to evaluate no-reference and full-reference image quality assessment metrics in the context of image restoration, focusing on real-world blur conditions. | Used to evaluate no-reference and full-reference image quality assessment metrics in the context of image dehazing, focusing on dense haze conditions. | Used to train a method for single image dehazing, focusing on the Outdoor Training Set and Unannotated real Hazy Images subsets. | Used to test TCDAE on non-homogeneous hazy images, evaluating its capability to handle varying haze densities and distributions. | Employed to test no-reference image quality assessment techniques, specifically for real-world blurred images in restoration scenarios. | Used to train models for single-image dehazing, comparing performance with and without specific branches of the D2C-DTL and S2R-DTL methods. | Used to evaluate no-reference and full-reference image quality assessment metrics in the context of image restoration, focusing on specific image degradation types.",
          "citing_paper_id": "272444309",
          "cited_paper_id": 39760169,
          "context_text": "For unlabeled datasets, including RTTS [32], SPA [63], and RealBlur-J [52], we rely on two renowned no-reference image quality assessment indicators: Blind/Referenceless Image Spatial Quality Evaluator (BRISQUE) [44] and Perception-based Image Quality Evaluator (PIQE) [62].",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions three specific datasets used for unlabeled data in the context of image restoration and quality assessment.",
          "citing_paper_doi": "10.1145/3694973",
          "cited_paper_doi": "10.1109/TIP.2018.2867951",
          "citing_paper_url": "https://www.semanticscholar.org/paper/2893ee935e0c5113a844cfa54f6e79dfda827bba",
          "cited_paper_url": "https://www.semanticscholar.org/paper/abf75b81b96ff2cd262086675deba55f907108db",
          "citing_paper_year": 2024,
          "cited_paper_year": 2017
        }
      ]
    },
    {
      "cited_paper_id": "837707",
      "citation_count": 0,
      "total_dataset_mentions": 3,
      "unique_datasets": [
        "Naturalness Preserved Enhancement dataset (NPE)"
      ],
      "dataset_details": [
        {
          "dataset_name": "Naturalness Preserved Enhancement dataset (NPE)",
          "dataset_description": "Used for image deraining, assessing the performance of rain removal techniques in various conditions. | Used for low-light enhancement, focusing on preserving naturalness in images while improving visibility. | Used for image dehazing, evaluating the effectiveness of dehazing algorithms in real-world scenarios. | Used for low-light enhancement, evaluating the performance of OneRestore in improving image quality under low-light conditions. | Used for single-image dehazing, assessing the ability of models to remove atmospheric haze and recover clear images from hazy scenes. | Used for low-light image enhancement, evaluating the effectiveness of models in improving visibility and color fidelity in dark conditions. | Used for deraining, testing the ability of OneRestore to remove rain streaks from images. | Used for image desnowing, testing the ability of models to remove snow from images in diverse settings. | Used for dehazing, assessing the effectiveness of OneRestore in removing haze from images. | Used for rain removal, testing the performance of models in eliminating rain streaks and restoring clean images from rainy scenes. | Used for desnowing, evaluating the performance of OneRestore in removing snow from images. | Used for snow removal, evaluating the capability of models to remove snowflakes and restore clear images from snowy scenes.",
          "citing_paper_id": "271039782",
          "cited_paper_id": 837707,
          "context_text": "…we aimed to address: the Naturalness Preserved Enhancement dataset (NPE) [67] for low-light enhancement, the RESIDE Real-world Task-driven Testing Set (RTTS) [34] for image dehazing, Yang’s dataset (RS) [79] for image deraining, and the real Snow100k dataset (Snow100k-R) [43] for image desnowing.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context explicitly mentions four datasets used for different image restoration tasks, which are directly relevant to the research topic.",
          "citing_paper_doi": "10.48550/arXiv.2407.04621",
          "cited_paper_doi": "10.1109/TIP.2018.2806202",
          "citing_paper_url": "https://www.semanticscholar.org/paper/7da6f70ecc09a2ac615d475d82e2a1d747428403",
          "cited_paper_url": "https://www.semanticscholar.org/paper/9e7487d7396acf298f972f0b8ab0397e0d9e7a0f",
          "citing_paper_year": 2024,
          "cited_paper_year": 2017
        },
        {
          "dataset_name": "Naturalness Preserved Enhancement dataset (NPE)",
          "dataset_description": "Used for rain removal, containing synthetic and real rainy images to train and test rain detection and removal methods. | Used for low-light image enhancement, focusing on improving illumination in underexposed images through illumination map estimation. | Used for snow removal, featuring realistic snowy images to evaluate the performance of snow removal techniques. | Used for image deraining, assessing the performance of rain removal techniques in various conditions. | Used for low-light enhancement, focusing on preserving naturalness in images while improving visibility. | Used for image dehazing, evaluating the effectiveness of dehazing algorithms in real-world scenarios. | Used for dehazing tasks, providing real-world images with varying degrees of haze to test and evaluate dehazing algorithms. | Used for image desnowing, testing the ability of models to remove snow from images in diverse settings.",
          "citing_paper_id": "271039782",
          "cited_paper_id": 15443600,
          "context_text": "…we aimed to address: the Naturalness Preserved Enhancement dataset (NPE) [67] for low-light enhancement, the RESIDE Real-world Task-driven Testing Set (RTTS) [34] for image dehazing, Yang’s dataset (RS) [79] for image deraining, and the real Snow100k dataset (Snow100k-R) [43] for image desnowing.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context explicitly mentions four datasets used for different image restoration tasks, which are directly relevant to the research topic.",
          "citing_paper_doi": "10.48550/arXiv.2407.04621",
          "cited_paper_doi": "10.1109/CVPR.2017.183",
          "citing_paper_url": "https://www.semanticscholar.org/paper/7da6f70ecc09a2ac615d475d82e2a1d747428403",
          "cited_paper_url": "https://www.semanticscholar.org/paper/6de27f256e97c014a95706c72af7fa651e8fa34a",
          "citing_paper_year": 2024,
          "cited_paper_year": 2016
        },
        {
          "dataset_name": "Naturalness Preserved Enhancement dataset (NPE)",
          "dataset_description": "Used for rain removal, containing synthetic and real rainy images to train and test rain detection and removal methods. | Used for low-light image enhancement, focusing on improving illumination in underexposed images through illumination map estimation. | Used for snow removal, featuring realistic snowy images to evaluate the performance of snow removal techniques. | Used for image deraining, assessing the performance of rain removal techniques in various conditions. | Used for low-light enhancement, focusing on preserving naturalness in images while improving visibility. | Used for image dehazing, evaluating the effectiveness of dehazing algorithms in real-world scenarios. | Used for dehazing tasks, providing real-world images with varying degrees of haze to test and evaluate dehazing algorithms. | Used for low-light enhancement, evaluating the performance of OneRestore in improving image quality under low-light conditions. | Used for single-image dehazing, assessing the ability of models to remove atmospheric haze and recover clear images from hazy scenes. | Used for low-light image enhancement, evaluating the effectiveness of models in improving visibility and color fidelity in dark conditions. | Used for deraining, testing the ability of OneRestore to remove rain streaks from images. | Used for image desnowing, testing the ability of models to remove snow from images in diverse settings. | Used for dehazing, assessing the effectiveness of OneRestore in removing haze from images. | Used for rain removal, testing the performance of models in eliminating rain streaks and restoring clean images from rainy scenes. | Used for desnowing, evaluating the performance of OneRestore in removing snow from images. | Used for snow removal, evaluating the capability of models to remove snowflakes and restore clear images from snowy scenes.",
          "citing_paper_id": "271039782",
          "cited_paper_id": 39760169,
          "context_text": "…we aimed to address: the Naturalness Preserved Enhancement dataset (NPE) [67] for low-light enhancement, the RESIDE Real-world Task-driven Testing Set (RTTS) [34] for image dehazing, Yang’s dataset (RS) [79] for image deraining, and the real Snow100k dataset (Snow100k-R) [43] for image desnowing.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context explicitly mentions four datasets used for different image restoration tasks, which are directly relevant to the research topic.",
          "citing_paper_doi": "10.48550/arXiv.2407.04621",
          "cited_paper_doi": "10.1109/TIP.2018.2867951",
          "citing_paper_url": "https://www.semanticscholar.org/paper/7da6f70ecc09a2ac615d475d82e2a1d747428403",
          "cited_paper_url": "https://www.semanticscholar.org/paper/abf75b81b96ff2cd262086675deba55f907108db",
          "citing_paper_year": 2024,
          "cited_paper_year": 2017
        }
      ]
    },
    {
      "cited_paper_id": "90260111",
      "citation_count": 0,
      "total_dataset_mentions": 3,
      "unique_datasets": [
        "DRealSR"
      ],
      "dataset_details": [
        {
          "dataset_name": "DRealSR",
          "dataset_description": "Used to evaluate the BFR task, focusing on face detection in real-world images. | Used to evaluate the BFR task, focusing on face recognition in real-world images. | Used to evaluate the BSR task, focusing on real-world image super-resolution performance. | Used for quantitative evaluation of image super-resolution methods, focusing on synthetic datasets to assess performance metrics. | Used for quantitative evaluation of image super-resolution methods, focusing on real-world datasets to assess performance metrics. | Used to evaluate the BSR task, focusing on synthetic image super-resolution performance. | Used to evaluate the BSR task, focusing on real-world image super-resolution performance with a custom dataset.",
          "citing_paper_id": "261276317",
          "cited_paper_id": 221041083,
          "context_text": "More Quantitative and Qualitative Comparisons for BSR on Synthetic Datasets The quantitative results on DRealSR [62] and RealSR [3] are presented in Table 10.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions two datasets, DRealSR and RealSR, which are used for quantitative comparisons in the context of image super-resolution.",
          "citing_paper_doi": "10.48550/arXiv.2308.15070",
          "cited_paper_doi": "10.1007/978-3-030-58598-3_7",
          "citing_paper_url": "https://www.semanticscholar.org/paper/858f0643110ccccb6a9103747f2169fecfb98668",
          "cited_paper_url": "https://www.semanticscholar.org/paper/45597186c691ce72f4bffba066d46f6a613b0a13",
          "citing_paper_year": 2023,
          "cited_paper_year": 2020
        },
        {
          "dataset_name": "DRealSR",
          "dataset_description": "Applied to assess the performance of real-world image super-resolution techniques, emphasizing natural and realistic image enhancement. | Used to evaluate image super-resolution methods, focusing on real-world city images with diverse lighting and weather conditions. | Used for real-world image super-resolution, focusing on the largest dataset with around 800 image pairs for each scale factor, enhancing the resolution of images in practical scenarios. | Utilized to benchmark image super-resolution algorithms, specifically targeting real-world images with complex textures and details.",
          "citing_paper_id": "235755417",
          "cited_paper_id": 221041083,
          "context_text": "Among them, DRealSR is the largest one with around 800 image pairs for each scale factor.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions 'DRealSR' as a dataset with a specific characteristic (largest with around 800 image pairs for each scale factor). It is used in the context of real-world image super-resolution.",
          "citing_paper_doi": "10.1109/TPAMI.2022.3203009",
          "cited_paper_doi": "10.1007/978-3-030-58598-3_7",
          "citing_paper_url": "https://www.semanticscholar.org/paper/0b8a64c8145d95e0fc0bd8f6eb54b5da7aca4fb7",
          "cited_paper_url": "https://www.semanticscholar.org/paper/45597186c691ce72f4bffba066d46f6a613b0a13",
          "citing_paper_year": 2021,
          "cited_paper_year": 2020
        },
        {
          "dataset_name": "DRealSR",
          "dataset_description": "Used for training single image super-resolution models, providing high-quality synthetic images for algorithm development. | Used for evaluating single image super-resolution models, focusing on real-world images captured with DSLR cameras and different lenses. | Used for evaluating the performance of trained models on real-world images, assessing their generalization to unseen data.",
          "citing_paper_id": "261031011",
          "cited_paper_id": 90260111,
          "context_text": "For evaluation, the testing data is composed of RealSR [268] and DRealSR [269], obtained by two DSLR cameras with different lenses.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions 'RealSR' and 'DRealSR' as specific datasets used for evaluation. These are multi-word proper nouns that appear to be specific datasets.",
          "citing_paper_doi": "10.48550/arXiv.2308.09388",
          "cited_paper_doi": "10.1109/ICCV.2019.00318",
          "citing_paper_url": "https://www.semanticscholar.org/paper/70ba95384664ab7becfd8196d20a46fe66112c91",
          "cited_paper_url": "https://www.semanticscholar.org/paper/a646c1e52c081a266638785134d3e6dc3a3e7068",
          "citing_paper_year": 2023,
          "cited_paper_year": 2019
        }
      ]
    },
    {
      "cited_paper_id": "210839013",
      "citation_count": 0,
      "total_dataset_mentions": 3,
      "unique_datasets": [
        "LOL-Blur"
      ],
      "dataset_details": [
        {
          "dataset_name": "LOL-Blur",
          "dataset_description": "Used for training deblurring models, focusing on enhancing low-light images with improved perceptual quality and fidelity.",
          "citing_paper_id": "246634936",
          "cited_paper_id": 219978876,
          "context_text": "Methods Enhancement → Deblurring Deblurring → Enhancement Training on LOL-Blur Zero-DCE [9] RUAS [22] Chen [5] DeblurGAN-v2† [16] MIMO [7] KinD++∗ DRBN∗ DeblurGAN-v2∗ DMPHN∗ MIMO∗ Ours → MIMO [7] → MIMO [7] → Zero-DCE [9] → Zero-DCE [9] → Zero-DCE [9] [53] [43] [16] [48] [7] PSNR↑ 17.",
          "confidence_score": 0.7,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions 'LOL-Blur' which is a plausible dataset name for low-light image enhancement. However, there is no explicit mention of how it is used or its specific characteristics.",
          "citing_paper_doi": "10.1007/978-3-031-20068-7_33",
          "cited_paper_doi": "10.1109/cvpr42600.2020.00313",
          "citing_paper_url": "https://www.semanticscholar.org/paper/be3b1b0a2eeddba876580da707105820782672f1",
          "cited_paper_url": "https://www.semanticscholar.org/paper/f478678516593b296f7e55aecae34a2b059460e6",
          "citing_paper_year": 2022,
          "cited_paper_year": 2020
        },
        {
          "dataset_name": "LOL-Blur",
          "dataset_description": "Used for training models in low-light image enhancement and deblurring tasks, focusing on improving image quality under low-light conditions.",
          "citing_paper_id": "246634936",
          "cited_paper_id": 210839013,
          "context_text": "Methods Enhancement→ Deblurring Deblurring→ Enhancement Training on LOL-Blur Zero-DCE [8] RUAS [20] Chen [4] DeblurGAN-v2† [14] MIMO [6] DRBN∗ DeblurGAN-v2∗ DMPHN∗ MIMO∗ Ours →MIMO [6] →MIMO [6] → Zero-DCE [8] → Zero-DCE [8] → Zero-DCE [8] [39] [14] [44] [6]",
          "confidence_score": 0.9,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The citation context mentions 'LOL-Blur' which appears to be a dataset used for training in low-light image enhancement and deblurring tasks. No other specific datasets are mentioned.",
          "citing_paper_doi": "10.1007/978-3-031-20068-7_33",
          "cited_paper_doi": "10.1109/CVPR42600.2020.00185",
          "citing_paper_url": "https://www.semanticscholar.org/paper/be3b1b0a2eeddba876580da707105820782672f1",
          "cited_paper_url": "https://www.semanticscholar.org/paper/a2a784f1f24bd400b9ef69f7e0cf7071530d4866",
          "citing_paper_year": 2022,
          "cited_paper_year": 2020
        },
        {
          "dataset_name": "LOL-Blur",
          "dataset_description": "Used to train and evaluate models for low-light and blurred image restoration, focusing on generating realistic synthetic data to enhance model performance.",
          "citing_paper_id": "246634936",
          "cited_paper_id": 244527344,
          "context_text": "In this work, inspired by the big success of data synthesis in the real-world super-resolution tasks [39,49,20,1], we introduce a synthesis pipeline that models low-light blur degradation jointly, hence allowing us to generate a large-scale dataset (LOL-Blur).",
          "confidence_score": 1.0,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions the creation of a large-scale dataset called LOL-Blur, which is relevant to the research topic of image restoration, particularly for low-light and blurred images.",
          "citing_paper_doi": "10.1007/978-3-031-20068-7_33",
          "cited_paper_doi": "10.1109/CVPR52688.2022.00587",
          "citing_paper_url": "https://www.semanticscholar.org/paper/be3b1b0a2eeddba876580da707105820782672f1",
          "cited_paper_url": "https://www.semanticscholar.org/paper/66c7ef327bda37f8cd33bd66fd0f89a78ef7a932",
          "citing_paper_year": 2022,
          "cited_paper_year": 2021
        }
      ]
    },
    {
      "cited_paper_id": "64193",
      "citation_count": 0,
      "total_dataset_mentions": 3,
      "unique_datasets": [
        "BSD500"
      ],
      "dataset_details": [
        {
          "dataset_name": "BSD500",
          "dataset_description": "Used to evaluate image restoration methods, specifically focusing on the performance of algorithms on natural images. The dataset consists of 100 high-quality images. | Used to provide real noisy images, with cropped patches of 512 × 512 pixels for image restoration. | Used to generate noisy synthetic images, contributing to the training set of 4k images for image restoration.",
          "citing_paper_id": "222103869",
          "cited_paper_id": 64193,
          "context_text": "To generate noisy synthetic images, we employ BSD500 [75], DIV2K [76], and MIT-Adobe FiveK [77], resulting in 4k images, while, for real noisy images, we use cropped patches of 512 × 512 from SIDD [78], Poly [79], and RENOIR [80].",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions specific datasets used for generating noisy synthetic images and real noisy images, which are relevant to the topic of image restoration.",
          "citing_paper_doi": "10.1109/TNNLS.2021.3131739",
          "cited_paper_doi": "10.1109/ICCV.2001.937655",
          "citing_paper_url": "https://www.semanticscholar.org/paper/3d18527d24ff382df0b7fb155d4ce456569d704f",
          "cited_paper_url": "https://www.semanticscholar.org/paper/9a1ed876196ec9733acb1daa6d65e35ff0414291",
          "citing_paper_year": 2020,
          "cited_paper_year": 2001
        },
        {
          "dataset_name": "BSD500",
          "dataset_description": "Used to provide real noisy images, with cropped patches of 512 × 512 pixels for image restoration. | Used to evaluate denoising algorithms on real-world noisy images from smartphone cameras, emphasizing realistic noise patterns and image quality. | Recently introduced for denoising images from smartphone cameras, used to enhance image quality and reduce noise in low-light conditions. | Used to generate noisy synthetic images, contributing to the training set of 4k images for image restoration. | Used to test denoising algorithms on real-world noisy images from smartphone cameras, focusing on high-quality denoising performance. | Used to evaluate denoising algorithms for smartphone cameras, focusing on high-quality image restoration and noise reduction techniques.",
          "citing_paper_id": "222103869",
          "cited_paper_id": 52059988,
          "context_text": "To generate noisy synthetic images, we employ BSD500 [75], DIV2K [76], and MIT-Adobe FiveK [77], resulting in 4k images, while, for real noisy images, we use cropped patches of 512 × 512 from SIDD [78], Poly [79], and RENOIR [80].",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions specific datasets used for generating noisy synthetic images and real noisy images, which are relevant to the topic of image restoration.",
          "citing_paper_doi": "10.1109/TNNLS.2021.3131739",
          "cited_paper_doi": "10.1109/CVPR.2018.00182",
          "citing_paper_url": "https://www.semanticscholar.org/paper/3d18527d24ff382df0b7fb155d4ce456569d704f",
          "cited_paper_url": "https://www.semanticscholar.org/paper/ebf35073e122782f685a0d6c231622412f28a53b",
          "citing_paper_year": 2020,
          "cited_paper_year": 2018
        },
        {
          "dataset_name": "BSD500",
          "dataset_description": "Used to provide real noisy images, with cropped patches of 512 × 512 pixels for image restoration. | Used to generate noisy synthetic images, contributing to the training set of 4k images for image restoration.",
          "citing_paper_id": "222103869",
          "cited_paper_id": 207778473,
          "context_text": "To generate noisy synthetic images, we employ BSD500 [75], DIV2K [76], and MIT-Adobe FiveK [77], resulting in 4k images, while, for real noisy images, we use cropped patches of 512 × 512 from SIDD [78], Poly [79], and RENOIR [80].",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions specific datasets used for generating noisy synthetic images and real noisy images, which are relevant to the topic of image restoration.",
          "citing_paper_doi": "10.1109/TNNLS.2021.3131739",
          "cited_paper_doi": null,
          "citing_paper_url": "https://www.semanticscholar.org/paper/3d18527d24ff382df0b7fb155d4ce456569d704f",
          "cited_paper_url": "https://www.semanticscholar.org/paper/5ee57b18a7be2b671ffc6f5ce5dbbe92112ee0fb",
          "citing_paper_year": 2020,
          "cited_paper_year": 2007
        }
      ]
    },
    {
      "cited_paper_id": "102351795",
      "citation_count": 0,
      "total_dataset_mentions": 3,
      "unique_datasets": [
        "Set6"
      ],
      "dataset_details": [
        {
          "dataset_name": "Set6",
          "dataset_description": "Used to evaluate the effectiveness of the proposed DPIR method, comparing it with four representative methods and summarizing PSNR results.",
          "citing_paper_id": "221377171",
          "cited_paper_id": 1726588,
          "context_text": "To evaluate the effectiveness of the proposed DPIR, we choose four representative methods for comparison, including model-based method EPLL [4], learning-based non-blind method FDN [77], learning-based blind method DMPHN [78] and plug-and-play method IRCNN and IR-CNN+. Table 6 summarizes the PSNR results on Set6.",
          "confidence_score": 1.0,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions 'Set6' as a dataset used to evaluate the effectiveness of the proposed DPIR method. No other datasets are explicitly named.",
          "citing_paper_doi": "10.1109/TPAMI.2021.3088914",
          "cited_paper_doi": "10.1109/ICCV.2011.6126278",
          "citing_paper_url": "https://www.semanticscholar.org/paper/d467d747dde65862d9640fe49f69d6b6d46d69ea",
          "cited_paper_url": "https://www.semanticscholar.org/paper/529403ab43b381b942b67751862b614cbd94341b",
          "citing_paper_year": 2020,
          "cited_paper_year": 2011
        },
        {
          "dataset_name": "Set6",
          "dataset_description": "Used to evaluate the effectiveness of the proposed DPIR method, comparing it with four representative methods and summarizing PSNR results.",
          "citing_paper_id": "221377171",
          "cited_paper_id": 102351795,
          "context_text": "To evaluate the effectiveness of the proposed DPIR, we choose four representative methods for comparison, including model-based method EPLL [4], learning-based non-blind method FDN [77], learning-based blind method DMPHN [78] and plug-and-play method IRCNN and IR-CNN+. Table 6 summarizes the PSNR results on Set6.",
          "confidence_score": 1.0,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions 'Set6' as a dataset used to evaluate the effectiveness of the proposed DPIR method. No other datasets are explicitly named.",
          "citing_paper_doi": "10.1109/TPAMI.2021.3088914",
          "cited_paper_doi": "10.1109/CVPR.2019.00613",
          "citing_paper_url": "https://www.semanticscholar.org/paper/d467d747dde65862d9640fe49f69d6b6d46d69ea",
          "cited_paper_url": "https://www.semanticscholar.org/paper/a5c1156c2c8185df4581cf139df05aab66b3bb22",
          "citing_paper_year": 2020,
          "cited_paper_year": 2019
        },
        {
          "dataset_name": "Set6",
          "dataset_description": "Used to evaluate the effectiveness of various image restoration methods, specifically comparing PSNR results to assess performance.",
          "citing_paper_id": "221377171",
          "cited_paper_id": 7028097,
          "context_text": "…effectiveness of the proposed DPIR, we choose four representative methods for comparison, including model-based method EPLL [4], learning-based non-blind method FDN [77], learning-based blind method DMPHN [78] and plug-and-play method IRCNN and IR-CNN+. Table 6 summarizes the PSNR results on Set6.",
          "confidence_score": 0.8,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions 'Set6' which appears to be a dataset used for evaluating image restoration methods. However, there is no additional information about the dataset's characteristics or specific usage beyond the PSNR results.",
          "citing_paper_doi": "10.1109/TPAMI.2021.3088914",
          "cited_paper_doi": "10.1109/ICCV.2017.491",
          "citing_paper_url": "https://www.semanticscholar.org/paper/d467d747dde65862d9640fe49f69d6b6d46d69ea",
          "cited_paper_url": "https://www.semanticscholar.org/paper/09d6a79dfc5713b080947ce4318c6be1f11864c8",
          "citing_paper_year": 2020,
          "cited_paper_year": 2017
        }
      ]
    },
    {
      "cited_paper_id": "251040466",
      "citation_count": 0,
      "total_dataset_mentions": 3,
      "unique_datasets": [
        "RealIR"
      ],
      "dataset_details": [
        {
          "dataset_name": "RealIR",
          "dataset_description": "Introduced to address limitations in current image restoration benchmarks, RealIR offers a broader variety of imaging devices and content diversity for evaluating restoration algorithms.",
          "citing_paper_id": "274437167",
          "cited_paper_id": 221041083,
          "context_text": "Finally, given the limitations of current real-world image restoration benchmarks [5, 60], such as the restricted variety of imaging devices, and narrow content diversity, we introduce a new benchmark called RealIR .",
          "confidence_score": 0.9,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions the limitations of existing benchmarks and introduces a new benchmark called RealIR, which is relevant to the topic of all-in-one image restoration.",
          "citing_paper_doi": "10.48550/arXiv.2412.00878",
          "cited_paper_doi": "10.1007/978-3-030-58598-3_7",
          "citing_paper_url": "https://www.semanticscholar.org/paper/5ca2a7d55aac655fa060bb1383b281d5ea2d102e",
          "cited_paper_url": "https://www.semanticscholar.org/paper/45597186c691ce72f4bffba066d46f6a613b0a13",
          "citing_paper_year": 2024,
          "cited_paper_year": 2020
        },
        {
          "dataset_name": "RealIR",
          "dataset_description": "Used as a test set without ground truth to evaluate image restoration methods using non-reference metrics aligned with human perception.",
          "citing_paper_id": "274437167",
          "cited_paper_id": 237048383,
          "context_text": "For test sets without ground truth, such as RealIR, we use non-reference evaluation metrics aligned with human perception, including MUSIQ [19], MANIQA [65], LIQE [73], NIQE [71], and CLIP-IQA [53].",
          "confidence_score": 0.9,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions 'RealIR' as a test set without ground truth, which fits the criteria for a dataset. However, the other names mentioned (MUSIQ, MANIQA, LIQE, NIQE, CLIP-IQA) are non-reference evaluation metrics, not datasets.",
          "citing_paper_doi": "10.48550/arXiv.2412.00878",
          "cited_paper_doi": "10.1109/ICCV48922.2021.00510",
          "citing_paper_url": "https://www.semanticscholar.org/paper/5ca2a7d55aac655fa060bb1383b281d5ea2d102e",
          "cited_paper_url": "https://www.semanticscholar.org/paper/e3d06054af531ee2f42270d43100b309c28546ef",
          "citing_paper_year": 2024,
          "cited_paper_year": 2021
        },
        {
          "dataset_name": "RealIR",
          "dataset_description": "Used as a test set without ground truth to evaluate image restoration methods using non-reference metrics aligned with human perception.",
          "citing_paper_id": "274437167",
          "cited_paper_id": 251040466,
          "context_text": "For test sets without ground truth, such as RealIR, we use non-reference evaluation metrics aligned with human perception, including MUSIQ [19], MANIQA [65], LIQE [73], NIQE [71], and CLIP-IQA [53].",
          "confidence_score": 0.9,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions 'RealIR' as a test set without ground truth, which fits the criteria for a dataset. However, the other names mentioned (MUSIQ, MANIQA, LIQE, NIQE, CLIP-IQA) are non-reference evaluation metrics, not datasets.",
          "citing_paper_doi": "10.48550/arXiv.2412.00878",
          "cited_paper_doi": "10.48550/arXiv.2207.12396",
          "citing_paper_url": "https://www.semanticscholar.org/paper/5ca2a7d55aac655fa060bb1383b281d5ea2d102e",
          "cited_paper_url": "https://www.semanticscholar.org/paper/03ae89e796b1a8b566ae1554fab65c8c88b3a55f",
          "citing_paper_year": 2024,
          "cited_paper_year": 2022
        }
      ]
    },
    {
      "cited_paper_id": "4710407",
      "citation_count": 0,
      "total_dataset_mentions": 3,
      "unique_datasets": [
        "WED"
      ],
      "dataset_details": [
        {
          "dataset_name": "WED",
          "dataset_description": "Used for training the network to handle various image degradations, focusing on robustness across different corruption types. | Used for testing the network's performance on synthetic noisy images with varying noise levels, evaluating restoration accuracy.",
          "citing_paper_id": "260680793",
          "cited_paper_id": 250551851,
          "context_text": "Moreover, to measure the network’s performance on images with different degradation levels, we follow [16] to train our network on WED [24] and test it on synthetic noisy images with different noise levels from CBSD68 [26].",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions two specific datasets, WED and CBSD68, which are used for training and testing the network, respectively.",
          "citing_paper_doi": "10.1145/3581783.3611825",
          "cited_paper_doi": "10.1109/CVPR52688.2022.01693",
          "citing_paper_url": "https://www.semanticscholar.org/paper/b764f1db97572275dfd7b7f106b190d06a7c9fa0",
          "cited_paper_url": "https://www.semanticscholar.org/paper/c85f67a49cfdc29a77bd741a59e739cdc814edde",
          "citing_paper_year": 2023,
          "cited_paper_year": 2022
        },
        {
          "dataset_name": "WED",
          "dataset_description": "Used for evaluating image restoration, particularly for diverse real-world images, to test generalization and robustness. | Used for evaluating image restoration, focusing on specific image degradation types to assess algorithm performance. | Used for evaluating image restoration techniques, focusing on high-resolution face images to assess texture recovery and realism. | Used for evaluating high-resolution image restoration, focusing on diverse content to test scalability and quality. | Used for evaluating curve text detection in wild images, assessing the ability to restore and recognize curved text.",
          "citing_paper_id": "260843368",
          "cited_paper_id": 4710407,
          "context_text": "A couple of other datasets are also often used for image restoration tasks including FFHQ [34], WED [55], OST [76], SCUT-CTW1500 [85], and DIV8K [26].",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions several datasets used for image restoration tasks, which align with the research topic of All-in-One Image Restoration.",
          "citing_paper_doi": "10.1109/CVPRW59228.2023.00178",
          "cited_paper_doi": "10.1109/CVPR.2018.00070",
          "citing_paper_url": "https://www.semanticscholar.org/paper/851446ea3c70c918cdf8c62db0dd0b1900806e5f",
          "cited_paper_url": "https://www.semanticscholar.org/paper/9e788f1530af08a1f2140e6016fd4aeaa8b29033",
          "citing_paper_year": 2023,
          "cited_paper_year": 2018
        },
        {
          "dataset_name": "WED",
          "dataset_description": "Used to test rain removal algorithms, focusing on heavy rain conditions. | Used to evaluate high-quality image restoration methods. | Used to test image deblurring methods, particularly for motion blur in video frames. | Used to assess the effectiveness of image restoration methods, particularly in handling various degradations. | Used to evaluate rain removal techniques, focusing on heavy rain conditions. | Used to evaluate image deraining techniques, focusing on synthetic outdoor rain scenarios. | Used to evaluate low-light image enhancement techniques, focusing on improving visibility in dark conditions. | Used to evaluate text-prompted image restoration methods. | Used to test image restoration methods, emphasizing urban scenes with rain, noise, and blur degradations. | Used to assess image deraining methods, specifically for outdoor synthetic rain scenarios. | Used to evaluate instruction-based image restoration methods. | Used to test adaptive image restoration techniques, focusing on frequency mining and modulation. | Used to assess dynamic image restoration techniques. | Used to assess image restoration techniques, particularly in handling medical images. | Used to assess noise reduction techniques in image restoration. | Used to evaluate image restoration techniques, emphasizing high-resolution urban scene restoration. | Used to assess image restoration performance, particularly under rain, noise, and blur degradations. | Used to test domain-adaptive image restoration methods. | Used to assess image restoration techniques, particularly in handling aerial images. | Used to evaluate image restoration techniques, focusing on denoising and deblurring performance. | Used to evaluate all-in-one image restoration techniques, focusing on versatility. | Used to evaluate dehazing algorithms, focusing on outdoor synthetic scenes. | Used to evaluate dehazing algorithms, focusing on outdoor real-world scenes. | Used to evaluate image restoration methods, particularly in handling underwater scenes. | Used to evaluate image restoration techniques, focusing on various degradations including rain, haze, noise, blur, and low-light conditions. | Used to evaluate image restoration methods, focusing on prompt-based approaches. | Used to test perceptual quality in image restoration. | Used to test image restoration techniques, particularly in handling multiple degradations. | Used to evaluate image restoration techniques, focusing on combined degradations including rain, haze, noise, blur, and darkness.",
          "citing_paper_id": "273502246",
          "cited_paper_id": 268553835,
          "context_text": "…5 Rain, Haze, Noise-σ 25 , Blur, Low-Light BSD400, WED, Urban100, Rain100L, RESIDE-OTS, RESIDE-SOTS, Gopro, LOL IDR [66], AdaIR [78], InstructIR [62], PIP [72], DaAIR [187], AnyIR [188], MEASNet [110], HAIR [189], Perceive-IR [111] rain-noise-blur-dark[61] 4 Rain, Noise, Blur,…",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions multiple datasets and methods, but only datasets are extracted. The datasets are used for evaluating image restoration techniques under various degradations.",
          "citing_paper_doi": "10.48550/arXiv.2410.15067",
          "cited_paper_doi": "10.48550/arXiv.2403.14614",
          "citing_paper_url": "https://www.semanticscholar.org/paper/f5ff17162a4936db3f64ed7bafae13abcfa63a27",
          "cited_paper_url": "https://www.semanticscholar.org/paper/8cf850abfec7ce894b498ef082282382e8d3374c",
          "citing_paper_year": 2024,
          "cited_paper_year": 2024
        }
      ]
    },
    {
      "cited_paper_id": "251040466",
      "citation_count": 0,
      "total_dataset_mentions": 3,
      "unique_datasets": [
        "RealPhoto60"
      ],
      "dataset_details": [
        {
          "dataset_name": "RealPhoto60",
          "dataset_description": "Used as a real-world test-set to evaluate the influence of semantic and restoration prompts on image quality using non-reference metrics.",
          "citing_paper_id": "266362278",
          "cited_paper_id": 237048383,
          "context_text": "We choose RealPhoto60 [78] as our real-world test-set, and use non-reference metrics CLIP-IQA [66], MUSIQ [27] and MANIQA [76] scores to study the influence of the semantic and restoration prompts.",
          "confidence_score": 1.0,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "RealPhoto60 is identified as a specific dataset used for testing. The other names (CLIP-IQA, MUSIQ, MANIQA) are methods or metrics, not datasets.",
          "citing_paper_doi": "10.1007/978-3-031-73661-2_25",
          "cited_paper_doi": "10.1109/ICCV48922.2021.00510",
          "citing_paper_url": "https://www.semanticscholar.org/paper/8dc8ae93e9fe95de8c816a81bc087031d689bdcc",
          "cited_paper_url": "https://www.semanticscholar.org/paper/e3d06054af531ee2f42270d43100b309c28546ef",
          "citing_paper_year": 2023,
          "cited_paper_year": 2021
        },
        {
          "dataset_name": "RealPhoto60",
          "dataset_description": "Used as a real-world test-set to evaluate the influence of semantic and restoration prompts on image quality using non-reference metrics.",
          "citing_paper_id": "266362278",
          "cited_paper_id": 248240148,
          "context_text": "We choose RealPhoto60 [78] as our real-world test-set, and use non-reference metrics CLIP-IQA [66], MUSIQ [27] and MANIQA [76] scores to study the influence of the semantic and restoration prompts.",
          "confidence_score": 1.0,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "RealPhoto60 is identified as a specific dataset used for testing. The other names (CLIP-IQA, MUSIQ, MANIQA) are methods or metrics, not datasets.",
          "citing_paper_doi": "10.1007/978-3-031-73661-2_25",
          "cited_paper_doi": "10.1109/CVPRW56347.2022.00126",
          "citing_paper_url": "https://www.semanticscholar.org/paper/8dc8ae93e9fe95de8c816a81bc087031d689bdcc",
          "cited_paper_url": "https://www.semanticscholar.org/paper/d2171803c7aae785f05e9f298f6025f4252262c6",
          "citing_paper_year": 2023,
          "cited_paper_year": 2022
        },
        {
          "dataset_name": "RealPhoto60",
          "dataset_description": "Used as a real-world test-set to evaluate the influence of semantic and restoration prompts on image quality using non-reference metrics.",
          "citing_paper_id": "266362278",
          "cited_paper_id": 251040466,
          "context_text": "We choose RealPhoto60 [78] as our real-world test-set, and use non-reference metrics CLIP-IQA [66], MUSIQ [27] and MANIQA [76] scores to study the influence of the semantic and restoration prompts.",
          "confidence_score": 1.0,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "RealPhoto60 is identified as a specific dataset used for testing. The other names (CLIP-IQA, MUSIQ, MANIQA) are methods or metrics, not datasets.",
          "citing_paper_doi": "10.1007/978-3-031-73661-2_25",
          "cited_paper_doi": "10.48550/arXiv.2207.12396",
          "citing_paper_url": "https://www.semanticscholar.org/paper/8dc8ae93e9fe95de8c816a81bc087031d689bdcc",
          "cited_paper_url": "https://www.semanticscholar.org/paper/03ae89e796b1a8b566ae1554fab65c8c88b3a55f",
          "citing_paper_year": 2023,
          "cited_paper_year": 2022
        }
      ]
    },
    {
      "cited_paper_id": "9715523",
      "citation_count": 0,
      "total_dataset_mentions": 2,
      "unique_datasets": [
        "DND"
      ],
      "dataset_details": [
        {
          "dataset_name": "DND",
          "dataset_description": "Used for real image denoising, focusing on noise characteristics in smartphone cameras, employing a high-quality dataset for training and evaluation. | Used for denoising experiments with real-world datasets, evaluating denoising algorithms on smartphone camera images. | Used for denoising experiments with real-world datasets, evaluating denoising algorithms on diverse real-world images. | Used for denoising experiments with synthetic benchmark datasets, evaluating denoising algorithms under controlled conditions. | Used for real image denoising, providing a benchmark for evaluating denoising algorithms on real photographs, emphasizing practical performance.",
          "citing_paper_id": "244346144",
          "cited_paper_id": 52059988,
          "context_text": "We perform denoising experiments on synthetic benchmark datasets generated with additive white Gaussian noise (Set12 [101], BSD68 [52], Urban100 [29], Kodak24 [20] and McMaster [104]) as well as on real-world datasets (SIDD [1] and DND [60]).",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions several datasets used for denoising experiments, both synthetic and real-world. These datasets are clearly identified and used for evaluating denoising algorithms.",
          "citing_paper_doi": "10.1109/CVPR52688.2022.00564",
          "cited_paper_doi": "10.1109/CVPR.2018.00182",
          "citing_paper_url": "https://www.semanticscholar.org/paper/1e88d5afe19aea324d33541f60a90b7036894c32",
          "cited_paper_url": "https://www.semanticscholar.org/paper/ebf35073e122782f685a0d6c231622412f28a53b",
          "citing_paper_year": 2021,
          "cited_paper_year": 2018
        },
        {
          "dataset_name": "DND",
          "dataset_description": "Used for training the network on real noisy photographs, focusing on improving denoising performance. | Used to benchmark denoising algorithms with real photographs, specifically evaluating performance on 50 images captured using four consumer cameras. | Used to benchmark denoising algorithms with real photographs, focusing on noise reduction in diverse imaging scenarios and evaluating algorithm robustness. | Used for training the denoising network, focusing on real-world image denoising challenges and performance. | Used for evaluating the trained network, assessing its effectiveness on diverse real-world noisy images. | Used to benchmark denoising algorithms with real photographs, focusing on performance metrics such as PSNR and SSIM. | Used for denoising comparisons, focusing on real-world photographs to evaluate denoising algorithms' performance. | Used to benchmark denoising algorithms with real photographs, focusing on noise reduction in low-light conditions and evaluating algorithm performance. | Used for evaluating the trained network on real noisy photographs, assessing the effectiveness of the denoising algorithm.",
          "citing_paper_id": "212725053",
          "cited_paper_id": 9715523,
          "context_text": "(1) DND [115] consists of 50 images captured with four consumer cameras.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions 'DND' which is a specific dataset name, and it describes the composition of the dataset.",
          "citing_paper_doi": "10.1109/TPAMI.2022.3167175",
          "cited_paper_doi": "10.1109/CVPR.2017.294",
          "citing_paper_url": "https://www.semanticscholar.org/paper/bb3c793a5a6d3be2454d22a76ada5b6a94ec42f5",
          "cited_paper_url": "https://www.semanticscholar.org/paper/d00d9e427d1ceb81164ab6487462127c337b1b0a",
          "citing_paper_year": 2022,
          "cited_paper_year": 2017
        }
      ]
    },
    {
      "cited_paper_id": "39760169",
      "citation_count": 0,
      "total_dataset_mentions": 2,
      "unique_datasets": [
        "real-world dehazing dataset"
      ],
      "dataset_details": [
        {
          "dataset_name": "real-world dehazing dataset",
          "dataset_description": "Used to train and evaluate image restoration models, focusing on removing rain artifacts from real-world videos. | Used for dehazing tasks, specifically to train and evaluate dehazing algorithms on real-world images, focusing on improving image clarity and visual quality. | Used to provide visual samples for real-world dehazing, specifically illustrating the quality of dehazing methods in the research context. | Used to train and evaluate image restoration models, focusing on removing snow artifacts from real-world images.",
          "citing_paper_id": "259144110",
          "cited_paper_id": 235719890,
          "context_text": "(a) and (b) illustrate the visual samples of the real-world dehazing dataset [99] and our proposed RealSnow, respectively.",
          "confidence_score": 0.9,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions 'real-world dehazing dataset' which is likely a specific dataset used for dehazing images. The cited paper title confirms it is a dataset.",
          "citing_paper_doi": "10.1109/CVPR52729.2023.02083",
          "cited_paper_doi": "10.1109/CVPR46437.2021.00912",
          "citing_paper_url": "https://www.semanticscholar.org/paper/25ac8b26688c0a2349c72337737e0252a0f8051e",
          "cited_paper_url": "https://www.semanticscholar.org/paper/cf29f0e9f66174ad274c6f6ef587c2f24c162358",
          "citing_paper_year": 2023,
          "cited_paper_year": 2021
        },
        {
          "dataset_name": "real-world dehazing dataset",
          "dataset_description": "Used to evaluate de-snowing algorithms, focusing on real-world images to assess performance in practical scenarios. | Used to evaluate de-hazing algorithms, focusing on real-world images to assess performance in practical scenarios.",
          "citing_paper_id": "274437167",
          "cited_paper_id": 39760169,
          "context_text": "The inputs are sourced from the real-world de-hazing dataset [24] and the real-world de-snowing dataset [41].",
          "confidence_score": 0.9,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions two specific datasets used for de-hazing and de-snowing, which are directly relevant to the topic of image restoration.",
          "citing_paper_doi": "10.48550/arXiv.2412.00878",
          "cited_paper_doi": "10.1109/TIP.2018.2867951",
          "citing_paper_url": "https://www.semanticscholar.org/paper/5ca2a7d55aac655fa060bb1383b281d5ea2d102e",
          "cited_paper_url": "https://www.semanticscholar.org/paper/abf75b81b96ff2cd262086675deba55f907108db",
          "citing_paper_year": 2024,
          "cited_paper_year": 2017
        }
      ]
    },
    {
      "cited_paper_id": "199543931",
      "citation_count": 0,
      "total_dataset_mentions": 2,
      "unique_datasets": [
        "RealBlur"
      ],
      "dataset_details": [
        {
          "dataset_name": "RealBlur",
          "dataset_description": "Used for training deblurring models, focusing on enhancing low-light images with improved perceptual quality and fidelity. | Used to train the DeblurGAN-v2 model for image deblurring, focusing on improving deblurring performance and speed in the context of all-in-one image restoration. | Used to retrain the network for improved low-light deblurring, enhancing the model's ability to handle low-light conditions. | Used to train the DeblurGAN-v2 model for image deblurring, focusing on improving performance on real-world blurred images. | Used to train DeblurGAN-v2 for image deblurring, focusing on real-world blur scenarios to improve deblurring performance. | Used to test the generalizability of the proposed method on real-world night blurry images, focusing on performance in challenging conditions.",
          "citing_paper_id": "246634936",
          "cited_paper_id": 199543931,
          "context_text": "The symbol ‘ † ’ indicates that we use DeblurGAN-v2 trained on RealBlur dataset, and ‘ ∗ ’ indicates the network is retrained on our LOL-Blur dataset.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions two datasets: 'RealBlur' and 'LOL-Blur'. Both are used for training deblurring networks, with 'RealBlur' being used for the original DeblurGAN-v2 and 'LOL-Blur' for retraining.",
          "citing_paper_doi": "10.1007/978-3-031-20068-7_33",
          "cited_paper_doi": "10.1109/ICCV.2019.00897",
          "citing_paper_url": "https://www.semanticscholar.org/paper/be3b1b0a2eeddba876580da707105820782672f1",
          "cited_paper_url": "https://www.semanticscholar.org/paper/81c1dc00b32d12edfab00656593f5db56cfa79e0",
          "citing_paper_year": 2022,
          "cited_paper_year": 2019
        },
        {
          "dataset_name": "RealBlur",
          "dataset_description": "Used to test BANet on 2,025 pairs of HD images, all for testing, to assess deblurring effectiveness on high-resolution images. | Used to train deblurring algorithms, specifically focusing on real-world blur scenarios to improve image restoration performance. | Used to evaluate BANet on 3,214 pairs of blurred and sharp images, with 2,103 pairs for training and the rest for testing, focusing on deblurring performance. | Used to train and test BANet on 3,758 pairs for training and 980 pairs for testing, focusing on real-world deblurring scenarios. | Used to evaluate deblurring algorithms, specifically comparing DeblurGAN-v2, SRN, MPRNet, MIMO-UNet+, and the proposed method on real-world blurred images. | Used to evaluate deblurring algorithms, specifically comparing performance of DeblurGAN-v2, SRN, MPRNet, MIMO-UNet+, and the proposed method on real-world blurred images.",
          "citing_paper_id": "231639270",
          "cited_paper_id": 222104551,
          "context_text": "13 demonstrate deblurred results using DeblurGAN-v2 [16], SRN [32], MPRNet [42], MIMO-UNet+ [5], and ours, on the RealBlur [28] test set.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions the RealBlur dataset, which is used for testing deblurring algorithms. The dataset is clearly identified and relevant to the research topic of image restoration.",
          "citing_paper_doi": "10.1109/TIP.2022.3216216",
          "cited_paper_doi": "10.22677/THESIS.200000333386",
          "citing_paper_url": "https://www.semanticscholar.org/paper/010e74e834c12e840e603b53ad14cbfc454ce7a1",
          "cited_paper_url": "https://www.semanticscholar.org/paper/2d6c14023087b5d5bd90a88da13e0fa765418d84",
          "citing_paper_year": 2021,
          "cited_paper_year": 2020
        }
      ]
    },
    {
      "cited_paper_id": "235702784",
      "citation_count": 0,
      "total_dataset_mentions": 2,
      "unique_datasets": [
        "RainDS"
      ],
      "dataset_details": [
        {
          "dataset_name": "RainDS",
          "dataset_description": "Used to provide real-world test cases for raindrop removal, consisting of 97 real test images to evaluate the effectiveness of the restoration method.",
          "citing_paper_id": "251197000",
          "cited_paper_id": 235702784,
          "context_text": "We also included additional real-world restoration test cases from the raindrop removal test set of the RainDS dataset presented by [57] which consisted of 97 real test images.",
          "confidence_score": 1.0,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions the 'RainDS dataset' as a source of real-world restoration test cases for raindrop removal, which is directly relevant to the topic of all-in-one image restoration.",
          "citing_paper_doi": "10.1109/TPAMI.2023.3238179",
          "cited_paper_doi": "10.1109/CVPR46437.2021.00903",
          "citing_paper_url": "https://www.semanticscholar.org/paper/69beb616ffdc42afe86b7487c5db82b6d88638b8",
          "cited_paper_url": "https://www.semanticscholar.org/paper/c86f565984f83232d925534b34f585a0a4278522",
          "citing_paper_year": 2022,
          "cited_paper_year": 2021
        },
        {
          "dataset_name": "RainDS",
          "dataset_description": "Used to evaluate the performance of the proposed CCN and other state-of-the-art methods in image deraining, focusing on detail recovery and context aggregation.",
          "citing_paper_id": "235702784",
          "cited_paper_id": 260436809,
          "context_text": "We conduct extensive experiments on RainDS dataset to evaluate the performance of the proposed CCN and the state-of-the-art, i.e. , PReNet [35], DRD-Net [4] and Attent-GAN [30].",
          "confidence_score": 1.0,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions the 'RainDS dataset' which is used for evaluating the performance of the proposed method and other state-of-the-art methods in image deraining.",
          "citing_paper_doi": "10.1109/CVPR46437.2021.00903",
          "cited_paper_doi": "10.1109/cvpr42600.2020.01457",
          "citing_paper_url": "https://www.semanticscholar.org/paper/c86f565984f83232d925534b34f585a0a4278522",
          "cited_paper_url": "https://www.semanticscholar.org/paper/861f291cd8629e04f942208c61a2a4e935ecfa9f",
          "citing_paper_year": 2021,
          "cited_paper_year": 2020
        }
      ]
    },
    {
      "cited_paper_id": "4493958",
      "citation_count": 0,
      "total_dataset_mentions": 2,
      "unique_datasets": [
        "Berkeley segmentation dataset (BSD)"
      ],
      "dataset_details": [
        {
          "dataset_name": "Berkeley segmentation dataset (BSD)",
          "dataset_description": "Used to train and validate image restoration models, providing 400 images of size 180x180 for segmentation tasks. | Used to expand the training dataset, contributing 400 selected images from the validation set for diverse image content. | Used to train and evaluate image restoration methods, focusing on diverse and challenging images for quality assessment.",
          "citing_paper_id": "1900475",
          "cited_paper_id": 4840263,
          "context_text": "Hence, instead of training on a small dataset consisting of 400 Berkeley segmentation dataset (BSD) images of size 180×180 [13], we collect a large dataset which includes 400 BSD images, 400 selected images from validation set of ImageNet database [20] and 4,744 images of Waterloo Exploration Database [40].",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions three specific datasets used for training a larger dataset for image restoration. Each dataset is clearly identified and used to enhance the training set.",
          "citing_paper_doi": "10.1109/cvpr.2017.300",
          "cited_paper_doi": "10.1109/TIP.2016.2631888",
          "citing_paper_url": "https://www.semanticscholar.org/paper/dc8861b4ab6799be542829ae1ace13f23cf807cd",
          "cited_paper_url": "https://www.semanticscholar.org/paper/1e2fe67d117a19a59a4f82e4950499c5ee5a7473",
          "citing_paper_year": 2017,
          "cited_paper_year": 2017
        },
        {
          "dataset_name": "Berkeley segmentation dataset (BSD)",
          "dataset_description": "Provided 900 high-quality images, contributing to the robustness and quality of the image restoration dataset. | Contributed 4,744 images to the large dataset, enhancing the diversity and scale of the image restoration training set. | Supplied 2,750 images, further expanding the dataset for comprehensive training and evaluation in image restoration. | Used to construct a large dataset for image restoration, providing 400 images of size 180x180 for training and evaluation.",
          "citing_paper_id": "221377171",
          "cited_paper_id": 4493958,
          "context_text": "includes 400 Berkeley segmentation dataset (BSD) images of size 180 180 [9], we construct a large dataset consisting of 400 BSD images, 4,744 images of Waterloo Exploration Database [65], 900 images from DIV2K dataset [66], and 2,750 images from Flick2K dataset [63].",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions specific datasets used to construct a large dataset for image restoration research.",
          "citing_paper_doi": "10.1109/TPAMI.2021.3088914",
          "cited_paper_doi": "10.1109/CVPRW.2017.150",
          "citing_paper_url": "https://www.semanticscholar.org/paper/d467d747dde65862d9640fe49f69d6b6d46d69ea",
          "cited_paper_url": "https://www.semanticscholar.org/paper/6aeb583c084ad2eb709154ec91f22ca8d6de4425",
          "citing_paper_year": 2020,
          "cited_paper_year": 2017
        }
      ]
    },
    {
      "cited_paper_id": "237266491",
      "citation_count": 0,
      "total_dataset_mentions": 2,
      "unique_datasets": [
        "Kodak"
      ],
      "dataset_details": [
        {
          "dataset_name": "Kodak",
          "dataset_description": "Used to evaluate image restoration performance, comparing various methods including GRL-S. Focuses on PSNR scores to assess restoration quality. | Used to evaluate image restoration performance, comparing various methods including GAN-based approaches. Focuses on PSNR and SSIM scores to measure restoration quality.",
          "citing_paper_id": "257255385",
          "cited_paper_id": 2877535,
          "context_text": "Datasets Matlab DDR [75] DeepJoint [21] MMNet [36] RLDD [24] RNAN [85] DRUNet [80] GRL-S (ours) Kodak [16] 35.78 41.11 42.00 40.19 42.49 43.16 42.68 43.57 McMaster [83] 34.43 37.12 39.14 37.09 39.25 39.70 39.39 40.22\nBSRGAN Real ESRGAN SwinIR GRL\nFigure 7.",
          "confidence_score": 0.9,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions several datasets and methods, but only 'Kodak' and 'McMaster' are identified as datasets. The others are likely methods or models.",
          "citing_paper_doi": "10.1109/CVPR52729.2023.01753",
          "cited_paper_doi": "10.1109/TIP.2016.2574984",
          "citing_paper_url": "https://www.semanticscholar.org/paper/2920b885278a6973a306b57201e3fc3273b71132",
          "cited_paper_url": "https://www.semanticscholar.org/paper/002c43f8eefe17368437431c7c5b48d75b662a0d",
          "citing_paper_year": 2023,
          "cited_paper_year": 2016
        },
        {
          "dataset_name": "Kodak",
          "dataset_description": "Used to evaluate image restoration performance, comparing various methods including GRL-S. Focuses on PSNR scores to assess restoration quality. | Used to evaluate image restoration performance, comparing various methods including GAN-based approaches. Focuses on PSNR and SSIM scores to measure restoration quality.",
          "citing_paper_id": "257255385",
          "cited_paper_id": 237266491,
          "context_text": "Datasets Matlab DDR [75] DeepJoint [21] MMNet [36] RLDD [24] RNAN [85] DRUNet [80] GRL-S (ours) Kodak [16] 35.78 41.11 42.00 40.19 42.49 43.16 42.68 43.57 McMaster [83] 34.43 37.12 39.14 37.09 39.25 39.70 39.39 40.22\nBSRGAN Real ESRGAN SwinIR GRL\nFigure 7.",
          "confidence_score": 0.9,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions several datasets and methods, but only 'Kodak' and 'McMaster' are identified as datasets. The others are likely methods or models.",
          "citing_paper_doi": "10.1109/CVPR52729.2023.01753",
          "cited_paper_doi": "10.1109/ICCVW54120.2021.00210",
          "citing_paper_url": "https://www.semanticscholar.org/paper/2920b885278a6973a306b57201e3fc3273b71132",
          "cited_paper_url": "https://www.semanticscholar.org/paper/7a9a708ca61c14886aa0dcd6d13dac7879713f5f",
          "citing_paper_year": 2023,
          "cited_paper_year": 2021
        }
      ]
    },
    {
      "cited_paper_id": "3406592",
      "citation_count": 0,
      "total_dataset_mentions": 2,
      "unique_datasets": [
        "SOTS-outdoor"
      ],
      "dataset_details": [
        {
          "dataset_name": "SOTS-outdoor",
          "dataset_description": "Used for evaluating dehazing algorithms, focusing on outdoor scenes to assess performance in removing atmospheric haze. | Used for evaluating deraining algorithms, combining multiple test sets to assess performance in removing rain streaks from images. | Used for evaluating kernel estimation in image restoration, focusing on the accuracy of kernel prediction for various degradation types. | Used for evaluating denoising algorithms, focusing on reducing noise while preserving image details. | Used for evaluating low-light enhancement algorithms, focusing on improving visibility in low-light conditions. | Used for evaluating motion deblurring algorithms, focusing on real-world motion blur to assess the clarity and sharpness of restored images.",
          "citing_paper_id": "272987034",
          "cited_paper_id": 3406592,
          "context_text": "For evaluation, we use SOTS-outdoor [20] for dehazing, Rain13k-Test (the combination of Rain100L [53], Rain100H [53], Test100 [59], Test1200 [58] and Test2800 [10]) for deraining, GoPro for motion deblurring, LOL [49] for low-light enhancement, BSD68 [38] for denoising, LSDIR-val for kernel…",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions several datasets used for evaluating different image restoration tasks, which are specific and relevant to the topic of all-in-one image restoration.",
          "citing_paper_doi": "10.48550/arXiv.2409.19403",
          "cited_paper_doi": "10.1109/CVPR.2018.00079",
          "citing_paper_url": "https://www.semanticscholar.org/paper/cc6a57aa1b8496efd3b9001226fd09b8d121f397",
          "cited_paper_url": "https://www.semanticscholar.org/paper/34aaa735409b666b8dc678c23acb600b1d87913b",
          "citing_paper_year": 2024,
          "cited_paper_year": 2018
        },
        {
          "dataset_name": "SOTS-outdoor",
          "dataset_description": "Used for evaluating dehazing algorithms, focusing on outdoor scenes to assess performance in removing atmospheric haze. | Used for evaluating deraining algorithms, combining multiple test sets to assess performance in removing rain streaks from images. | Used for evaluating kernel estimation in image restoration, focusing on the accuracy of kernel prediction for various degradation types. | Used for evaluating denoising algorithms, focusing on reducing noise while preserving image details. | Used for evaluating low-light enhancement algorithms, focusing on improving visibility in low-light conditions. | Used for evaluating motion deblurring algorithms, focusing on real-world motion blur to assess the clarity and sharpness of restored images.",
          "citing_paper_id": "272987034",
          "cited_paper_id": 11922819,
          "context_text": "For evaluation, we use SOTS-outdoor [20] for dehazing, Rain13k-Test (the combination of Rain100L [53], Rain100H [53], Test100 [59], Test1200 [58] and Test2800 [10]) for deraining, GoPro for motion deblurring, LOL [49] for low-light enhancement, BSD68 [38] for denoising, LSDIR-val for kernel…",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions several datasets used for evaluating different image restoration tasks, which are specific and relevant to the topic of all-in-one image restoration.",
          "citing_paper_doi": "10.48550/arXiv.2409.19403",
          "cited_paper_doi": "10.1109/TCSVT.2019.2920407",
          "citing_paper_url": "https://www.semanticscholar.org/paper/cc6a57aa1b8496efd3b9001226fd09b8d121f397",
          "cited_paper_url": "https://www.semanticscholar.org/paper/920f0c070701caabf023c600f3e310f1906ca818",
          "citing_paper_year": 2024,
          "cited_paper_year": 2017
        }
      ]
    },
    {
      "cited_paper_id": "4885085",
      "citation_count": 0,
      "total_dataset_mentions": 2,
      "unique_datasets": [
        "NH-HAZE"
      ],
      "dataset_details": [
        {
          "dataset_name": "NH-HAZE",
          "dataset_description": "Used as a benchmark for image dehazing, comparing the performance of different methods in the context of image restoration.",
          "citing_paper_id": "258179052",
          "cited_paper_id": 218538083,
          "context_text": "This dataset has a large diversity of contents and is collected similar to NH-HAZE [1,2], but with all images having 6000 × 4000 × 3 pixels.",
          "confidence_score": 0.7,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions a dataset with specific characteristics but does not provide a clear name. NH-HAZE is mentioned but is referenced as a comparison, not the primary dataset used.",
          "citing_paper_doi": "10.1109/CVPRW59228.2023.00169",
          "cited_paper_doi": "10.1109/CVPRW50498.2020.00230",
          "citing_paper_url": "https://www.semanticscholar.org/paper/9b8b9c4f742998d3730cc13b30313a95fc0077fd",
          "cited_paper_url": "https://www.semanticscholar.org/paper/88e8a624825f0fb457fac26f65d43d9da76beab0",
          "citing_paper_year": 2023,
          "cited_paper_year": 2020
        },
        {
          "dataset_name": "NH-HAZE",
          "dataset_description": "Used to test dehazing algorithms on real indoor hazy and haze-free images, providing a benchmark for indoor scenarios. | Used to evaluate the model on real-world outdoor dehazing tasks, focusing on diverse environmental conditions. | Used to test dehazing algorithms, emphasizing outdoor scenes with varying levels of haze. | Used to evaluate the model on synthetic and real-world dehazing tasks, focusing on high-resolution images. | Used to evaluate the model on real-world indoor dehazing tasks, focusing on realistic hazy and haze-free images. | Used to evaluate dehazing algorithms on remote sensing images, focusing on satellite and aerial imagery. | Used to assess dehazing performance, specifically on natural hazy images captured under various conditions. | Used to evaluate dehazing models, focusing on dense haze conditions in real-world images. | Used to assess dehazing performance on real-world outdoor hazy images, emphasizing realistic conditions. | Used to benchmark dehazing methods, specifically on real hazy and haze-free indoor images. | Used to evaluate image dehazing algorithms, focusing on real-world hazy and haze-free outdoor images.",
          "citing_paper_id": "270736284",
          "cited_paper_id": 4885085,
          "context_text": "[72], NH-HAZE [73], O-HAZE [39], AND I-H AZE [40] TABLE IV I MAGE D EHAZING C OMPARISONS ON THE R EMOTE S ENSING D ATASETS : S ATE H AZE 1 K -",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions several datasets by name, which are relevant to image restoration, particularly dehazing. These datasets are used for evaluating image dehazing algorithms.",
          "citing_paper_doi": "10.1109/TPAMI.2024.3419007",
          "cited_paper_doi": "10.1007/978-3-030-01449-0_52",
          "citing_paper_url": "https://www.semanticscholar.org/paper/804d2302880b91965e84a7c72ac26429d26792a0",
          "cited_paper_url": "https://www.semanticscholar.org/paper/013a74b9c78e292a8ea9e7dcbce6615f20b34276",
          "citing_paper_year": 2024,
          "cited_paper_year": 2018
        }
      ]
    },
    {
      "cited_paper_id": "3619954",
      "citation_count": 0,
      "total_dataset_mentions": 2,
      "unique_datasets": [
        "RealSR"
      ],
      "dataset_details": [
        {
          "dataset_name": "RealSR",
          "dataset_description": "Used to compare performance of CNN-based, transformer-based, and diffusion model-based methods for image super-resolution, focusing on out-of-distribution generalization.",
          "citing_paper_id": "261031011",
          "cited_paper_id": 3619954,
          "context_text": "TABLE 7: Comparisons of CNN-based(RRDB [295]),transformer-based(SwinIR [53]) and DM-based(SRdiff [101], StableSR [121]) on out of distribution dataset RealSR [268]",
          "confidence_score": 1.0,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions 'RealSR' as an out-of-distribution dataset used for comparisons. No other specific datasets are mentioned.",
          "citing_paper_doi": "10.48550/arXiv.2308.09388",
          "cited_paper_doi": "10.1109/CVPR.2018.00262",
          "citing_paper_url": "https://www.semanticscholar.org/paper/70ba95384664ab7becfd8196d20a46fe66112c91",
          "cited_paper_url": "https://www.semanticscholar.org/paper/4ef1476dec02c62227187edbba88615278b3edba",
          "citing_paper_year": 2023,
          "cited_paper_year": 2018
        },
        {
          "dataset_name": "RealSR",
          "dataset_description": "Used to test and compare MIRNet-v2 against other super-resolution algorithms, focusing on upscaling factors of ×2, ×3, and ×4. | Used to test MIRNet-v2 against state-of-the-art SR algorithms for upscaling factors of ×2, ×3, and ×4, focusing on image quality and performance.",
          "citing_paper_id": "212725053",
          "cited_paper_id": 49657846,
          "context_text": "We compare our MIRNet-v2 against the state-of-the-art SR algorithms (VDSR [59], SRResNet [79], RCAN [71], LP-KPN [120]) on the testing images of the RealSR [120] for upscaling factors of × 2 , × 3 and × 4 .",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions 'RealSR' as a dataset used for testing image super-resolution algorithms. It is a specific dataset with a clear identifier and is relevant to the topic of image restoration.",
          "citing_paper_doi": "10.1109/TPAMI.2022.3167175",
          "cited_paper_doi": "10.1007/978-3-030-01234-2_18",
          "citing_paper_url": "https://www.semanticscholar.org/paper/bb3c793a5a6d3be2454d22a76ada5b6a94ec42f5",
          "cited_paper_url": "https://www.semanticscholar.org/paper/9775f8964a2eea1c9e35a02b1b906487396ea1f5",
          "citing_paper_year": 2022,
          "cited_paper_year": 2018
        }
      ]
    },
    {
      "cited_paper_id": "249709314",
      "citation_count": 0,
      "total_dataset_mentions": 2,
      "unique_datasets": [
        "Rain800"
      ],
      "dataset_details": [
        {
          "dataset_name": "Rain800",
          "dataset_description": "Used to train and evaluate methods for removing rain from single images, focusing on synthetic rain streaks.",
          "citing_paper_id": "261031011",
          "cited_paper_id": 17115407,
          "context_text": "], Rain800 [48], DDN-Data [280] contains amounts of synthesis rain streaks.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The citation mentions specific datasets used for training or evaluation in the context of image restoration, particularly for removing rain from images.",
          "citing_paper_doi": "10.48550/arXiv.2308.09388",
          "cited_paper_doi": "10.1109/CVPR.2017.186",
          "citing_paper_url": "https://www.semanticscholar.org/paper/70ba95384664ab7becfd8196d20a46fe66112c91",
          "cited_paper_url": "https://www.semanticscholar.org/paper/29ad37a306732676a41f25e05960ff0adbca631e",
          "citing_paper_year": 2023,
          "cited_paper_year": 2017
        },
        {
          "dataset_name": "Rain800",
          "dataset_description": "Used to train models for image de-raining, providing a larger set of synthetic rain images to improve generalization. | Used to train models for image de-raining, focusing on high-resolution synthetic rain images to enhance de-raining quality. | Used to train models for real-world deraining, specifically evaluating the performance of CoIC, DGUNet, and IDT to validate the proposed method's superiority. | Used to train models for image de-raining, containing diverse synthetic rain images to test and improve de-raining algorithms. | Used to evaluate the impact of dataset collision in image de-raining, focusing on low-resolution rain images. | Used to train models for image de-raining, providing additional synthetic rain images to enhance the robustness of de-raining methods. | Used to evaluate the impact of dataset collision in image de-raining, focusing on high-resolution rain images. | Used to train models for image de-raining, focusing on low-resolution synthetic rain images to improve de-raining performance.",
          "citing_paper_id": "269214582",
          "cited_paper_id": 249709314,
          "context_text": "Our subsequent analysis examines the real-world deraining capabilities by training models on an individual dataset ( e . g ., Rain800), Equipped with the proposed CoIC, DGUNet and IDT achieves the best real-world derain-ing quality, validating the superiority of our proposed method.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions 'Rain800' as a specific dataset used for training models to examine real-world deraining capabilities.",
          "citing_paper_doi": "10.48550/arXiv.2404.12091",
          "cited_paper_doi": "10.1109/TPAMI.2022.3183612",
          "citing_paper_url": "https://www.semanticscholar.org/paper/348009f068853d56cec79e0eecdeb0279d00893a",
          "cited_paper_url": "https://www.semanticscholar.org/paper/591fc9d7642d4501686f418d6059c66a914e7cb0",
          "citing_paper_year": 2024,
          "cited_paper_year": 2022
        }
      ]
    },
    {
      "cited_paper_id": "131773964",
      "citation_count": 0,
      "total_dataset_mentions": 2,
      "unique_datasets": [
        "Outdoor-Rain dataset"
      ],
      "dataset_details": [
        {
          "dataset_name": "Outdoor-Rain dataset",
          "dataset_description": "Used to model heavy rainy images, focusing on rain accumulation for more realistic heavy rain scenarios in image restoration.",
          "citing_paper_id": "261031011",
          "cited_paper_id": 131773964,
          "context_text": "Outdoor-Rain [281] considers the rain accumulation, which provides more reasonable modeling for heavy rainy images.",
          "confidence_score": 0.9,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions 'Outdoor-Rain' as a dataset that considers rain accumulation, which is relevant to the topic of image restoration.",
          "citing_paper_doi": "10.48550/arXiv.2308.09388",
          "cited_paper_doi": "10.1109/CVPR.2019.00173",
          "citing_paper_url": "https://www.semanticscholar.org/paper/70ba95384664ab7becfd8196d20a46fe66112c91",
          "cited_paper_url": "https://www.semanticscholar.org/paper/b1b7eeac57e1e78e02d9d97eb809d8459bd06050",
          "citing_paper_year": 2023,
          "cited_paper_year": 2019
        },
        {
          "dataset_name": "Outdoor-Rain dataset",
          "dataset_description": "Used to train and compare Pix2Pix GAN and CycleGAN models, focusing on image-to-image translation for rain removal in outdoor scenes.",
          "citing_paper_id": "131773964",
          "cited_paper_id": 206770979,
          "context_text": "In addition, we also compare with Pix2Pix GAN [13] and CycleGAN [42] trained on the Outdoor-Rain dataset.",
          "confidence_score": 1.0,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions the 'Outdoor-Rain dataset' which is a specific dataset used for training and comparison in the research.",
          "citing_paper_doi": "10.1109/CVPR.2019.00173",
          "cited_paper_doi": "10.1109/ICCV.2017.244",
          "citing_paper_url": "https://www.semanticscholar.org/paper/b1b7eeac57e1e78e02d9d97eb809d8459bd06050",
          "cited_paper_url": "https://www.semanticscholar.org/paper/c43d954cf8133e6254499f3d68e45218067e4941",
          "citing_paper_year": 2019,
          "cited_paper_year": 2017
        }
      ]
    },
    {
      "cited_paper_id": "253347622",
      "citation_count": 0,
      "total_dataset_mentions": 2,
      "unique_datasets": [
        "RDD"
      ],
      "dataset_details": [
        {
          "dataset_name": "RDD",
          "dataset_description": "Used to train a model for image restoration, contributing 4,371 real images from its training set. | Used to train a model for document shadow removal, providing 14,200 synthetic images for the task.",
          "citing_paper_id": "269614217",
          "cited_paper_id": 253347622,
          "context_text": "The training set for this task consists of 14,200 synthetic images from FSDSRD [35] and 4,371 real images from the training set of RDD [71].",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions two specific datasets, FSDSRD and RDD, which are used for training a model for image restoration tasks.",
          "citing_paper_doi": "10.1109/CVPR52733.2024.01482",
          "cited_paper_doi": "10.1109/ICIP46576.2022.9897217",
          "citing_paper_url": "https://www.semanticscholar.org/paper/cd1ea89f90f9fe48fbf815ef5cfb18e92c56d022",
          "cited_paper_url": "https://www.semanticscholar.org/paper/9c8a5e1dfe16211aefb1eebbfd40f4d238027194",
          "citing_paper_year": 2024,
          "cited_paper_year": 2022
        },
        {
          "dataset_name": "RDD",
          "dataset_description": "Used to train BGSNet for document shadow removal, focusing on real-world scenarios and high-resolution images. | Used to train DocShadow for document shadow removal, emphasizing a large-scale dataset with diverse shadow conditions.",
          "citing_paper_id": "269614217",
          "cited_paper_id": 261244108,
          "context_text": "In the deshadowing domain, we compare DocRes with the latest SOTA models, namely BGSNet [71] (utilizing the model trained on the RDD dataset provided by the authors) and DocShadow [25] (based on the model trained on the SD7K dataset provided by the authors).",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions two datasets, RDD and SD7K, which are used to train models for document shadow removal. These datasets are specific and relevant to the research topic.",
          "citing_paper_doi": "10.1109/CVPR52733.2024.01482",
          "cited_paper_doi": "10.1109/ICCV51070.2023.01144",
          "citing_paper_url": "https://www.semanticscholar.org/paper/cd1ea89f90f9fe48fbf815ef5cfb18e92c56d022",
          "cited_paper_url": "https://www.semanticscholar.org/paper/2cdbd9e3ecdc1280e3faf959297c7917e2789aff",
          "citing_paper_year": 2024,
          "cited_paper_year": 2023
        }
      ]
    },
    {
      "cited_paper_id": "226254406",
      "citation_count": 0,
      "total_dataset_mentions": 2,
      "unique_datasets": [
        "Hypersim"
      ],
      "dataset_details": [
        {
          "dataset_name": "Hypersim",
          "dataset_description": "Used to evaluate image restoration models, providing synthetic data with controlled variations to simulate real-world conditions. | Used for holistic indoor scene understanding, providing photorealistic synthetic data to train and evaluate image restoration models.",
          "citing_paper_id": "272826725",
          "cited_paper_id": 210942959,
          "context_text": "The datasets we use include Hypersim (Roberts et al., 2021) and VKITTI 2 (Cabon et al., 2020), and our collected depth maps from controllable dataset.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "Hypersim and VKITTI 2 are explicitly mentioned as datasets used in the research. The 'controllable dataset' is too generic and lacks a specific identifier.",
          "citing_paper_doi": "10.48550/arXiv.2409.15278",
          "cited_paper_doi": null,
          "citing_paper_url": "https://www.semanticscholar.org/paper/2c0697de3deccb0fb88ca6674cfa3648943a8b00",
          "cited_paper_url": "https://www.semanticscholar.org/paper/c6e57b2d36a4231cef82d7e0e27413ae4bb4dee5",
          "citing_paper_year": 2024,
          "cited_paper_year": 2020
        },
        {
          "dataset_name": "Hypersim",
          "dataset_description": "Used to evaluate image restoration models, providing synthetic data with controlled variations to simulate real-world conditions. | Used for holistic indoor scene understanding, providing photorealistic synthetic data to train and evaluate image restoration models.",
          "citing_paper_id": "272826725",
          "cited_paper_id": 226254406,
          "context_text": "The datasets we use include Hypersim (Roberts et al., 2021) and VKITTI 2 (Cabon et al., 2020), and our collected depth maps from controllable dataset.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "Hypersim and VKITTI 2 are explicitly mentioned as datasets used in the research. The 'controllable dataset' is too generic and lacks a specific identifier.",
          "citing_paper_doi": "10.48550/arXiv.2409.15278",
          "cited_paper_doi": "10.1109/ICCV48922.2021.01073",
          "citing_paper_url": "https://www.semanticscholar.org/paper/2c0697de3deccb0fb88ca6674cfa3648943a8b00",
          "cited_paper_url": "https://www.semanticscholar.org/paper/84c40cf28afdf84ec6941d92cacd49fed3c7ef9a",
          "citing_paper_year": 2024,
          "cited_paper_year": 2020
        }
      ]
    },
    {
      "cited_paper_id": "265221391",
      "citation_count": 0,
      "total_dataset_mentions": 2,
      "unique_datasets": [
        "MagicBrush Test"
      ],
      "dataset_details": [
        {
          "dataset_name": "MagicBrush Test",
          "dataset_description": "Used to evaluate PixWizard's effectiveness in instruction-guided image editing, focusing on the accuracy and quality of edits based on user instructions. | Used to assess PixWizard's performance in precise image editing tasks, combining recognition and generation to evaluate the system's ability to make accurate edits.",
          "citing_paper_id": "272826725",
          "cited_paper_id": 259187796,
          "context_text": "We evaluate PixWizard across two benchmarks, the MagicBrush Test (Zhang et al., 2024a) and the Emu Edit Test (Sheynin et al., 2024), to assess its effectiveness in image editing Results.",
          "confidence_score": 0.9,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions two benchmarks, 'MagicBrush Test' and 'Emu Edit Test', which are used to evaluate the effectiveness of PixWizard in image editing. These benchmarks are likely datasets or collections of images and instructions.",
          "citing_paper_doi": "10.48550/arXiv.2409.15278",
          "cited_paper_doi": "10.48550/arXiv.2306.10012",
          "citing_paper_url": "https://www.semanticscholar.org/paper/2c0697de3deccb0fb88ca6674cfa3648943a8b00",
          "cited_paper_url": "https://www.semanticscholar.org/paper/ecf38d702bea560c1f9d372645e2be7661a71f37",
          "citing_paper_year": 2024,
          "cited_paper_year": 2023
        },
        {
          "dataset_name": "MagicBrush Test",
          "dataset_description": "Used to evaluate PixWizard's effectiveness in instruction-guided image editing, focusing on the accuracy and quality of edits based on user instructions. | Used to assess PixWizard's performance in precise image editing tasks, combining recognition and generation to evaluate the system's ability to make accurate edits.",
          "citing_paper_id": "272826725",
          "cited_paper_id": 265221391,
          "context_text": "We evaluate PixWizard across two benchmarks, the MagicBrush Test (Zhang et al., 2024a) and the Emu Edit Test (Sheynin et al., 2024), to assess its effectiveness in image editing Results.",
          "confidence_score": 0.9,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions two benchmarks, 'MagicBrush Test' and 'Emu Edit Test', which are used to evaluate the effectiveness of PixWizard in image editing. These benchmarks are likely datasets or collections of images and instructions.",
          "citing_paper_doi": "10.48550/arXiv.2409.15278",
          "cited_paper_doi": "10.1109/CVPR52733.2024.00847",
          "citing_paper_url": "https://www.semanticscholar.org/paper/2c0697de3deccb0fb88ca6674cfa3648943a8b00",
          "cited_paper_url": "https://www.semanticscholar.org/paper/5bcb0153dd0840113eb27d4d6f753414ef656a03",
          "citing_paper_year": 2024,
          "cited_paper_year": 2023
        }
      ]
    },
    {
      "cited_paper_id": "257952310",
      "citation_count": 0,
      "total_dataset_mentions": 2,
      "unique_datasets": [
        "LAION-5B"
      ],
      "dataset_details": [
        {
          "dataset_name": "LAION-5B",
          "dataset_description": "Used to compare image quality against other large datasets, emphasizing the importance of image quality in the research. | Used to compare image quality against other large datasets, highlighting the need for high-quality images in the research.",
          "citing_paper_id": "267199774",
          "cited_paper_id": 252917726,
          "context_text": "Larger datasets like ImageNet (IN) [17], LAION-5B [67], and SA-1B [44] contain more images, but their image quality does not meet our high standards.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions three datasets by name, all of which are multi-word proper nouns or acronyms with digits, making them specific and plausible. The context indicates that these datasets are compared based on image quality.",
          "citing_paper_doi": "10.1109/CVPR52733.2024.02425",
          "cited_paper_doi": "10.48550/arXiv.2210.08402",
          "citing_paper_url": "https://www.semanticscholar.org/paper/3a63ae4086248e4ba4bd106839a26a08256909c4",
          "cited_paper_url": "https://www.semanticscholar.org/paper/e5c8960eb2ec034ffbd353ef39fd1cb541d3c7c9",
          "citing_paper_year": 2024,
          "cited_paper_year": 2022
        },
        {
          "dataset_name": "LAION-5B",
          "dataset_description": "Used to compare image quality against other large datasets, emphasizing the importance of image quality in the research. | Used to compare image quality against other large datasets, highlighting the need for high-quality images in the research.",
          "citing_paper_id": "267199774",
          "cited_paper_id": 257952310,
          "context_text": "Larger datasets like ImageNet (IN) [17], LAION-5B [67], and SA-1B [44] contain more images, but their image quality does not meet our high standards.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions three datasets by name, all of which are multi-word proper nouns or acronyms with digits, making them specific and plausible. The context indicates that these datasets are compared based on image quality.",
          "citing_paper_doi": "10.1109/CVPR52733.2024.02425",
          "cited_paper_doi": "10.1109/ICCV51070.2023.00371",
          "citing_paper_url": "https://www.semanticscholar.org/paper/3a63ae4086248e4ba4bd106839a26a08256909c4",
          "cited_paper_url": "https://www.semanticscholar.org/paper/7470a1702c8c86e6f28d32cfa315381150102f5b",
          "citing_paper_year": 2024,
          "cited_paper_year": 2023
        }
      ]
    },
    {
      "cited_paper_id": "10987457",
      "citation_count": 0,
      "total_dataset_mentions": 2,
      "unique_datasets": [
        "Berkeley Segmentation Dataset (BSD)"
      ],
      "dataset_details": [
        {
          "dataset_name": "Berkeley Segmentation Dataset (BSD)",
          "dataset_description": "Used to generate image patches for training in image denoising, specifically employing 300 images from the BSD train and val sets.",
          "citing_paper_id": "8550762",
          "cited_paper_id": 10987457,
          "context_text": "Datasets For image denoising, we follow [27] to use 300 images from the Berkeley Segmentation Dataset (BSD) [28], known as the train and val sets, to generate image patches as the training set.",
          "confidence_score": 1.0,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions the Berkeley Segmentation Dataset (BSD) as a specific dataset used for generating image patches for training in the context of image denoising.",
          "citing_paper_doi": "10.1109/ICCV.2017.486",
          "cited_paper_doi": null,
          "citing_paper_url": "https://www.semanticscholar.org/paper/5fdd40601a2ccdaa2d2ade27872bd8b3f43b2c1c",
          "cited_paper_url": "https://www.semanticscholar.org/paper/18168aea48a22f6fe2fe407c0ff70083cba225a7",
          "citing_paper_year": 2017,
          "cited_paper_year": 2016
        },
        {
          "dataset_name": "Berkeley Segmentation Dataset (BSD)",
          "dataset_description": "Used for evaluating image segmentation and restoration algorithms, focusing on boundary detection and region segmentation tasks. | Used for super-resolution and image restoration, providing high-quality images for training and evaluation of upscaling models. | Used to collect 200 images for training an image restoration model, focusing on cropping patches for model training. | Used to collect 800 images for training an image restoration model, focusing on cropping patches for model training. | Used to collect 4,744 images for training an image restoration model, focusing on cropping patches for model training. | Used to assess image quality and restoration methods, offering a diverse set of images with various distortions and degradation types. | Used to train the MWCNN model, providing a diverse set of images for segmentation and restoration tasks. | Used to train the MWCNN model, offering high-resolution images for super-resolution and restoration tasks. | Used to train the MWCNN model, introducing new challenges for image quality assessment and restoration.",
          "citing_paper_id": "29151865",
          "cited_paper_id": 4840263,
          "context_text": "To train our MWCNN, a large training set is constructed by using images from three dataset, i.e. Berkeley Segmentation Dataset (BSD) [38], DIV2K [3] and Waterloo Exploration Database (WED) [35].",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions three specific datasets used for training the MWCNN model. These datasets are clearly identified and are relevant to the image restoration research.",
          "citing_paper_doi": "10.1109/CVPRW.2018.00121",
          "cited_paper_doi": "10.1109/TIP.2016.2631888",
          "citing_paper_url": "https://www.semanticscholar.org/paper/5813ae0a48ff50d48f406df52ee5b9795c34a8bd",
          "cited_paper_url": "https://www.semanticscholar.org/paper/1e2fe67d117a19a59a4f82e4950499c5ee5a7473",
          "citing_paper_year": 2018,
          "cited_paper_year": 2017
        }
      ]
    },
    {
      "cited_paper_id": "231802205",
      "citation_count": 0,
      "total_dataset_mentions": 2,
      "unique_datasets": [
        "BSDS400"
      ],
      "dataset_details": [
        {
          "dataset_name": "BSDS400",
          "dataset_description": "Used for rain removal tasks, providing a large dataset of rainy images for training and evaluation. | Used for image restoration, containing 1212 clean images. Compared with HQ-50K to highlight the larger scale of the latter. | Used to compare the average pixel number of images, highlighting its size relative to other datasets in image restoration and super-resolution tasks. | Part of the DF2K dataset, used for training and evaluating single image super-resolution models, providing a diverse set of images. | Used for image restoration, containing 4744 clean images. Compared with HQ-50K to highlight the larger scale of the latter. | Used for single image super-resolution tasks, including 2,650 high-resolution images. The dataset is designed to address specific degradation types. | Used as a benchmark dataset for single image super-resolution, providing high-quality images for training and evaluation. | Used for image restoration, particularly for evaluating models on web images with various degradations. | Used for image restoration, containing 2650 clean images. Compared with HQ-50K to highlight the larger scale of the latter. | Used for single image super-resolution tasks, including 800 high-resolution images. The dataset is designed to address specific degradation types. | Used for image restoration, containing 400 clean images. Compared with HQ-50K to highlight the larger scale of the latter. | Used as a large-scale dataset for image restoration, containing diverse images for training and testing models. | Part of the DF2K dataset, used for training and evaluating single image super-resolution models, providing high-quality images. | Used for single image super-resolution, comprising outdoor natural images to train and evaluate restoration models. | Used as a baseline for single image super-resolution, combining images from DIV2K and Flickr2K to train and evaluate models. | Used for image restoration, containing 800 clean images. Compared with HQ-50K to highlight the larger scale of the latter.",
          "citing_paper_id": "259108489",
          "cited_paper_id": 6540453,
          "context_text": "Our HQ-50K contains 50,000 images, while the commonly used restoration dataset BSDS400 [4], DIV2K [2], Flickr2K [32], WED [35], and Rain13K [58] only contains 400, 800, 2650, 4744 and 1212 clean images respectively.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions several datasets used for image restoration, providing specific image counts for each. These datasets are clearly identified and used for comparison with the HQ-50K dataset.",
          "citing_paper_doi": "10.48550/arXiv.2306.05390",
          "cited_paper_doi": "10.1109/CVPRW.2017.151",
          "citing_paper_url": "https://www.semanticscholar.org/paper/4296bf82e67051e4cba10e510932f55fc12dd1e2",
          "cited_paper_url": "https://www.semanticscholar.org/paper/7ba5d3808e117e7a68dc40331ce1d483ceeedcb2",
          "citing_paper_year": 2023,
          "cited_paper_year": 2017
        },
        {
          "dataset_name": "BSDS400",
          "dataset_description": "Constituent part of Rain13K, used for deraining experiments, providing a moderate set of rainy images for training and evaluation. | Used for image restoration, containing 4744 clean images. Compared with HQ-50K to highlight the larger scale of the latter. | Used for training and evaluating deraining models, providing 800 diverse images with varying rain intensities. | Used as a benchmark dataset for single image super-resolution, providing high-quality images for training and evaluation. | Used for image restoration, containing 400 clean images. Compared with HQ-50K to highlight the larger scale of the latter. | Used for rain removal tasks, providing a large dataset of rainy images for training and evaluation. | Used for deraining experiments, combining multiple datasets to provide a comprehensive set of rainy images for training and evaluation. | Constituent part of Rain13K, used for deraining experiments, providing a large set of rainy images for training and evaluation. | Used for large-scale deraining experiments, comprising 13,000 images with a wide range of rain conditions. | Constituent part of Rain13K, used for deraining experiments, providing a diverse set of rainy images for training and evaluation. | Used for image restoration, containing 2650 clean images. Compared with HQ-50K to highlight the larger scale of the latter. | Used for comprehensive evaluation of deraining methods, containing 1200 images with complex rain patterns. | Used as a large-scale dataset for image restoration, containing diverse images for training and testing models. | Used for image restoration, containing 1212 clean images. Compared with HQ-50K to highlight the larger scale of the latter. | Used to compare the average pixel number of images, highlighting its size relative to other datasets in image restoration and super-resolution tasks. | Used for image restoration, particularly for evaluating models on web images with various degradations. | Used for evaluating deraining algorithms, focusing on high-resolution images with 100 samples, emphasizing detailed rain streak removal. | Used for robust deraining algorithm testing, featuring 1400 images with realistic rain effects. | Constituent part of Rain13K, used for deraining experiments, providing a small set of rainy images for training and evaluation. | Used to train and evaluate deraining models, focusing on progressive image restoration techniques and performance metrics. | Used for evaluating deraining algorithms, focusing on lightweight rain streak removal with 100 low-resolution images. | Used for image restoration, containing 800 clean images. Compared with HQ-50K to highlight the larger scale of the latter.",
          "citing_paper_id": "259108489",
          "cited_paper_id": 231802205,
          "context_text": "Our HQ-50K contains 50,000 images, while the commonly used restoration dataset BSDS400 [4], DIV2K [2], Flickr2K [32], WED [35], and Rain13K [58] only contains 400, 800, 2650, 4744 and 1212 clean images respectively.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions several datasets used for image restoration, providing specific image counts for each. These datasets are clearly identified and used for comparison with the HQ-50K dataset.",
          "citing_paper_doi": "10.48550/arXiv.2306.05390",
          "cited_paper_doi": "10.1109/CVPR46437.2021.01458",
          "citing_paper_url": "https://www.semanticscholar.org/paper/4296bf82e67051e4cba10e510932f55fc12dd1e2",
          "cited_paper_url": "https://www.semanticscholar.org/paper/92d50602db5746f03b91562e2cc8a98bec584e9b",
          "citing_paper_year": 2023,
          "cited_paper_year": 2021
        }
      ]
    },
    {
      "cited_paper_id": "253523872",
      "citation_count": 0,
      "total_dataset_mentions": 2,
      "unique_datasets": [
        "WorldView II"
      ],
      "dataset_details": [
        {
          "dataset_name": "WorldView II",
          "dataset_description": "Used for evaluating image restoration techniques, focusing on high-resolution satellite imagery to enhance image quality.",
          "citing_paper_id": "260957038",
          "cited_paper_id": 253523853,
          "context_text": "The WorldView II, WorldView III, and GaoFen2 datasets (Zhou et al., 2022a; Yan et al., 2022) are used.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The citation mentions specific satellite image datasets used in the research, which are relevant to image restoration.",
          "citing_paper_doi": null,
          "cited_paper_doi": "10.1007/978-3-031-19800-7_10",
          "citing_paper_url": "https://www.semanticscholar.org/paper/4b688c486f2b223212682fd2b3f72cc0caa94b1b",
          "cited_paper_url": "https://www.semanticscholar.org/paper/f330e64b263af1c0631418619fd130e9a2314525",
          "citing_paper_year": 2023,
          "cited_paper_year": 2022
        },
        {
          "dataset_name": "WorldView II",
          "dataset_description": "Used for evaluating image restoration techniques, focusing on high-resolution satellite imagery to enhance image quality.",
          "citing_paper_id": "260957038",
          "cited_paper_id": 253523872,
          "context_text": "The WorldView II, WorldView III, and GaoFen2 datasets (Zhou et al., 2022a; Yan et al., 2022) are used.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The citation mentions specific satellite image datasets used in the research, which are relevant to image restoration.",
          "citing_paper_doi": null,
          "cited_paper_doi": "10.1007/978-3-031-19800-7_11",
          "citing_paper_url": "https://www.semanticscholar.org/paper/4b688c486f2b223212682fd2b3f72cc0caa94b1b",
          "cited_paper_url": "https://www.semanticscholar.org/paper/e0ce3187c1fbb691fdd4d6c2bae16ce4a4670141",
          "citing_paper_year": 2023,
          "cited_paper_year": 2022
        }
      ]
    },
    {
      "cited_paper_id": "4715123",
      "citation_count": 0,
      "total_dataset_mentions": 2,
      "unique_datasets": [
        "polyU Real World Noisy Images Dataset"
      ],
      "dataset_details": [
        {
          "dataset_name": "polyU Real World Noisy Images Dataset",
          "dataset_description": "Used to train color denoising models, specifically targeting noise reduction in smartphone camera images. | Used to train color denoising models, focusing on real-world noisy images to improve denoising performance.",
          "citing_paper_id": "259108489",
          "cited_paper_id": 4715123,
          "context_text": "Some color denoising models are trained on the polyU Real World Noisy Images Dataset [51] and Smartphone Image Denoising Dataset (SIDD) [1].",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions two specific datasets used for training color denoising models, which are directly relevant to the topic of image restoration.",
          "citing_paper_doi": "10.48550/arXiv.2306.05390",
          "cited_paper_doi": null,
          "citing_paper_url": "https://www.semanticscholar.org/paper/4296bf82e67051e4cba10e510932f55fc12dd1e2",
          "cited_paper_url": "https://www.semanticscholar.org/paper/fb52f9294fa1cd83660eccac486367eac01ea32d",
          "citing_paper_year": 2023,
          "cited_paper_year": 2018
        },
        {
          "dataset_name": "polyU Real World Noisy Images Dataset",
          "dataset_description": "Used to train color denoising models, specifically targeting noise reduction in smartphone camera images. | Used to train color denoising models, focusing on real-world noisy images to improve denoising performance.",
          "citing_paper_id": "259108489",
          "cited_paper_id": 52059988,
          "context_text": "Some color denoising models are trained on the polyU Real World Noisy Images Dataset [51] and Smartphone Image Denoising Dataset (SIDD) [1].",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions two specific datasets used for training color denoising models, which are directly relevant to the topic of image restoration.",
          "citing_paper_doi": "10.48550/arXiv.2306.05390",
          "cited_paper_doi": "10.1109/CVPR.2018.00182",
          "citing_paper_url": "https://www.semanticscholar.org/paper/4296bf82e67051e4cba10e510932f55fc12dd1e2",
          "cited_paper_url": "https://www.semanticscholar.org/paper/ebf35073e122782f685a0d6c231622412f28a53b",
          "citing_paper_year": 2023,
          "cited_paper_year": 2018
        }
      ]
    },
    {
      "cited_paper_id": "6628106",
      "citation_count": 0,
      "total_dataset_mentions": 2,
      "unique_datasets": [
        "NYU-Rain"
      ],
      "dataset_details": [
        {
          "dataset_name": "NYU-Rain",
          "dataset_description": "Used to train a physics-based stage for synthetic rain generation, focusing on creating realistic rain effects in images. | Provided background images for generating the synthetic rain dataset NYU-Rain, enhancing the realism of the training data.",
          "citing_paper_id": "131773964",
          "cited_paper_id": 545361,
          "context_text": "Hence, for the training of the physics-based stage, we create a new synthetic rain dataset named NYU-Rain, using images from NYU-Depthv2 [27] dataset as background.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions the creation of a new synthetic rain dataset named NYU-Rain, which is used for training a physics-based stage. It also references the NYU-Depthv2 dataset as the source of background images.",
          "citing_paper_doi": "10.1109/CVPR.2019.00173",
          "cited_paper_doi": "10.1007/978-3-642-33715-4_54",
          "citing_paper_url": "https://www.semanticscholar.org/paper/b1b7eeac57e1e78e02d9d97eb809d8459bd06050",
          "cited_paper_url": "https://www.semanticscholar.org/paper/c1994ba5946456fc70948c549daf62363f13fa2d",
          "citing_paper_year": 2019,
          "cited_paper_year": 2012
        },
        {
          "dataset_name": "NYU-Rain",
          "dataset_description": "Used to train the physics-based stage of an image restoration model, focusing on rain removal using the Adam optimizer with weight decay.",
          "citing_paper_id": "131773964",
          "cited_paper_id": 6628106,
          "context_text": "To train the physics-based stage on the NYU-Rain dataset, we use Adam [18] optimizer with weight decay 10 − 4 and only supervise L Θ .",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions the NYU-Rain dataset, which is a specific dataset used for training a physics-based stage. The optimizer and loss function are also mentioned, but they are not datasets.",
          "citing_paper_doi": "10.1109/CVPR.2019.00173",
          "cited_paper_doi": null,
          "citing_paper_url": "https://www.semanticscholar.org/paper/b1b7eeac57e1e78e02d9d97eb809d8459bd06050",
          "cited_paper_url": "https://www.semanticscholar.org/paper/a6cb366736791bcccc5c8639de5a8f9636bf87e8",
          "citing_paper_year": 2019,
          "cited_paper_year": 2014
        }
      ]
    },
    {
      "cited_paper_id": "219978541",
      "citation_count": 0,
      "total_dataset_mentions": 2,
      "unique_datasets": [
        "Rain-Haze-Snow"
      ],
      "dataset_details": [
        {
          "dataset_name": "Rain-Haze-Snow",
          "dataset_description": "Used to train a single-encoder, multi-decoder framework for removing weather-based degradations, focusing on rain, haze, and snow conditions.",
          "citing_paper_id": "278740510",
          "cited_paper_id": 219978541,
          "context_text": "Li et al. [23] introduce a single-encoder, multi-decoder framework targeting weather-based degradations using the Rain-Haze-Snow dataset.",
          "confidence_score": 1.0,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions a specific dataset, 'Rain-Haze-Snow', which is used for training a model to handle weather-based degradations.",
          "citing_paper_doi": "10.48550/arXiv.2505.12630",
          "cited_paper_doi": "10.1109/cvpr42600.2020.00324",
          "citing_paper_url": "https://www.semanticscholar.org/paper/18b73e3db636e6ded820bc0b5b5ad13993e287bc",
          "cited_paper_url": "https://www.semanticscholar.org/paper/2e89dd08b5ea206128724419e259c8750e8d1381",
          "citing_paper_year": 2025,
          "cited_paper_year": 2020
        },
        {
          "dataset_name": "Rain-Haze-Snow",
          "dataset_description": "Used to train and evaluate a neural architecture search for all-in-one image restoration, focusing on rain, haze, and snow removal tasks.",
          "citing_paper_id": "260107992",
          "cited_paper_id": 219978541,
          "context_text": "[29] proposed a neural architecture search for all-in-one image restoration focusing on Rain-Haze-Snow dataset using task-specific encoders Chen et al.",
          "confidence_score": 1.0,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions the 'Rain-Haze-Snow dataset' which is a specific dataset used for all-in-one image restoration tasks. The dataset is clearly named and relevant to the research topic.",
          "citing_paper_doi": "10.1109/CVPR52729.2023.00563",
          "cited_paper_doi": "10.1109/cvpr42600.2020.00324",
          "citing_paper_url": "https://www.semanticscholar.org/paper/9211fa2e22f8ccdafc87245232c764f47ccdf7b9",
          "cited_paper_url": "https://www.semanticscholar.org/paper/2e89dd08b5ea206128724419e259c8750e8d1381",
          "citing_paper_year": 2023,
          "cited_paper_year": 2020
        }
      ]
    },
    {
      "cited_paper_id": "14542261",
      "citation_count": 0,
      "total_dataset_mentions": 2,
      "unique_datasets": [
        "CelebA"
      ],
      "dataset_details": [
        {
          "dataset_name": "CelebA",
          "dataset_description": "Used to test GDP's image restoration capabilities on 256x256 celebrity faces, assessing the method's ability to preserve facial details. | Used for qualitative evaluation of 25% inpainting and 4x super-resolution, focusing on visual quality and restoration accuracy in face images. | Used to evaluate GDP's performance in restoring 256x256 images, emphasizing the method's ability to handle diverse image types. | Used to demonstrate the effectiveness of GDP in restoring 256x256 images, focusing on image quality and restoration capabilities. | Used to evaluate the empirical effectiveness of GDP compared to unsupervised methods, focusing on consistency and FID scores in image restoration.",
          "citing_paper_id": "257921922",
          "cited_paper_id": 3568073,
          "context_text": "Qualitative results of (a) 25 % inpainting and (b) 4× super-resolution on CelebA [30].",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions 'CelebA' which is a well-known dataset used for face image processing tasks. The citation indicates its use for qualitative results in inpainting and super-resolution.",
          "citing_paper_doi": "10.1109/CVPR52729.2023.00958",
          "cited_paper_doi": null,
          "citing_paper_url": "https://www.semanticscholar.org/paper/5549dc3ceff07561d9fb59610c0f78c71617901a",
          "cited_paper_url": "https://www.semanticscholar.org/paper/744fe47157477235032f7bb3777800f9f2f45e52",
          "citing_paper_year": 2023,
          "cited_paper_year": 2017
        },
        {
          "dataset_name": "CelebA",
          "dataset_description": "Used in a downscaled version (64x64) to solve inverse problems, demonstrating the limitations of simple datasets in showing severe mode collapse. | Used to solve inverse problems with generative models, specifically a downscaled version (64x64), to highlight its simplicity and limitations in demonstrating mode collapse.",
          "citing_paper_id": "189762039",
          "cited_paper_id": 14542261,
          "context_text": "Note that previous works, which use generative models for solving inverse problems, have considered much simpler datasets, such as MNIST (Le-Cun et al. 1998) or a small version of CelebA (downscaled to size 64 × 64 ), which perhaps do not demonstrate how severe the effect of mode collapse is.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions MNIST and a small version of CelebA, both of which are datasets used in previous works for solving inverse problems. These datasets are used to highlight the simplicity of the data compared to the complexity required to demonstrate mode collapse.",
          "citing_paper_doi": "10.1609/AAAI.V34I04.5708",
          "cited_paper_doi": "10.1109/5.726791",
          "citing_paper_url": "https://www.semanticscholar.org/paper/944d54502895811d3b2c72d1a1d49a395588f67e",
          "cited_paper_url": "https://www.semanticscholar.org/paper/162d958ff885f1462aeda91cd72582323fd6a1f4",
          "citing_paper_year": 2019,
          "cited_paper_year": 1998
        }
      ]
    },
    {
      "cited_paper_id": "244461440",
      "citation_count": 0,
      "total_dataset_mentions": 2,
      "unique_datasets": [
        "AIR40K"
      ],
      "dataset_details": [
        {
          "dataset_name": "AIR40K",
          "dataset_description": "Used to evaluate rain removal algorithms on low-resolution images with light rain, focusing on noise reduction and clarity enhancement. | Used to benchmark rain removal algorithms on a small, curated dataset with challenging rain scenarios. | Used to train and test rain removal models, offering a large dataset with varied rain conditions and image complexities. | Used to test and validate rain removal techniques, providing a smaller but high-quality dataset with synthetic rain effects. | Used to train and evaluate snow removal algorithms, providing a large dataset with diverse snow conditions and image types. | Used to test and validate synthetic snow generation and removal methods, focusing on realistic snow simulation. | Used to evaluate image dehazing and deraining algorithms in indoor scenes, emphasizing the removal of atmospheric disturbances. | Used to assess the performance of rain removal methods on high-resolution images with heavy rain, emphasizing detail preservation. | Used to train and evaluate rain removal algorithms, focusing on diverse rain patterns and intensities in real-world images. | Used to test and validate outdoor scene dehazing and deraining techniques, focusing on natural lighting and environmental conditions. | Used to assess the performance of haze and rain removal algorithms in synthetic and real-world images, emphasizing robustness and generalization. | Used to compile a comprehensive all-in-one image restoration dataset, integrating various rain and snow removal datasets for training and evaluation. | Used to develop and test raindrop removal techniques, focusing on the unique challenges posed by raindrops on surfaces.",
          "citing_paper_id": "260271048",
          "cited_paper_id": 15443600,
          "context_text": "We collect an all-in-one image restoration dataset named AIR40K, including Rain14000 [26], Rain800 [79], Rain100H [80], Rain100L [80], Rain1200 [33], Rain12 [81], Raindrop [43], Snow100K [27], CSD [10], ITS [34], OTS [34], HSTS [34].",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions multiple datasets used for image restoration, which are specific and relevant to the research topic.",
          "citing_paper_doi": "10.1109/TCSVT.2023.3299324",
          "cited_paper_doi": "10.1109/CVPR.2017.183",
          "citing_paper_url": "https://www.semanticscholar.org/paper/2a828c28938b196fff7b31e58a08853daaaa8b96",
          "cited_paper_url": "https://www.semanticscholar.org/paper/6de27f256e97c014a95706c72af7fa651e8fa34a",
          "citing_paper_year": 2024,
          "cited_paper_year": 2016
        },
        {
          "dataset_name": "AIR40K",
          "dataset_description": "Used to evaluate rain removal algorithms on low-resolution images with light rain, focusing on noise reduction and clarity enhancement. | Used to benchmark rain removal algorithms on a small, curated dataset with challenging rain scenarios. | Used to evaluate the desnowing performance of the AIRFormer model, focusing on single image desnowing using hierarchical dual-tree complex wavelet representation and contradict channel loss. | Used to train and test rain removal models, offering a large dataset with varied rain conditions and image complexities. | Used to test and validate rain removal techniques, providing a smaller but high-quality dataset with synthetic rain effects. | Used to train and evaluate snow removal algorithms, providing a large dataset with diverse snow conditions and image types. | Used to evaluate the desnowing performance of the AIRFormer model, providing a complementary test set for assessing the effectiveness of the desnowing algorithm. | Used to test and validate synthetic snow generation and removal methods, focusing on realistic snow simulation. | Used to evaluate image dehazing and deraining algorithms in indoor scenes, emphasizing the removal of atmospheric disturbances. | Used to assess the performance of rain removal methods on high-resolution images with heavy rain, emphasizing detail preservation. | Used to train and evaluate rain removal algorithms, focusing on diverse rain patterns and intensities in real-world images. | Used to test and validate outdoor scene dehazing and deraining techniques, focusing on natural lighting and environmental conditions. | Used to assess the performance of haze and rain removal algorithms in synthetic and real-world images, emphasizing robustness and generalization. | Used to compile a comprehensive all-in-one image restoration dataset, integrating various rain and snow removal datasets for training and evaluation. | Used to develop and test raindrop removal techniques, focusing on the unique challenges posed by raindrops on surfaces.",
          "citing_paper_id": "260271048",
          "cited_paper_id": 244461440,
          "context_text": "We collect an all-in-one image restoration dataset named AIR40K, including Rain14000 [26], Rain800 [79], Rain100H [80], Rain100L [80], Rain1200 [33], Rain12 [81], Raindrop [43], Snow100K [27], CSD [10], ITS [34], OTS [34], HSTS [34].",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions multiple datasets used for image restoration, which are specific and relevant to the research topic.",
          "citing_paper_doi": "10.1109/TCSVT.2023.3299324",
          "cited_paper_doi": "10.1109/ICCV48922.2021.00416",
          "citing_paper_url": "https://www.semanticscholar.org/paper/2a828c28938b196fff7b31e58a08853daaaa8b96",
          "cited_paper_url": "https://www.semanticscholar.org/paper/51b09bddafff606138fe3e69acb9121f3415aad9",
          "citing_paper_year": 2024,
          "cited_paper_year": 2021
        }
      ]
    },
    {
      "cited_paper_id": "26789744",
      "citation_count": 0,
      "total_dataset_mentions": 2,
      "unique_datasets": [
        "TURBID"
      ],
      "dataset_details": [
        {
          "dataset_name": "TURBID",
          "dataset_description": "Used to evaluate underwater image enhancement methods, specifically comparing Euclidean distance metrics across various techniques including Retinex, CLAHE, FUnIE-GAN, Water-Net, UGAN, and DGD-cGAN. | Used to evaluate underwater image enhancement methods, comparing results against ground truth images using various restoration techniques. | Used to evaluate underwater image enhancement methods, comparing results from various models including Retinex, CLAHE, FUnIE-GAN, Water-Net, UGAN, and DGD-cGAN. | Used to train a generative adversarial network by generating paired images, focusing on enhancing underwater imagery through CycleGAN. | Used to evaluate underwater image enhancement methods, providing high-resolution images for benchmarking various restoration techniques.",
          "citing_paper_id": "253708090",
          "cited_paper_id": 26789744,
          "context_text": "Table 1: Euclidean distance (ED) [66, 67] for TURBID dataset [58] yielded by Retinex [72], CLAHE [73], FUnIE-GAN [15], Water-Net [35], UGAN [32] and DGD-cGAN.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions the TURBID dataset, which is used for evaluating image enhancement methods. The dataset is relevant to the topic of image restoration.",
          "citing_paper_doi": "10.48550/arXiv.2211.10026",
          "cited_paper_doi": "10.1109/ICRA.2018.8460552",
          "citing_paper_url": "https://www.semanticscholar.org/paper/29412f7b8cff20da3221937e066619aaa5f78655",
          "cited_paper_url": "https://www.semanticscholar.org/paper/7180c41111877b304bc131c3235f6c20067a0466",
          "citing_paper_year": 2022,
          "cited_paper_year": 2018
        },
        {
          "dataset_name": "TURBID",
          "dataset_description": "Used to evaluate image restoration methods, focusing on PSNR, SSIM, and ED metrics to assess error visibility and structural similarity. | Used to evaluate image restoration performance, focusing on PSNR and SSIM metrics to assess the quality of restored images.",
          "citing_paper_id": "253708090",
          "cited_paper_id": 207761262,
          "context_text": "In Table 2, we show the corresponding results of PSNR and SSIM for the TURBID dataset [58], with the absolute best results denoted in bold.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions the TURBID dataset, which is used to evaluate image restoration performance using PSNR and SSIM metrics.",
          "citing_paper_doi": "10.48550/arXiv.2211.10026",
          "cited_paper_doi": "10.1109/TIP.2003.819861",
          "citing_paper_url": "https://www.semanticscholar.org/paper/29412f7b8cff20da3221937e066619aaa5f78655",
          "cited_paper_url": "https://www.semanticscholar.org/paper/eae2e0fa72e898c289365c0af16daf57a7a6cf40",
          "citing_paper_year": 2022,
          "cited_paper_year": 2004
        }
      ]
    },
    {
      "cited_paper_id": "64193",
      "citation_count": 0,
      "total_dataset_mentions": 2,
      "unique_datasets": [
        "ImageNet 1K"
      ],
      "dataset_details": [
        {
          "dataset_name": "ImageNet 1K",
          "dataset_description": "Used for evaluating face super-resolution, focusing on high-quality facial details and realistic face reconstruction. | Commonly used for evaluating super-resolution, providing a diverse set of natural images for benchmarking. | Commonly used for evaluating super-resolution, providing urban scenes to test the ability to reconstruct complex textures and structures. | Used for evaluating natural image super-resolution, providing a diverse set of images to test the generalizability of SR models. | Commonly used for evaluating super-resolution, focusing on manga images to test the ability to reconstruct line art and text. | Commonly used for evaluating super-resolution, providing a small but widely recognized set of images for benchmarking. | Commonly used for evaluating super-resolution, offering a slightly larger set of images for more comprehensive testing.",
          "citing_paper_id": "261031011",
          "cited_paper_id": 64193,
          "context_text": "…the test process, it utilizes the ImageNet 1K [263] for the evaluation of natural image SR and CelebA-HQ for face SR. Depart from that, a series of works also introduce the commonly-used SR testing dataset for evaluation, such as Set5 [46], Set14 [47], BSD100 [264], Manga109 [265], Ur-ban100 [266].",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions several datasets used for evaluating super-resolution (SR) techniques, which are directly relevant to the topic of image restoration.",
          "citing_paper_doi": "10.48550/arXiv.2308.09388",
          "cited_paper_doi": "10.1109/ICCV.2001.937655",
          "citing_paper_url": "https://www.semanticscholar.org/paper/70ba95384664ab7becfd8196d20a46fe66112c91",
          "cited_paper_url": "https://www.semanticscholar.org/paper/9a1ed876196ec9733acb1daa6d65e35ff0414291",
          "citing_paper_year": 2023,
          "cited_paper_year": 2001
        },
        {
          "dataset_name": "ImageNet 1K",
          "dataset_description": "Used to evaluate zero-shot DM-based IR methods on the Gaussian deblurring task, focusing on image restoration performance. | Used to test 4x super-resolution models, focusing on facial detail and texture restoration in celebrity images. | Used to test 4x super-resolution models, focusing on image quality and restoration performance across diverse categories.",
          "citing_paper_id": "261031011",
          "cited_paper_id": 459456,
          "context_text": "We test six open-source diffusion models for 4x super-resolution on two datasets: ImageNet 1K [263] and CelebA 1K [294].",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions two specific datasets used for testing super-resolution models. Both datasets are well-known and have clear identifiers.",
          "citing_paper_doi": "10.48550/arXiv.2308.09388",
          "cited_paper_doi": "10.1109/ICCV.2015.425",
          "citing_paper_url": "https://www.semanticscholar.org/paper/70ba95384664ab7becfd8196d20a46fe66112c91",
          "cited_paper_url": "https://www.semanticscholar.org/paper/6424b69f3ff4d35249c0bb7ef912fbc2c86f4ff4",
          "citing_paper_year": 2023,
          "cited_paper_year": 2014
        }
      ]
    },
    {
      "cited_paper_id": "7023610",
      "citation_count": 0,
      "total_dataset_mentions": 2,
      "unique_datasets": [
        "ImageNet ctest10k"
      ],
      "dataset_details": [
        {
          "dataset_name": "ImageNet ctest10k",
          "dataset_description": "Used for evaluating image restoration models, specifically focusing on the 10,000 image subset from the ImageNet validation set to assess colorization performance. | Used to benchmark the performance of the Palette model on image colorization, specifically evaluating its effectiveness on a challenging test set of 10,000 images. | Used as a standard subset for benchmarking image-to-image translation tasks on ImageNet, focusing on evaluating model performance across various restoration challenges. | Used to benchmark the performance of the Palette model on colorization tasks, focusing on the quality of colorized images compared to ground truth. | Used for evaluating image restoration models, specifically focusing on the 10,000 image subset from the ImageNet validation set.",
          "citing_paper_id": "243938678",
          "cited_paper_id": 7023610,
          "context_text": "For benchmarking purposes, we also report the performance of Palette on ImageNet ctest10k [Larsson et al. 2016] dataset in Table C.1.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions the 'ImageNet ctest10k' dataset, which is a specific subset of ImageNet used for colorization tasks. The dataset is used for benchmarking the performance of the Palette model.",
          "citing_paper_doi": "10.1145/3528233.3530757",
          "cited_paper_doi": "10.1007/978-3-319-46493-0_35",
          "citing_paper_url": "https://www.semanticscholar.org/paper/37c9c4e7648f639c0b36f150fc6c6c90b3682f4a",
          "cited_paper_url": "https://www.semanticscholar.org/paper/8e63784bd5a24d5e3035e2a11753e65e6e56625d",
          "citing_paper_year": 2021,
          "cited_paper_year": 2016
        },
        {
          "dataset_name": "ImageNet ctest10k",
          "dataset_description": "Used for benchmarking image restoration models, specifically evaluating the performance of DeepFillv2 and HiFill on ultra high-resolution image inpainting tasks.",
          "citing_paper_id": "243938678",
          "cited_paper_id": 218718746,
          "context_text": "It is important to note that DeepFillv2 and HiFill are not trained on ImageNet, but we report their performance on ImageNet ctest10k primarily for benchmarking purposes.",
          "confidence_score": 1.0,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions ImageNet ctest10k, which is a specific subset of the ImageNet dataset used for benchmarking purposes.",
          "citing_paper_doi": "10.1145/3528233.3530757",
          "cited_paper_doi": "10.1109/cvpr42600.2020.00753",
          "citing_paper_url": "https://www.semanticscholar.org/paper/37c9c4e7648f639c0b36f150fc6c6c90b3682f4a",
          "cited_paper_url": "https://www.semanticscholar.org/paper/11e9d4849c545a7523c52c9b3a92811a32ffb624",
          "citing_paper_year": 2021,
          "cited_paper_year": 2020
        }
      ]
    },
    {
      "cited_paper_id": "14113767",
      "citation_count": 0,
      "total_dataset_mentions": 2,
      "unique_datasets": [
        "MS-COCO"
      ],
      "dataset_details": [
        {
          "dataset_name": "MS-COCO",
          "dataset_description": "Used for training and evaluating JPEG restoration, offering a comprehensive dataset for benchmarking image restoration algorithms. | Used for training models in recent works, providing a large-scale dataset for image restoration tasks.",
          "citing_paper_id": "243938678",
          "cited_paper_id": 86672387,
          "context_text": "Recent works such as [Galteri et al. 2019] use a relatively larger MS-COCO dataset for training, however, to the best of our knowledge, we are the first to train and evaluate JPEG restoration on ImageNet.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions the use of the MS-COCO and ImageNet datasets for training and evaluation, respectively. These are specific, verifiable datasets used in the research.",
          "citing_paper_doi": "10.1145/3528233.3530757",
          "cited_paper_doi": "10.1109/TMM.2019.2895280",
          "citing_paper_url": "https://www.semanticscholar.org/paper/37c9c4e7648f639c0b36f150fc6c6c90b3682f4a",
          "cited_paper_url": "https://www.semanticscholar.org/paper/b937b188ab959171d6c0a72fc21d2f388b5a3a13",
          "citing_paper_year": 2021,
          "cited_paper_year": 2019
        },
        {
          "dataset_name": "MS-COCO",
          "dataset_description": "Used to select 3000 pairs of images and semantic prompts for validating the image restoration model, focusing on common objects in context.",
          "citing_paper_id": "266362278",
          "cited_paper_id": 14113767,
          "context_text": "We randomly choose 3000 pairs of image x and semantic prompt c s from the MS-COCO [33] validation set.",
          "confidence_score": 1.0,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "MS-COCO is a well-known dataset used for various computer vision tasks, including image restoration. The citation indicates it is used for selecting image-prompt pairs for validation.",
          "citing_paper_doi": "10.1007/978-3-031-73661-2_25",
          "cited_paper_doi": "10.1007/978-3-319-10602-1_48",
          "citing_paper_url": "https://www.semanticscholar.org/paper/8dc8ae93e9fe95de8c816a81bc087031d689bdcc",
          "cited_paper_url": "https://www.semanticscholar.org/paper/71b7178df5d2b112d07e45038cb5637208659ff7",
          "citing_paper_year": 2023,
          "cited_paper_year": 2014
        }
      ]
    },
    {
      "cited_paper_id": "246904639",
      "citation_count": 0,
      "total_dataset_mentions": 2,
      "unique_datasets": [
        "RSBlur"
      ],
      "dataset_details": [
        {
          "dataset_name": "RSBlur",
          "dataset_description": "Used to evaluate the model on real-world blurred images, assessing the robustness and generalization of the deblurring algorithm. | Used to evaluate the performance of the image deblurring model, focusing on realistic blur synthesis and deblurring in real-world scenarios. | Used for numerical comparisons in image motion deblurring, focusing on real-world scenarios to evaluate deblurring algorithms. | Used to evaluate image deblurring performance, focusing on realistic blur synthesis and deblurring techniques. | Used to train and evaluate image deblurring models, focusing on realistic blur synthesis and real-world scenarios. | Used to evaluate the model on synthetic blur images, focusing on the effectiveness of the deblurring algorithm in a controlled environment.",
          "citing_paper_id": "260927679",
          "cited_paper_id": 246904639,
          "context_text": "Image motion deblurring numerical comparisons on the lately proposed real-world dataset RSBlur (Rim et al., 2022).",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions a specific dataset 'RSBlur' which is used for image motion deblurring numerical comparisons. The dataset is clearly identified and relevant to the research topic.",
          "citing_paper_doi": null,
          "cited_paper_doi": "10.1007/978-3-031-20071-7_29",
          "citing_paper_url": "https://www.semanticscholar.org/paper/3fb08a1c07eba3461d6ed784b25d952979e90e0d",
          "cited_paper_url": "https://www.semanticscholar.org/paper/5ed43eddae1d4d9e653d5a498ad8dcb0dc5cc1af",
          "citing_paper_year": 2023,
          "cited_paper_year": 2022
        },
        {
          "dataset_name": "RSBlur",
          "dataset_description": "Used to evaluate image deblurring performance, focusing on realistic blur synthesis in real-world scenarios. | Used to assess the model's performance on real-world deblurring challenges, emphasizing its capability to handle authentic blur variations. | Used to evaluate the model on synthetic deblurring tasks, focusing on the effectiveness of the restoration algorithm in a controlled environment.",
          "citing_paper_id": "270736284",
          "cited_paper_id": 246904639,
          "context_text": "In addition, we report the results on the real-world RS-Blur [101] in Table XI.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "RS-Blur is mentioned as a dataset used for reporting results, and the cited paper title confirms it is a dataset for image deblurring.",
          "citing_paper_doi": "10.1109/TPAMI.2024.3419007",
          "cited_paper_doi": "10.1007/978-3-031-20071-7_29",
          "citing_paper_url": "https://www.semanticscholar.org/paper/804d2302880b91965e84a7c72ac26429d26792a0",
          "cited_paper_url": "https://www.semanticscholar.org/paper/5ed43eddae1d4d9e653d5a498ad8dcb0dc5cc1af",
          "citing_paper_year": 2024,
          "cited_paper_year": 2022
        }
      ]
    },
    {
      "cited_paper_id": "201106503",
      "citation_count": 0,
      "total_dataset_mentions": 2,
      "unique_datasets": [
        "Places2"
      ],
      "dataset_details": [
        {
          "dataset_name": "Places2",
          "dataset_description": "Used to train uncropping methods, focusing on image extension and outpainting techniques. The dataset provides diverse subsets for training generative adversarial networks. | Used to evaluate the performance of the proposed method against the uncropping method Boundless, focusing on top-50 categories for image extension tasks. | Used for evaluating image extension performance, focusing on generative adversarial networks and their ability to extend images.",
          "citing_paper_id": "243938678",
          "cited_paper_id": 201106503,
          "context_text": "3: Comparison with uncropping method Boundless [Teterwak et al., 2019] on top-50 Places2 categories.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions 'Places2 categories' which is a known dataset used for image restoration tasks. The citation is used to compare methods, indicating the dataset is used for evaluation.",
          "citing_paper_doi": "10.1145/3528233.3530757",
          "cited_paper_doi": "10.1109/ICCV.2019.01062",
          "citing_paper_url": "https://www.semanticscholar.org/paper/37c9c4e7648f639c0b36f150fc6c6c90b3682f4a",
          "cited_paper_url": "https://www.semanticscholar.org/paper/05355e895988589f19d67ed5de7c727a7de706c7",
          "citing_paper_year": 2021,
          "cited_paper_year": 2019
        },
        {
          "dataset_name": "Places2",
          "dataset_description": "Used to train uncropping methods, focusing on image extension and outpainting techniques. The dataset provides diverse subsets for training generative adversarial networks.",
          "citing_paper_id": "243938678",
          "cited_paper_id": 232478397,
          "context_text": "C.3 Uncropping Many existing uncropping methods [Cheng et al. 2021; Teterwak et al. 2019] have been trained on different subsets of Places2 [Zhou et al. 2017] dataset.",
          "confidence_score": 1.0,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions the Places2 dataset, which is a specific, verifiable dataset used for training uncropping methods.",
          "citing_paper_doi": "10.1145/3528233.3530757",
          "cited_paper_doi": "10.1109/CVPR52688.2022.01114",
          "citing_paper_url": "https://www.semanticscholar.org/paper/37c9c4e7648f639c0b36f150fc6c6c90b3682f4a",
          "cited_paper_url": "https://www.semanticscholar.org/paper/089b1ebb1d8fa8b9a7058954e2dc4d70507bd60f",
          "citing_paper_year": 2021,
          "cited_paper_year": 2021
        }
      ]
    },
    {
      "cited_paper_id": "261276317",
      "citation_count": 0,
      "total_dataset_mentions": 2,
      "unique_datasets": [
        "RealSet80"
      ],
      "dataset_details": [
        {
          "dataset_name": "RealSet80",
          "dataset_description": "Used to evaluate image restoration methods, focusing on low-quality images commonly referenced in recent literature. The dataset supports the development and testing of blind image restoration techniques.",
          "citing_paper_id": "268364340",
          "cited_paper_id": 4676177,
          "context_text": "Additionally, we collect another real-world dataset named RealSet80: It comprises 50 LQ images widely used in recent literature [39], [81], [99], [100], [101], [102].",
          "confidence_score": 1.0,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions a specific dataset named 'RealSet80' which is used for image restoration research. The dataset is described as comprising 50 LQ images widely used in recent literature.",
          "citing_paper_doi": "10.1109/TPAMI.2024.3461721",
          "cited_paper_doi": "10.1109/ICCV.2017.355",
          "citing_paper_url": "https://www.semanticscholar.org/paper/decac28a79f9cbb5be2658f35a6cdb5bc705c2bc",
          "cited_paper_url": "https://www.semanticscholar.org/paper/471f9ee62b8b1a744df61e28b239988c0f57fdd0",
          "citing_paper_year": 2024,
          "cited_paper_year": 2017
        },
        {
          "dataset_name": "RealSet80",
          "dataset_description": "Used to evaluate image restoration methods, focusing on low-quality images commonly referenced in recent literature. The dataset supports the development and testing of blind image restoration techniques.",
          "citing_paper_id": "268364340",
          "cited_paper_id": 261276317,
          "context_text": "Additionally, we collect another real-world dataset named RealSet80: It comprises 50 LQ images widely used in recent literature [39], [81], [99], [100], [101], [102].",
          "confidence_score": 1.0,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions a specific dataset named 'RealSet80' which is used for image restoration research. The dataset is described as comprising 50 LQ images widely used in recent literature.",
          "citing_paper_doi": "10.1109/TPAMI.2024.3461721",
          "cited_paper_doi": "10.48550/arXiv.2308.15070",
          "citing_paper_url": "https://www.semanticscholar.org/paper/decac28a79f9cbb5be2658f35a6cdb5bc705c2bc",
          "cited_paper_url": "https://www.semanticscholar.org/paper/858f0643110ccccb6a9103747f2169fecfb98668",
          "citing_paper_year": 2024,
          "cited_paper_year": 2023
        }
      ]
    },
    {
      "cited_paper_id": "218538083",
      "citation_count": 0,
      "total_dataset_mentions": 2,
      "unique_datasets": [
        "O-HAZE"
      ],
      "dataset_details": [
        {
          "dataset_name": "O-HAZE",
          "dataset_description": "Used to assess dehazing techniques on non-homogeneous hazy and haze-free images, emphasizing variability in haze distribution. | Used to benchmark single-image dehazing methods, focusing on dense-haze and haze-free images to evaluate performance under challenging conditions. | Used to assess the model's effectiveness in dehazing non-homogeneous hazy images, emphasizing the ability to handle varying haze densities. | Used to evaluate dehazing algorithms on real hazy and haze-free outdoor images, providing a realistic testbed for assessing algorithm robustness. | Used to test the model's capability in dehazing real outdoor images, ensuring robustness across diverse environmental conditions. | Used to evaluate the model's performance in dehazing dense haze conditions, focusing on the robustness in challenging real-world scenarios.",
          "citing_paper_id": "265537221",
          "cited_paper_id": 102350705,
          "context_text": "We report the quantitative performance of image dehazing approaches on both syn-13005 thetic [27] and real-world [2, 3, 4] datasets in Table 1.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions synthetic and real-world datasets used for evaluating image dehazing approaches. The cited papers provide specific names for these datasets.",
          "citing_paper_doi": "10.1109/ICCV51070.2023.01195",
          "cited_paper_doi": "10.1109/ICIP.2019.8803046",
          "citing_paper_url": "https://www.semanticscholar.org/paper/1e277206e1a59e7e22f1fdcab886d3869cdd0871",
          "cited_paper_url": "https://www.semanticscholar.org/paper/a3be7b43518abc11a074e6c15d1b5c443e53737c",
          "citing_paper_year": 2023,
          "cited_paper_year": 2019
        },
        {
          "dataset_name": "O-HAZE",
          "dataset_description": "Used to assess dehazing techniques on non-homogeneous hazy and haze-free images, emphasizing variability in haze distribution. | Used to assess the model's effectiveness in dehazing non-homogeneous hazy images, emphasizing the ability to handle varying haze densities. | Used to benchmark single-image dehazing methods, focusing on dense-haze and haze-free images to evaluate performance under challenging conditions. | Used to evaluate dehazing algorithms on real hazy and haze-free outdoor images, providing a realistic testbed for assessing algorithm robustness. | Used to test the model's capability in dehazing real outdoor images, ensuring robustness across diverse environmental conditions. | Used to evaluate the model's performance in dehazing dense haze conditions, focusing on the robustness in challenging real-world scenarios.",
          "citing_paper_id": "265537221",
          "cited_paper_id": 218538083,
          "context_text": "We report the quantitative performance of image dehazing approaches on both syn-13005 thetic [27] and real-world [2, 3, 4] datasets in Table 1.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions synthetic and real-world datasets used for evaluating image dehazing approaches. The cited papers provide specific names for these datasets.",
          "citing_paper_doi": "10.1109/ICCV51070.2023.01195",
          "cited_paper_doi": "10.1109/CVPRW50498.2020.00230",
          "citing_paper_url": "https://www.semanticscholar.org/paper/1e277206e1a59e7e22f1fdcab886d3869cdd0871",
          "cited_paper_url": "https://www.semanticscholar.org/paper/88e8a624825f0fb457fac26f65d43d9da76beab0",
          "citing_paper_year": 2023,
          "cited_paper_year": 2020
        }
      ]
    },
    {
      "cited_paper_id": "253523895",
      "citation_count": 0,
      "total_dataset_mentions": 2,
      "unique_datasets": [
        "RESIDE-Indoor"
      ],
      "dataset_details": [
        {
          "dataset_name": "RESIDE-Indoor",
          "dataset_description": "Used for evaluating the trained models on indoor scenes, providing a benchmark for indoor dehazing performance. | Used to evaluate the performance of the model in single-image dehazing, specifically measuring PSNR improvement over the DeHamer model with significantly fewer parameters. | Used for training models focused on outdoor scenes, assessing the performance of dehazing algorithms in diverse outdoor conditions. | Used for training models focused on indoor scenes, evaluating the effectiveness of dehazing algorithms in controlled environments. | Used for evaluating the trained models on outdoor scenes, serving as a benchmark for outdoor dehazing performance.",
          "citing_paper_id": "265537221",
          "cited_paper_id": 250085592,
          "context_text": "Following [21, 59], we train separate models on the RESIDE-Indoor and RESIDE-Outdoor datasets [27], and evaluate the resulting models on the corresponding test sets of RESIDE, i.e., SOTS-Indoor and SOTS-Outdoor, respectively.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions specific datasets used for training and evaluation in the context of image dehazing, which is relevant to all-in-one image restoration.",
          "citing_paper_doi": "10.1109/ICCV51070.2023.01195",
          "cited_paper_doi": "10.1109/CVPR52688.2022.00572",
          "citing_paper_url": "https://www.semanticscholar.org/paper/1e277206e1a59e7e22f1fdcab886d3869cdd0871",
          "cited_paper_url": "https://www.semanticscholar.org/paper/36169ad27b5ef7a7c664bf9b01d334a4fc0bdf52",
          "citing_paper_year": 2023,
          "cited_paper_year": 2022
        },
        {
          "dataset_name": "RESIDE-Indoor",
          "dataset_description": "Used to evaluate the performance of FocalNet in image dehazing, specifically comparing PSNR and SSIM metrics against PMNet. | Used for evaluating the trained models on indoor scenes, providing a benchmark for indoor dehazing performance. | Used to evaluate the performance of the proposed method in image dehazing, comparing PSNR, parameters, and FLOPs against PMNet. | Used for training models focused on outdoor scenes, assessing the performance of dehazing algorithms in diverse outdoor conditions. | Used for training models focused on indoor scenes, evaluating the effectiveness of dehazing algorithms in controlled environments. | Used for evaluating the trained models on outdoor scenes, serving as a benchmark for outdoor dehazing performance.",
          "citing_paper_id": "265537221",
          "cited_paper_id": 253523895,
          "context_text": "Following [21, 59], we train separate models on the RESIDE-Indoor and RESIDE-Outdoor datasets [27], and evaluate the resulting models on the corresponding test sets of RESIDE, i.e., SOTS-Indoor and SOTS-Outdoor, respectively.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions specific datasets used for training and evaluation in the context of image dehazing, which is relevant to all-in-one image restoration.",
          "citing_paper_doi": "10.1109/ICCV51070.2023.01195",
          "cited_paper_doi": "10.1007/978-3-031-19800-7_8",
          "citing_paper_url": "https://www.semanticscholar.org/paper/1e277206e1a59e7e22f1fdcab886d3869cdd0871",
          "cited_paper_url": "https://www.semanticscholar.org/paper/3309d78b6dfb03629c5ea2b8424a12142bb34c5e",
          "citing_paper_year": 2023,
          "cited_paper_year": 2022
        }
      ]
    },
    {
      "cited_paper_id": "53416878",
      "citation_count": 0,
      "total_dataset_mentions": 2,
      "unique_datasets": [
        "PIPAL full set"
      ],
      "dataset_details": [
        {
          "dataset_name": "PIPAL full set",
          "dataset_description": "Used to evaluate the performance of anti-aliasing pooling layers, focusing on image quality assessment metrics. | Used to assess the impact of anti-aliasing pooling layers on GAN-generated images, specifically evaluating distortion effects.",
          "citing_paper_id": "220713808",
          "cited_paper_id": 53416878,
          "context_text": "The anti-aliasing pooling layers (l2 pooling [11] and BlurPool [59]) improve the performance both on the PIPAL full set and GAN-based distortion subset",
          "confidence_score": 0.9,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions 'PIPAL full set' and 'GAN-based distortion subset', which appear to be specific datasets used for evaluating the performance of anti-aliasing pooling layers.",
          "citing_paper_doi": "10.1007/978-3-030-58621-8_37",
          "cited_paper_doi": null,
          "citing_paper_url": "https://www.semanticscholar.org/paper/d49cac9f62ec2c4b90332b45edadfed1946e75b0",
          "cited_paper_url": "https://www.semanticscholar.org/paper/8c92054c26fb4c6dd7435bc99fbb8af3323eae1b",
          "citing_paper_year": 2020,
          "cited_paper_year": 2019
        },
        {
          "dataset_name": "PIPAL full set",
          "dataset_description": "Used to assess the effectiveness of anti-aliasing pooling layers on GAN-generated distortions, specifically examining the impact on synthetic image quality. | Used to evaluate the performance of anti-aliasing pooling layers in image quality assessment, focusing on full reference image distortions.",
          "citing_paper_id": "220713808",
          "cited_paper_id": 215785896,
          "context_text": "The anti-aliasing pooling layers (l2 pooling [11] and BlurPool [59]) improve the performance both on the PIPAL full set and GAN-based distortion subset based distortion.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The citation mentions 'PIPAL full set' and 'GAN-based distortion subset', which are specific datasets used for evaluating image quality assessment methods.",
          "citing_paper_doi": "10.1007/978-3-030-58621-8_37",
          "cited_paper_doi": "10.1109/TPAMI.2020.3045810",
          "citing_paper_url": "https://www.semanticscholar.org/paper/d49cac9f62ec2c4b90332b45edadfed1946e75b0",
          "cited_paper_url": "https://www.semanticscholar.org/paper/f0f6998fb5a8f88c003860ca06f6ecaf6b5feec3",
          "citing_paper_year": 2020,
          "cited_paper_year": 2020
        }
      ]
    },
    {
      "cited_paper_id": "91184545",
      "citation_count": 0,
      "total_dataset_mentions": 2,
      "unique_datasets": [
        "Rain200H"
      ],
      "dataset_details": [
        {
          "dataset_name": "Rain200H",
          "dataset_description": "Used to train the model for 200 epochs, focusing on high-resolution synthetic rain images for deraining. | Used to train the model for 750 epochs, focusing on real raindrop images for raindrop removal. | Used to train the model for 500 epochs, focusing on removing raindrops from images using a large, high-quality real rain dataset. | Used to evaluate rain removal performance on high-resolution synthetic images, focusing on PSNR and SSIM metrics. | Used to train the model for 500 epochs, focusing on low-resolution synthetic rain images for deraining. | Used for evaluating the performance of the proposed method and comparable models, specifically assessing deraining quality and accuracy. | Used as a real-world testing image set to evaluate the performance of deraining models, focusing on capturing authentic rain streak properties. | Used to train and evaluate rain removal models, focusing on synthetic rain images with diverse scenes and rain intensities. | Used to evaluate rain removal performance on real-world images, focusing on PSNR and SSIM metrics. | Used to train the model for 500 epochs, focusing on high-resolution synthetic rain images for deraining. | Used for evaluating rain removal algorithms, focusing on high-density rain scenarios in single images. | Used to train the model for 100 epochs, offering a diverse set of synthetic rain images for deraining. | Used to evaluate deraining performance on real-world rain images, focusing on the method's effectiveness in practical scenarios. | Used to evaluate rain removal performance on an extensive set of synthetic images, focusing on PSNR and SSIM metrics. | Used to evaluate deraining performance on real-world rain images, assessing the method's ability to improve image quality and clarity. | Used for training models in single-image deraining, focusing on high-quality real rain data to improve deraining performance. | Used to train the model for 5 epochs, providing a specialized dataset for single-image deraining. | Used to train and evaluate rain removal models, focusing on synthetic rain images with high resolution and varied weather conditions. | Used to train and evaluate rain removal models, focusing on real-world rain images with high quality and diverse environmental conditions. | Used to train the model for 200 epochs, providing a large set of synthetic rain images for deraining. | Used to evaluate deraining performance on a larger set of synthetic rain images, testing the robustness of the method across diverse scenarios. | Used to evaluate deraining performance on an extensive set of synthetic rain images, providing a comprehensive test of the method's generalization capabilities. | Used to evaluate rain removal performance on a larger set of synthetic images, focusing on PSNR and SSIM metrics. | Used to evaluate rain removal performance on low-resolution synthetic images, focusing on PSNR and SSIM metrics. | Used to train and evaluate rain removal models, focusing on synthetic rain images of cityscapes with realistic rain effects. | Used to evaluate deraining performance on synthetic rain images of cityscapes, assessing the method's ability to preserve scene details and remove rain. | Used to evaluate the performance of MCW-Net in single-image deraining, comparing it against other state-of-the-art methods. | Used to evaluate deraining performance on high-resolution synthetic rain images, focusing on the effectiveness of the proposed method in removing rain streaks. | Used to evaluate deraining performance on low-resolution synthetic rain images, assessing the method's ability to handle varying rain densities. | Used to train the model for 500 epochs, providing a large set of synthetic rain images for deraining. | Used for evaluating rain removal algorithms, focusing on low-density rain scenarios in single images. | Used to train the model for 200 epochs, focusing on low-resolution synthetic rain images for deraining. | Used to train the model for 100 epochs, containing real-world cityscape images with synthetic rain for deraining. | Used to guide the model in capturing the properties of authentic rain streaks, enhancing the realism of deraining results. | Used to train and evaluate rain removal models, focusing on synthetic rain images with a large number of samples and varied rain types. | Used for spatial attentive single-image deraining, providing a high-quality real rain dataset for training and evaluation. | Used to evaluate deraining performance on real-world rain images, comparing the proposed method against existing approaches. | Used to train the model for 3 epochs, providing a specialized dataset for single-image deraining.",
          "citing_paper_id": "247940251",
          "cited_paper_id": 91184545,
          "context_text": "For the large model, we set the learning rate to be 10−4 and train our model for 200 epochs on the Rain200H, Rain200L, and Rain800 datasets, 100 epochs on the Rain1200 and the RainCityscapes datasets, 3 epochs on the SPA-Data dataset, and 500 epochs on the Raindrop dataset.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions several datasets used for training a model for rain removal from images, which is directly relevant to the topic of image restoration.",
          "citing_paper_doi": "10.1016/j.image.2022.116701",
          "cited_paper_doi": "10.1109/CVPR.2019.01255",
          "citing_paper_url": "https://www.semanticscholar.org/paper/a52f1afdd11d9a902729ecb1e5be852470756711",
          "cited_paper_url": "https://www.semanticscholar.org/paper/d3fa312b1f12ee60391705d3cac34cbcad42db14",
          "citing_paper_year": 2020,
          "cited_paper_year": 2019
        },
        {
          "dataset_name": "Rain200H",
          "dataset_description": "Used to evaluate rain removal performance on a larger set of synthetic images, focusing on PSNR and SSIM metrics. | Used for evaluating rain removal algorithms, focusing on low-density rain scenarios in single images. | Used to evaluate rain removal performance on low-resolution synthetic images, focusing on PSNR and SSIM metrics. | Used to evaluate rain removal performance on an extensive set of synthetic images, focusing on PSNR and SSIM metrics. | Used to evaluate rain removal performance on high-resolution synthetic images, focusing on PSNR and SSIM metrics. | Used to train and evaluate rain removal models, focusing on synthetic rain images of cityscapes with realistic rain effects. | Used to train and evaluate rain removal models, focusing on synthetic rain images with a large number of samples and varied rain types. | Used for spatial attentive single-image deraining, providing a high-quality real rain dataset for training and evaluation. | Used to train and evaluate rain removal models, focusing on synthetic rain images with diverse scenes and rain intensities. | Used to evaluate rain removal performance on real-world images, focusing on PSNR and SSIM metrics. | Used for evaluating rain removal algorithms, focusing on high-density rain scenarios in single images. | Used to train and evaluate rain removal models, focusing on synthetic rain images with high resolution and varied weather conditions. | Used to train and evaluate rain removal models, focusing on real-world rain images with high quality and diverse environmental conditions.",
          "citing_paper_id": "247940251",
          "cited_paper_id": 15443600,
          "context_text": "Rain200H [41] 1,800 200 synthetic Rain800 [48] 700 100 synthetic Rain1200 [47] 12,000 1,200 synthetic RainCityscapes [6, 14] 9,432 1,188 synthetic SPA-Data [34] 640k 1,000 real-world Yang et al.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions several datasets used for training and evaluating rain removal models. These datasets are specific and have clear identifiers.",
          "citing_paper_doi": "10.1016/j.image.2022.116701",
          "cited_paper_doi": "10.1109/CVPR.2017.183",
          "citing_paper_url": "https://www.semanticscholar.org/paper/a52f1afdd11d9a902729ecb1e5be852470756711",
          "cited_paper_url": "https://www.semanticscholar.org/paper/6de27f256e97c014a95706c72af7fa651e8fa34a",
          "citing_paper_year": 2020,
          "cited_paper_year": 2016
        }
      ]
    },
    {
      "cited_paper_id": "54482423",
      "citation_count": 0,
      "total_dataset_mentions": 2,
      "unique_datasets": [
        "FFHQ 256×256"
      ],
      "dataset_details": [
        {
          "dataset_name": "FFHQ 256×256",
          "dataset_description": "Used for the colorization task, focusing on bedroom scenes to evaluate color restoration in diverse indoor environments. | Used for the colorization task, focusing on high-resolution facial images to enhance color restoration quality.",
          "citing_paper_id": "249282628",
          "cited_paper_id": 8317437,
          "context_text": "For the colorization task, we use FFHQ 256 × 256, and LSUN-bedroom 256 × 256 [51].",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions two specific datasets, FFHQ and LSUN-bedroom, which are used for the colorization task. Both are multi-word proper nouns and are clearly identified.",
          "citing_paper_doi": "10.48550/arXiv.2206.00941",
          "cited_paper_doi": null,
          "citing_paper_url": "https://www.semanticscholar.org/paper/b3f5cf32178bcbed91aa5303b70963c6463f48a2",
          "cited_paper_url": "https://www.semanticscholar.org/paper/4dcdae25a5e33682953f0853ee4cf7ca93be58a9",
          "citing_paper_year": 2022,
          "cited_paper_year": 2015
        },
        {
          "dataset_name": "FFHQ 256×256",
          "dataset_description": "Used to validate the method for image inpainting, focusing on high-resolution facial images. | Used to validate the method for image inpainting, focusing on a diverse set of natural images.",
          "citing_paper_id": "249282628",
          "cited_paper_id": 54482423,
          "context_text": "Datasets and Implementation For inpainting, we use FFHQ 256×256 [20], and ImageNet 256×256 [9] to validate our method.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions two specific datasets used for validating the method in the context of image inpainting.",
          "citing_paper_doi": "10.48550/arXiv.2206.00941",
          "cited_paper_doi": "10.1109/CVPR.2019.00453",
          "citing_paper_url": "https://www.semanticscholar.org/paper/b3f5cf32178bcbed91aa5303b70963c6463f48a2",
          "cited_paper_url": "https://www.semanticscholar.org/paper/ceb2ebef0b41e31c1a21b28c2734123900c005e2",
          "citing_paper_year": 2022,
          "cited_paper_year": 2018
        }
      ]
    },
    {
      "cited_paper_id": "235703693",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "NTIRE"
      ],
      "dataset_details": [
        {
          "dataset_name": "NTIRE",
          "dataset_description": "Used for training and evaluation in image restoration, containing 2 million high-resolution images for dehazing and other restoration tasks. | Used to provide high-quality, high-resolution natural images for training and evaluation in image restoration tasks. | Used to provide high-quality, high-resolution natural images for training and evaluation in image restoration tasks, specifically from the NTIRE challenge. | Used for training and evaluation in image restoration, containing 2,000 high-resolution images for dehazing and other restoration tasks. | Used for training and evaluation in image restoration, containing 84,991 images for dehazing and other restoration tasks.",
          "citing_paper_id": "268856875",
          "cited_paper_id": 235703693,
          "context_text": "We combine the existing high-quality, high-resolution natural image datasets such as LSDIR [27], DIV2K [2], Flickr2K [35], and NTIRE [4].",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The citation mentions specific datasets used for high-quality, high-resolution natural images, which are relevant to the topic of all-in-one image restoration.",
          "citing_paper_doi": "10.48550/arXiv.2404.02154",
          "cited_paper_doi": "10.1109/CVPRW53098.2021.00074",
          "citing_paper_url": "https://www.semanticscholar.org/paper/1d99d28c3cf5a21671a9cf5de8b51009c72d4624",
          "cited_paper_url": "https://www.semanticscholar.org/paper/1c214c00c5733ab368712c9ac7ec9f103b304a40",
          "citing_paper_year": 2024,
          "cited_paper_year": 2021
        }
      ]
    },
    {
      "cited_paper_id": "459456",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "CelebA-Test"
      ],
      "dataset_details": [
        {
          "dataset_name": "CelebA-Test",
          "dataset_description": "Used to evaluate the performance of the DiffBIR model, specifically focusing on achieving the highest FID score in image restoration tasks. | Synthetic dataset used to evaluate the BFR task, focusing on face attributes in a controlled environment.",
          "citing_paper_id": "261276317",
          "cited_paper_id": 459456,
          "context_text": "For the synthetic dataset CelebA-Test [39], our DiffBIR achieves the highest FID score.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions 'CelebA-Test' as a specific dataset used to evaluate the performance of the model DiffBIR. The dataset is clearly identified and used for evaluation.",
          "citing_paper_doi": "10.48550/arXiv.2308.15070",
          "cited_paper_doi": "10.1109/ICCV.2015.425",
          "citing_paper_url": "https://www.semanticscholar.org/paper/858f0643110ccccb6a9103747f2169fecfb98668",
          "cited_paper_url": "https://www.semanticscholar.org/paper/6424b69f3ff4d35249c0bb7ef912fbc2c86f4ff4",
          "citing_paper_year": 2023,
          "cited_paper_year": 2014
        }
      ]
    },
    {
      "cited_paper_id": "10514149",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "DIV2K-Val"
      ],
      "dataset_details": [
        {
          "dataset_name": "DIV2K-Val",
          "dataset_description": "Used for assessing real-world super-resolution, focusing on realistic image enhancement and artifact reduction. | Used for validating image restoration models, focusing on high-resolution image quality and detail preservation. | Used for evaluating face recognition performance after image restoration, focusing on facial detail and identity preservation. | Used for mixed real-world image restoration, providing additional real-world scenarios. | Used for evaluating face detection performance after image restoration, focusing on robustness to varying conditions. | Used for evaluating real-world super-resolution performance, emphasizing naturalness and perceptual quality. | Used for evaluating real-world super-resolution, providing diverse and challenging real-world images. | Used for mixed real-world image restoration, contributing to a diverse evaluation set. | Used for mixed real-world image restoration, offering a range of challenging real-world images.",
          "citing_paper_id": "261276317",
          "cited_paper_id": 10514149,
          "context_text": "…DIV2K-Val [1], DRealSR [62], RealSR [3], and two real-world datasets: RealSRSet [73] and our collected real47, 2) BFR task on the real-world datasets LFW-Test [55] and WIDER-Test [77], and 3) BID task on a mixed real-world dataset, which contains images from real3 [74], real9 [74], and RNI15 [72].",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions several datasets used for different tasks in image restoration, including DIV2K-Val, DRealSR, RealSR, RealSRSet, real47, LFW-Test, WIDER-Test, real3, real9, and RNI15. These datasets are used for evaluating various aspects of image restoration.",
          "citing_paper_doi": "10.48550/arXiv.2308.15070",
          "cited_paper_doi": "10.1109/TIP.2018.2839891",
          "citing_paper_url": "https://www.semanticscholar.org/paper/858f0643110ccccb6a9103747f2169fecfb98668",
          "cited_paper_url": "https://www.semanticscholar.org/paper/e248ac3596d48ce338244624c2fd194dc0651bc6",
          "citing_paper_year": 2023,
          "cited_paper_year": 2017
        }
      ]
    },
    {
      "cited_paper_id": "5778488",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "LIME low-light enhancement dataset"
      ],
      "dataset_details": [
        {
          "dataset_name": "LIME low-light enhancement dataset",
          "dataset_description": "Used for rain removal, containing synthetic and real rainy images to train and test rain detection and removal methods. | Used for snow removal, featuring realistic snowy images to evaluate the performance of snow removal techniques. | Used for low-light image enhancement, focusing on improving illumination in underexposed images through illumination map estimation. | Used for dehazing tasks, providing real-world images with varying degrees of haze to test and evaluate dehazing algorithms.",
          "citing_paper_id": "271039782",
          "cited_paper_id": 5778488,
          "context_text": "…to the dearth of specialized datasets for real-world composite degradation, we have curated a selection from the LIME low-light enhancement dataset [24], the RESIDE Real-world Task-driven Testing Set (RESIDE-RTTS) [34], Yang’s rainy dataset [79], and the realistically snowy Snow100k dataset [34].",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions specific datasets used for image restoration tasks, particularly for low-light enhancement, dehazing, rain removal, and snow removal.",
          "citing_paper_doi": "10.48550/arXiv.2407.04621",
          "cited_paper_doi": "10.1109/TIP.2016.2639450",
          "citing_paper_url": "https://www.semanticscholar.org/paper/7da6f70ecc09a2ac615d475d82e2a1d747428403",
          "cited_paper_url": "https://www.semanticscholar.org/paper/aea5b4139f1a72e52d799906a782ba94b2687ee7",
          "citing_paper_year": 2024,
          "cited_paper_year": 2017
        }
      ]
    },
    {
      "cited_paper_id": "996788",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "CSBD68"
      ],
      "dataset_details": [
        {
          "dataset_name": "CSBD68",
          "dataset_description": "Used to train and evaluate image denoising models, focusing on residual learning of deep CNNs for improving image restoration performance.",
          "citing_paper_id": "119308964",
          "cited_paper_id": 996788,
          "context_text": "Following DnCNN [22], the models are trained with RGB channels and evaluated in RGB channels on CSBD68 [13] dataset.",
          "confidence_score": 1.0,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions the CSBD68 dataset, which is a specific dataset used for image denoising. The dataset is used to train and evaluate models in the context of image restoration.",
          "citing_paper_doi": "10.1109/CVPR.2019.01131",
          "cited_paper_doi": "10.1109/TIP.2017.2662206",
          "citing_paper_url": "https://www.semanticscholar.org/paper/e6ab7b7fee9a25342ef1cac082c77a7f1021a982",
          "cited_paper_url": "https://www.semanticscholar.org/paper/0c00a328fa7cd56ee60338c54e89bd48310db80b",
          "citing_paper_year": 2019,
          "cited_paper_year": 2016
        }
      ]
    },
    {
      "cited_paper_id": "4539586",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "AGAN-Data"
      ],
      "dataset_details": [
        {
          "dataset_name": "AGAN-Data",
          "dataset_description": "Used for training and testing an attentive generative adversarial network for raindrop removal from single images, providing 861 images for training and 58 for testing. | Used to evaluate the method's performance on high-resolution rainy images, focusing on challenging degradations and comparing results with the IDT algorithm. | Used to evaluate the method's performance on low-resolution rainy images, focusing on challenging degradations and comparing results with the IDT algorithm. | Used to assess raindrop removal effectiveness, focusing on low-resolution images with synthetic rain. | Used to evaluate raindrop removal performance, focusing on low-density rain scenarios. | Used to quantitatively evaluate raindrop removal methods, providing a benchmark for comparing state-of-the-art techniques in image restoration. | Used to evaluate raindrop removal performance, providing a diverse set of rainy images. | Used to evaluate the method for raindrop removal from single images, focusing on the performance and effectiveness of the proposed technique. | Used to evaluate the method's performance on synthetic rainy images generated by the AGAN model, focusing on challenging degradations and comparing results with the IDT algorithm. | Used to test the robustness of rain removal algorithms, containing diverse real-world rainy scenes. | Used to evaluate raindrop removal performance, focusing on high-density rain scenarios. | Used to validate the network's performance, featuring a variety of images with artificial rain for controlled testing. | Used to evaluate raindrop removal performance, focusing on high-resolution images with realistic rain effects.",
          "citing_paper_id": "261245351",
          "cited_paper_id": 4539586,
          "context_text": "We also evaluate our method in a raindrop dataset (i.e., AGAN-Data) collected by Qian et al. [47].",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions a specific dataset 'AGAN-Data' used for evaluating the method in the context of raindrop removal.",
          "citing_paper_doi": "10.1109/ICCV51070.2023.01205",
          "cited_paper_doi": "10.1109/CVPR.2018.00263",
          "citing_paper_url": "https://www.semanticscholar.org/paper/cb4f2f44d5ce37b7d7bf69fe7139f204eb79d199",
          "cited_paper_url": "https://www.semanticscholar.org/paper/34e38559df559bfedfc3e1edc2b120ab2b5d444e",
          "citing_paper_year": 2023,
          "cited_paper_year": 2017
        }
      ]
    },
    {
      "cited_paper_id": "260843368",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "LSDIR"
      ],
      "dataset_details": [
        {
          "dataset_name": "LSDIR",
          "dataset_description": "Used to address the need for practical tasks and datasets reflecting real-world image degradations, specifically for image restoration tasks.",
          "citing_paper_id": "273502246",
          "cited_paper_id": 260843368,
          "context_text": "There is a need to focus on more practical tasks and datasets that reflect the complexities of real-world image degradations [224], [225].",
          "confidence_score": 0.9,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions the need for practical tasks and datasets reflecting real-world image degradations, which aligns with the cited paper's title 'LSDIR: A Large Scale Dataset for Image Restoration'.",
          "citing_paper_doi": "10.48550/arXiv.2410.15067",
          "cited_paper_doi": "10.1109/CVPRW59228.2023.00178",
          "citing_paper_url": "https://www.semanticscholar.org/paper/f5ff17162a4936db3f64ed7bafae13abcfa63a27",
          "cited_paper_url": "https://www.semanticscholar.org/paper/851446ea3c70c918cdf8c62db0dd0b1900806e5f",
          "citing_paper_year": 2024,
          "cited_paper_year": 2023
        }
      ]
    },
    {
      "cited_paper_id": "8235184",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "synthetic dataset of 10 Jerlov water types"
      ],
      "dataset_details": [
        {
          "dataset_name": "synthetic dataset of 10 Jerlov water types",
          "dataset_description": "Used to generate synthetic underwater images for training and evaluating image restoration models, focusing on the impact of different water types on image quality.",
          "citing_paper_id": "173188673",
          "cited_paper_id": 8235184,
          "context_text": "[14] tries to improve on the above model by computing attenuation coefficients in the 3D RGB space, whereas [3] uses the above model to generate a synthetic dataset of 10 Jerlov water types.",
          "confidence_score": 0.9,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions a synthetic dataset of 10 Jerlov water types, which is a specific, verifiable resource used for generating synthetic data for underwater computer vision.",
          "citing_paper_doi": null,
          "cited_paper_doi": "10.1109/CVPR.2017.68",
          "citing_paper_url": "https://www.semanticscholar.org/paper/336433b50560e394239a2da3a12591c84c524052",
          "cited_paper_url": "https://www.semanticscholar.org/paper/64b9e48e71cd766b2503cc6079d52000a1133761",
          "citing_paper_year": 2019,
          "cited_paper_year": 2017
        }
      ]
    },
    {
      "cited_paper_id": "58014237",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "Underwater Image Enhancement Benchmark Dataset (UIEBD)"
      ],
      "dataset_details": [
        {
          "dataset_name": "Underwater Image Enhancement Benchmark Dataset (UIEBD)",
          "dataset_description": "Used as a real-world underwater image dataset to evaluate image enhancement techniques, focusing on improving visual quality in underwater environments. | Used to evaluate the performance of the proposed model on underwater image enhancement, focusing on visual quality and restoration accuracy.",
          "citing_paper_id": "173188673",
          "cited_paper_id": 58014237,
          "context_text": "We use Underwater Image Enhancement Benchmark Dataset (UIEBD) built by [18] as our real-world underwater image dataset.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context clearly mentions the use of UIEBD, which is a specific dataset used for underwater image enhancement. The cited paper title confirms it is a benchmark dataset.",
          "citing_paper_doi": null,
          "cited_paper_doi": "10.1109/TIP.2019.2955241",
          "citing_paper_url": "https://www.semanticscholar.org/paper/336433b50560e394239a2da3a12591c84c524052",
          "cited_paper_url": "https://www.semanticscholar.org/paper/9496c94a6630a165a24db4e32f0b3f12a6616307",
          "citing_paper_year": 2019,
          "cited_paper_year": 2019
        }
      ]
    },
    {
      "cited_paper_id": "62732555",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "Snow100K-S"
      ],
      "dataset_details": [
        {
          "dataset_name": "Snow100K-S",
          "dataset_description": "Used to evaluate image desnowing performance, focusing on larger-scale images with moderate PSNR and SSIM metrics. | Used to evaluate image desnowing performance, focusing on smaller-scale images with high PSNR and SSIM metrics.",
          "citing_paper_id": "251197000",
          "cited_paper_id": 62732555,
          "context_text": "For the image desnowing task, WeatherDiff 64 achieves the best PSNR/SSIM metrics with 35.83/0.9566 and 30.09/0.9041 for Snow100K-S and Snow100K-L respectively.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions 'Snow100K-S' and 'Snow100K-L', which are specific datasets used for evaluating image desnowing performance. The cited paper does not disambiguate these as methods or tools.",
          "citing_paper_doi": "10.1109/TPAMI.2023.3238179",
          "cited_paper_doi": "10.1049/EL:20080522",
          "citing_paper_url": "https://www.semanticscholar.org/paper/69beb616ffdc42afe86b7487c5db82b6d88638b8",
          "cited_paper_url": "https://www.semanticscholar.org/paper/dad5932e2a40c87d17e40cf8d072ffdb43046fd9",
          "citing_paper_year": 2022,
          "cited_paper_year": 2008
        }
      ]
    },
    {
      "cited_paper_id": "212737191",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "CycleISP"
      ],
      "dataset_details": [
        {
          "dataset_name": "CycleISP",
          "dataset_description": "Used to simulate real-world noise in the training dataset, enhancing the model's ability to handle actual noisy images during testing.",
          "citing_paper_id": "246634936",
          "cited_paper_id": 212737191,
          "context_text": "Notably, benefiting from the noise simulation via CycleISP in the training dataset, our model handles real-world noises well.",
          "confidence_score": 0.8,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions 'CycleISP' in relation to noise simulation in the training dataset, which is relevant to the research topic of image restoration.",
          "citing_paper_doi": "10.1007/978-3-031-20068-7_33",
          "cited_paper_doi": "10.1109/CVPR42600.2020.00277",
          "citing_paper_url": "https://www.semanticscholar.org/paper/be3b1b0a2eeddba876580da707105820782672f1",
          "cited_paper_url": "https://www.semanticscholar.org/paper/51352023ed22068fbfaa7693fc37123c0a5fce9f",
          "citing_paper_year": 2022,
          "cited_paper_year": 2020
        }
      ]
    },
    {
      "cited_paper_id": "230795419",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "LOL-simulation"
      ],
      "dataset_details": [
        {
          "dataset_name": "LOL-simulation",
          "dataset_description": "Used to re-train the KinD++ network for comparison, focusing on low-light image restoration techniques and performance metrics. | Used to retrain the KinD++ network for comparison, focusing on low-light image restoration techniques and performance metrics. | Used for training deblurring models, focusing on enhancing low-light images and addressing image restoration challenges. | Used to train the official KinD++ model, focusing on low-light image restoration techniques and performance metrics.",
          "citing_paper_id": "246634936",
          "cited_paper_id": 230795419,
          "context_text": "We re-train the network KinD++ [53] using the LOL-simulation dataset for comparison with the official model that was trained on the original LOL dataset.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions two datasets: 'LOL-simulation' and 'LOL'. Both are used for training and comparing the performance of the KinD++ network.",
          "citing_paper_doi": "10.1007/978-3-031-20068-7_33",
          "cited_paper_doi": "10.1007/s11263-020-01407-x",
          "citing_paper_url": "https://www.semanticscholar.org/paper/be3b1b0a2eeddba876580da707105820782672f1",
          "cited_paper_url": "https://www.semanticscholar.org/paper/5e87044d805a43823ace73cb8f2bf1f4058ccf47",
          "citing_paper_year": 2022,
          "cited_paper_year": 2021
        }
      ]
    },
    {
      "cited_paper_id": "232306906",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "SnowTest100K"
      ],
      "dataset_details": [
        {
          "dataset_name": "SnowTest100K",
          "dataset_description": "Used to qualitatively compare the performance of the best model with DesnowNet and DDMSNet, focusing on snow removal using semantic and depth priors.",
          "citing_paper_id": "251197000",
          "cited_paper_id": 232306906,
          "context_text": "Qualitative reconstruction comparisons of our best model on SnowTest100K test samples with DesnowNet [3] and DDMSNet [59].",
          "confidence_score": 1.0,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions 'SnowTest100K' as a test dataset used for qualitative reconstruction comparisons. No other specific datasets are mentioned.",
          "citing_paper_doi": "10.1109/TPAMI.2023.3238179",
          "cited_paper_doi": "10.1109/TIP.2021.3104166",
          "citing_paper_url": "https://www.semanticscholar.org/paper/69beb616ffdc42afe86b7487c5db82b6d88638b8",
          "cited_paper_url": "https://www.semanticscholar.org/paper/662528c1bc636314c5e4132bb079acab2ea40250",
          "citing_paper_year": 2022,
          "cited_paper_year": 2021
        }
      ]
    },
    {
      "cited_paper_id": "131691077",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "LFDOF"
      ],
      "dataset_details": [
        {
          "dataset_name": "LFDOF",
          "dataset_description": "Used for light field based defocus deblurring, leveraging synthetic aperture and refocusing features of light field technology. The dataset contains image pairs acquired in single shots.",
          "citing_paper_id": "247922539",
          "cited_paper_id": 131691077,
          "context_text": "[29] proposed the light field based defocus deblurring dataset LFDOF leveraging the synthetic aperture and refocusing features of light field technology [22], where the image pairs were acquired in single shots.",
          "confidence_score": 1.0,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions a specific dataset 'LFDOF' which is used for light field based defocus deblurring. The dataset is described as leveraging synthetic aperture and refocusing features of light field technology.",
          "citing_paper_doi": "10.1109/CVPR52688.2022.01582",
          "cited_paper_doi": null,
          "citing_paper_url": "https://www.semanticscholar.org/paper/60be91a493530eec8cf969bce80f59af0f15b6b4",
          "cited_paper_url": "https://www.semanticscholar.org/paper/df170c5e3807731fc711d6a9846934053f748309",
          "citing_paper_year": 2022,
          "cited_paper_year": 2005
        }
      ]
    },
    {
      "cited_paper_id": "64193",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "BSD-grayscale"
      ],
      "dataset_details": [
        {
          "dataset_name": "BSD-grayscale",
          "dataset_description": "Used to train and test the DuRN-P model for additive Gaussian noise removal, evaluating performance at noise levels 30, 50, and 70 using PSNR/SSIM metrics.",
          "citing_paper_id": "84843405",
          "cited_paper_id": 64193,
          "context_text": "ing the same experimental protocols used by previous studies, we trained and tested the proposed DuRN-P using the training and test subsets (300 and 200 grayscale images) of the BSD-grayscale dataset [27]. Table 2: Results for additive Gaussian noise removal on BSD200-grayscale and noise levels (30, 50, 70). The numbers are PSNR/SSIM. 30 50 70 REDNet [26] 27.95 / 0.8019 25.75 / 0.7167 24.37 / 0.6551 M",
          "confidence_score": 1.0,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions the BSD-grayscale dataset, which is a specific dataset used for training and testing image restoration models. The dataset is used to evaluate the performance of the proposed DuRN-P model on additive Gaussian noise removal.",
          "citing_paper_doi": "10.1109/CVPR.2019.00717",
          "cited_paper_doi": "10.1109/ICCV.2001.937655",
          "citing_paper_url": "https://www.semanticscholar.org/paper/646736bca08f11140f0a3e77a2273533acefa6a6",
          "cited_paper_url": "https://www.semanticscholar.org/paper/9a1ed876196ec9733acb1daa6d65e35ff0414291",
          "citing_paper_year": 2019,
          "cited_paper_year": 2001
        }
      ]
    },
    {
      "cited_paper_id": "2930547",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "Dehaze-TestA"
      ],
      "dataset_details": [
        {
          "dataset_name": "Dehaze-TestA",
          "dataset_description": "Used to evaluate the proposed DuRN-US model on synthetic outdoor scenes, assessing its effectiveness in removing haze and restoring image clarity. | Used to evaluate the performance of the proposed DuRN-US model on dehazing tasks, focusing on image quality and restoration accuracy.",
          "citing_paper_id": "84843405",
          "cited_paper_id": 2930547,
          "context_text": "ii) ground truth images, iii) ground truth global atmosphere light , iv) ground truth transmission maps; additionally, its weights are initialized by those of DenseNet [16] pretrained on the ImageNet [37]. The proposed DuRN-US is trained only using i) and ii). Table 6 show results on Dehaze-TestA and RESIDE-SOTS datasets, respectively. Figure 10 shows examples of the results obtained by the proposed n",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions 'Dehaze-TestA' and 'RESIDE-SOTS' as datasets used for evaluation. 'ImageNet' is mentioned but in the context of pretraining DenseNet, which is not relevant to the image restoration task.",
          "citing_paper_doi": "10.1109/CVPR.2019.00717",
          "cited_paper_doi": "10.1007/s11263-015-0816-y",
          "citing_paper_url": "https://www.semanticscholar.org/paper/646736bca08f11140f0a3e77a2273533acefa6a6",
          "cited_paper_url": "https://www.semanticscholar.org/paper/e74f9b7f8eec6ba4704c206b93bc8079af3da4bd",
          "citing_paper_year": 2019,
          "cited_paper_year": 2014
        }
      ]
    },
    {
      "cited_paper_id": "52008443",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "LOL (Low-Light)"
      ],
      "dataset_details": [
        {
          "dataset_name": "LOL (Low-Light)",
          "dataset_description": "Used to train DNet and FNet for texture/structure split and low-light correction, focusing on enhancing images under low-light conditions.",
          "citing_paper_id": "278326960",
          "cited_paper_id": 52008443,
          "context_text": "First, the DNet and FNet are trained to perform texture/structure split and low-light correction on the LOL (Low-Light) dataset [58].",
          "confidence_score": 1.0,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The citation mentions the use of the LOL (Low-Light) dataset for training DNet and FNet in the context of texture/structure split and low-light correction.",
          "citing_paper_doi": "10.48550/arXiv.2505.01882",
          "cited_paper_doi": null,
          "citing_paper_url": "https://www.semanticscholar.org/paper/d6c7f3bc3e7b317c652f3c59b55015667ba3f77b",
          "cited_paper_url": "https://www.semanticscholar.org/paper/c7798bbbcd2e85593a2b2d37b92dabac63420cd8",
          "citing_paper_year": 2025,
          "cited_paper_year": 2018
        }
      ]
    },
    {
      "cited_paper_id": "4563057",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "https://github.com/rwenqi/GFNdehazing"
      ],
      "dataset_details": [
        {
          "dataset_name": "https://github.com/rwenqi/GFNdehazing",
          "dataset_description": "Used to provide images for dehazing experiments, specifically to test the effectiveness of the gated fusion network in estimating ground truth images from hazy inputs.",
          "citing_paper_id": "84843405",
          "cited_paper_id": 4563057,
          "context_text": "Following this two-stage approach, the state-of-theart method [30] uses an attentive-recurrent network to produce an attention map that conveys information about raindrops; then, the attention map along with the input image\n3The images are available from https://github.com/rwenqi/GFNdehazing\nare fed to a convolutional encoder-decoder network to estimate the ground truth image.",
          "confidence_score": 0.8,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions a GitHub repository for images, which is a specific, verifiable resource. However, it does not mention a traditional dataset name. The repository is used to provide images for dehazing experiments.",
          "citing_paper_doi": "10.1109/CVPR.2019.00717",
          "cited_paper_doi": "10.1109/CVPR.2018.00343",
          "citing_paper_url": "https://www.semanticscholar.org/paper/646736bca08f11140f0a3e77a2273533acefa6a6",
          "cited_paper_url": "https://www.semanticscholar.org/paper/dbd21420cd1299fdab9a04c6b4b3b5b4d4a38a6d",
          "citing_paper_year": 2019,
          "cited_paper_year": 2018
        }
      ]
    },
    {
      "cited_paper_id": "4715123",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "Real-World Noisy Image Dataset"
      ],
      "dataset_details": [
        {
          "dataset_name": "Real-World Noisy Image Dataset",
          "dataset_description": "Used to evaluate denoising algorithms on real-world noisy images, focusing on performance in practical scenarios and comparing different methods. | Used for motion deblurring, testing the effectiveness of algorithms on video frames. | Used to test the DuRN-P model on real-world noise removal, specifically evaluating performance on 40 pairs of noisy and mean images captured by a CMOS camera. | Used for rain removal, assessing the performance on images with rain streaks. | Used for joint dehazing and deraining, testing the robustness of models in complex conditions. | Used for real-world noisy image denoising, assessing performance on authentic noise patterns. | Used for image dehazing, evaluating the ability to remove atmospheric effects. | Used for grayscale image restoration, evaluating denoising and deblurring techniques.",
          "citing_paper_id": "84843405",
          "cited_paper_id": 4715123,
          "context_text": "Results: Real-World Noise Removal We also tested the DuRN-P on the Real-World Noisy Image Dataset [44], which consists of 40 pairs of an instance image (a photograph taken by a CMOS camera) and the mean image (mean of multiple shots of the same scene taken by the CMOS camera).",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions the 'Real-World Noisy Image Dataset' which is a specific dataset used for testing noise removal algorithms.",
          "citing_paper_doi": "10.1109/CVPR.2019.00717",
          "cited_paper_doi": null,
          "citing_paper_url": "https://www.semanticscholar.org/paper/646736bca08f11140f0a3e77a2273533acefa6a6",
          "cited_paper_url": "https://www.semanticscholar.org/paper/fb52f9294fa1cd83660eccac486367eac01ea32d",
          "citing_paper_year": 2019,
          "cited_paper_year": 2018
        }
      ]
    },
    {
      "cited_paper_id": "252917726",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "Laion-High-Resolution"
      ],
      "dataset_details": [
        {
          "dataset_name": "Laion-High-Resolution",
          "dataset_description": "Used to select 12,225 high-resolution images as source images for the current research, ensuring diversity and quality.",
          "citing_paper_id": "277667714",
          "cited_paper_id": 252917726,
          "context_text": "To ensure the highest quality and most diverse image dataset, we select 12,225 images from the Laion-High-Resolution dataset [33] as source images.",
          "confidence_score": 1.0,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions the Laion-High-Resolution dataset, which is a specific, verifiable dataset. It is used to select source images for the current research.",
          "citing_paper_doi": null,
          "cited_paper_doi": "10.48550/arXiv.2210.08402",
          "citing_paper_url": "https://www.semanticscholar.org/paper/7d7712fbd2f7cdda845c176b03b84f2df93a8b2f",
          "cited_paper_url": "https://www.semanticscholar.org/paper/e5c8960eb2ec034ffbd353ef39fd1cb541d3c7c9",
          "citing_paper_year": 2025,
          "cited_paper_year": 2022
        }
      ]
    },
    {
      "cited_paper_id": "8671030",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "GoPro-test"
      ],
      "dataset_details": [
        {
          "dataset_name": "GoPro-test",
          "dataset_description": "Used to test the proposed DuRNU model for dynamic scene deblurring, comparing its performance with state-of-the-art methods. The dataset contains real-world blurry images and their corresponding sharp ground truth images.",
          "citing_paper_id": "84843405",
          "cited_paper_id": 8671030,
          "context_text": "Results: GoPro Dataset We tested the proposed DuRNU on the GoPro-test dataset [28] and compared its results with the state-of-the-art DeblurGAN2 [20].",
          "confidence_score": 1.0,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The GoPro-test dataset is explicitly mentioned and used for testing the proposed DuRNU model. The dataset is relevant to the research topic of image restoration, specifically deblurring.",
          "citing_paper_doi": "10.1109/CVPR.2019.00717",
          "cited_paper_doi": "10.1109/CVPR.2017.35",
          "citing_paper_url": "https://www.semanticscholar.org/paper/646736bca08f11140f0a3e77a2273533acefa6a6",
          "cited_paper_url": "https://www.semanticscholar.org/paper/65b16da51891a6b98140d425804c8a0fd0299219",
          "citing_paper_year": 2019,
          "cited_paper_year": 2016
        }
      ]
    },
    {
      "cited_paper_id": "1610415",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "VIRAT"
      ],
      "dataset_details": [
        {
          "dataset_name": "VIRAT",
          "dataset_description": "Used to evaluate image restoration techniques on real-world images, focusing on compression artifacts and quality enhancement. | Applied to assess the performance of image restoration methods on surveillance videos, emphasizing anomaly detection and video quality improvement. | Utilized to test image restoration algorithms on crime-related videos, specifically addressing the challenges of low-quality and compressed video data.",
          "citing_paper_id": "235755417",
          "cited_paper_id": 1610415,
          "context_text": "Images are from DPED [11], VIRAT [19], UCF-Crime [20] datasets. data, images and videos are always compressed.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The citation mentions three specific datasets used for image and video data. These datasets are relevant to the topic of image restoration, particularly in the context of surveillance and real-world scenarios.",
          "citing_paper_doi": "10.1109/TPAMI.2022.3203009",
          "cited_paper_doi": "10.1109/CVPR.2018.00678",
          "citing_paper_url": "https://www.semanticscholar.org/paper/0b8a64c8145d95e0fc0bd8f6eb54b5da7aca4fb7",
          "cited_paper_url": "https://www.semanticscholar.org/paper/96ed8ce9ef9fc475db9e02c79f984dc110409b62",
          "citing_paper_year": 2021,
          "cited_paper_year": 2018
        }
      ]
    },
    {
      "cited_paper_id": "208193206",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "DIV8K"
      ],
      "dataset_details": [
        {
          "dataset_name": "DIV8K",
          "dataset_description": "Used to provide example images with typical patch recurrence at different scales, supporting the study of image restoration techniques.",
          "citing_paper_id": "235755417",
          "cited_paper_id": 208193206,
          "context_text": "The example image is from DIV8K [9]; (b) Image with typical patch recurrence, both within and across different scales of the same image.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "DIV8K is explicitly mentioned and is a specific dataset name. The context indicates it is used for providing example images, particularly those with typical patch recurrence at different scales.",
          "citing_paper_doi": "10.1109/TPAMI.2022.3203009",
          "cited_paper_doi": "10.1109/ICCVW.2019.00435",
          "citing_paper_url": "https://www.semanticscholar.org/paper/0b8a64c8145d95e0fc0bd8f6eb54b5da7aca4fb7",
          "cited_paper_url": "https://www.semanticscholar.org/paper/98179a590ef8afdca2c27ca7af405039f9c3ceda",
          "citing_paper_year": 2021,
          "cited_paper_year": 2019
        }
      ]
    },
    {
      "cited_paper_id": "262543458",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "Raindrop-A"
      ],
      "dataset_details": [
        {
          "dataset_name": "Raindrop-A",
          "dataset_description": "Used to evaluate raindrop removal methods, specifically comparing AGAN, DuRN, Quan, and MAXIM-2S in visual performance.",
          "citing_paper_id": "245837508",
          "cited_paper_id": 262543458,
          "context_text": "Visual comparisons for raindrop removal on Raindrop-A [71] among AGAN [71], DuRN [55], Quan [73], and MAXIM-2S.",
          "confidence_score": 0.9,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions 'Raindrop-A' which is likely a dataset used for evaluating raindrop removal methods. The cited papers confirm the relevance to raindrop removal.",
          "citing_paper_doi": "10.1109/CVPR52688.2022.00568",
          "cited_paper_doi": "10.1109/ICCV.2019.00255",
          "citing_paper_url": "https://www.semanticscholar.org/paper/fbf68eb0cf8237cef59ebdb301569c79b9676ff9",
          "cited_paper_url": "https://www.semanticscholar.org/paper/44bebf52c3b6aad3a2d4fff5664128998b8da043",
          "citing_paper_year": 2022,
          "cited_paper_year": 2019
        }
      ]
    },
    {
      "cited_paper_id": "4746623",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "sequentially-applied (or mixed) distortion dataset"
      ],
      "dataset_details": [
        {
          "dataset_name": "sequentially-applied (or mixed) distortion dataset",
          "dataset_description": "Used to integrate multiple distortion scenarios, serving as a general form of previous multi-degraded datasets for image restoration research. | Used to evaluate image restoration methods, focusing on sequential application of distortions to images, enhancing the robustness of restoration algorithms. | Used to evaluate image restoration methods, specifically focusing on mixed distortions and comparing performance against other approaches.",
          "citing_paper_id": "251772829",
          "cited_paper_id": 4746623,
          "context_text": "The first regime is the sequentially-applied (or mixed) distortion dataset [39] as shown in Figure 1b.",
          "confidence_score": 0.85,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions a 'sequentially-applied (or mixed) distortion dataset' which appears to be a specific dataset used for image restoration research.",
          "citing_paper_doi": "10.1109/CVPRW56347.2022.00069",
          "cited_paper_doi": "10.1109/CVPR.2018.00259",
          "citing_paper_url": "https://www.semanticscholar.org/paper/ad7a07d34fc95885301241f5943ffe99ba369206",
          "cited_paper_url": "https://www.semanticscholar.org/paper/9fb3707a0f90c6620251d202a972a2c626dce976",
          "citing_paper_year": 2022,
          "cited_paper_year": 2018
        }
      ]
    },
    {
      "cited_paper_id": "4539586",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "All-in-One weather dataset"
      ],
      "dataset_details": [
        {
          "dataset_name": "All-in-One weather dataset",
          "dataset_description": "Used to compare with other datasets, specifically for evaluating image restoration tasks involving various weather conditions, emphasizing its comprehensive nature.",
          "citing_paper_id": "259340208",
          "cited_paper_id": 4539586,
          "context_text": "Compare with GT-RAIN [2] which is manually collected and for rain only; or the All-in-One weather dataset [32] which combines various simulated pairs [31, 40, 48].",
          "confidence_score": 0.9,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions 'All-in-One weather dataset' as a comparison to other datasets, indicating it is a reusable resource used for image restoration tasks involving various weather conditions.",
          "citing_paper_doi": "10.1109/CVPR52729.2023.01297",
          "cited_paper_doi": "10.1109/CVPR.2018.00263",
          "citing_paper_url": "https://www.semanticscholar.org/paper/1b2ec6027043f5696df8c44a8225e9caf0b7bf6d",
          "cited_paper_url": "https://www.semanticscholar.org/paper/34e38559df559bfedfc3e1edc2b120ab2b5d444e",
          "citing_paper_year": 2023,
          "cited_paper_year": 2017
        }
      ]
    },
    {
      "cited_paper_id": "251903039",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "WeatherStream Dataset"
      ],
      "dataset_details": [
        {
          "dataset_name": "WeatherStream Dataset",
          "dataset_description": "Used to train a model for removing rain streaks and snowflakes, demonstrating superior performance compared to synthetic training data.",
          "citing_paper_id": "259340208",
          "cited_paper_id": 251903039,
          "context_text": "We note that the synthetically trained version of the Rain-robust model [3] is particularly bad at removing rain streaks and snowflakes when compared to training on WeatherStream Dataset.",
          "confidence_score": 1.0,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions 'WeatherStream Dataset' as a specific dataset used for training a model to remove rain streaks and snowflakes. The dataset is clearly identified and used for a specific purpose.",
          "citing_paper_doi": "10.1109/CVPR52729.2023.01297",
          "cited_paper_doi": "10.1007/978-3-031-20071-7_42",
          "citing_paper_url": "https://www.semanticscholar.org/paper/1b2ec6027043f5696df8c44a8225e9caf0b7bf6d",
          "cited_paper_url": "https://www.semanticscholar.org/paper/cd2a6f9dde5a297be0eb58cbfb879be3b42c306d",
          "citing_paper_year": 2023,
          "cited_paper_year": 2022
        }
      ]
    },
    {
      "cited_paper_id": "64193",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "Rain13k-Test"
      ],
      "dataset_details": [
        {
          "dataset_name": "Rain13k-Test",
          "dataset_description": "Used for deraining, combining multiple datasets to evaluate the effectiveness of deraining algorithms in diverse conditions. | Used for motion deblurring, assessing the performance of deblurring algorithms on real-world motion blur scenarios. | Used for denoising, testing the effectiveness of denoising algorithms on a standard benchmark dataset. | Used for low-light enhancement, evaluating the ability of algorithms to improve image quality in low-light conditions. | Used for kernel deblurring and JPEG artifact removal, evaluating the performance of algorithms in handling specific image degradation issues.",
          "citing_paper_id": "272987034",
          "cited_paper_id": 64193,
          "context_text": "…[20] for dehazing, Rain13k-Test (the combination of Rain100L [53], Rain100H [53], Test100 [59], Test1200 [58] and Test2800 [10]) for deraining, GoPro for motion deblurring, LOL [49] for low-light enhancement, BSD68 [38] for denoising, LSDIR-val for kernel deblurring and jpeg artifact removal.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The citation mentions several datasets used for various image restoration tasks, including dehazing, deraining, motion deblurring, low-light enhancement, denoising, kernel deblurring, and JPEG artifact removal.",
          "citing_paper_doi": "10.48550/arXiv.2409.19403",
          "cited_paper_doi": "10.1109/ICCV.2001.937655",
          "citing_paper_url": "https://www.semanticscholar.org/paper/cc6a57aa1b8496efd3b9001226fd09b8d121f397",
          "cited_paper_url": "https://www.semanticscholar.org/paper/9a1ed876196ec9733acb1daa6d65e35ff0414291",
          "citing_paper_year": 2024,
          "cited_paper_year": 2001
        }
      ]
    },
    {
      "cited_paper_id": "14113767",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "SID Sony dataset"
      ],
      "dataset_details": [
        {
          "dataset_name": "SID Sony dataset",
          "dataset_description": "Used to evaluate image restoration methods, focusing on low-light images and their quality after restoration using different techniques.",
          "citing_paper_id": "235640808",
          "cited_paper_id": 14113767,
          "context_text": "We ran Faster RCNN [53] trained on MS COCO [40] over the SID Sony dataset’s low-light images restored by different methods.",
          "confidence_score": 1.0,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions the use of the MS COCO and SID Sony dataset for training and evaluating image restoration methods.",
          "citing_paper_doi": "10.1109/CVPR46437.2021.00349",
          "cited_paper_doi": "10.1007/978-3-319-10602-1_48",
          "citing_paper_url": "https://www.semanticscholar.org/paper/39e70be1d3d9907271aeeb6733a98eb38fe27d26",
          "cited_paper_url": "https://www.semanticscholar.org/paper/71b7178df5d2b112d07e45038cb5637208659ff7",
          "citing_paper_year": 2021,
          "cited_paper_year": 2014
        }
      ]
    },
    {
      "cited_paper_id": "214714493",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "SID Sony camera dataset"
      ],
      "dataset_details": [
        {
          "dataset_name": "SID Sony camera dataset",
          "dataset_description": "This dataset 'ELD dataset' was mentioned in the citation context but no detailed description was generated. | Used to train models for image restoration, focusing on low-light conditions and camera-specific noise patterns.",
          "citing_paper_id": "235640808",
          "cited_paper_id": 214714493,
          "context_text": "To investigate this, we employed the models trained on the SID Sony camera dataset to restore the test images captured by other cameras present in the ELD dataset [64].",
          "confidence_score": 0.9,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions two specific datasets: 'SID Sony camera dataset' and 'ELD dataset'. Both are used for training and testing image restoration models.",
          "citing_paper_doi": "10.1109/CVPR46437.2021.00349",
          "cited_paper_doi": "10.1109/cvpr42600.2020.00283",
          "citing_paper_url": "https://www.semanticscholar.org/paper/39e70be1d3d9907271aeeb6733a98eb38fe27d26",
          "cited_paper_url": "https://www.semanticscholar.org/paper/c0d498cc84bd2b02580d53311edb54af3f81717a",
          "citing_paper_year": 2021,
          "cited_paper_year": 2020
        }
      ]
    },
    {
      "cited_paper_id": "218538042",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "NTIRE 2023 HR NonHomogeneous Dehazing competition"
      ],
      "dataset_details": [
        {
          "dataset_name": "NTIRE 2023 HR NonHomogeneous Dehazing competition",
          "dataset_description": "This dataset 'NTIRE 2023 HR NonHomogeneous Dehazing competition' was mentioned in the citation context but no detailed description was generated.",
          "citing_paper_id": "258179052",
          "cited_paper_id": 218538042,
          "context_text": "Here we use a new dataset proposed in the NTIRE 2023 HR NonHomogeneous Dehazing competition [3].",
          "confidence_score": 0.9,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions a new dataset from the NTIRE 2023 HR NonHomogeneous Dehazing competition, which is a specific and verifiable resource.",
          "citing_paper_doi": "10.1109/CVPRW59228.2023.00169",
          "cited_paper_doi": "10.1109/CVPRW50498.2020.00253",
          "citing_paper_url": "https://www.semanticscholar.org/paper/9b8b9c4f742998d3730cc13b30313a95fc0077fd",
          "cited_paper_url": "https://www.semanticscholar.org/paper/f4a618f2ce22808dc1f9e34e7c0ff08d215f36eb",
          "citing_paper_year": 2023,
          "cited_paper_year": 2020
        }
      ]
    },
    {
      "cited_paper_id": "244346144",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "Bokeh Effect Transformation dataset"
      ],
      "dataset_details": [
        {
          "dataset_name": "Bokeh Effect Transformation dataset",
          "dataset_description": "Used to evaluate the performance of the proposed method in the context of high-resolution image restoration, focusing on the bokeh effect transformation.",
          "citing_paper_id": "258179052",
          "cited_paper_id": 244346144,
          "context_text": "Comparison of our methods with Restormer [72] on the Bokeh Effect Transformation dataset.",
          "confidence_score": 1.0,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions the 'Bokeh Effect Transformation dataset' which is a specific dataset used for image restoration tasks. The dataset is used to compare the performance of the current method with Restormer.",
          "citing_paper_doi": "10.1109/CVPRW59228.2023.00169",
          "cited_paper_doi": "10.1109/CVPR52688.2022.00564",
          "citing_paper_url": "https://www.semanticscholar.org/paper/9b8b9c4f742998d3730cc13b30313a95fc0077fd",
          "cited_paper_url": "https://www.semanticscholar.org/paper/1e88d5afe19aea324d33541f60a90b7036894c32",
          "citing_paper_year": 2023,
          "cited_paper_year": 2021
        }
      ]
    },
    {
      "cited_paper_id": "unknown",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "NTIRE 2023 Stereo Image Super-Resolution Challenge"
      ],
      "dataset_details": [
        {
          "dataset_name": "NTIRE 2023 Stereo Image Super-Resolution Challenge",
          "dataset_description": "This dataset 'NTIRE 2023 Stereo Image Super-Resolution Challenge' was mentioned in the citation context but no detailed description was generated. | Provided the source images for the NTIRE 2023 challenge, used to evaluate stereo image super-resolution performance.",
          "citing_paper_id": "258179052",
          "cited_paper_id": null,
          "context_text": "To train and evaluate our model, we use the dataset provided by the NTIRE 2023 Stereo Image Super-Resolution Challenge [64], which consists of 800 training stereo images and 112 validation stereo images from the Flickr1024 [66] dataset.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions two specific datasets used for training and evaluation: the NTIRE 2023 Stereo Image Super-Resolution Challenge dataset and the Flickr1024 dataset.",
          "citing_paper_doi": "10.1109/CVPRW59228.2023.00169",
          "cited_paper_doi": null,
          "citing_paper_url": "https://www.semanticscholar.org/paper/9b8b9c4f742998d3730cc13b30313a95fc0077fd",
          "cited_paper_url": null,
          "citing_paper_year": 2023,
          "cited_paper_year": null
        }
      ]
    },
    {
      "cited_paper_id": "4828378",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "DICM"
      ],
      "dataset_details": [
        {
          "dataset_name": "DICM",
          "dataset_description": "Used to assess naturalness-preserved enhancement algorithms, specifically for non-uniform illumination images without ground truth. | Used to test image restoration methods, particularly for dynamic range compression and multi-exposure fusion without ground truth. | Used to evaluate multi-exposure image fusion techniques, focusing on perceptual quality and naturalness preservation in non-uniform illumination images.",
          "citing_paper_id": "268513542",
          "cited_paper_id": 4828378,
          "context_text": "We further generalize to the widely used real-world datasets MEF [34], NPE [56] and DICM [21] without ground truth.",
          "confidence_score": 0.9,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions three datasets: MEF, NPE, and DICM. These are likely specific datasets used for image restoration and enhancement tasks.",
          "citing_paper_doi": "10.1109/CVPR52733.2024.02404",
          "cited_paper_doi": "10.1109/TIP.2015.2442920",
          "citing_paper_url": "https://www.semanticscholar.org/paper/1ff44c75c361d2ed28f868361a29437197b0270c",
          "cited_paper_url": "https://www.semanticscholar.org/paper/19986f7af0086f0e329779f3497e97440765d170",
          "citing_paper_year": 2024,
          "cited_paper_year": 2015
        }
      ]
    },
    {
      "cited_paper_id": "271039782",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "UIRD-12"
      ],
      "dataset_details": [
        {
          "dataset_name": "UIRD-12",
          "dataset_description": "Used to evaluate various One-to-Many and One-to-Composite image restoration methods, focusing on performance across different types of image degradation. | Used to evaluate the CoR method in image restoration experiments, encompassing 11 degradation types including l, h, r, s, and their combinations. | Used to evaluate the CoR method in image restoration experiments, focusing on various degradation types.",
          "citing_paper_id": "273323238",
          "cited_paper_id": 271039782,
          "context_text": "In the experiment on UIRD-12, we primarily select 4 One-to-Many methods including AirNet [37], PromptIR [52], InstructIR [14], HAIR [3], and a One-to-Composite method OneRestore [25].",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions UIRD-12, which appears to be a dataset used for evaluating image restoration methods. The other names mentioned are methods, not datasets.",
          "citing_paper_doi": "10.48550/arXiv.2410.08688",
          "cited_paper_doi": "10.48550/arXiv.2407.04621",
          "citing_paper_url": "https://www.semanticscholar.org/paper/3b58b1884ffa794d8095cbf98ebe012c0735a61e",
          "cited_paper_url": "https://www.semanticscholar.org/paper/7da6f70ecc09a2ac615d475d82e2a1d747428403",
          "citing_paper_year": 2024,
          "cited_paper_year": 2024
        }
      ]
    },
    {
      "cited_paper_id": "263784964",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "CDD-11"
      ],
      "dataset_details": [
        {
          "dataset_name": "CDD-11",
          "dataset_description": "Used to compare low-order methods integrated with CoR against end-to-end methods in image restoration, focusing on One-to-One and One-to-Many restoration techniques.",
          "citing_paper_id": "273323238",
          "cited_paper_id": 263784964,
          "context_text": "…on CDD-11, we compare low-order methods integrated with CoR against end-to-end methods, comprising 9 One-to-One image restoration methods (MIRNet [69], MPR-Net [70], MIRNetv2 [72], Restormer [71], DGUNet [49], NAFNet [6], SRUDC [57], Fourmer [78], OKNet [15]) and 6 One-to-Many image restoration…",
          "confidence_score": 1.0,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions 'CDD-11' as a dataset used for comparing low-order methods against end-to-end methods in image restoration. No other specific datasets are mentioned.",
          "citing_paper_doi": "10.48550/arXiv.2410.08688",
          "cited_paper_doi": "10.1007/978-3-030-58595-2_30",
          "citing_paper_url": "https://www.semanticscholar.org/paper/3b58b1884ffa794d8095cbf98ebe012c0735a61e",
          "cited_paper_url": "https://www.semanticscholar.org/paper/26f27bd5892e869b54c69d140b20c33638f26c57",
          "citing_paper_year": 2024,
          "cited_paper_year": 2020
        }
      ]
    },
    {
      "cited_paper_id": "3406592",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "DID-MDN"
      ],
      "dataset_details": [
        {
          "dataset_name": "DID-MDN",
          "dataset_description": "Used for single image de-raining, focusing on density-aware methods using a multi-stream dense network to improve restoration quality. | Used to construct a more complex hybrid distortion dataset (DID-HY) for testing the approach on hybrid-distorted image restoration, increasing the difficulty of the task. | Used to evaluate single image de-raining methods, focusing on density-aware performance using a multi-stream dense network.",
          "citing_paper_id": "220686623",
          "cited_paper_id": 3406592,
          "context_text": "on DID-MDN dataset [40].",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions the DID-MDN dataset, which is a specific dataset used for image de-raining. The dataset is clearly identified and relevant to the topic of image restoration.",
          "citing_paper_doi": "10.1007/978-3-030-58526-6_19",
          "cited_paper_doi": "10.1109/CVPR.2018.00079",
          "citing_paper_url": "https://www.semanticscholar.org/paper/78a92b98a173a54ccf3cbc02b9b7fe3384dbef1d",
          "cited_paper_url": "https://www.semanticscholar.org/paper/34aaa735409b666b8dc678c23acb600b1d87913b",
          "citing_paper_year": 2020,
          "cited_paper_year": 2018
        }
      ]
    },
    {
      "cited_paper_id": "2166703",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "Lai dataset"
      ],
      "dataset_details": [
        {
          "dataset_name": "Lai dataset",
          "dataset_description": "Used to evaluate single image blind deblurring methods, containing real-world blurry images of varying qualities and resolutions across diverse scenes.",
          "citing_paper_id": "199543931",
          "cited_paper_id": 2166703,
          "context_text": "The Lai dataset [22] has real-world blurry images of different qualities and resolutions collected in various types of scenes.",
          "confidence_score": 1.0,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions a specific dataset, 'Lai dataset', which is used for image restoration research, particularly for single image blind deblurring.",
          "citing_paper_doi": "10.1109/ICCV.2019.00897",
          "cited_paper_doi": "10.1109/CVPR.2016.188",
          "citing_paper_url": "https://www.semanticscholar.org/paper/81c1dc00b32d12edfab00656593f5db56cfa79e0",
          "cited_paper_url": "https://www.semanticscholar.org/paper/d8fcb27eff4509a505c3c08860158c12d06a67a4",
          "citing_paper_year": 2019,
          "cited_paper_year": 2016
        }
      ]
    },
    {
      "cited_paper_id": "262341811",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "Kohler dataset"
      ],
      "dataset_details": [
        {
          "dataset_name": "Kohler dataset",
          "dataset_description": "Used to benchmark blind deconvolution algorithms, specifically evaluating performance on real-world camera shake with 4 images blurred using 12 different kernels.",
          "citing_paper_id": "199543931",
          "cited_paper_id": 262341811,
          "context_text": "The Kohler dataset [19] consists of 4 images, each blurred with 12 different kernels.",
          "confidence_score": 1.0,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The Kohler dataset is explicitly mentioned and described, fitting the criteria for a specific, verifiable dataset.",
          "citing_paper_doi": "10.1109/ICCV.2019.00897",
          "cited_paper_doi": "10.1007/978-3-642-33786-4_3",
          "citing_paper_url": "https://www.semanticscholar.org/paper/81c1dc00b32d12edfab00656593f5db56cfa79e0",
          "cited_paper_url": "https://www.semanticscholar.org/paper/97ce794dac6aecdca90c7222c4165914a280789b",
          "citing_paper_year": 2019,
          "cited_paper_year": 2012
        }
      ]
    },
    {
      "cited_paper_id": "13787274",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "RealDAE"
      ],
      "dataset_details": [
        {
          "dataset_name": "RealDAE",
          "dataset_description": "Used for evaluating document image unwarping, providing 130 images from the dataset for assessment. | Used for evaluating image restoration methods, specifically focusing on real-world degraded images from the testing set.",
          "citing_paper_id": "269614217",
          "cited_paper_id": 13787274,
          "context_text": "150 images from the testing set of RealDAE and 130 images from DocUNet [33] are used for evaluation.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions two specific datasets, RealDAE and DocUNet, which are used for evaluation in the image restoration research.",
          "citing_paper_doi": "10.1109/CVPR52733.2024.01482",
          "cited_paper_doi": "10.1109/CVPR.2018.00494",
          "citing_paper_url": "https://www.semanticscholar.org/paper/cd1ea89f90f9fe48fbf815ef5cfb18e92c56d022",
          "cited_paper_url": "https://www.semanticscholar.org/paper/00fc2f71a3fc3e67d8cc96b9a345f6b223c64aa6",
          "citing_paper_year": 2024,
          "cited_paper_year": 2018
        }
      ]
    },
    {
      "cited_paper_id": "14143575",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "Text Deblur Dataset (TDD)"
      ],
      "dataset_details": [
        {
          "dataset_name": "Text Deblur Dataset (TDD)",
          "dataset_description": "Used to train a model for text deblurring, specifically selecting 40K samples from the 66K available training samples.",
          "citing_paper_id": "269614217",
          "cited_paper_id": 14143575,
          "context_text": "The Text Deblur Dataset (TDD) [14] consists of 66K training samples, from which we randomly select 40K samples to train our model.",
          "confidence_score": 1.0,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions a specific dataset, TDD, which is used for training a model for text deblurring. The dataset is clearly identified and its usage is described.",
          "citing_paper_doi": "10.1109/CVPR52733.2024.01482",
          "cited_paper_doi": "10.5244/C.29.6",
          "citing_paper_url": "https://www.semanticscholar.org/paper/cd1ea89f90f9fe48fbf815ef5cfb18e92c56d022",
          "cited_paper_url": "https://www.semanticscholar.org/paper/423584105e5f6adc5981c87f2af8fc5ff2bb9064",
          "citing_paper_year": 2024,
          "cited_paper_year": 2015
        }
      ]
    },
    {
      "cited_paper_id": "52839416",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "Jung’s dataset"
      ],
      "dataset_details": [
        {
          "dataset_name": "Jung’s dataset",
          "dataset_description": "Used to form part of the testing set for image restoration, contributing 87 images to evaluate the performance of the proposed method. | Used to form part of the testing set for image restoration, contributing 300 images to evaluate the performance of the proposed method. | Used to form part of the testing set for image restoration, contributing 237 images to evaluate the performance of the proposed method.",
          "citing_paper_id": "269614217",
          "cited_paper_id": 52839416,
          "context_text": "We use Jung’s dataset [18] (87 images), Kligler’s dataset [19] (300 images) and OSR [55] (237 images) to form our testing set.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions three specific datasets by name, each with a clear number of images, which are used to form the testing set for image restoration.",
          "citing_paper_doi": "10.1109/CVPR52733.2024.01482",
          "cited_paper_doi": "10.1109/CVPR.2018.00252",
          "citing_paper_url": "https://www.semanticscholar.org/paper/cd1ea89f90f9fe48fbf815ef5cfb18e92c56d022",
          "cited_paper_url": "https://www.semanticscholar.org/paper/23d05ba7b8ebdff14c6eae7cf679932e38980e97",
          "citing_paper_year": 2024,
          "cited_paper_year": 2018
        }
      ]
    },
    {
      "cited_paper_id": "221541496",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "TDD"
      ],
      "dataset_details": [
        {
          "dataset_name": "TDD",
          "dataset_description": "Used to train models for deblurring photographed document images, specifically comparing the performance of DocRes, DE-GAN, and DocDiff in the deblurring task.",
          "citing_paper_id": "269614217",
          "cited_paper_id": 221541496,
          "context_text": "7, DocRes exhibits superior deblurring performance on photographed document images compared to DE-GAN [50] and DocDiff [61], both of which were also trained exclusively on the TDD dataset for the deblurring task.",
          "confidence_score": 1.0,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions 'TDD dataset' as a specific dataset used for training models for the deblurring task. The dataset is clearly identified and used in the research context.",
          "citing_paper_doi": "10.1109/CVPR52733.2024.01482",
          "cited_paper_doi": "10.1109/TPAMI.2020.3022406",
          "citing_paper_url": "https://www.semanticscholar.org/paper/cd1ea89f90f9fe48fbf815ef5cfb18e92c56d022",
          "cited_paper_url": "https://www.semanticscholar.org/paper/824d7f06c640c1c7f2d2905e29795f2b700247ce",
          "citing_paper_year": 2024,
          "cited_paper_year": 2020
        }
      ]
    },
    {
      "cited_paper_id": "222186074",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "Doc3DShade"
      ],
      "dataset_details": [
        {
          "dataset_name": "Doc3DShade",
          "dataset_description": "Used to train a model for intrinsic decomposition of document images, providing 90K synthetic images to enhance the training set. | Used to complement the synthetic data with 450 real-world images, ensuring the model generalizes well to actual document images.",
          "citing_paper_id": "269614217",
          "cited_paper_id": 222186074,
          "context_text": "The training set for this task contains 90K synthetic images from the Doc3DShade [6] dataset and 450 real-world images from the RealDAE [70] training set.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions two specific datasets, Doc3DShade and RealDAE, which are used for training a model for image restoration.",
          "citing_paper_doi": "10.1109/CVPR52733.2024.01482",
          "cited_paper_doi": "10.5244/c.34.188",
          "citing_paper_url": "https://www.semanticscholar.org/paper/cd1ea89f90f9fe48fbf815ef5cfb18e92c56d022",
          "cited_paper_url": "https://www.semanticscholar.org/paper/76e0f5f20b5d3f8b8988f9d217020013e78c0126",
          "citing_paper_year": 2024,
          "cited_paper_year": 2020
        }
      ]
    },
    {
      "cited_paper_id": "252917905",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "Doc3D"
      ],
      "dataset_details": [
        {
          "dataset_name": "Doc3D",
          "dataset_description": "Used for training models in document image rectification, focusing on geometric representation learning and enhancing restoration quality. | Used for testing the performance of the trained models, evaluating their effectiveness in document image rectification tasks.",
          "citing_paper_id": "269614217",
          "cited_paper_id": 252917905,
          "context_text": "We adopt the Doc3D [5] dataset and the DIR300 benchmark [11] for the training and testing, respectively.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions two specific datasets, Doc3D and DIR300, which are used for training and testing in the research.",
          "citing_paper_doi": "10.1109/CVPR52733.2024.01482",
          "cited_paper_doi": "10.48550/arXiv.2210.08161",
          "citing_paper_url": "https://www.semanticscholar.org/paper/cd1ea89f90f9fe48fbf815ef5cfb18e92c56d022",
          "cited_paper_url": "https://www.semanticscholar.org/paper/6afb9024f4f7efb2e7c5817d87cfb8267e2c758c",
          "citing_paper_year": 2024,
          "cited_paper_year": 2022
        }
      ]
    },
    {
      "cited_paper_id": "256616274",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "(H)-DIBCO"
      ],
      "dataset_details": [
        {
          "dataset_name": "(H)-DIBCO",
          "dataset_description": "Used to evaluate document binarization methods, focusing on the performance of gated convolutions in restoring degraded document images.",
          "citing_paper_id": "269614217",
          "cited_paper_id": 256616274,
          "context_text": "Following [61, 62], the remaining years of (H)-DIBCO datasets [12, 37, 40–45,",
          "confidence_score": 0.9,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The citation mentions (H)-DIBCO datasets, which are known datasets in the document binarization field. These datasets are likely used for evaluation or training in the context of image restoration.",
          "citing_paper_doi": "10.1109/CVPR52733.2024.01482",
          "cited_paper_doi": "10.48550/arXiv.2302.02073",
          "citing_paper_url": "https://www.semanticscholar.org/paper/cd1ea89f90f9fe48fbf815ef5cfb18e92c56d022",
          "cited_paper_url": "https://www.semanticscholar.org/paper/8870881d55aae315c5da8e3d76e74d72d5eac3bb",
          "citing_paper_year": 2024,
          "cited_paper_year": 2023
        }
      ]
    },
    {
      "cited_paper_id": "17093085",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "RNI15"
      ],
      "dataset_details": [
        {
          "dataset_name": "RNI15",
          "dataset_description": "Used to evaluate image denoising algorithms, specifically focusing on real noisy face images to assess performance in realistic conditions.",
          "citing_paper_id": "222103869",
          "cited_paper_id": 17093085,
          "context_text": "Real noisy face image from the RNI15 dataset [15].",
          "confidence_score": 1.0,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions 'RNI15 dataset' which is a specific dataset used for image restoration, particularly for noisy face images.",
          "citing_paper_doi": "10.1109/TNNLS.2021.3131739",
          "cited_paper_doi": "10.5201/ipol.2015.125",
          "citing_paper_url": "https://www.semanticscholar.org/paper/3d18527d24ff382df0b7fb155d4ce456569d704f",
          "cited_paper_url": "https://www.semanticscholar.org/paper/f322958f36e53a9f0611960e8927dfaa52bdc2a2",
          "citing_paper_year": 2020,
          "cited_paper_year": 2015
        }
      ]
    },
    {
      "cited_paper_id": "49672261",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "DND-SRGB IMAGES"
      ],
      "dataset_details": [
        {
          "dataset_name": "DND-SRGB IMAGES",
          "dataset_description": "Used to evaluate image restoration algorithms, specifically measuring PSNR improvements over existing methods. The dataset consists of real photographs for denoising tasks.",
          "citing_paper_id": "222103869",
          "cited_paper_id": 49672261,
          "context_text": "A LGORITHMS E VALUATED ON THE D N D S RGB I MAGES [83] i.e., 1.17-dB PSNR, compared to the second performing method, CBDNet [14].",
          "confidence_score": 0.9,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions 'DND-SRGB IMAGES' which appears to be a specific dataset used for evaluating image restoration algorithms. The dataset is used to measure performance in terms of PSNR.",
          "citing_paper_doi": "10.1109/TNNLS.2021.3131739",
          "cited_paper_doi": "10.1109/CVPR.2019.00181",
          "citing_paper_url": "https://www.semanticscholar.org/paper/3d18527d24ff382df0b7fb155d4ce456569d704f",
          "cited_paper_url": "https://www.semanticscholar.org/paper/852bad998bc2a1c8c86314da3b5b5a162a76d500",
          "citing_paper_year": 2020,
          "cited_paper_year": 2018
        }
      ]
    },
    {
      "cited_paper_id": "195787503",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "Rain100"
      ],
      "dataset_details": [
        {
          "dataset_name": "Rain100",
          "dataset_description": "Used for training and testing single-task image deraining models, consisting of 200 clean-rainy image pairs for training and 100 pairs for testing. | Used to evaluate deraining results in a single-task setting, focusing on the performance of the proposed method in removing rain from images. | Used to evaluate the deraining performance of DINO-IR, specifically testing on unseen rain degradation. The dataset provides synthetic rain images for benchmarking image restoration algorithms.",
          "citing_paper_id": "265609570",
          "cited_paper_id": 195787503,
          "context_text": "To evaluate the generalizability of our DINO-IR, we train on combined datasets with four types of degradation: 1. testing on unseen dataset Rain100 [44] for deraining; 2. testing on unseen degradation level σ = 100 for denoising.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions 'Rain100' as a dataset used for evaluating deraining performance. The dataset is clearly named and used for a specific purpose.",
          "citing_paper_doi": "10.48550/arXiv.2312.01677",
          "cited_paper_doi": "10.1109/TPAMI.2019.2925793",
          "citing_paper_url": "https://www.semanticscholar.org/paper/c2fbbe80f7374a3aade7dcc1c6617f91818ab729",
          "cited_paper_url": "https://www.semanticscholar.org/paper/40762c365a3ef8dac29d499426669d97fea4f0d2",
          "citing_paper_year": 2023,
          "cited_paper_year": 2019
        }
      ]
    },
    {
      "cited_paper_id": "545361",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "NYUv2"
      ],
      "dataset_details": [
        {
          "dataset_name": "NYUv2",
          "dataset_description": "Used for surface normal estimation, providing high-resolution RGB-D images of indoor scenes. | Used for monocular depth estimation, offering a large-scale RGB-D dataset with diverse indoor environments. | Used for evaluating image restoration techniques, specifically focusing on indoor scenes with RGBD images and surface normals. | Used for monocular depth estimation and surface normal estimation, providing RGB-D images of indoor scenes.",
          "citing_paper_id": "272826725",
          "cited_paper_id": 545361,
          "context_text": "Dense image prediction tasks are evaluated on the three vision tasks: ADE20k (2017b) for semantic segmentation, NYUv2 (2012) and SUNRGB-D (2014) for monocular depth estimation, and NYU-Depth v2 (2012) fir surface normal estimation.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The citation mentions specific datasets used for evaluating dense image prediction tasks, which are relevant to the topic of all-in-one image restoration.",
          "citing_paper_doi": "10.48550/arXiv.2409.15278",
          "cited_paper_doi": "10.1007/978-3-642-33715-4_54",
          "citing_paper_url": "https://www.semanticscholar.org/paper/2c0697de3deccb0fb88ca6674cfa3648943a8b00",
          "cited_paper_url": "https://www.semanticscholar.org/paper/c1994ba5946456fc70948c549daf62363f13fa2d",
          "citing_paper_year": 2024,
          "cited_paper_year": 2012
        }
      ]
    },
    {
      "cited_paper_id": "252917726",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "LAION Art dataset"
      ],
      "dataset_details": [
        {
          "dataset_name": "LAION Art dataset",
          "dataset_description": "Used to gather natural images for image restoration experiments, focusing on high-quality images from the Internet. The dataset supports the development and evaluation of image restoration techniques.",
          "citing_paper_id": "272826725",
          "cited_paper_id": 252917726,
          "context_text": "We first gather natural images from the LAION Art dataset (Schuhmann et al., 2022) and our own collection of high-quality images from the Internet.",
          "confidence_score": 0.9,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions the LAION Art dataset, which is a specific, verifiable dataset. The 'our own collection' is too generic and not included.",
          "citing_paper_doi": "10.48550/arXiv.2409.15278",
          "cited_paper_doi": "10.48550/arXiv.2210.08402",
          "citing_paper_url": "https://www.semanticscholar.org/paper/2c0697de3deccb0fb88ca6674cfa3648943a8b00",
          "cited_paper_url": "https://www.semanticscholar.org/paper/e5c8960eb2ec034ffbd353ef39fd1cb541d3c7c9",
          "citing_paper_year": 2024,
          "cited_paper_year": 2022
        }
      ]
    },
    {
      "cited_paper_id": "54482423",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "FFHQ-raw"
      ],
      "dataset_details": [
        {
          "dataset_name": "FFHQ-raw",
          "dataset_description": "Used to improve the model’s face restoration performance by providing 70K unaligned high-resolution facial images.",
          "citing_paper_id": "267199774",
          "cited_paper_id": 54482423,
          "context_text": "We also included an additional 70K unaligned high-resolution facial images from the FFHQ-raw dataset [40] to improve the model’s face restoration performance.",
          "confidence_score": 1.0,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions the FFHQ-raw dataset, which is a specific dataset used to enhance face restoration performance.",
          "citing_paper_doi": "10.1109/CVPR52733.2024.02425",
          "cited_paper_doi": "10.1109/CVPR.2019.00453",
          "citing_paper_url": "https://www.semanticscholar.org/paper/3a63ae4086248e4ba4bd106839a26a08256909c4",
          "cited_paper_url": "https://www.semanticscholar.org/paper/ceb2ebef0b41e31c1a21b28c2734123900c005e2",
          "citing_paper_year": 2024,
          "cited_paper_year": 2018
        }
      ]
    },
    {
      "cited_paper_id": "233476615",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "REDS-val-300"
      ],
      "dataset_details": [
        {
          "dataset_name": "REDS-val-300",
          "dataset_description": "Used to evaluate image restoration techniques, specifically focusing on reducing JPEG compression artifacts. The dataset is characterized by images with a PSNR of 29.70. | Used for evaluating image deblurring methods, focusing on the validation set of 300 images from the REDS dataset, as part of the NTIRE 2021 Challenge.",
          "citing_paper_id": "234482841",
          "cited_paper_id": 233476615,
          "context_text": "For evaluation, we follow the setting in the NTIRE 2021 Challenge on Image Deblurring [32], i.e . use 300 images in the validation set of REDS, denoted as REDS-val-300 next.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions the use of the REDS dataset for evaluation, specifically the validation set containing 300 images. This is a clear and specific dataset reference.",
          "citing_paper_doi": "10.1109/CVPRW53098.2021.00027",
          "cited_paper_doi": "10.1109/CVPRW53098.2021.00025",
          "citing_paper_url": "https://www.semanticscholar.org/paper/9b8cbe9a636b00b3543d8e1f98c24de12d6c3d17",
          "cited_paper_url": "https://www.semanticscholar.org/paper/149ed92d2e52acc6645aec45cbda486e071e5fe4",
          "citing_paper_year": 2021,
          "cited_paper_year": 2021
        }
      ]
    },
    {
      "cited_paper_id": "502946",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "Cityscapes"
      ],
      "dataset_details": [
        {
          "dataset_name": "Cityscapes",
          "dataset_description": "Used for training and evaluating image restoration models, focusing on semantic understanding of urban scenes with high-resolution images and detailed annotations.",
          "citing_paper_id": "234335673",
          "cited_paper_id": 502946,
          "context_text": "(2019) based on the Cityscapes dataset (Cordts et al., 2016) .",
          "confidence_score": 1.0,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions the Cityscapes dataset, which is a well-known dataset for urban scene understanding. The dataset is used for training and evaluation in the context of image restoration.",
          "citing_paper_doi": "10.1007/s11263-022-01620-w",
          "cited_paper_doi": "10.1109/CVPR.2016.350",
          "citing_paper_url": "https://www.semanticscholar.org/paper/e4838d4210ad5663b3ea65182a32d47964874357",
          "cited_paper_url": "https://www.semanticscholar.org/paper/c8c494ee5488fe20e0aa01bddf3fc4632086d654",
          "citing_paper_year": 2021,
          "cited_paper_year": 2016
        }
      ]
    },
    {
      "cited_paper_id": "9455111",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "KITTI stereo 2012"
      ],
      "dataset_details": [
        {
          "dataset_name": "KITTI stereo 2012",
          "dataset_description": "Used to create a synthetic RainKITTI2012 dataset for evaluating image restoration techniques, focusing on rain removal and stereo vision tasks.",
          "citing_paper_id": "234335673",
          "cited_paper_id": 9455111,
          "context_text": "In this paper, we ﬁrst use Pho-toshop to create a synthetic RainKITTI2012 dataset based on the public KITTI stereo 2012 dataset [15].",
          "confidence_score": 1.0,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions the creation of a synthetic dataset called 'RainKITTI2012' based on the 'KITTI stereo 2012 dataset'. The KITTI dataset is a well-known benchmark for computer vision tasks, particularly relevant to image restoration.",
          "citing_paper_doi": "10.1007/s11263-022-01620-w",
          "cited_paper_doi": "10.1177/0278364913491297",
          "citing_paper_url": "https://www.semanticscholar.org/paper/e4838d4210ad5663b3ea65182a32d47964874357",
          "cited_paper_url": "https://www.semanticscholar.org/paper/79b949d9b35c3f51dd20fb5c746cc81fc87147eb",
          "citing_paper_year": 2021,
          "cited_paper_year": 2013
        }
      ]
    },
    {
      "cited_paper_id": "39760169",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "Indoor Training Set (ITS)"
      ],
      "dataset_details": [
        {
          "dataset_name": "Indoor Training Set (ITS)",
          "dataset_description": "Used to train and evaluate single-image dehazing models, focusing on synthetic scenes to benchmark performance and robustness. | Used for training dehazing models, containing 13,990 hazy indoor images generated from 1,399 clear images. | Serves as a comprehensive resource for single-image dehazing, containing the SOTS subset with synthetic hazy images for evaluating restoration algorithms. | A larger dataset that includes the Indoor Training Set (ITS), used for benchmarking single-image dehazing algorithms. | Used to benchmark single-image dehazing methods, focusing on synthetic hazy images both indoor and outdoor, to evaluate restoration quality.",
          "citing_paper_id": "260957038",
          "cited_paper_id": 39760169,
          "context_text": "The subset Indoor Training Set (ITS) of RESIDE contains a total of 13,990 hazy indoor images, generated from 1,399 clear images.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions a specific subset of the RESIDE dataset, which is used for training purposes in the context of single-image dehazing.",
          "citing_paper_doi": null,
          "cited_paper_doi": "10.1109/TIP.2018.2867951",
          "citing_paper_url": "https://www.semanticscholar.org/paper/4b688c486f2b223212682fd2b3f72cc0caa94b1b",
          "cited_paper_url": "https://www.semanticscholar.org/paper/abf75b81b96ff2cd262086675deba55f907108db",
          "citing_paper_year": 2023,
          "cited_paper_year": 2017
        }
      ]
    },
    {
      "cited_paper_id": "39760169",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "ITS dataset"
      ],
      "dataset_details": [
        {
          "dataset_name": "ITS dataset",
          "dataset_description": "Used for dehazing tasks, providing images with varying levels of haze to evaluate restoration algorithms. | Used for deraining tasks, containing a large set of images with rain streaks to assess the effectiveness of restoration methods.",
          "citing_paper_id": "271891992",
          "cited_paper_id": 39760169,
          "context_text": "ITS dataset [27] and Rain13K [22] are utilized for dehazing and deraining.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions two specific datasets, ITS and Rain13K, which are used for dehazing and deraining tasks. These datasets are clearly identified and relevant to the research topic.",
          "citing_paper_doi": "10.1145/3664647.3681621",
          "cited_paper_doi": "10.1109/TIP.2018.2867951",
          "citing_paper_url": "https://www.semanticscholar.org/paper/ca2e8f7b9e902aa917904cb4b75f8dc0845edbcd",
          "cited_paper_url": "https://www.semanticscholar.org/paper/abf75b81b96ff2cd262086675deba55f907108db",
          "citing_paper_year": 2024,
          "cited_paper_year": 2017
        }
      ]
    },
    {
      "cited_paper_id": "264693087",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "Adobe-MIT Fivek"
      ],
      "dataset_details": [
        {
          "dataset_name": "Adobe-MIT Fivek",
          "dataset_description": "Used for Low-Light Enhancement (LLE), focusing on improving image quality in low-light conditions through specific enhancement techniques. | Used to generate image pairs for training and evaluating image restoration models, focusing on global tonal adjustments and retouching techniques. | Used for retouching, Low-Light Filtering (LLF), and Multi-Task Modeling (MTM), focusing on expert-level image adjustments and multi-task learning approaches.",
          "citing_paper_id": "271891992",
          "cited_paper_id": 264693087,
          "context_text": "Expert-C retouched images of Adobe-MIT Fivek dataset [5] are also used to generate the image pairs.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions the 'Adobe-MIT Fivek dataset', which is a specific dataset used for generating image pairs for image restoration tasks.",
          "citing_paper_doi": "10.1145/3664647.3681621",
          "cited_paper_doi": null,
          "citing_paper_url": "https://www.semanticscholar.org/paper/ca2e8f7b9e902aa917904cb4b75f8dc0845edbcd",
          "cited_paper_url": "https://www.semanticscholar.org/paper/0250f9026b540ae05e2b6528b3c9064e6db637dd",
          "citing_paper_year": 2024,
          "cited_paper_year": 2011
        }
      ]
    },
    {
      "cited_paper_id": "254973973",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "large-scale dataset on low-light enhancement for ultra-high definition (UHD) images"
      ],
      "dataset_details": [
        {
          "dataset_name": "large-scale dataset on low-light enhancement for ultra-high definition (UHD) images",
          "dataset_description": "Used to train and evaluate models for enhancing low-light UHD images, focusing on improving visual quality and detail preservation.",
          "citing_paper_id": "267177503",
          "cited_paper_id": 254973973,
          "context_text": "Wang et al. [23] proposed a large-scale dataset on low-light enhancement for ultra-high deﬁnition (UHD) images.",
          "confidence_score": 0.9,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The citation mentions a large-scale dataset for low-light enhancement of UHD images, which is directly relevant to the topic of image restoration.",
          "citing_paper_doi": "10.1109/TMM.2024.3355634",
          "cited_paper_doi": "10.48550/arXiv.2212.11548",
          "citing_paper_url": "https://www.semanticscholar.org/paper/31566ce4292c83691135cecb60074c563c4958b8",
          "cited_paper_url": "https://www.semanticscholar.org/paper/baf3d202261f1eb9122a157fc6480d93e2c3d03c",
          "citing_paper_year": 2024,
          "cited_paper_year": 2022
        }
      ]
    },
    {
      "cited_paper_id": "233241040",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "synthetic image pairs"
      ],
      "dataset_details": [
        {
          "dataset_name": "synthetic image pairs",
          "dataset_description": "Used to train a conditional diffusion model for image super-resolution, focusing on generating high-resolution images from low-resolution inputs using synthetic data.",
          "citing_paper_id": "254125609",
          "cited_paper_id": 233241040,
          "context_text": "SR3 (Saharia et al., 2021) trains a conditional diffusion model for image super-resolution with synthetic image pairs as the training data.",
          "confidence_score": 0.6,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions 'synthetic image pairs' as the training data, which is a domain-qualified data phrase. However, it does not specify a named dataset.",
          "citing_paper_doi": "10.48550/arXiv.2212.00490",
          "cited_paper_doi": "10.1109/TPAMI.2022.3204461",
          "citing_paper_url": "https://www.semanticscholar.org/paper/3a75ed3e9e81c9db573ef73d20e2c66c12aaedf8",
          "cited_paper_url": "https://www.semanticscholar.org/paper/bc7e6165b00f0c39d40ca2c7a4eb33fcc0e3200d",
          "citing_paper_year": 2022,
          "cited_paper_year": 2021
        }
      ]
    },
    {
      "cited_paper_id": "unknown",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "ImageNet-50K"
      ],
      "dataset_details": [
        {
          "dataset_name": "ImageNet-50K",
          "dataset_description": "Used to compare DAMoE’s performance, focusing on image restoration quality and efficiency. | Used to highlight the limitations of large-scale, high-resolution image datasets in providing sufficient texture information for training image restoration models. | A high-resolution subset of Laion-5B used to demonstrate the insufficiency of texture information in training a good restoration model. | A high-resolution subset of Laion-5B, specifically used to create a large-scale restoration dataset. | Used to compare the size of image restoration datasets, highlighting its larger scale compared to other low-level datasets but smaller than multimodal pretraining datasets. | Used for multimodal pretraining, providing a large-scale dataset for creating a restoration dataset. | Proposed dataset used to compare DAMoE’s performance, focusing on high-quality image restoration.",
          "citing_paper_id": "259108489",
          "cited_paper_id": null,
          "context_text": "To further demonstrate the value of the proposed dataset, we compared DAMoE’s performance using ImageNet-50K [21], Laion-50K [41], and our proposed HQ-50K .",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions three specific datasets used for comparing performance, which are relevant to the image restoration topic.",
          "citing_paper_doi": "10.48550/arXiv.2306.05390",
          "cited_paper_doi": null,
          "citing_paper_url": "https://www.semanticscholar.org/paper/4296bf82e67051e4cba10e510932f55fc12dd1e2",
          "cited_paper_url": null,
          "citing_paper_year": 2023,
          "cited_paper_year": null
        }
      ]
    },
    {
      "cited_paper_id": "11935683",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "HYDICE DC Mall data"
      ],
      "dataset_details": [
        {
          "dataset_name": "HYDICE DC Mall data",
          "dataset_description": "Used to evaluate an algorithm's ability to remove noise by adding synthetic Gaussian noise and testing denoising performance.",
          "citing_paper_id": "207991682",
          "cited_paper_id": 11935683,
          "context_text": "We evaluate the ability of an algorithm to remove noise using HYDICE DC Mall data [31] with synthetically added Gaussian noise of σ = 100 .",
          "confidence_score": 1.0,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The citation mentions the use of HYDICE DC Mall data for evaluating an algorithm's ability to remove noise. The dataset is used to add synthetic Gaussian noise and test denoising performance.",
          "citing_paper_doi": "10.1109/ICCVW.2019.00477",
          "cited_paper_doi": "10.1109/JSTARS.2014.2329322",
          "citing_paper_url": "https://www.semanticscholar.org/paper/fce4a31248ef4c0e0bf227debf3038360b15b2d7",
          "cited_paper_url": "https://www.semanticscholar.org/paper/bd993b329b02b647a10f8407ad9916b14e96dea7",
          "citing_paper_year": 2019,
          "cited_paper_year": 2014
        }
      ]
    },
    {
      "cited_paper_id": "212647851",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "under-display camera dataset"
      ],
      "dataset_details": [
        {
          "dataset_name": "under-display camera dataset",
          "dataset_description": "Used to demonstrate AgenticIR's capability in restoring images from under-display cameras through motion deblurring, defocus deblurring, and brightening. | Used to showcase AgenticIR's performance in restoring underwater images by applying de-focus deblurring.",
          "citing_paper_id": "273532643",
          "cited_paper_id": 212647851,
          "context_text": "11 shows some examples: the first one is from an under-display camera dataset (Zhou et al., 2021), and AgenticIR restores it by motion deblurring, defocus deblurring, and brightening; the second one is from an underwater dataset (Zhou et al., 2021), and AgenticIR restores it by de-focus deblurring,…",
          "confidence_score": 0.9,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions two specific datasets: 'under-display camera dataset' and 'underwater dataset'. Both are used for image restoration tasks, specifically for demonstrating the capabilities of the AgenticIR method.",
          "citing_paper_doi": "10.48550/arXiv.2410.17809",
          "cited_paper_doi": "10.1109/CVPR46437.2021.00906",
          "citing_paper_url": "https://www.semanticscholar.org/paper/b13aaf8bd57e10823a1e5374750fafd0699ef8f7",
          "cited_paper_url": "https://www.semanticscholar.org/paper/2050ca7f9535710f74d698f4fc227eade31d546b",
          "citing_paper_year": 2024,
          "cited_paper_year": 2020
        }
      ]
    },
    {
      "cited_paper_id": "91184545",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "SPAData"
      ],
      "dataset_details": [
        {
          "dataset_name": "SPAData",
          "dataset_description": "Used to evaluate deraining performance, focusing on single-image deraining techniques with a high-quality real rain dataset. | Used to benchmark deraining results, specifically comparing SPDNet and SPDNet-local methods on single-image deraining tasks. | Used to evaluate deraining results with different inference methods, focusing on single-image deraining performance and quality.",
          "citing_paper_id": "250451699",
          "cited_paper_id": 91184545,
          "context_text": "Table 5: Deraining results on SPAData [44] dataset",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions the use of the SPAData dataset for deraining results, which is relevant to the topic of image restoration.",
          "citing_paper_doi": "10.1007/978-3-031-20071-7_4",
          "cited_paper_doi": "10.1109/CVPR.2019.01255",
          "citing_paper_url": "https://www.semanticscholar.org/paper/6f1cd5055f85ed15e7b61f599946a934c25d39b6",
          "cited_paper_url": "https://www.semanticscholar.org/paper/d3fa312b1f12ee60391705d3cac34cbcad42db14",
          "citing_paper_year": 2021,
          "cited_paper_year": 2019
        }
      ]
    },
    {
      "cited_paper_id": "227239228",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "IPT Dataset"
      ],
      "dataset_details": [
        {
          "dataset_name": "IPT Dataset",
          "dataset_description": "Used to train and evaluate the MIMO-UNet+ model for image restoration, focusing on the performance of the transformer-based approach in handling various image degradation issues.",
          "citing_paper_id": "250451699",
          "cited_paper_id": 227239228,
          "context_text": "SPAIR MIMO-UNet+ IPT Dataset [10] [62] [31] [57] [40] [33] [8] [5]",
          "confidence_score": 0.85,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions 'IPT Dataset' which appears to be a specific dataset used in the research. The title does not provide additional disambiguation but confirms the dataset is related to image processing.",
          "citing_paper_doi": "10.1007/978-3-031-20071-7_4",
          "cited_paper_doi": "10.1109/CVPR46437.2021.01212",
          "citing_paper_url": "https://www.semanticscholar.org/paper/6f1cd5055f85ed15e7b61f599946a934c25d39b6",
          "cited_paper_url": "https://www.semanticscholar.org/paper/6f6f73e69ee0d9d5d7d088bb882db1851d98175a",
          "citing_paper_year": 2021,
          "cited_paper_year": 2020
        }
      ]
    },
    {
      "cited_paper_id": "16119123",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "CUB"
      ],
      "dataset_details": [
        {
          "dataset_name": "CUB",
          "dataset_description": "Used to compare different IRMD methods on bird classification tasks, specifically evaluating performance under low contrast corruption conditions. | Used for evaluating image restoration methods under single and multiple degradations, focusing on image classification performance. | Used to pre-train recognition models (VGG16, ResNet50) on clean images before evaluating their performance on restored test images from a degraded test set.",
          "citing_paper_id": "260810536",
          "cited_paper_id": 16119123,
          "context_text": "Complete comparison with state-of-the-art IRSD (i.e., image restoration for single degradation) and IRMD (i.e., image restoration for multiple degradation) approaches for image classification on CUB dataset among two different degradation.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions the CUB dataset, which is a specific dataset used for image classification tasks, particularly in the domain of image restoration for single and multiple degradations.",
          "citing_paper_doi": "10.1109/CVPR52729.2023.01351",
          "cited_paper_doi": null,
          "citing_paper_url": "https://www.semanticscholar.org/paper/c2e9a21f9f0fee0c8b18a3eb275298a406737f14",
          "cited_paper_url": "https://www.semanticscholar.org/paper/c069629a51f6c1c301eb20ed77bc6b586c24ce32",
          "citing_paper_year": 2023,
          "cited_paper_year": 2011
        }
      ]
    },
    {
      "cited_paper_id": "54482423",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "CelebAHQ"
      ],
      "dataset_details": [
        {
          "dataset_name": "CelebAHQ",
          "dataset_description": "Used to align and crop images, ensuring consistency in preprocessing steps for image restoration tasks. | Used as the training dataset for generative models, focusing on high-resolution face images to improve model performance in image restoration tasks. | Used to select high-quality images for generating low-quality images for the synthetic test dataset, ensuring no identity overlap with FFHQ.",
          "citing_paper_id": "221802293",
          "cited_paper_id": 54482423,
          "context_text": "For the synthetic test dataset, we randomly choose 2, 800 HQ images from CelebAHQ [14] which has no identity intersection with FFHQ, and then generate the corresponding LQ images in the same way as training dataset.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions two specific datasets, CelebAHQ and FFHQ, used for generating synthetic test data. Both are relevant to image restoration and have clear identifiers.",
          "citing_paper_doi": "10.1109/CVPR46437.2021.01172",
          "cited_paper_doi": "10.1109/CVPR.2019.00453",
          "citing_paper_url": "https://www.semanticscholar.org/paper/1b8e9c1cc34846e4396f4accd4114bab686829f4",
          "cited_paper_url": "https://www.semanticscholar.org/paper/ceb2ebef0b41e31c1a21b28c2734123900c005e2",
          "citing_paper_year": 2020,
          "cited_paper_year": 2018
        }
      ]
    },
    {
      "cited_paper_id": "52059988",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "SIDD Validation"
      ],
      "dataset_details": [
        {
          "dataset_name": "SIDD Validation",
          "dataset_description": "Used to evaluate denoising performance on real-world noisy images from smartphone cameras, focusing on validation aspects. | Used to assess denoising performance on a diverse set of real-world noisy images, providing a comprehensive evaluation framework. | Used to benchmark denoising algorithms on real-world noisy images from smartphone cameras, emphasizing performance metrics.",
          "citing_paper_id": "260887508",
          "cited_paper_id": 52059988,
          "context_text": "We evaluate the denoising performance on three widely used real-world noisy datasets SIDD[1] Validation, SIDD[1] Benchmark, and the DND[40] Benchmark.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions three specific datasets used for evaluating denoising performance. These datasets are clearly named and relevant to the research topic of image restoration.",
          "citing_paper_doi": "10.1109/ICCV51070.2023.01162",
          "cited_paper_doi": "10.1109/CVPR.2018.00182",
          "citing_paper_url": "https://www.semanticscholar.org/paper/9ceb9a02486d6cb41b5b49bd9745f899b95330bb",
          "cited_paper_url": "https://www.semanticscholar.org/paper/ebf35073e122782f685a0d6c231622412f28a53b",
          "citing_paper_year": 2023,
          "cited_paper_year": 2018
        }
      ]
    },
    {
      "cited_paper_id": "102351079",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "DND Benchmark"
      ],
      "dataset_details": [
        {
          "dataset_name": "DND Benchmark",
          "dataset_description": "Used to evaluate and compare the performance of SCPGabNet and PD+ in image denoising, focusing on real noise scenarios. | Used to train SCPGabNet, enabling it to achieve superior performance on both SIDD Benchmark and SIDD validation sets. | Used to evaluate SCPGabNet’s performance, demonstrating its superiority over PD+ by more than 2dB in denoising tasks. | Used to assess SCPGabNet's performance on real-world image denoising, demonstrating its superiority over PD+ by more than 2dB. | Used to train SCPGabNet, highlighting its ability to generalize well to other benchmarks despite being trained on a single dataset. | Used to evaluate the performance of PD+ and SCPGabNet, comparing their effectiveness in denoising images under real noise conditions.",
          "citing_paper_id": "260887508",
          "cited_paper_id": 102351079,
          "context_text": "Although PD+[18] performs slightly better than SCPGabNet on the DND Benchmark, SCPGabNet’s performance is significantly superior to PD+ on both SIDD Benchmark and SIDD validation by more than 2dB. SCPGabNet is trained only on the SIDD training set and still exhibits a greater advantage on other benchmarks, indicating its greater generalizability than two-step methods.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions the DND Benchmark and SIDD Benchmark, which are specific datasets used for evaluating image denoising algorithms. These datasets are used to compare the performance of SCPGabNet against PD+.",
          "citing_paper_doi": "10.1109/ICCV51070.2023.01162",
          "cited_paper_doi": "10.1609/AAAI.V34I07.7009",
          "citing_paper_url": "https://www.semanticscholar.org/paper/9ceb9a02486d6cb41b5b49bd9745f899b95330bb",
          "cited_paper_url": "https://www.semanticscholar.org/paper/4476e8ae97eabc5710c98ad073061e9f610dd825",
          "citing_paper_year": 2023,
          "cited_paper_year": 2019
        }
      ]
    },
    {
      "cited_paper_id": "502946",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "RainCityscape"
      ],
      "dataset_details": [
        {
          "dataset_name": "RainCityscape",
          "dataset_description": "Provides urban scene images used as the base for synthesizing rain and haze effects in the RainCityscape and Rendering datasets. | Used to synthesize rain and haze effects on urban scenes, enhancing the Cityscape dataset for all-in-one image restoration tasks. | Used to synthesize rain effects on urban scenes, enhancing the Cityscape dataset for all-in-one image restoration tasks.",
          "citing_paper_id": "232352745",
          "cited_paper_id": 502946,
          "context_text": "We utilize two different synthetic rain datasets, RainCityscape [12] and Rendering [11], which synthesize rain and haze with different models on the Cityscape dataset [5].",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions two synthetic rain datasets, RainCityscape and Rendering, which are used to synthesize rain and haze on the Cityscape dataset. These are specific datasets used for image restoration tasks.",
          "citing_paper_doi": "10.1109/CVPR46437.2021.00209",
          "cited_paper_doi": "10.1109/CVPR.2016.350",
          "citing_paper_url": "https://www.semanticscholar.org/paper/d033c802467b2dd8e91a69abaf62a2e5b941d3a6",
          "cited_paper_url": "https://www.semanticscholar.org/paper/c8c494ee5488fe20e0aa01bddf3fc4632086d654",
          "citing_paper_year": 2021,
          "cited_paper_year": 2016
        }
      ]
    },
    {
      "cited_paper_id": "84187160",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "real rain dataset"
      ],
      "dataset_details": [
        {
          "dataset_name": "real rain dataset",
          "dataset_description": "Used to evaluate single image deraining methods, focusing on real-world scenarios in driving and surveillance without clean counterparts.",
          "citing_paper_id": "232352745",
          "cited_paper_id": 84187160,
          "context_text": "[17] provided real rain dataset in driving and surveillance without clean counterparts.",
          "confidence_score": 0.85,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions a 'real rain dataset' which is specific and relevant to the topic of image restoration, particularly deraining.",
          "citing_paper_doi": "10.1109/CVPR46437.2021.00209",
          "cited_paper_doi": "10.1109/CVPR.2019.00396",
          "citing_paper_url": "https://www.semanticscholar.org/paper/d033c802467b2dd8e91a69abaf62a2e5b941d3a6",
          "cited_paper_url": "https://www.semanticscholar.org/paper/fbc0b44b7a93721978a362465a3046646b075e57",
          "citing_paper_year": 2021,
          "cited_paper_year": 2019
        }
      ]
    },
    {
      "cited_paper_id": "4539586",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "Raindrop test dataset"
      ],
      "dataset_details": [
        {
          "dataset_name": "Raindrop test dataset",
          "dataset_description": "Used for training an image restoration model, focusing on rain removal from outdoor scenes using a dataset of real rainy images. | Used for training an image restoration model, focusing on raindrop removal from images using a dataset of real and synthetic raindrop images. | Used to assess the effectiveness of the method in removing raindrops from images, specifically measuring the quality of raindrop removal. | Used to evaluate the performance of the image restoration method, focusing on general image quality and restoration accuracy. | Introduced for raindrop removal, this dataset supports training and evaluation of models designed to restore images degraded by raindrops. | Used for evaluating the performance of trained models on raindrop removal, specifically assessing their effectiveness in real-world scenarios. | Used for training an image restoration model, focusing on snow removal from images using a large dataset of synthetic snowy images. | Used to evaluate the method's ability to remove snow from images, focusing on the quality and realism of snow removal. | Used for training models to handle various weather conditions, focusing on raindrop removal from images using generative adversarial networks.",
          "citing_paper_id": "244714491",
          "cited_paper_id": 4539586,
          "context_text": "All of these models are trained on All-Weather and tested on the Raindrop test dataset.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions 'All-Weather' and 'Raindrop test dataset', which are specific datasets used for training and testing models in image restoration, particularly for raindrop removal.",
          "citing_paper_doi": "10.1109/CVPR52688.2022.00239",
          "cited_paper_doi": "10.1109/CVPR.2018.00263",
          "citing_paper_url": "https://www.semanticscholar.org/paper/b27d3be4264dcd06f990b44968f4382526f24f1e",
          "cited_paper_url": "https://www.semanticscholar.org/paper/34e38559df559bfedfc3e1edc2b120ab2b5d444e",
          "citing_paper_year": 2021,
          "cited_paper_year": 2017
        }
      ]
    },
    {
      "cited_paper_id": "261035981",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "Sea-thru"
      ],
      "dataset_details": [
        {
          "dataset_name": "Sea-thru",
          "dataset_description": "Used to test the removal of water effects from underwater images, evaluating the effectiveness of restoration algorithms. | Used as a general-purpose image dataset to test the robustness and generalization of image restoration methods. | Used to test image restoration methods on underwater images, specifically evaluating the removal of water effects to enhance image clarity. | Used to evaluate underwater image enhancement techniques, assessing color correction and clarity improvements. | Used to enhance low-resolution distorted underwater images, providing high-resolution pairs for image restoration evaluation. | Used to remove water effects from underwater images, focusing on real images with depth map information for image restoration. | Used to evaluate image restoration techniques on a diverse set of underwater images, focusing on improving visual quality and detail recovery. | Used to test underwater image enhancement methods, focusing on visual quality and restoration accuracy. | Used to evaluate image restoration methods, specifically comparing performance on underwater image de-watering. The dataset provides a benchmark for assessing the effectiveness of various restoration techniques.",
          "citing_paper_id": "253708090",
          "cited_paper_id": 261035981,
          "context_text": "-UG), the testing images from Sea-thru [11] and the UFO-120 [61] dataset.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions two specific datasets, 'Sea-thru' and 'UFO-120', which are used for testing images in the context of underwater image restoration.",
          "citing_paper_doi": "10.48550/arXiv.2211.10026",
          "cited_paper_doi": "10.1109/CVPR.2019.00178",
          "citing_paper_url": "https://www.semanticscholar.org/paper/29412f7b8cff20da3221937e066619aaa5f78655",
          "cited_paper_url": "https://www.semanticscholar.org/paper/192e3271d658b27b23c7cbbce178a74ccc1002db",
          "citing_paper_year": 2022,
          "cited_paper_year": 2019
        }
      ]
    },
    {
      "cited_paper_id": "250551851",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "Snow100K-real"
      ],
      "dataset_details": [
        {
          "dataset_name": "Snow100K-real",
          "dataset_description": "Used to compare MWFormer against other models on real weather-degraded images, focusing on restoration performance under unknown corruptions.",
          "citing_paper_id": "274281468",
          "cited_paper_id": 250551851,
          "context_text": "We also compared MWFormer against other models on the real weather-degraded images from the Snow100K-real Input AirNet [13] TransWeather [2] WeatherDiffusion [1] MWFormer (ours) Fig.",
          "confidence_score": 1.0,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions 'Snow100K-real' as a dataset used for comparing model performance on weather-degraded images. No other specific datasets are mentioned.",
          "citing_paper_doi": "10.1109/TIP.2024.3501855",
          "cited_paper_doi": "10.1109/CVPR52688.2022.01693",
          "citing_paper_url": "https://www.semanticscholar.org/paper/d39a9d1e085cc1d74fa7b9ed27e13c6356647de2",
          "cited_paper_url": "https://www.semanticscholar.org/paper/c85f67a49cfdc29a77bd741a59e739cdc814edde",
          "citing_paper_year": 2024,
          "cited_paper_year": 2022
        }
      ]
    },
    {
      "cited_paper_id": "251903039",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "real-world rainy images"
      ],
      "dataset_details": [
        {
          "dataset_name": "real-world rainy images",
          "dataset_description": "Used to train a novel deraining network, focusing on real-world scenarios to improve image restoration quality.",
          "citing_paper_id": "274281468",
          "cited_paper_id": 251903039,
          "context_text": "Ba et al. [30] proposed a novel deraining network trained on a new and comprehensive dataset of real-world rainy images.",
          "confidence_score": 0.85,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions a 'new and comprehensive dataset of real-world rainy images' which is specific and plausible. The dataset is used for training a deraining network.",
          "citing_paper_doi": "10.1109/TIP.2024.3501855",
          "cited_paper_doi": "10.1007/978-3-031-20071-7_42",
          "citing_paper_url": "https://www.semanticscholar.org/paper/d39a9d1e085cc1d74fa7b9ed27e13c6356647de2",
          "cited_paper_url": "https://www.semanticscholar.org/paper/cd2a6f9dde5a297be0eb58cbfb879be3b42c306d",
          "citing_paper_year": 2024,
          "cited_paper_year": 2022
        }
      ]
    },
    {
      "cited_paper_id": "20372",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "dataset by Heide et al. (2015)"
      ],
      "dataset_details": [
        {
          "dataset_name": "dataset by Heide et al. (2015)",
          "dataset_description": "Used to evaluate image restoration methods on 11 grayscale images, focusing on performance and quality of restored images. | Used to evaluate image restoration methods on 68 RGB images, assessing the effectiveness of restoration techniques on color images.",
          "citing_paper_id": "209323842",
          "cited_paper_id": 20372,
          "context_text": "We evaluate on the standard dataset by Heide et al. (2015), consisting of 11 grayscale images, and the CBSD68 dataset by (Roth and Black 2009) consisting of 68 RGB images.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions two specific datasets used for evaluation: the dataset by Heide et al. (2015) and the CBSD68 dataset. Both are relevant to image restoration.",
          "citing_paper_doi": "10.1007/s11263-021-01572-7",
          "cited_paper_doi": "10.1109/CVPR.2015.7299149",
          "citing_paper_url": "https://www.semanticscholar.org/paper/594adc4a13d5be3545de97bd94fc5b268bca0f90",
          "cited_paper_url": "https://www.semanticscholar.org/paper/f236aa74a84156c175771384c605e8d85f4e05fc",
          "citing_paper_year": 2021,
          "cited_paper_year": 2015
        }
      ]
    },
    {
      "cited_paper_id": "1475121",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "standard dataset by Dabov et al. (2007)"
      ],
      "dataset_details": [
        {
          "dataset_name": "standard dataset by Dabov et al. (2007)",
          "dataset_description": "Used for image denoising experiments, consisting of 68 RGB images to assess the performance of denoising methods. | Used for image restoration tasks, specifically image denoising and JPEG image deblocking, to evaluate performance under different noise levels and compression qualities. | Used for empirical studies on image restoration tasks, specifically image denoising, JPEG image deblocking, and image inpainting, to evaluate performance under various conditions. | Used for image denoising experiments, consisting of 9 RGB images to evaluate denoising algorithms. | Used for evaluating image restoration tasks, specifically image denoising, JPEG image deblocking, and image inpainting, with specific degradation parameters. | Used to evaluate image denoising techniques, specifically focusing on the performance of sparse 3-D transform-domain collaborative filtering methods. | Used for denoising comparison, consisting of 68 RGB images to assess the performance of image restoration methods. | Used for denoising comparison, consisting of 9 RGB images to evaluate image restoration techniques.",
          "citing_paper_id": "209323842",
          "cited_paper_id": 1475121,
          "context_text": "For the denoising comparison we use two datasets, i.e. , the standard dataset by Dabov et al. (2007) consisting of 9 RGB images, and CBSD68 by Roth and Black (2009) consisting of 68 RGB images.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions two specific datasets used for denoising comparisons, both of which are clearly identified and relevant to the research topic.",
          "citing_paper_doi": "10.1007/s11263-021-01572-7",
          "cited_paper_doi": "10.1109/TIP.2007.901238",
          "citing_paper_url": "https://www.semanticscholar.org/paper/594adc4a13d5be3545de97bd94fc5b268bca0f90",
          "cited_paper_url": "https://www.semanticscholar.org/paper/51d267b782e7caf2b6bc7240b1a5f48044ffe115",
          "citing_paper_year": 2021,
          "cited_paper_year": 2007
        }
      ]
    },
    {
      "cited_paper_id": "53770387",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "RENOIR"
      ],
      "dataset_details": [
        {
          "dataset_name": "RENOIR",
          "dataset_description": "Used to evaluate image restoration methods, specifically focusing on denoising images corrupted by realistic noise, enhancing the quality of low-light photography.",
          "citing_paper_id": "232170566",
          "cited_paper_id": 53770387,
          "context_text": "from [10,50] and 120 images corrupted by realistic noise from RENOIR [44] dataset, which is similar to CBDNet [8].",
          "confidence_score": 1.0,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions the RENOIR dataset, which is used for image denoising experiments. The dataset is explicitly named and used in the research context.",
          "citing_paper_doi": "10.1109/TMM.2021.3063916",
          "cited_paper_doi": "10.1109/CVPR.2019.01129",
          "citing_paper_url": "https://www.semanticscholar.org/paper/7d76ae738c806330632255ad2c592525b3d2df22",
          "cited_paper_url": "https://www.semanticscholar.org/paper/735ddb8a24aedc27a39d3401de9e942dce4c6983",
          "citing_paper_year": 2021,
          "cited_paper_year": 2018
        }
      ]
    },
    {
      "cited_paper_id": "64193",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "91 images"
      ],
      "dataset_details": [
        {
          "dataset_name": "91 images",
          "dataset_description": "Used to train and evaluate early image restoration methods, focusing on a small set of 91 images to assess performance. | Used to train and evaluate early image restoration methods, focusing on a larger set of 400 images to assess generalization and robustness.",
          "citing_paper_id": "260843368",
          "cited_paper_id": 64193,
          "context_text": "Early methods uses small datasets such as the 91 images proposed in [82] and the 400 images from BSD dataset [57].",
          "confidence_score": 0.9,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions two specific datasets: '91 images' and 'BSD dataset'. The '91 images' is a small, specific dataset, and 'BSD dataset' is a well-known dataset in image processing.",
          "citing_paper_doi": "10.1109/CVPRW59228.2023.00178",
          "cited_paper_doi": "10.1109/ICCV.2001.937655",
          "citing_paper_url": "https://www.semanticscholar.org/paper/851446ea3c70c918cdf8c62db0dd0b1900806e5f",
          "cited_paper_url": "https://www.semanticscholar.org/paper/9a1ed876196ec9733acb1daa6d65e35ff0414291",
          "citing_paper_year": 2023,
          "cited_paper_year": 2001
        }
      ]
    },
    {
      "cited_paper_id": "9007541",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "Rain12"
      ],
      "dataset_details": [
        {
          "dataset_name": "Rain12",
          "dataset_description": "Used to train and evaluate rain streak removal algorithms, focusing on synthetic rain streaks in images. | Used to train and evaluate raindrop removal algorithms, focusing on synthetic raindrops in images. | Used to train and evaluate rain streak removal algorithms, focusing on synthetic rain streaks in high-resolution images.",
          "citing_paper_id": "235702784",
          "cited_paper_id": 9007541,
          "context_text": "Rain12 [25] 12 syn rain streak Rain200H [43] 2,000 syn rain streak Rain800 [52] 800 syn rain streak Rain1200 [51] 12,000 syn rain streak RainDrop [30] 2,238 syn raindrop",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions several datasets with specific names and sizes, all related to synthetic rain streaks and raindrops, which are relevant to image restoration.",
          "citing_paper_doi": "10.1109/CVPR46437.2021.00903",
          "cited_paper_doi": "10.1109/CVPR.2016.299",
          "citing_paper_url": "https://www.semanticscholar.org/paper/c86f565984f83232d925534b34f585a0a4278522",
          "cited_paper_url": "https://www.semanticscholar.org/paper/7c17025c540b88df14da35229618b5e896ab9528",
          "citing_paper_year": 2021,
          "cited_paper_year": 2016
        }
      ]
    },
    {
      "cited_paper_id": "837707",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "Real-world Task-driven Testing Set (RTTS)"
      ],
      "dataset_details": [
        {
          "dataset_name": "Real-world Task-driven Testing Set (RTTS)",
          "dataset_description": "Used for evaluating image restoration methods on real-world degraded images, focusing on task-driven performance metrics. | Used for real-world experimental analysis in snow removal, offering realistic synthetic snow images to evaluate the effectiveness of the model. | Used to test snow removal algorithms, specifically assessing the realism and quality of desnowed images. | Used for real-world experimental analysis in snow removal, providing diverse real-world scenarios to test the robustness of the model. | Used to evaluate rain removal techniques in driving scenarios, focusing on the clarity and detail preservation in rainy conditions. | Used for real-world experimental analysis in snow removal, providing a range of real and synthetic images to assess the performance of the model.",
          "citing_paper_id": "267023529",
          "cited_paper_id": 837707,
          "context_text": "To scrutinize this, we have used three different real-world degraded image datasets named as Real-world Task-driven Testing Set (RTTS) [13], Snow realistic [19] and Rain In Driving (RID) [31] for experimental analysis .",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions three specific datasets used for experimental analysis in the context of image restoration, particularly for real-world degraded images.",
          "citing_paper_doi": "10.1109/ICCV51070.2023.01983",
          "cited_paper_doi": "10.1109/TIP.2018.2806202",
          "citing_paper_url": "https://www.semanticscholar.org/paper/853b59cd5b3bcb70a1b18d79a62e916c4e31a2bd",
          "cited_paper_url": "https://www.semanticscholar.org/paper/9e7487d7396acf298f972f0b8ab0397e0d9e7a0f",
          "citing_paper_year": 2023,
          "cited_paper_year": 2017
        }
      ]
    },
    {
      "cited_paper_id": "204959605",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "PIE"
      ],
      "dataset_details": [
        {
          "dataset_name": "PIE",
          "dataset_description": "Used to generate synthetic rainy data for image deraining, focusing on first-person view driving scenes for autonomous driving applications. | Used to generate synthetic rainy data for image deraining, focusing on first-person view driving scenes for pedestrian intention estimation and trajectory prediction.",
          "citing_paper_id": "235702784",
          "cited_paper_id": 204959605,
          "context_text": "As image deraining is highly demanded in autonomous driving, we generate synthetic rainy data based on the ﬁrst-person view driving scenes collected from two public autonomous driving datasets, i.e. , PIE [34] and KITTI [13].",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions the use of two public autonomous driving datasets, PIE and KITTI, for generating synthetic rainy data. These datasets are used to simulate first-person view driving scenes for image deraining.",
          "citing_paper_doi": "10.1109/CVPR46437.2021.00903",
          "cited_paper_doi": "10.1109/ICCV.2019.00636",
          "citing_paper_url": "https://www.semanticscholar.org/paper/c86f565984f83232d925534b34f585a0a4278522",
          "cited_paper_url": "https://www.semanticscholar.org/paper/36194c76ce53be8e8fba71acbf8d235c7b39342b",
          "citing_paper_year": 2021,
          "cited_paper_year": 2019
        }
      ]
    },
    {
      "cited_paper_id": "244461440",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "Comprehensive Snow Database (CSD)"
      ],
      "dataset_details": [
        {
          "dataset_name": "Comprehensive Snow Database (CSD)",
          "dataset_description": "Used for training and testing image restoration algorithms, specifically addressing snow and fog degradation with 8000 training and 2000 testing images.",
          "citing_paper_id": "267023529",
          "cited_paper_id": 244461440,
          "context_text": "Comprehensive Snow Database (CSD) [3] : 8000 and 2000 images are provided for training and testing purpose respectively with combination of snow and fog degradation.",
          "confidence_score": 1.0,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions a specific dataset, Comprehensive Snow Database (CSD), which is used for training and testing purposes in image restoration, particularly for snow and fog degradation.",
          "citing_paper_doi": "10.1109/ICCV51070.2023.01983",
          "cited_paper_doi": "10.1109/ICCV48922.2021.00416",
          "citing_paper_url": "https://www.semanticscholar.org/paper/853b59cd5b3bcb70a1b18d79a62e916c4e31a2bd",
          "cited_paper_url": "https://www.semanticscholar.org/paper/51b09bddafff606138fe3e69acb9121f3415aad9",
          "citing_paper_year": 2023,
          "cited_paper_year": 2021
        }
      ]
    },
    {
      "cited_paper_id": "250581068",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "ORD database"
      ],
      "dataset_details": [
        {
          "dataset_name": "ORD database",
          "dataset_description": "Used to evaluate the proposed image restoration method against state-of-the-art approaches, focusing on outdoor scenes with adverse weather conditions. | Used to test the effectiveness of the proposed method in removing rain artifacts from outdoor images, comparing it with other state-of-the-art techniques. | Used to evaluate the performance of the proposed multi-weather image restoration method, focusing on object detection and depth estimation in degraded and restored images. | Used to assess the performance of the proposed method in restoring images, particularly in challenging outdoor scenarios.",
          "citing_paper_id": "267023529",
          "cited_paper_id": 250581068,
          "context_text": "Object detection and depth estimation analysis on degraded and restored images by existing UMVR [11], KD [4], TW [33], and proposed multi-weather image restoration method. posed architecture in the ablation study conducted on ORD database.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions the ORD database, which is a specific dataset used for evaluating the performance of image restoration methods under various weather conditions.",
          "citing_paper_doi": "10.1109/ICCV51070.2023.01983",
          "cited_paper_doi": "10.1109/CVPR52688.2022.01713",
          "citing_paper_url": "https://www.semanticscholar.org/paper/853b59cd5b3bcb70a1b18d79a62e916c4e31a2bd",
          "cited_paper_url": "https://www.semanticscholar.org/paper/50eb3c6c167d3aa1a06cb4f451c05729c9f58a99",
          "citing_paper_year": 2023,
          "cited_paper_year": 2022
        }
      ]
    },
    {
      "cited_paper_id": "244488366",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "RESIDE/SOTS"
      ],
      "dataset_details": [
        {
          "dataset_name": "RESIDE/SOTS",
          "dataset_description": "Used to evaluate IRNeXt on real-world dehazing, particularly in dense haze scenarios, to test robustness and performance. | Used to evaluate IRNeXt on synthetic dehazing, focusing on performance metrics like PSNR to compare with other methods. | Used to evaluate IRNeXt on real-world dehazing, assessing its effectiveness in handling natural haze conditions.",
          "citing_paper_id": "260927679",
          "cited_paper_id": 244488366,
          "context_text": "We evaluate IRNeXt on the synthetic dataset (RESIDE/SOTS (Li et al., 2018a)) and real-world datasets (NHHAZE (Ancuti et al., 2020) and Dense-Haze (Ancuti et al.,\nDeepRFT\nDRBNet\nRestormer\nIRNeXt\nImage\n11.04 dB\nInput\nPSNR\nGT\n33.77 dB FFA-Net 33.88 dB DeHamer\n29.18 dB MAXIM 38.77 dB IRNeXt\nInput/18.00 dB GT/PSNR DBGAN/23.27 dB DMPHN/25.14 dB\nMIMO-Unet++/24.15 dB MPRNet/24.14 dB Restormer/27.34 dB IRNeXt/28.17 dBBlurry Image\nFigure 5.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions specific datasets used for evaluating IRNeXt, including RESIDE/SOTS, NHHAZE, and Dense-Haze. These datasets are relevant to the topic of image restoration.",
          "citing_paper_doi": null,
          "cited_paper_doi": null,
          "citing_paper_url": "https://www.semanticscholar.org/paper/3fb08a1c07eba3461d6ed784b25d952979e90e0d",
          "cited_paper_url": "https://www.semanticscholar.org/paper/3c5ab1aba2091c690729b5a8f0762ecda3d06163",
          "citing_paper_year": 2023,
          "cited_paper_year": 2021
        }
      ]
    },
    {
      "cited_paper_id": "244346143",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "APEX"
      ],
      "dataset_details": [
        {
          "dataset_name": "APEX",
          "dataset_description": "Used to simulate noisy hyperspectral images by introducing band-specific noise levels, focusing on the restoration of hyperspectral images under varying noise conditions.",
          "citing_paper_id": "273230367",
          "cited_paper_id": 244346143,
          "context_text": "We introduce band-speciﬁc noise levels ranging from 0 to 55 to the APEX dataset for simulating the noisy HSIs, following the same conﬁgurations as in the previous study [5].",
          "confidence_score": 1.0,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions the APEX dataset, which is used to simulate noisy hyperspectral images (HSIs) by introducing band-specific noise levels. This is relevant to the research topic of image restoration.",
          "citing_paper_doi": "10.1109/TPAMI.2024.3475249",
          "cited_paper_doi": null,
          "citing_paper_url": "https://www.semanticscholar.org/paper/fb7d060045ef6e2614d6fa36d2b00cb1f4b62737",
          "cited_paper_url": "https://www.semanticscholar.org/paper/7b86bf1452b21f52d8c714b52c742fcbf6e4645c",
          "citing_paper_year": 2024,
          "cited_paper_year": 2021
        }
      ]
    },
    {
      "cited_paper_id": "241033103",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "Laion400M"
      ],
      "dataset_details": [
        {
          "dataset_name": "Laion400M",
          "dataset_description": "Used to pre-train the CLIP model, providing a large-scale dataset of image-text pairs to enhance generalization ability in image restoration tasks.",
          "citing_paper_id": "268364340",
          "cited_paper_id": 241033103,
          "context_text": "Particularly, CLIPIQA leverages the CLIP [108] model, pre-trained on the extensive Laion400M [109] dataset, thereby demonstrating strong generalization ability.",
          "confidence_score": 1.0,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "Laion400M is explicitly mentioned as a dataset used for pre-training the CLIP model, which is relevant to the research context of image restoration.",
          "citing_paper_doi": "10.1109/TPAMI.2024.3461721",
          "cited_paper_doi": null,
          "citing_paper_url": "https://www.semanticscholar.org/paper/decac28a79f9cbb5be2658f35a6cdb5bc705c2bc",
          "cited_paper_url": "https://www.semanticscholar.org/paper/b668ce936cff0b0ca8b635cd5f25a62eaf4eb3df",
          "citing_paper_year": 2024,
          "cited_paper_year": 2021
        }
      ]
    },
    {
      "cited_paper_id": "251040466",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "RealSR-V3"
      ],
      "dataset_details": [
        {
          "dataset_name": "RealSR-V3",
          "dataset_description": "Used for evaluating image restoration approaches, focusing on real-world super-resolution performance and quality. | Used for evaluating image restoration approaches, focusing on real-world image enhancement and restoration quality.",
          "citing_paper_id": "268364340",
          "cited_paper_id": 251040466,
          "context_text": "Table VI lists the comparative evaluation using CLIP-IQA [106] and MUSIQ [107] for various approaches on two real-world datasets, namely RealSR -V3 [98] and RealSet80 .",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions two real-world datasets, RealSR-V3 and RealSet80, which are used for comparative evaluation of image restoration approaches.",
          "citing_paper_doi": "10.1109/TPAMI.2024.3461721",
          "cited_paper_doi": "10.48550/arXiv.2207.12396",
          "citing_paper_url": "https://www.semanticscholar.org/paper/decac28a79f9cbb5be2658f35a6cdb5bc705c2bc",
          "cited_paper_url": "https://www.semanticscholar.org/paper/03ae89e796b1a8b566ae1554fab65c8c88b3a55f",
          "citing_paper_year": 2024,
          "cited_paper_year": 2022
        }
      ]
    },
    {
      "cited_paper_id": "8187730",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "91 images from [36]"
      ],
      "dataset_details": [
        {
          "dataset_name": "91 images from [36]",
          "dataset_description": "Used for training a model for image de-noising tasks, providing a larger set of diverse images. | Used for training a model for image de-noising tasks, providing a small set of high-quality images.",
          "citing_paper_id": "996788",
          "cited_paper_id": 8187730,
          "context_text": "To learn a single model for the three general image de-noising tasks, as in [35], we use a dataset which consists of 91 images from [36] and 200 training images from the Berkeley segmentation dataset.",
          "confidence_score": 0.9,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions two specific datasets used for training a model for image de-noising tasks. The datasets are clearly identified and have specific names.",
          "citing_paper_doi": "10.1109/TIP.2017.2662206",
          "cited_paper_doi": "10.1109/TIP.2010.2050625",
          "citing_paper_url": "https://www.semanticscholar.org/paper/0c00a328fa7cd56ee60338c54e89bd48310db80b",
          "cited_paper_url": "https://www.semanticscholar.org/paper/a230000e34fbd0e5e16be6cb89d998691b62ccc2",
          "citing_paper_year": 2016,
          "cited_paper_year": 2010
        }
      ]
    },
    {
      "cited_paper_id": "46898260",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "GOPRO TEST SET"
      ],
      "dataset_details": [
        {
          "dataset_name": "GOPRO TEST SET",
          "dataset_description": "Used to compare the performance of BA and SA methods using BANET (STACK-4) for image restoration, focusing on deblurring tasks.",
          "citing_paper_id": "231639270",
          "cited_paper_id": 46898260,
          "context_text": "TABLE IX PERFORMANCE COMPARISON BETWEEN BA AND SA [15] USING BANET (STACK-4) ON GOPRO TEST SET.",
          "confidence_score": 1.0,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions 'GOPRO TEST SET', which is a specific dataset used for image restoration and deblurring tasks. The dataset is used to compare performance between BA and SA using BANET (STACK-4).",
          "citing_paper_doi": "10.1109/TIP.2022.3216216",
          "cited_paper_doi": null,
          "citing_paper_url": "https://www.semanticscholar.org/paper/010e74e834c12e840e603b53ad14cbfc454ce7a1",
          "cited_paper_url": "https://www.semanticscholar.org/paper/a8f3dc53e321fbb2565f5925def4365b9f68d1af",
          "citing_paper_year": 2021,
          "cited_paper_year": 2018
        }
      ]
    },
    {
      "cited_paper_id": "57246310",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "ImageNet/CVF"
      ],
      "dataset_details": [
        {
          "dataset_name": "ImageNet/CVF",
          "dataset_description": "Used for training and evaluation, comprising a diverse array of figures from computer vision papers to enhance model performance on visual data. | Used to create degraded-clean training pairs for image restoration, focusing on introducing specific distortions to the images. | Used to extract crude images for MAE training and to stitch paired images for inference, focusing on image restoration techniques.",
          "citing_paper_id": "264146360",
          "cited_paper_id": 57246310,
          "context_text": "2, MAE-VQGAN utilizes crude images extracted from the ImageNet/CVF dataset during MAE training, and stitches paired images as a whole image for inference.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions the ImageNet/CVF dataset, which is a specific, verifiable dataset used for training and inference in the described method.",
          "citing_paper_doi": "10.48550/arXiv.2310.10513",
          "cited_paper_doi": "10.1109/CVPR.2009.5206848",
          "citing_paper_url": "https://www.semanticscholar.org/paper/836906c334159d46a691dfc5466c33caa3d22f65",
          "cited_paper_url": "https://www.semanticscholar.org/paper/d2c733e34d48784a37d717fe43d9e93277a8c53e",
          "citing_paper_year": 2023,
          "cited_paper_year": 2009
        }
      ]
    },
    {
      "cited_paper_id": "2972940",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "MEF"
      ],
      "dataset_details": [
        {
          "dataset_name": "MEF",
          "dataset_description": "Used to assess naturalness-preserved enhancement algorithms, specifically for non-uniform illumination images without ground truth. | Used to test image restoration methods, particularly for dynamic range compression and multi-exposure fusion without ground truth. | Used to evaluate multi-exposure image fusion techniques, focusing on perceptual quality and naturalness preservation in non-uniform illumination images.",
          "citing_paper_id": "268513542",
          "cited_paper_id": 2972940,
          "context_text": "We further generalize to the widely used real-world datasets MEF [34], NPE [56] and DICM [21] without ground truth.",
          "confidence_score": 0.9,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions three datasets: MEF, NPE, and DICM. These are likely specific datasets used for image restoration and enhancement tasks.",
          "citing_paper_doi": "10.1109/CVPR52733.2024.02404",
          "cited_paper_doi": "10.1109/TIP.2013.2261309",
          "citing_paper_url": "https://www.semanticscholar.org/paper/1ff44c75c361d2ed28f868361a29437197b0270c",
          "cited_paper_url": "https://www.semanticscholar.org/paper/e6887e7b20007c88aa0e9c48626f165d05eee273",
          "citing_paper_year": 2024,
          "cited_paper_year": 2013
        }
      ]
    },
    {
      "cited_paper_id": "207761262",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "FiveK"
      ],
      "dataset_details": [
        {
          "dataset_name": "FiveK",
          "dataset_description": "Used to evaluate the model's performance in image restoration, specifically measuring PSNR values. | Used to evaluate the model's performance in low-light image enhancement, specifically measuring SSIM values.",
          "citing_paper_id": "245837508",
          "cited_paper_id": 207761262,
          "context_text": "6 illustrates, our model achieved the best PSNR and SSIM values on FiveK [7] and LOL [96], respectively.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions 'FiveK' and 'LOL' as datasets used for evaluating the model's performance in terms of PSNR and SSIM values.",
          "citing_paper_doi": "10.1109/CVPR52688.2022.00568",
          "cited_paper_doi": "10.1109/TIP.2003.819861",
          "citing_paper_url": "https://www.semanticscholar.org/paper/fbf68eb0cf8237cef59ebdb301569c79b9676ff9",
          "cited_paper_url": "https://www.semanticscholar.org/paper/eae2e0fa72e898c289365c0af16daf57a7a6cf40",
          "citing_paper_year": 2022,
          "cited_paper_year": 2004
        }
      ]
    },
    {
      "cited_paper_id": "52008443",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "Low-Light enhancement dataset (LOL)"
      ],
      "dataset_details": [
        {
          "dataset_name": "Low-Light enhancement dataset (LOL)",
          "dataset_description": "Used for the task of low-light image enhancement, specifically to improve the quality of images captured in low-light conditions using deep retinex decomposition techniques.",
          "citing_paper_id": "259243623",
          "cited_paper_id": 52008443,
          "context_text": "The Low-Light enhancement dataset (LOL) [14] is designed for the task of low-light image enhancement.",
          "confidence_score": 1.0,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The citation clearly mentions the 'Low-Light enhancement dataset (LOL)', which is a specific dataset used for low-light image enhancement. The context indicates that this dataset is designed for the specific task of enhancing images captured in low-light conditions.",
          "citing_paper_doi": "10.48550/arXiv.2306.13653",
          "cited_paper_doi": null,
          "citing_paper_url": "https://www.semanticscholar.org/paper/fe94fa22e2ffc22d61cff41a96cc2f012ebde3c1",
          "cited_paper_url": "https://www.semanticscholar.org/paper/c7798bbbcd2e85593a2b2d37b92dabac63420cd8",
          "citing_paper_year": 2023,
          "cited_paper_year": 2018
        }
      ]
    },
    {
      "cited_paper_id": "207778473",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "RESIDE-6K"
      ],
      "dataset_details": [
        {
          "dataset_name": "RESIDE-6K",
          "dataset_description": "Used for image dehazing, a new task for ProRes, to remove haze and improve clarity in images. | Used for low-light enhancement, focusing on improving image quality under low-light conditions using the ProRes framework.",
          "citing_paper_id": "259243623",
          "cited_paper_id": 207778473,
          "context_text": "Specifically, we adopt the FiveK dataset [44] for low-light enhancement and the RESIDE-6K dataset [45] for image dehazing, which is a new task for ProRes.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions two specific datasets, FiveK and RESIDE-6K, which are used for low-light enhancement and image dehazing, respectively, in the ProRes framework.",
          "citing_paper_doi": "10.48550/arXiv.2306.13653",
          "cited_paper_doi": null,
          "citing_paper_url": "https://www.semanticscholar.org/paper/fe94fa22e2ffc22d61cff41a96cc2f012ebde3c1",
          "cited_paper_url": "https://www.semanticscholar.org/paper/5ee57b18a7be2b671ffc6f5ce5dbbe92112ee0fb",
          "citing_paper_year": 2023,
          "cited_paper_year": 2007
        }
      ]
    },
    {
      "cited_paper_id": "174788791",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "Set14 × 2"
      ],
      "dataset_details": [
        {
          "dataset_name": "Set14 × 2",
          "dataset_description": "Used for performance comparison in single image super-resolution, focusing on efficiency and performance metrics such as FLOPs, running time, and peak memory consumption.",
          "citing_paper_id": "260748050",
          "cited_paper_id": 174788791,
          "context_text": "Here we present the efﬁciency (FLOPs, running time, and peak memory consumption) and performance comparison (on Set14 × 2) with prior state-of-the-art SAN Dai et al. (2019) and our conference version CSNLN Mei et al. (2020).",
          "confidence_score": 0.9,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions 'Set14 × 2' which appears to be a specific dataset used for performance comparison in image super-resolution tasks.",
          "citing_paper_doi": "10.1007/s11263-023-01843-5",
          "cited_paper_doi": "10.1109/CVPR.2019.01132",
          "citing_paper_url": "https://www.semanticscholar.org/paper/e98cc6c252a7d69898730df17449fc1a09d1a1ae",
          "cited_paper_url": "https://www.semanticscholar.org/paper/fd2a0a326db4f034fe22340c20b7bacd9a14c3d6",
          "citing_paper_year": 2023,
          "cited_paper_year": 2019
        }
      ]
    },
    {
      "cited_paper_id": "85498726",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "EUVP"
      ],
      "dataset_details": [
        {
          "dataset_name": "EUVP",
          "dataset_description": "Used for qualitative comparisons in underwater image enhancement, providing visual results to demonstrate improved visual perception.",
          "citing_paper_id": "276960871",
          "cited_paper_id": 85498726,
          "context_text": "For qualitative comparisons, we provide visual re-sults on real-world underwater unpaired dataset EUVP [24] in Figure 6.",
          "confidence_score": 1.0,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions a specific dataset 'EUVP' used for visual results in underwater image enhancement.",
          "citing_paper_doi": "10.48550/arXiv.2503.10120",
          "cited_paper_doi": "10.1109/LRA.2020.2974710",
          "citing_paper_url": "https://www.semanticscholar.org/paper/8d5b315df8c45238becce7237c1512145d4bb36d",
          "cited_paper_url": "https://www.semanticscholar.org/paper/ee56d8dc8d5900716d763d1cc94f3d226d8f9a25",
          "citing_paper_year": 2025,
          "cited_paper_year": 2019
        }
      ]
    },
    {
      "cited_paper_id": "13058320",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "standard test dataset of 68 natural images"
      ],
      "dataset_details": [
        {
          "dataset_name": "standard test dataset of 68 natural images",
          "dataset_description": "Used to evaluate the denoising performance of a trained model, focusing on Gaussian denoising testing with a standard set of 68 natural images.",
          "citing_paper_id": "15799108",
          "cited_paper_id": 13058320,
          "context_text": "We then evaluate the denoising performance of a trained model on a standard test dataset of 68 natural images, which is suggested by [48], and later widely used for Gaussian denoising testing.",
          "confidence_score": 0.9,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions a 'standard test dataset of 68 natural images' which is widely used for Gaussian denoising testing. This dataset is specific and has a clear identifier, making it a valid dataset.",
          "citing_paper_doi": "10.1109/TPAMI.2016.2596743",
          "cited_paper_doi": "10.1007/s11263-008-0197-6",
          "citing_paper_url": "https://www.semanticscholar.org/paper/eb04068416ade86de63cf9d9939e14d0bc9b96f9",
          "cited_paper_url": "https://www.semanticscholar.org/paper/dfcae80f4d34ac09ba8063c5cfb5be954d0bf5f1",
          "citing_paper_year": 2015,
          "cited_paper_year": 2009
        }
      ]
    },
    {
      "cited_paper_id": "545361",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "Middlebury Stereo"
      ],
      "dataset_details": [
        {
          "dataset_name": "Middlebury Stereo",
          "dataset_description": "Used to synthesize the RESIDE dataset, contributing stereo images for both indoor and outdoor scenes.",
          "citing_paper_id": "259459919",
          "cited_paper_id": 545361,
          "context_text": "RESIDE is synthesized from NYU V2 [68] and Middlebury Stereo datasets [69] including both indoor and outdoor scenes.",
          "confidence_score": 1.0,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions two specific datasets, NYU V2 and Middlebury Stereo, which are used to synthesize the RESIDE dataset. These datasets are relevant to the topic of image restoration.",
          "citing_paper_doi": "10.1109/TCSVT.2023.3286405",
          "cited_paper_doi": "10.1007/978-3-642-33715-4_54",
          "citing_paper_url": "https://www.semanticscholar.org/paper/fca6cb29677c6fe278290ee1814d9bc31c155a65",
          "cited_paper_url": "https://www.semanticscholar.org/paper/c1994ba5946456fc70948c549daf62363f13fa2d",
          "citing_paper_year": 2024,
          "cited_paper_year": 2012
        }
      ]
    },
    {
      "cited_paper_id": "6158161",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "Nature dataset"
      ],
      "dataset_details": [
        {
          "dataset_name": "Nature dataset",
          "dataset_description": "Used to evaluate image restoration methods on 20 image pairs containing both indoor and outdoor scenes, focusing on the effectiveness of restoration algorithms in diverse environments. | Used to evaluate single-image reflection removal algorithms on diverse wild scenes, containing 55 image triplets. | Used to evaluate single-image reflection removal algorithms, specifically measuring performance using PSNR, to assess the effectiveness of the proposed U^2 method. | Used to test image restoration algorithms on 55 image triplets in wild scenes, evaluating the robustness of methods in uncontrolled, real-world conditions. | Used to assess reflection removal performance in diverse wild scenes, comprising 55 image triplets. | Used to evaluate reflection removal algorithms on indoor solid object scenes, containing 200 image triplets. | Used to evaluate single-image reflection removal algorithms on challenging postcard images, containing 199 triplet images. | Used to assess image restoration techniques on 200 image triplets featuring indoor solid object scenes, emphasizing the accuracy of restoration in controlled settings. | Used to evaluate single-image reflection removal algorithms on indoor solid object scenes, containing 200 image triplets. | Challenging dataset for evaluating reflection removal on 199 triplet images from postcards.",
          "citing_paper_id": "259459919",
          "cited_paper_id": 6158161,
          "context_text": "…scenes; 2) Nature dataset [18] which is constructed by IBCLN [18] with 20 image pairs containing indoor and outdoor scenes; 3) Solid dataset [60] that contains 200 image triplets involving indoor solid object scenes; 4) Wild dataset [60] which contains 55 image triplets in wild scenes; 5)…",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions specific datasets with clear identifiers and their usage in the research. These datasets are used for evaluating image restoration methods.",
          "citing_paper_doi": "10.1109/TCSVT.2023.3286405",
          "cited_paper_doi": "10.1109/ICCV.2017.423",
          "citing_paper_url": "https://www.semanticscholar.org/paper/fca6cb29677c6fe278290ee1814d9bc31c155a65",
          "cited_paper_url": "https://www.semanticscholar.org/paper/d71494fcefb86978348474036ccb92f65e5c6dab",
          "citing_paper_year": 2024,
          "cited_paper_year": 2017
        }
      ]
    },
    {
      "cited_paper_id": "49209675",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "Real20"
      ],
      "dataset_details": [
        {
          "dataset_name": "Real20",
          "dataset_description": "Used to evaluate image restoration methods on 20 real image pairs from various scenes, focusing on the effectiveness of restoration techniques across diverse environments. | Contains images used for evaluating the robustness of restoration algorithms, though specific details about the content are not provided in the context. | Constructed by IBCLN, this dataset is used to assess restoration performance on 20 image pairs with both indoor and outdoor scenes, emphasizing the versatility of the methods.",
          "citing_paper_id": "259459919",
          "cited_paper_id": 49209675,
          "context_text": "Quantitative evaluation is performed on five real world datasets including: 1) Real20 [62] consisting of 20 real image pairs from various scenes; 2) Nature dataset [18] which is constructed by IBCLN [18] with 20 image pairs containing indoor and outdoor scenes; 3) Solid dataset [60] that contains…",
          "confidence_score": 0.9,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions three specific datasets used for quantitative evaluation in the context of image restoration. These datasets are clearly named and their contents are described.",
          "citing_paper_doi": "10.1109/TCSVT.2023.3286405",
          "cited_paper_doi": "10.1109/CVPR.2018.00503",
          "citing_paper_url": "https://www.semanticscholar.org/paper/fca6cb29677c6fe278290ee1814d9bc31c155a65",
          "cited_paper_url": "https://www.semanticscholar.org/paper/1a70351b6fe7d14a6928e730416f7f28f84237f1",
          "citing_paper_year": 2024,
          "cited_paper_year": 2018
        }
      ]
    },
    {
      "cited_paper_id": "268856875",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "RESIDE-OTS"
      ],
      "dataset_details": [
        {
          "dataset_name": "RESIDE-OTS",
          "dataset_description": "Used to evaluate all-in-one image restoration models, focusing on outdoor scene dehazing and other degradation types. | Used to evaluate all-in-one image restoration models, focusing on rain, haze, noise, blur, and low-light conditions. | Used to evaluate all-in-one image restoration models, focusing on synthetic outdoor scene dehazing and other degradation types.",
          "citing_paper_id": "273502246",
          "cited_paper_id": 268856875,
          "context_text": "…RESIDE-OTS, RESIDE-SOTS AirNet [63], PromptIR [65],PIP [72], Textpromp-tIR [94], NDR [96], InstructIR [62], AdaIR [78], U-WADN [186], DyNet [185], DaAIR [187], AnyIR [188], MEASNet [110], HAIR [189], Perceive-IR [111] rain-haze-noise-blur-dark[66] 5 Rain, Haze, Noise-σ 25 , Blur, Low-Light…",
          "confidence_score": 0.9,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions several datasets and methods, but only 'RESIDE-OTS', 'RESIDE-SOTS', and 'rain-haze-noise-blur-dark' appear to be specific datasets used for image restoration tasks.",
          "citing_paper_doi": "10.48550/arXiv.2410.15067",
          "cited_paper_doi": "10.48550/arXiv.2404.02154",
          "citing_paper_url": "https://www.semanticscholar.org/paper/f5ff17162a4936db3f64ed7bafae13abcfa63a27",
          "cited_paper_url": "https://www.semanticscholar.org/paper/1d99d28c3cf5a21671a9cf5de8b51009c72d4624",
          "citing_paper_year": 2024,
          "cited_paper_year": 2024
        }
      ]
    },
    {
      "cited_paper_id": "4710407",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "OST"
      ],
      "dataset_details": [
        {
          "dataset_name": "OST",
          "dataset_description": "Used to evaluate the reconstruction task in image super-resolution, focusing on the quality of texture recovery in 300 images with rich textures.",
          "citing_paper_id": "259137590",
          "cited_paper_id": 4710407,
          "context_text": "In the test stage, we evaluate the reconstruction task on OST dataset [43], which contains 300 images with rich textures.",
          "confidence_score": 1.0,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions the OST dataset, which is used for evaluating the reconstruction task in image super-resolution. The dataset is described as containing 300 images with rich textures.",
          "citing_paper_doi": "10.1109/ICCV51070.2023.00495",
          "cited_paper_doi": "10.1109/CVPR.2018.00070",
          "citing_paper_url": "https://www.semanticscholar.org/paper/81c739551f9122f5dc5ddf78900c577e716ad49a",
          "cited_paper_url": "https://www.semanticscholar.org/paper/9e788f1530af08a1f2140e6016fd4aeaa8b29033",
          "citing_paper_year": 2023,
          "cited_paper_year": 2018
        }
      ]
    },
    {
      "cited_paper_id": "64193",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "Berkeley image segmentation database"
      ],
      "dataset_details": [
        {
          "dataset_name": "Berkeley image segmentation database",
          "dataset_description": "Used to extract fog-aware statistical features from a corpus of fog-free images, focusing on image quality assessment and segmentation. | Used to extract fog-aware statistical features from a corpus of fog-free images, focusing on image quality and segmentation. | Used to extract fog-aware statistical features from a corpus of fog-free images, focusing on image segmentation and natural scene analysis.",
          "citing_paper_id": "10225720",
          "cited_paper_id": 64193,
          "context_text": "To extract fog aware statistical features from a corpus of fog-free images, we selected 500 natural fog-free images from the LIVE IQA database [47], the Berkeley image segmentation database [48], the IRCCyN/IVC database [49], and the CSIQ database [50].",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions specific databases used to extract fog-aware statistical features from fog-free images. These databases are clearly identified and used for image processing and segmentation.",
          "citing_paper_doi": "10.1109/TIP.2015.2456502",
          "cited_paper_doi": "10.1109/ICCV.2001.937655",
          "citing_paper_url": "https://www.semanticscholar.org/paper/c299f73ff1e50f91f64700e07dea1371d4afcafa",
          "cited_paper_url": "https://www.semanticscholar.org/paper/9a1ed876196ec9733acb1daa6d65e35ff0414291",
          "citing_paper_year": 2015,
          "cited_paper_year": 2001
        }
      ]
    },
    {
      "cited_paper_id": "9341989",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "LIVE IQA database"
      ],
      "dataset_details": [
        {
          "dataset_name": "LIVE IQA database",
          "dataset_description": "Used to extract fog-aware statistical features from a corpus of fog-free images, focusing on image quality assessment and segmentation. | Used to extract fog-aware statistical features from a corpus of fog-free images, focusing on image quality and segmentation. | Used to extract fog-aware statistical features from a corpus of fog-free images, focusing on image segmentation and natural scene analysis.",
          "citing_paper_id": "10225720",
          "cited_paper_id": 9341989,
          "context_text": "To extract fog aware statistical features from a corpus of fog-free images, we selected 500 natural fog-free images from the LIVE IQA database [47], the Berkeley image segmentation database [48], the IRCCyN/IVC database [49], and the CSIQ database [50].",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions specific databases used to extract fog-aware statistical features from fog-free images. These databases are clearly identified and used for image processing and segmentation.",
          "citing_paper_doi": "10.1109/TIP.2015.2456502",
          "cited_paper_doi": "10.1117/1.3267105",
          "citing_paper_url": "https://www.semanticscholar.org/paper/c299f73ff1e50f91f64700e07dea1371d4afcafa",
          "cited_paper_url": "https://www.semanticscholar.org/paper/4a298957d060a986b50609e297739ecfa93db57b",
          "citing_paper_year": 2015,
          "cited_paper_year": 2010
        }
      ]
    },
    {
      "cited_paper_id": "5804823",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "AAPM 2016 grand challenge dataset"
      ],
      "dataset_details": [
        {
          "dataset_name": "AAPM 2016 grand challenge dataset",
          "dataset_description": "Used for training and evaluating a model for low-dose X-ray CT reconstruction, focusing on improving image quality and reducing noise.",
          "citing_paper_id": "249282628",
          "cited_paper_id": 5804823,
          "context_text": "For experiments with CT, we train our model based on ncsnpp as a VE-SDE from score-SDE [41], on the 2016 American Association of Physicists in Medicine (AAPM) grand challenge dataset, and we process the data as in [23].",
          "confidence_score": 1.0,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions the AAPM 2016 grand challenge dataset, which is a specific, verifiable dataset used for training and evaluation in the context of low-dose X-ray CT reconstruction.",
          "citing_paper_doi": "10.48550/arXiv.2206.00941",
          "cited_paper_doi": "10.1002/mp.12344",
          "citing_paper_url": "https://www.semanticscholar.org/paper/b3f5cf32178bcbed91aa5303b70963c6463f48a2",
          "cited_paper_url": "https://www.semanticscholar.org/paper/5bc52b33c3a0555025c9bc1a71eb533cd2702bbc",
          "citing_paper_year": 2022,
          "cited_paper_year": 2016
        }
      ]
    },
    {
      "cited_paper_id": "212647851",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "Enhancing Under-water Visual Perception (EUVP)"
      ],
      "dataset_details": [
        {
          "dataset_name": "Enhancing Under-water Visual Perception (EUVP)",
          "dataset_description": "Used to evaluate underexposure and blur artifact reduction techniques, focusing on enhancing visual perception in underwater images.",
          "citing_paper_id": "264145824",
          "cited_paper_id": 212647851,
          "context_text": "Step 1: “A photo needs undereposure artifact reduction” Step 2: “A photo needs blur artifact reduction” [76] and unseen Enhancing Under-water Visual Perception (EUVP) dataset [18].",
          "confidence_score": 0.9,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions the EUVP dataset, which is relevant to the image restoration topic, particularly for underexposure and blur artifact reduction.",
          "citing_paper_doi": "10.48550/arXiv.2310.10123",
          "cited_paper_doi": "10.1109/CVPR46437.2021.00906",
          "citing_paper_url": "https://www.semanticscholar.org/paper/8efc22d25fc33d73aa8de1044e5afe775ed87e31",
          "cited_paper_url": "https://www.semanticscholar.org/paper/2050ca7f9535710f74d698f4fc227eade31d546b",
          "citing_paper_year": 2023,
          "cited_paper_year": 2020
        }
      ]
    },
    {
      "cited_paper_id": "261697392",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "unseen under-display-camera dataset"
      ],
      "dataset_details": [
        {
          "dataset_name": "unseen under-display-camera dataset",
          "dataset_description": "Used to test image restoration techniques on under-display-camera images, focusing on artifact reduction and image quality enhancement. | Used to evaluate image restoration methods on underwater images, addressing issues like color correction and clarity improvement.",
          "citing_paper_id": "264145824",
          "cited_paper_id": 261697392,
          "context_text": "To demonstrate this, we conduct experiments on an unseen under-display-camera dataset [77] and unseen Enhancing Underwater Visual Perception (EUVP) dataset [18] containing various unknown artifacts.",
          "confidence_score": 0.9,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions two specific datasets used for experiments: an 'unseen under-display-camera dataset' and the 'Enhancing Underwater Visual Perception (EUVP) dataset'. Both are relevant to image restoration.",
          "citing_paper_doi": "10.48550/arXiv.2310.10123",
          "cited_paper_doi": "10.48550/arXiv.2309.06380",
          "citing_paper_url": "https://www.semanticscholar.org/paper/8efc22d25fc33d73aa8de1044e5afe775ed87e31",
          "cited_paper_url": "https://www.semanticscholar.org/paper/448e7e873a6e520072301b7d9c1d0d9289778418",
          "citing_paper_year": 2023,
          "cited_paper_year": 2023
        }
      ]
    },
    {
      "cited_paper_id": "70350065",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "U45"
      ],
      "dataset_details": [
        {
          "dataset_name": "U45",
          "dataset_description": "Used to test the effectiveness of image enhancement techniques in diverse underwater settings, including natural lighting conditions. | Used to assess the performance of image restoration methods in underwater environments, emphasizing natural lighting conditions. | Used to evaluate the robustness of image restoration methods in extensive underwater video sequences, capturing natural light variations. | Used to evaluate NU2Net's performance on underwater image enhancement, focusing on minor color cast issues under natural light conditions. | Used for visual comparison of restoration results in underwater image enhancement, focusing on non-reference datasets to evaluate contrast and structural detail improvements. | Used to benchmark image restoration algorithms in challenging underwater scenarios, particularly under natural light. | Used to assess NU2Net's effectiveness in enhancing underwater images, particularly addressing minor color cast problems in natural lighting scenarios. | Used to evaluate underwater image enhancement techniques, focusing on real-world conditions under natural light. | Used to test NU2Net's capabilities in restoring underwater images, specifically evaluating its performance on minor color cast corrections under natural light.",
          "citing_paper_id": "275789940",
          "cited_paper_id": 70350065,
          "context_text": "Visual comparison of restoration results on the non-reference datasets: U45 [52], SQUID-16 [53], Challenge-60 [7], UCCS [54], and EUVP-330 [55]. contrast, and enhanced structural details.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions several datasets used for visual comparison of restoration results in underwater image enhancement. These datasets are specific and have clear identifiers.",
          "citing_paper_doi": "10.48550/arXiv.2501.12981",
          "cited_paper_doi": "10.1109/TCSVT.2019.2963772",
          "citing_paper_url": "https://www.semanticscholar.org/paper/298b051872e5b7cf945ba4bf7bbfb9d83c9f738f",
          "cited_paper_url": "https://www.semanticscholar.org/paper/e35b97c958ce9897c9b23a0ba30ff11e63b2dee8",
          "citing_paper_year": 2025,
          "cited_paper_year": 2019
        }
      ]
    },
    {
      "cited_paper_id": "257952310",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "T90"
      ],
      "dataset_details": [
        {
          "dataset_name": "T90",
          "dataset_description": "Used to evaluate the impact of underwater image restoration on segmentation performance by applying the Segment Anything Model (SAM) to enhanced images from various UIR methods.",
          "citing_paper_id": "275789940",
          "cited_paper_id": 257952310,
          "context_text": "…performance on depth estimation tasks. ii) For image segmentation task, we used enhanced images from the T90 dataset produced by various UIR methods and directly applied the Segment Anything Model (SAM) [69] to evaluate the impact of underwater image restoration on segmentation performance.",
          "confidence_score": 1.0,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions the T90 dataset, which is used for evaluating the impact of underwater image restoration on segmentation performance.",
          "citing_paper_doi": "10.48550/arXiv.2501.12981",
          "cited_paper_doi": "10.1109/ICCV51070.2023.00371",
          "citing_paper_url": "https://www.semanticscholar.org/paper/298b051872e5b7cf945ba4bf7bbfb9d83c9f738f",
          "cited_paper_url": "https://www.semanticscholar.org/paper/7470a1702c8c86e6f28d32cfa315381150102f5b",
          "citing_paper_year": 2025,
          "cited_paper_year": 2023
        }
      ]
    },
    {
      "cited_paper_id": "43933109",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "PieAPP"
      ],
      "dataset_details": [
        {
          "dataset_name": "PieAPP",
          "dataset_description": "Used as a benchmark for perceptual image quality assessment, evaluating the effectiveness of image restoration methods. | Used to assess perceptual image errors through pairwise preference, focusing on human perception in image restoration tasks. | Used to assess perceptual image error through pairwise preference, focusing on human preference between two images, enhancing image restoration evaluation. | Used to assess perceptual image error through pairwise preference, focusing on building a dataset that provides accurate propensity probabilities for image restoration evaluation.",
          "citing_paper_id": "227238849",
          "cited_paper_id": 43933109,
          "context_text": "The PieAPP dataset (Prashnani et al., 2018) records the probability of human preference between two images.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions the PieAPP dataset, which is a specific, verifiable dataset used for assessing image error through pairwise preference.",
          "citing_paper_doi": null,
          "cited_paper_doi": "10.1109/CVPR.2018.00194",
          "citing_paper_url": "https://www.semanticscholar.org/paper/f942a4a56e6549c83844747ad6c4ae58000b2988",
          "cited_paper_url": "https://www.semanticscholar.org/paper/a41175002926ccc7754c0fb3c989ff38846434f1",
          "citing_paper_year": 2020,
          "cited_paper_year": 2018
        }
      ]
    },
    {
      "cited_paper_id": "64193",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "Berkeley segmentation database"
      ],
      "dataset_details": [
        {
          "dataset_name": "Berkeley segmentation database",
          "dataset_description": "Used to evaluate the proposed image restoration algorithm, focusing on segmentation quality and ecological statistics of natural images.",
          "citing_paper_id": "2627705",
          "cited_paper_id": 64193,
          "context_text": "We chose to evaluate our algorithm on some images from the Berkeley segmentation database, [56], presented in Fig.",
          "confidence_score": 1.0,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The citation mentions the Berkeley segmentation database, which is a well-known dataset used for evaluating image segmentation algorithms.",
          "citing_paper_doi": "10.1109/TIP.2007.911828",
          "cited_paper_doi": "10.1109/ICCV.2001.937655",
          "citing_paper_url": "https://www.semanticscholar.org/paper/92281d5002178003bd7060fc66677a3471cdaa4b",
          "cited_paper_url": "https://www.semanticscholar.org/paper/9a1ed876196ec9733acb1daa6d65e35ff0414291",
          "citing_paper_year": 2008,
          "cited_paper_year": 2001
        }
      ]
    },
    {
      "cited_paper_id": "1900911",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "LabelMe"
      ],
      "dataset_details": [
        {
          "dataset_name": "LabelMe",
          "dataset_description": "Used to build the image database for training and evaluation, focusing on image annotation and representation tasks.",
          "citing_paper_id": "2627705",
          "cited_paper_id": 1900911,
          "context_text": "representations). We used the database LabelMe, [ 50 ], to build our image database.",
          "confidence_score": 1.0,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "LabelMe is identified as a database used to build the image database for the research. The context clearly indicates its use as a dataset.",
          "citing_paper_doi": "10.1109/TIP.2007.911828",
          "cited_paper_doi": "10.1007/s11263-007-0090-8",
          "citing_paper_url": "https://www.semanticscholar.org/paper/92281d5002178003bd7060fc66677a3471cdaa4b",
          "cited_paper_url": "https://www.semanticscholar.org/paper/092c275005ae49dc1303214f6d02d134457c7053",
          "citing_paper_year": 2008,
          "cited_paper_year": 2008
        }
      ]
    },
    {
      "cited_paper_id": "44132158",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "ExDark"
      ],
      "dataset_details": [
        {
          "dataset_name": "ExDark",
          "dataset_description": "Used to evaluate low-light detection results after enhancement by various algorithms, focusing on improving visibility and detail in dark images. | Used for visual comparisons of enhancement-based low-light object detection, focusing on the performance of different methods under low-light conditions. | Used for low-light object detection experiments to compare the preprocessing effects of different enhancement algorithms for high-level vision understanding.",
          "citing_paper_id": "257496232",
          "cited_paper_id": 44132158,
          "context_text": "We conduct low-light object detection experiments on the ExDark [33] dataset to compare the preprocessing effects of different enhancement algorithms for high-level vision understanding.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions the ExDark dataset, which is used for low-light object detection experiments to compare preprocessing effects of different enhancement algorithms.",
          "citing_paper_doi": "10.1109/ICCV51070.2023.01149",
          "cited_paper_doi": "10.1016/J.CVIU.2018.10.010",
          "citing_paper_url": "https://www.semanticscholar.org/paper/42754ad2b7b2193b4d8e3d067808cea8913869c4",
          "cited_paper_url": "https://www.semanticscholar.org/paper/12e94e888a9966738c56430abff7ed7fa452221b",
          "citing_paper_year": 2023,
          "cited_paper_year": 2018
        }
      ]
    }
  ],
  "citation_count_distribution": {
    "64193": 23,
    "4563057": 3,
    "4591284": 1,
    "6628106": 6,
    "47007607": 1,
    "49333383": 9,
    "62732555": 4,
    "73439498": 6,
    "119188226": 3,
    "203042751": 7,
    "207761262": 7,
    "207930212": 2,
    "227239228": 16,
    "996788": 14,
    "1900475": 9,
    "4840263": 18,
    "8236317": 2,
    "8671030": 20,
    "14092238": 8,
    "18774783": 3,
    "49867180": 2,
    "198185751": 5,
    "202733344": 4,
    "206769988": 4,
    "210838848": 6,
    "214623217": 10,
    "216562731": 6,
    "231639270": 1,
    "232404237": 1,
    "233296808": 3,
    "236976210": 4,
    "237627482": 1,
    "244346144": 27,
    "245837508": 7,
    "246285588": 2,
    "247939726": 3,
    "248426764": 4,
    "250551851": 24,
    "259243623": 7,
    "265659287": 2,
    "267199899": 3,
    "267320695": 18,
    "270736284": 2,
    "271500033": 1,
    "273644217": 1,
    "4766599": 8,
    "14124313": 2,
    "15443600": 13,
    "16388844": 1,
    "16568282": 3,
    "30151664": 4,
    "52008443": 15,
    "195787503": 15,
    "218971783": 1,
    "231802205": 23,
    "233444273": 2,
    "245005809": 1,
    "247411392": 7,
    "248085101": 5,
    "248085491": 17,
    "248562703": 2,
    "249625528": 2,
    "256594710": 1,
    "257766864": 1,
    "257833938": 1,
    "258170077": 1,
    "258832675": 1,
    "258959359": 3,
    "259164829": 2,
    "260107992": 14,
    "263784964": 5,
    "264289165": 5,
    "264305756": 3,
    "265609570": 2,
    "266933199": 1,
    "267938238": 8,
    "1887989": 3,
    "4539586": 8,
    "5778488": 1,
    "5945696": 2,
    "7492030": 1,
    "8282555": 10,
    "10878983": 1,
    "13756489": 7,
    "14511898": 1,
    "49313245": 2,
    "52073194": 1,
    "52967399": 3,
    "91184545": 4,
    "173188673": 1,
    "186206211": 1,
    "195908774": 2,
    "202660933": 1,
    "209515898": 1,
    "210839011": 1,
    "220265496": 1,
    "221173039": 1,
    "225067129": 1,
    "232359322": 1,
    "235755417": 1,
    "236171006": 6,
    "237497252": 1,
    "238857040": 1,
    "244461440": 6,
    "245218925": 1,
    "246411364": 2,
    "246411402": 5,
    "249282628": 1,
    "249674493": 1,
    "252683961": 1,
    "252735181": 2,
    "254246343": 3,
    "255546477": 1,
    "257206115": 1,
    "257636509": 6,
    "259144110": 6,
    "259274648": 1,
    "259937812": 1,
    "260085391": 21,
    "260843368": 2,
    "260871139": 1,
    "261031011": 1,
    "263311190": 1,
    "264146360": 3,
    "265149704": 1,
    "265295126": 2,
    "266149517": 6,
    "266191538": 1,
    "266573162": 1,
    "266643865": 1,
    "267061016": 1,
    "268553835": 7,
    "268856875": 3,
    "268876071": 2,
    "269214582": 1,
    "269614217": 1,
    "270045122": 1,
    "270045495": 3,
    "270123740": 4,
    "270869793": 3,
    "271039782": 6,
    "272367682": 1,
    "272724496": 1,
    "272987034": 4,
    "273323238": 1,
    "273532643": 1,
    "274281468": 2,
    "275789940": 1,
    "276960871": 1,
    "277244267": 2,
    "277502494": 1,
    "277667714": 1,
    "277755777": 1,
    "278326960": 1,
    "278528939": 1,
    "837707": 9,
    "206764694": 6,
    "235358213": 15,
    "235719890": 4,
    "247134149": 1,
    "266844752": 6,
    "3312944": 2,
    "3619954": 2,
    "3846544": 3,
    "8550762": 3,
    "9985555": 1,
    "39760169": 30,
    "55358798": 3,
    "57246310": 2,
    "85501306": 5,
    "206593880": 1,
    "206594692": 4,
    "212737191": 3,
    "225039882": 8,
    "232352874": 10,
    "244714491": 16,
    "254018182": 2,
    "257219872": 1,
    "257900969": 1,
    "264145824": 7,
    "265551773": 2,
    "6540453": 2,
    "8858625": 2,
    "13911460": 2,
    "20282961": 2,
    "69993921": 2,
    "91183909": 2,
    "196834421": 3,
    "201103729": 1,
    "201624746": 6,
    "208138077": 10,
    "211146177": 4,
    "212647851": 5,
    "219955663": 3,
    "220713808": 1,
    "231591445": 11,
    "233241040": 1,
    "237048383": 3,
    "237266491": 19,
    "243938678": 1,
    "245335086": 2,
    "247778989": 1,
    "248069347": 7,
    "248097655": 3,
    "250615985": 2,
    "251252882": 3,
    "253581213": 2,
    "254535902": 1,
    "256416326": 3,
    "257804691": 3,
    "257921922": 2,
    "258615282": 7,
    "258762187": 1,
    "259137590": 2,
    "261244427": 2,
    "261697392": 2,
    "267199774": 5,
    "4054776": 3,
    "5872410": 1,
    "11880723": 1,
    "15780954": 1,
    "16747630": 2,
    "207977878": 2,
    "208138230": 1,
    "209376714": 4,
    "218487055": 3,
    "219978541": 14,
    "222104551": 3,
    "231419143": 2,
    "231728368": 1,
    "232146599": 1,
    "233296013": 4,
    "233296254": 1,
    "235693182": 1,
    "237213661": 2,
    "244908890": 4,
    "246904639": 1,
    "250451699": 2,
    "1372266": 2,
    "3406592": 11,
    "3652822": 1,
    "11922819": 5,
    "226282220": 2,
    "236772838": 2,
    "243985980": 2,
    "248986859": 2,
    "250581068": 8,
    "257102476": 1,
    "257687687": 1,
    "258048853": 3,
    "258557157": 1,
    "260680793": 4,
    "260887508": 3,
    "261081748": 1,
    "684325": 1,
    "6593498": 3,
    "17763780": 1,
    "21712570": 5,
    "49864080": 8,
    "52059988": 3,
    "52952008": 1,
    "53751136": 1,
    "59523708": 1,
    "119308964": 2,
    "122775231": 1,
    "221771219": 1,
    "222141081": 1,
    "234338988": 1,
    "247218518": 1,
    "248006282": 1,
    "248498308": 1,
    "248572065": 4,
    "250296280": 1,
    "259224666": 8,
    "260870102": 1,
    "2927709": 2,
    "16892725": 5,
    "195489762": 3,
    "214802286": 3,
    "219530930": 5,
    "221703405": 1,
    "235702920": 4,
    "235703693": 1,
    "238354065": 1,
    "260397544": 1,
    "264307339": 1,
    "2972940": 2,
    "6200260": 4,
    "18235798": 1,
    "29436769": 1,
    "49657846": 4,
    "235458009": 3,
    "235702840": 2,
    "252693111": 3,
    "257255385": 6,
    "257532815": 2,
    "257952310": 4,
    "259924765": 1,
    "262546756": 4,
    "268064460": 14,
    "272724853": 5,
    "10328909": 1,
    "13133466": 3,
    "17115407": 11,
    "56657799": 1,
    "128362447": 2,
    "195657934": 8,
    "221852418": 1,
    "247594264": 1,
    "10514149": 6,
    "13692444": 1,
    "27333569": 1,
    "131773964": 6,
    "226641165": 2,
    "249605449": 1,
    "254973973": 1,
    "257069681": 1,
    "257999356": 2,
    "259298517": 3,
    "260271048": 4,
    "263605463": 4,
    "264555119": 2,
    "265609786": 3,
    "266690738": 4,
    "268667967": 1,
    "271064424": 1,
    "3719281": 6,
    "4552226": 4,
    "56475900": 1,
    "199528450": 3,
    "199543931": 2,
    "210839013": 1,
    "218613859": 1,
    "219488928": 1,
    "253321998": 1,
    "258509678": 1,
    "28487645": 1,
    "221802736": 1,
    "226674012": 1,
    "234482841": 4,
    "256033931": 1,
    "260003105": 1,
    "264363267": 1,
    "15197266": 2,
    "19494799": 1,
    "20656092": 1,
    "26910155": 2,
    "202542259": 2,
    "234357913": 1,
    "251800180": 1,
    "254926772": 1,
    "256390509": 2,
    "256616163": 1,
    "257757078": 1,
    "258179174": 1,
    "265506768": 1,
    "267026544": 2,
    "268513542": 6,
    "268680796": 1,
    "9007541": 2,
    "53011763": 1,
    "56657912": 1,
    "59316941": 9,
    "218470249": 4,
    "226308542": 6,
    "229923112": 1,
    "231641545": 1,
    "235703231": 1,
    "247922539": 3,
    "251979350": 1,
    "253518543": 1,
    "253761139": 4,
    "257496232": 4,
    "260927679": 1,
    "265609653": 1,
    "195510077": 1,
    "206598041": 4,
    "218889832": 3,
    "221883930": 1,
    "227130078": 1,
    "232222608": 2,
    "232306906": 3,
    "247011083": 1,
    "247319040": 1,
    "248512492": 1,
    "250574953": 1,
    "257767233": 3,
    "258509512": 1,
    "258865387": 1,
    "261081712": 1,
    "261556765": 3,
    "267177503": 1,
    "268678353": 1,
    "271274210": 1,
    "211205159": 1,
    "221068158": 1,
    "237386023": 3,
    "247410579": 1,
    "211227": 1,
    "236493269": 2,
    "237198939": 1,
    "247363011": 1,
    "249953544": 1,
    "251197000": 5,
    "272552320": 1,
    "272724277": 1,
    "8235201": 3,
    "201134031": 1,
    "247748724": 2,
    "261046875": 1,
    "265999804": 1,
    "268364340": 2,
    "273320871": 1,
    "273502246": 4,
    "273659089": 1,
    "275133634": 1,
    "275921949": 3,
    "1974825": 1,
    "43925777": 1,
    "202781591": 2,
    "209323842": 2,
    "211572645": 1,
    "214775210": 2,
    "244398686": 2,
    "246823327": 2,
    "247411040": 2,
    "254069870": 2,
    "968094": 1,
    "1779661": 1,
    "4072789": 1,
    "15799108": 1,
    "40504781": 1,
    "53047249": 1,
    "85498726": 3,
    "115196811": 1,
    "131776850": 1,
    "221119346": 1,
    "230770098": 2,
    "232170566": 1,
    "235691711": 1,
    "244488578": 1,
    "245339944": 1,
    "247597149": 1,
    "248634766": 1,
    "250426326": 2,
    "254179587": 1,
    "256900743": 1,
    "260436809": 1,
    "260748050": 1,
    "263895657": 1,
    "264833084": 1,
    "266244375": 1,
    "267786640": 1,
    "268838239": 2,
    "715896": 1,
    "7037846": 1,
    "7044126": 2,
    "9335112": 1,
    "14925285": 2,
    "43854597": 1,
    "195350771": 1,
    "204187430": 1,
    "214802288": 1,
    "258509666": 1,
    "258741296": 1,
    "265658932": 2,
    "267334929": 1,
    "269605449": 3,
    "270440448": 2,
    "273643057": 1,
    "276318260": 1,
    "604334": 1,
    "3488815": 1,
    "146121358": 1,
    "195791557": 1,
    "195886255": 1,
    "208909851": 1,
    "241040811": 1,
    "244270535": 1,
    "246015506": 1,
    "248834527": 1,
    "249890116": 1,
    "14337532": 2,
    "49672261": 2,
    "220686623": 2,
    "247596985": 1,
    "247618727": 3,
    "257622589": 1,
    "259459919": 1,
    "259634520": 1,
    "260006244": 2,
    "8142135": 1,
    "8525940": 1,
    "160025533": 1,
    "219781060": 1,
    "229297973": 1,
    "232135095": 1,
    "251040466": 5,
    "256868405": 1,
    "257219404": 2,
    "259341735": 2,
    "260704481": 4,
    "268230646": 1,
    "272826725": 1,
    "273346791": 1,
    "274116624": 1,
    "276079427": 1,
    "277451671": 1,
    "2428314": 1,
    "84843405": 3,
    "140309863": 1,
    "150373946": 1,
    "206596913": 1,
    "222103869": 1,
    "235640808": 1,
    "260926685": 1,
    "268030812": 3,
    "268063754": 1,
    "1242324": 1,
    "53755281": 1,
    "189928152": 1,
    "213183248": 1,
    "257804739": 2,
    "259251801": 1,
    "262216939": 1,
    "266741509": 1,
    "268856944": 1,
    "269757927": 1,
    "270379537": 1,
    "270440599": 1,
    "271489742": 1,
    "271903072": 1,
    "272722348": 1,
    "273351043": 1,
    "274437167": 1,
    "6917137": 2,
    "10519397": 1,
    "26229170": 2,
    "118347220": 1,
    "118734031": 1,
    "122853046": 1,
    "221370709": 1,
    "235390890": 1,
    "248085000": 1,
    "249848207": 1,
    "249848272": 1,
    "253264930": 1,
    "254564501": 1,
    "259674925": 1,
    "271891992": 2,
    "53039974": 1,
    "109612799": 1,
    "129948333": 1,
    "202143808": 2,
    "209940843": 1,
    "219423517": 1,
    "219919444": 1,
    "220835853": 1,
    "220871180": 1,
    "221939192": 1,
    "225026402": 1,
    "228987244": 1,
    "230511190": 1,
    "231715795": 1,
    "234011980": 1,
    "234107267": 1,
    "235689508": 1,
    "238262738": 1,
    "238803304": 1,
    "246233298": 1,
    "247605912": 1,
    "247940251": 1,
    "249314049": 1,
    "250450940": 1,
    "251035417": 1,
    "252089325": 1,
    "252148753": 1,
    "252477950": 1,
    "253708090": 1,
    "254439006": 1,
    "255626371": 1,
    "255825808": 1,
    "255915952": 1,
    "255995928": 1,
    "257122519": 1,
    "258383425": 1,
    "1629541": 1,
    "11091110": 1,
    "38030033": 1,
    "248299814": 1,
    "12874183": 1,
    "249432930": 1,
    "254591838": 1,
    "273230367": 1,
    "247221335": 1,
    "249062606": 1,
    "212725053": 1,
    "326772": 2,
    "5560643": 1,
    "218538083": 2,
    "222140788": 3,
    "235694314": 1,
    "238215243": 3,
    "242630136": 1,
    "250085592": 3,
    "251020317": 1,
    "253018768": 1,
    "254018130": 1,
    "254535802": 1,
    "256358842": 1,
    "256827727": 2,
    "257038979": 1,
    "258179052": 1,
    "259837088": 1,
    "260125321": 2,
    "261244079": 2,
    "261245351": 2,
    "261276317": 1,
    "263671990": 1,
    "265466804": 1,
    "269430366": 1,
    "272593061": 1,
    "4828378": 1,
    "206590483": 1,
    "221113156": 1,
    "253384197": 1,
    "259287283": 1,
    "261242635": 1,
    "4715123": 1,
    "10225720": 1,
    "29860648": 1,
    "211020567": 1,
    "247748613": 1,
    "249097890": 1,
    "250126736": 1,
    "252046269": 1,
    "257321888": 1,
    "257423144": 1,
    "257521024": 1,
    "258258066": 1,
    "269430992": 1,
    "269865211": 1,
    "272444309": 1,
    "278770160": 1,
    "215785896": 5,
    "235417196": 1,
    "263151865": 1,
    "271543778": 1,
    "274305801": 2,
    "278740510": 1,
    "279464475": 1,
    "59599816": 3,
    "207998010": 1,
    "229924195": 4,
    "230435805": 1,
    "247218062": 1,
    "247362538": 2,
    "252780157": 1,
    "257365136": 1,
    "2356353": 2,
    "3638670": 1,
    "27755051": 1,
    "29151865": 1,
    "207991474": 1,
    "209389923": 1,
    "227311248": 1,
    "229363322": 2,
    "232428140": 1,
    "233714958": 1,
    "234335673": 1,
    "235606034": 1,
    "235703017": 1,
    "236493453": 1,
    "250569514": 1,
    "9971732": 1,
    "12462234": 4,
    "21275979": 1,
    "201847718": 1,
    "245371686": 1,
    "247158080": 1,
    "249538647": 3,
    "252110918": 1,
    "259766392": 1,
    "3468628": 1,
    "209862064": 1,
    "235458657": 1,
    "238243504": 1,
    "256808230": 1,
    "257482703": 1,
    "258211854": 1,
    "260810536": 2,
    "268031926": 1,
    "5427519": 1,
    "214713957": 1,
    "226291870": 2,
    "259340208": 1,
    "232168628": 1,
    "235702784": 2,
    "247939260": 1,
    "249709314": 3,
    "262055269": 1,
    "264364007": 1,
    "271285150": 1,
    "221097144": 1,
    "232352745": 1,
    "246634936": 2,
    "246823759": 1,
    "1663191": 1,
    "1916689": 1,
    "2627705": 1,
    "14460720": 2,
    "15941714": 1,
    "26598341": 1,
    "128359047": 1,
    "235656945": 1,
    "251066779": 1,
    "251468222": 1,
    "235829480": 1,
    "259236069": 1,
    "190906": 2,
    "189762039": 1,
    "212634162": 1,
    "231662712": 1,
    "244954250": 2,
    "246634906": 1,
    "248986576": 2,
    "249926514": 1,
    "250264643": 2,
    "254125609": 3,
    "254408758": 2,
    "17006174": 1,
    "70350065": 1,
    "197545095": 1,
    "240584211": 1,
    "247494896": 1,
    "257557425": 1,
    "258999258": 1,
    "261582781": 1,
    "267435558": 1,
    "269005685": 1,
    "272326006": 1,
    "4493958": 3,
    "219619080": 1,
    "235742820": 1,
    "244799261": 1,
    "252918469": 2,
    "245335280": 2,
    "258762776": 1,
    "265466382": 2,
    "266362278": 1,
    "268819753": 1,
    "1724361": 1,
    "14877173": 1,
    "85544221": 1,
    "4746623": 1,
    "227238849": 1,
    "244714180": 1,
    "260735888": 1,
    "260869421": 1,
    "264146906": 1,
    "265013798": 1,
    "265043538": 1,
    "267750841": 1,
    "7593476": 1,
    "231573431": 1,
    "232428282": 1,
    "244729320": 1,
    "248240148": 1,
    "265453871": 2,
    "265466453": 1,
    "268008125": 1,
    "272956729": 1,
    "1739295": 1,
    "30069667": 1,
    "41804650": 1,
    "52281312": 1,
    "120241709": 1,
    "122832280": 1,
    "212640422": 1,
    "250423266": 1,
    "250493155": 1,
    "251131864": 1,
    "251142006": 1,
    "252696059": 1,
    "253448255": 1,
    "253657310": 1,
    "256805793": 1,
    "257771327": 1,
    "257804820": 1,
    "258822824": 1,
    "258999508": 1,
    "259111470": 1,
    "259111819": 1,
    "260845940": 1,
    "261220627": 1,
    "261461912": 1,
    "265645010": 1,
    "266293945": 1,
    "266560129": 1,
    "267041499": 1,
    "268692926": 1,
    "269053553": 1,
    "269148750": 1,
    "269483490": 1,
    "270208891": 1,
    "270219541": 1,
    "270551039": 1,
    "271218448": 1,
    "275970554": 1,
    "102350705": 1,
    "211732223": 1,
    "235694312": 1,
    "238634689": 1,
    "250408169": 1,
    "252090171": 1,
    "252595608": 1,
    "256615401": 1,
    "260862867": 1,
    "265006592": 1,
    "265039963": 1,
    "265537221": 1,
    "268678266": 1,
    "2213896": 1,
    "4703661": 1,
    "4800342": 1,
    "4962395": 1,
    "12817931": 1,
    "14873001": 1,
    "15901990": 1,
    "30651858": 2,
    "45998148": 1,
    "18406556": 1,
    "247011948": 1,
    "257757038": 1,
    "267499648": 2,
    "253761491": 1,
    "2057420": 1,
    "2141622": 2,
    "3481010": 1,
    "4531078": 1,
    "16113001": 1,
    "40060575": 1,
    "206724273": 1,
    "215763824": 1,
    "233148602": 1,
    "235253954": 1,
    "246411466": 1,
    "252596252": 1,
    "259075580": 1,
    "260957038": 1,
    "4077800": 1,
    "4095486": 1,
    "11905372": 1,
    "20658546": 1,
    "62771950": 1,
    "67751636": 1,
    "122575721": 1,
    "213209603": 1,
    "213782253": 2,
    "219050837": 1,
    "226850925": 1,
    "233471949": 1,
    "246650660": 1,
    "247179672": 1,
    "247689711": 1,
    "251281305": 1,
    "251704647": 1,
    "254737528": 1,
    "255682697": 1,
    "257461627": 1,
    "257466355": 1,
    "258179535": 1,
    "258587991": 1,
    "259051233": 1,
    "260920447": 1,
    "266180641": 1,
    "2658359": 1,
    "235719471": 1,
    "237091588": 1,
    "244130382": 1,
    "268531408": 1,
    "271244778": 1,
    "202577442": 1,
    "204955838": 1,
    "221377171": 1,
    "232035663": 1,
    "235212350": 1,
    "235313426": 1,
    "235353006": 1,
    "235702618": 1,
    "247628171": 1,
    "249926846": 1,
    "255372955": 1,
    "256105441": 1,
    "572361": 1,
    "232352764": 2,
    "237500626": 1,
    "245986500": 1,
    "247762315": 1,
    "248665570": 1,
    "249394802": 1,
    "250520618": 1,
    "253510104": 1,
    "254685665": 1,
    "259108489": 1,
    "259287066": 1,
    "264591546": 1,
    "268030722": 1,
    "271874811": 2,
    "2375110": 2,
    "3523851": 1,
    "10913825": 1,
    "11987926": 1,
    "13119501": 1,
    "44131640": 1,
    "207991682": 1,
    "246088517": 1,
    "248370717": 1,
    "254017867": 1,
    "258303516": 1,
    "260704626": 1,
    "268529518": 1,
    "268603780": 1,
    "270155352": 1,
    "270845666": 1,
    "272367542": 1,
    "272755634": 1,
    "201646309": 1,
    "245095057": 1,
    "249474184": 1,
    "255089439": 1,
    "256655517": 1,
    "260905883": 1,
    "272680196": 1,
    "484327": 1,
    "5250573": 1,
    "8887614": 1,
    "18874645": 1,
    "238583580": 1,
    "248834106": 1,
    "249538657": 1,
    "252918870": 1,
    "256900990": 1,
    "259144860": 1,
    "260333877": 1,
    "6576859": 1,
    "6736352": 1,
    "198897678": 1,
    "201070037": 1,
    "220835852": 1,
    "221181117": 1,
    "221802293": 1,
    "238055733": 1,
    "246904703": 1,
    "251772829": 1,
    "267023529": 2,
    "15774646": 1,
    "116174761": 1,
    "201811641": 1,
    "212785907": 1,
    "225792549": 1,
    "226301401": 1,
    "237778076": 1,
    "247338236": 1,
    "219752103": 1,
    "251937856": 1,
    "252257747": 1,
    "254044390": 1,
    "258494659": 1,
    "260081620": 1,
    "269151233": 1,
    "271029800": 1,
    "271177978": 1,
    "271633341": 1,
    "272018886": 1,
    "1665683": 1,
    "7051992": 1,
    "18785401": 1,
    "233296606": 1,
    "235390405": 1,
    "257833781": 1,
    "258967184": 1,
    "259075577": 1,
    "260351380": 1,
    "271432364": 1,
    "273662196": 1,
    "274435587": 1,
    "279000557": 1,
    "1543021": 1,
    "4710407": 1,
    "9569924": 1,
    "58014237": 1,
    "229221619": 1,
    "232134936": 1,
    "249626057": 1,
    "257365045": 1,
    "260125438": 1,
    "269761529": 1,
    "271909255": 1,
    "273346034": 1,
    "215827763": 1,
    "219633388": 1,
    "244114613": 1,
    "252383409": 1,
    "252421835": 1,
    "252917726": 1,
    "258291930": 1,
    "258352455": 1,
    "258547300": 1,
    "258615266": 1,
    "259262082": 1,
    "259360671": 1,
    "265498818": 1,
    "267752981": 1,
    "268297180": 1,
    "270199505": 1,
    "272704132": 1,
    "274130569": 1,
    "274130620": 1,
    "3072879": 2,
    "3831826": 1,
    "16074195": 1,
    "125617073": 2,
    "206677332": 2,
    "214803044": 1,
    "220734147": 1,
    "231879586": 1,
    "250601992": 1,
    "257496264": 1,
    "265042422": 1,
    "269148707": 1,
    "272724641": 1,
    "254848": 1,
    "22540825": 1,
    "52211898": 1,
    "57759394": 1,
    "207217221": 1,
    "219629285": 1,
    "227255179": 1,
    "231986084": 1,
    "232417787": 1,
    "233997468": 1,
    "235624247": 1,
    "237403103": 1,
    "132226104": 1
  },
  "merged_dataset_groups": [
    {
      "display_name": "GoPro",
      "normalized_name": "gopro",
      "name_variants": [
        "GoPRO",
        "GoPro",
        "GoPro dataset",
        "GoPro datasets",
        "GoPro test set"
      ],
      "mention_count": 46,
      "cited_papers_count": 22,
      "topic_summary": "The GoPro dataset is primarily used for deblurring tasks, providing a benchmark for evaluating and improving image restoration algorithms in motion blur scenarios. It is employed to train and test deep multi-scale convolutional neural networks, focusing on real-world and synthetic dynamic scenes. The dataset supports the assessment of deblurring performance using metrics like PSNR and SSIM, and it is also used for low-light enhancement, deraining, denoising, and dehazing, enhancing the robustness and effectiveness of image restoration methods in various challenging conditions."
    },
    {
      "display_name": "DIV2K",
      "normalized_name": "div2k",
      "name_variants": [
        "DIV2K",
        "DIV2K train set"
      ],
      "mention_count": 43,
      "cited_papers_count": 18,
      "topic_summary": "The DIV2K dataset is primarily used for training and evaluating single image super-resolution (SISR) models, focusing on enhancing texture fidelity, PSNR values, and overall image quality. It contains 800 high-quality images, typically divided into 750 for training and 50 for testing. The dataset is also utilized for Gaussian color image denoising, providing a diverse set of natural images to improve model robustness and performance. Additionally, it is used to evaluate face super-resolution, manga image restoration, and the generalizability of SISR models across various image types and corruption scenarios."
    },
    {
      "display_name": "Set5",
      "normalized_name": "set5",
      "name_variants": [
        "Set5"
      ],
      "mention_count": 38,
      "cited_papers_count": 21,
      "topic_summary": "Set5 is primarily used to evaluate single-image super-resolution methods, focusing on high-quality image restoration and detail enhancement. It is employed to assess PSNR and SSIM metrics, measure convergence trends, and compare the performance of various models (e.g., DTFN, FSRCNN, EDSR, RCAN) across different scaling factors, particularly at 4x. The dataset is also used to test the effectiveness of image restoration techniques in handling various types of image degradation, emphasizing detail preservation and texture recovery in both urban scenes and manga images."
    },
    {
      "display_name": "BSD400",
      "normalized_name": "bsd400",
      "name_variants": [
        "BSD400"
      ],
      "mention_count": 37,
      "cited_papers_count": 19,
      "topic_summary": "The BSD400 dataset is extensively used in image restoration research, focusing on evaluating and benchmarking various restoration methods. It is employed to assess the performance of algorithms in removing degradations such as rain, haze, noise, and motion blur, particularly in urban and outdoor scenes. The dataset includes high-resolution images and synthetic degradations, enabling researchers to test the robustness and effectiveness of restoration techniques across diverse conditions. It is also used for training and evaluating denoising, deblurring, dehazing, and low-light enhancement models, emphasizing the preservation of fine details, color fidelity, and natural appearance."
    },
    {
      "display_name": "ImageNet",
      "normalized_name": "imagenet",
      "name_variants": [
        "ImageNet"
      ],
      "mention_count": 27,
      "cited_papers_count": 18,
      "topic_summary": "ImageNet is primarily used for pretraining and validating image restoration models, focusing on improving restoration quality and computational efficiency. It is utilized to train models like DiffBIR and DDRM, and to evaluate their performance on large validation sets. The dataset enhances the representation space with contextual information about common objects, aiding in semantic understanding and high-fidelity output generation. Pretraining on ImageNet often precedes fine-tuning on specialized datasets for specific restoration tasks."
    },
    {
      "display_name": "SIDD",
      "normalized_name": "sidd",
      "name_variants": [
        "SIDD",
        "SIDD validation set"
      ],
      "mention_count": 23,
      "cited_papers_count": 16,
      "topic_summary": "The SIDD dataset is primarily used for image denoising, particularly for real-world noisy images from smartphone cameras. It is employed in training and evaluating denoising models, providing a benchmark for performance metrics like PSNR and SSIM. Additionally, the dataset supports research in low-light image restoration, deblurring, deraining, and snow removal, often combining with other datasets to comprehensively assess restoration techniques. The dataset's high-quality images and real-world noise characteristics enable robust evaluation and comparison of various image restoration algorithms."
    },
    {
      "display_name": "BSD68",
      "normalized_name": "bsd68",
      "name_variants": [
        "BSD68"
      ],
      "mention_count": 19,
      "cited_papers_count": 9,
      "topic_summary": "The BSD68 dataset is extensively used for evaluating and benchmarking image restoration methods, particularly in denoising and deblurring tasks. It features a diverse set of natural and urban scene images with ground truth annotations, enabling researchers to assess the performance of restoration algorithms across various noise levels and blur types. The dataset supports the evaluation of super-resolution, deraining, dehazing, and low-light enhancement techniques, focusing on visual fidelity, detail preservation, and structural integrity. It serves as a robust benchmark for comparing different restoration methods and models, emphasizing high-resolution and high-quality image processing."
    },
    {
      "display_name": "Rain100L",
      "normalized_name": "rain100l",
      "name_variants": [
        "Rain100 L",
        "Rain100L"
      ],
      "mention_count": 17,
      "cited_papers_count": 14,
      "topic_summary": "The Rain100L dataset is primarily used for image deraining, focusing on removing rain streaks from images to enhance clarity. It consists of 200 clean-rainy image pairs for training and 100 pairs for testing, enabling researchers to evaluate deraining algorithms' performance, particularly in texture preservation and rain removal effectiveness. Additionally, the dataset is utilized for low-light enhancement, deblurring, dehazing, and desnowing tasks, often combined with other datasets to address diverse image restoration challenges. It supports the development and evaluation of deep learning models, providing a benchmark for visual and quantitative assessments in various image degradation scenarios."
    },
    {
      "display_name": "RESIDE",
      "normalized_name": "reside",
      "name_variants": [
        "RE-SIDE",
        "RESIDE",
        "RESIDE-β"
      ],
      "mention_count": 17,
      "cited_papers_count": 11,
      "topic_summary": "The RESIDE dataset is primarily used for training and evaluating image restoration models, particularly in dehazing, deraining, low-light enhancement, and motion deblurring. It provides paired images (e.g., hazy/clear, rainy/clean, low-light/normal-light, blurred/sharp) to support deep learning techniques. The dataset is extensively used as a benchmark for assessing the performance of various restoration algorithms, including those focused on outdoor scenes, nighttime conditions, and remote sensing imagery. It enables researchers to improve image clarity, detail preservation, and overall quality under different degradation types."
    },
    {
      "display_name": "CBSD68",
      "normalized_name": "cbsd68",
      "name_variants": [
        "CBSD68"
      ],
      "mention_count": 15,
      "cited_papers_count": 10,
      "topic_summary": "The CBSD68 dataset is primarily used for evaluating and benchmarking image restoration methods, particularly denoising and super-resolution algorithms. It features a set of 68 color images with real-world noise characteristics and is used to assess performance at various noise levels (σ=15, σ=25, σ=50). The dataset supports visual comparisons, PSNR evaluations, and the testing of generalization and robustness in restoration tasks. It is also utilized for training models in image restoration, including denoising and deblurring, and for evaluating the effectiveness of different restoration techniques on diverse image sets."
    },
    {
      "display_name": "LOL dataset",
      "normalized_name": "lol",
      "name_variants": [
        "LOL",
        "LOL dataset",
        "LOL datasets v1",
        "LoL"
      ],
      "mention_count": 14,
      "cited_papers_count": 9,
      "topic_summary": "The LOL dataset is primarily used for evaluating and benchmarking low-light image enhancement techniques, focusing on improving visibility, color fidelity, and detail preservation in dark images. It consists of 485 training pairs and 15 testing pairs, making it suitable for both training and testing deep learning models. The dataset is also utilized for multi-exposure fusion, non-uniform illumination correction, and noise reduction, and is often used to compare the performance of different enhancement methods using metrics like PSNR, SSIM, and NIQE. Additionally, it supports research in video enhancement and HDR recovery, demonstrating its versatility in addressing various low-light imaging challenges."
    },
    {
      "display_name": "LIVE1",
      "normalized_name": "live1",
      "name_variants": [
        "LIVE1"
      ],
      "mention_count": 14,
      "cited_papers_count": 9,
      "topic_summary": "The LIVE1 dataset is primarily used to evaluate the performance of image restoration and quality assessment algorithms. It focuses on high-quality denoising, deblocking, and super-resolution, often using PSNR and SSIM metrics to compare methods across different distortion levels and compression factors. The dataset supports research in reducing compression artifacts, preserving color consistency, and enhancing perceptual quality, making it valuable for both quantitative and qualitative assessments of image restoration techniques."
    },
    {
      "display_name": "BSD100",
      "normalized_name": "bsd100",
      "name_variants": [
        "BSD 100",
        "BSD100"
      ],
      "mention_count": 12,
      "cited_papers_count": 11,
      "topic_summary": "The BSD100 dataset is extensively used for evaluating and training image restoration models, including deblurring, rain removal, super-resolution, and denoising. It features a diverse set of images, such as real and synthetic rain, blurred, and low-light images, as well as high-quality urban and manga scenes. Researchers use these images to test and improve the performance of algorithms in handling motion blur, rain streaks, low-light conditions, and enhancing resolution and texture. The dataset's variety and quality enable comprehensive evaluation and robust model training across multiple restoration tasks."
    },
    {
      "display_name": "Rain100H",
      "normalized_name": "rain100h",
      "name_variants": [
        "Rain100H"
      ],
      "mention_count": 12,
      "cited_papers_count": 11,
      "topic_summary": "The Rain100H dataset is primarily used to evaluate and train rain removal algorithms, focusing on both low-resolution and high-resolution images with synthetic rain. It is extensively utilized for deraining tasks, assessing the performance of models using metrics like PSNR and SSIM. The dataset supports comprehensive testing, intermediate evaluation, and training of image restoration models, particularly in removing rain streaks and preserving image details. It also aids in evaluating the generalization and robustness of these models across diverse and challenging conditions."
    },
    {
      "display_name": "Rain14000",
      "normalized_name": "rain14000",
      "name_variants": [
        "Rain14000"
      ],
      "mention_count": 12,
      "cited_papers_count": 8,
      "topic_summary": "The Rain14000 dataset is primarily used for evaluating and training deraining models, providing a large and diverse set of rainy images and their clean counterparts. It is utilized to assess the performance of deraining algorithms across various rain densities, resolutions, and image qualities, focusing on metrics like PSNR and SSIM. The dataset supports research on rain removal, dehazing, denoising, and deblurring, enabling comprehensive validation of image restoration techniques. It includes synthetic and real-world images, enhancing the robustness and generalization of models."
    },
    {
      "display_name": "Urban100",
      "normalized_name": "urban100",
      "name_variants": [
        "Urban100"
      ],
      "mention_count": 11,
      "cited_papers_count": 9,
      "topic_summary": "The Urban100 dataset is extensively used for evaluating image restoration techniques, particularly in denoising and super-resolution tasks. It is employed to assess the performance of various models under challenging conditions, such as high noise levels (σ=50) and 4x scaling factors. The dataset includes urban scenes with high-resolution details, enabling researchers to test and compare methods for noise reduction, super-resolution, and handling multiple degradations like rain, haze, and blur. It supports both synthetic and real-world image experiments, facilitating comprehensive evaluation of restoration quality and effectiveness."
    },
    {
      "display_name": "REDS",
      "normalized_name": "reds",
      "name_variants": [
        "REDS"
      ],
      "mention_count": 11,
      "cited_papers_count": 11,
      "topic_summary": "The REDS dataset is primarily used for motion deblurring in image restoration, focusing on improving the representation capability of image restoration networks for complex degradations. It is utilized to train and evaluate models, particularly in the NTIRE 2021 Image Deblurring Challenge, addressing JPEG artifacts and real-world motion blur. The dataset provides high-quality video sequences and a large set of sharp and blurred image pairs, enabling robust testing and visual comparisons of deblurring algorithms across various conditions."
    },
    {
      "display_name": "DPDD",
      "normalized_name": "dpdd",
      "name_variants": [
        "DPDD",
        "DPDD dataset"
      ],
      "mention_count": 11,
      "cited_papers_count": 6,
      "topic_summary": "The DPDD dataset is primarily used for evaluating and training defocus deblurring methods, focusing on dual-pixel data to enhance image restoration quality. It provides a diverse set of images, including real-world and synthetic scenes, captured with various cameras, to assess algorithm performance using metrics like MAE, LPIPS, and PSNR. The dataset supports both quantitative and qualitative evaluations, enabling researchers to compare different techniques and models, particularly in single-image defocus deblurring. It includes 350 scenes for training, 74 for validation, and 76 for testing, facilitating robust algorithm development and validation."
    },
    {
      "display_name": "CSD",
      "normalized_name": "csd",
      "name_variants": [
        "CSD",
        "CSD dataset"
      ],
      "mention_count": 11,
      "cited_papers_count": 8,
      "topic_summary": "The CSD dataset is primarily used for training and evaluating image desnowing algorithms, focusing on performance metrics such as PSNR and image quality. It provides a large-scale synthetic dataset for benchmarking, emphasizing real-world scenarios and robustness. The dataset is also used to compare the proposed methods against state-of-the-art techniques, including IRNeXt, MSP-Former, and FocalNet, and to assess context-aware deep network performance. Additionally, it is relevant for evaluating other image restoration tasks like dehazing and defocus deblurring, where it measures PSNR improvements over existing methods."
    },
    {
      "display_name": "SOTS",
      "normalized_name": "sots",
      "name_variants": [
        "SOTS"
      ],
      "mention_count": 10,
      "cited_papers_count": 4,
      "topic_summary": "The SOTS dataset is primarily used for evaluating and benchmarking single-image dehazing algorithms, featuring both synthetic and real-world hazy images. It consists of 72,135 training and 500 testing images, enabling researchers to assess the performance, robustness, and efficiency of dehazing methods. The dataset supports comparisons of PSNR scores, computational complexity, and visual quality, facilitating advancements in image restoration techniques."
    },
    {
      "display_name": "Snow100K",
      "normalized_name": "snow100k",
      "name_variants": [
        "Snow100K",
        "Snow100k"
      ],
      "mention_count": 9,
      "cited_papers_count": 6,
      "topic_summary": "The Snow100K dataset is primarily used for training and evaluating desnowing algorithms, focusing on both synthetic and real-world snow removal scenarios. It comprises 50,000 training images, 50,000 testing images, and 1,329 real-world snow images. The dataset is employed to assess the effectiveness of models in removing snow from complex scenes and images with varying snow intensities, emphasizing context-aware deep network performance and robust desnowing capabilities. It is also used for benchmarking desnowing algorithms and comparing model performance using metrics like PSNR."
    },
    {
      "display_name": "WeatherStream",
      "normalized_name": "weatherstream",
      "name_variants": [
        "WeatherStream"
      ],
      "mention_count": 8,
      "cited_papers_count": 8,
      "topic_summary": "The WeatherStream dataset is used to train deweathering models for image restoration, enhancing their robustness and generalization across various real-world degradations such as rain and fog. These models are then applied to downstream vision tasks including depth completion, stereo, optical flow, object detection, segmentation, and monocular depth prediction. The dataset supports the testing and comparison of models like MWFormer-real and TransWeather-real, focusing on realistic weather degradation scenarios in single images."
    },
    {
      "display_name": "Set14",
      "normalized_name": "set14",
      "name_variants": [
        "Set14"
      ],
      "mention_count": 7,
      "cited_papers_count": 3,
      "topic_summary": "Set14 is primarily used for testing and evaluating image restoration models, particularly in super-resolution tasks. It provides a diverse set of high-quality images, including urban scenes and manga, which are used to assess performance metrics such as PSNR and LPIPS. The dataset is also utilized for training models, focusing on upscaling, denoising, and detail preservation. Its well-curated and varied content enables researchers to evaluate the robustness and generalization of restoration techniques in real-world scenarios."
    },
    {
      "display_name": "HIDE",
      "normalized_name": "hide",
      "name_variants": [
        "HIDE"
      ],
      "mention_count": 7,
      "cited_papers_count": 5,
      "topic_summary": "The HIDE dataset is primarily used for evaluating and validating deblurring algorithms, particularly in the context of motion deblurring. It includes both synthetic and real-world images, providing a benchmark for comparing the performance of various deblurring methods. The dataset is used to assess zero-shot generalization, visual quality improvements, and the effectiveness of specific techniques like PromptRestorer and BANet. It contains diverse blur types and conditions, making it suitable for testing under challenging scenarios such as high-speed scenes and varying lighting conditions."
    },
    {
      "display_name": "UIEB",
      "normalized_name": "uieb",
      "name_variants": [
        "UIEB"
      ],
      "mention_count": 7,
      "cited_papers_count": 6,
      "topic_summary": "The UIEB dataset is primarily used for evaluating and benchmarking underwater image enhancement algorithms, focusing on improving visual quality, clarity, and color correction in underwater environments. It contains 950 underwater images and 850 synthetic reference images enhanced using various methods like Retinex, CLAHE, FUnIE-GAN, Water-Net, UGAN, and DGD-cGAN. The dataset supports the assessment of restoration techniques by providing high-resolution images and ground truth references, enabling researchers to compare performance metrics such as Euclidean distance and visual perception improvements."
    },
    {
      "display_name": "Rain200L",
      "normalized_name": "rain200l",
      "name_variants": [
        "Rain200L"
      ],
      "mention_count": 6,
      "cited_papers_count": 6,
      "topic_summary": "The Rain200L dataset is extensively used for training and evaluating image restoration algorithms, particularly for rain removal, deblurring, denoising, and super resolution. It contains synthetic rain images and real-world noisy images, enabling researchers to test and improve algorithms across various image degradation types. The dataset supports the development of methods for removing raindrops, reducing motion blur, enhancing image clarity, and improving noise reduction in smartphone camera images. It is also used to assess the performance of task-agnostic pre-training and to compare different restoration techniques, focusing on metrics like PSNR and SSIM."
    },
    {
      "display_name": "SOTS-Indoor",
      "normalized_name": "sotsindoor",
      "name_variants": [
        "SOTS-Indoor",
        "SOTS-indoor"
      ],
      "mention_count": 6,
      "cited_papers_count": 6,
      "topic_summary": "The SOTS-Indoor dataset is primarily used for evaluating and benchmarking image dehazing methods, particularly in indoor scenes. It is employed to compare the performance of various dehazing algorithms, focusing on metrics such as PSNR, FLOPs, and visual quality. The dataset includes both synthetic and real-world hazy images, enabling researchers to test the robustness and effectiveness of dehazing techniques under controlled and challenging conditions."
    },
    {
      "display_name": "MIT-Adobe FiveK",
      "normalized_name": "mitadobefivek",
      "name_variants": [
        "MIT-Adobe FiveK"
      ],
      "mention_count": 6,
      "cited_papers_count": 5,
      "topic_summary": "The MIT-Adobe FiveK dataset is primarily used for evaluating and comparing image retouching and restoration techniques, particularly focusing on images retouched by expert C. It is employed to form input-output pairs for training and testing image restoration models, assess the quality and effectiveness of photo retouching networks, and quantitatively compare restoration results using metrics like PSNR and SSIM. The dataset also serves as a source for exposure correction studies, enabling researchers to improve image quality in low-light conditions."
    },
    {
      "display_name": "Rain13K",
      "normalized_name": "rain13k",
      "name_variants": [
        "Rain13K"
      ],
      "mention_count": 6,
      "cited_papers_count": 6,
      "topic_summary": "The Rain13K dataset is primarily used for image restoration and deraining tasks, providing a large and diverse set of images for training and evaluating models. It contains 13,712 paired clean-rain images, making it suitable for robust model development in denoising, dejpeg, and super-resolution tasks. The dataset is often compared with HQ-50K to highlight its scale and diversity. It is used to train and evaluate deraining models, focusing on synthetic and real rain removal, and to enhance the model's ability to handle various image degradations."
    },
    {
      "display_name": "All-Weather dataset",
      "normalized_name": "allweather",
      "name_variants": [
        "All-Weather dataset",
        "All-weather",
        "All-weather dataset"
      ],
      "mention_count": 6,
      "cited_papers_count": 5,
      "topic_summary": "The All-Weather dataset is primarily used to evaluate and train models for image restoration tasks under various weather conditions, including rain, fog, and snow. It is employed to assess the performance of techniques like deweathering, rain removal, and image segmentation, often using metrics such as PSNR, SSIM, FLOPs, and parameter counts. The dataset supports the development and comparison of models, particularly those aiming to handle multiple degradations simultaneously, and includes subsets partitioned by degradation severity."
    },
    {
      "display_name": "UFO-120",
      "normalized_name": "ufo120",
      "name_variants": [
        "UFO-120"
      ],
      "mention_count": 6,
      "cited_papers_count": 6,
      "topic_summary": "The UFO-120 dataset is primarily used to evaluate image restoration and enhancement methods. Researchers employ it to measure the performance of these methods using metrics such as PSNR, SSIM, and UIQM, focusing on quality improvements in restored images. The dataset also aids in analyzing the distribution of CMI scores, particularly noting the skew towards lower values in distorted samples compared to ground truth. This enables detailed assessment and comparison of different image restoration techniques."
    },
    {
      "display_name": "BSD",
      "normalized_name": "bsd",
      "name_variants": [
        "BSD",
        "BSD dataset"
      ],
      "mention_count": 5,
      "cited_papers_count": 5,
      "topic_summary": "The BSD dataset is extensively used for training and evaluating image restoration methods, including denoising, deblurring, dehazing, and rain removal. It provides a diverse set of natural images with ground truth annotations, synthetic degradations, and real-world noise patterns. Researchers use it to assess performance, generalization, and robustness of restoration algorithms, often concatenating it with other datasets to enhance training sets. The dataset's large collection of images, ranging from 91 to 72,135, supports both small-scale and large-scale evaluations, making it a versatile resource for image restoration research."
    },
    {
      "display_name": "Test100",
      "normalized_name": "test100",
      "name_variants": [
        "Test100"
      ],
      "mention_count": 5,
      "cited_papers_count": 3,
      "topic_summary": "The Test100 dataset is primarily used for evaluating and comparing deraining methods, focusing on performance metrics such as PSNR and restoration accuracy. It includes a variety of image sets, ranging from 100 to 2800 images, with both high and low resolutions. The dataset is used to test the effectiveness of multi-stream dense networks and conditional generative adversarial networks in removing rain artifacts. It serves as a benchmark for initial testing, quick validation, and comprehensive evaluation of de-raining algorithms, ensuring robustness and generalization across different rain conditions and image qualities. Additionally, it is used for training models on image denoising and deblurring tasks."
    },
    {
      "display_name": "NHR",
      "normalized_name": "nhr",
      "name_variants": [
        "NHR"
      ],
      "mention_count": 5,
      "cited_papers_count": 5,
      "topic_summary": "The NHR dataset is primarily used for evaluating image dehazing algorithms, particularly in nighttime scenes. It assesses the performance of various models, including CSNet, FocalNet, and HCD, using metrics like PSNR and SSIM. The dataset also supports research in defocus deblurring and desnowing, enabling comparisons of visual quality and quantitative improvements. Its focus on real-world and synthetic nighttime images makes it valuable for advancing image restoration techniques."
    },
    {
      "display_name": "Haze-4K",
      "normalized_name": "haze4k",
      "name_variants": [
        "Haze-4K",
        "Haze4K"
      ],
      "mention_count": 5,
      "cited_papers_count": 4,
      "topic_summary": "The Haze-4K dataset is primarily used for evaluating and benchmarking image dehazing techniques. It includes a diverse set of synthetic and real-world hazy and haze-free images, which are used to assess the generalization, accuracy, and robustness of dehazing algorithms. The dataset contains 4,000 hazy images and 5,342 testing images, enabling comprehensive evaluations in various environmental conditions, both indoor and outdoor, and focusing on high-resolution images to ensure detailed performance assessments."
    },
    {
      "display_name": "LOL v1",
      "normalized_name": "lolv1",
      "name_variants": [
        "LOL v1",
        "LOL-v1"
      ],
      "mention_count": 5,
      "cited_papers_count": 5,
      "topic_summary": "The LOL v1 dataset is primarily used to evaluate and compare low-light image enhancement methods, focusing on improving visibility, color accuracy, and noise reduction in both indoor and outdoor scenes. It supports quantitative assessments and ablation studies, often using synthetic and real-world image pairs. The dataset is also utilized to test the performance of image restoration techniques, particularly in dynamic and low-light conditions, and to validate the effectiveness of methods like UniUIR against others."
    },
    {
      "display_name": "Kodak24",
      "normalized_name": "kodak24",
      "name_variants": [
        "Ko-dak24",
        "Kodak24"
      ],
      "mention_count": 5,
      "cited_papers_count": 5,
      "topic_summary": "The Kodak24 dataset is extensively used in image restoration research, particularly for evaluating the performance of various restoration techniques such as deraining, denoising, dehazing, and super-resolution. It is often employed to test the effectiveness of proposed methods like the FROT cost and transport residual condition module. The dataset supports experiments with specific noise levels (σ = 25, 50) and is used to validate models on both synthetic and real-world images, ensuring robustness and visual fidelity across diverse image types, including urban scenes and natural images."
    },
    {
      "display_name": "RealSRSet",
      "normalized_name": "realsrset",
      "name_variants": [
        "RealSRSet"
      ],
      "mention_count": 4,
      "cited_papers_count": 3,
      "topic_summary": "The RealSRSet dataset is primarily used to evaluate the performance of various models in real-world single image super-resolution tasks, focusing on metric scores and visual comparisons. It is also employed to assess face recognition, image denoising, and the effectiveness of deep blind image super-resolution techniques. The dataset supports the evaluation of models like DiffBIR and SwinIR, emphasizing practical degradation models and low-quality image synthesis."
    },
    {
      "display_name": "BSDS100",
      "normalized_name": "bsds100",
      "name_variants": [
        "BSDS100"
      ],
      "mention_count": 4,
      "cited_papers_count": 4,
      "topic_summary": "The BSDS100 dataset is primarily used to test the generalization ability of image restoration models on images with blended distortions and varying resolutions. It focuses on evaluating visual quality, restoration accuracy, edge detection, segmentation performance, and super-resolution. The dataset is particularly useful for assessing the performance of restoration techniques on diverse image types, including urban scenes and manga images, with an emphasis on preserving fine details, textures, and high contrast."
    },
    {
      "display_name": "Classic5",
      "normalized_name": "classic5",
      "name_variants": [
        "Classic5"
      ],
      "mention_count": 4,
      "cited_papers_count": 4,
      "topic_summary": "The Classic5 dataset is primarily used to evaluate image restoration methods, particularly focusing on JPEG compression artifacts and noise reduction. Researchers employ this dataset to test and compare the performance of different models under varying compression quality factors and noise levels. The dataset facilitates the assessment of perceptual quality, visual quality, and artifact reduction, often using deep convolutional networks. It is crucial for benchmarking and validating image restoration techniques in the context of compressed images."
    },
    {
      "display_name": "Test1",
      "normalized_name": "test1",
      "name_variants": [
        "Test1"
      ],
      "mention_count": 4,
      "cited_papers_count": 3,
      "topic_summary": "The Test1 dataset is primarily used for evaluating and training rain and snow removal algorithms, focusing on generative modeling with GANs and integrating physics models. It supports the development and testing of methods like HRGAN, MPRNet, and AIRFormer, assessing their performance in removing rain streaks, raindrops, and snow from images under various conditions, including heavy rain and synthetic environments. The dataset is also used to benchmark and compare different rain and snow removal techniques, emphasizing robustness, detail preservation, and generalization across diverse image types and environmental conditions."
    },
    {
      "display_name": "DF2K",
      "normalized_name": "df2k",
      "name_variants": [
        "DF2K"
      ],
      "mention_count": 4,
      "cited_papers_count": 3,
      "topic_summary": "The DF2K dataset is primarily used for training and evaluating image restoration models, particularly for image super-resolution. It combines high-resolution images from DIV2K and Flickr2K, providing a large and diverse training set of 84,991 images. Researchers use it to enhance model performance, focusing on efficient training, evaluation, and comparison of different methods, including the Swin Transformer and prompt-learning-based approaches. The dataset's extensive size and high-quality images enable robust training and benchmarking in image restoration tasks."
    },
    {
      "display_name": "Raindrop dataset",
      "normalized_name": "raindrop",
      "name_variants": [
        "RainDrop",
        "Raindrop",
        "Raindrop dataset"
      ],
      "mention_count": 4,
      "cited_papers_count": 1,
      "topic_summary": "The Raindrop dataset is primarily used for training and evaluating raindrop removal algorithms, with a focus on image restoration tasks. It contains 1,119 pairs of clean and rainy images, enabling researchers to assess the effectiveness of different models using metrics like PSNR and SSIM. The dataset supports various methodologies, including synthetic and real raindrop images, and is utilized for deraining, deblurring, dehazing, and low-light enhancement. It facilitates the comparison of different techniques, such as AGAN, DuRN, Quan, and MAXIM-2S, and is often used to train models over multiple epochs to improve performance in real-world scenarios."
    },
    {
      "display_name": "RTTS",
      "normalized_name": "rtts",
      "name_variants": [
        "RTTS"
      ],
      "mention_count": 4,
      "cited_papers_count": 4,
      "topic_summary": "The RTTS dataset is primarily used for evaluating image dehazing and quality assessment techniques, particularly in non-homogeneous and dense haze conditions. It is employed to test and benchmark dehazing algorithms, including TCDAE, using metrics like PSNR and SSIM. The dataset provides hazy and haze-free images, enabling researchers to assess dehazing performance and visual clarity. Additionally, it supports the evaluation of no-reference and full-reference image quality assessment methods, focusing on specific image degradation types and real-world blur conditions."
    },
    {
      "display_name": "Dense-Haze",
      "normalized_name": "densehaze",
      "name_variants": [
        "Dense-Haze"
      ],
      "mention_count": 4,
      "cited_papers_count": 4,
      "topic_summary": "The Dense-Haze dataset is primarily used to evaluate and benchmark dehazing algorithms, focusing on their performance in dense and non-homogeneous haze conditions. It is employed in both synthetic and real-world dehazing experiments, assessing the effectiveness of models in indoor and outdoor scenes. The dataset emphasizes variability in haze distribution and is used to measure performance metrics such as PSNR, SSIM, and FLOPs, comparing results against other models like DeHamer and PMNet. It serves as a realistic testbed for assessing algorithm robustness and generalization in challenging real-world scenarios."
    },
    {
      "display_name": "FFHQ",
      "normalized_name": "ffhq",
      "name_variants": [
        "FFHQ"
      ],
      "mention_count": 4,
      "cited_papers_count": 4,
      "topic_summary": "The FFHQ dataset is primarily used for evaluating image restoration techniques, particularly focusing on high-resolution face images to assess texture recovery, realism, and reconstruction quality. It is also utilized to test the generalization and robustness of algorithms across diverse real-world images and specific degradation types. The dataset serves as a benchmark for comparing generative models and assessing the performance of methods like IAGAN and LaMa in tasks such as large-scale image inpainting and quality evaluation."
    },
    {
      "display_name": "SRRS",
      "normalized_name": "srrs",
      "name_variants": [
        "SRRS"
      ],
      "mention_count": 4,
      "cited_papers_count": 3,
      "topic_summary": "The SRRS dataset is primarily used to train and evaluate algorithms for snow removal and image deraining, focusing on enhancing image clarity by reducing snow and rain artifacts. It employs context-aware deep networks, modified partial convolution, and veiling effect removal techniques. The dataset provides a benchmark for comparing desnowing and deraining algorithms, assessing their performance through metrics like PSNR, particularly highlighting the effectiveness of FocalNet over NAFNet."
    },
    {
      "display_name": "SOTS indoor testset",
      "normalized_name": "sotsindoortestset",
      "name_variants": [
        "SOTS indoor testset"
      ],
      "mention_count": 4,
      "cited_papers_count": 4,
      "topic_summary": "The SOTS indoor testset is used to evaluate and compare the performance of various image dehazing methods, such as GCANet, GridDehaze, DuRN, MSBDN, FFA-Net, and MAXIM-2S. Research focuses on assessing the visual quality and effectiveness of these methods, utilizing the dataset's indoor scenes to provide a standardized benchmark for dehazing algorithms."
    },
    {
      "display_name": "LIME",
      "normalized_name": "lime",
      "name_variants": [
        "LIME"
      ],
      "mention_count": 4,
      "cited_papers_count": 4,
      "topic_summary": "The LIME dataset is used to evaluate various image processing techniques, particularly in scenarios where ground truth is unavailable. It is employed to test and assess methods such as illumination compensation, dynamic range compression, low-light image enhancement, multi-exposure image fusion, and noise reduction. Research focuses on visual quality, color fidelity, detail retention, and user perception, emphasizing the dataset's utility in evaluating these aspects without ground truth data."
    },
    {
      "display_name": "Set12",
      "normalized_name": "set12",
      "name_variants": [
        "Set12"
      ],
      "mention_count": 3,
      "cited_papers_count": 2,
      "topic_summary": "Set12 is primarily used to evaluate image restoration and denoising methods, focusing on performance at various noise levels (σ=15, σ=25, σ=50). It includes both real-world and synthetic datasets, featuring high-resolution urban scenes and a variety of grayscale images. The dataset is employed to assess the robustness, generalization, and visual quality of denoising algorithms, particularly on complex textures and structures. It supports detailed performance evaluations using quantitative metrics and visual assessments."
    },
    {
      "display_name": "Manga109",
      "normalized_name": "manga109",
      "name_variants": [
        "Manga109"
      ],
      "mention_count": 3,
      "cited_papers_count": 3,
      "topic_summary": "The Manga109 dataset is primarily used for evaluating image restoration techniques, particularly in manga images. It assesses the performance of methods such as super-resolution, deraining, denoising, and dejpeging, focusing on metrics like PSNR and SSIM. The dataset emphasizes the preservation of line art, color fidelity, and detail, making it suitable for testing the generalization and quality of restoration algorithms across diverse image conditions."
    },
    {
      "display_name": "Snow100K-L",
      "normalized_name": "snow100kl",
      "name_variants": [
        "Snow100K-L"
      ],
      "mention_count": 3,
      "cited_papers_count": 3,
      "topic_summary": "The Snow100K-L dataset is used to train and evaluate models for various image restoration tasks, including snow removal, raindrop removal, and haze removal. It provides synthetic and real images of degraded conditions to enhance the robustness and effectiveness of restoration algorithms. The dataset supports comprehensive testing and assessment of methods across different weather-related image degradations, ensuring models perform well under diverse conditions."
    },
    {
      "display_name": "SPA+",
      "normalized_name": "spa",
      "name_variants": [
        "SPA",
        "SPA+"
      ],
      "mention_count": 3,
      "cited_papers_count": 3,
      "topic_summary": "The SPA+ dataset is primarily used for evaluating and enhancing image restoration techniques, particularly in deraining, dehazing, and desnowing. It serves as a benchmark for assessing the performance of various models in removing specific image artifacts, such as rain, haze, and snow. The dataset is utilized to train and test methods, focusing on spatial attention mechanisms and no-reference image quality assessment metrics. It includes subsets of real-world hazy and rainy images, enabling researchers to evaluate the visual and objective quality of restored images under diverse conditions."
    },
    {
      "display_name": "Naturalness Preserved Enhancement dataset (NPE)",
      "normalized_name": "naturalnesspreservedenhancement",
      "name_variants": [
        "Naturalness Preserved Enhancement dataset (NPE)"
      ],
      "mention_count": 3,
      "cited_papers_count": 3,
      "topic_summary": "The Naturalness Preserved Enhancement (NPE) dataset is used for evaluating and testing various image restoration techniques, including deraining, dehazing, desnowing, and low-light enhancement. It contains synthetic and real images to assess the performance of models like OneRestore in removing rain, haze, and snow, as well as improving visibility and color fidelity in low-light conditions. The dataset focuses on preserving naturalness while enhancing image quality, making it suitable for real-world applications and algorithm evaluations."
    },
    {
      "display_name": "DRealSR",
      "normalized_name": "drealsr",
      "name_variants": [
        "DRealSR"
      ],
      "mention_count": 3,
      "cited_papers_count": 2,
      "topic_summary": "DRealSR is primarily used for evaluating and benchmarking image super-resolution (BSR) methods, particularly in real-world scenarios. It is applied to assess the performance of super-resolution techniques on real-world images, including those with diverse lighting, weather conditions, and complex textures. The dataset supports both synthetic and real-world image evaluations, enabling researchers to train and test models on high-quality images, enhancing natural and realistic image resolution. It is also used for face detection and recognition tasks in real-world images, contributing to the broader field of image restoration and enhancement."
    },
    {
      "display_name": "LOL-Blur",
      "normalized_name": "lolblur",
      "name_variants": [
        "LOL-Blur"
      ],
      "mention_count": 3,
      "cited_papers_count": 3,
      "topic_summary": "The LOL-Blur dataset is used for training and evaluating deblurring models, particularly in the context of low-light image enhancement. It focuses on improving perceptual quality and fidelity in images captured under low-light conditions. The dataset is also utilized to generate realistic synthetic data, which enhances the performance of models in low-light and blurred image restoration tasks."
    },
    {
      "display_name": "BSD500",
      "normalized_name": "bsd500",
      "name_variants": [
        "BSD500"
      ],
      "mention_count": 3,
      "cited_papers_count": 3,
      "topic_summary": "The BSD500 dataset is primarily used to evaluate and train image restoration methods, particularly focusing on denoising algorithms. It consists of 100 high-quality natural images, often cropped into 512 × 512 pixel patches. The dataset is utilized to generate both real and synthetic noisy images, enhancing the training sets for image restoration models. It is especially valuable for testing and improving denoising performance on real-world noisy images from smartphone cameras, emphasizing realistic noise patterns and high-quality image restoration."
    },
    {
      "display_name": "Set6",
      "normalized_name": "set6",
      "name_variants": [
        "Set6"
      ],
      "mention_count": 3,
      "cited_papers_count": 3,
      "topic_summary": "Set6 is used to evaluate the effectiveness of image restoration methods, particularly in comparing the performance of different techniques. The dataset facilitates the assessment of methods like DPIR by measuring and summarizing PSNR results, enabling researchers to quantitatively compare the quality of restored images. This supports the development and refinement of image restoration algorithms."
    },
    {
      "display_name": "RealIR",
      "normalized_name": "realir",
      "name_variants": [
        "RealIR"
      ],
      "mention_count": 3,
      "cited_papers_count": 3,
      "topic_summary": "The RealIR dataset is used to evaluate image restoration algorithms by addressing limitations in existing benchmarks. It provides a diverse range of imaging devices and content, enhancing the evaluation's breadth. Specifically, it serves as a test set without ground truth, facilitating the assessment of restoration methods using non-reference metrics that align with human perception. This enables researchers to more accurately gauge the performance and perceptual quality of image restoration techniques."
    },
    {
      "display_name": "WED",
      "normalized_name": "wed",
      "name_variants": [
        "WED"
      ],
      "mention_count": 3,
      "cited_papers_count": 3,
      "topic_summary": "The WED dataset is extensively used for evaluating and training image restoration techniques, focusing on robustness across various degradations such as rain, noise, blur, haze, and low-light conditions. It is employed to test the performance of algorithms on diverse content, including high-resolution faces, urban scenes, and medical images, emphasizing generalization, scalability, and quality. The dataset supports research in specific areas like curve text detection, dehazing, and text-prompted restoration, and is crucial for assessing the effectiveness of all-in-one restoration methods."
    },
    {
      "display_name": "RealPhoto60",
      "normalized_name": "realphoto60",
      "name_variants": [
        "RealPhoto60"
      ],
      "mention_count": 3,
      "cited_papers_count": 3,
      "topic_summary": "The RealPhoto60 dataset is used as a real-world test set to evaluate the impact of semantic and restoration prompts on image quality. Researchers employ non-reference metrics to assess these influences, focusing on how different prompts affect the restoration outcomes. This dataset enables the evaluation of image restoration techniques in practical scenarios, providing insights into the effectiveness of various prompts."
    },
    {
      "display_name": "DND",
      "normalized_name": "dnd",
      "name_variants": [
        "DND"
      ],
      "mention_count": 2,
      "cited_papers_count": 2,
      "topic_summary": "The DND dataset is primarily used for real image denoising, focusing on noise characteristics in smartphone and consumer camera images. It is employed for training and evaluating denoising algorithms, providing a benchmark for practical performance. The dataset includes 50 real photographs captured using various consumer cameras, enabling researchers to assess denoising effectiveness through metrics like PSNR and SSIM, particularly in diverse and low-light imaging scenarios."
    },
    {
      "display_name": "real-world dehazing dataset",
      "normalized_name": "realworlddehazing",
      "name_variants": [
        "real-world de-hazing dataset",
        "real-world dehazing dataset"
      ],
      "mention_count": 2,
      "cited_papers_count": 2,
      "topic_summary": "The real-world dehazing dataset is primarily used to train and evaluate image restoration models, focusing on dehazing tasks to improve image clarity and visual quality in real-world images. It is also utilized to assess the performance of de-raining and de-snowing algorithms, providing visual samples to illustrate the effectiveness of these methods in practical scenarios. This dataset enables researchers to test and refine algorithms on authentic, challenging conditions, ensuring robustness and reliability in various image restoration applications."
    },
    {
      "display_name": "RealBlur",
      "normalized_name": "realblur",
      "name_variants": [
        "RealBlur"
      ],
      "mention_count": 2,
      "cited_papers_count": 2,
      "topic_summary": "The RealBlur dataset is primarily used for training and evaluating deblurring models, particularly in real-world scenarios. It is utilized to enhance low-light images, improve deblurring performance, and test generalizability on challenging conditions like night blurry images. The dataset includes high-resolution image pairs, enabling researchers to assess and compare the effectiveness of various deblurring algorithms, including DeblurGAN-v2, BANet, SRN, MPRNet, and MIMO-UNet+."
    },
    {
      "display_name": "RainDS",
      "normalized_name": "rainds",
      "name_variants": [
        "RainDS"
      ],
      "mention_count": 2,
      "cited_papers_count": 2,
      "topic_summary": "RainDS is used in research for evaluating raindrop removal techniques, providing 97 real-world test images to assess the effectiveness of restoration methods. It is specifically employed to test the performance of the proposed CCN and other state-of-the-art methods, focusing on detail recovery and context aggregation in image deraining."
    },
    {
      "display_name": "Berkeley segmentation dataset (BSD)",
      "normalized_name": "berkeleysegmentation",
      "name_variants": [
        "Berkeley segmentation dataset (BSD)"
      ],
      "mention_count": 2,
      "cited_papers_count": 2,
      "topic_summary": "The Berkeley segmentation dataset (BSD) is primarily used to train and evaluate image restoration models. It provides a diverse set of high-quality images, including 400 images of size 180x180, which are utilized for both training and validation. The dataset's extensive image content enhances the robustness and quality of image restoration methods, enabling researchers to assess performance on challenging and varied images."
    },
    {
      "display_name": "Kodak",
      "normalized_name": "kodak",
      "name_variants": [
        "Kodak"
      ],
      "mention_count": 2,
      "cited_papers_count": 2,
      "topic_summary": "The Kodak dataset is used to evaluate image restoration performance by comparing various methods, including GRL-S and GAN-based approaches. Research focuses on assessing restoration quality using PSNR and SSIM scores, enabling detailed comparisons of different restoration techniques. This dataset facilitates the objective measurement and benchmarking of image restoration algorithms."
    },
    {
      "display_name": "SOTS-outdoor",
      "normalized_name": "sotsoutdoor",
      "name_variants": [
        "SOTS-outdoor"
      ],
      "mention_count": 2,
      "cited_papers_count": 2,
      "topic_summary": "The SOTS-outdoor dataset is used for evaluating various image restoration algorithms, including dehazing, deraining, kernel estimation, denoising, low-light enhancement, and motion deblurring. It focuses on outdoor scenes to assess the performance of these algorithms in realistic conditions, ensuring the removal of atmospheric haze, rain streaks, noise, and motion blur while preserving image details and enhancing visibility. The dataset's diverse and realistic scenarios enable comprehensive evaluation and comparison of different restoration techniques."
    },
    {
      "display_name": "NH-HAZE",
      "normalized_name": "nhhaze",
      "name_variants": [
        "NH-HAZE"
      ],
      "mention_count": 2,
      "cited_papers_count": 2,
      "topic_summary": "The NH-HAZE dataset is primarily used as a benchmark for evaluating image dehazing algorithms across various environments, including indoor, outdoor, and remote sensing scenarios. It is employed to test and compare the performance of dehazing methods on both synthetic and real-world images, focusing on high-resolution, diverse, and realistic conditions. The dataset emphasizes dense haze and varying environmental conditions, enabling researchers to assess the effectiveness of dehazing techniques in different contexts."
    },
    {
      "display_name": "RealSR",
      "normalized_name": "realsr",
      "name_variants": [
        "RealSR"
      ],
      "mention_count": 2,
      "cited_papers_count": 2,
      "topic_summary": "The RealSR dataset is used to evaluate and compare various deep learning models, including CNNs, transformers, and diffusion models, for image super-resolution tasks. It focuses on assessing out-of-distribution generalization and performance at upscaling factors of ×2, ×3, and ×4. The dataset enables researchers to benchmark image quality and performance, particularly when testing advanced algorithms like MIRNet-v2 against state-of-the-art methods."
    },
    {
      "display_name": "Rain800",
      "normalized_name": "rain800",
      "name_variants": [
        "Rain800"
      ],
      "mention_count": 2,
      "cited_papers_count": 2,
      "topic_summary": "The Rain800 dataset is primarily used to train and evaluate models for image de-raining, focusing on both synthetic and low-to-high resolution rain images. It provides a diverse set of synthetic rain images to enhance the robustness and generalization of de-raining algorithms. The dataset is employed to test and improve de-raining quality, validate the performance of specific methods like CoIC, DGUNet, and IDT, and assess the impact of dataset collision in de-raining tasks."
    },
    {
      "display_name": "Outdoor-Rain dataset",
      "normalized_name": "outdoorrain",
      "name_variants": [
        "Outdoor-Rain",
        "Outdoor-Rain dataset"
      ],
      "mention_count": 2,
      "cited_papers_count": 2,
      "topic_summary": "The Outdoor-Rain dataset is used to model and simulate heavy rain accumulation in images, enhancing realism in heavy rain scenarios for image restoration. It is employed to train and compare Pix2Pix GAN and CycleGAN models, focusing on image-to-image translation techniques to remove rain from outdoor scenes. This dataset enables researchers to develop and evaluate methods for improving image quality under rainy conditions."
    },
    {
      "display_name": "RDD",
      "normalized_name": "rdd",
      "name_variants": [
        "RDD"
      ],
      "mention_count": 2,
      "cited_papers_count": 2,
      "topic_summary": "The RDD dataset is primarily used for training models focused on document shadow removal, providing a large-scale dataset with 14,200 synthetic images and emphasizing real-world, high-resolution scenarios. It has been utilized to train BGSNet and DocShadow, addressing diverse shadow conditions and enhancing model performance in practical applications. While it also contributes 4,371 real images for general image restoration, its primary application is in document shadow removal."
    },
    {
      "display_name": "Hypersim",
      "normalized_name": "hypersim",
      "name_variants": [
        "Hypersim"
      ],
      "mention_count": 2,
      "cited_papers_count": 2,
      "topic_summary": "The Hypersim dataset is used to evaluate and train image restoration models by providing photorealistic synthetic data with controlled variations, simulating real-world conditions. It supports holistic indoor scene understanding, enabling researchers to assess model performance in diverse and complex environments. The dataset's synthetic nature allows for precise control over variables, enhancing the robustness of image restoration techniques."
    },
    {
      "display_name": "MagicBrush Test",
      "normalized_name": "magicbrushtest",
      "name_variants": [
        "MagicBrush Test"
      ],
      "mention_count": 2,
      "cited_papers_count": 2,
      "topic_summary": "The MagicBrush Test dataset is used to evaluate the effectiveness of PixWizard in instruction-guided image editing. It focuses on assessing the accuracy and quality of edits based on user instructions, as well as the system's performance in precise image editing tasks that combine recognition and generation. This dataset enables researchers to test and refine the capabilities of image editing systems in making accurate and contextually appropriate edits."
    },
    {
      "display_name": "LAION-5B",
      "normalized_name": "laion5b",
      "name_variants": [
        "LAION-5B"
      ],
      "mention_count": 2,
      "cited_papers_count": 2,
      "topic_summary": "The LAION-5B dataset is used in research to compare image quality against other large datasets. This comparison emphasizes the importance of high-quality images in various research contexts. The dataset's extensive size and diverse image content enable researchers to conduct rigorous evaluations and highlight the need for maintaining image quality standards in their studies."
    },
    {
      "display_name": "Berkeley Segmentation Dataset (BSD)",
      "normalized_name": "berkeleysegmentationdatasetbsd",
      "name_variants": [
        "Berkeley Segmentation Dataset (BSD)"
      ],
      "mention_count": 2,
      "cited_papers_count": 2,
      "topic_summary": "The Berkeley Segmentation Dataset (BSD) is extensively used in image restoration and segmentation research. It provides a diverse set of high-quality images for training and evaluating models, particularly in tasks such as image denoising, super-resolution, and boundary detection. Researchers use the dataset to generate image patches, train models like MWCNN, and assess image quality and restoration methods, leveraging its varied content and high resolution to introduce new challenges and improve algorithm performance."
    },
    {
      "display_name": "BSDS400",
      "normalized_name": "bsds400",
      "name_variants": [
        "BSDS400"
      ],
      "mention_count": 2,
      "cited_papers_count": 2,
      "topic_summary": "The BSDS400 dataset is primarily used for image restoration and single image super-resolution tasks, providing a diverse set of clean images for training and evaluating models. It is often compared with HQ-50K to highlight differences in scale. Additionally, it serves as a benchmark for deraining experiments, offering a moderate set of rainy images. The dataset's large and diverse image collection supports comprehensive model training and evaluation, addressing specific degradation types and rain conditions."
    },
    {
      "display_name": "WorldView II",
      "normalized_name": "worldviewii",
      "name_variants": [
        "WorldView II"
      ],
      "mention_count": 2,
      "cited_papers_count": 2,
      "topic_summary": "The WorldView II dataset is used for evaluating image restoration techniques, particularly focusing on high-resolution satellite imagery. Researchers employ this dataset to enhance image quality, addressing issues such as noise reduction and resolution improvement. The dataset's high-resolution characteristics make it suitable for testing and validating advanced image restoration algorithms."
    },
    {
      "display_name": "polyU Real World Noisy Images Dataset",
      "normalized_name": "polyurealworldnoisyimagesdataset",
      "name_variants": [
        "polyU Real World Noisy Images Dataset"
      ],
      "mention_count": 2,
      "cited_papers_count": 2,
      "topic_summary": "The polyU Real World Noisy Images Dataset is used to train color denoising models, particularly for improving noise reduction in smartphone camera images and enhancing denoising performance in real-world noisy conditions. This dataset enables researchers to develop and refine algorithms that effectively address the specific challenges of real-world image noise, thereby advancing the field of image restoration."
    },
    {
      "display_name": "NYU-Rain",
      "normalized_name": "nyurain",
      "name_variants": [
        "NYU-Rain"
      ],
      "mention_count": 2,
      "cited_papers_count": 2,
      "topic_summary": "The NYU-Rain dataset is used to train physics-based models for generating realistic synthetic rain effects in images and for rain removal tasks. It provides background images to enhance the realism of training data. The dataset supports the development of image restoration models, particularly focusing on rain removal using the Adam optimizer with weight decay."
    },
    {
      "display_name": "Rain-Haze-Snow",
      "normalized_name": "rainhazesnow",
      "name_variants": [
        "Rain-Haze-Snow"
      ],
      "mention_count": 2,
      "cited_papers_count": 1,
      "topic_summary": "The 'Rain-Haze-Snow' dataset is used to train and evaluate machine learning models for removing weather-based degradations such as rain, haze, and snow from images. It supports the development of a single-encoder, multi-decoder framework and is utilized in neural architecture search for all-in-one image restoration tasks. This dataset enables researchers to focus on improving the performance and efficiency of image restoration models under various weather conditions."
    },
    {
      "display_name": "CelebA",
      "normalized_name": "celeba",
      "name_variants": [
        "CelebA"
      ],
      "mention_count": 2,
      "cited_papers_count": 2,
      "topic_summary": "The CelebA dataset is primarily used to evaluate image restoration techniques, particularly focusing on 256x256 celebrity face images. It assesses methods like GDP for tasks such as 25% inpainting and 4x super-resolution, emphasizing visual quality, detail preservation, and restoration accuracy. Additionally, a downscaled version (64x64) is used to solve inverse problems, highlighting limitations in demonstrating mode collapse with simpler datasets. The dataset enables researchers to compare supervised and unsupervised methods, focusing on consistency and FID scores."
    },
    {
      "display_name": "AIR40K",
      "normalized_name": "air40k",
      "name_variants": [
        "AIR40K"
      ],
      "mention_count": 2,
      "cited_papers_count": 2,
      "topic_summary": "The AIR40K dataset is primarily used to train, test, and evaluate rain and snow removal algorithms. It provides a large, diverse set of images with varied rain and snow conditions, enabling researchers to focus on noise reduction, clarity enhancement, and detail preservation. The dataset supports both synthetic and real-world scenarios, facilitating the development and validation of robust image restoration techniques, including dehazing, deraining, and desnowing. It is also used to benchmark and compare the performance of different algorithms, ensuring they generalize well across different environmental conditions and image complexities."
    },
    {
      "display_name": "TURBID",
      "normalized_name": "turbid",
      "name_variants": [
        "TURBID"
      ],
      "mention_count": 2,
      "cited_papers_count": 2,
      "topic_summary": "The TURBID dataset is primarily used to evaluate and compare underwater image enhancement methods. It provides high-resolution images for benchmarking various restoration techniques, including Retinex, CLAHE, FUnIE-GAN, Water-Net, UGAN, and DGD-cGAN. The dataset supports training generative adversarial networks, particularly CycleGAN, for enhancing underwater imagery. Research focuses on assessing the quality of restored images using metrics such as PSNR, SSIM, and Euclidean distance to evaluate error visibility and structural similarity."
    },
    {
      "display_name": "ImageNet 1K",
      "normalized_name": "imagenet1k",
      "name_variants": [
        "ImageNet 1K"
      ],
      "mention_count": 2,
      "cited_papers_count": 2,
      "topic_summary": "The ImageNet 1K dataset is primarily used for evaluating super-resolution techniques, providing a diverse set of natural images, urban scenes, and manga images to test the generalizability and performance of models. It is commonly employed to benchmark high-quality facial detail reconstruction, complex texture restoration, and the ability to handle various image categories. The dataset's wide recognition and slight size expansion enable comprehensive testing and comparison of super-resolution and image restoration methods."
    },
    {
      "display_name": "ImageNet ctest10k",
      "normalized_name": "imagenetctest10k",
      "name_variants": [
        "ImageNet ctest10k"
      ],
      "mention_count": 2,
      "cited_papers_count": 2,
      "topic_summary": "The ImageNet ctest10k dataset is primarily used for evaluating and benchmarking image restoration models, particularly focusing on colorization and image-to-image translation tasks. It consists of a 10,000 image subset from the ImageNet validation set and is used to assess the performance of models like Palette and DeepFillv2 on colorization and ultra high-resolution image inpainting. The dataset enables researchers to compare the quality of restored images against ground truth, providing a standardized test set for these specific restoration challenges."
    },
    {
      "display_name": "MS-COCO",
      "normalized_name": "mscoco",
      "name_variants": [
        "MS-COCO"
      ],
      "mention_count": 2,
      "cited_papers_count": 2,
      "topic_summary": "The MS-COCO dataset is used for training and evaluating image restoration algorithms, particularly for JPEG restoration. It provides a large-scale benchmark for these tasks. Researchers also use it to select 3000 pairs of images and semantic prompts to validate models, focusing on common objects in context. This dataset enables comprehensive testing and improvement of image restoration techniques."
    },
    {
      "display_name": "RSBlur",
      "normalized_name": "rsblur",
      "name_variants": [
        "RS-Blur",
        "RSBlur"
      ],
      "mention_count": 2,
      "cited_papers_count": 1,
      "topic_summary": "The RSBlur dataset is primarily used to evaluate and train image deblurring models, focusing on realistic blur synthesis and deblurring in real-world scenarios. It is employed to assess the robustness, generalization, and effectiveness of deblurring algorithms, both on synthetic and real-world blurred images. This dataset enables researchers to conduct numerical comparisons and performance evaluations, emphasizing the model's capability to handle authentic blur variations and synthetic deblurring tasks."
    },
    {
      "display_name": "Places2",
      "normalized_name": "places2",
      "name_variants": [
        "Places2"
      ],
      "mention_count": 2,
      "cited_papers_count": 2,
      "topic_summary": "The Places2 dataset is primarily used to train and evaluate uncropping methods, focusing on image extension and outpainting techniques. It provides diverse subsets for training generative adversarial networks (GANs), enabling researchers to assess the performance of these models in extending images, particularly in top-50 categories. This dataset facilitates the development and comparison of GAN-based methods for image extension tasks."
    },
    {
      "display_name": "RealSet80",
      "normalized_name": "realset80",
      "name_variants": [
        "RealSet80"
      ],
      "mention_count": 2,
      "cited_papers_count": 2,
      "topic_summary": "RealSet80 is used to evaluate image restoration methods, particularly focusing on low-quality images. It supports the development and testing of blind image restoration techniques, enabling researchers to assess the effectiveness of their methods against common challenges in image restoration as documented in recent literature."
    },
    {
      "display_name": "O-HAZE",
      "normalized_name": "ohaze",
      "name_variants": [
        "O-HAZE"
      ],
      "mention_count": 2,
      "cited_papers_count": 2,
      "topic_summary": "The O-HAZE dataset is used to evaluate and benchmark dehazing techniques, particularly focusing on non-homogeneous hazy and haze-free images. It emphasizes the variability in haze distribution and the ability to handle dense haze conditions. Researchers use it to assess the robustness and effectiveness of dehazing models in real-world scenarios, ensuring performance under challenging and diverse environmental conditions."
    },
    {
      "display_name": "RESIDE-Indoor",
      "normalized_name": "resideindoor",
      "name_variants": [
        "RESIDE-Indoor"
      ],
      "mention_count": 2,
      "cited_papers_count": 2,
      "topic_summary": "The RESIDE-Indoor dataset is primarily used for evaluating and training dehazing models, focusing on both indoor and outdoor scenes. It serves as a benchmark for indoor dehazing performance, assessing metrics like PSNR and SSIM. The dataset is utilized to compare the effectiveness of different dehazing algorithms, often highlighting improvements in performance with fewer parameters and computational resources. It enables researchers to test and validate their models in controlled and diverse environmental conditions."
    },
    {
      "display_name": "PIPAL full set",
      "normalized_name": "pipalfullset",
      "name_variants": [
        "PIPAL full set"
      ],
      "mention_count": 2,
      "cited_papers_count": 2,
      "topic_summary": "The PIPAL full set dataset is used to evaluate the performance of anti-aliasing pooling layers in image quality assessment, particularly focusing on full-reference image distortions and GAN-generated images. It assesses the impact of these layers on synthetic image quality and distortion effects, enabling researchers to refine techniques for improving image quality metrics."
    },
    {
      "display_name": "Rain200H",
      "normalized_name": "rain200h",
      "name_variants": [
        "Rain200H"
      ],
      "mention_count": 2,
      "cited_papers_count": 2,
      "topic_summary": "The Rain200H dataset is primarily used for training and evaluating rain removal models, focusing on both synthetic and real rain images. It is utilized to enhance deraining performance by training models for various epochs, ranging from 3 to 750, and assessing methods using metrics like PSNR and SSIM. The dataset includes high-resolution and low-resolution images, diverse rain intensities, and realistic rain effects, enabling researchers to test the robustness and generalization capabilities of their models in both synthetic and real-world scenarios."
    },
    {
      "display_name": "FFHQ 256×256",
      "normalized_name": "ffhq256256",
      "name_variants": [
        "FFHQ 256 × 256",
        "FFHQ 256×256"
      ],
      "mention_count": 2,
      "cited_papers_count": 2,
      "topic_summary": "The FFHQ 256×256 dataset is primarily used for image restoration tasks, specifically colorization and inpainting. It focuses on high-resolution facial images and diverse indoor scenes to enhance color restoration and validate inpainting methods. The dataset's high resolution and varied content enable researchers to evaluate and improve the quality of image restoration techniques in complex and realistic scenarios."
    },
    {
      "display_name": "NTIRE",
      "normalized_name": "ntire",
      "name_variants": [
        "NTIRE"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The NTIRE dataset is used for training and evaluation in image restoration, particularly for dehazing and other restoration tasks. It provides high-quality, high-resolution natural images, with varying sizes ranging from 2,000 to 84,991 images, supporting robust model development and benchmarking in the field."
    },
    {
      "display_name": "CelebA-Test",
      "normalized_name": "celebatest",
      "name_variants": [
        "CelebA-Test"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The CelebA-Test dataset is used to evaluate the performance of image restoration models, particularly the DiffBIR model, by focusing on achieving high FID scores. It is also utilized in a synthetic environment to assess the BFR task, emphasizing face attributes in controlled conditions. This dataset enables researchers to benchmark and refine image restoration techniques and attribute manipulation in facial images."
    },
    {
      "display_name": "DIV2K-Val",
      "normalized_name": "div2kval",
      "name_variants": [
        "DIV2K-Val"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The DIV2K-Val dataset is primarily used for evaluating and validating image restoration models, particularly in real-world super-resolution tasks. It focuses on enhancing image quality, reducing artifacts, and preserving details and naturalness. The dataset is also utilized to assess the performance of face recognition and detection systems post-restoration, ensuring robustness and identity preservation under varying conditions. Its diverse and challenging real-world images make it suitable for mixed real-world image restoration research, contributing to a comprehensive evaluation set."
    },
    {
      "display_name": "LIME low-light enhancement dataset",
      "normalized_name": "limelowlightenhancement",
      "name_variants": [
        "LIME low-light enhancement dataset"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The LIME low-light enhancement dataset is primarily used for enhancing underexposed images by estimating illumination maps. It is also utilized for rain removal, snow removal, and dehazing tasks, providing both synthetic and real-world images to train and evaluate various image restoration algorithms. This dataset enables researchers to test the effectiveness of their methods across different challenging conditions, ensuring robust performance in diverse scenarios."
    },
    {
      "display_name": "CSBD68",
      "normalized_name": "csbd68",
      "name_variants": [
        "CSBD68"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The CSBD68 dataset is used to train and evaluate image denoising models, specifically focusing on residual learning with deep CNNs. This enhances image restoration performance by improving the model's ability to remove noise while preserving important image features. The dataset enables researchers to benchmark and refine their models, contributing to advancements in image denoising techniques."
    },
    {
      "display_name": "AGAN-Data",
      "normalized_name": "agandata",
      "name_variants": [
        "AGAN-Data"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The AGAN-Data dataset is primarily used for evaluating and training raindrop removal techniques from single images. It consists of 861 training images and 58 testing images, supporting both high-resolution and low-resolution evaluations. Researchers use it to assess performance on various rain densities and synthetic rain, comparing their methods against the IDT algorithm. The dataset facilitates quantitative benchmarking and robustness testing of rain removal algorithms, providing a diverse set of real-world and synthetic rainy scenes."
    },
    {
      "display_name": "LSDIR",
      "normalized_name": "lsdir",
      "name_variants": [
        "LSDIR"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The LSDIR dataset is used to address practical tasks and real-world image degradations, specifically for image restoration. It enables researchers to develop and evaluate methods that can handle diverse and realistic image degradations, enhancing the practical applicability of image restoration techniques."
    },
    {
      "display_name": "synthetic dataset of 10 Jerlov water types",
      "normalized_name": "synthetic",
      "name_variants": [
        "synthetic dataset of 10 Jerlov water types"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The synthetic dataset of 10 Jerlov water types is used to generate synthetic underwater images for training and evaluating image restoration models. This dataset focuses on assessing the impact of different water types on image quality, enabling researchers to develop and test algorithms that improve underwater image clarity and restoration."
    },
    {
      "display_name": "Underwater Image Enhancement Benchmark Dataset (UIEBD)",
      "normalized_name": "underwaterimageenhancementbenchmarkdatasetuiebd",
      "name_variants": [
        "Underwater Image Enhancement Benchmark Dataset (UIEBD)"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The Underwater Image Enhancement Benchmark Dataset (UIEBD) is used to evaluate image enhancement techniques, specifically focusing on improving visual quality and restoration accuracy in underwater environments. Researchers employ this dataset to assess the performance of their models, ensuring enhanced clarity and detail in underwater images. The dataset's real-world images provide a robust testbed for these evaluations."
    },
    {
      "display_name": "Snow100K-S",
      "normalized_name": "snow100ks",
      "name_variants": [
        "Snow100K-S"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The Snow100K-S dataset is used to evaluate image desnowing performance, with a focus on both larger-scale and smaller-scale images. Research employs this dataset to measure and compare desnowing effectiveness using PSNR and SSIM metrics, addressing the specific challenges of snow removal in images of varying sizes. This dataset enables researchers to assess the robustness and efficiency of desnowing algorithms across different image scales."
    },
    {
      "display_name": "CycleISP",
      "normalized_name": "cycleisp",
      "name_variants": [
        "CycleISP"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The CycleISP dataset is used to simulate real-world noise in training datasets, which enhances the model's ability to handle noisy images during testing. This approach improves the robustness and generalization of image restoration models by exposing them to a variety of realistic noise conditions. The dataset's realistic noise simulation capabilities are crucial for developing and evaluating models that can effectively restore images under diverse and challenging conditions."
    },
    {
      "display_name": "LOL-simulation",
      "normalized_name": "lolsimulation",
      "name_variants": [
        "LOL-simulation"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The LOL-simulation dataset is primarily used for training and retraining models focused on low-light image restoration, particularly the KinD++ network. It is employed to enhance low-light images and address image restoration challenges, including deblurring. The dataset facilitates comparisons and performance evaluations of different restoration techniques, ensuring robust and effective model performance in low-light conditions."
    },
    {
      "display_name": "SnowTest100K",
      "normalized_name": "snowtest100k",
      "name_variants": [
        "SnowTest100K"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The SnowTest100K dataset is used to qualitatively evaluate and compare the performance of different models, specifically the best model, DesnowNet, and DDMSNet, in snow removal tasks. It leverages semantic and depth priors to enhance the accuracy of snow removal, focusing on image restoration in snowy conditions. This dataset enables researchers to assess the effectiveness of various techniques in this specialized domain."
    },
    {
      "display_name": "LFDOF",
      "normalized_name": "lfdof",
      "name_variants": [
        "LFDOF"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The LFDOF dataset is used for light field based defocus deblurring, utilizing the synthetic aperture and refocusing capabilities of light field technology. It consists of image pairs captured in single shots, enabling researchers to develop and evaluate algorithms that enhance image clarity by addressing defocus blur. This dataset supports the advancement of computational photography techniques, specifically in the domain of light field imaging."
    },
    {
      "display_name": "BSD-grayscale",
      "normalized_name": "bsdgrayscale",
      "name_variants": [
        "BSD-grayscale"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The BSD-grayscale dataset is used to train and test the DuRN-P model for additive Gaussian noise removal. It evaluates the model's performance at noise levels 30, 50, and 70 using PSNR and SSIM metrics. This dataset enables researchers to assess the effectiveness of noise reduction techniques in image restoration."
    },
    {
      "display_name": "Dehaze-TestA",
      "normalized_name": "dehazetesta",
      "name_variants": [
        "Dehaze-TestA"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The Dehaze-TestA dataset is used to evaluate the performance of the DuRN-US model on dehazing tasks, specifically focusing on synthetic outdoor scenes. It assesses the model's effectiveness in removing haze and restoring image clarity, emphasizing image quality and restoration accuracy. This dataset enables researchers to benchmark and validate their dehazing algorithms in controlled environments."
    },
    {
      "display_name": "LOL (Low-Light)",
      "normalized_name": "lollowlight",
      "name_variants": [
        "LOL (Low-Light)"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The LOL (Low-Light) dataset is used to train DNet and FNet models for texture/structure separation and low-light correction, specifically enhancing images captured under low-light conditions. This dataset enables researchers to develop and evaluate methods that improve image quality in challenging lighting environments, focusing on both structural and textural aspects of the images."
    },
    {
      "display_name": "https://github.com/rwenqi/GFNdehazing",
      "normalized_name": "httpsgithubcomrwenqigfndehazing",
      "name_variants": [
        "https://github.com/rwenqi/GFNdehazing"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The dataset from 'https://github.com/rwenqi/GFNdehazing' is used to provide images for dehazing experiments. It specifically tests the effectiveness of the gated fusion network in estimating ground truth images from hazy inputs. The dataset enables researchers to evaluate and validate the performance of dehazing algorithms by providing a benchmark set of hazy and corresponding clear images."
    },
    {
      "display_name": "Real-World Noisy Image Dataset",
      "normalized_name": "realworldnoisyimagedataset",
      "name_variants": [
        "Real-World Noisy Image Dataset"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The Real-World Noisy Image Dataset is used to evaluate various image restoration algorithms, including denoising, motion deblurring, rain removal, dehazing, and joint dehazing and deraining. It tests the performance of these algorithms on real-world images, often captured by a CMOS camera, to assess their effectiveness in practical scenarios. The dataset includes noisy and mean image pairs, grayscale images, and images with rain streaks, enabling comprehensive evaluation of restoration techniques."
    },
    {
      "display_name": "Laion-High-Resolution",
      "normalized_name": "laionhighresolution",
      "name_variants": [
        "Laion-High-Resolution"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The Laion-High-Resolution dataset is used to curate a diverse and high-quality set of 12,225 images for research purposes. It ensures the selection of source images that meet specific standards of resolution and diversity, which is crucial for maintaining the integrity and relevance of the research. This dataset supports methodologies requiring high-resolution imagery, enhancing the accuracy and applicability of the findings."
    },
    {
      "display_name": "GoPro-test",
      "normalized_name": "goprotest",
      "name_variants": [
        "GoPro-test"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The GoPro-test dataset is used to evaluate the performance of the DuRNU model for dynamic scene deblurring. It contains real-world blurry images and their corresponding sharp ground truth images, enabling researchers to compare the effectiveness of the proposed model against state-of-the-art methods in deblurring tasks."
    },
    {
      "display_name": "VIRAT",
      "normalized_name": "virat",
      "name_variants": [
        "VIRAT"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The VIRAT dataset is used to evaluate image restoration techniques, particularly in the context of surveillance and crime-related videos. It focuses on addressing compression artifacts and enhancing video quality. Researchers employ the dataset to assess the performance of restoration methods, emphasizing anomaly detection and quality improvement in low-quality, compressed video data."
    },
    {
      "display_name": "DIV8K",
      "normalized_name": "div8k",
      "name_variants": [
        "DIV8K"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The DIV8K dataset is used to provide example images with typical patch recurrence at different scales, which supports the study of image restoration techniques. This dataset enables researchers to analyze and develop methods that leverage patch recurrence for improving image restoration, focusing on the characteristic of patch recurrence across various scales."
    },
    {
      "display_name": "Raindrop-A",
      "normalized_name": "raindropa",
      "name_variants": [
        "Raindrop-A"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The Raindrop-A dataset is used to evaluate the effectiveness of various raindrop removal methods, including AGAN, DuRN, Quan, and MAXIM-2S, by comparing their visual performance. This dataset enables researchers to assess and contrast these techniques, focusing on the quality and clarity of images after raindrop removal."
    },
    {
      "display_name": "sequentially-applied (or mixed) distortion dataset",
      "normalized_name": "sequentiallyapplieddistortion",
      "name_variants": [
        "sequentially-applied (or mixed) distortion dataset"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The sequentially-applied (or mixed) distortion dataset is used in image restoration research to integrate and evaluate multiple distortion scenarios. It serves as a general form of previous multi-degraded datasets, focusing on the sequential and mixed application of distortions to images. This dataset enhances the robustness of restoration algorithms by providing a comprehensive evaluation framework, allowing researchers to compare performance against other approaches."
    },
    {
      "display_name": "All-in-One weather dataset",
      "normalized_name": "allinoneweather",
      "name_variants": [
        "All-in-One weather dataset"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The All-in-One weather dataset is used to evaluate image restoration tasks involving various weather conditions. It is compared with other datasets to highlight its comprehensive nature, enabling researchers to assess the effectiveness of image restoration techniques under diverse weather scenarios. This dataset's broad range of weather conditions facilitates robust evaluation and comparison in image restoration research."
    },
    {
      "display_name": "WeatherStream Dataset",
      "normalized_name": "weatherstreamdataset",
      "name_variants": [
        "WeatherStream Dataset"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The WeatherStream Dataset is used to train models for removing rain streaks and snowflakes from images, demonstrating superior performance compared to synthetic training data. This dataset enables researchers to develop and evaluate image restoration techniques, specifically addressing the challenge of natural weather-induced degradations in real-world scenarios."
    },
    {
      "display_name": "Rain13k-Test",
      "normalized_name": "rain13ktest",
      "name_variants": [
        "Rain13k-Test"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The Rain13k-Test dataset is used across various image restoration tasks, including deraining, motion deblurring, denoising, low-light enhancement, and kernel deblurring. It serves as a benchmark to evaluate the effectiveness of algorithms in diverse real-world conditions. Researchers use it to assess performance metrics such as image quality improvement and robustness against specific degradations like motion blur and JPEG artifacts. The dataset's versatility and standardization enable comprehensive comparisons of different restoration techniques."
    },
    {
      "display_name": "SID Sony dataset",
      "normalized_name": "sidsony",
      "name_variants": [
        "SID Sony dataset"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The SID Sony dataset is used to evaluate image restoration methods, particularly focusing on enhancing the quality of low-light images. Researchers employ this dataset to test and compare various restoration techniques, assessing their effectiveness in improving image clarity and detail under low-light conditions. This dataset enables rigorous evaluation by providing a standardized set of low-light images, facilitating consistent and comparable results across different methodologies."
    },
    {
      "display_name": "SID Sony camera dataset",
      "normalized_name": "sidsonycamera",
      "name_variants": [
        "SID Sony camera dataset"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The SID Sony camera dataset is used to train models for image restoration, particularly focusing on low-light conditions and camera-specific noise patterns. This dataset enables researchers to develop and evaluate algorithms that enhance image quality under challenging lighting conditions, leveraging the unique characteristics of Sony camera sensors."
    },
    {
      "display_name": "NTIRE 2023 HR NonHomogeneous Dehazing competition",
      "normalized_name": "ntire2023hrnonhomogeneousdehazingcompetition",
      "name_variants": [
        "NTIRE 2023 HR NonHomogeneous Dehazing competition"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The NTIRE 2023 HR NonHomogeneous Dehazing competition dataset is primarily used for dehazing high-resolution images. It serves as a benchmark for evaluating dehazing algorithms, focusing on non-homogeneous haze conditions. The dataset enables researchers to test and compare the effectiveness of their methods in removing haze while preserving image details and natural appearance. No specific methodologies or research questions beyond dehazing are mentioned."
    },
    {
      "display_name": "Bokeh Effect Transformation dataset",
      "normalized_name": "bokeheffecttransformation",
      "name_variants": [
        "Bokeh Effect Transformation dataset"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The Bokeh Effect Transformation dataset is used to evaluate the performance of methods in high-resolution image restoration, specifically focusing on the bokeh effect transformation. Researchers employ this dataset to assess how well their proposed techniques can enhance and transform the bokeh effect in images, addressing the challenge of maintaining or improving visual quality in high-resolution settings."
    },
    {
      "display_name": "NTIRE 2023 Stereo Image Super-Resolution Challenge",
      "normalized_name": "ntire2023stereoimagesuperresolutionchallenge",
      "name_variants": [
        "NTIRE 2023 Stereo Image Super-Resolution Challenge"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The NTIRE 2023 Stereo Image Super-Resolution Challenge dataset is used to evaluate the performance of stereo image super-resolution techniques. It provides source images for the NTIRE 2023 challenge, enabling researchers to assess and compare different methods in enhancing the resolution of stereo images. The dataset's primary application is in the evaluation of super-resolution algorithms, focusing on improving the quality and detail of stereo imagery."
    },
    {
      "display_name": "DICM",
      "normalized_name": "dicm",
      "name_variants": [
        "DICM"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The DICM dataset is used to evaluate image restoration and enhancement algorithms, particularly in scenarios involving non-uniform illumination and dynamic range compression. It is employed to test methods for multi-exposure fusion, focusing on perceptual quality and naturalness preservation without requiring ground truth images. This dataset enables researchers to assess the effectiveness of algorithms in enhancing and restoring images under challenging lighting conditions."
    },
    {
      "display_name": "UIRD-12",
      "normalized_name": "uird12",
      "name_variants": [
        "UIRD-12"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The UIRD-12 dataset is used to evaluate image restoration methods, particularly focusing on One-to-Many and One-to-Composite techniques. It assesses performance across 11 degradation types, including l, h, r, s, and their combinations. This dataset enables researchers to test and compare the effectiveness of methods like CoR in handling diverse image degradations, providing a comprehensive evaluation framework for image restoration."
    },
    {
      "display_name": "CDD-11",
      "normalized_name": "cdd11",
      "name_variants": [
        "CDD-11"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The CDD-11 dataset is used to evaluate and compare low-order methods integrated with CoR against end-to-end methods in image restoration. It focuses on One-to-One and One-to-Many restoration techniques, enabling researchers to assess the effectiveness of different methodologies in these specific areas."
    },
    {
      "display_name": "DID-MDN",
      "normalized_name": "didmdn",
      "name_variants": [
        "DID-MDN"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The DID-MDN dataset is primarily used for single image de-raining, employing density-aware methods through a multi-stream dense network to enhance restoration quality. It is also utilized to construct a more complex hybrid distortion dataset (DID-HY) for evaluating the effectiveness of restoration approaches on images with multiple distortions, thereby increasing the complexity and difficulty of the restoration tasks."
    },
    {
      "display_name": "Lai dataset",
      "normalized_name": "lai",
      "name_variants": [
        "Lai dataset"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The Lai dataset is used to evaluate single image blind deblurring methods, featuring real-world blurry images of varying qualities and resolutions across diverse scenes. It enables researchers to assess the effectiveness of deblurring algorithms in handling natural, uncontrolled conditions, providing a robust benchmark for performance evaluation."
    },
    {
      "display_name": "Kohler dataset",
      "normalized_name": "kohler",
      "name_variants": [
        "Kohler dataset"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The Kohler dataset is used to benchmark blind deconvolution algorithms, focusing on evaluating their performance on real-world camera shake. It consists of 4 images blurred using 12 different kernels, enabling researchers to assess algorithm effectiveness in realistic conditions. This dataset facilitates the comparison and improvement of deconvolution techniques by providing a standardized testbed."
    },
    {
      "display_name": "RealDAE",
      "normalized_name": "realdae",
      "name_variants": [
        "RealDAE"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The RealDAE dataset is used for evaluating image restoration methods, particularly in the context of document image unwarping and real-world degraded images. It provides 130 images for assessment, enabling researchers to test and compare the performance of various restoration techniques on authentic, challenging image samples. This dataset facilitates the development and evaluation of algorithms designed to improve the quality of degraded images, ensuring they perform well on real-world data."
    },
    {
      "display_name": "Text Deblur Dataset (TDD)",
      "normalized_name": "textdeblurdatasettdd",
      "name_variants": [
        "Text Deblur Dataset (TDD)"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The Text Deblur Dataset (TDD) is used to train models specifically for text deblurring. Researchers select 40K samples from the 66K available training samples to develop and refine deblurring algorithms. This dataset enables the enhancement of text clarity in images, addressing the challenge of restoring legibility in blurred text regions."
    },
    {
      "display_name": "Jung’s dataset",
      "normalized_name": "jungs",
      "name_variants": [
        "Jung’s dataset"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "Jung’s dataset is used as a testing set for image restoration, contributing a total of 624 images across multiple studies to evaluate the performance of proposed methods. This dataset enables researchers to assess the effectiveness of their image restoration techniques by providing a standardized set of images for benchmarking."
    },
    {
      "display_name": "TDD",
      "normalized_name": "tdd",
      "name_variants": [
        "TDD"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The TDD dataset is used to train and evaluate models for deblurring photographed document images. Specifically, it facilitates the comparison of performance between DocRes, DE-GAN, and DocDiff models in the deblurring task. This dataset enables researchers to assess the effectiveness of different approaches in enhancing the clarity of document images, focusing on the specific challenge of deblurring."
    },
    {
      "display_name": "Doc3DShade",
      "normalized_name": "doc3dshade",
      "name_variants": [
        "Doc3DShade"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The Doc3DShade dataset is used to train models for intrinsic decomposition of document images, leveraging 90K synthetic images to enrich the training set. It complements synthetic data with 450 real-world images to ensure the model's robust generalization to actual document images. This dataset enhances the accuracy and reliability of intrinsic image decomposition, crucial for improving document image processing and analysis."
    },
    {
      "display_name": "Doc3D",
      "normalized_name": "doc3d",
      "name_variants": [
        "Doc3D"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The Doc3D dataset is used for training and testing models in document image rectification, focusing on geometric representation learning to enhance restoration quality. It enables researchers to evaluate the effectiveness of their models in rectifying document images, ensuring they perform well in real-world scenarios. The dataset's emphasis on geometric transformations makes it particularly useful for improving the accuracy and robustness of document image processing techniques."
    },
    {
      "display_name": "(H)-DIBCO",
      "normalized_name": "hdibco",
      "name_variants": [
        "(H)-DIBCO"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The (H)-DIBCO dataset is used to evaluate document binarization methods, particularly focusing on the performance of gated convolutions in restoring degraded document images. This dataset enables researchers to assess the effectiveness of these techniques in enhancing the readability and quality of historical documents."
    },
    {
      "display_name": "RNI15",
      "normalized_name": "rni15",
      "name_variants": [
        "RNI15"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The RNI15 dataset is used to evaluate image denoising algorithms, particularly focusing on real noisy face images. Researchers employ this dataset to assess algorithm performance in realistic conditions, ensuring that the methods can effectively handle the complexities and variations present in real-world noisy images. This dataset enables rigorous testing and comparison of denoising techniques, contributing to advancements in image restoration."
    },
    {
      "display_name": "DND-SRGB IMAGES",
      "normalized_name": "dndsrgbimages",
      "name_variants": [
        "DND-SRGB IMAGES"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The DND-SRGB IMAGES dataset is used to evaluate image restoration algorithms, focusing on denoising tasks. It consists of real photographs and is employed to measure PSNR improvements over existing methods. This dataset enables researchers to assess the effectiveness of their algorithms in enhancing image quality through quantitative metrics."
    },
    {
      "display_name": "Rain100",
      "normalized_name": "rain100",
      "name_variants": [
        "Rain100"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The Rain100 dataset is used for training and testing single-task image deraining models, comprising 200 clean-rainy image pairs for training and 100 pairs for testing. It evaluates the performance of deraining algorithms, particularly in removing rain from images and handling unseen rain degradation. The dataset's synthetic rain images enable benchmarking and comparing the effectiveness of different deraining methods."
    },
    {
      "display_name": "NYUv2",
      "normalized_name": "nyuv2",
      "name_variants": [
        "NYUv2"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The NYUv2 dataset is primarily used for monocular depth estimation and surface normal estimation, leveraging its large-scale, high-resolution RGB-D images of diverse indoor scenes. It provides researchers with the necessary data to evaluate and develop image restoration techniques, particularly focusing on indoor environments. The dataset's rich RGB-D and surface normal information enables precise and detailed analysis, enhancing the accuracy of these estimations."
    },
    {
      "display_name": "LAION Art dataset",
      "normalized_name": "laionart",
      "name_variants": [
        "LAION Art dataset"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The LAION Art dataset is used to gather high-quality natural images from the Internet for image restoration experiments. It supports the development and evaluation of image restoration techniques by providing a rich source of diverse, high-resolution images. This enables researchers to test and refine algorithms aimed at improving image quality in various restoration tasks."
    },
    {
      "display_name": "FFHQ-raw",
      "normalized_name": "ffhqraw",
      "name_variants": [
        "FFHQ-raw"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The FFHQ-raw dataset is used to enhance face restoration performance in models by providing 70,000 unaligned high-resolution facial images. This dataset specifically aids in improving the quality and accuracy of facial image restoration, contributing to more realistic and detailed facial reconstructions. The large number of high-resolution images enables researchers to train models more effectively, addressing challenges in facial detail recovery and alignment."
    },
    {
      "display_name": "REDS-val-300",
      "normalized_name": "redsval300",
      "name_variants": [
        "REDS-val-300"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The REDS-val-300 dataset is used to evaluate image restoration techniques, particularly for reducing JPEG compression artifacts and deblurring images. It consists of a validation set of 300 images with a PSNR of 29.70. This dataset is employed in the NTIRE 2021 Challenge to assess the performance of image deblurring methods, providing a standardized benchmark for comparing different restoration approaches."
    },
    {
      "display_name": "Cityscapes",
      "normalized_name": "cityscapes",
      "name_variants": [
        "Cityscapes"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The Cityscapes dataset is used for training and evaluating image restoration models, particularly focusing on the semantic understanding of urban scenes. It features high-resolution images and detailed annotations, enabling researchers to enhance and restore images while maintaining contextual accuracy. This dataset supports research in improving the quality and interpretability of urban scene imagery."
    },
    {
      "display_name": "KITTI stereo 2012",
      "normalized_name": "kittistereo2012",
      "name_variants": [
        "KITTI stereo 2012"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The KITTI stereo 2012 dataset is used to create a synthetic RainKITTI2012 dataset, which is employed for evaluating image restoration techniques, particularly focusing on rain removal and stereo vision tasks. This involves generating synthetic rain effects to test and improve the performance of image restoration algorithms in challenging weather conditions. The dataset's stereo imagery is crucial for assessing the effectiveness of these techniques in maintaining depth perception and visual clarity."
    },
    {
      "display_name": "Indoor Training Set (ITS)",
      "normalized_name": "indoortrainingsetits",
      "name_variants": [
        "Indoor Training Set (ITS)"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The Indoor Training Set (ITS) is primarily used to train and evaluate single-image dehazing models, focusing on synthetic indoor scenes. It contains 13,990 hazy images generated from 1,399 clear images, serving as a benchmark for assessing the performance and robustness of dehazing algorithms. The dataset's synthetic nature allows researchers to systematically test and improve restoration quality in controlled conditions."
    },
    {
      "display_name": "ITS dataset",
      "normalized_name": "its",
      "name_variants": [
        "ITS dataset"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The ITS dataset is primarily used for evaluating image restoration algorithms in dehazing and deraining tasks. It provides a diverse set of images with varying levels of haze and rain streaks, enabling researchers to assess the effectiveness of restoration methods. This dataset supports the development and testing of algorithms designed to improve image quality under challenging atmospheric conditions."
    },
    {
      "display_name": "Adobe-MIT Fivek",
      "normalized_name": "adobemitfivek",
      "name_variants": [
        "Adobe-MIT Fivek"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The Adobe-MIT Fivek dataset is primarily used for enhancing image quality in low-light conditions (Low-Light Enhancement, LLE) and for retouching tasks. It generates image pairs for training and evaluating models focused on global tonal adjustments, retouching techniques, and multi-task learning (Multi-Task Modeling, MTM). This dataset enables researchers to develop and test algorithms that perform expert-level image adjustments, particularly in challenging lighting scenarios."
    },
    {
      "display_name": "large-scale dataset on low-light enhancement for ultra-high definition (UHD) images",
      "normalized_name": "largescale",
      "name_variants": [
        "large-scale dataset on low-light enhancement for ultra-high definition (UHD) images"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The large-scale dataset on low-light enhancement for ultra-high definition (UHD) images is used to train and evaluate models aimed at enhancing the visual quality and detail preservation of low-light UHD images. This dataset supports research focused on developing algorithms that improve image clarity and detail in low-light conditions, ensuring that enhanced images maintain high fidelity and natural appearance."
    },
    {
      "display_name": "synthetic image pairs",
      "normalized_name": "syntheticimagepairs",
      "name_variants": [
        "synthetic image pairs"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The synthetic image pairs dataset is used to train conditional diffusion models for image super-resolution, specifically focusing on generating high-resolution images from low-resolution inputs. This dataset enables researchers to develop and test algorithms that enhance image quality, leveraging synthetic data to improve model performance and robustness."
    },
    {
      "display_name": "ImageNet-50K",
      "normalized_name": "imagenet50k",
      "name_variants": [
        "ImageNet-50K"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The ImageNet-50K dataset is used to evaluate and compare the performance of image restoration models, particularly focusing on restoration quality and efficiency. It highlights the limitations of large-scale, high-resolution datasets in providing sufficient texture information for training effective restoration models. The dataset is also utilized to create large-scale restoration datasets and to compare the sizes of different image restoration datasets, emphasizing its role in multimodal pretraining."
    },
    {
      "display_name": "HYDICE DC Mall data",
      "normalized_name": "hydicedcmall",
      "name_variants": [
        "HYDICE DC Mall data"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The HYDICE DC Mall data is used to evaluate the performance of denoising algorithms by adding synthetic Gaussian noise and testing the algorithm's ability to remove it. This dataset enables researchers to assess denoising effectiveness in a controlled environment, focusing on noise reduction in hyperspectral images."
    },
    {
      "display_name": "under-display camera dataset",
      "normalized_name": "underdisplaycamera",
      "name_variants": [
        "under-display camera dataset"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The under-display camera dataset is used to evaluate AgenticIR's capabilities in image restoration, specifically focusing on motion deblurring, defocus deblurring, and brightening. It is also applied to restore underwater images using de-focus deblurring techniques. This dataset enables researchers to assess the effectiveness of AgenticIR in enhancing image quality from challenging imaging conditions."
    },
    {
      "display_name": "SPAData",
      "normalized_name": "spadata",
      "name_variants": [
        "SPAData"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The SPAData dataset is primarily used for evaluating and benchmarking single-image deraining techniques. It provides a high-quality real rain dataset to assess deraining performance, compare methods like SPDNet and SPDNet-local, and test different inference approaches. This dataset enables researchers to focus on improving the quality and effectiveness of deraining algorithms through rigorous evaluation and comparison."
    },
    {
      "display_name": "IPT Dataset",
      "normalized_name": "iptdataset",
      "name_variants": [
        "IPT Dataset"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The IPT Dataset is used to train and evaluate the MIMO-UNet+ model for image restoration, specifically focusing on the performance of transformer-based approaches in addressing various image degradation issues. This dataset enables researchers to assess the effectiveness of advanced models in restoring images with different types of damage."
    },
    {
      "display_name": "CUB",
      "normalized_name": "cub",
      "name_variants": [
        "CUB"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The CUB dataset is primarily used for evaluating image restoration methods in the context of bird classification tasks. It involves comparing different IRMD methods under low contrast and multiple degradation conditions. The dataset is also used to pre-train recognition models like VGG16 and ResNet50 on clean images, followed by testing their performance on restored images from degraded test sets. This enables researchers to assess the impact of image restoration on classification accuracy."
    },
    {
      "display_name": "CelebAHQ",
      "normalized_name": "celebahq",
      "name_variants": [
        "CelebAHQ"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The CelebAHQ dataset is used in image restoration research to align and crop images, ensuring consistent preprocessing. It serves as a training dataset for generative models, particularly for high-resolution face images, enhancing model performance in image restoration. Additionally, it is used to select high-quality images for generating synthetic low-quality images, maintaining no identity overlap with FFHQ."
    },
    {
      "display_name": "SIDD Validation",
      "normalized_name": "siddvalidation",
      "name_variants": [
        "SIDD Validation"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The SIDD Validation dataset is used to evaluate and benchmark denoising algorithms on real-world noisy images from smartphone cameras. It provides a comprehensive validation framework, focusing on performance metrics and assessing denoising effectiveness across a diverse set of images. This dataset enables researchers to validate and compare denoising techniques rigorously."
    },
    {
      "display_name": "DND Benchmark",
      "normalized_name": "dndbenchmark",
      "name_variants": [
        "DND Benchmark"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The DND Benchmark dataset is primarily used for evaluating and comparing the performance of image denoising algorithms, specifically SCPGabNet and PD+, under real noise conditions. It is utilized to train SCPGabNet, which demonstrates superior denoising performance, achieving more than 2dB improvement over PD+. The dataset enables researchers to assess the generalization capabilities of these models, particularly SCPGabNet, across various benchmarks and real-world scenarios."
    },
    {
      "display_name": "RainCityscape",
      "normalized_name": "raincityscape",
      "name_variants": [
        "RainCityscape"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The RainCityscape dataset is used to synthesize rain and haze effects on urban scenes, enhancing the Cityscape dataset for all-in-one image restoration tasks. It provides a base of urban scene images that are modified to simulate various weather conditions, enabling researchers to develop and test image restoration algorithms that can handle multiple degradations simultaneously."
    },
    {
      "display_name": "real rain dataset",
      "normalized_name": "realrain",
      "name_variants": [
        "real rain dataset"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The 'real rain dataset' is used to evaluate single image deraining methods, particularly in real-world scenarios such as driving and surveillance. It focuses on assessing the performance of these methods without the need for clean, paired images, enabling researchers to test the robustness and effectiveness of deraining algorithms in practical conditions."
    },
    {
      "display_name": "Raindrop test dataset",
      "normalized_name": "raindroptest",
      "name_variants": [
        "Raindrop test dataset"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The Raindrop test dataset is primarily used for training and evaluating image restoration models, specifically focusing on raindrop removal from images. It includes both real and synthetic raindrop images, enabling researchers to assess the effectiveness and quality of raindrop removal methods. The dataset supports the development of models using generative adversarial networks and is also applied to evaluate general image restoration accuracy and performance in real-world scenarios. Additionally, it has been used for snow removal, expanding its utility in handling various weather conditions."
    },
    {
      "display_name": "Sea-thru",
      "normalized_name": "seathru",
      "name_variants": [
        "Sea-thru"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The Sea-thru dataset is primarily used to test and evaluate image restoration techniques focused on removing water effects from underwater images. It assesses the effectiveness of restoration algorithms in enhancing image clarity, color correction, and detail recovery. The dataset includes real underwater images with depth map information, enabling researchers to benchmark and compare the performance of various image restoration methods. It is also used to enhance low-resolution distorted underwater images, providing high-resolution pairs for evaluation."
    },
    {
      "display_name": "Snow100K-real",
      "normalized_name": "snow100kreal",
      "name_variants": [
        "Snow100K-real"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The Snow100K-real dataset is used to evaluate and compare the performance of MWFormer and other models on real weather-degraded images, specifically focusing on restoration under unknown corruptions. This dataset enables researchers to assess the effectiveness of image restoration techniques in handling diverse and unpredictable environmental conditions."
    },
    {
      "display_name": "real-world rainy images",
      "normalized_name": "realworldrainyimages",
      "name_variants": [
        "real-world rainy images"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The 'real-world rainy images' dataset is used to train a novel deraining network, specifically focusing on real-world scenarios to enhance image restoration quality. This dataset enables researchers to develop and test algorithms that effectively remove rain artifacts, improving the clarity and usability of images in practical applications."
    },
    {
      "display_name": "dataset by Heide et al. (2015)",
      "normalized_name": "byheideetal2015",
      "name_variants": [
        "dataset by Heide et al. (2015)"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The dataset by Heide et al. (2015) is used to evaluate image restoration methods, specifically focusing on the performance and quality of restored images. It includes 11 grayscale and 68 RGB images, enabling researchers to assess the effectiveness of restoration techniques on both monochrome and color images. This dataset facilitates the comparison and validation of different image restoration approaches."
    },
    {
      "display_name": "standard dataset by Dabov et al. (2007)",
      "normalized_name": "standard",
      "name_variants": [
        "standard dataset by Dabov et al. (2007)"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The standard dataset by Dabov et al. (2007) is primarily used for image denoising experiments and broader image restoration tasks, including JPEG image deblocking and image inpainting. It consists of 68 or 9 RGB images, depending on the study, and is employed to evaluate the performance of denoising and restoration methods under various noise levels, compression qualities, and degradation parameters. This dataset enables researchers to compare and assess the effectiveness of different image restoration techniques, particularly those involving sparse 3-D transform-domain collaborative filtering."
    },
    {
      "display_name": "RENOIR",
      "normalized_name": "renoir",
      "name_variants": [
        "RENOIR"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The RENOIR dataset is used to evaluate image restoration methods, particularly for denoising images corrupted by realistic noise and enhancing the quality of low-light photography. Researchers employ this dataset to test and compare the effectiveness of various denoising algorithms, leveraging its realistic noise characteristics to ensure practical applicability."
    },
    {
      "display_name": "91 images",
      "normalized_name": "91images",
      "name_variants": [
        "91 images"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The '91 images' dataset is used to train and evaluate early image restoration methods, focusing on a small set of 91 images to assess performance. It helps researchers evaluate the effectiveness and robustness of restoration techniques, particularly in terms of generalization capabilities. This dataset is crucial for benchmarking and comparing different restoration algorithms."
    },
    {
      "display_name": "Rain12",
      "normalized_name": "rain12",
      "name_variants": [
        "Rain12"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The Rain12 dataset is used to train and evaluate algorithms for removing synthetic rain streaks and raindrops from images. It focuses on both standard and high-resolution images, enabling researchers to develop and test methods for enhancing image quality in rainy conditions. The dataset supports the evaluation of rain removal techniques, contributing to advancements in image restoration."
    },
    {
      "display_name": "Real-world Task-driven Testing Set (RTTS)",
      "normalized_name": "realworldtaskdriventestingsetrtts",
      "name_variants": [
        "Real-world Task-driven Testing Set (RTTS)"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The Real-world Task-driven Testing Set (RTTS) is primarily used for evaluating image restoration methods, particularly in snow and rain removal tasks. It provides both real and synthetic images to assess the realism, quality, and robustness of desnowing and deraining algorithms. The dataset supports real-world experimental analysis, focusing on task-driven performance metrics and the clarity and detail preservation in challenging conditions. This enables researchers to test and improve the effectiveness of their models in diverse and realistic scenarios."
    },
    {
      "display_name": "PIE",
      "normalized_name": "pie",
      "name_variants": [
        "PIE"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The PIE dataset is used to generate synthetic rainy data for image deraining, specifically focusing on first-person view driving scenes. This synthetic data is employed in autonomous driving applications to enhance pedestrian intention estimation and trajectory prediction. The dataset's focus on realistic driving scenarios enables researchers to test and improve image deraining algorithms in critical real-world conditions."
    },
    {
      "display_name": "Comprehensive Snow Database (CSD)",
      "normalized_name": "comprehensivesnowdatabasecsd",
      "name_variants": [
        "Comprehensive Snow Database (CSD)"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The Comprehensive Snow Database (CSD) is used for training and testing image restoration algorithms, focusing on addressing snow and fog degradation. It contains 8000 training and 2000 testing images, enabling researchers to develop and evaluate methods that enhance image quality under these specific atmospheric conditions."
    },
    {
      "display_name": "ORD database",
      "normalized_name": "orddatabase",
      "name_variants": [
        "ORD database"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The ORD database is used to evaluate and compare image restoration methods, particularly in outdoor scenes with adverse weather conditions. It focuses on removing rain artifacts and enhancing object detection and depth estimation in degraded images. The dataset enables researchers to test and validate their methods against state-of-the-art techniques, especially in challenging outdoor environments."
    },
    {
      "display_name": "RESIDE/SOTS",
      "normalized_name": "residesots",
      "name_variants": [
        "RESIDE/SOTS"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The RESIDE/SOTS dataset is primarily used to evaluate the performance of IRNeXt in dehazing tasks, both in real-world and synthetic environments. It assesses the model's robustness and effectiveness in handling dense and natural haze conditions, using metrics like PSNR to compare with other methods. This dataset enables researchers to test and validate dehazing algorithms under varied and challenging conditions."
    },
    {
      "display_name": "APEX",
      "normalized_name": "apex",
      "name_variants": [
        "APEX"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The APEX dataset is used to simulate noisy hyperspectral images by introducing band-specific noise levels. This enables researchers to focus on the restoration of hyperspectral images under varying noise conditions, employing methodologies that test and improve image restoration techniques. The dataset's characteristic of allowing controlled noise introduction makes it valuable for evaluating restoration algorithms in realistic scenarios."
    },
    {
      "display_name": "Laion400M",
      "normalized_name": "laion400m",
      "name_variants": [
        "Laion400M"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The Laion400M dataset is used to pre-train the CLIP model, leveraging its large-scale collection of image-text pairs. This pre-training enhances the model's generalization ability, which is particularly beneficial for image restoration tasks. The dataset's extensive size and diverse content enable researchers to develop more robust and versatile models, improving performance in various image restoration applications."
    },
    {
      "display_name": "RealSR-V3",
      "normalized_name": "realsrv3",
      "name_variants": [
        "RealSR-V3"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The RealSR-V3 dataset is used for evaluating image restoration approaches, specifically focusing on real-world super-resolution performance and image enhancement quality. It enables researchers to assess the effectiveness of restoration techniques in practical scenarios, ensuring that the methods can handle the complexities and variations present in real-world images. This dataset supports the development and refinement of algorithms aimed at improving image clarity and detail."
    },
    {
      "display_name": "91 images from [36]",
      "normalized_name": "91imagesfrom36",
      "name_variants": [
        "91 images from [36]"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The '91 images from [36]' dataset is used for training models in image de-noising tasks. It provides both a larger set of diverse images and a smaller set of high-quality images, enabling researchers to enhance the robustness and performance of de-noising algorithms. This dataset supports the development of more effective image restoration techniques by offering varied and high-quality visual data."
    },
    {
      "display_name": "GOPRO TEST SET",
      "normalized_name": "goprotestset",
      "name_variants": [
        "GOPRO TEST SET"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The GOPRO TEST SET is used to evaluate and compare the performance of BA and SA methods, specifically using BANET (STACK-4), for image restoration tasks, with a focus on deblurring. This dataset enables researchers to assess the effectiveness of these methods in enhancing image clarity and quality, providing a standardized benchmark for deblurring algorithms."
    },
    {
      "display_name": "ImageNet/CVF",
      "normalized_name": "imagenetcvf",
      "name_variants": [
        "ImageNet/CVF"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The ImageNet/CVF dataset is used for training and evaluating models in image restoration, specifically by creating degraded-clean image pairs and introducing specific distortions. It enhances model performance on visual data and is utilized to extract crude images for MAE training and to stitch paired images for inference. This dataset supports research in improving image quality and robustness in various visual tasks."
    },
    {
      "display_name": "MEF",
      "normalized_name": "mef",
      "name_variants": [
        "MEF"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The MEF dataset is used to evaluate image restoration and enhancement algorithms, particularly for non-uniform illumination images without ground truth. It is employed to test dynamic range compression, multi-exposure fusion, and naturalness-preserved enhancement techniques. Research focuses on assessing perceptual quality and naturalness preservation, enabling the development of methods that improve visual fidelity in challenging lighting conditions."
    },
    {
      "display_name": "FiveK",
      "normalized_name": "fivek",
      "name_variants": [
        "FiveK"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The FiveK dataset is used to evaluate model performance in image restoration and low-light image enhancement. Researchers measure PSNR values for image restoration and SSIM values for low-light enhancement, focusing on quantitative assessments of image quality improvements. This dataset enables rigorous performance comparisons across different models and techniques in these specific areas."
    },
    {
      "display_name": "Low-Light enhancement dataset (LOL)",
      "normalized_name": "lowlightenhancement",
      "name_variants": [
        "Low-Light enhancement dataset (LOL)"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The Low-Light enhancement dataset (LOL) is primarily used for improving the quality of images captured in low-light conditions. Researchers employ deep retinex decomposition techniques to enhance these images, focusing on specific challenges such as noise reduction and brightness adjustment. This dataset enables the development and evaluation of algorithms designed to produce clearer, more visually appealing images under low-light conditions."
    },
    {
      "display_name": "RESIDE-6K",
      "normalized_name": "reside6k",
      "name_variants": [
        "RESIDE-6K"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The RESIDE-6K dataset is used for image dehazing and low-light enhancement, employing the ProRes framework to improve image clarity and quality. It addresses the specific research questions of removing atmospheric haze and enhancing images captured in low-light conditions. The dataset's large scale and diverse image content enable robust testing and validation of these image restoration techniques."
    },
    {
      "display_name": "Set14 × 2",
      "normalized_name": "set142",
      "name_variants": [
        "Set14 × 2"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The Set14 × 2 dataset is used for performance comparison in single image super-resolution research. It focuses on evaluating efficiency and performance metrics such as FLOPs, running time, and peak memory consumption. This dataset enables researchers to benchmark and optimize algorithms by providing a standardized set of images for testing and validation."
    },
    {
      "display_name": "EUVP",
      "normalized_name": "euvp",
      "name_variants": [
        "EUVP"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The EUVP dataset is used for qualitative comparisons in underwater image enhancement research. It provides visual results to demonstrate improvements in visual perception, enabling researchers to evaluate and compare the effectiveness of different enhancement techniques. This dataset supports the development and assessment of algorithms aimed at improving the clarity and quality of underwater images."
    },
    {
      "display_name": "standard test dataset of 68 natural images",
      "normalized_name": "standardtest",
      "name_variants": [
        "standard test dataset of 68 natural images"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The standard test dataset of 68 natural images is primarily used to evaluate the denoising performance of trained models, specifically focusing on Gaussian denoising. This dataset serves as a benchmark to assess and compare the effectiveness of different denoising algorithms, ensuring consistent and standardized testing conditions across various studies."
    },
    {
      "display_name": "Middlebury Stereo",
      "normalized_name": "middleburystereo",
      "name_variants": [
        "Middlebury Stereo"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The Middlebury Stereo dataset is used to synthesize the RESIDE dataset, providing stereo images for both indoor and outdoor scenes. This contributes to the creation of diverse and realistic image pairs, which are essential for training and evaluating algorithms in stereo vision tasks. The dataset's rich variety of scenes enhances the robustness and generalization of models in stereo matching and depth estimation research."
    },
    {
      "display_name": "Nature dataset",
      "normalized_name": "nature",
      "name_variants": [
        "Nature dataset"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The Nature dataset is primarily used to evaluate single-image reflection removal and image restoration algorithms. It consists of image triplets and pairs from diverse environments, including indoor, outdoor, and postcard scenes. Researchers use this dataset to assess the effectiveness, robustness, and accuracy of restoration methods, particularly in uncontrolled and challenging conditions. Performance metrics like PSNR are employed to measure algorithmic performance. The dataset's variety and complexity enable comprehensive evaluation of restoration techniques in real-world scenarios."
    },
    {
      "display_name": "Real20",
      "normalized_name": "real20",
      "name_variants": [
        "Real20"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The Real20 dataset is used to evaluate image restoration methods on 20 real image pairs from diverse indoor and outdoor scenes. It assesses the effectiveness and robustness of restoration techniques across various environments, emphasizing the versatility of these methods. This dataset, constructed by IBCLN, enables researchers to test and compare the performance of restoration algorithms in realistic conditions."
    },
    {
      "display_name": "RESIDE-OTS",
      "normalized_name": "resideots",
      "name_variants": [
        "RESIDE-OTS"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The RESIDE-OTS dataset is used to evaluate all-in-one image restoration models, specifically focusing on outdoor scene dehazing and a range of other degradation types including rain, haze, noise, blur, and low-light conditions. It employs synthetic outdoor scenes to test and validate the performance of these models under various challenging environmental conditions. This dataset enables researchers to comprehensively assess the robustness and effectiveness of image restoration techniques in realistic scenarios."
    },
    {
      "display_name": "OST",
      "normalized_name": "ost",
      "name_variants": [
        "OST"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The OST dataset is used to evaluate the reconstruction task in image super-resolution, specifically focusing on the quality of texture recovery. It consists of 300 images with rich textures, enabling researchers to assess the effectiveness of super-resolution techniques in preserving and enhancing fine details. This dataset facilitates the comparison of different super-resolution methods by providing a standardized set of images with complex textures."
    },
    {
      "display_name": "Berkeley image segmentation database",
      "normalized_name": "berkeleyimagesegmentationdatabase",
      "name_variants": [
        "Berkeley image segmentation database"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The Berkeley image segmentation database is used to extract fog-aware statistical features from fog-free images, focusing on image quality assessment, segmentation, and natural scene analysis. This dataset enables researchers to develop and evaluate methods for improving image segmentation and understanding natural scenes under various conditions."
    },
    {
      "display_name": "LIVE IQA database",
      "normalized_name": "liveiqadatabase",
      "name_variants": [
        "LIVE IQA database"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The LIVE IQA database is used to extract fog-aware statistical features from a corpus of fog-free images, primarily focusing on image quality assessment, segmentation, and natural scene analysis. This dataset enables researchers to develop and evaluate algorithms that enhance image quality and perform accurate segmentation in various natural scenes."
    },
    {
      "display_name": "AAPM 2016 grand challenge dataset",
      "normalized_name": "aapm2016grandchallenge",
      "name_variants": [
        "AAPM 2016 grand challenge dataset"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The AAPM 2016 grand challenge dataset is used for training and evaluating models focused on low-dose X-ray CT reconstruction. It specifically aims to improve image quality and reduce noise, employing methodologies that enhance the clarity and diagnostic utility of low-dose CT images. This dataset enables researchers to develop and test algorithms that address the challenges of low-dose imaging, contributing to safer and more effective radiological practices."
    },
    {
      "display_name": "Enhancing Under-water Visual Perception (EUVP)",
      "normalized_name": "enhancingunderwatervisualperceptioneuvp",
      "name_variants": [
        "Enhancing Under-water Visual Perception (EUVP)"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The EUVP dataset is used to evaluate techniques for reducing underexposure and blur artifacts in underwater images, enhancing visual perception. Researchers employ this dataset to test and validate image restoration methods, focusing on improving clarity and detail in challenging underwater environments. The dataset's relevance lies in its ability to provide realistic scenarios for assessing these specific image enhancement challenges."
    },
    {
      "display_name": "unseen under-display-camera dataset",
      "normalized_name": "unseenunderdisplaycamera",
      "name_variants": [
        "unseen under-display-camera dataset"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The unseen under-display-camera dataset is used to test and evaluate image restoration techniques, specifically focusing on artifact reduction and image quality enhancement for under-display-camera images. It is also utilized to address issues such as color correction and clarity improvement in underwater images. The dataset enables researchers to assess the effectiveness of various restoration methods in these challenging imaging scenarios."
    },
    {
      "display_name": "U45",
      "normalized_name": "u45",
      "name_variants": [
        "U45"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The U45 dataset is primarily used to evaluate and benchmark image restoration and enhancement techniques in underwater environments, with a focus on natural lighting conditions. It is employed to test the effectiveness of methods like NU2Net in addressing minor color cast issues and improving contrast and structural details in underwater images. The dataset supports visual comparisons and performance assessments in diverse and challenging underwater scenarios, emphasizing real-world conditions and natural light variations."
    },
    {
      "display_name": "T90",
      "normalized_name": "t90",
      "name_variants": [
        "T90"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The T90 dataset is used to assess the effectiveness of underwater image restoration techniques on segmentation performance. Researchers apply the Segment Anything Model (SAM) to restored images from various underwater image restoration (UIR) methods, evaluating how these enhancements improve segmentation accuracy. This dataset enables the comparison of different UIR approaches and their impact on downstream computer vision tasks."
    },
    {
      "display_name": "PieAPP",
      "normalized_name": "pieapp",
      "name_variants": [
        "PieAPP"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The PieAPP dataset is used as a benchmark for perceptual image quality assessment, specifically to evaluate the effectiveness of image restoration methods. It employs pairwise preference to assess perceptual image errors, focusing on human perception and preference between two images. This dataset provides accurate propensity probabilities, enhancing the evaluation of image restoration techniques by aligning closely with human judgment."
    },
    {
      "display_name": "Berkeley segmentation database",
      "normalized_name": "berkeleysegmentationdatabase",
      "name_variants": [
        "Berkeley segmentation database"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The Berkeley segmentation database is used to evaluate image restoration algorithms, specifically focusing on segmentation quality and the ecological statistics of natural images. Researchers employ this dataset to assess how well their algorithms can restore and segment images, leveraging its rich set of natural image samples to validate performance in realistic scenarios."
    },
    {
      "display_name": "LabelMe",
      "normalized_name": "labelme",
      "name_variants": [
        "LabelMe"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The LabelMe dataset is primarily used for building image databases for training and evaluation, with a focus on image annotation and representation tasks. It provides a rich set of annotated images that enable researchers to develop and test algorithms for image understanding and object recognition. The dataset's extensive annotations facilitate the creation of robust models for various computer vision applications."
    },
    {
      "display_name": "ExDark",
      "normalized_name": "exdark",
      "name_variants": [
        "ExDark"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The ExDark dataset is used in research to evaluate and compare the performance of low-light image enhancement algorithms. It focuses on improving visibility and detail in dark images, specifically for object detection tasks. The dataset enables researchers to conduct visual comparisons and assess the effectiveness of different enhancement methods in low-light conditions, contributing to high-level vision understanding."
    }
  ]
}