Analysis_Source	Name (extracted)	Citing Article	Citied Article	Features	Name_Variants	Homepage_URL
cited_context	91 images	https://www.semanticscholar.org/paper/cbd65255f6266f798326acc8e0654e40ea0918cf (2018), https://doi.org/10.1109/CVPRW59228.2023.00178 (2023)	https://doi.org/10.1109/ICCV.2001.937655 (2001)	The '91 images' dataset is used primarily for training and evaluating image restoration methods, particularly in the domain of image super-resolution. It provides a small, manageable set of 91 high-quality images, enabling researchers to assess the performance of their models efficiently. This dataset facilitates the development and benchmarking of early image restoration techniques by offering a consistent and well-defined subset for both training and evaluation.		
cited_context	91 images from Yang et al.	https://doi.org/10.1109/ICCV.2017.486 (2017)	https://doi.org/10.1109/CVPR.2015.7299156 (2015)	The '91 images from Yang et al.' dataset is used for training in single image super-resolution, providing a subset of high-resolution images to enhance model performance. This dataset enables researchers to develop and refine algorithms that improve image resolution, focusing on the quality and detail of restored images.		
cited_context	AAHCS	https://doi.org/10.1109/TGRS.2024.3378828 (2024)	https://doi.org/10.1109/LGRS.2017.2771212 (2017)	The AAHCS dataset is used for hyperspectral image reconstruction, employing compressive sensing and tensor decomposition techniques. It enables researchers to address challenges in efficiently reconstructing high-dimensional hyperspectral images from compressed measurements, facilitating advancements in hyperspectral imaging technology.		
cited_context | citing_context	Adair	https://doi.org/10.48550/arXiv.2410.15067 (2024)	https://doi.org/10.48550/arXiv.2401.13221 (2024)	The AdaIR dataset is used to evaluate and test adaptive image restoration methods, specifically focusing on dynamic adjustment to image degradations and frequency mining and modulation. It enables researchers to assess the effectiveness of these techniques in handling various image degradations, thereby advancing the field of adaptive image restoration.; The AdaIR dataset is used for adaptive image restoration, specifically to test and develop techniques that dynamically adjust restoration parameters. Research focuses on methods like frequency mining and modulation to enhance image restoration adaptively. This dataset enables researchers to evaluate and refine algorithms that can dynamically respond to varying image conditions, improving restoration outcomes.	AdaIR	
cited_context	Adobe-MIT Fivek	https://doi.org/10.1145/3664647.3681621 (2024)	https://www.semanticscholar.org/paper/0250f9026b540ae05e2b6528b3c9064e6db637dd (2011)	The Adobe-MIT Fivek dataset is used to generate image pairs for training and evaluating image restoration models, specifically focusing on global tonal adjustments and retouching techniques. This dataset enables researchers to assess the effectiveness of various retouching methods by providing a diverse set of before-and-after image pairs, facilitating the development and refinement of image processing algorithms.		
cited_context	Adobe-MIT Fivek dataset	https://doi.org/10.1145/3664647.3681621 (2024), https://doi.org/10.48550/arXiv.2310.10513 (2023)	https://www.semanticscholar.org/paper/5ee57b18a7be2b671ffc6f5ce5dbbe92112ee0fb (2007)	The Adobe-MIT Fivek dataset is used for training and evaluating image restoration models, particularly focusing on expert-retouched images. It supports research in retouching, Low-Light Filtering (LLF), and Multi-Task Modeling (MTM), enabling the development of algorithms that perform expert-level image adjustments and multi-task learning. The dataset's emphasis on high-quality, professionally adjusted images facilitates the creation of more sophisticated and realistic image processing techniques.		
cited_context	AGAN	https://doi.org/10.1109/ICCV48922.2021.00231 (2021)	https://doi.org/10.1109/CVPR.2017.186 (2017)	The AGAN dataset is used to evaluate the performance of the SPAIR model for image restoration, specifically focusing on adversarial generation and enhancing image quality. It employs a dataset generated by an adversarial network to assess the effectiveness of the SPAIR model in improving image restoration outcomes.		
cited_context | citing_context	Airnet	https://doi.org/10.48550/arXiv.2410.15067 (2024)	https://doi.org/10.48550/arXiv.2401.13221 (2024), https://doi.org/10.1007/978-3-031-72764-1_1 (2024)	The AirNet dataset is used to evaluate and assess image restoration techniques, particularly focusing on aerial images under various atmospheric conditions such as haze and rain. Researchers employ this dataset to test and compare the quality of image restoration methods, ensuring they effectively handle the unique challenges posed by aerial imagery. This enables the development and refinement of robust image restoration algorithms.; The AirNet dataset is primarily used for testing and assessing air-light estimation in hazy images, enhancing visibility in outdoor and aerial scenes. It serves as a benchmark for evaluating image restoration techniques, particularly in handling various degradations. The dataset's focus on atmospheric light recovery makes it valuable for dehazing research, enabling researchers to improve the clarity and quality of images affected by haze.	AirNet	
cited_context | citing_context	All-Weather	https://doi.org/10.48550/arXiv.2410.15385 (2024), https://doi.org/10.48550/arXiv.2504.05135 (2025), https://doi.org/10.1109/CVPR52688.2022.00239 (2021) (+1), https://doi.org/10.48550/arXiv.2312.16610 (2023), https://doi.org/10.1109/CVPR52688.2022.00239 (2021), https://doi.org/10.48550/arXiv.2410.15067 (2024)	https://doi.org/10.1109/CVPR52688.2022.00239 (2021), https://doi.org/10.1109/CVPR.2019.00821 (2019)	The All-Weather dataset is used for training and evaluating models designed to restore images degraded by various adverse weather conditions, such as rain, haze, and snow. It employs methodologies like generative adversarial networks and benchmarks like DA 2 Diff to assess the effectiveness of restoration algorithms under different weather scenarios, ensuring robust performance and fair comparisons with previous methods.; The All-weather dataset is primarily used for training and evaluating image restoration models across various weather conditions, such as raindrop removal and deweathering. It employs generative adversarial networks and assesses model performance using metrics like PSNR and SSIM. The dataset supports research on improving image clarity in degraded scenes and enhancing downstream tasks like segmentation. It also facilitates the comparison of different models, including MoFME, highlighting efficiency and effectiveness in image restoration.	All-Weather, All-weather	
cited_context | citing_context	All-Weather Dataset	https://doi.org/10.1109/ACCESS.2025.3526168 (2023), https://doi.org/10.1109/ACCESS.2025.3526168 (2023), https://doi.org/10.1109/CVPR52733.2024.02352 (2023)	https://doi.org/10.1109/cvpr42600.2020.00324 (2020)	The All-Weather dataset is used to evaluate the performance of image restoration methods under various weather conditions. Researchers employ this dataset to demonstrate both subjective and objective improvements of their proposed methods over existing state-of-the-art techniques. The dataset's diverse weather conditions enable comprehensive testing and validation of image restoration algorithms.; The All-Weather dataset is used to train and evaluate image restoration techniques under various weather conditions, such as rain, fog, and snow. It supports the analysis of L2 norm distribution across image channels and partitions images into subsets based on degradation severity (slight, moderate, heavy). This enables researchers to assess and compare the performance of their methods against state-of-the-art techniques in restoring weather-degraded images.	All-Weather dataset	
cited_context | citing_context	Anyir	https://doi.org/10.48550/arXiv.2410.15067 (2024)	https://doi.org/10.48550/arXiv.2401.13221 (2024)	The AnyIR dataset is used to evaluate all-in-one image restoration methods, specifically focusing on their ability to handle multiple degradations and their versatility. This dataset enables researchers to assess the performance of these techniques under diverse conditions, ensuring they can effectively restore images with various types of degradation.; The AnyIR dataset is used for evaluating and testing all-in-one image restoration techniques, specifically focusing on handling multiple degradation types simultaneously. It enables researchers to assess the versatility of restoration methods across various degradations, ensuring robust performance in a single framework. This dataset supports the development and evaluation of comprehensive image restoration solutions.	AnyIR	
citing_context	AVIRIS	https://www.semanticscholar.org/paper/1fb8e5cc1d55d637a55be02138224cb4b9e87465 (2024)	https://doi.org/10.1016/j.isprsjprs.2022.04.007 (2022)	The AVIRIS dataset is used for evaluating and experimenting with hyperspectral image restoration methods, particularly focusing on composite degradations. It leverages extensive HSI data with numerous spectral bands across diverse geographical areas, enabling researchers to assess the performance of restoration techniques like PromptHSI against others. This dataset's comprehensive coverage and high spectral resolution facilitate robust testing and validation in hyperspectral image restoration research.		
cited_context	B100	https://doi.org/10.48550/arXiv.2210.01427 (2022), https://www.semanticscholar.org/paper/34439db81b482cd562e1cdba974c70a2b89cd6d4 (2019), https://doi.org/10.48550/arXiv.2210.00405 (2022) (+2)	https://doi.org/10.5244/C.26.135 (2012)	Dataset B100 is primarily used for evaluating image super-resolution methods, focusing on low-complexity algorithms and sparse-representation techniques. It consists of 100 high-resolution natural images and is employed to assess the performance of image restoration algorithms, particularly in scaling up images. Research questions often center on the effectiveness of these methods, measured using PSNR and SSIM metrics.		
cited_context | citing_context	Baid	https://doi.org/10.48550/arXiv.2501.12981 (2025)	https://doi.org/10.1016/j.cviu.2022.103403 (2022)	The BAID dataset is used to validate the performance of UniUIR on backlit image enhancement, specifically by comparing it against other methods. This dataset enables researchers to assess and benchmark the effectiveness of image restoration techniques in enhancing backlit images, focusing on the improvement of visual quality and detail recovery.	BAID	
cited_context	BAPPS	https://www.semanticscholar.org/paper/f942a4a56e6549c83844747ad6c4ae58000b2988 (2020)	https://doi.org/10.1109/CVPR.2018.00194 (2018)	The BAPPS dataset is used as a benchmark for perceptual image quality assessment, specifically to evaluate the effectiveness of image restoration algorithms in preserving visual fidelity. It enables researchers to assess how well these algorithms maintain the perceptual quality of restored images, focusing on the visual fidelity aspect rather than just technical metrics. This dataset facilitates the comparison and validation of different image restoration techniques in terms of their ability to produce visually pleasing results.		
cited_context	BBD-100k	https://doi.org/10.1109/TIP.2021.3051462 (2019)	https://doi.org/10.1109/cvpr42600.2020.00271 (2018)	The BBD-100k dataset is used to evaluate and demonstrate the effectiveness of image restoration techniques, particularly in diverse driving scenarios. It supports visual comparisons and showcases the ability of models like EnlightenGAN to improve image quality. The dataset's diverse scenarios enable researchers to test and validate multitask learning approaches in image restoration.		
cited_context	Berkeley Segmentation Dataset (BSD)	https://www.semanticscholar.org/paper/cbd65255f6266f798326acc8e0654e40ea0918cf (2018), https://doi.org/10.1109/CVPRW.2018.00121 (2018), https://doi.org/10.1109/ICCV.2017.486 (2017) (+1)	https://doi.org/10.1109/ICCV.2001.937655 (2001)	The Berkeley Segmentation Dataset (BSD) is primarily used for training and evaluating image restoration and denoising models. It provides a diverse set of natural images, often combined from train, validation, and test sets, to generate image patches and construct comprehensive datasets. Researchers use the dataset to focus on natural image boundaries and structures, enhancing the performance of segmentation and restoration algorithms. The dataset's versatility in providing high-quality, varied images makes it a valuable resource for improving the robustness and accuracy of image processing techniques.		
cited_context	Bokeh Effect Transformation dataset	https://doi.org/10.1109/CVPRW59228.2023.00169 (2023)	https://doi.org/10.1109/CVPR52688.2022.00564 (2021)	The Bokeh Effect Transformation dataset is used to evaluate the effectiveness of image restoration methods, particularly in high-resolution image restoration tasks. It is employed to compare the performance of the proposed method against state-of-the-art techniques like Restormer. This dataset enables researchers to assess and benchmark the quality and efficiency of their restoration algorithms.		
cited_context | citing_context	Bsd	https://doi.org/10.48550/arXiv.2410.15385 (2024), https://doi.org/10.1109/CVPR52733.2024.00277 (2023), https://doi.org/10.48550/arXiv.2407.00676 (2024) (+1), https://www.semanticscholar.org/paper/cbd65255f6266f798326acc8e0654e40ea0918cf (2018), https://doi.org/10.48550/arXiv.2407.00676 (2024), https://doi.org/10.48550/arXiv.2404.12091 (2024) (+2)	https://doi.org/10.1109/TIP.2018.2867951 (2017), https://doi.org/10.1109/ICCV.2001.937655 (2001)	The BSD dataset is primarily used for training and evaluating models in image restoration, specifically for denoising and deblurring tasks. It provides a benchmark with 400 natural images, often concatenated with other datasets like WED, to form robust training sets. This dataset enables researchers to assess the performance of denoising algorithms and improve image restoration techniques through diverse, ground-truth annotated images.; The BSD dataset is primarily used for training and testing image restoration models, focusing on denoising and deblurring tasks. It provides a diverse set of natural images with ground truth annotations, enabling researchers to enhance model performance through patch cropping and concatenation with other datasets like WED. The dataset's 200 training, 100 validation, and 200 test images are used to evaluate model effectiveness at various noise levels (σ = 30, 50, 70). Additionally, BSD is utilized to explore overlaps in ground truths with UCID, impacting the separation of dataset-level or density-level properties in instance-level representations.	BSD	
cited_context	BSD dataset	https://doi.org/10.1109/CVPRW59228.2023.00178 (2023)	https://doi.org/10.1109/ICCV.2001.937655 (2001)	The BSD dataset is used to train and evaluate early image restoration methods, specifically employing a larger set of 400 images. This dataset helps assess the generalization and robustness of these methods, enabling researchers to test and improve the performance of image restoration techniques across diverse image content.		
cited_context	BSD train set	https://www.semanticscholar.org/paper/cbd65255f6266f798326acc8e0654e40ea0918cf (2018), https://doi.org/10.1109/ICCV.2017.486 (2017)	https://doi.org/10.1109/ICCV.2001.937655 (2001)	The BSD train set is used for training models in image super-resolution, specifically contributing 200 images from the Berkeley Segmentation Dataset. These images are utilized to enhance model performance and are integral to the training process in single image super-resolution tasks. The dataset's images help improve the resolution and quality of output images in these models.		
cited_context	BSD-grayscale	https://doi.org/10.1109/CVPR.2019.00717 (2019)	https://doi.org/10.1109/ICCV.2001.937655 (2001)	The BSD-grayscale dataset is used for training and testing image restoration models, particularly focusing on grayscale images. It evaluates the performance of techniques such as denoising and deblurring, specifically assessing the effectiveness of the DuRN-P model in removing additive Gaussian noise at levels 30, 50, and 70. This dataset enables researchers to benchmark and compare different restoration methods in controlled conditions.		
cited_context | citing_context	Bsd100	https://doi.org/10.48550/arXiv.2410.15067 (2024), https://doi.org/10.1109/ICCVW54120.2021.00210 (2021), https://www.semanticscholar.org/paper/cbd65255f6266f798326acc8e0654e40ea0918cf (2018), https://doi.org/10.1109/CVPRW.2018.00121 (2018) (+6)	https://doi.org/10.1109/ICCV.2001.937655 (2001), https://doi.org/10.1007/978-3-642-27413-8_47 (2010)	The BSD100 dataset is primarily used for super-resolution tasks, specifically single image super-resolution. It provides high-quality images for both training and evaluation, enabling researchers to enhance the resolution of low-quality images. The dataset's high-quality images facilitate the development and testing of algorithms aimed at improving image clarity and detail.; The BSD100 dataset is primarily used for evaluating image super-resolution methods, focusing on performance metrics such as PSNR and SSIM at various upscaling factors (×2, ×3, and ×4). It is employed to assess the quality and effectiveness of single-image super-resolution techniques, enabling researchers to compare and validate their methods against established benchmarks.	BSD100	
cited_context | citing_context	Bsd300	https://doi.org/10.1109/TCSVT.2024.3398810 (2023)	https://doi.org/10.1109/TCSVT.2019.2920407 (2017)	The BSD300 dataset is primarily used for image denoising research, where it serves as a benchmark for evaluating the performance of noise reduction techniques. Specifically, it is utilized in studies employing conditional generative adversarial networks (cGANs) to enhance image quality by effectively reducing noise. This dataset's diverse and high-quality images enable researchers to test and refine their models, ensuring robustness and generalizability in various denoising applications.; The BSD300 dataset is primarily used for denoising research, serving as a benchmark for evaluating denoising methods. It provides a well-known collection of clean and noisy images, enabling researchers to test and compare the effectiveness of various denoising algorithms. This dataset facilitates the development and validation of image restoration techniques by offering standardized data for consistent performance assessment.	BSD300	
cited_context | citing_context	Bsd400	https://doi.org/10.48550/arXiv.2410.15067 (2024), https://doi.org/10.1109/CVPR52688.2022.01693 (2022), https://doi.org/10.48550/arXiv.2411.18466 (2024) (+21), https://doi.org/10.48550/arXiv.2410.15067 (2024), https://doi.org/10.1109/CVPR52688.2022.01693 (2022), https://doi.org/10.48550/arXiv.2312.05038 (2023) (+19)	https://doi.org/10.1109/TIP.2016.2631888 (2017)	The BSD400 dataset is primarily used for image denoising research, focusing on improving image quality by reducing noise while preserving important features. It is employed to train and evaluate denoising models using synthetic Gaussian noise at various levels (σ = 15, 25, 50). The dataset provides a diverse set of natural images, enhancing model robustness and serving as a benchmark for comparing denoising algorithms. Additionally, it is used to evaluate image restoration models under various degradations, including rain, haze, blur, and low-light conditions, making it a valuable resource for assessing performance across diverse image content.; The BSD400 dataset is primarily used for training and evaluating image restoration models, with a focus on denoising, deblurring, and addressing various degradations such as rain, haze, noise, blur, and low-light conditions. It provides 400 high-quality, diverse images that enhance model performance and robustness, serving as a benchmark for assessing restoration quality.	BSD400	
cited_context	BSD500	https://doi.org/10.1145/3528233.3530757 (2021), https://doi.org/10.48550/arXiv.2407.20928 (2024), https://doi.org/10.1109/TNNLS.2021.3131739 (2020) (+4)	https://doi.org/10.1109/TPAMI.2010.161 (2011)	The BSD500 dataset is primarily used for training and evaluating models in image restoration tasks, particularly focusing on image denoising and reducing JPEG compression artifacts. It provides a diverse set of natural images, which enhances the generalization and performance of restoration models. The dataset is also utilized to generate synthetic noisy images, contributing to comprehensive training sets and enabling robust model evaluation.		
cited_context | citing_context	Bsd68	https://doi.org/10.1109/CVPR52688.2022.01693 (2022), https://doi.org/10.48550/arXiv.2404.02154 (2024), https://doi.org/10.48550/arXiv.2403.14614 (2024) (+20), https://doi.org/10.1109/CVPR52688.2022.01693 (2022), https://doi.org/10.1007/s11263-023-01843-5 (2023), https://doi.org/10.1109/TPAMI.2021.3088914 (2020) (+26)	https://doi.org/10.1109/ICCV.2001.937655 (2001)	The BSD68 dataset is primarily used for evaluating image denoising and restoration algorithms. It serves as a testing set containing 68 ground truth images, often used after training on larger datasets like BSD400 and WED. Researchers employ it to assess performance at various noise levels, focusing on natural images with human-segmented annotations. The dataset is crucial for benchmarking denoising models, evaluating restoration quality, and enhancing visual performance metrics.; The BSD68 dataset is primarily used for evaluating image denoising and restoration algorithms. It focuses on performance metrics under controlled conditions, particularly at noise levels σ=15, σ=25, and σ=50. The dataset includes a diverse set of natural images, some with human-segmented annotations, which allows researchers to test the robustness and generalization capabilities of their methods. It is used to assess both grayscale and color image denoising, emphasizing the performance of algorithms across varying complexities and noise intensities.	BSD68	
cited_context	BSDS	https://doi.org/10.48550/arXiv.2306.05390 (2023)	https://doi.org/10.7892/BORIS.113226 (2017)	The BSDS dataset is used for evaluating image restoration algorithms, specifically for denoising and dejpeg tasks. It contains 400 images, which researchers use to assess the performance of their algorithms. The dataset's diverse image content enables robust testing and validation of restoration techniques, ensuring they effectively handle various types of image degradation.		
cited_context	BSDS100	https://doi.org/10.1007/978-3-030-58523-5_36 (2020), https://doi.org/10.48550/arXiv.2310.10513 (2023)	https://doi.org/10.1007/978-3-642-27413-8_47 (2010)	The BSDS100 dataset is primarily used to evaluate and test image restoration methods, focusing on edge preservation, texture recovery, and segmentation accuracy under various distortions and resolutions. It assesses the generalization ability of restoration models on images with blended distortions, enabling researchers to compare performance metrics and refine techniques for better edge detection and overall image quality.		
cited_context	BSDS400	https://doi.org/10.48550/arXiv.2306.05390 (2023)	https://doi.org/10.1109/CVPRW.2017.150 (2017)	The BSDS400 dataset is used for image restoration, featuring 400 clean images. It serves as a benchmark to compare with larger datasets like HQ-50K, highlighting differences in scale. This dataset enables researchers to evaluate and contrast the performance of image restoration techniques, providing a standardized set of high-quality images for method validation and comparison.		
cited_context	BSDS500	https://doi.org/10.1109/CVPRW63382.2024.00645 (2024)		The BSDS500 dataset is primarily used to evaluate edge detection and segmentation performance, which is crucial for assessing the effectiveness of image restoration algorithms in preserving fine image details. Researchers employ this dataset to test and compare different methodologies, focusing on how well these techniques maintain structural integrity and clarity in restored images. The dataset's rich set of annotated images enables rigorous and standardized evaluation, facilitating advancements in image processing and computer vision.		
cited_context	Caltech-UCSD Birds-200-2011	https://doi.org/10.1109/CVPR52729.2023.01351 (2023)	https://www.semanticscholar.org/paper/c069629a51f6c1c301eb20ed77bc6b586c24ce32 (2011)	The Caltech-UCSD Birds-200-2011 dataset is used to evaluate image restoration methods, particularly in the context of bird classification tasks. Researchers focus on assessing the performance of these methods under low contrast conditions, utilizing the dataset's detailed images of birds to test and improve the accuracy of classification algorithms in challenging visual environments.		
cited_context | citing_context	Cbsd68	https://doi.org/10.48550/arXiv.2406.18242 (2024), https://doi.org/10.48550/arXiv.2310.10123 (2023), https://doi.org/10.48550/arXiv.2505.12630 (2025) (+3), https://doi.org/10.48550/arXiv.2407.20928 (2024), https://doi.org/10.1109/TPAMI.2021.3088914 (2020), https://doi.org/10.48550/arXiv.2310.10123 (2023) (+12)	https://doi.org/10.1109/CVPR.2015.7299156 (2015), https://doi.org/10.1109/ICCV.2001.937655 (2001)	The CBSD68 dataset is primarily used for evaluating and benchmarking image denoising and restoration algorithms. It consists of 68 color images with known ground truth, enabling researchers to test performance under various noise levels (σ = 15, 25, 50). The dataset facilitates visual comparisons and quantitative assessments, focusing on denoising accuracy, detail preservation, and overall image quality. It serves as a standard benchmark for comparing state-of-the-art methods in image restoration.; The CBSD68 dataset is primarily used for evaluating image denoising performance, particularly in natural images with complex textures and structures. It is employed to test and benchmark various denoising algorithms, including those focused on synthetic noisy images with varying noise levels. The dataset supports research by providing human-segmented images, enabling detailed comparisons of PSNR values and restoration quality across different methods and noise conditions.	CBSD68	
cited_context | citing_context	Cdd-11	https://doi.org/10.1109/TIP.2025.3572788 (2025), https://www.semanticscholar.org/paper/4af9a92a556981375dab533ccc55860f01c8b5da (2025), https://doi.org/10.48550/arXiv.2410.08688 (2024)	https://doi.org/10.48550/arXiv.2407.04621 (2024), https://doi.org/10.1007/978-3-030-58595-2_30 (2020)	The CDD-11 dataset is used to evaluate and simulate image restoration under various compositional degradations, including 11 types such as haze, rain, snow, and low-light conditions. It focuses on assessing the performance of the OneRestore framework in handling these degradation scenarios, enabling researchers to test and improve image restoration techniques in realistic settings.; The CDD-11 dataset is used to evaluate and compare low-order methods integrated with CoR against end-to-end methods in image restoration. It focuses on One-to-One and One-to-Many restoration techniques, encompassing 11 degradation types such as l, h, r, s, and their combinations. This dataset enables researchers to assess the effectiveness of different restoration approaches under various degradation scenarios.	CDD-11	
cited_context	CelebA	https://doi.org/10.1109/CVPR52729.2023.00958 (2023)	https://www.semanticscholar.org/paper/744fe47157477235032f7bb3777800f9f2f45e52 (2017)	The CelebA dataset is used in research to evaluate image restoration techniques, specifically focusing on inpainting and super-resolution tasks. It is employed to demonstrate the visual quality and effectiveness of models in restoring 256x256 celebrity face images, with an emphasis on facial detail recovery. Additionally, it is used to test generative models' capabilities in producing high-fidelity facial images, assessing consistency and FID scores.		
cited_context	CelebA 1K	https://doi.org/10.48550/arXiv.2308.09388 (2023)	https://doi.org/10.1109/ICCV.2015.425 (2014)	The CelebA 1K dataset is used to test and evaluate image restoration models, specifically focusing on 4x super-resolution and zero-shot deblurring tasks. It is employed to enhance facial details and textures in celebrity images, enabling researchers to assess the performance of these models in restoring and deblurring facial features.		
cited_context	CelebA-HQ	https://doi.org/10.48550/arXiv.2308.09388 (2023), https://doi.org/10.1109/TPAMI.2024.3461721 (2024)	https://doi.org/10.1109/ICCV.2001.937655 (2001)	The CelebA-HQ dataset is used for evaluating face super-resolution models, focusing on high-quality facial images to test accuracy and detail preservation. It is also utilized to construct a test dataset of 2,000 facial images, specifically for assessing the quality of facial image restoration. The dataset's high-resolution and detailed facial images enable researchers to rigorously evaluate and compare different restoration techniques.		
cited_context	CelebA-Test	https://doi.org/10.48550/arXiv.2308.15070 (2023)	https://doi.org/10.1109/ICCV.2015.425 (2014)	The CelebA-Test dataset is used to evaluate the performance of image restoration methods, specifically DiffBIR and BFR. It focuses on achieving high FID scores and improving attribute restoration and recognition accuracy on synthetic face images. The dataset's facial attributes and synthetic nature enable researchers to rigorously test and compare different restoration techniques.		
cited_context	CelebChild-Test	https://doi.org/10.48550/arXiv.2308.15070 (2023)	https://doi.org/10.1109/ICCV.2015.425 (2014)	The CelebChild-Test dataset is used to evaluate the BFR method on real-world child face images, focusing on the robustness and generalization of the model. This dataset specifically supports research in facial image restoration, enabling researchers to test how well their models perform on diverse and challenging child face images.		
cited_context | citing_context	Challenge-60	https://doi.org/10.48550/arXiv.2501.12981 (2025)	https://doi.org/10.1109/TCSVT.2019.2963772 (2019)	The Challenge-60 dataset is used for evaluating and comparing the performance of underwater image restoration methods, particularly focusing on enhancing images under natural light conditions. It is employed to assess color correction and visual quality, enabling researchers to visually compare restoration results and test algorithms like NU2Net. This dataset facilitates the development and refinement of techniques for improving underwater imagery in challenging natural light environments.; The Challenge-60 dataset is used for evaluating and comparing the performance of image restoration methods, particularly in enhancing structural details and color correction under natural light conditions. It is specifically applied to assess underwater image restoration techniques, focusing on visual quality and the effectiveness of methods like NU2Net in improving images captured in diverse natural lighting environments.	Challenge-60	
citing_context	Cityscape	https://doi.org/10.1109/TMM.2024.3377136 (2024)	https://doi.org/10.1109/TPAMI.2016.2577031 (2015)	The Cityscape dataset is primarily used for pre-training the Faster-RCNN object detector, enhancing object detection accuracy in the context of all-in-one image restoration. This dataset's rich annotations and diverse urban scenes enable researchers to improve the robustness and precision of object detection models, which is crucial for effective image restoration tasks.		
cited_context	Classic5	https://doi.org/10.1007/s11263-023-01843-5 (2023), https://doi.org/10.48550/arXiv.2210.00405 (2022), https://doi.org/10.1109/ICCVW54120.2021.00210 (2021) (+4)	https://doi.org/10.1109/TIP.2007.891788 (2007)	The Classic5 dataset is primarily used to evaluate the performance of image restoration techniques, including denoising, deblocking, and super-resolution. It consists of five classic images and is employed to test methods under various conditions such as different noise levels, JPEG quality factors, and compression artifacts. Researchers use it to compare models like BNNs and deep CNNs, focusing on metrics like image quality and artifact reduction. The dataset's ground truth images enable precise performance assessment in these specific applications.		
cited_context	Clean	https://doi.org/10.48550/arXiv.2404.12091 (2024)	https://doi.org/10.1109/CVPR.2017.183 (2016)	The 'Clean' dataset is used as a baseline to evaluate the quality of de-rained images by comparing them against clean images. This ensures that the feature extractor maintains image fidelity. The dataset enables researchers to assess the effectiveness of image restoration techniques, specifically focusing on de-raining, by providing a standard reference for image quality.		
cited_context	CoCo	https://doi.org/10.1109/CVPR52729.2023.01351 (2023)	https://doi.org/10.1007/s11263-015-0816-y (2014)	The CoCo dataset is used to provide a rich semantic representation space for image restoration, particularly focusing on common objects in context. This enables researchers to enhance the contextual understanding and accuracy of restored images, addressing specific challenges related to object-centric restoration.		
cited_context	Comprehensive Snow Database (CSD)	https://doi.org/10.1109/ICCV51070.2023.01983 (2023)	https://doi.org/10.1109/ICCV48922.2021.00416 (2021)	The Comprehensive Snow Database (CSD) is used for training and testing image restoration algorithms, focusing on mitigating snow and fog degradation. It contains 8000 training and 2000 testing images, enabling researchers to develop and evaluate algorithms that enhance image quality under adverse weather conditions. This dataset supports the advancement of image restoration techniques by providing a diverse set of images that simulate real-world challenges.		
cited_context	CSBD68	https://doi.org/10.1109/CVPR.2019.01131 (2019)	https://doi.org/10.1109/TIP.2017.2662206 (2016)	The CSBD68 dataset is used to train and evaluate image denoising models, specifically focusing on residual learning with deep CNNs in RGB channels. This dataset enables researchers to assess the performance of their models in removing noise while preserving image details, contributing to advancements in image restoration techniques.		
cited_context | citing_context	Csd	https://doi.org/10.3390/electronics13142817 (2024), https://doi.org/10.1109/TIP.2025.3572788 (2025), https://doi.org/10.1109/TCSVT.2024.3429557 (2024) (+3), https://doi.org/10.24963/ijcai.2024/80 (2024), https://doi.org/10.1109/TPAMI.2023.3330416 (2023), https://doi.org/10.1109/TCSVT.2024.3429557 (2024) (+10)	https://doi.org/10.1109/ICCV48922.2021.00416 (2021)	The CSD dataset is primarily used for training and evaluating image restoration algorithms, particularly for haze and snow removal. It serves as a benchmark for assessing the performance of dehazing and desnowing models using metrics like PSNR, MS-SSIM, and BD-PSNR. The dataset includes synthetic hazy and snowy images, enabling researchers to focus on visual fidelity and artifact reduction in image restoration.; The CSD dataset is primarily used for training and evaluating image restoration algorithms, particularly focusing on desnowing and deraining tasks. It provides a diverse set of images with various weather degradations, enabling researchers to assess the effectiveness of models in removing rain, snow, and other weather-related distortions. The dataset supports the development and comparison of algorithms by offering realistic scenarios and facilitating the evaluation of performance metrics such as PSNR and model complexity.	CSD	
cited_context	ctest10k	https://doi.org/10.1145/3528233.3530757 (2021)	https://doi.org/10.1007/978-3-319-46493-0_35 (2016)	The ctest10k dataset is used for evaluating image restoration models, particularly focusing on the 10,000 image subset from the ImageNet validation set. It is employed to assess the performance of these models, with specific emphasis on colorization tasks. This dataset enables researchers to benchmark and compare different image restoration techniques, ensuring robust evaluation through a standardized set of images.		
cited_context	CUB	https://doi.org/10.1109/CVPR52729.2023.01351 (2023)	https://www.semanticscholar.org/paper/c069629a51f6c1c301eb20ed77bc6b586c24ce32 (2011)	The CUB dataset is used for pre-training recognition models like VGG16 and ResNet50 on clean images, which aids in evaluating restored test images from degraded sets. It is also employed in image classification tasks to compare state-of-the-art image restoration methods for both single and multiple degradations. This dataset facilitates the assessment of restoration techniques by providing a benchmark for classification accuracy post-restoration.		
cited_context	CVF dataset	https://doi.org/10.48550/arXiv.2310.10513 (2023)	https://doi.org/10.1109/CVPR.2009.5206848 (2009)	The CVF dataset is used for training and evaluating models in the context of enhancing performance on visual data. It comprises a diverse array of figures from computer vision papers, which helps in improving model robustness and generalization. This dataset is specifically utilized to address research questions related to image restoration and visual data processing, enabling researchers to test and refine their methodologies on a wide range of visual content.		
cited_context | citing_context	Daair	https://doi.org/10.48550/arXiv.2410.15067 (2024)	https://doi.org/10.48550/arXiv.2401.13221 (2024)	The DaAIR dataset is used to evaluate and enhance data-augmented image restoration techniques, specifically focusing on improving the generalization capabilities of these methods. It is employed in testing and assessing various data-augmentation strategies to ensure that image restoration models perform well across diverse and unseen data. This dataset enables researchers to address the challenge of robustness in image restoration by providing a platform to rigorously test and refine their methodologies.; The DaAIR dataset is used for data-augmented image restoration, specifically to enhance the robustness and generalization of image restoration algorithms. It is employed to evaluate and test these algorithms under diverse and challenging degradations, focusing on improving their performance through data augmentation techniques.	DaAIR	
citing_context	DCIE	https://doi.org/10.48550/arXiv.2410.15385 (2024)	https://doi.org/10.1109/TIP.2018.2867951 (2017)	The DCIE dataset is used for low-light image enhancement, focusing on improving color and contrast in low-light conditions. Researchers employ this dataset to develop and evaluate algorithms that address specific challenges in low-light imaging, enhancing the visual quality of images captured in dim environments. This dataset enables the testing and validation of enhancement techniques, contributing to advancements in low-light image processing.		
citing_context	DDN	https://doi.org/10.48550/arXiv.2412.20157 (2024)	https://doi.org/10.1109/CVPR.2017.186 (2017)	The DDN dataset is used for training deraining models by providing additional synthetic rain images. This expands the training dataset, enhancing model performance in removing rain from images. The dataset's synthetic nature allows researchers to augment their training data, addressing the challenge of limited real-world rain imagery.		
cited_context	DDN 12	https://doi.org/10.48550/arXiv.2404.12091 (2024)	https://doi.org/10.1109/CVPR.2017.183 (2016)	The DDN 12 dataset is used to evaluate de-raining techniques on a large set of synthetic rainy images. Researchers employ this dataset to test the scalability and performance of feature extractors in de-raining tasks, focusing on how well these methods generalize to a broader range of synthetic data. This evaluation helps in assessing the robustness and efficiency of de-raining algorithms.		
cited_context	DDN 4	https://doi.org/10.48550/arXiv.2404.12091 (2024)	https://doi.org/10.1109/CVPR.2017.183 (2016)	The DDN 4 dataset is used to evaluate de-raining performance on a limited set of synthetic rainy images. Researchers focus on the accuracy of the feature extractor, employing the dataset to assess how effectively it can isolate and correct rain artifacts. This specific application highlights the dataset's utility in refining image restoration techniques, particularly in controlled, synthetic environments.		
cited_context | citing_context	Ddn-Data	https://doi.org/10.1109/CVPR52733.2024.00277 (2023), https://doi.org/10.48550/arXiv.2404.12091 (2024), https://doi.org/10.1109/CVPR52733.2024.00277 (2023), https://doi.org/10.48550/arXiv.2308.09388 (2023)	https://doi.org/10.1109/CVPR.2017.183 (2016), https://doi.org/10.1109/CVPR.2017.186 (2017)	The DDN-Data dataset is used to train and evaluate methods for synthetic rainstreak degradation and image denoising, focusing on realistic rain effects and real-world noise patterns. It enables researchers to develop and test algorithms that address specific image degradation issues, enhancing the robustness and effectiveness of image restoration techniques.; The DDN-Data dataset is primarily used for training and evaluating image de-raining models, focusing on synthetic rain effects. It features synthetic rain images generated by a deep detail network, which are used to test and enhance the robustness of rain removal algorithms. The dataset emphasizes realistic rain patterns and preserves image details, enabling researchers to assess the effectiveness of various rain augmentations, particularly light and heavy rain effects.	DDN-Data	
cited_context	Dehaze Dataset	https://doi.org/10.1109/CVPR.2019.00717 (2019)	https://www.semanticscholar.org/paper/fb52f9294fa1cd83660eccac486367eac01ea32d (2018)	The Dehaze Dataset is primarily used for image dehazing research, focusing on the evaluation of techniques to remove atmospheric effects and enhance image clarity. This dataset enables researchers to assess the performance of dehazing algorithms by providing a benchmark for comparing different methods. It is specifically tailored to address the challenge of improving visual quality in hazy images, making it a valuable resource for advancing image restoration methodologies.		
cited_context	Dehaze-TestA	https://doi.org/10.1109/CVPR.2019.00717 (2019)	https://doi.org/10.1007/s11263-015-0816-y (2014)	The Dehaze-TestA dataset is used to evaluate the performance of the DuRN-US model on dehazing tasks, specifically focusing on the quality of image restoration. This dataset enables researchers to assess how effectively their models can remove haze from images, enhancing clarity and visual quality.		
cited_context | citing_context	Dense-Haze	https://doi.org/10.48550/arXiv.2506.16960 (2025), https://doi.org/10.1109/TCSVT.2024.3429557 (2024), https://doi.org/10.1109/TIP.2025.3572788 (2025), https://doi.org/10.48550/arXiv.2308.09388 (2023), https://doi.org/10.24963/ijcai.2024/80 (2024), https://doi.org/10.1109/TCSVT.2024.3429557 (2024) (+6)	https://doi.org/10.1109/ICIP.2019.8803046 (2019), https://doi.org/10.1109/TIP.2018.2867951 (2017)	The Dense-Haze dataset is primarily used for dehazing images, particularly in dense haze conditions, to evaluate and improve the performance of dehazing algorithms. It is employed to test the robustness and effectiveness of methods in enhancing visibility and color fidelity in hazy images. The dataset serves as a benchmark, providing dense-haze and haze-free images for real-world scenario evaluations.; The Dense-Haze dataset is primarily used for evaluating and benchmarking dehazing algorithms under real-world dense haze conditions. It consists of 33 pairs of outdoor hazy and haze-free images, which are utilized to assess the performance, robustness, and accuracy of dehazing methods. Researchers employ this dataset to compare algorithmic improvements, focusing on metrics like PSNR to measure image quality enhancement in challenging atmospheric scenarios.	Dense-Haze	
cited_context | citing_context	Df2K	https://doi.org/10.48550/arXiv.2504.09973 (2025), https://doi.org/10.48550/arXiv.2412.20066 (2024), https://doi.org/10.48550/arXiv.2306.05390 (2023), https://doi.org/10.1109/CVPRW59228.2023.00178 (2023) (+2)	https://doi.org/10.48550/arXiv.2401.03379 (2024), https://doi.org/10.1109/CVPRW.2017.150 (2017)	The DF2K dataset is used to evaluate the versatility of image restoration methods across a comprehensive seven-task benchmark. This benchmark assesses the method's performance on diverse image restoration tasks, leveraging the dataset's extensive and varied content. The dataset enables researchers to test and validate their approaches in a robust, multi-faceted manner, ensuring broad applicability and effectiveness in real-world scenarios.; The DF2K dataset is used primarily for single image super-resolution and image restoration research. It combines images from DIV2K and Flickr2K to create a larger, more diverse training set, enhancing model performance and evaluation. Researchers use it to train and compare classic and advanced models, such as those employing the Swin Transformer architecture, to improve the quality of degraded images.	DF2K	
cited_context	DICM	https://doi.org/10.1109/CVPR52733.2024.02404 (2024)	https://doi.org/10.1109/TIP.2013.2261309 (2013)	The DICM dataset is used to test image restoration methods in real-world settings, particularly focusing on scenarios where ground truth data is unavailable. This dataset enables researchers to evaluate the practical applicability and robustness of their restoration techniques in more realistic conditions, enhancing the reliability of image restoration methods.		
citing_context	DID	https://doi.org/10.48550/arXiv.2412.20157 (2024)	https://doi.org/10.1109/CVPR.2017.186 (2017)	The DID dataset is primarily used for training deraining models, providing a diverse set of synthetic rain images. This enhances the robustness of the models by exposing them to varied rain conditions. The dataset's synthetic nature allows researchers to control and manipulate rain characteristics, improving the model's generalization and performance in real-world scenarios.		
cited_context	DID 1	https://doi.org/10.48550/arXiv.2404.12091 (2024)	https://doi.org/10.1109/TCSVT.2019.2920407 (2017)	The DID 1 dataset is used to evaluate de-raining performance on a small, diverse set of synthetic rainy images. Researchers employ this dataset to assess the generalizability of feature extractors, focusing on how well these models can handle varied synthetic rain conditions. This evaluation helps in understanding the robustness and adaptability of image restoration techniques.		
cited_context	DID 3	https://doi.org/10.48550/arXiv.2404.12091 (2024)	https://doi.org/10.1109/TCSVT.2019.2920407 (2017)	The DID 3 dataset is used to test de-raining algorithms on a moderate set of diverse synthetic rainy images. Researchers focus on evaluating the consistency of the feature extractor, ensuring that the model can reliably remove rain artifacts while preserving image details. This dataset enables the assessment of de-raining techniques in controlled, synthetic environments.		
cited_context | citing_context	Did-Data	https://doi.org/10.1109/TCSVT.2023.3299324 (2024), https://doi.org/10.1109/CVPR52733.2024.00277 (2023), https://doi.org/10.48550/arXiv.2404.12091 (2024), https://doi.org/10.1109/TCSVT.2023.3299324 (2024), https://doi.org/10.1109/CVPR52733.2024.00277 (2023)	https://doi.org/10.1109/CVPR.2017.183 (2016), https://doi.org/10.1109/TCSVT.2019.2920407 (2017)	The DID-Data dataset is used in research for training and evaluating image restoration methods, particularly in synthetic rainstreak degradation and image denoising. It integrates physics models and conditional adversarial learning to test robustness under diverse image conditions and degradation scenarios. This dataset enables researchers to assess the performance of their methods in challenging and varied environments.; The DID-Data dataset is primarily used for training and evaluating image restoration models, particularly in de-raining and denoising tasks. It contains paired rainy and clean images, as well as diverse synthetic rain and noise conditions, enabling researchers to test the robustness and effectiveness of their methods in various degradation scenarios. The dataset supports the integration of physics models and conditional adversarial learning, enhancing the performance and detail preservation in image restoration.	DID-Data	
cited_context	DID-MDN	https://doi.org/10.1007/978-3-030-58526-6_19 (2020), https://doi.org/10.48550/arXiv.2308.09388 (2023)	https://doi.org/10.1109/CVPR.2018.00079 (2018)	The DID-MDN dataset is primarily used for evaluating and enhancing single image de-raining methods, with a focus on density-aware performance. It is employed in constructing more complex hybrid distortion datasets (DID-HY) to test image restoration approaches under increased difficulty. The dataset facilitates the development of multi-stream dense networks to improve network representation capabilities for handling complex degradations.		
cited_context	DID-MDN Data	https://doi.org/10.1109/CVPR.2019.00717 (2019)	https://www.semanticscholar.org/paper/fb52f9294fa1cd83660eccac486367eac01ea32d (2018)	The DID-MDN Data dataset is used for joint dehazing and deraining in image restoration research. It tests the robustness of models in handling multiple degradations simultaneously. This dataset enables researchers to evaluate and improve the performance of models designed to address complex image degradation issues, ensuring they can effectively restore images under varied conditions.		
cited_context	DIR300	https://doi.org/10.1109/CVPR52733.2024.01482 (2024)	https://doi.org/10.48550/arXiv.2210.08161 (2022)	The DIR300 dataset is used for evaluating the performance of trained models in document image restoration. It serves as a benchmark with 300 challenging document images, enabling researchers to test and compare the effectiveness of their restoration techniques. This dataset facilitates the assessment of model accuracy and robustness in handling complex document images.		
cited_context | citing_context	Div2K	https://doi.org/10.1109/TIP.2025.3572788 (2025), https://doi.org/10.48550/arXiv.2310.10123 (2023), https://doi.org/10.48550/arXiv.2404.02154 (2024) (+1), https://doi.org/10.48550/arXiv.2306.05390 (2023), https://doi.org/10.48550/arXiv.2210.01427 (2022), https://doi.org/10.48550/arXiv.2405.02843 (2024) (+28)	https://doi.org/10.1109/CVPRW.2017.151 (2017), https://doi.org/10.1109/CVPRW.2017.150 (2017)	The DIV2K dataset is primarily used for training and evaluating image restoration models, focusing on high-resolution image quality and detail preservation. It is utilized for tasks such as image super-resolution, denoising, and compression, particularly in single-degradation scenarios. The dataset provides high-quality, high-resolution natural images, enabling researchers to develop and assess algorithms that enhance image clarity and reduce noise.; The DIV2K dataset is primarily used for training and evaluating models in single image super-resolution, image denoising, and compression artifact reduction. It provides high-quality images that enhance restoration performance, focusing on improving image resolution, quality, and reducing artifacts. Models like COLA-Net and SwinIR are trained using this dataset to achieve better results in these tasks, particularly at various scale factors.	DIV2K	
cited_context	DIV2K train set	https://doi.org/10.1109/ICCV51070.2023.00495 (2023)	https://doi.org/10.1109/CVPRW.2017.150 (2017)	The DIV2K train set is used for training image restoration models, specifically focusing on high-resolution image recovery and enhancement. This dataset enables researchers to develop and refine algorithms that improve image quality, addressing issues such as resolution, clarity, and detail. Its high-resolution images provide a robust foundation for training models to perform advanced image restoration tasks.		
cited_context | citing_context	Div2K Validation Set	https://doi.org/10.48550/arXiv.2310.10123 (2023), https://doi.org/10.48550/arXiv.2310.10123 (2023), https://www.semanticscholar.org/paper/dbfdb4235ac0d766b2f1a2dde89de6ed60e86605 (2021), https://doi.org/10.1109/TPAMI.2021.3088914 (2020)	https://doi.org/10.1109/CVPR.2017.35 (2016), https://doi.org/10.1109/CVPRW.2017.150 (2017)	The DIV2K validation set is primarily used for super-resolution tasks, where it assesses the ability to enhance image resolution. Researchers employ this dataset to evaluate and compare different super-resolution algorithms, focusing on the quality and effectiveness of resolution enhancement. The dataset's high-resolution images provide a benchmark for validating these techniques, ensuring they meet performance standards in image restoration.; The DIV2K validation set is primarily used for super-resolution tasks, providing a high-quality set of 100 images to validate image restoration models. It supports the development and testing of algorithms by offering a benchmark for upscaling and enhancing image quality. The dataset's high-resolution images enable researchers to evaluate the performance and effectiveness of their models in producing detailed, high-quality outputs.	DIV2K validation set	
cited_context	DIV2K-Val	https://doi.org/10.48550/arXiv.2308.15070 (2023)	https://doi.org/10.1109/TIP.2018.2839891 (2017)	The DIV2K-Val dataset is used for validating and evaluating image restoration models, particularly focusing on high-resolution image quality and restoration accuracy. It is also employed to assess synthetic image super-resolution performance in the BSR task. The dataset's high-resolution images enable researchers to rigorously test and refine their models, ensuring they meet stringent quality standards.		
cited_context	DIV2K-Valid	https://doi.org/10.48550/arXiv.2310.10513 (2023)	https://doi.org/10.1007/s11042-016-4020-z (2015)	The DIV2K-Valid dataset is used for validating and benchmarking image restoration models. It provides high-quality images essential for evaluating the performance of these models. Researchers employ this dataset to ensure their methods achieve accurate and reliable image restoration, focusing on benchmarking and validation rather than training. The dataset's high-quality images enable precise assessment of restoration techniques, facilitating robust comparisons across different models and approaches.		
cited_context	DIV2K100	https://doi.org/10.1109/ICCV51070.2023.01204 (2023)	https://doi.org/10.1007/978-3-642-27413-8_47 (2010)	The DIV2K100 dataset is primarily used to evaluate GAN-based super-resolution methods and image restoration techniques. Research focuses on performance metrics such as PSNR and LPIPS to assess image quality and perceptual similarity in high-resolution images. This dataset enables researchers to benchmark and compare the effectiveness of different super-resolution and restoration approaches, emphasizing high-quality image output.		
cited_context	DIV8K	https://doi.org/10.1109/CVPRW59228.2023.00178 (2023), https://doi.org/10.1109/ICCV51070.2023.00495 (2023)	https://doi.org/10.1109/CVPRW.2017.150 (2017)	The DIV8K dataset is used for evaluating and training image restoration models, particularly focusing on high-resolution image inpainting and quality assessment. It provides high-resolution images to test the scalability and quality of restoration techniques, enabling researchers to assess the performance of their models in handling detailed and large-scale images.		
cited_context	DIV8K train set	https://doi.org/10.1109/ICCV51070.2023.00495 (2023)	https://doi.org/10.1109/CVPRW.2017.150 (2017)	The DIV8K train set is used for training image restoration models, providing a larger dataset to enhance model performance. This dataset supports the development and improvement of image restoration techniques by offering a substantial amount of data, which is crucial for training robust and effective models.		
cited_context	DND	https://doi.org/10.1109/CVPR52688.2022.00564 (2021), https://doi.org/10.48550/arXiv.2210.01427 (2022), https://doi.org/10.48550/arXiv.2211.13654 (2022) (+4)	https://doi.org/10.1109/CVPR.2018.00182 (2018)	The DND dataset is primarily used for real-world image denoising experiments, focusing on evaluating the effectiveness of denoising algorithms on diverse sets of real-world noisy images, particularly from digital photographs and smartphone cameras. It serves as a benchmark for testing and comparing various denoising models, including the IPT-V2 and ART models, under different conditions and parameter capacities. The dataset's real-world nature allows researchers to assess high-quality denoising performance and validate the superiority of new methods over existing state-of-the-art techniques.		
cited_context	DND-SRGB IMAGES	https://doi.org/10.1109/TNNLS.2021.3131739 (2020)	https://doi.org/10.1109/CVPR.2019.00181 (2018)	The DND-SRGB IMAGES dataset is used to evaluate image restoration algorithms, focusing on denoising performance. It contains real photographs and is employed to measure PSNR improvements over existing methods, serving as a benchmark for algorithmic advancements in image restoration.		
cited_context	Doc3D	https://doi.org/10.1109/CVPR52733.2024.01482 (2024)	https://doi.org/10.48550/arXiv.2210.08161 (2022)	The Doc3D dataset is primarily used for training models in document image rectification, focusing on correcting geometric distortions in both synthetic and real document images. This dataset provides a large, diverse set of images, enabling researchers to develop and evaluate algorithms that improve the quality and readability of distorted document images.		
cited_context	Doc3DShade	https://doi.org/10.1109/CVPR52733.2024.01482 (2024)	https://doi.org/10.5244/c.34.188 (2020)	The Doc3DShade dataset is used to train models for image restoration, specifically enhancing the model's ability to handle various shading conditions. It provides 90K synthetic images, which are crucial for improving the robustness and performance of image restoration algorithms under different lighting scenarios. This dataset enables researchers to develop more effective and versatile image restoration techniques.		
cited_context	DocUNet	https://doi.org/10.1109/CVPR52733.2024.01482 (2024)	https://doi.org/10.1109/CVPR.2018.00494 (2018)	The DocUNet dataset is used for evaluating document image unwarping, specifically providing 130 images for assessment. It enables researchers to test and compare the performance of unwarping algorithms, focusing on the accuracy and effectiveness of restoring distorted document images. The dataset's curated images facilitate rigorous evaluation in this domain.		
cited_context | citing_context	Dpdd	https://doi.org/10.48550/arXiv.2506.16960 (2025), https://doi.org/10.1109/TCSVT.2024.3429557 (2024), https://doi.org/10.1109/TIP.2025.3572788 (2025) (+2), https://doi.org/10.1109/TCSVT.2024.3429557 (2024), https://doi.org/10.24963/ijcai.2024/80 (2024), https://doi.org/10.1109/TPAMI.2023.3330416 (2023) (+4)	https://doi.org/10.1007/978-3-030-58607-2_7 (2020)	The DPDD dataset is primarily used for defocus deblurring research, focusing on restoring sharp images from out-of-focus regions. It is utilized for training and evaluating single-image and dual-pixel defocus deblurring methods, particularly in high-resolution image restoration. The dataset enables researchers to assess the performance of their models, compare PSNR gains, and optimize parameter efficiency. Dual-pixel data is a key feature, enhancing the effectiveness of defocus deblurring algorithms.; The DPDD dataset is primarily used to evaluate and compare single-image defocus deblurring methods, focusing on performance metrics such as PSNR, MAE, and LPIPS. It is employed to assess the trade-offs between model complexity (number of parameters) and image quality, as well as to verify the effectiveness of proposed algorithms against established methods. The dataset supports both training and evaluation, particularly for high-resolution image restoration tasks.	DPDD	
cited_context	DPDD dataset	https://doi.org/10.1007/978-3-031-20071-7_4 (2021)	https://doi.org/10.1007/978-3-030-58607-2_7 (2020)	The DPDD dataset is used for qualitative evaluation of the TLC method in dual-pixel defocus deblurring, specifically focusing on image restoration quality and performance. It enables researchers to assess and compare the effectiveness of defocus deblurring techniques, providing a benchmark for evaluating restoration algorithms.		
cited_context	DPDD testset	https://doi.org/10.1007/978-3-031-20071-7_4 (2021)	https://doi.org/10.1007/978-3-030-58607-2_7 (2020)	The DPDD testset is used to evaluate defocus deblurring methods, focusing on performance comparisons across 37 indoor and 39 outdoor scenes. This dataset enables researchers to assess the effectiveness of various deblurring techniques in diverse environmental conditions, providing a standardized benchmark for methodological advancements in image restoration.		
cited_context | citing_context	Drealsr	https://doi.org/10.48550/arXiv.2503.09403 (2025), https://doi.org/10.48550/arXiv.2308.15070 (2023), https://doi.org/10.48550/arXiv.2308.09388 (2023)	https://doi.org/10.1109/CVPR.2018.00182 (2018), https://doi.org/10.1109/ICCV.2019.00318 (2019)	The DRealSR dataset is mentioned in the citation context but lacks detailed descriptions of its usage in specific research studies. Therefore, there is no concrete evidence to describe its application, methodology, research questions, or enabling capabilities in any particular research area.; DRealSR is used for evaluating image super-resolution performance, particularly in real-world and synthetic contexts. It focuses on enhancing image details and assessing restoration quality. The dataset includes real-world images captured with DSLR cameras and various lenses, enabling researchers to quantitatively evaluate and compare single image super-resolution models.	DRealSR	
citing_context	DRealSR-Val	https://doi.org/10.48550/arXiv.2503.09403 (2025)	https://doi.org/10.1109/CVPR.2018.00182 (2018)	The DRealSR-Val dataset is used for validating image super-resolution models, specifically focusing on real-world low-resolution and high-resolution image pairs. It serves as a benchmark to evaluate the performance of these models in practical scenarios, ensuring they can effectively enhance image quality in real-world conditions. The dataset's emphasis on real-world images makes it particularly relevant for assessing the robustness and generalizability of super-resolution techniques.		
cited_context	DVD	https://doi.org/10.1109/ICCV.2019.00897 (2019)	https://doi.org/10.1109/CVPR.2017.33 (2017)	The DVD dataset is used in image restoration research, specifically for evaluating and training deblurring models. It consists of 600 images, primarily focusing on motion blur correction. Researchers select every second frame to diversify the training data and prevent overfitting, enhancing the robustness of deblurring techniques. This dataset enables the development and assessment of advanced image restoration methods by providing a diverse set of blurred images.		
cited_context	DVD dataset	https://doi.org/10.1109/ICCV.2019.00897 (2019)	https://doi.org/10.1109/CVPR.2017.33 (2017)	The DVD dataset is used in deep video deblurring research, focusing on real-world videos captured at 240 fps by various devices. It enables researchers to develop and test algorithms that enhance video clarity, addressing issues like motion blur. The dataset's high frame rate and diverse device sources provide a robust foundation for evaluating deblurring techniques in realistic scenarios.		
cited_context	DVD testing set	https://doi.org/10.1109/ICCV.2019.00897 (2019)	https://doi.org/10.1109/CVPR.2017.33 (2017)	The DVD testing set is used to evaluate the performance of DeblurGAN-v2 in deblurring tasks, specifically in a single-frame setting where each frame is treated as an individual image. This dataset enables researchers to assess the effectiveness of deblurring algorithms by providing a standardized set of images for benchmarking.		
cited_context | citing_context	Dynet	https://doi.org/10.48550/arXiv.2410.15067 (2024)	https://doi.org/10.48550/arXiv.2401.13221 (2024)	The DyNet dataset is used to evaluate and assess dynamic network architectures and techniques for image restoration, specifically focusing on their adaptability to various image degradations. This dataset enables researchers to test and compare the performance of dynamic networks in handling different types of image restoration challenges, ensuring robustness and versatility in restoration methods.; The DyNet dataset is used for dynamic network training and testing, specifically focusing on adaptive and real-time image restoration techniques. It enables researchers to optimize restoration models for various conditions, assessing the performance of dynamic image restoration methods in practical applications. The dataset supports the development of more flexible and responsive image restoration algorithms.	DyNet	
cited_context	Emu Edit Test	https://doi.org/10.48550/arXiv.2409.15278 (2024)	https://doi.org/10.48550/arXiv.2306.10012 (2023)	The 'Emu Edit Test' dataset is used to assess the performance of PixWizard in precise image editing tasks. It combines recognition and generation to evaluate the precision and coherence of editing outcomes. This dataset enables researchers to test and refine algorithms for image editing, focusing on the accuracy and contextual consistency of the edits.		
cited_context | citing_context	Enhancing Under-Water Visual Perception (Euvp)	https://doi.org/10.48550/arXiv.2310.10123 (2023)	https://doi.org/10.1109/CVPR46437.2021.00906 (2020)	The EUVP dataset is used to evaluate techniques for reducing underexposure and blur artifacts in underwater images, enhancing visual perception. Researchers employ this dataset to test and validate methods that improve image clarity and quality, focusing on specific challenges like low light and distortion. This enables more accurate and reliable underwater imaging for various applications.; The EUVP dataset is used to evaluate techniques for reducing underexposure and blur artifacts in underwater images, enhancing visual perception. Researchers employ this dataset to test and compare methods that improve image clarity and quality, focusing on specific visual perception challenges in underwater environments. This dataset enables rigorous assessment of image restoration algorithms tailored for underwater conditions.	Enhancing Under-water Visual Perception (EUVP)	
cited_context | citing_context	Enhancing Underwater Visual Perception (Euvp) Dataset	https://doi.org/10.48550/arXiv.2310.10123 (2023)	https://doi.org/10.48550/arXiv.2309.06380 (2023)	The EUVP dataset is used to evaluate image restoration methods on underwater images, focusing on issues such as color correction and clarity improvement. Researchers employ this dataset to test and validate their algorithms, ensuring they effectively enhance visual perception in underwater environments. The dataset's relevance lies in its ability to provide realistic challenges for image restoration techniques, making it a valuable resource for advancing underwater visual perception technologies.; The EUVP dataset is used to evaluate image restoration methods on underwater images, focusing on issues such as color correction and clarity improvement. Researchers employ this dataset to test and validate their algorithms, ensuring they effectively enhance visual perception in underwater environments. The dataset's relevance lies in its ability to provide realistic challenges for image restoration techniques, making it a valuable resource for advancing underwater visual perception.	Enhancing Underwater Visual Perception (EUVP) dataset	
cited_context | citing_context	Euvp	https://doi.org/10.3390/electronics13142817 (2024), https://doi.org/10.48550/arXiv.2506.16960 (2025), https://doi.org/10.48550/arXiv.2503.10120 (2025)	https://doi.org/10.1109/LRA.2020.2974710 (2019)	The EUVP dataset is primarily used for evaluating and comparing underwater image enhancement techniques, focusing on visual perception improvements. It consists of 900 pairs of degraded and clean images for training and 100 pairs for testing, enabling researchers to assess the performance of various enhancement methods and restoration algorithms. The dataset supports experiments aimed at improving visual perception in underwater images, using specific restoration techniques and models.; The EUVP dataset is used for qualitative comparisons in underwater image enhancement research. It provides visual results to demonstrate improvements in visual perception, enabling researchers to evaluate and compare the effectiveness of different enhancement techniques. This dataset supports the development and assessment of algorithms aimed at improving the clarity and quality of underwater images.	EUVP	
cited_context | citing_context	Euvp-330	https://doi.org/10.48550/arXiv.2501.12981 (2025)	https://doi.org/10.1109/TCSVT.2019.2963772 (2019)	The EUVP-330 dataset is used for testing and comparing underwater image restoration algorithms, particularly focusing on enhancing images under natural light conditions. It evaluates the robustness of these algorithms in diverse lighting scenarios and assesses their ability to reduce color casts and improve visual clarity. This dataset enables researchers to visually compare restoration results and validate the performance of models like NU2Net.; The EUVP-330 dataset is used for evaluating and comparing underwater image restoration techniques, particularly focusing on enhancing structural details and reducing color casts under natural light conditions. It is employed to test the robustness of restoration methods, such as NU2Net, by assessing their performance in improving visual clarity in natural light environments. This dataset enables researchers to visually compare restoration results and validate the effectiveness of different algorithms in realistic underwater settings.	EUVP-330	
cited_context	ExDark	https://doi.org/10.1109/TIP.2021.3051462 (2019)	https://doi.org/10.1109/ICCV.2017.511 (2017)	The ExDark dataset is used to investigate the impact of light enhancement on extremely dark images, focusing on improving subsequent high-level vision tasks. Researchers employ this dataset to evaluate and develop methods for enhancing image quality under low-light conditions, which is crucial for applications such as surveillance and nighttime photography. The dataset's characteristic low-light images enable the assessment of light enhancement techniques and their effectiveness in supporting advanced vision tasks.		
cited_context	exposure correction dataset	https://doi.org/10.1109/CVPR52729.2023.01350 (2022)	https://www.semanticscholar.org/paper/5ee57b18a7be2b671ffc6f5ce5dbbe92112ee0fb (2007)	The exposure correction dataset, derived from the MIT-Adobe FiveK dataset, is used to evaluate the ShadowDiffusion model for improving image quality through exposure correction. Researchers focus on assessing the model's performance in enhancing images, specifically addressing issues related to shadow and exposure. This publicly available dataset enables rigorous evaluation and comparison of image restoration techniques.		
cited_context	FFHQ	https://doi.org/10.1109/CVPRW59228.2023.00178 (2023), https://doi.org/10.1109/ICCV51070.2023.00495 (2023), https://www.semanticscholar.org/paper/3d3c5fcbc40aadccceda58d3d9c5cd00588ea0b7 (2022)	https://doi.org/10.1109/CVPRW.2017.150 (2017)	The FFHQ dataset is used in research for training, testing, and evaluating image restoration models, specifically focusing on high-quality face images. It enables researchers to enhance facial details, assess the quality and realism of restored faces, and evaluate texture recovery in high-resolution images. The dataset's high-quality and high-resolution characteristics are crucial for these applications.		
cited_context	FFHQ-raw	https://doi.org/10.1109/CVPR52733.2024.02425 (2024)	https://doi.org/10.1109/CVPR.2019.00453 (2018)	The FFHQ-raw dataset is used to enhance face restoration performance by providing 70,000 unaligned high-resolution facial images for training models. This dataset specifically supports research in improving the quality and accuracy of face restoration techniques, enabling models to better handle diverse and complex facial images.		
cited_context | citing_context	Fivek	https://doi.org/10.48550/arXiv.2406.18242 (2024), https://doi.org/10.48550/arXiv.2306.13653 (2023)	https://doi.org/10.1109/TIP.2018.2806202 (2017), https://www.semanticscholar.org/paper/5ee57b18a7be2b671ffc6f5ce5dbbe92112ee0fb (2007)	The FiveK dataset is primarily used for low-light enhancement research, where it tests and evaluates the ability of algorithms to improve image quality in low-light conditions. It contains a diverse set of images with professional retouching, enabling researchers to assess the effectiveness of various image enhancement methods. This dataset facilitates the development and comparison of low-light enhancement techniques by providing a standardized benchmark.; The FiveK dataset is primarily used for low-light image enhancement research. It employs the ProRes framework to improve image quality under low-light conditions. This dataset enables researchers to develop and test algorithms that enhance visual clarity and detail in images captured in dim environments, focusing on practical applications such as photography and surveillance.	FiveK	
cited_context	Flick2K	https://doi.org/10.48550/arXiv.2306.05390 (2023), https://doi.org/10.48550/arXiv.2308.09388 (2023)	https://doi.org/10.1109/CVPRW.2017.150 (2017)	The Flick2K dataset is primarily used for single image super-resolution, particularly focusing on enhancing the quality and detail of outdoor natural images. It is employed in training models using traditional image super-resolution techniques, with a specific emphasis on bicubic downsampling. This dataset enables researchers to develop and evaluate algorithms that improve image resolution and clarity.		
cited_context	Flick2K dataset	https://doi.org/10.1109/TPAMI.2021.3088914 (2020)	https://doi.org/10.1109/CVPRW.2017.150 (2017)	The Flick2K dataset is used to expand the variety and quantity of images for image restoration tasks. It supplies 2,750 images, enhancing the diversity of the dataset. This expansion supports research in image restoration by providing a larger, more varied set of images, which can improve the robustness and generalizability of restoration models.		
cited_context	Flicke2K	https://doi.org/10.48550/arXiv.2210.01427 (2022)	https://doi.org/10.1109/ICCV.2013.241 (2013)	The Flicke2K dataset is primarily used for training models focused on image super-resolution, specifically enhancing image quality at a 2x scale. It is employed to generate high-resolution images from low-resolution inputs, enabling research in improving visual clarity and detail in digital images. The dataset's focus on 2x scaling is a key characteristic, supporting methodologies aimed at efficient and effective image enhancement.		
cited_context	Flickr1024	https://doi.org/10.1109/CVPRW59228.2023.00169 (2023)		The Flickr1024 dataset is primarily used for validating stereo images in the NTIRE 2023 challenge, focusing on evaluating model performance. It provides a standardized set of images to assess the effectiveness of image restoration techniques, ensuring consistent and comparable results across different models and approaches.		
cited_context | citing_context	Flickr2K	https://doi.org/10.48550/arXiv.2310.10123 (2023), https://doi.org/10.48550/arXiv.2412.20157 (2024), https://doi.org/10.48550/arXiv.2404.02154 (2024), https://doi.org/10.48550/arXiv.2306.05390 (2023), https://doi.org/10.48550/arXiv.2210.01427 (2022), https://doi.org/10.48550/arXiv.2402.15648 (2024) (+10)	https://doi.org/10.1109/CVPRW.2017.151 (2017), https://doi.org/10.1109/CVPRW.2017.150 (2017)	The Flickr2K dataset is primarily used for image restoration tasks, including Gaussian color image denoising, compression, and super-resolution. It provides high-quality, high-resolution natural images, which are essential for training and evaluating models like AutoDIR. The dataset complements other datasets like DIV2K, offering diverse images for comprehensive model training, particularly in single-degradation scenarios and at various downsampled scales (x8 and x4).; The Flickr2K dataset is primarily used for training and enhancing image restoration models, particularly in single image super-resolution and image denoising. It provides a large, diverse set of high-quality images that improve model robustness and generalization. The dataset supports various methodologies, including training models for specific tasks like Gaussian denoising and enhancing image quality at different scales (x4 and x8). Its extensive and varied image content enables researchers to develop more effective and versatile image restoration techniques.	Flickr2K	
cited_context | citing_context	Flikr2K	https://doi.org/10.48550/arXiv.2404.02154 (2024)	https://doi.org/10.1109/CVPRW53098.2021.00074 (2021)	The Flikr2K dataset is used for training and evaluating image restoration models, focusing on diverse content and real-world image challenges. It enables researchers to test the robustness and versatility of their models by providing a wide range of image types and conditions, enhancing the reliability of image restoration techniques.; The Flikr2K dataset is used for training and evaluating image restoration models, focusing on diverse content and real-world scenarios. It enables researchers to test the robustness and generalizability of their models by providing a wide range of images, enhancing the reliability of image restoration techniques.	Flikr2K	
citing_context	FoggyCityscape	https://doi.org/10.1109/TMM.2024.3377136 (2024)	https://doi.org/10.1007/s11263-018-1072-8 (2017)	The FoggyCityscape dataset is used to evaluate image restoration techniques, particularly focusing on images with synthetic fog and noise. Researchers employ this dataset to assess the effectiveness of restoration methods in challenging atmospheric conditions, ensuring that algorithms can enhance image clarity and reduce noise. This dataset enables rigorous testing and comparison of different restoration approaches, contributing to advancements in image processing under foggy conditions.		
cited_context	FSDSRD	https://doi.org/10.1109/CVPR52733.2024.01482 (2024)	https://doi.org/10.1109/ICIP46576.2022.9897217 (2022)	The FSDSRD dataset is used to train models for document shadow removal, specifically employing 14,200 synthetic images. This dataset enables researchers to develop and evaluate algorithms that effectively remove shadows from documents, enhancing image quality and readability. The large number of synthetic images supports robust training and testing of these models.		
cited_context	GAN-based distortion subset	https://doi.org/10.1007/978-3-030-58621-8_37 (2020)	https://www.semanticscholar.org/paper/8c92054c26fb4c6dd7435bc99fbb8af3323eae1b (2019)	The GAN-based distortion subset is used to evaluate the impact of anti-aliasing pooling layers on GAN-generated images, focusing on distortion effects and synthetic artifacts. This dataset enhances the assessment of robustness in GAN-generated images, specifically addressing how anti-aliasing techniques improve image quality and reduce distortions.		
cited_context	GaoFen2	https://www.semanticscholar.org/paper/4b688c486f2b223212682fd2b3f72cc0caa94b1b (2023)	https://doi.org/10.1007/978-3-031-19800-7_10 (2022)	The GaoFen2 dataset is used for evaluating image restoration techniques, particularly in enhancing the clarity and detail of high-resolution satellite imagery. Researchers employ this dataset to test and validate their methods, focusing on improving image quality in satellite applications. The dataset's high-resolution characteristics make it suitable for assessing the effectiveness of restoration algorithms in real-world scenarios.		
citing_context	GenDS	https://doi.org/10.48550/arXiv.2411.17687 (2024)	https://www.semanticscholar.org/paper/20d4e85814f64977590bc9276eec84203f0fcf9b (2023)	The GenDS dataset is used to train and evaluate models for all-in-one image restoration, focusing on improving image quality across various degradation types. It is employed to train and compare models such as NAFNet, PromptIR, and Swin Transformer, assessing their performance and effectiveness in comprehensive restoration tasks. The dataset enables researchers to evaluate the impact of the data on model accuracy and to compare different models' capabilities in handling diverse image degradations.		
cited_context	General100	https://doi.org/10.1109/ICCV51070.2023.01204 (2023), https://doi.org/10.48550/arXiv.2310.10513 (2023)	https://doi.org/10.1007/978-3-642-27413-8_47 (2010)	The General100 dataset is used to evaluate GAN-based and other image restoration methods, focusing on performance metrics such as PSNR and LPIPS to assess image quality and perceptual similarity. It covers a wide range of image types and degradation scenarios, enabling researchers to test and compare high-quality image restoration techniques.		
citing_context	Google Landmarks dataset	https://doi.org/10.48550/arXiv.2411.18412 (2024)	https://doi.org/10.1109/CVPRW.2019.00251 (2019)	The Google Landmarks dataset is used to train and evaluate image restoration models, particularly focusing on high-quality images with NIMA scores exceeding 4.90 and resolutions greater than 400 pixels. This dataset provides a diverse set of real-world scenes, enabling researchers to test and improve the performance of their models in realistic conditions.		
cited_context | citing_context	Gopro	https://doi.org/10.48550/arXiv.2410.15067 (2024), https://doi.org/10.48550/arXiv.2411.18412 (2024), https://doi.org/10.48550/arXiv.2506.16960 (2025) (+25), https://doi.org/10.48550/arXiv.2410.15067 (2024), https://doi.org/10.1007/978-3-031-20071-7_4 (2021), https://doi.org/10.1109/CVPRW53098.2021.00027 (2021) (+32)	https://doi.org/10.1109/CVPR.2017.35 (2016)	The GoPro dataset is primarily used for motion deblurring research, focusing on both synthetic and real-world motion blur in images and video sequences. It is employed to train and evaluate deep learning models, particularly multi-scale convolutional neural networks, to improve image clarity and high-resolution quality in dynamic scenes. The dataset includes motion-blurred images and their corresponding sharp versions, enabling researchers to address camera and object motion blur effectively.; The GoPro dataset is primarily used for training and testing image deblurring models, particularly focusing on motion blur. It consists of paired blurred and sharp images, enabling researchers to develop and evaluate algorithms that restore image clarity. The dataset is widely used in the context of dynamic scene deblurring, providing a benchmark for assessing restoration quality under various motion blur conditions. It supports the development of deep learning models, such as multi-scale convolutional neural networks, and is crucial for improving image clarity in both static and video sequences.	GoPro	
cited_context	GoPro dataset	https://doi.org/10.1007/978-3-031-20071-7_4 (2021), https://doi.org/10.48550/arXiv.2310.10123 (2023), https://doi.org/10.48550/arXiv.2301.11699 (2023) (+11)	https://doi.org/10.1109/CVPR.2017.35 (2016)	The GoPro dataset is primarily used for evaluating and training deblurring algorithms, particularly in dynamic scenes with camera shake and motion blur. It features 2103 blurry/sharp image pairs for training and 1111 pairs for evaluation. Researchers use it to assess the performance of various models, including multi-scale convolutional neural networks, in tasks such as single-image and video deblurring, focusing on quantitative metrics like PSNR and visual quality improvements.		
cited_context	GoPro test dataset	https://doi.org/10.48550/arXiv.2208.05244 (2022)	https://doi.org/10.1109/CVPR.2017.35 (2016)	The GoPro test dataset is used to evaluate and compare the performance of deblurring methods, particularly in dynamic scenes. It focuses on assessing quantitative metrics and visual quality, enabling researchers to measure image restoration accuracy and overall performance against state-of-the-art techniques. This dataset supports the development and validation of advanced deblurring algorithms by providing a standardized benchmark for image quality and restoration.		
citing_context	GoPro test set	https://doi.org/10.48550/arXiv.2310.10123 (2023), https://doi.org/10.48550/arXiv.2405.15475 (2024), https://doi.org/10.1007/978-3-031-72764-1_1 (2024) (+1)	https://doi.org/10.1109/CVPR.2017.35 (2016)	The GoPro test set is primarily used for evaluating and comparing the performance of image restoration methods, particularly in motion deblurring. It consists of 2103 training and 1111 testing images, enabling researchers to assess the effectiveness of algorithms in removing motion blur and handling noise (σ=25) in dynamic scenes. This dataset facilitates the development and benchmarking of deep learning models, such as multi-scale convolutional neural networks, for high-resolution image restoration tasks.		
cited_context	GoPro testset	https://www.semanticscholar.org/paper/6af728164e4424218967fd356b3448f5037acd88 (2023)	https://doi.org/10.1109/CVPR.2017.35 (2016)	The GoPro testset is used to evaluate the computational costs and performance metrics of motion deblurring methods in dynamic scenes. It focuses on assessing the efficiency of various deblurring techniques, enabling researchers to compare and optimize algorithms for real-world applications. The dataset's characteristics support rigorous testing of these methods under realistic conditions.		
cited_context	GoPro-test	https://doi.org/10.1109/CVPR.2019.00717 (2019)	https://doi.org/10.1109/CVPR.2017.35 (2016)	The GoPro-test dataset is used to evaluate the performance of the DuRNU model for dynamic scene deblurring. It facilitates comparisons against state-of-the-art methods, focusing on the effectiveness of deblurring techniques in dynamic scenes. This dataset enables researchers to assess and validate the model's capabilities in enhancing image clarity in motion-blurred images.		
cited_context | citing_context	Gta5	https://doi.org/10.1109/TCSVT.2024.3429557 (2024), https://doi.org/10.1109/TIP.2025.3572788 (2025), https://doi.org/10.1109/TPAMI.2024.3419007 (2024), https://doi.org/10.1109/TCSVT.2024.3429557 (2024), https://doi.org/10.24963/ijcai.2024/80 (2024)	https://doi.org/10.1145/3394171.3413763 (2020), https://doi.org/10.1007/978-3-030-58610-2_28 (2020)	The GTA5 dataset is primarily used for dehazing research, particularly in synthetic nighttime dehazing tasks. It provides synthetic nighttime hazy images that are utilized for both training and evaluating methods. The dataset's focus on complex lighting conditions and urban scenes enables researchers to test the robustness of dehazing algorithms in challenging environments.; The GTA5 dataset is primarily used in research for evaluating nighttime dehazing algorithms. It provides synthetic nighttime hazy images, which are utilized to test and assess the performance of image restoration methods, particularly in simulated urban environments. Researchers employ this dataset to demonstrate improvements in dehazing results and overall image quality, focusing on synthetic benchmark performance.	GTA5	
cited_context	H-LSS	https://doi.org/10.1109/TGRS.2024.3378828 (2024)	https://doi.org/10.1109/LGRS.2017.2771212 (2017)	The H-LSS dataset is primarily used for hyperspectral image reconstruction, employing methodologies such as compressive sensing and tensor decomposition. This dataset enables researchers to address challenges in efficiently reconstructing high-dimensional hyperspectral images from compressed measurements, enhancing the quality and resolution of the reconstructed images. The dataset's characteristics support advanced signal processing techniques, making it valuable for improving the accuracy and efficiency of hyperspectral imaging applications.		
cited_context | citing_context	Hair	https://doi.org/10.48550/arXiv.2410.15067 (2024)	https://doi.org/10.48550/arXiv.2401.13221 (2024)	The HAIR dataset is used to evaluate hybrid adaptive and hybrid image restoration techniques, focusing on the combination of multiple restoration approaches. It enables researchers to assess the effectiveness of integrating various methods in image restoration, enhancing the robustness and adaptability of restoration algorithms.; The HAIR dataset is used for high-quality image restoration, with a focus on preserving fine details and textures in high-resolution images. It is employed to test both single and hybrid restoration techniques, enabling researchers to evaluate and combine multiple methods to achieve superior image restoration outcomes.	HAIR	
cited_context	Haze4K	https://doi.org/10.48550/arXiv.2305.17863 (2023), https://doi.org/10.1109/TPAMI.2024.3419007 (2024), https://doi.org/10.48550/arXiv.2308.09388 (2023) (+1)	https://doi.org/10.1145/3474085.3475331 (2021)	The Haze4K dataset is primarily used for evaluating and comparing dehazing algorithms, particularly in high-resolution synthetic and real-world images. It contains 4,000 hazy images, enabling researchers to assess the effectiveness, performance, and robustness of dehazing techniques in both daytime and nighttime conditions. The dataset supports visual comparisons and benchmarking, focusing on ultra-high-definition images and realistic synthetic haze conditions.		
cited_context	HDR dataset from the NTIRE2021 Multi-Frame HDR Challenge	https://doi.org/10.1109/CVPR52729.2023.00958 (2023)	https://doi.org/10.1109/TPAMI.2021.3126387 (2021)	The HDR dataset from the NTIRE2021 Multi-Frame HDR Challenge is used to evaluate HDR recovery methods, particularly for comparing HDR-GDP-x0 with other state-of-the-art HDR techniques. It involves testing on 100 randomly selected scenes, enabling researchers to assess the performance and effectiveness of different HDR algorithms in recovering high dynamic range images from multiple low dynamic range frames.		
cited_context | citing_context	Hide	https://doi.org/10.48550/arXiv.2410.15385 (2024), https://doi.org/10.48550/arXiv.2410.15067 (2024), https://doi.org/10.1109/CVPR52733.2024.00277 (2023) (+4), https://www.semanticscholar.org/paper/6af728164e4424218967fd356b3448f5037acd88 (2023), https://doi.org/10.1007/978-3-031-20071-7_4 (2021), https://doi.org/10.1109/TPAMI.2023.3330416 (2023) (+14)	https://doi.org/10.1109/ICCV.2019.00567 (2019)	The HIDE dataset is used for evaluating and testing image restoration methods, particularly in high-resolution image restoration tasks. It assesses performance across diverse image degradation scenarios, including real-world motion blur, haze, and noise. The dataset supports research on deblurring, haze removal, and generalization in image restoration, featuring synthetic and real distortions. It enables researchers to validate the effectiveness of their algorithms in handling complex and varied image degradations.; The HIDE dataset is primarily used for evaluating and comparing image restoration models, particularly in the areas of deblurring and denoising. It features a large collection of high-resolution images with varying degrees of blur and noise, enabling researchers to assess model performance on diverse and challenging real-world scenarios. The dataset supports the evaluation of techniques such as DiffIR, human-aware motion deblurring, and the Test-time Local Converter, focusing on robustness, generalization, and reducing train-test inconsistencies. It also facilitates numerical comparisons and performance metrics in dynamic scene deblurring and synthetic image deblurring, providing realistic blur synthesis for both training and testing.	HIDE	
cited_context	HMDD	https://doi.org/10.1109/CVPRW56347.2022.00069 (2022)	https://doi.org/10.1109/CVPR.2018.00259 (2018)	The HMDD dataset is utilized in image restoration research to integrate multiple distortion scenarios, providing a comprehensive resource that generalizes previous multi-degraded datasets. This integration allows researchers to address a broader range of image degradation issues, enhancing the robustness and versatility of image restoration models. The dataset's diverse and inclusive nature supports the development of more generalized and effective restoration algorithms.		
citing_context	Houston University	https://doi.org/10.48550/arXiv.2503.09131 (2025)		The Houston University dataset is used to evaluate all-in-one image restoration methods, particularly focusing on the effectiveness of partitioning strategies in urban areas. This dataset enables researchers to assess how well these methods perform in complex urban environments, providing insights into the practical applicability of image restoration techniques.		
cited_context	HQ-50K	https://doi.org/10.48550/arXiv.2306.05390 (2023)	https://doi.org/10.1109/CVPRW.2017.150 (2017)	The HQ-50K dataset is primarily used for high-quality image restoration tasks. It is utilized to compare the dataset's size and resolution with other datasets, emphasizing its suitability for image restoration. Researchers use it to train and evaluate models, focusing on performance metrics and comparing results with benchmarks like DIV2K. The dataset's large scale and high resolution are key characteristics that enable these comparisons and evaluations.		
cited_context | citing_context	Hsts	https://doi.org/10.1109/TCSVT.2023.3299324 (2024), https://doi.org/10.1109/TCSVT.2023.3299324 (2024), https://www.semanticscholar.org/paper/0c291c421fb11b5b27e2d3526efb2353d88c27a8 (2023)	https://doi.org/10.1109/CVPR.2018.00079 (2018), https://doi.org/10.1016/j.patrec.2018.01.010 (2018)	The HSTS dataset is used for training and evaluating haze removal algorithms, specifically focusing on synthetic hazy images with varying conditions. This dataset enables researchers to test the robustness and effectiveness of their algorithms across different haze scenarios, contributing to advancements in image dehazing techniques.; The HSTS dataset is used for training and evaluating image restoration algorithms, particularly for deraining and dehazing. It includes high-quality synthetic rain images and a mix of synthetic and real-world hazy images, enabling researchers to benchmark and compare the performance of different methods in both controlled and real-world scenarios.	HSTS	
cited_context	https://github.com/rwenqi/GFNdehazing	https://doi.org/10.1109/CVPR.2019.00717 (2019)	https://doi.org/10.1109/CVPR.2018.00343 (2018)	The dataset from 'https://github.com/rwenqi/GFNdehazing' is used to provide images for dehazing experiments. Specifically, these images are fed into a convolutional encoder-decoder network to estimate the ground truth image. This approach helps researchers evaluate and improve dehazing algorithms by providing a standardized set of images for testing and validation.		
citing_context	Hybrid Subjective Testing Set (HSTS)	https://doi.org/10.1109/TCSVT.2024.3519352 (2025)	https://doi.org/10.1109/TIP.2018.2867951 (2017)	The Hybrid Subjective Testing Set (HSTS) is used for subjective evaluation of dehazing results, integrating both synthetic and real-world images to assess visual quality. This dataset enables researchers to evaluate and compare the effectiveness of dehazing algorithms by providing a diverse set of images that reflect various real-world conditions and synthetic scenarios.		
cited_context	HYDICE DC Mall data	https://doi.org/10.1109/ICCVW.2019.00477 (2019)	https://doi.org/10.1109/JSTARS.2014.2329322 (2014)	The HYDICE DC Mall data is used to evaluate the performance of denoising algorithms by adding synthetic Gaussian noise (σ = 100) and assessing the algorithm's effectiveness in removing it. This dataset enables researchers to test and validate denoising methodologies, focusing on the improvement of image quality in noisy conditions.		
cited_context	Hypersim	https://doi.org/10.48550/arXiv.2409.15278 (2024)	https://www.semanticscholar.org/paper/c6e57b2d36a4231cef82d7e0e27413ae4bb4dee5 (2020)	The Hypersim dataset is used for holistic indoor scene understanding, leveraging photorealistic synthetic data to train and evaluate image restoration models. It provides a rich set of annotations and high-quality images, enabling researchers to address complex challenges in scene understanding and image restoration within indoor environments.		
cited_context | citing_context	I-Haze	https://doi.org/10.48550/arXiv.2503.09403 (2025), https://doi.org/10.1109/TCSVT.2024.3429557 (2024), https://doi.org/10.1109/TCSVT.2024.3519352 (2025), https://doi.org/10.1109/TPAMI.2024.3419007 (2024), https://doi.org/10.1109/TCSVT.2024.3429557 (2024)	https://doi.org/10.1109/CVPRW.2018.00119 (2018), https://doi.org/10.1007/978-3-030-01449-0_52 (2018)	The I-Haze dataset is used for evaluating dehazing algorithms by providing 10 paired low-quality and high-quality images, as well as real-world hazy and haze-free indoor images. It is employed in dehazing experiments to test the robustness and performance of dehazing algorithms, particularly in the context of all-in-one image restoration models. The dataset's real-world images enable researchers to assess the effectiveness of their methods in practical scenarios.; The I-Haze dataset is used to test and benchmark dehazing algorithms on real indoor hazy and haze-free images. It serves as a critical resource for evaluating dehazing performance in indoor scenarios, providing researchers with a standardized set of real-world images to assess algorithm effectiveness. This dataset enables the comparison of different dehazing models, facilitating advancements in image restoration techniques for indoor environments.	I-Haze	
cited_context	ICB	https://doi.org/10.1109/CVPRW63382.2024.00645 (2024)	https://doi.org/10.1109/CVPRW.2017.151 (2017)	The ICB dataset is used to evaluate and benchmark image restoration and compression techniques. It focuses on assessing the performance of methods like PromptCIR on image restoration quality at various factors (10, 20, 30, 40) and examines the impact of compression on image restoration and artifact reduction. This dataset enables researchers to compare and refine algorithms for enhancing image quality and reducing artifacts in restored images.		
cited_context | citing_context	Imagenet	https://doi.org/10.48550/arXiv.2411.17687 (2024), https://doi.org/10.48550/arXiv.2412.00878 (2024), https://doi.org/10.1109/CVPR52729.2023.00958 (2023), https://doi.org/10.48550/arXiv.2306.02342 (2023) (+14)	https://doi.org/10.1109/CVPR.2009.5206848 (2009)	The ImageNet dataset is primarily used to pre-train transformer encoders, which enhances their feature extraction capabilities. This pre-training focuses on improving generalization for image restoration tasks. The dataset's large scale and diverse image content enable researchers to develop models that perform well across various restoration challenges, leveraging the rich feature representations learned from ImageNet.; ImageNet is used in research to evaluate and improve image restoration techniques, including super-resolution and deblurring. It serves as a pre-training dataset for models like diffusion models and vision transformers, enhancing their performance in image restoration tasks. The dataset's large-scale, diverse, and high-quality images provide a robust foundation for training and validating these models, enabling researchers to achieve better restoration outcomes and richer semantic representations.	ImageNet	
cited_context	ImageNet 1K	https://doi.org/10.48550/arXiv.2308.09388 (2023), https://www.semanticscholar.org/paper/3d3c5fcbc40aadccceda58d3d9c5cd00588ea0b7 (2022), https://doi.org/10.1109/CVPRW59228.2023.00178 (2023)	https://doi.org/10.1109/ICCV.2015.425 (2014)	The ImageNet 1K dataset is primarily used for evaluating and comparing image restoration techniques, particularly in super-resolution and deblurring tasks. It provides a large-scale, diverse set of images that enable researchers to assess model performance across various categories. Studies use it to test 4x super-resolution models, evaluate zero-shot methods, and compare different approaches like DGP, RED, and SNIPS, focusing on image quality and restoration performance without noise.		
cited_context	ImageNet ctest10k	https://doi.org/10.1145/3528233.3530757 (2021)	https://doi.org/10.1007/978-3-319-46493-0_35 (2016)	The ImageNet ctest10k dataset is primarily used for benchmarking the performance of various image restoration and translation models. It is employed to evaluate colorization tasks, comparing the Palette model against other methods, and to assess image-to-image translation tasks, focusing on model performance across different scenarios. Additionally, it is used to benchmark image inpainting models like DeepFillv2 and HiFill, particularly on ultra high-resolution images. This dataset serves as a standardized test set, enabling researchers to compare and validate the effectiveness of their models in specific image processing tasks.		
cited_context	ImageNet-50K	https://doi.org/10.48550/arXiv.2306.05390 (2023)		The ImageNet-50K dataset is used to evaluate the performance of DAMoE, specifically focusing on image restoration quality and efficiency. It serves as a benchmark to compare different methods, ensuring that the restoration processes are both effective and computationally efficient. The dataset's large scale and diverse image content enable robust testing and validation of image restoration techniques.		
citing_context	ImageNet-C	https://doi.org/10.48550/arXiv.2406.18242 (2024)	https://www.semanticscholar.org/paper/49b64383fe36268410c430352637ed23b16820c5 (2019)	ImageNet-C is used to benchmark the robustness of neural networks against common corruptions and perturbations, particularly during the pre-training stage. Researchers adjust the intensity of degradation in images to systematically evaluate how well models withstand various types of image corruption. This dataset enables the assessment and improvement of model resilience in real-world conditions.		
cited_context	ImageNet-Test	https://doi.org/10.1109/TPAMI.2024.3461721 (2024)	https://doi.org/10.1109/CVPR.2009.5206848 (2009)	The ImageNet-Test dataset, comprising 3000 randomly selected images from the validation set, is primarily used for synthetic testing in image restoration research. It serves as a benchmark to evaluate the performance of restoration algorithms under controlled conditions. This dataset enables researchers to assess the effectiveness of their methods by providing a standardized set of images for consistent testing and comparison.		
cited_context	ImageNet/CVF	https://doi.org/10.48550/arXiv.2310.10513 (2023)	https://doi.org/10.1109/CVPR.2009.5206848 (2009)	The ImageNet/CVF dataset is used to extract crude images for Masked Autoencoder (MAE) training and to stitch paired images for inference, specifically focusing on image restoration techniques. This dataset facilitates the development and evaluation of methods aimed at restoring degraded images, leveraging its extensive collection of visual data.		
cited_context	Indoor Training Set	https://doi.org/10.1145/3664647.3680762 (2024)	https://doi.org/10.1109/CVPR.2017.183 (2016)	The Indoor Training Set, part of the RESIDE dataset, is used for dehazing indoor scenes to enhance visibility and clarity. This dataset supports research focused on image dehazing techniques, employing methodologies aimed at improving the visual quality of indoor images by removing haze effects. It enables researchers to develop and evaluate algorithms that restore clear images from hazy ones, addressing specific challenges in indoor environments.		
cited_context | citing_context	Indoor Training Set (Its)	https://doi.org/10.1109/TCSVT.2024.3519352 (2025), https://www.semanticscholar.org/paper/4b688c486f2b223212682fd2b3f72cc0caa94b1b (2023)	https://doi.org/10.1109/TIP.2018.2867951 (2017)	The Indoor Training Set (ITS) is used for training dehazing models on indoor scenes. It provides a diverse set of synthetic images with known ground truth, enabling researchers to develop and evaluate dehazing algorithms effectively. The dataset's synthetic nature and ground truth data facilitate the assessment of model performance in various indoor conditions.; The Indoor Training Set (ITS) is used to train models for single-image dehazing, specifically containing 13,990 hazy indoor images generated from 1,399 clear images. This dataset enables researchers to develop and evaluate dehazing algorithms by providing a large and diverse set of synthetic hazy images, facilitating advancements in image restoration techniques.	Indoor Training Set (ITS)	
cited_context | citing_context	Instructir	https://doi.org/10.48550/arXiv.2410.15067 (2024)	https://doi.org/10.48550/arXiv.2401.13221 (2024)	The InstructIR dataset is used to assess and evaluate instruction-based image restoration methods, focusing on the ability to follow complex restoration commands. It enables researchers to test and compare different approaches in executing detailed restoration tasks, ensuring that the methodologies can accurately interpret and apply complex instructions to restore images.; The InstructIR dataset is used for instruction-based image restoration, enhancing images according to specific human instructions. It evaluates high-quality image restoration methods, focusing on diverse restoration tasks and techniques. This dataset enables researchers to assess and improve image restoration models guided by detailed instructions, ensuring more precise and contextually appropriate image enhancements.	InstructIR	
cited_context	IPT Dataset	https://doi.org/10.1007/978-3-031-20071-7_4 (2021)	https://doi.org/10.1109/CVPR46437.2021.01212 (2020)	The IPT Dataset is used for training and evaluating image processing transformers, specifically focusing on restoration tasks. It enables researchers to assess performance metrics, ensuring that models can effectively restore images. This dataset supports the development and refinement of image restoration techniques through rigorous evaluation.		
cited_context	ISTD	https://doi.org/10.48550/arXiv.2203.06074 (2022)	https://doi.org/10.1109/CVPR.2018.00182 (2018)	The ISTD dataset is primarily used for shadow removal in images, enhancing overall image quality by effectively reducing shadow effects. This dataset supports research focused on improving visual clarity and detail in images affected by shadows. The methodologies employed involve techniques to identify and mitigate shadow impacts, contributing to advancements in image restoration and enhancement.		
cited_context | citing_context	Its	https://doi.org/10.1109/TCSVT.2023.3299324 (2024)	https://doi.org/10.1109/CVPR.2018.00079 (2018)	The ITS dataset is primarily used for training and evaluating haze removal algorithms, specifically focusing on indoor synthetic hazy images. Researchers employ this dataset to develop and test algorithms that enhance image clarity by removing haze, addressing the challenge of improving visual quality in hazy conditions. The synthetic nature of the dataset allows for controlled experimentation and robust algorithm validation.; The ITS dataset is primarily used for training and evaluating image deraining algorithms, specifically focusing on indoor scenes with synthetic rain. It enables researchers to assess the performance of deraining techniques in controlled, synthetic environments, ensuring that the algorithms can effectively remove rain artifacts from images. This dataset supports the development and refinement of image restoration methods tailored to indoor settings.	ITS	
cited_context	ITS dataset	https://doi.org/10.1145/3664647.3681621 (2024)	https://doi.org/10.1109/TIP.2018.2867951 (2017)	The ITS dataset is primarily used for dehazing tasks in image restoration research. It provides images with varying levels of haze, enabling researchers to evaluate the performance of dehazing algorithms. The dataset's diverse haze conditions facilitate the testing and validation of restoration techniques, ensuring robustness across different scenarios.		
cited_context	JPEG Artifacts	https://doi.org/10.1109/CVPRW53098.2021.00027 (2021)	https://doi.org/10.1109/CVPRW53098.2021.00025 (2021)	The JPEG Artifacts dataset is used to evaluate image restoration methods, particularly in reducing JPEG compression artifacts. It features images with a PSNR of 29.70, which helps researchers assess the effectiveness of their restoration techniques. This dataset enables the comparison and validation of different methods aimed at improving image quality post-compression.		
citing_context	JRSRD	https://doi.org/10.1109/TCSVT.2024.3519352 (2025), https://doi.org/10.1109/TCSVT.2024.3516074 (2024)	https://doi.org/10.48550/arXiv.2212.11548 (2022)	The JRSRD dataset is used for restoring images degraded by rain and other real-world factors, enhancing performance in challenging environments. It is specifically leveraged to evaluate deraining performance, providing images with rain streaks and raindrops to test the effectiveness of all-in-one image restoration models. This dataset enables researchers to assess and improve the robustness of image restoration techniques in realistic conditions.		
cited_context	Jung’s dataset	https://doi.org/10.1109/CVPR52733.2024.01482 (2024)	https://doi.org/10.1109/CVPR.2018.00252 (2018)	Jung’s dataset is used as a testing set for image restoration, contributing 87 images to evaluate the performance of proposed methods. It is specifically employed to assess the effectiveness of restoration techniques, ensuring robust evaluation through its diverse image content. This dataset enables researchers to validate their models against a standardized set of images, facilitating comparative analysis and benchmarking.		
cited_context	Kligler’s dataset	https://doi.org/10.1109/CVPR52733.2024.01482 (2024)	https://doi.org/10.1109/CVPR.2018.00252 (2018)	Kligler’s dataset is used as part of the testing set for image restoration, providing 300 images to evaluate the performance of proposed methods. It contributes to assessing the effectiveness and robustness of image restoration techniques by offering a diverse set of images for validation.		
cited_context	Kodak	https://doi.org/10.1109/CVPR52729.2023.01753 (2023), https://doi.org/10.1109/TMM.2024.3407656 (2024)	https://doi.org/10.1109/TIP.2016.2574984 (2016)	The Kodak dataset is used in research for evaluating image denoising and restoration techniques. It serves as test data to assess the effectiveness of denoising methods on high-quality images and to compare the performance of various restoration models, including GRL-S, with a focus on PSNR scores. This dataset enables researchers to benchmark and validate their algorithms against a standardized set of images.		
cited_context | citing_context	Kodak24	https://doi.org/10.1109/TCSI.2024.3519532 (2025), https://doi.org/10.1109/CVPR52729.2023.00564 (2023), https://doi.org/10.48550/arXiv.2403.14614 (2024) (+2), https://doi.org/10.1007/s11263-023-01843-5 (2023), https://doi.org/10.48550/arXiv.2405.02843 (2024), https://doi.org/10.1109/CVPR52688.2022.00564 (2021) (+10)	https://doi.org/10.1109/ICCV.2001.937655 (2001)	The Kodak24 dataset is primarily used for image denoising and restoration research, serving as a benchmark for evaluating the effectiveness of various algorithms. It provides a standard set of 24 high-quality, diverse, and natural images, enabling researchers to assess the preservation of color and detail under different noise conditions. The dataset supports both visual and quantitative evaluations, facilitating comparisons between different denoising and restoration methods.; The Kodak24 dataset is primarily used for evaluating image denoising and restoration algorithms. It provides a standard set of high-quality images for both visual and quantitative assessments, focusing on color fidelity and detail preservation. Researchers use it to test and compare the effectiveness of their methods, often under controlled conditions, and to measure performance using established image quality metrics.	Kodak24	
cited_context	Kohler dataset	https://doi.org/10.1109/ICCV.2019.00897 (2019)	https://doi.org/10.1007/978-3-642-33786-4_3 (2012)	The Kohler dataset is used to benchmark blind deconvolution algorithms, focusing on evaluating their performance on real-world images blurred with various kernels. This dataset enables researchers to assess algorithm effectiveness in restoring images degraded by different types of blurring, providing a robust testbed for image restoration methodologies.		
cited_context	Lai dataset	https://doi.org/10.1109/ICCV.2019.00897 (2019)	https://doi.org/10.1109/CVPR.2016.188 (2016)	The Lai dataset is used to evaluate single image blind deblurring methods, featuring real-world blurry images of varying qualities and resolutions across diverse scenes. It enables researchers to assess the effectiveness of deblurring algorithms in handling natural, uncontrolled conditions, providing a robust benchmark for performance evaluation.		
citing_context	Lai [18]	https://doi.org/10.48550/arXiv.2505.21637 (2025)	https://doi.org/10.1109/NCC.2015.7084843 (2015)	The Lai [18] dataset is used to evaluate image restoration performance, specifically addressing issues of blur and noise. Researchers employ metrics such as PSNR, SSIM, LPIPS, and FID to assess the effectiveness of restoration techniques. This dataset enables rigorous quantitative analysis, facilitating comparisons between different image restoration methods.		
cited_context	LAION Art dataset	https://doi.org/10.48550/arXiv.2409.15278 (2024)	https://doi.org/10.48550/arXiv.2210.08402 (2022)	The LAION Art dataset is used to gather high-quality natural images from the Internet for image restoration experiments. It enhances the diversity and quality of training data, which is crucial for improving the performance and robustness of image restoration models. This dataset supports research focused on enhancing image quality and addressing various degradation issues in images.		
cited_context	Laion-50K	https://doi.org/10.48550/arXiv.2306.05390 (2023)		The Laion-50K dataset is used to evaluate the performance of DAMoE, specifically focusing on image restoration quality and efficiency. It serves as a benchmark to compare different models and methodologies in the field of image restoration, enabling researchers to assess and enhance the effectiveness of their techniques.		
cited_context	LAION-5B	https://doi.org/10.1109/CVPR52733.2024.02425 (2024), https://doi.org/10.48550/arXiv.2306.05390 (2023)	https://doi.org/10.48550/arXiv.2210.08402 (2022)	The LAION-5B dataset is used to evaluate and compare image quality against other large datasets, emphasizing the importance of high-quality images for restoration tasks. It highlights the limitations of large-scale, high-resolution datasets in providing sufficient texture information for training restoration models. Additionally, it serves as a foundation for multimodal pretraining, aiding in the creation of specialized restoration datasets.		
cited_context	Laion-Figure HR	https://doi.org/10.48550/arXiv.2306.05390 (2023)		The Laion-Figure HR dataset, a high-resolution subset of Laion-5B, is used in research to demonstrate the limitations of texture information in training effective image restoration models. This dataset helps researchers identify and address the insufficiencies in current training methodologies, contributing to the development of more robust restoration techniques.		
cited_context | citing_context	Laion-High-Resolution	https://www.semanticscholar.org/paper/7d7712fbd2f7cdda845c176b03b84f2df93a8b2f (2025)	https://doi.org/10.48550/arXiv.2210.08402 (2022)	The Laion-High-Resolution dataset is used to select high-quality, diverse images for image restoration experiments, specifically focusing on enhancing image resolution and quality. Researchers employ this dataset to evaluate and improve image restoration techniques, leveraging its rich and varied content to ensure robust and effective enhancement methods.; The Laion-High-Resolution dataset is used to select high-quality, diverse images for image restoration experiments. Researchers focus on enhancing image clarity and detail by leveraging the dataset's rich, high-resolution content. This enables the development and evaluation of advanced image restoration techniques, ensuring robust performance across a wide range of image types and conditions.	Laion-High-Resolution	
cited_context | citing_context	Laion-Hr	https://doi.org/10.48550/arXiv.2404.02154 (2024), https://doi.org/10.48550/arXiv.2404.02154 (2024), https://doi.org/10.48550/arXiv.2306.05390 (2023)	https://doi.org/10.1109/CVPRW53098.2021.00074 (2021)	The Laion-HR dataset is used for large-scale image restoration, providing millions of high-resolution images to enhance the generalization and performance of restoration models. This dataset enables researchers to train and evaluate models on a diverse set of high-quality images, improving the robustness and effectiveness of image restoration techniques.; The Laion-HR dataset is used for training and evaluating image restoration models, particularly focusing on high-resolution image enhancement. It serves as a large-scale, high-quality visual content subset of Laion-5B, enabling researchers to develop and test advanced image restoration techniques. This dataset's massive size and high resolution are crucial for improving the performance and robustness of image restoration models.	Laion-HR	
cited_context	Laion400M	https://doi.org/10.1109/TPAMI.2024.3461721 (2024)	https://www.semanticscholar.org/paper/b668ce936cff0b0ca8b635cd5f25a62eaf4eb3df (2021)	The Laion400M dataset is used to pre-train the CLIP model, leveraging its large-scale collection of image-text pairs to enhance the model's generalization ability in image restoration tasks. This pre-training approach helps improve the performance and robustness of image restoration models by providing diverse and extensive data.		
cited_context	LFW-Test	https://doi.org/10.48550/arXiv.2308.15070 (2023)	https://doi.org/10.1109/ICCV.2015.425 (2014)	The LFW-Test dataset is primarily used to evaluate the BFR (Blur, Face Recognition) method on real-world face images. It assesses performance in diverse lighting and pose conditions, focusing on face recognition and restoration tasks. The dataset's real-world images enable researchers to test and improve the robustness of face recognition systems under varied and challenging conditions.		
citing_context	LHP	https://doi.org/10.48550/arXiv.2506.16960 (2025), https://doi.org/10.48550/arXiv.2411.17687 (2024), https://doi.org/10.48550/arXiv.2505.18047 (2025) (+1)	https://doi.org/10.1109/CVPR52729.2023.02083 (2023)	The LHP dataset is primarily used for evaluating and improving real-world de-raining techniques, focusing on the performance of image restoration methods under varying rain intensities and environmental conditions. It is also utilized to test the generalization capability of models on under-display camera images and to assess their effectiveness in handling low-light and high-dynamic-range images. The dataset provides a diverse set of images, enabling researchers to enhance and evaluate deraining and low-light image enhancement methods.		
citing_context	LHP-/Real-Rain-Val	https://doi.org/10.48550/arXiv.2503.09403 (2025)	https://doi.org/10.1109/CVPR.2018.00182 (2018)	The LHP-/Real-Rain-Val dataset is used for validating image deraining models by providing rain-degraded and clean image pairs. This enables researchers to assess the effectiveness of deraining algorithms in restoring images to their original quality, focusing on the accuracy and performance of these models in removing rain artifacts.		
citing_context	LHP-Rain	https://doi.org/10.48550/arXiv.2503.09403 (2025)	https://doi.org/10.1109/CVPR.2018.00182 (2018)	The LHP-Rain dataset is used for evaluating rain removal algorithms in image restoration. It consists of 20 paired images, which researchers use to assess the performance of these algorithms. The dataset enables the comparison of different methods by providing ground truth images, facilitating the analysis of effectiveness in rain removal tasks.		
cited_context	LIME low-light enhancement dataset	https://doi.org/10.48550/arXiv.2407.04621 (2024)	https://doi.org/10.1109/TIP.2016.2639450 (2017)	The LIME low-light enhancement dataset is used for improving illumination in underexposed images through illumination map estimation. Researchers employ this dataset to enhance low-light images, focusing on methodologies that estimate and adjust illumination maps to achieve better visual quality. This dataset enables specific research into low-light image enhancement techniques, addressing challenges such as poor visibility and color fidelity in dimly lit conditions.		
cited_context | citing_context	Live1	https://doi.org/10.48550/arXiv.2506.16960 (2025), https://doi.org/10.48550/arXiv.2406.18242 (2024), https://doi.org/10.48550/arXiv.2410.15385 (2024), https://doi.org/10.1109/CVPRW63382.2024.00645 (2024), https://doi.org/10.1109/CVPR.2019.01131 (2019), https://doi.org/10.48550/arXiv.2210.00405 (2022) (+8)	https://doi.org/10.1109/TIP.2018.2806202 (2017), https://doi.org/10.1109/TIP.2006.881959 (2006)	The LIVE1 dataset is primarily used for JPEG artifact removal, focusing on reducing compression artifacts, particularly at low quality factors such as q = 10. It is employed to evaluate and demonstrate the effectiveness of image restoration algorithms in removing these artifacts, as well as to assess the loss of degradation information post-processing. This dataset enables researchers to test and compare different methods for improving image quality in the context of JPEG compression.; The LIVE1 dataset is primarily used for evaluating the performance of image restoration methods, including artifact removal, denoising, and deblocking of both grayscale and color images. It is employed to assess perceptual quality and distortion types, particularly under JPEG compression at various quality factors. The dataset, consisting of real-world images with diverse content, enables researchers to test and compare restoration algorithms using metrics like PSNR, focusing on high-quality restoration and super resolution. It also serves as a benchmark for full-reference image quality assessment algorithms.	LIVE1	
cited_context | citing_context	Lol	https://doi.org/10.48550/arXiv.2410.15067 (2024), https://doi.org/10.48550/arXiv.2411.18412 (2024), https://doi.org/10.48550/arXiv.2310.10123 (2023) (+12), https://doi.org/10.48550/arXiv.2410.15067 (2024), https://doi.org/10.48550/arXiv.2503.10120 (2025), https://doi.org/10.48550/arXiv.2310.10123 (2023) (+11)	https://www.semanticscholar.org/paper/c7798bbbcd2e85593a2b2d37b92dabac63420cd8 (2018)	The LOL dataset is primarily used for low-light image enhancement research, focusing on improving image quality, visibility, and color fidelity in low-light conditions. It is employed to train and evaluate deep learning models, including those using deep retinex decomposition and ERFM techniques. The dataset enables researchers to assess and compare the performance of various image restoration algorithms, often demonstrating significant improvements in metrics like PSNR.; The LOL dataset is primarily used for evaluating low-light image enhancement methods, focusing on improving visibility, color fidelity, and overall image quality in dark conditions. It serves as a benchmark for assessing restoration techniques and contains low-light and normal-light image pairs, enabling researchers to test and compare the performance of various enhancement algorithms.	LOL	
cited_context | citing_context	Lol Dataset	https://doi.org/10.1109/TMM.2025.3535316 (2024), https://doi.org/10.1109/TCSVT.2024.3519352 (2025), https://doi.org/10.1109/CVPR46437.2021.01594 (2021), https://doi.org/10.1109/TMM.2024.3407656 (2024), https://doi.org/10.1145/3664647.3681621 (2024) (+4)	https://www.semanticscholar.org/paper/c7798bbbcd2e85593a2b2d37b92dabac63420cd8 (2018)	The LOL dataset is used to evaluate the performance of low-light image enhancement methods, focusing on both quantitative metrics and visual quality improvements in low-light conditions. It enables researchers to assess and compare the effectiveness of different enhancement techniques, ensuring they meet the necessary standards for improving image clarity and detail in challenging lighting environments.; The LOL dataset is primarily used for low-light image enhancement research, focusing on improving image quality and detail preservation in low-light conditions. It contains real scene images captured at different exposure times and ISO settings, enabling the training and evaluation of supervised, unsupervised, and zero-shot methods. The dataset supports quantitative comparisons of various enhancement techniques, including deep retinex decomposition and MIRNet, and provides ground-truth images for benchmarking performance metrics.	LOL dataset	
cited_context | citing_context	Lol Prores	https://doi.org/10.48550/arXiv.2410.15067 (2024)	https://doi.org/10.1109/CVPR52729.2023.02083 (2023)	The LOL ProRes dataset is used for low-light image restoration, specifically to enhance images captured in low-light environments. Researchers employ this dataset to develop and evaluate algorithms that improve image quality under low-light conditions. The dataset's focus on low-light scenarios makes it particularly relevant for advancing techniques in this specialized area of image processing.; The LOL ProRes dataset is primarily used for low-light image enhancement, focusing on improving visibility and detail in dark images. Researchers employ this dataset to develop and evaluate algorithms that enhance image quality under low-light conditions. The dataset's characteristics, such as a wide range of low-light images, enable robust testing and validation of enhancement techniques. This application addresses the specific research question of how to effectively restore and enhance images captured in low-light environments.	LOL ProRes	
citing_context	LoL v1	https://doi.org/10.48550/arXiv.2406.18242 (2024)	https://doi.org/10.1109/TIP.2018.2806202 (2017)	The LoL v1 dataset is primarily used for low-light image enhancement research, focusing on improving image brightness and clarity in various low-light conditions. Researchers employ this dataset to evaluate the effectiveness of different enhancement techniques, assessing their performance across a range of challenging lighting scenarios. This dataset enables the development and testing of algorithms designed to enhance images captured in very low light, providing a standardized benchmark for comparing methods.		
citing_context	LoL v2	https://doi.org/10.48550/arXiv.2406.18242 (2024)	https://doi.org/10.1109/TIP.2018.2806202 (2017)	The LoL v2 dataset is primarily used for low-light image enhancement research. It provides a newer, more challenging set of images, including additional images, to evaluate and improve low-light enhancement techniques. Researchers use this dataset to test the effectiveness of advanced enhancement methods, focusing on the quality and realism of the enhanced images.		
citing_context	LOL-Blur	https://doi.org/10.1109/TIP.2025.3572788 (2025)	https://doi.org/10.1007/978-3-031-20068-7_33 (2022)	The 'LOL-Blur' dataset is mentioned in the citation context but lacks detailed descriptions of its usage in research. There is no explicit information regarding its application in All-in-One Image Restoration or any other specific research area, methodology, or research questions. The dataset's characteristics and how it enables research are not provided in the available descriptions.		
cited_context | citing_context	Lol-V1	https://www.semanticscholar.org/paper/4af9a92a556981375dab533ccc55860f01c8b5da (2025), https://doi.org/10.48550/arXiv.2411.17687 (2024), https://doi.org/10.48550/arXiv.2411.18412 (2024) (+9), https://doi.org/10.48550/arXiv.2501.12981 (2025), https://doi.org/10.1109/TMM.2024.3407656 (2024), https://doi.org/10.48550/arXiv.2408.08091 (2024) (+4)	https://doi.org/10.1109/CVPR.2017.35 (2016), https://www.semanticscholar.org/paper/c7798bbbcd2e85593a2b2d37b92dabac63420cd8 (2018)	The LOL-v1 dataset is primarily used for low-light image enhancement, focusing on improving image quality and visibility in underexposed conditions. It is utilized to evaluate and validate models, particularly those employing deep retinex decomposition, for enhancing images captured in low-light environments. The dataset serves as a benchmark for comparing the performance of different enhancement techniques.; The LOL-v1 dataset is primarily used for low-light image enhancement research, where it serves as a benchmark to validate and compare the performance of various enhancement methods. It provides a collection of low-light and normal-light image pairs, enabling researchers to assess restoration quality using metrics like PSNR and SSIM. The dataset supports the development and evaluation of algorithms aimed at improving image visibility and quality in low-light conditions, often employing techniques such as deep retinex decomposition.	LOL-v1	
cited_context | citing_context	Lol-V2	https://doi.org/10.48550/arXiv.2409.19403 (2024), https://doi.org/10.1109/TCSVT.2024.3519352 (2025), https://doi.org/10.48550/arXiv.2412.20157 (2024) (+2), https://doi.org/10.48550/arXiv.2409.19403 (2024), https://doi.org/10.1109/TMM.2024.3407656 (2024)	https://doi.org/10.1109/CVPR52688.2022.01719 (2022), https://doi.org/10.1109/TIP.2021.3050850 (2021)	The LOL-v2 dataset is primarily used for evaluating and testing low-light image enhancement techniques. It contains paired low-light and normal-light images, which researchers use to assess the effectiveness of image restoration methods in improving visibility and color accuracy in dark environments. This dataset enables the evaluation of models like AllRestorer, focusing on enhancing visibility and color fidelity in low-light conditions.; The LOL-v2 dataset is used to evaluate and train low-light image enhancement (LLIE) methods. It provides paired low-light and normal-light images, enabling researchers to assess restoration quality using metrics like PSNR and SSIM. This dataset supports the development and testing of LLIE techniques by offering realistic low-light scenarios and corresponding ground truth images.	LOL-v2	
cited_context | citing_context	Lol-V2-Real	https://doi.org/10.48550/arXiv.2501.12981 (2025)	https://doi.org/10.1016/j.cviu.2022.103403 (2022)	The LOL-v2-real dataset is used to validate the performance of UniUIR on real-world low-light image enhancement, specifically by comparing it against other methods. This dataset enables researchers to assess the effectiveness of image enhancement techniques in practical, low-light conditions, ensuring that the methods can be reliably applied in real-world scenarios.; The LOL-v2-real dataset is used to validate the performance of UniUIR on real-world low-light image enhancement. It is employed to compare UniUIR against other methods, focusing on enhancing images captured in low-light conditions. This dataset enables researchers to assess the effectiveness and superiority of different image enhancement techniques in practical scenarios.	LOL-v2-real	
citing_context	LOL-v2-syn	https://doi.org/10.1109/TIP.2025.3572788 (2025)	https://doi.org/10.1109/TIP.2021.3050850 (2021)	The dataset 'LOL-v2-syn' is mentioned in the citation context but lacks detailed descriptions of its usage in research. There is no explicit information regarding its application, methodology, research questions, or specific characteristics. Therefore, it cannot be accurately described as being used for All-in-One Image Restoration or any other specific research area based on the provided evidence.		
citing_context	LOL-v2-synthetic	https://doi.org/10.1109/TIP.2025.3572788 (2025)	https://doi.org/10.1109/TIP.2021.3050850 (2021)	The dataset 'LOL-v2-synthetic' is mentioned in the citation context but lacks detailed descriptions of its usage in research. There is no explicit information regarding its application, methodology, research questions, or specific characteristics. Therefore, it cannot be accurately described as being used for All-in-One Image Restoration or any other specific research area based on the provided evidence.		
cited_context	LoLi-Phone	https://doi.org/10.1109/CVPR52729.2023.00958 (2023)	https://doi.org/10.1109/TPAMI.2021.3126387 (2021)	The LoLi-Phone dataset is primarily used for low-light image and video enhancement, specifically tailored for mobile phone cameras. It is employed to train and evaluate deep learning models, addressing real-world performance and user experience challenges. The dataset helps researchers enhance visual quality and detail in low-light conditions, pushing the boundaries of current enhancement techniques.		
cited_context	LoLi-Phone Dataset	https://doi.org/10.1109/CVPR52729.2023.00958 (2023)	https://doi.org/10.1109/TPAMI.2021.3126387 (2021)	The LoLi-Phone Dataset is used to evaluate low-light image and video enhancement techniques, specifically focusing on improving visual quality in dark conditions. Researchers employ deep learning methods to enhance images and videos captured in low-light environments. This dataset enables the assessment of these techniques by providing a benchmark for visual quality improvements under challenging lighting conditions.		
cited_context	Low-Light enhancement dataset (LOL)	https://doi.org/10.48550/arXiv.2306.13653 (2023)	https://www.semanticscholar.org/paper/c7798bbbcd2e85593a2b2d37b92dabac63420cd8 (2018)	The Low-Light enhancement dataset (LOL) is used for improving visibility in dark images through low-light image enhancement. Specifically, it employs deep retinex decomposition techniques to enhance image quality. This dataset enables researchers to address the challenge of enhancing images captured in low-light conditions, focusing on methods that can effectively boost visibility and detail in such images.		
cited_context | citing_context	Lsdir	https://doi.org/10.48550/arXiv.2404.02154 (2024), https://doi.org/10.48550/arXiv.2410.15067 (2024), https://doi.org/10.48550/arXiv.2404.02154 (2024), https://doi.org/10.1109/CVPRW63382.2024.00645 (2024)	https://doi.org/10.1109/CVPRW53098.2021.00074 (2021)	The LSDIR dataset is used for large-scale image restoration research, providing a vast number of high-quality, high-resolution natural images. It focuses on real-world image degradations, enabling the training and evaluation of robust restoration models. This dataset supports practical tasks in image restoration by offering a comprehensive resource for both training and evaluation.; The LSDIR dataset is used for training and evaluating image restoration models, providing a large-scale collection of 84,991 high-quality, high-resolution natural images. It addresses complex image degradation by enhancing model robustness and performance, particularly in tasks requiring extensive data diversity and quality.	LSDIR	
cited_context | citing_context	Lsdir-Val	https://doi.org/10.48550/arXiv.2409.19403 (2024)	https://doi.org/10.1109/ICCV.2001.937655 (2001)	The LSDIR-val dataset is used for evaluating the performance of image restoration models, specifically in kernel deblurring and JPEG artifact removal. It assesses the accuracy of kernel estimation for various degradation types, enabling researchers to test and refine models that handle multiple image degradation issues effectively.; The LSDIR-val dataset is used for evaluating and validating image restoration models, specifically in kernel deblurring and JPEG artifact removal. It assesses model performance in handling specific image degradation issues and focuses on the accuracy of estimated degradation kernels. This dataset enables researchers to test and refine methods for improving image quality under various degradation conditions.	LSDIR-val	
citing_context	LSRW	https://doi.org/10.1109/TCSVT.2024.3519352 (2025)	https://www.semanticscholar.org/paper/c7798bbbcd2e85593a2b2d37b92dabac63420cd8 (2018)	The LSRW dataset is used for low-light image enhancement, focusing on training and testing models. It employs a partition ratio of 5600:50 for training and testing sets, respectively. This dataset enables researchers to develop and evaluate algorithms that improve image quality in low-light conditions, addressing specific challenges in image restoration and enhancement.		
cited_context	LSUN	https://doi.org/10.1109/CVPR52729.2023.00958 (2023), https://www.semanticscholar.org/paper/3d3c5fcbc40aadccceda58d3d9c5cd00588ea0b7 (2022)	https://www.semanticscholar.org/paper/744fe47157477235032f7bb3777800f9f2f45e52 (2017)	The LSUN dataset is used to evaluate image restoration models across various categories, leveraging internet-sourced images to test generalization capabilities. It assesses model performance on large-scale, 256x256 images, focusing on consistency and FID scores. The dataset enables researchers to compare supervised and unsupervised methods, emphasizing the model's ability to handle diverse and complex scenes.		
cited_context	LSUN bedrooms	https://www.semanticscholar.org/paper/3d3c5fcbc40aadccceda58d3d9c5cd00588ea0b7 (2022)	https://www.semanticscholar.org/paper/4dcdae25a5e33682953f0853ee4cf7ca93be58a9 (2015)	The LSUN bedrooms dataset is used to train and evaluate image restoration models, specifically focusing on bedroom images at 256x256 resolution. This dataset enables researchers to assess the performance of restoration techniques in enhancing or repairing degraded images within a consistent and well-defined context.		
cited_context	LSUN cats	https://www.semanticscholar.org/paper/3d3c5fcbc40aadccceda58d3d9c5cd00588ea0b7 (2022)	https://www.semanticscholar.org/paper/4dcdae25a5e33682953f0853ee4cf7ca93be58a9 (2015)	The LSUN cats dataset is used to train and evaluate image restoration models, specifically focusing on cat images at 256x256 resolution. This dataset enables researchers to assess the performance of restoration techniques tailored to high-resolution cat images, providing a standardized benchmark for comparing different models.		
cited_context	MagicBrush Test	https://doi.org/10.48550/arXiv.2409.15278 (2024)	https://doi.org/10.48550/arXiv.2306.10012 (2023)	The MagicBrush Test dataset is used to evaluate the effectiveness of PixWizard in instruction-guided image editing. Research focuses on assessing the accuracy and quality of image edits based on manual annotations. This dataset enables researchers to measure how well the model follows specific instructions, providing insights into the performance and reliability of instruction-based image editing techniques.		
cited_context | citing_context	Manga109	https://doi.org/10.48550/arXiv.2410.15067 (2024), https://doi.org/10.1109/ICCVW54120.2021.00210 (2021), https://doi.org/10.48550/arXiv.2210.01427 (2022), https://doi.org/10.1007/978-3-030-58523-5_36 (2020) (+11)	https://doi.org/10.1109/ICCV.2001.937655 (2001), https://doi.org/10.1007/978-3-642-27413-8_47 (2010)	The Manga109 dataset is primarily used for super-resolution tasks, specifically focusing on enhancing the resolution and detail of manga images. Researchers employ this dataset to improve image clarity and detail through single image super-resolution techniques. The dataset's focus on manga images makes it particularly useful for evaluating and developing algorithms tailored to this specific type of content.; The Manga109 dataset is primarily used for evaluating image restoration and super-resolution methods, particularly in manga images. Researchers employ this dataset to assess the performance of various techniques using metrics such as PSNR, SSIM, and LPIPS, focusing on image quality, perceptual similarity, and character detail preservation. The dataset enables the evaluation of models' generalization abilities on images with blended distortions and different resolutions, as well as the effectiveness of low-complexity super-resolution methods.	Manga109	
citing_context	MC-blur	https://doi.org/10.1109/TCSVT.2024.3519352 (2025), https://doi.org/10.1109/TCSVT.2024.3516074 (2024)	https://doi.org/10.1109/TCSVT.2023.3319330 (2021)	The MC-blur dataset is used to advance and evaluate deblurring algorithms, addressing various types of blur such as motion, ultra-high-definition, and defocus blur. It serves as an unseen dataset to test the robustness of these algorithms in real-world scenarios, ensuring they can handle diverse and complex blurring conditions. This dataset enables researchers to refine and validate their methods, enhancing the performance and reliability of deblurring techniques.		
cited_context	McMaster	https://doi.org/10.1109/CVPR52688.2022.00564 (2021), https://doi.org/10.48550/arXiv.2407.20928 (2024), https://doi.org/10.1109/TPAMI.2021.3088914 (2020) (+7)	https://doi.org/10.1109/ICCV.2001.937655 (2001)	The McMaster dataset is primarily used for evaluating image denoising and enhancement algorithms. It contains a diverse set of images, which are used to test and compare the performance of denoising methods under controlled conditions, particularly with additive white Gaussian noise. The dataset is also applied to assess texture and edge preservation in natural scenes, and it supports the evaluation of color image denoising at specific noise levels (15, 25, and 50). This diversity enables comprehensive testing and benchmarking of image restoration techniques.		
cited_context | citing_context	Measnet	https://doi.org/10.48550/arXiv.2410.15067 (2024)	https://doi.org/10.48550/arXiv.2401.13221 (2024)	The MEASNet dataset is used to evaluate multi-exposure adaptive synthesis networks and other multi-exposure image restoration techniques, specifically focusing on exposure correction. It enables researchers to test and assess the performance of these methods in enhancing image quality under varying exposure conditions.; The MEASNet dataset is used for evaluating and enhancing multi-exposure image fusion techniques. It focuses on combining multiple images taken under varying lighting conditions into a single high-quality image, assessing methods for detail enhancement and overall image restoration. This dataset enables researchers to test and compare different fusion algorithms, specifically addressing the challenges of multi-exposure adaptive synthesis and image quality improvement.	MEASNet	
cited_context | citing_context	Mef	https://doi.org/10.48550/arXiv.2410.15385 (2024), https://doi.org/10.1109/CVPR52733.2024.02404 (2024)	https://doi.org/10.1109/TIP.2018.2867951 (2017), https://doi.org/10.1109/TIP.2013.2261309 (2013)	The MEF dataset is used for low-light image enhancement, specifically focusing on multi-exposure fusion techniques. This methodology improves the dynamic range and detail in low-light scenes. The dataset enables researchers to address challenges in enhancing image quality under low-light conditions, providing a robust set of images for testing and validating multi-exposure fusion algorithms.; The MEF dataset is used to evaluate multi-exposure image fusion techniques, specifically focusing on perceptual quality and the preservation of naturalness in real-world scenarios. Researchers employ this dataset to assess and compare different fusion methods, ensuring that the resulting images maintain high visual fidelity and realism. This evaluation helps in advancing image processing algorithms for practical applications.	MEF	
cited_context	Microsoft COCO	https://doi.org/10.1109/CVPR52729.2023.01351 (2023)	https://doi.org/10.1007/s11263-015-0816-y (2014)	The Microsoft COCO dataset is used for training and evaluating image restoration models, particularly focusing on common objects in context. It helps researchers assess model performance on high-quality images and identify issues with low-quality inputs, ensuring robustness in real-world scenarios. The dataset's rich annotations and diverse image content enable comprehensive evaluation and improvement of image restoration techniques.		
cited_context	Middlebury Stereo	https://doi.org/10.1109/TCSVT.2023.3286405 (2024)	https://doi.org/10.1007/978-3-642-33715-4_54 (2012)	The Middlebury Stereo dataset is used to synthesize the RESIDE dataset, providing stereo images for both indoor and outdoor scenes. This contributes to the creation of diverse and realistic image pairs, which are essential for training and evaluating algorithms in stereo vision tasks. The dataset's rich variety of scenes enhances the robustness and generalization of models in stereo matching and depth estimation research.		
citing_context	MiO100	https://doi.org/10.48550/arXiv.2503.09403 (2025)	https://doi.org/10.48550/arXiv.2401.03379 (2024)	The MiO100 dataset is used to generate low-quality images with mixed degradations, specifically for testing multiple-in-one image restoration methods. Researchers employ sequential and prompt learning strategies to evaluate these methods, focusing on their effectiveness in handling various image degradations. This dataset enables the development and assessment of advanced image restoration techniques by providing a diverse set of degraded images.		
citing_context	MIR	https://doi.org/10.48550/arXiv.2503.09131 (2025)	https://doi.org/10.4231/R7RX991C (2015)	The MIR dataset is used for fine-tuning models in real-world denoising scenarios, specifically to enhance denoising performance across diverse conditions. This dataset enables researchers to improve the robustness and effectiveness of image restoration models by providing a variety of real-world noise patterns, thus addressing the challenge of achieving high-quality denoising in practical applications.		
cited_context	MIT-Adobe FiveK	https://doi.org/10.48550/arXiv.2310.10513 (2023), https://doi.org/10.1109/CVPR52729.2023.01350 (2022), https://doi.org/10.1109/TNNLS.2021.3131739 (2020) (+1)	https://www.semanticscholar.org/paper/5ee57b18a7be2b671ffc6f5ce5dbbe92112ee0fb (2007)	The MIT-Adobe FiveK dataset is primarily used to evaluate and compare image restoration and retouching techniques. It involves applying operators on images retouched by expert C to assess the performance and effectiveness of these techniques using metrics like PSNR and SSIM. The dataset also contributes to generating synthetic noisy images and exposure correction datasets, enhancing the training, validation, and testing of models like ShadowDiffusion and MIRNet for image enhancement and restoration.		
citing_context	Mix Degradations datasets	https://doi.org/10.48550/arXiv.2406.18242 (2024)	https://doi.org/10.1109/CVPR52688.2022.00564 (2021)	The Mix Degradations dataset is used to evaluate the performance of ConStyle models converted from U-Net models, specifically focusing on image restoration tasks under various degradations. This dataset enables researchers to assess how well these models handle different types of image degradations, providing insights into their effectiveness and robustness in real-world scenarios.		
cited_context	Mixed RainStreak Dataset	https://doi.org/10.1109/ICCV48922.2021.00231 (2021)	https://doi.org/10.1109/CVPR42600.2020.00837 (2020)	The Mixed RainStreak Dataset is used for training and evaluating the SPAIR model in deraining tasks. It specifically supports the assessment of deraining performance, as evidenced by its use in generating results presented in Table 1 of the main paper. This dataset enables researchers to test and improve image restoration techniques focused on removing rain streaks from images.		
citing_context	Motion Deblur	https://doi.org/10.48550/arXiv.2506.16960 (2025)	https://doi.org/10.1109/CVPR.2017.35 (2016)	The Motion Deblur dataset is used to train and evaluate image deblurring models, specifically focusing on dynamic scenes captured by GoPro cameras. This dataset enables researchers to address the challenge of restoring clear images from motion-blurred ones, enhancing the quality of visual data in dynamic environments.		
cited_context	MS-COCO	https://doi.org/10.1145/3528233.3530757 (2021), https://doi.org/10.1007/978-3-031-73661-2_25 (2023)	https://doi.org/10.1109/TMM.2019.2895280 (2019)	The MS-COCO dataset is used for training and evaluation in image restoration research. It features a large-scale collection of images with diverse annotations, enabling the selection of image-prompt pairs from the validation set to assess restoration methods, particularly focusing on common objects in context. This dataset's extensive and varied content supports robust model training and evaluation.		
cited_context	Naturalness Preserved Enhancement dataset (NPE)	https://doi.org/10.48550/arXiv.2407.04621 (2024)	https://doi.org/10.1109/TIP.2018.2806202 (2017)	The Naturalness Preserved Enhancement (NPE) dataset is primarily used for low-light image enhancement research, where the focus is on enhancing image visibility while maintaining natural appearance. This dataset enables researchers to develop and evaluate algorithms that improve image quality under low-light conditions, ensuring that enhanced images look realistic and preserve natural details.		
cited_context	Nature dataset	https://doi.org/10.1109/TCSVT.2023.3286405 (2024)	https://doi.org/10.1109/ICCV.2017.423 (2017)	The Nature dataset is used to evaluate image restoration methods, particularly focusing on reflection removal performance. It consists of 20 indoor and outdoor scene pairs, constructed by IBCLN, to assess restoration quality in diverse natural settings. This dataset emphasizes the versatility of image restoration techniques in handling various environmental conditions.		
cited_context | citing_context	Ndr	https://doi.org/10.48550/arXiv.2410.15067 (2024)	https://doi.org/10.48550/arXiv.2401.13221 (2024)	The NDR dataset is used to evaluate and assess noise reduction techniques in image restoration, specifically focusing on natural image datasets. It enables researchers to test and compare the effectiveness of various noise reduction methods, contributing to advancements in image restoration methodologies.; The NDR dataset is primarily used for evaluating noise reduction and deblurring techniques in image restoration. It focuses on denoising images under various noise levels and testing non-uniform deblurring methods, particularly for motion blur. Researchers use this dataset to assess the performance of noise reduction algorithms on natural image datasets, enabling the development and refinement of image restoration methodologies.	NDR	
cited_context | citing_context	New Dataset	https://doi.org/10.1109/CVPR52688.2022.00239 (2021)	https://doi.org/10.1109/CVPR.2018.00263 (2017)	The 'new dataset' is specifically used for raindrop removal in image restoration research. It supports the training and evaluation of models aimed at restoring images degraded by raindrops. This dataset enables researchers to develop and test algorithms that enhance image quality by effectively removing raindrop artifacts, focusing on improving visual clarity and detail in rainy conditions.; The 'new dataset' is specifically used for raindrop removal in image restoration research. It supports the training and evaluation of models aimed at restoring images degraded by raindrops. This dataset enables researchers to develop and test algorithms that enhance image quality by effectively removing raindrop artifacts, thereby improving the clarity and usability of images in various applications.	new dataset	
cited_context	NFS	https://doi.org/10.1109/ICCV.2019.00897 (2019)	https://doi.org/10.1109/CVPR.2017.33 (2017)	The NFS dataset is used to train deblurring models in image restoration research. Researchers select every tenth frame from the dataset to ensure a diverse training set, which helps prevent overfitting. This method leverages the dataset's temporal diversity to enhance model generalization and effectiveness in deblurring tasks.		
cited_context | citing_context	Nh-Haze	https://doi.org/10.48550/arXiv.2503.09403 (2025), https://doi.org/10.48550/arXiv.2506.16960 (2025), https://doi.org/10.1109/TCSVT.2024.3429557 (2024) (+1), https://doi.org/10.1109/TCSVT.2024.3429557 (2024), https://doi.org/10.1109/TPAMI.2023.3330416 (2023), https://doi.org/10.1109/TPAMI.2024.3419007 (2024) (+5)	https://doi.org/10.1109/CVPRW50498.2020.00230 (2020), https://doi.org/10.1109/TIP.2018.2867951 (2017)	The NH-HAZE dataset is primarily used for non-homogeneous dehazing research, providing paired hazy and haze-free images to evaluate dehazing algorithms. It is utilized to assess the performance of models under varying conditions, focusing on the restoration of clear and sharp images from real-world hazy environments. This dataset enables researchers to test and compare dehazing methods effectively.; The NH-HAZE dataset is primarily used for evaluating and testing dehazing algorithms on real-world hazy images, particularly in outdoor and natural environments. It serves as a benchmark for assessing the robustness, effectiveness, and performance of dehazing techniques, often focusing on metrics like PSNR to measure image quality improvement. The dataset includes both hazy and haze-free images, enabling researchers to compare and validate dehazing methods under diverse conditions.	NH-HAZE	
cited_context | citing_context	Nhr	https://doi.org/10.1109/TCSVT.2024.3429557 (2024), https://doi.org/10.1109/TIP.2025.3572788 (2025), https://doi.org/10.24963/ijcai.2024/80 (2024), https://doi.org/10.1109/TPAMI.2023.3330416 (2023), https://doi.org/10.1109/TPAMI.2024.3419007 (2024) (+2)	https://doi.org/10.1145/3394171.3413763 (2020)	The NHR dataset is primarily used for training and evaluating models in nighttime dehazing tasks. It provides real-world nighttime hazy images to assess the performance of proposed methods, focusing on enhancing visibility and color fidelity. This dataset enables researchers to test and refine dehazing algorithms specifically tailored for low-light conditions.; The NHR dataset is primarily used for evaluating and comparing nighttime image dehazing algorithms. It contains 16,146 training and 1,794 testing image pairs, providing both synthetic and real-world hazy images. Researchers use it to assess dehazing performance, focusing on techniques like high-low frequency decomposition, grayscale-color networks, and state-of-the-art models such as CSNet. The dataset enables detailed comparisons of various dehazing methods, particularly under low-light conditions, and supports the development of more effective nighttime image restoration techniques.	NHR	
cited_context | citing_context	Npe	https://doi.org/10.48550/arXiv.2410.15385 (2024), https://doi.org/10.1109/CVPR52733.2024.02404 (2024)	https://doi.org/10.1109/TIP.2018.2867951 (2017), https://doi.org/10.1109/TIP.2013.2261309 (2013)	The NPE dataset is used for low-light image enhancement, focusing on noise reduction and detail preservation in low-light conditions. Researchers employ this dataset to develop and evaluate algorithms that improve image quality in dimly lit environments, addressing specific challenges such as noise and loss of detail. The dataset's emphasis on these aspects enables robust testing and validation of enhancement techniques.; The NPE dataset is used to evaluate non-uniform illumination image enhancement techniques, focusing on preserving naturalness and enhancing visual quality in images with challenging lighting conditions. Researchers employ this dataset to assess the effectiveness of their methods in improving image appearance while maintaining realistic and natural visual attributes.	NPE	
cited_context | citing_context	Ntire	https://doi.org/10.48550/arXiv.2404.02154 (2024), https://doi.org/10.48550/arXiv.2404.02154 (2024), https://doi.org/10.1109/CVPR52729.2023.00958 (2023)	https://doi.org/10.1109/CVPRW53098.2021.00074 (2021)	The NTIRE dataset is used to provide high-quality, high-resolution natural images for training and evaluation in image restoration tasks. It supports the development and assessment of algorithms designed to enhance image quality, focusing on restoring degraded images to their original state. This dataset enables researchers to benchmark and compare the performance of different restoration techniques effectively.; The NTIRE dataset is used to provide high-quality, high-resolution natural images for training and evaluating image restoration models. It supports tasks such as low-light enhancement and HDR recovery, enabling researchers to demonstrate superior qualitative and quantitative performance compared to zero-shot baselines. The dataset's high-resolution images facilitate robust model training and rigorous performance evaluation in these specific restoration tasks.	NTIRE	
cited_context	NTIRE 2023 Stereo Image Super-Resolution Challenge	https://doi.org/10.1109/CVPRW59228.2023.00169 (2023)		The NTIRE 2023 Stereo Image Super-Resolution Challenge dataset is primarily used for evaluating and benchmarking stereo image super-resolution techniques. It provides researchers with a standardized set of low-resolution stereo image pairs to upscale, enabling the comparison of different super-resolution algorithms. The dataset's focus on stereo images allows for the assessment of depth and disparity accuracy in high-resolution outputs, facilitating advancements in 3D imaging and computer vision tasks.		
cited_context	NYU V2	https://doi.org/10.1109/TCSVT.2023.3286405 (2024)	https://doi.org/10.1007/978-3-642-33715-4_54 (2012)	The NYU V2 dataset is primarily used to synthesize the RESIDE dataset, which provides indoor RGBD images. These images are utilized for segmentation tasks and support inference, enabling researchers to develop and evaluate algorithms that can accurately segment and interpret indoor environments. The dataset's rich RGBD data facilitates robust training and testing of computer vision models.		
cited_context	NYU-Depth v2	https://doi.org/10.48550/arXiv.2409.15278 (2024)	https://doi.org/10.1007/978-3-642-33715-4_54 (2012)	The NYU-Depth v2 dataset is primarily used for surface normal estimation, leveraging its high-resolution RGB-D images of indoor scenes. Researchers employ this dataset to develop and evaluate algorithms that accurately estimate surface normals from depth and color data, enhancing the understanding of 3D scene geometry in indoor environments. The dataset's rich visual and depth information enables robust training and testing of these algorithms, contributing to advancements in computer vision and robotics.		
cited_context	NYUv2	https://doi.org/10.48550/arXiv.2409.15278 (2024)	https://doi.org/10.1007/978-3-642-33715-4_54 (2012)	The NYUv2 dataset is utilized for monocular depth estimation and surface normal estimation, primarily in indoor scenes. It provides RGB-D images, which are used to evaluate image restoration techniques. The dataset's detailed RGBD images and surface normals enable researchers to assess and improve algorithms for these specific tasks, enhancing the accuracy of depth and normal predictions in indoor environments.		
cited_context | citing_context	O-Haze	https://doi.org/10.48550/arXiv.2505.21637 (2025), https://doi.org/10.1109/TPAMI.2025.3562211 (2024), https://doi.org/10.48550/arXiv.2411.17687 (2024) (+2), https://doi.org/10.1109/TPAMI.2024.3419007 (2024), https://doi.org/10.24963/ijcai.2024/80 (2024), https://doi.org/10.1109/TCSVT.2024.3429557 (2024) (+1)	https://doi.org/10.1109/CVPRW.2018.00119 (2018), https://doi.org/10.1007/978-3-030-01449-0_52 (2018)	The O-HAZE dataset is primarily used for evaluating dehazing algorithms on real-world hazy and haze-free outdoor images. It provides a benchmark for assessing the performance and generalization of these algorithms under realistic conditions, often comparing them with state-of-the-art methods. The dataset's real-world images enable researchers to test and validate dehazing techniques, focusing on practical applications and performance metrics such as PSNR gains.; The O-HAZE dataset is used to assess and validate the performance of dehazing algorithms on real-world outdoor hazy images. It provides realistic conditions and challenging scenarios, enabling researchers to evaluate the effectiveness of their methods in removing haze. The dataset includes both hazy and haze-free images, facilitating direct comparisons and performance metrics such as PSNR improvements.	O-HAZE	
cited_context	ORD database	https://doi.org/10.1109/ICCV51070.2023.01983 (2023)	https://doi.org/10.1109/CVPR52688.2022.01713 (2022)	The ORD database is used to evaluate the performance of multi-weather image restoration methods, specifically focusing on object detection and depth estimation in both degraded and restored images. This dataset enables researchers to assess how well restoration techniques improve image quality under various weather conditions, enhancing the accuracy of subsequent computer vision tasks.		
cited_context	OSR	https://doi.org/10.1109/CVPR52733.2024.01482 (2024)	https://doi.org/10.1109/CVPR.2018.00252 (2018)	The OSR dataset is used as part of the testing set for image restoration, contributing 237 images to evaluate the performance of proposed methods. It enables researchers to assess the effectiveness of their restoration techniques by providing a standardized set of images for benchmarking.		
cited_context	OST	https://doi.org/10.1109/CVPRW59228.2023.00178 (2023), https://doi.org/10.1109/ICCV51070.2023.00495 (2023)	https://doi.org/10.1109/CVPR.2018.00070 (2018)	The OST dataset is used to evaluate and assess image restoration algorithms, particularly focusing on the reconstruction task in image super-resolution and the restoration of textures and details in outdoor scenes. It contains 300 images with rich textures, enabling researchers to test and improve the performance of deep spatial feature transform methods in realistic texture recovery.		
cited_context | citing_context	Ots	https://doi.org/10.1109/TCSVT.2023.3299324 (2024)	https://doi.org/10.1109/CVPR.2018.00079 (2018)	The OTS dataset is primarily used for training and evaluating haze removal algorithms, specifically focusing on outdoor synthetic hazy images. Researchers employ this dataset to develop and test algorithms that enhance image clarity by removing haze, contributing to advancements in image restoration techniques. The synthetic nature of the dataset allows for controlled experimentation and robust algorithm validation.; The OTS dataset is primarily used for training and evaluating image deraining algorithms, specifically focusing on outdoor scenes with synthetic rain. This dataset enables researchers to assess the performance of deraining techniques in realistic outdoor environments, contributing to advancements in image restoration and enhancement methodologies.	OTS	
citing_context	OTS dataset of RESIDE-β	https://doi.org/10.48550/arXiv.2408.15994 (2024)	https://doi.org/10.1109/ICCV.2001.937655 (2001)	The OTS dataset of RESIDE-β is primarily used for training image dehazing models, providing a large set of paired images that facilitate the development and improvement of these models. This dataset enables researchers to enhance the clarity of hazy images by training algorithms to effectively remove atmospheric haze, thereby improving image quality and visual clarity.		
cited_context	our unpaired training set	https://doi.org/10.1109/TIP.2021.3051462 (2019)	https://www.semanticscholar.org/paper/c7798bbbcd2e85593a2b2d37b92dabac63420cd8 (2018)	The 'our unpaired training set' dataset is used to train a vanilla CycleGAN for low-light image enhancement. It is employed to compare the performance of this method against other techniques such as RetinexNet, SRIE, LIME, and NPE. This dataset enables researchers to evaluate and benchmark different low-light image enhancement approaches, focusing on the effectiveness and efficiency of CycleGAN in this context.		
cited_context	Outdoor	https://doi.org/10.1109/TIP.2024.3368961 (2023)	https://doi.org/10.1109/TIP.2018.2806202 (2017)	The 'Outdoor' dataset is used to train models on heavy rain images containing rain streaks and haze, enhancing the model's performance in complex weather conditions. This dataset specifically supports research in image restoration under challenging environmental scenarios, focusing on improving visual clarity and detail recovery in adverse weather.		
citing_context	Outdoor Training Set (OTS)	https://doi.org/10.1109/TCSVT.2024.3519352 (2025)	https://doi.org/10.1109/TIP.2018.2867951 (2017)	The Outdoor Training Set (OTS) is primarily used for training dehazing models on outdoor scenes. It offers a wide range of environmental conditions and lighting, enabling researchers to develop and test algorithms that effectively remove haze from images. This dataset supports the creation of robust dehazing models by providing diverse and realistic scenarios.		
cited_context | citing_context	Outdoor-Rain	https://doi.org/10.48550/arXiv.2410.15067 (2024), https://doi.org/10.48550/arXiv.2504.05135 (2025), https://doi.org/10.48550/arXiv.2506.16960 (2025) (+2), https://doi.org/10.48550/arXiv.2410.15067 (2024), https://doi.org/10.1109/TPAMI.2023.3238179 (2022), https://doi.org/10.48550/arXiv.2305.17863 (2023) (+5)	https://doi.org/10.1109/CVPR.2018.00263 (2017), https://doi.org/10.1109/CVPR.2019.00173 (2019)	The Outdoor-Rain dataset is primarily used for rain removal in outdoor scenes, focusing on realistic rain effects and their impact on image quality. It is employed to train and evaluate rain removal algorithms, using both synthetic and real rain images to enhance image clarity and model robustness in various rainy conditions, including heavy rain with streaks and haze. The dataset supports deraining tasks by providing real-world outdoor rain images, enabling researchers to test and improve the performance of rain removal techniques.; The Outdoor-Rain dataset is primarily used for training and evaluating image restoration models focused on rain removal from outdoor scenes. It contains real-world outdoor rain images, which are utilized to test model robustness and performance in deraining tasks. The dataset supports methodologies such as physics models, conditional adversarial learning, and generative modeling with GANs, enabling researchers to enhance image quality and compare the effectiveness of different restoration techniques.	Outdoor-Rain	
cited_context	Outdoor-Rain dataset	https://doi.org/10.48550/arXiv.2305.17863 (2023)	https://doi.org/10.1109/CVPR.2019.00173 (2019)	The Outdoor-Rain dataset is mentioned in the citation context but lacks detailed descriptions of its usage in research. Therefore, there is no specific evidence to indicate how it is used, the methodologies employed, or the research questions it addresses. Its role in All-in-One Image Restoration or any other research area is not substantiated by the provided information.		
cited_context	Outdoor-Rain test images	https://doi.org/10.1109/TPAMI.2023.3238179 (2022)	https://doi.org/10.1109/CVPR.2019.00173 (2019)	The 'Outdoor-Rain test images' dataset is used to evaluate the performance of rain image restoration models, particularly in comparing RainHazeDiff64 against HRGAN and MPRNet. Visual quality assessments are employed to assess model effectiveness. This dataset enables researchers to benchmark and refine algorithms designed to restore images degraded by rain, focusing on enhancing visual clarity and detail.		
cited_context	Outdoor-Rain test samples	https://doi.org/10.1109/TPAMI.2023.3238179 (2022)	https://doi.org/10.1109/CVPR.2019.00173 (2019)	The 'Outdoor-Rain test samples' dataset is used to qualitatively evaluate and compare the performance of different models, such as the best model, HRGAN, and MPRNet, in heavy rain image restoration. Research focuses on visual quality and detail preservation, employing the dataset to assess and contrast the effectiveness of these models in restoring images degraded by heavy rain.		
cited_context	Outdoor-Rain-Test	https://doi.org/10.48550/arXiv.2305.17863 (2023)	https://doi.org/10.1109/CVPR.2019.00173 (2019)	The Outdoor-Rain-Test dataset is used for evaluating image restoration models, particularly in outdoor scenes affected by haze and rain. It focuses on assessing the quality of dehazing and restoration under real-world degradation conditions, especially heavy rain. This dataset enables researchers to test and compare the effectiveness of various image restoration methods in challenging environmental scenarios.		
cited_context | citing_context	Perceive-Ir	https://doi.org/10.48550/arXiv.2410.15067 (2024)	https://doi.org/10.48550/arXiv.2401.13221 (2024)	The Perceive-IR dataset is used to assess and test perception-driven image restoration methods, focusing on enhancing visual quality. It enables researchers to evaluate the effectiveness of these methods by providing a benchmark for visual improvement, ensuring that restored images meet high perceptual standards.; The Perceive-IR dataset is used for perceptual image restoration, specifically to enhance visual quality and realism. It is employed to evaluate image restoration methods, focusing on how well these methods improve the perceptual quality and realism of images for human observers. This dataset enables researchers to assess and compare different restoration techniques based on their ability to produce visually pleasing results.	Perceive-IR	
cited_context	PieAPP	https://www.semanticscholar.org/paper/f942a4a56e6549c83844747ad6c4ae58000b2988 (2020)	https://doi.org/10.1109/CVPR.2018.00194 (2018)	The PieAPP dataset is used to assess perceptual errors in image restoration by conducting pairwise comparisons to evaluate human preference and perception of image quality. It focuses on measuring the differences between restored and original images, enabling researchers to refine restoration techniques based on human visual preferences.		
cited_context | citing_context	Pip	https://doi.org/10.48550/arXiv.2410.15067 (2024)	https://doi.org/10.48550/arXiv.2401.13221 (2024), https://doi.org/10.1007/978-3-031-72764-1_1 (2024)	The PIP dataset is used to evaluate and test image restoration methods, focusing on their ability to handle multiple and complex degradations simultaneously. This dataset enables researchers to assess the effectiveness of restoration techniques in diverse and challenging scenarios, ensuring robust performance across various image degradations.; The PIP dataset is used in research to test and evaluate image restoration algorithms, focusing on tasks such as image inpainting and handling complex degradations. It is employed to assess the performance of methods in filling in missing regions and addressing various types of image degradation, enabling researchers to compare and improve restoration techniques.	PIP	
cited_context	PIPAL full set	https://doi.org/10.1007/978-3-030-58621-8_37 (2020)	https://www.semanticscholar.org/paper/8c92054c26fb4c6dd7435bc99fbb8af3323eae1b (2019)	The PIPAL full set dataset is used to evaluate the performance of anti-aliasing pooling layers in image quality assessment, specifically focusing on full-reference image distortions. This dataset enables researchers to assess and compare different anti-aliasing techniques by providing a comprehensive set of distorted images and their corresponding reference images, facilitating the development and validation of image quality metrics.		
cited_context	Places2	https://doi.org/10.1145/3528233.3530757 (2021)	https://doi.org/10.1109/ICCV.2019.01062 (2019)	The Places2 dataset is primarily used for evaluating and training image extension methods, particularly uncropping and outpainting techniques. It supports research by providing a diverse set of images, enabling comparisons of performance across different categories and methodologies, such as generative adversarial networks. The dataset facilitates the assessment of image quality metrics and the effectiveness of various image extension approaches.		
citing_context	POLED	https://doi.org/10.48550/arXiv.2505.18047 (2025)	https://doi.org/10.1109/CVPR46437.2021.00912 (2021)	The POLED dataset is used to evaluate models' generalization capabilities on real-world under-display camera images from OLED displays. Researchers focus on assessing image clarity and noise reduction, employing the dataset to test and validate the performance of their models in enhancing image quality under specific display conditions.		
cited_context	Poly	https://doi.org/10.1109/TNNLS.2021.3131739 (2020)	https://doi.org/10.1109/ICCV.2001.937655 (2001)	The Poly dataset is used in image restoration research, specifically to provide real noisy images. It consists of cropped patches of 512 × 512 pixels, which are utilized to enhance the quality of degraded images. This dataset enables researchers to test and develop algorithms for noise reduction and image enhancement, focusing on improving the fidelity of restored images.		
cited_context	polyU Real World Noisy Images Dataset	https://doi.org/10.48550/arXiv.2306.05390 (2023)	https://www.semanticscholar.org/paper/fb52f9294fa1cd83660eccac486367eac01ea32d (2018)	The polyU Real World Noisy Images Dataset is used to train color denoising models, specifically focusing on real-world noisy images. This dataset enhances the denoising performance in practical scenarios by providing realistic noise patterns, enabling researchers to develop more effective and robust image restoration techniques.		
cited_context	Postcard dataset	https://doi.org/10.1109/TCSVT.2023.3286405 (2024)	https://doi.org/10.1109/ICCV.2017.423 (2017)	The Postcard dataset is used to assess and test image restoration techniques, particularly focusing on the preservation of fine details and color accuracy in 199 triplet images from postcards. It evaluates algorithm performance, including single-image reflection removal, using metrics like PSNR to measure effectiveness. This dataset challenges researchers to improve restoration methods while maintaining high visual quality.		
cited_context | citing_context	Promptir	https://doi.org/10.48550/arXiv.2410.15067 (2024)	https://doi.org/10.48550/arXiv.2401.13221 (2024), https://doi.org/10.1007/978-3-031-72764-1_1 (2024)	The PromptIR dataset is used to assess and evaluate prompt-based image restoration techniques, focusing on diverse image degradations. It enables researchers to test and compare image restoration methods, particularly those that incorporate prompts, to address various degradation issues. This dataset supports the development and refinement of prompt-based approaches in image restoration.; The PromptIR dataset is used to evaluate and enhance image restoration methods, particularly focusing on instruction-based and prompt-driven approaches. It supports research in human-guided and adaptive restoration techniques, aiming to generate high-quality restored images. This dataset facilitates the assessment of these methodologies by providing a platform for testing and comparing different restoration algorithms.	PromptIR	
cited_context	RAIN 100L	https://doi.org/10.1109/TPAMI.2023.3330416 (2023)	https://doi.org/10.1109/TCSVT.2019.2920407 (2017)	The RAIN 100L dataset is used to evaluate the performance of rain removal algorithms, specifically focusing on synthetic rain images with varying intensities and complexities. This dataset enables researchers to test and compare different methodologies for rain removal, ensuring robustness across diverse rain conditions.		
cited_context	Rain In Driving (RID)	https://doi.org/10.1109/ICCV51070.2023.01983 (2023)	https://doi.org/10.1109/TIP.2018.2806202 (2017)	The Rain In Driving (RID) dataset is utilized to evaluate rain removal techniques in driving scenarios. It focuses on analyzing the impact of these techniques on visibility and safety. The dataset enables researchers to assess how effectively different methods can enhance image clarity and, consequently, improve driving conditions during rainy weather.		
cited_context	Rain-1 Drop	https://doi.org/10.48550/arXiv.2503.10120 (2025)	https://www.semanticscholar.org/paper/c7798bbbcd2e85593a2b2d37b92dabac63420cd8 (2018)	The Rain-1 Drop dataset is used to train and evaluate models for raindrop removal from images, focusing on realistic raindrop effects. This dataset enables researchers to develop and test algorithms that enhance image quality by removing raindrops, addressing the specific challenge of realistic raindrop simulation in image restoration tasks.		
cited_context | citing_context	Rain-Haze-Noise-Blur-Dark	https://doi.org/10.48550/arXiv.2410.15067 (2024)	https://doi.org/10.48550/arXiv.2403.14614 (2024), https://doi.org/10.48550/arXiv.2401.13221 (2024)	The 'rain-haze-noise-blur-dark' dataset is used to evaluate image restoration techniques and all-in-one image restoration models. It focuses on images degraded by rain, haze, noise, blur, and low-light conditions. This dataset enables researchers to assess the effectiveness of restoration methods in handling multiple degradations simultaneously, providing a comprehensive evaluation framework for advanced image restoration algorithms.; The 'rain-haze-noise-blur-dark' dataset is used for comprehensive image restoration research, focusing on handling multiple degradations such as rain, haze, noise, blur, and low light. It is employed to test and evaluate image restoration algorithms and models designed to address these combined degradations, enabling researchers to assess the effectiveness of all-in-one restoration techniques.	rain-haze-noise-blur-dark	
cited_context | citing_context	Rain-Haze-Snow	https://doi.org/10.48550/arXiv.2505.12630 (2025), https://doi.org/10.1109/CVPR52729.2023.00563 (2023), https://doi.org/10.1109/CVPR52729.2023.00563 (2023), https://doi.org/10.48550/arXiv.2505.12630 (2025)	https://doi.org/10.1109/cvpr42600.2020.00324 (2020)	The 'Rain-Haze-Snow' dataset is used to train and evaluate machine learning models for removing weather-based degradations such as rain, haze, and snow from images. It supports a single-encoder, multi-decoder framework and is utilized in neural architecture search for all-in-one image restoration, focusing on these specific weather conditions. This dataset enables researchers to develop and test models that can handle multiple types of image degradations simultaneously.; The 'Rain-Haze-Snow' dataset is used to train and evaluate machine learning models for all-in-one image restoration, specifically targeting rain, haze, and snow removal. It supports neural architecture search methods and single-encoder, multi-decoder frameworks, enabling researchers to develop and test algorithms that effectively handle multiple weather-based image degradations.	Rain-Haze-Snow	
cited_context	rain-streak datasets	https://doi.org/10.1109/ICCV48922.2021.00231 (2021)	https://doi.org/10.1109/CVPR.2017.186 (2017)	Rain-streak datasets are used to train deep detail networks for removing rain streaks from single images. The focus is on addressing degradation patterns and preserving image details. These datasets enable researchers to develop and refine algorithms that enhance image quality by effectively eliminating rain streaks, thereby improving the clarity and usability of images in various applications.		
cited_context	Rain/HazeCityscapes	https://doi.org/10.48550/arXiv.2312.16610 (2023)	https://doi.org/10.1109/CVPR.2019.00821 (2019)	The Rain/HazeCityscapes dataset is used to evaluate the performance of image restoration techniques, specifically focusing on deweathering and segmentation accuracy under rain and haze conditions. Researchers employ this dataset to test and compare algorithms designed to enhance image quality and maintain segmentation accuracy in adverse weather scenarios. The dataset's inclusion of diverse weather conditions enables robust assessment of these methodologies.		
cited_context | citing_context	Rain100	https://doi.org/10.48550/arXiv.2310.10123 (2023), https://doi.org/10.48550/arXiv.2310.10123 (2023), https://doi.org/10.48550/arXiv.2312.01677 (2023)	https://doi.org/10.1109/CVPR.2017.35 (2016)	The Rain100 dataset is primarily used for rain removal research, focusing on the elimination of rain streaks from images. It is employed to test and evaluate the effectiveness of various image restoration techniques in removing rain artifacts. This dataset enables researchers to assess the performance of their methods in enhancing image clarity and quality under rainy conditions.; The Rain100 dataset is used for evaluating rain removal techniques in image restoration. It contains both synthetic and real rain streaks, enabling researchers to test the deraining capabilities of models like DINO-IR on unseen data with rain degradation. This dataset facilitates the assessment of model performance in removing rain artifacts, enhancing image clarity and quality.	Rain100	
cited_context	Rain100 L	https://doi.org/10.1109/TIV.2023.3347952 (2024)	https://doi.org/10.18535/ijecs/v5i1.12 (2016)	The Rain100 L dataset is used to train and evaluate the AirNet model, specifically addressing overfitting issues in image de-raining. The dataset employs attention mechanisms and contrastive learning to enhance the model's performance. It is crucial for assessing the effectiveness of these techniques in removing rain from images, making it a valuable resource for improving image restoration algorithms.		
cited_context | citing_context	Rain100H	https://doi.org/10.48550/arXiv.2411.18412 (2024), https://doi.org/10.48550/arXiv.2410.15067 (2024), https://doi.org/10.1109/TCSVT.2023.3299324 (2024) (+3), https://doi.org/10.1109/TMM.2024.3407656 (2024), https://www.semanticscholar.org/paper/6af728164e4424218967fd356b3448f5037acd88 (2023), https://doi.org/10.48550/arXiv.2503.10120 (2025) (+12)	https://doi.org/10.1109/CVPR.2017.183 (2016), https://doi.org/10.1109/CVPR.2018.00079 (2018)	The Rain100H dataset is primarily used for training and evaluating rain removal algorithms, focusing on high-resolution images with synthetic and heavy rain streaks. It challenges and refines models by providing a large set of rain-streaked images, enabling researchers to assess the effectiveness of deraining techniques in both synthetic and real-world scenarios.; The Rain100H dataset is primarily used for training and evaluating rain removal algorithms, particularly focusing on high-resolution images with heavy and synthetic rain. It assesses the performance of deraining methods using metrics like PSNR and SSIM, emphasizing the effectiveness in removing rain streaks and preserving image details. This dataset enables researchers to test and improve deraining techniques in challenging rain conditions.	Rain100H	
cited_context | citing_context	Rain100L	https://doi.org/10.48550/arXiv.2410.15067 (2024), https://doi.org/10.1109/TPAMI.2025.3562211 (2024), https://doi.org/10.1109/CVPR52688.2022.01693 (2022) (+28), https://doi.org/10.48550/arXiv.2410.15067 (2024), https://doi.org/10.48550/arXiv.2405.02843 (2024), https://doi.org/10.1109/CVPR52688.2022.01693 (2022) (+26)	https://doi.org/10.1109/TIP.2018.2867951 (2017), https://doi.org/10.1109/CVPR.2017.183 (2016)	The Rain100L dataset is primarily used for image deraining, focusing on the removal of rain streaks to enhance image clarity. It consists of 200 clean-rainy image pairs for training and 100 pairs for testing. Researchers employ deep multi-scale convolutional neural networks and other models to train, test, and evaluate deraining performance, often in single-task settings. The dataset is used for ablation experiments, visual comparisons with state-of-the-art methods, and assessing PSNR gains, specifically in the context of synthetic rainy images.; The Rain100L dataset is primarily used for testing and evaluating rain removal algorithms, with a focus on both light and heavy rain conditions. It is employed in training and assessing deraining techniques, particularly for low-resolution images with synthetic rain. The dataset includes 100 pairs of original and rainy images, enabling researchers to measure restoration quality using metrics like PSNR and SSIM. It serves as a benchmark for single-task deraining, evaluating the effectiveness and performance of various methods.	Rain100L	
cited_context | citing_context	Rain12	https://doi.org/10.1109/TCSVT.2023.3299324 (2024), https://doi.org/10.48550/arXiv.2406.18242 (2024), https://doi.org/10.1109/TCSVT.2024.3398810 (2023), https://doi.org/10.1109/TCSVT.2023.3299324 (2024), https://doi.org/10.48550/arXiv.2306.05390 (2023), https://doi.org/10.1109/CVPR52688.2022.01688 (2022) (+1)	https://doi.org/10.1109/TCSVT.2019.2920407 (2017), https://doi.org/10.1109/CVPR.2016.299 (2016)	The Rain12 dataset is used for training and evaluating rain removal algorithms, particularly focusing on real-world rain images. It serves as a benchmark for deraining models, enabling researchers to test and improve methods for removing rain streaks from images using techniques such as conditional generative adversarial networks. The dataset's challenging rainy images facilitate the development of robust deraining algorithms.; The Rain12 dataset is used for training and evaluating rain removal algorithms, particularly focusing on a small set of challenging and high-quality rainy images. It serves as a specialized subset within larger datasets like Rain13K, enabling detailed performance analysis and the development of models for rain streak removal in both synthetic and real-world images. The dataset's size and quality make it suitable for refining and testing layer priors in deraining techniques.	Rain12	
cited_context | citing_context	Rain1200	https://doi.org/10.1109/TCSVT.2023.3299324 (2024), https://doi.org/10.48550/arXiv.2504.09973 (2025), https://doi.org/10.1109/TCSVT.2023.3299324 (2024), https://doi.org/10.48550/arXiv.2407.04621 (2024), https://doi.org/10.48550/arXiv.2306.05390 (2023)	https://doi.org/10.1109/CVPR.2018.00079 (2018), https://doi.org/10.1109/TIP.2018.2806202 (2017)	The Rain1200 dataset is used for training and evaluating rain removal algorithms, specifically focusing on synthetic rain images with varying intensities. It is employed to synthesize images with rain degradation to assess the performance of image restoration techniques, particularly in de-raining applications. This dataset enables researchers to test and improve the effectiveness of algorithms in removing rain from images, enhancing their quality and usability.; The Rain1200 dataset is primarily used for training and evaluating rain removal algorithms, focusing on synthetic rain images. It is employed to test the capability of models like OneRestore in removing rain streaks and restoring clean images. The dataset's large set of synthetic rainy images enables researchers to assess the performance and effectiveness of deraining models.	Rain1200	
cited_context | citing_context	Rain13K	https://doi.org/10.48550/arXiv.2409.19403 (2024), https://doi.org/10.48550/arXiv.2411.17687 (2024), https://doi.org/10.48550/arXiv.2505.18047 (2025), https://doi.org/10.48550/arXiv.2306.05390 (2023), https://doi.org/10.1109/CVPRW53098.2021.00027 (2021), https://doi.org/10.48550/arXiv.2409.19403 (2024) (+3)	https://doi.org/10.1109/CVPR.2017.35 (2016), https://doi.org/10.1109/CVPR46437.2021.01458 (2021)	The Rain13k dataset is used for training and testing deraining models, specifically to remove rain streaks and improve image clarity. It consists of 13,711 training images and 4,298 test images, providing a large set of paired rainy and clean images. This dataset enables researchers to develop and evaluate deraining algorithms effectively.; The Rain13K dataset is primarily used for deraining tasks, providing a large-scale collection of paired rainy and clean images. It is employed to train and evaluate deraining algorithms, focusing on diverse rain models and progressive image restoration techniques to improve the quality of rain removal. The dataset's extensive size and inclusion of both synthetic and real-world images enhance its utility in assessing the effectiveness of restoration methods.	Rain13K, Rain13k	
cited_context | citing_context	Rain13K-Test	https://doi.org/10.48550/arXiv.2409.19403 (2024)	https://doi.org/10.1109/ICCV.2001.937655 (2001), https://doi.org/10.1109/CVPR.2018.00079 (2018)	The Rain13k-Test dataset is used for evaluating deraining algorithms by combining multiple datasets to assess their effectiveness in diverse conditions. It is specifically employed to test the performance of algorithms in removing rain streaks from images, ensuring robustness across various scenarios. This dataset enables researchers to benchmark and compare deraining techniques comprehensively.; The Rain13k-Test dataset is used for evaluating deraining algorithms by combining multiple test sets. It assesses the performance of these algorithms in removing rain streaks from images under diverse conditions. This dataset enables researchers to compare the effectiveness of different deraining methods, ensuring robustness across various scenarios.	Rain13k-Test	
cited_context | citing_context	Rain1400	https://doi.org/10.48550/arXiv.2506.16960 (2025), https://doi.org/10.1109/WACV61041.2025.00069 (2025), https://doi.org/10.48550/arXiv.2410.08177 (2024), https://doi.org/10.48550/arXiv.2306.05390 (2023)	https://doi.org/10.1109/CVPR.2017.186 (2017), https://doi.org/10.1109/CVPR.2016.299 (2016)	The Rain1400 dataset is primarily used for deraining tasks, focusing on the removal of rain streaks from images. It serves as a benchmark for evaluating the performance of various restoration algorithms, particularly in synthetic data. Researchers use metrics like PSNR, MS-SSIM, and BD-PSNR to assess the effectiveness of methods such as TANet and deep detail networks in enhancing image quality and clarity.; The Rain1400 dataset is used for training and evaluating deraining models, offering a diverse collection of synthetic and real-world rainy images. This dataset enables researchers to test the effectiveness of their models in removing rain artifacts from images, thereby improving image clarity and quality. The diversity of the dataset supports robust model evaluation across various rainy conditions.	Rain1400	
cited_context | citing_context	Rain14000	https://doi.org/10.1109/TCSVT.2023.3299324 (2024), https://doi.org/10.48550/arXiv.2406.18242 (2024), https://doi.org/10.1109/TCSVT.2024.3398810 (2023), https://doi.org/10.1109/TCSVT.2023.3299324 (2024), https://doi.org/10.1109/TMM.2024.3407656 (2024), https://doi.org/10.48550/arXiv.2306.05390 (2023) (+2)	https://doi.org/10.1109/TCSVT.2019.2920407 (2017), https://doi.org/10.1109/CVPR.2017.183 (2016)	The Rain14000 dataset is primarily used for training and evaluating rain removal algorithms, with a focus on diverse rain patterns and intensities. It contains 13,712 images with rain streaks, enabling robustness testing of deraining models. Researchers employ this dataset to develop and assess methods, such as conditional generative adversarial networks, aimed at effectively removing rain streaks from images.; The Rain14000 dataset is primarily used for training and evaluating rain removal algorithms, focusing on diverse rain patterns and intensities. It provides a large set of 11,200 clean-rain image pairs, enhancing model robustness and generalization. This dataset is crucial for improving deraining models through deep learning techniques, ensuring they perform well across various rain conditions.	Rain14000	
cited_context | citing_context	Rain1800	https://doi.org/10.48550/arXiv.2406.18242 (2024), https://doi.org/10.1109/TCSVT.2024.3398810 (2023), https://doi.org/10.48550/arXiv.2306.05390 (2023), https://doi.org/10.1109/CVPR52688.2022.01688 (2022), https://doi.org/10.1109/TCSVT.2024.3398810 (2023)	https://doi.org/10.1109/CVPR.2016.299 (2016)	The Rain1800 dataset is used to train and evaluate deraining models, providing a diverse set of rainy images for algorithm validation. It focuses on removing rain streaks from images, often employing conditional generative adversarial networks. This dataset enables researchers to test the effectiveness of deraining algorithms in various conditions, ensuring robust performance.; The Rain1800 dataset is used primarily for training and evaluating rain removal models in the field of image restoration. It provides 1,800 clean-rain image pairs, which are utilized to enhance the performance of deep learning algorithms in removing rain streaks from images. This moderate-sized dataset, part of the larger Rain13K collection, offers varied rainy conditions, making it suitable for improving and assessing deraining techniques.	Rain1800	
cited_context | citing_context	Rain200H	https://doi.org/10.3390/electronics13142817 (2024), https://doi.org/10.1109/TMM.2025.3535316 (2024), https://doi.org/10.1109/CVPR52733.2024.00277 (2023) (+1), https://doi.org/10.48550/arXiv.2404.12091 (2024), https://doi.org/10.48550/arXiv.2203.06074 (2022), https://doi.org/10.1109/CVPR52733.2024.00277 (2023)	https://doi.org/10.1109/CVPR.2017.183 (2016)	The Rain200H dataset is primarily used for training and evaluating deraining models, focusing on high-resolution images with synthetic rain. It provides 1800 pairs of degraded and clean images for training and 200 pairs for testing, enhancing the degradation space and increasing the robustness of rain removal techniques. The dataset is crucial for assessing the effectiveness of various models in restoring images with heavy rain and synthetic rainstreaks, making it a valuable resource for single-image rain detection and removal research.; The Rain200H dataset is primarily used for training and evaluating rain removal methods, particularly focusing on high-resolution images with synthetic rain. It challenges de-raining techniques by including heavy rain conditions and high-complexity rain patterns. Researchers use it to assess and improve the performance of models in removing severe rain streaks, enhancing image clarity, and achieving specific metrics like PSNR improvements. The dataset is also utilized to train models like DGUNet, emphasizing the impact of rain-/detail-aware negative exemplars and plotting layer-wise log T lcc ′ for performance evaluation.	Rain200H	
cited_context | citing_context	Rain200L	https://doi.org/10.3390/electronics13142817 (2024), https://doi.org/10.48550/arXiv.2411.18412 (2024), https://doi.org/10.1109/CVPR52733.2024.00277 (2023) (+4), https://doi.org/10.48550/arXiv.2404.12091 (2024), https://doi.org/10.48550/arXiv.2203.06074 (2022), https://doi.org/10.1109/CVPR52733.2024.00277 (2023) (+3)	https://doi.org/10.1109/TIP.2016.2631888 (2017), https://doi.org/10.1109/CVPR.2017.183 (2016)	The Rain200L dataset is primarily used for deraining research, providing synthetic rain images to train and evaluate models focused on removing rain streaks from low-resolution images. It enhances the degradation space by offering low-quality synthetic rain images, which increases the diversity of training data. This dataset is crucial for improving the visual quality of images and comparing the effectiveness of various deraining methods, particularly in scenarios with light rain.; The Rain200L dataset is primarily used for training and evaluating rain removal methods, particularly focusing on low-resolution images with synthetic rain and light rain conditions. It is employed to assess the performance of deraining algorithms, emphasizing the removal of rain streaks to improve image clarity and visual quality. The dataset facilitates the evaluation of PSNR improvements and algorithm robustness in handling low-complexity rain patterns.	Rain200L	
cited_context | citing_context	Rain800	https://doi.org/10.3390/electronics13142817 (2024), https://doi.org/10.1109/TCSVT.2023.3299324 (2024), https://doi.org/10.48550/arXiv.2406.18242 (2024) (+1), https://doi.org/10.48550/arXiv.2404.12091 (2024), https://doi.org/10.1109/TCSVT.2023.3299324 (2024), https://doi.org/10.48550/arXiv.2306.05390 (2023) (+3)	https://doi.org/10.1109/CVPR.2018.00079 (2018), https://doi.org/10.1109/TCSVT.2019.2920407 (2017)	The Rain800 dataset is primarily used for training and evaluating rain removal algorithms, focusing on synthetic rain images with varying densities. It provides 1800 pairs of degraded and clean images for training and 200 pairs for testing, enabling detailed performance analysis and comparison of deraining methods. The dataset assesses the robustness and generalization of models, particularly in handling diverse rain intensities.; The Rain800 dataset is primarily used for training and evaluating rain removal algorithms, particularly focusing on both synthetic and real-world rainy images with varying densities. It is employed to assess the deraining capabilities of models like AIRFormer, DGUNet, and CoIC, emphasizing their performance in handling diverse rain conditions and improving the quality of restored images. The dataset's high-quality images and ground truth data enable researchers to rigorously test and refine deraining techniques.	Rain800	
cited_context	RainCityscapes	https://doi.org/10.48550/arXiv.2312.16610 (2023)	https://doi.org/10.1109/CVPR.2019.00821 (2019)	The RainCityscapes dataset is used to evaluate and compare single-image rain removal techniques, focusing on performance metrics such as FLOPs, number of parameters, PSNR, and SSIM. It is particularly useful for assessing the effectiveness of the MoFME model, which has shown improved results over prior models in rain removal tasks. The dataset enables researchers to rigorously test and benchmark different rain removal algorithms.		
cited_context | citing_context	Raindrop	https://doi.org/10.48550/arXiv.2506.16960 (2025), https://doi.org/10.48550/arXiv.2410.15067 (2024), https://doi.org/10.1109/TCSVT.2023.3299324 (2024) (+6), https://doi.org/10.48550/arXiv.2410.15067 (2024), https://doi.org/10.1109/TIP.2024.3368961 (2023), https://doi.org/10.1109/TCSVT.2023.3299324 (2024) (+6)	https://doi.org/10.1109/CVPR.2018.00263 (2017), https://doi.org/10.1109/CVPR.2019.00173 (2019)	The RainDrop dataset is primarily used for raindrop removal from images, both in synthetic and real-world settings. It is employed to train and evaluate algorithms that aim to eliminate raindrops, focusing on the effectiveness of these methods in restoring image quality. The dataset includes realistic raindrop effects, enabling researchers to assess the performance of restoration techniques on images with varying raindrop conditions.; The RainDrop dataset is primarily used for training and evaluating raindrop removal algorithms, focusing on enhancing image clarity by removing raindrops from images. It includes both real and synthetic raindrop images, which are used to train models such as AttentiveGAN and other generative models. The dataset enables researchers to address the challenge of realistic raindrop effects, improving visibility and image quality in rainy conditions.	RainDrop	
cited_context | citing_context	Raindrop Datasets	https://doi.org/10.48550/arXiv.2310.10123 (2023), https://doi.org/10.48550/arXiv.2504.05135 (2025), https://doi.org/10.48550/arXiv.2310.10123 (2023), https://doi.org/10.1109/ACCESS.2025.3526168 (2023), https://doi.org/10.1109/ICCV48922.2021.00231 (2021) (+2)	https://doi.org/10.1109/CVPR.2017.35 (2016)	The Raindrop datasets are used for evaluating raindrop removal algorithms, specifically to restore images degraded by raindrops. Containing 58 images, the dataset is designed to assess the effectiveness of these algorithms in removing raindrops, thereby enhancing image quality. This enables researchers to compare and improve the performance of different raindrop removal techniques.; The Raindrop datasets are primarily used for raindrop and rain streak removal from images, focusing on realistic raindrop effects and enhancing image clarity. These datasets are employed to train and evaluate models, such as SPAIR, in deraining tasks, specifically assessing their performance in removing raindrop artifacts and improving visual quality.	Raindrop datasets	
cited_context | citing_context	Raindrop Test Dataset	https://doi.org/10.1109/CVPR52688.2022.00239 (2021)	https://doi.org/10.1109/CVPR.2018.00263 (2017)	The RainDrop test dataset is used for evaluating the performance of image restoration models, specifically focusing on the task of removing raindrops from images. It tests the effectiveness of restoration methods by providing a benchmark for assessing model accuracy and efficiency in this domain.; The RainDrop test dataset is used for evaluating the performance of trained models, particularly attentive generative adversarial networks, in removing raindrops from images. It assesses the effectiveness of these methods in enhancing visual clarity and preserving image details. This dataset enables researchers to rigorously test and compare different raindrop removal techniques.	RainDrop test dataset	
citing_context	Raindrop-A	https://doi.org/10.1109/TMM.2025.3535316 (2024)	https://doi.org/10.1109/CVPR.2017.183 (2016)	The Raindrop-A dataset is used to test and evaluate raindrop removal algorithms, focusing on the impact of raindrops on image quality and the effectiveness of restoration methods. It features images with realistic raindrop effects, enabling researchers to assess the performance of techniques designed to remove raindrops from degraded images.		
cited_context	RainDrop-Test	https://doi.org/10.48550/arXiv.2305.17863 (2023)	https://doi.org/10.1109/CVPR.2019.00173 (2019)	The RainDrop-Test dataset is used for evaluating raindrop removal models, particularly under heavy rain conditions. Researchers employ this dataset to test and assess the quality of image restoration techniques, focusing on the effectiveness of removing raindrops from images. This dataset enables the rigorous evaluation of restoration algorithms by providing challenging scenarios with dense raindrops.		
cited_context	Raindrop800	https://doi.org/10.48550/arXiv.2203.06074 (2022)	https://doi.org/10.1109/CVPR.2018.00182 (2018)	The Raindrop800 dataset is primarily used for evaluating and training de-raining algorithms, with a specific focus on raindrop effects. It is employed to enhance image clarity and visibility in rainy scenes by removing raindrops, improving PSNR by 0.84dB. The dataset features realistic raindrop patterns, enabling researchers to assess and improve the performance of rain removal models in real-world scenarios.		
cited_context | citing_context	Rainds	https://doi.org/10.48550/arXiv.2506.16960 (2025), https://doi.org/10.48550/arXiv.2411.17687 (2024), https://doi.org/10.1109/TPAMI.2023.3238179 (2022)	https://doi.org/10.1109/CVPR.2017.35 (2016), https://doi.org/10.1109/CVPR46437.2021.00903 (2021)	RainDS is primarily used for raindrop and rain removal research, providing a diverse set of synthetic and real rainy images. It is employed to train and evaluate algorithms, assessing their performance in removing raindrops and rain from images. The dataset's variety of rainy conditions enables researchers to rigorously test and improve image restoration techniques.; The RainDS dataset is used to evaluate the effectiveness of raindrop removal methods in image restoration. It consists of 97 real-world test images, providing practical test cases to assess the performance of restoration techniques. This dataset enables researchers to validate their algorithms against real raindrop distortions, ensuring robustness and reliability in various conditions.	RainDS	
citing_context	RainDS-syn	https://doi.org/10.1109/TMM.2025.3535316 (2024)	https://www.semanticscholar.org/paper/c7798bbbcd2e85593a2b2d37b92dabac63420cd8 (2018)	The RainDS-syn dataset is used to evaluate rain and raindrop removal techniques, featuring synthetic images with both rain and raindrop effects. Researchers employ this dataset to test and validate algorithms designed to restore images degraded by rain, focusing on the effectiveness of removing both rain streaks and raindrops. This dataset enables the assessment of image restoration methods under controlled conditions, providing a benchmark for comparing different approaches.		
citing_context	RainKITTI2012	https://doi.org/10.1109/TCSVT.2024.3519352 (2025), https://doi.org/10.1109/TCSVT.2024.3516074 (2024)	https://doi.org/10.48550/arXiv.2212.11548 (2022)	The RainKITTI2012 dataset is utilized to test and improve image restoration algorithms, particularly in rainy conditions. It focuses on synthetic rain effects and is used as an unseen dataset to evaluate deraining algorithms, especially in the context of stereo image deraining with semantic understanding. This dataset enables researchers to assess the performance and robustness of deraining techniques under controlled synthetic rain scenarios.		
cited_context	RDD	https://doi.org/10.1109/CVPR52733.2024.01482 (2024)	https://doi.org/10.1109/ICIP46576.2022.9897217 (2022)	The RDD dataset is used to train models for image restoration tasks, specifically contributing 4,371 real images to the training set. It is also utilized to train BGSNet for document shadow removal, emphasizing real-world scenarios and high-resolution images. This dataset enables researchers to develop and refine algorithms that enhance image quality and remove shadows in documents, leveraging its large, realistic image collection.		
cited_context	real Snow100k dataset (Snow100k-R)	https://doi.org/10.48550/arXiv.2407.04621 (2024)	https://doi.org/10.1109/TIP.2018.2806202 (2017)	The real Snow100k dataset (Snow100k-R) is used for image desnowing research, specifically to assess the effectiveness of desnowing algorithms on real snow-covered scenes. This dataset enables researchers to evaluate and improve desnowing techniques by providing a realistic benchmark for testing and validation.		
citing_context	Real-ESRGAN	https://doi.org/10.48550/arXiv.2406.18242 (2024)	https://www.semanticscholar.org/paper/49b64383fe36268410c430352637ed23b16820c5 (2019)	The Real-ESRGAN dataset is used to generate degraded images for training and evaluation, focusing on various degradation configurations. This enhances the robustness of image restoration models. The dataset's ability to simulate diverse degradations is crucial for improving model performance across different scenarios. It enables researchers to test and refine algorithms specifically designed to handle complex image restoration challenges.		
cited_context	real-world de-snowing dataset	https://doi.org/10.48550/arXiv.2412.00878 (2024)	https://doi.org/10.1109/TIP.2018.2867951 (2017)	The real-world de-snowing dataset is used to evaluate de-snowing algorithms by assessing their performance on practical, real-world images. This dataset enables researchers to focus on the effectiveness of these algorithms in realistic scenarios, ensuring that they can handle the complexities and variations present in actual snowy conditions. The dataset's emphasis on real-world images provides a robust testbed for algorithmic improvements and performance validation.		
cited_context	real-world dehazing dataset	https://doi.org/10.1109/CVPR52729.2023.02083 (2023), https://doi.org/10.48550/arXiv.2412.00878 (2024)	https://doi.org/10.1109/CVPR46437.2021.00912 (2021)	The real-world dehazing dataset is used to evaluate and train de-hazing algorithms, focusing on real-world images to assess performance in practical scenarios. It provides visual samples for dehazing tasks, enabling comparisons with other datasets like RealSnow. The dataset enhances image clarity and visual quality, specifically supporting research in dehazing and image restoration.		
cited_context	Real-World Noisy Image Dataset	https://doi.org/10.1109/CVPR.2019.00717 (2019)	https://www.semanticscholar.org/paper/fb52f9294fa1cd83660eccac486367eac01ea32d (2018)	The Real-World Noisy Image Dataset is used to evaluate and test denoising algorithms on real-world noisy images, particularly focusing on practical performance. It assesses models like DuRN-P on 40 pairs of noisy and mean images captured by a CMOS camera, emphasizing authentic noise patterns. This dataset enables researchers to validate the effectiveness of denoising techniques in realistic scenarios.		
cited_context	real-world rainy images	https://doi.org/10.1109/TIP.2024.3501855 (2024)	https://doi.org/10.1007/978-3-031-20071-7_42 (2022)	The 'real-world rainy images' dataset is used to train a novel deraining network, specifically focusing on enhancing the robustness and generalization of the model in real-world scenarios. This dataset enables researchers to address the challenge of removing rain artifacts from images, improving image quality and usability in various applications.		
cited_context | citing_context	Real-World Task-Driven Testing Set (Rtts)	https://doi.org/10.1109/TCSVT.2024.3519352 (2025), https://doi.org/10.1109/ICCV51070.2023.01983 (2023)	https://doi.org/10.1109/TIP.2018.2867951 (2017), https://doi.org/10.1109/TIP.2018.2806202 (2017)	The Real-world Task-driven Testing Set (RTTS) is used for evaluating dehazing models on real-world images, with a focus on task-specific performance in practical applications. This dataset enables researchers to assess how well dehazing algorithms function in real-world scenarios, ensuring that the models are effective beyond controlled environments. The dataset's emphasis on real-world conditions enhances the validity and applicability of the research findings.; The Real-world Task-driven Testing Set (RTTS) is used for evaluating image restoration techniques on real-world degraded images. It focuses on task-driven performance metrics, enabling researchers to assess the effectiveness of restoration methods in practical scenarios. This dataset provides a benchmark for comparing different restoration algorithms under realistic conditions.	Real-world Task-driven Testing Set (RTTS)	
cited_context	Real20	https://doi.org/10.1109/TCSVT.2023.3286405 (2024)	https://doi.org/10.1109/CVPR.2018.00503 (2018)	The Real20 dataset is used to evaluate image restoration methods by providing 20 real image pairs from diverse scenes. Researchers focus on assessing the performance of these methods across various visual contexts, ensuring robustness and effectiveness in real-world applications. This dataset enables detailed performance comparisons and validation of restoration techniques in complex, realistic scenarios.		
cited_context	real3	https://doi.org/10.48550/arXiv.2308.15070 (2023)	https://doi.org/10.1109/TIP.2018.2839891 (2017)	The real3 dataset is used for the BID task, contributing to a mixed real-world dataset for comprehensive image restoration evaluation, particularly focusing on image denoising. It enables researchers to assess the effectiveness of image restoration techniques in real-world scenarios, providing a robust benchmark for evaluating denoising performance.		
cited_context	real47	https://doi.org/10.48550/arXiv.2308.15070 (2023)	https://doi.org/10.1109/ICCV.2019.00318 (2019)	The real47 dataset is used to evaluate the performance of models on real-world single image super-resolution tasks, specifically focusing on metric scores. It is employed in the evaluation of the DiffBIR and BSR models, with an emphasis on assessing their effectiveness in enhancing real-world images. The dataset's real-world characteristics make it suitable for benchmarking super-resolution techniques.		
cited_context	real9	https://doi.org/10.48550/arXiv.2308.15070 (2023)	https://doi.org/10.1109/TIP.2018.2839891 (2017)	The real9 dataset is used for the BID task, primarily to provide additional real-world images for mixed dataset evaluation and to focus on image denoising in real-world scenarios. It enhances the robustness of models by incorporating diverse and realistic image conditions, enabling more accurate performance assessments in practical applications.		
cited_context | citing_context	Realblur	https://doi.org/10.48550/arXiv.2506.16960 (2025), https://doi.org/10.48550/arXiv.2410.15385 (2024), https://doi.org/10.1109/CVPR52733.2024.00277 (2023), https://doi.org/10.48550/arXiv.2208.05244 (2022), https://doi.org/10.1109/TPAMI.2023.3330416 (2023), https://doi.org/10.48550/arXiv.2211.07317 (2022) (+2)	https://doi.org/10.1109/CVPR52729.2023.02083 (2023), https://doi.org/10.1109/CVPR.2017.35 (2016)	The RealBlur dataset is primarily used for motion deblurring research, focusing on real-world motion blur in images. It consists of authentic out-of-focus and motion-blurred images captured under various conditions. Researchers use this dataset to test and assess the effectiveness of deblurring algorithms, emphasizing sharpness and detail recovery, and improving overall image clarity.; The RealBlur dataset is primarily used for training and evaluating deblurring methods, particularly in real-world scenarios. It provides a realistic testbed with authentic, motion-blurred images, enabling researchers to assess the robustness, generalization, and effectiveness of deblurring algorithms. The dataset features varying degrees of blur, enhancing the reliability of benchmarking and experimental results in image motion deblurring.	RealBlur	
cited_context	RealBlur dataset	https://doi.org/10.48550/arXiv.2208.05244 (2022)	https://doi.org/10.1109/CVPR.2017.35 (2016)	The RealBlur dataset is used to train deblurring generators, specifically focusing on real-world blur scenarios. This enhances the performance of deblurring algorithms by providing realistic training data. The dataset's emphasis on authentic blur conditions enables researchers to develop more effective and robust deblurring techniques.		
cited_context | citing_context	Realblur-J	https://doi.org/10.1109/CVPR52733.2024.00277 (2023), https://doi.org/10.48550/arXiv.2506.16960 (2025), https://doi.org/10.48550/arXiv.2406.18242 (2024), https://doi.org/10.48550/arXiv.2306.13653 (2023), https://doi.org/10.1109/CVPR52733.2024.00277 (2023), https://doi.org/10.48550/arXiv.2310.11881 (2023) (+3)	https://doi.org/10.1109/ICCV.2019.00567 (2019)	The RealBlur-J dataset is primarily used for evaluating motion deblurring algorithms, containing real-world blurred images with ground truth. It is employed to test the robustness and effectiveness of proposed methods in practical scenarios, focusing on improving image clarity and quality in real-world conditions. This dataset enables researchers to assess the performance of their algorithms on authentic, challenging data, ensuring that the solutions are viable for real-world applications.; The RealBlur-J dataset is primarily used to evaluate deblurring performance on real-world images, focusing on robustness and generalization to various challenging conditions such as low light, JPEG compression, and different types of blur. It is employed to compare proposed methods against state-of-the-art techniques, enhancing deblurring robustness and assessing zero-shot generalization capabilities. The dataset's real-world blurred images with varying degrees of blur and noise make it suitable for benchmarking and testing the effectiveness of deblurring algorithms in practical scenarios.	RealBlur-J	
cited_context | citing_context	Realblur-R	https://doi.org/10.1109/CVPR52733.2024.00277 (2023), https://doi.org/10.48550/arXiv.2406.18242 (2024), https://doi.org/10.1109/CVPR52729.2023.01753 (2023), https://doi.org/10.48550/arXiv.2306.13653 (2023), https://doi.org/10.1109/CVPR52733.2024.00277 (2023) (+4)	https://doi.org/10.1109/ICCV.2019.00567 (2019)	The RealBlur-R dataset is used for evaluating motion deblurring algorithms on real-world images, assessing the robustness and effectiveness of these methods in practical scenarios. It contains real-world blurred images with ground truth, enabling researchers to test and validate their algorithms against realistic conditions.; The RealBlur-R dataset is primarily used to evaluate deblurring performance on real-world images, focusing on robustness, generalization, and practical applicability. It contains real-world blurred images captured under various conditions, including out-of-focus and motion blur, which are used to assess the effectiveness and adaptability of deblurring algorithms. The dataset enables researchers to test and improve the accuracy and zero-shot generalization capabilities of their methods, often using performance metrics such as PSNR and SSIM.	RealBlur-R	
cited_context	RealDAE	https://doi.org/10.1109/CVPR52733.2024.01482 (2024)	https://doi.org/10.1109/CVPR.2018.00494 (2018)	The RealDAE dataset is used in image restoration research, specifically for evaluating and training models on real-world degraded images. It contributes 450 real-world images to the training process, enhancing the model's performance on actual data. The dataset is crucial for assessing the effectiveness of image restoration methods on authentic, challenging test cases.		
cited_context	RealInt	https://doi.org/10.48550/arXiv.2404.12091 (2024)	https://doi.org/10.1109/CVPR.2017.183 (2016)	The RealInt dataset is used to evaluate de-raining techniques on real-world rainy images. Researchers focus on the practical effectiveness of feature extractors by testing their performance on this dataset. This enables the assessment of how well these methods can remove rain artifacts and improve image quality in real-world conditions.		
cited_context	RealIR	https://doi.org/10.48550/arXiv.2412.00878 (2024)	https://doi.org/10.1109/ICCV48922.2021.00510 (2021)	The RealIR dataset is used as a test set without ground truth for evaluating image restoration methods. It employs non-reference metrics aligned with human perception to assess the performance of these methods. This enables researchers to validate the effectiveness of image restoration techniques in scenarios where ground truth images are unavailable, focusing on perceptual quality rather than pixel-level accuracy.		
cited_context	RealPhoto60	https://doi.org/10.1007/978-3-031-73661-2_25 (2023)	https://doi.org/10.1109/ICCV48922.2021.00510 (2021)	The RealPhoto60 dataset is used as a real-world test set to evaluate the impact of semantic and restoration prompts on image quality. Researchers employ non-reference metrics to assess these influences, focusing on how different prompts affect the restoration outcomes. This dataset enables the evaluation of image restoration techniques in practical scenarios, providing insights into the effectiveness of various prompts.		
citing_context	RealRain-1k-L	https://doi.org/10.48550/arXiv.2408.15994 (2024)	https://doi.org/10.48550/arXiv.2206.05514 (2022)	The RealRain-1k-L dataset is used to test rain removal algorithms, specifically focusing on their performance in real-world rainy scenes. Researchers employ this dataset to evaluate the effectiveness of deraining methodologies, ensuring they can handle authentic rain conditions. This dataset enables the assessment of algorithmic robustness and reliability in practical deraining applications.		
cited_context	RealSet80	https://doi.org/10.1109/TPAMI.2024.3461721 (2024)	https://doi.org/10.1109/ICCV.2017.355 (2017)	RealSet80 is used to evaluate and develop image restoration methods, particularly focusing on low-quality and real-world images. The dataset supports the testing of blind image restoration techniques and enhances the quality of images commonly referenced in recent literature. It enables researchers to assess the effectiveness of various restoration approaches in practical scenarios.		
cited_context | citing_context	Realsnow	https://doi.org/10.48550/arXiv.2506.16960 (2025), https://doi.org/10.48550/arXiv.2412.20157 (2024), https://doi.org/10.48550/arXiv.2410.15067 (2024) (+1), https://doi.org/10.48550/arXiv.2410.15067 (2024), https://doi.org/10.1145/3664647.3680762 (2024), https://doi.org/10.1109/CVPR52729.2023.02083 (2023)	https://doi.org/10.1109/CVPR52729.2023.02083 (2023), https://doi.org/10.1109/CVPR46437.2021.00912 (2021)	The RealSnow dataset is primarily used for evaluating and testing desnowing methods in image restoration. It provides real-world snow images to assess the effectiveness of algorithms in removing snow artifacts, focusing on clarity and texture preservation. Researchers use this dataset to evaluate the performance of desnowing techniques, ensuring they can handle realistic snow conditions and improve image quality.; The RealSnow dataset is primarily used for image desnowing, focusing on the evaluation and training of image restoration models to remove snow artifacts from real-world images. It enables researchers to assess the performance and effectiveness of snow removal algorithms, enhancing image clarity and restoration quality in snowy scenes.	RealSnow	
cited_context | citing_context	Realsr	https://doi.org/10.48550/arXiv.2503.09403 (2025), https://doi.org/10.48550/arXiv.2308.15070 (2023), https://doi.org/10.48550/arXiv.2308.09388 (2023), https://doi.org/10.1007/978-3-030-58595-2_30 (2020)	https://doi.org/10.1109/CVPR.2018.00182 (2018), https://doi.org/10.1109/ICCV.2019.00318 (2019)	The RealSR dataset is used for real-world super-resolution tasks, specifically to test the effectiveness of restoration algorithms. It consists of 15 paired images, enabling researchers to evaluate how well these algorithms perform in enhancing image resolution in practical scenarios. This dataset facilitates the development and assessment of super-resolution techniques by providing realistic test cases.; The RealSR dataset is primarily used for evaluating and comparing image super-resolution methods, particularly focusing on real-world data. It supports the assessment of natural image enhancement, detail preservation, and out-of-distribution generalization. Researchers use it to quantitatively evaluate the performance of various models, including CNN-based, transformer-based, and diffusion models, on single image super-resolution tasks. The dataset provides training and test image pairs at scale factors 2, 3, and 4, enabling comprehensive evaluation of restoration quality and model generalization to unseen data.	RealSR	
cited_context	RealSR-V3	https://doi.org/10.1109/TPAMI.2024.3461721 (2024)	https://doi.org/10.48550/arXiv.2207.12396 (2022)	The RealSR-V3 dataset is used for evaluating image restoration approaches, particularly focusing on real-world super-resolution performance and quality. Researchers employ this dataset to assess and compare the effectiveness of different super-resolution techniques, ensuring they perform well under realistic conditions. This dataset enables the validation of algorithms designed to enhance image clarity and detail in practical scenarios.		
citing_context	RealSR-Val	https://doi.org/10.48550/arXiv.2503.09403 (2025)	https://doi.org/10.1109/CVPR.2018.00182 (2018)	The RealSR-Val dataset is used for validating image super-resolution models, specifically focusing on real-world low-resolution (LQ) and high-resolution (HR) image pairs. It enables researchers to assess the performance of super-resolution techniques in realistic scenarios, ensuring that models generalize well to real-world conditions. This dataset is crucial for evaluating the effectiveness of super-resolution algorithms in handling authentic LQ-HR pairs.		
cited_context	RealSRSet	https://doi.org/10.48550/arXiv.2308.15070 (2023), https://doi.org/10.1109/ICCVW54120.2021.00210 (2021)	https://doi.org/10.1109/CVPRW50498.2020.00241 (2020)	The RealSRSet dataset is primarily used for evaluating and comparing real-world super-resolution methods, focusing on visual quality, performance, and metric scores. It provides a diverse set of real-world images, enabling comprehensive testing of algorithms like SwinIR and DiffBIR, particularly at ×4 scale. The dataset supports research on low-quality image synthesis, degradation models, and techniques such as kernel estimation and noise injection.		
cited_context	RED	https://www.semanticscholar.org/paper/3d3c5fcbc40aadccceda58d3d9c5cd00588ea0b7 (2022)	https://doi.org/10.1137/16M1102884 (2016)	The RED dataset is used for image restoration experiments, focusing on denoising and regularization techniques to enhance image quality and reduce noise. Researchers employ this dataset to test and validate algorithms that improve visual clarity, leveraging its characteristics to address specific challenges in image processing.		
cited_context	REDS	https://doi.org/10.48550/arXiv.2308.09388 (2023), https://doi.org/10.1109/CVPRW53098.2021.00027 (2021), https://doi.org/10.1109/TMM.2024.3407656 (2024) (+2)	https://doi.org/10.1109/CVPR.2017.35 (2016)	The REDS dataset is primarily used for training and evaluating video deblurring methods, with a focus on handling realistic motion blur in video sequences. It is utilized to improve the representation capability of image restoration networks, particularly in addressing complex degradations such as motion blur and JPEG compression artifacts. The dataset enables researchers to assess performance metrics and validate the effectiveness of their models, including the 3× speedup in dynamic scene deblurring and the use of HIN in video deblurring.		
citing_context	REDS dataset	https://doi.org/10.48550/arXiv.2411.18412 (2024)	https://doi.org/10.1109/CVPR.2017.35 (2016)	The REDS dataset is used to evaluate methods for handling blur and JPEG artifacts in image restoration. Research focuses on assessing the effectiveness of approaches in managing compression artifacts, utilizing the dataset's characteristics to test and validate restoration techniques. This enables researchers to improve image quality in scenarios involving both blurring and compression.		
cited_context	REDS-val-300	https://doi.org/10.1109/CVPRW53098.2021.00027 (2021)	https://doi.org/10.1109/CVPRW53098.2021.00025 (2021)	The REDS-val-300 dataset is used for evaluating image deblurring methods, specifically focusing on the performance of algorithms on a subset of 300 validation images. This dataset enables researchers to assess the effectiveness of deblurring techniques by providing a standardized set of images for benchmarking.		
cited_context	RENOIR	https://doi.org/10.1109/TNNLS.2021.3131739 (2020), https://doi.org/10.1109/TMM.2021.3063916 (2021)	https://doi.org/10.1109/CVPR.2019.01129 (2018)	The RENOIR dataset is used to evaluate image restoration methods, particularly for denoising images corrupted by realistic noise. It provides real noisy images, typically cropped into 512 × 512 pixel patches, which are used to enhance the quality of visual data. This dataset enables researchers to test and compare the effectiveness of various image restoration techniques under realistic conditions.		
cited_context | citing_context	Reside	https://doi.org/10.48550/arXiv.2410.15385 (2024), https://doi.org/10.1109/WACV61041.2025.00069 (2025), https://doi.org/10.48550/arXiv.2506.16960 (2025) (+18), https://doi.org/10.48550/arXiv.2308.09388 (2023), https://doi.org/10.48550/arXiv.2409.19403 (2024), https://doi.org/10.1109/CVPR52688.2022.01693 (2022) (+18)	https://doi.org/10.1109/TIP.2018.2867951 (2017)	The RESIDE dataset is primarily used for training and evaluating models focused on image dehazing. It provides a comprehensive benchmark for assessing dehazing techniques, including single-degradation scenarios, using metrics like PSNR, MS-SSIM, and BD-PSNR. The dataset contains hazy images that help optimize models such as TANet and Restormer+EVC to improve visibility and clarity in both synthetic and real-world hazy conditions.; The RESIDE dataset is primarily used for training and evaluating single-image dehazing models. It includes diverse indoor and outdoor scenes, synthetic daytime scenes, and paired images, which are crucial for benchmarking performance across various weather conditions. The dataset enables researchers to focus on improving image clarity, color fidelity, and overall quality in hazy conditions, making it a valuable resource for dehazing research.	RESIDE	
cited_context	RESIDE Real-world Task-driven Testing Set (RESIDE-RTTS)	https://doi.org/10.48550/arXiv.2407.04621 (2024)	https://doi.org/10.1109/TIP.2016.2639450 (2017)	The RESIDE Real-world Task-driven Testing Set (RESIDE-RTTS) is primarily used for dehazing tasks, providing real-world images with varying degrees of haze. Researchers employ this dataset to test and benchmark dehazing algorithms, focusing on the effectiveness and performance of these algorithms in realistic conditions. The dataset's real-world images enable robust evaluation and comparison, facilitating advancements in image restoration techniques specifically for dehazing.		
cited_context	RESIDE Real-world Task-driven Testing Set (RTTS)	https://doi.org/10.48550/arXiv.2407.04621 (2024)	https://doi.org/10.1109/TIP.2018.2806202 (2017)	The RESIDE Real-world Task-driven Testing Set (RTTS) is primarily used for evaluating image dehazing algorithms. It provides real-world images with varying levels of haze, enabling researchers to assess the performance of dehazing techniques in practical scenarios. This dataset supports the development and refinement of algorithms by offering diverse and realistic test cases, ensuring that the methods can handle a wide range of hazy conditions.		
cited_context	RESIDE SOTS-Indoor	https://www.semanticscholar.org/paper/caabf4abb96ee83f650aa7b707bd9a99007b3e74 (2023)	https://doi.org/10.1109/TIP.2018.2867951 (2017)	The RESIDE SOTS-Indoor dataset is primarily used for synthetic image dehazing experiments, focusing on indoor scenes. Researchers employ this dataset to evaluate the performance of dehazing algorithms, leveraging its synthetic nature to provide controlled conditions for testing and validation. This enables precise assessment of algorithmic effectiveness in enhancing image clarity in hazy indoor environments.		
cited_context | citing_context	Reside-6K	https://doi.org/10.1109/TMM.2025.3535316 (2024), https://doi.org/10.48550/arXiv.2503.10120 (2025), https://doi.org/10.48550/arXiv.2306.13653 (2023)	https://doi.org/10.1109/CVPR.2017.183 (2016), https://www.semanticscholar.org/paper/c7798bbbcd2e85593a2b2d37b92dabac63420cd8 (2018)	The RESIDE-6K dataset is primarily used to assess and evaluate haze removal methods in image restoration. It contains 6,000 pairs of hazy and clear images, enabling researchers to test the effectiveness and robustness of dehazing techniques. This large-scale dataset supports the development and evaluation of algorithms designed to restore images degraded by haze, ensuring they perform well under various conditions.; The RESIDE-6k dataset is primarily used for image dehazing research, enhancing image clarity by removing haze effects. It provides a large set of diverse real-world hazy images, enabling the training and evaluation of dehazing algorithms. The dataset supports comprehensive testing and assessment of these algorithms under various atmospheric conditions, facilitating advancements in image restoration techniques.	RESIDE-6K, RESIDE-6k	
cited_context | citing_context	Reside-Ots	https://doi.org/10.48550/arXiv.2410.15067 (2024), https://doi.org/10.48550/arXiv.2402.03738 (2024), https://doi.org/10.48550/arXiv.2410.15067 (2024), https://doi.org/10.48550/arXiv.2407.04621 (2024), https://doi.org/10.1109/TIV.2023.3347952 (2024)	https://doi.org/10.1109/CVPR52729.2023.00564 (2023), https://doi.org/10.1007/978-3-031-72764-1_1 (2024)	The RESIDE-OTS dataset is primarily used to evaluate the performance of image restoration models, particularly in dehazing outdoor synthetic scenes. It provides a variety of synthetic outdoor images with different levels of haze, enabling researchers to test and compare the effectiveness of dehazing algorithms. The dataset supports the assessment of models' ability to restore clear images from hazy conditions, making it a valuable resource for advancing dehazing techniques in image restoration.; The RESIDE-OTS dataset is primarily used to evaluate and assess dehazing and image restoration techniques, particularly for outdoor synthetic scenes. It focuses on evaluating the performance of dehazing algorithms, deraining methods, and all-in-one image restoration models under various degradations such as haze, rain, noise, blur, and low-light conditions. The dataset serves as a benchmark for quantitatively and qualitatively measuring the effectiveness of these techniques in improving image quality and clarity.	RESIDE-OTS	
cited_context | citing_context	Reside-Sots	https://doi.org/10.48550/arXiv.2410.15067 (2024), https://www.semanticscholar.org/paper/4af9a92a556981375dab533ccc55860f01c8b5da (2025), https://doi.org/10.48550/arXiv.2410.15067 (2024), https://doi.org/10.1109/CVPR.2019.00717 (2019)	https://doi.org/10.1109/CVPR52729.2023.00564 (2023), https://doi.org/10.1007/978-3-031-72764-1_1 (2024)	The RESIDE-SOTS dataset is primarily used for evaluating dehazing algorithms and image restoration models in outdoor scenes, both synthetic and real-world. It assesses the performance and robustness of these models under various conditions, including realistic haze effects and other degradations like rain and low-light. This dataset enables researchers to benchmark and compare different dehazing methods, focusing on the quality and effectiveness of haze removal in outdoor environments.; The RESIDE-SOTS dataset is primarily used to evaluate and benchmark image restoration techniques, particularly focusing on dehazing and deraining. It is utilized to assess the performance of algorithms in both real-world and synthetic outdoor scenes, emphasizing realistic haze removal and image quality. The dataset supports research by providing a standardized set of images with controlled degradations, enabling consistent evaluation of restoration models.	RESIDE-SOTS	
cited_context | citing_context	Reside-Β	https://doi.org/10.48550/arXiv.2408.15994 (2024), https://doi.org/10.1109/TCSVT.2024.3429557 (2024), https://doi.org/10.1109/TCSVT.2024.3429557 (2024)	https://doi.org/10.1109/CVPR.2017.183 (2016), https://doi.org/10.1007/978-3-030-01234-2_16 (2018)	The RESIDE-β dataset is used for training models in image deraining and dehazing. It contains 72,135 image pairs, which are utilized to enhance model performance in removing rain and improving visibility in hazy images. This dataset enables researchers to develop and refine algorithms that address specific image degradation issues, focusing on practical applications in image restoration.; The RESIDE-β dataset is primarily used for training deep learning models in image dehazing, aimed at enhancing the visibility and quality of hazy images. This dataset supports research focused on developing and refining algorithms to remove haze, leveraging its extensive collection of hazy and clear image pairs. The dataset's characteristics enable researchers to evaluate and improve the performance of dehazing models, contributing to advancements in image restoration techniques.	RESIDE-β	
cited_context	RESIDE/SOTS	https://www.semanticscholar.org/paper/3fb08a1c07eba3461d6ed784b25d952979e90e0d (2023)	https://www.semanticscholar.org/paper/3c5ab1aba2091c690729b5a8f0762ecda3d06163 (2021)	The RESIDE/SOTS dataset is used to evaluate the performance of IRNeXt on synthetic dehazing tasks. Researchers focus on metrics such as PSNR to assess improvements in image quality. This dataset enables the systematic evaluation of dehazing algorithms, providing synthetic images that simulate various atmospheric conditions, thus facilitating the comparison of different restoration techniques.		
citing_context	RESIDED-OTS	https://doi.org/10.48550/arXiv.2402.03738 (2024)	https://doi.org/10.1109/TITS.2016.2634580 (2016)	The RESIDED-OTS dataset is primarily used for enhancing and restoring images of land and underwater scenes. It incorporates depth information to improve image quality, detail, and contrast. Specifically, it supports techniques such as dual prior optimized contrast enhancement for underwater images and low-light image restoration for land scenes. This dataset enables researchers to focus on advanced image restoration methods, enhancing visibility and color accuracy in challenging environments.		
cited_context	RESIDE’s Synthetic Objective Testing Set (SOTS) outdoor	https://doi.org/10.48550/arXiv.2409.00263 (2024)	https://doi.org/10.1109/TIP.2018.2867951 (2017)	The RESIDE’s Synthetic Objective Testing Set (SOTS) outdoor dataset is used for evaluating de-hazing algorithms. It consists of 500 paired images, enabling researchers to assess algorithm performance in outdoor conditions. This dataset facilitates the testing and comparison of de-hazing techniques by providing a standardized set of images, ensuring consistent evaluation across different studies.		
cited_context | citing_context	Revide	https://doi.org/10.48550/arXiv.2411.17687 (2024), https://doi.org/10.48550/arXiv.2410.15067 (2024), https://doi.org/10.48550/arXiv.2505.18047 (2025) (+2), https://doi.org/10.48550/arXiv.2410.15067 (2024), https://doi.org/10.1145/3664647.3680762 (2024), https://doi.org/10.1109/CVPR52729.2023.02083 (2023)	https://doi.org/10.1109/CVPR46437.2021.00912 (2021)	The REVIDE dataset is primarily used for evaluating and comparing methods in image and video restoration, specifically focusing on rain and haze removal. It is utilized to test the effectiveness of dehazing and deraining techniques in both static images and video sequences, enabling researchers to assess performance in real-world scenarios. The dataset includes dynamic scene data, which is crucial for training and evaluating deblurring models.; The REVIDE dataset is used for training and evaluating image restoration models, primarily focusing on rain and haze removal. It is utilized to test methods for dehazing and improving visibility in hazy images, as well as removing rain artifacts from real-world videos. The dataset supports the synthesis of realistic rain and haze conditions, enabling researchers to assess the effectiveness of restoration techniques in these challenging scenarios.	REVIDE	
cited_context	RID	https://doi.org/10.1109/ICCV51070.2023.01983 (2023)	https://doi.org/10.1109/TIP.2018.2806202 (2017)	The RID dataset is used for real-world experimental analysis in snow removal, incorporating both real and synthetic images to evaluate model performance. It provides a diverse set of images that enable researchers to assess the effectiveness of image restoration techniques specifically in snow removal scenarios.		
cited_context	RNI15	https://doi.org/10.48550/arXiv.2308.15070 (2023), https://doi.org/10.1109/TNNLS.2021.3131739 (2020)	https://doi.org/10.1109/TIP.2018.2839891 (2017)	The RNI15 dataset is primarily used for evaluating the BID task, particularly in the context of image denoising. It provides a variety of real-world images, including real noisy face images, which are used to assess the performance of denoising algorithms under realistic conditions. This dataset enables researchers to test and compare the effectiveness of different image restoration techniques in practical scenarios.		
cited_context	RO	https://doi.org/10.1109/TPAMI.2023.3330416 (2023)	https://doi.org/10.1109/CVPR.2017.35 (2016)	The RO dataset is used to evaluate image restoration models, focusing on performance and inference speed comparisons, particularly against Restormer. It enables researchers to assess and benchmark the effectiveness of different restoration techniques, ensuring that new models meet high standards in both quality and efficiency.		
citing_context	RS100K-L	https://doi.org/10.1109/TMM.2025.3535316 (2024)	https://doi.org/10.1109/CVPR.2017.183 (2016)	The RS100K-L dataset is used to evaluate image restoration techniques, specifically focusing on the removal of combined rain and snow degradations. It tests algorithms' ability to handle multiple degradations simultaneously, featuring images degraded by both rain and snow. This dataset enables researchers to assess the effectiveness of restoration methods in complex, real-world scenarios.		
cited_context	RSBlur	https://doi.org/10.1109/TPAMI.2023.3330416 (2023), https://www.semanticscholar.org/paper/3fb08a1c07eba3461d6ed784b25d952979e90e0d (2023), https://doi.org/10.1109/TPAMI.2024.3419007 (2024)	https://doi.org/10.1007/978-3-031-20071-7_29 (2022)	The RSBlur dataset is primarily used to evaluate and train image deblurring models, focusing on real-world scenarios and realistic blur synthesis. It assesses performance on naturally and motion-blurred images, emphasizing the trade-off between PSNR and model complexity. The dataset helps researchers test the robustness, generalization, and effectiveness of deblurring algorithms in practical conditions.		
citing_context	RSID	https://doi.org/10.3390/electronics13142817 (2024)	https://doi.org/10.1016/J.INS.2019.02.058 (2019)	The RSID dataset is used for comparing and evaluating image restoration methods, particularly in dehazing and restoration performance in remote sensing images. It provides 7000 pairs of degraded and clean images for training and 1000 pairs for testing, enabling researchers to assess the effectiveness of different restoration algorithms.		
citing_context	RSVD	https://doi.org/10.48550/arXiv.2411.17687 (2024)	https://doi.org/10.1109/CVPR.2017.35 (2016)	The RSVD dataset is primarily used for video deblurring research, providing real-world blurred video data. It enables the training and evaluation of deblurring models, focusing on enhancing the clarity of motion-blurred videos. The dataset's real-world characteristics make it valuable for developing robust deblurring algorithms.		
cited_context | citing_context	Rtts	https://doi.org/10.48550/arXiv.2408.15994 (2024), https://doi.org/10.1109/ICCV51070.2023.01983 (2023)	https://doi.org/10.1109/ACSSC.2011.6190099 (2011), https://doi.org/10.1109/TIP.2018.2806202 (2017)	The RTTS dataset is used for evaluating image restoration methods, particularly in scenarios where reference images are unavailable. It employs non-reference metrics for quantitative comparison, enabling researchers to assess the performance of restoration algorithms. Specifically, it is used to measure the effectiveness of haze removal techniques in real-world hazy conditions, providing a practical testbed for algorithmic improvements.; The RTTS dataset is used for real-world experimental analysis in snow removal, providing diverse scenarios to test the robustness of models. It enables researchers to evaluate and improve the performance of image restoration techniques under varied and challenging conditions, focusing on the practical application of these methods in real-world settings.	RTTS	
citing_context	RWBI	https://doi.org/10.1109/TCSVT.2024.3519352 (2025), https://doi.org/10.1109/TCSVT.2024.3516074 (2024)	https://doi.org/10.48550/arXiv.2212.11548 (2022)	The RWBI dataset is used to enhance and evaluate image restoration performance, particularly in real-world conditions involving complex degradations such as blur and varying illumination. It serves as an unseen benchmark for testing deblurring algorithms, providing realistic scenarios to assess and improve algorithm robustness.		
cited_context	SA-1B	https://doi.org/10.1109/CVPR52733.2024.02425 (2024)	https://doi.org/10.48550/arXiv.2210.08402 (2022)	The SA-1B dataset is used to evaluate and compare image quality against other large datasets, highlighting its significance in image restoration tasks. Researchers employ this dataset to assess the effectiveness of different restoration methods, focusing on the importance of high-quality images in achieving better restoration outcomes. This dataset enables rigorous benchmarking and validation of image restoration techniques.		
cited_context	SATE HAZE 1K	https://doi.org/10.1109/TPAMI.2024.3419007 (2024)	https://doi.org/10.1007/978-3-030-01449-0_52 (2018)	The SATE HAZE 1K dataset is used to evaluate dehazing algorithms specifically for remote sensing images, including satellite and aerial imagery. Researchers employ this dataset to assess the performance of dehazing techniques, focusing on improving image clarity and quality in these types of images. The dataset's relevance lies in its application to enhance the visibility and usability of remote sensing data in various environmental and monitoring studies.		
cited_context | citing_context	Satehaze1K	https://doi.org/10.1109/TCSVT.2024.3429557 (2024), https://doi.org/10.1109/TCSVT.2024.3429557 (2024), https://doi.org/10.1109/TPAMI.2024.3419007 (2024)	https://doi.org/10.1109/WACV45572.2020.9093471 (2020)	The SateHaze1k dataset is primarily used for dehazing experiments in remote sensing, specifically for satellite image restoration. Researchers employ this dataset to improve image clarity using methods such as SAR image priors and conditional GANs. It is utilized to evaluate the performance of dehazing algorithms, particularly the Asimage dehazing model, in enhancing the quality of satellite imagery.; The SateHaze1k dataset is primarily used for evaluating and enhancing image restoration techniques in remote sensing, with a focus on haze removal and improving image clarity. It is utilized in dehazing experiments to assess the performance of various dehazing algorithms, particularly in satellite imagery. The dataset supports the application of models like Asimage and leverages SAR image priors in conditional GANs to enhance optical imagery quality.	SateHaze1k	
cited_context	SateHaze1k-Moderate	https://doi.org/10.1109/TPAMI.2024.3419007 (2024)	https://doi.org/10.1007/978-3-030-58610-2_28 (2020)	The SateHaze1k-Moderate dataset is used to evaluate the effectiveness of dehazing models in moderate haze conditions for remote sensing images. It specifically tests the model's ability to achieve high-quality dehazing, focusing on improving image clarity and detail in hazy satellite imagery. This dataset enables researchers to assess and enhance dehazing algorithms, ensuring they perform well under realistic atmospheric conditions.		
cited_context	SateHaze1k-Thick	https://doi.org/10.1109/TPAMI.2024.3419007 (2024)	https://doi.org/10.1007/978-3-030-58610-2_28 (2020)	The SateHaze1k-Thick dataset is used to evaluate the robustness of models in thick haze conditions for remote sensing images. It is employed to demonstrate state-of-the-art performance in this challenging environment, focusing on the ability of models to maintain accuracy and reliability under dense atmospheric haze. This dataset specifically enables researchers to test and improve image restoration techniques in real-world, hazy conditions.		
cited_context	SateHaze1k-Thin	https://doi.org/10.1109/TPAMI.2024.3419007 (2024)	https://doi.org/10.1007/978-3-030-58610-2_28 (2020)	The SateHaze1k-Thin dataset is used to evaluate model performance in remote sensing image dehazing, specifically under thin haze conditions. Researchers apply this dataset to assess the effectiveness of dehazing algorithms, focusing on the clarity and quality of restored images. The dataset's emphasis on thin haze scenarios makes it particularly useful for refining and testing dehazing techniques in less severe atmospheric conditions.		
cited_context	SCUT-CTW1500	https://doi.org/10.1109/CVPRW59228.2023.00178 (2023)	https://doi.org/10.1109/CVPR.2018.00070 (2018)	The SCUT-CTW1500 dataset is primarily used for evaluating curve text detection in wild images. It focuses on the restoration and recognition of curved text in diverse environments, employing methodologies that test the accuracy and robustness of text detection algorithms. This dataset enables researchers to address specific challenges in recognizing and restoring curved text, enhancing the performance of text detection systems in real-world scenarios.		
cited_context	SD7K	https://doi.org/10.1109/CVPR52733.2024.01482 (2024)	https://doi.org/10.1109/ICCV51070.2023.01144 (2023)	The SD7K dataset is used to train DocShadow, a model focused on document shadow removal. It emphasizes a large-scale dataset with diverse shadow conditions, enabling researchers to develop and test algorithms that effectively handle various shadow scenarios in document images. This dataset's extensive and varied content supports the training and evaluation of robust shadow removal techniques.		
cited_context	Seaships	https://doi.org/10.1109/TIV.2023.3347952 (2024)	https://doi.org/10.1109/TMM.2018.2865686 (2018)	The Seaships dataset is primarily used for evaluating and comparing image restoration techniques, specifically dehazing, deraining, and low-light enhancement, in maritime contexts. It focuses on ship detection and visual quality assessments, employing metrics like PSNR, SSIM, and FSIM. The dataset's precise annotations and large-scale, synthetic images enable robust training and evaluation of ship detection models under various environmental conditions.		
cited_context	sequentially-applied (or mixed) distortion dataset	https://doi.org/10.1109/CVPRW56347.2022.00069 (2022)	https://doi.org/10.1109/CVPR.2018.00259 (2018)	The sequentially-applied (or mixed) distortion dataset is used to evaluate image restoration methods by applying a sequence of distortions to images. This enhances the robustness of restoration algorithms, focusing on their ability to handle multiple, cumulative distortions. The dataset enables researchers to test and improve the performance of these algorithms under complex, real-world conditions.		
cited_context	Set11	https://doi.org/10.1109/CVPR52688.2022.01688 (2022)	https://doi.org/10.1109/ICCV.2001.937655 (2001)	Set11 is used for evaluating image restoration methods, offering a small but diverse set of images to assess performance. This dataset enables researchers to test and compare the effectiveness of various restoration techniques, focusing on how well these methods handle different types of image degradation. The diversity of images in Set11 provides a robust basis for performance assessment in image restoration research.		
cited_context | citing_context	Set12	https://doi.org/10.1109/CVPR52733.2024.00277 (2023), https://doi.org/10.1109/CVPR52688.2022.00564 (2021), https://doi.org/10.48550/arXiv.2210.00405 (2022), https://doi.org/10.1109/TPAMI.2021.3088914 (2020) (+5)	https://doi.org/10.1109/TIP.2017.2662206 (2016), https://doi.org/10.1109/ICCV.2001.937655 (2001)	Set12 is used to evaluate methods for grayscale image denoising, focusing on performance and quality improvements. The dataset's small, well-curated nature makes it suitable for detailed assessments of denoising techniques, enabling researchers to measure specific enhancements in image restoration accuracy and efficiency.; Set12 is primarily used for evaluating image denoising and restoration algorithms under controlled conditions. It focuses on a small set of high-quality, grayscale images to assess performance metrics such as PSNR and visual quality at specific noise levels (σ=15, σ=25, and σ=50). The dataset enables researchers to benchmark and compare denoising methods, emphasizing the accuracy and effectiveness of restoration techniques on synthetic benchmarks.	Set12	
cited_context	Set14	https://doi.org/10.1109/CVPRW.2018.00121 (2018), https://doi.org/10.1109/ICCVW54120.2021.00210 (2021), https://www.semanticscholar.org/paper/cbd65255f6266f798326acc8e0654e40ea0918cf (2018) (+15)	https://doi.org/10.1007/978-3-642-27413-8_47 (2010)	Set14 is primarily used to evaluate image super-resolution and restoration methods, focusing on performance metrics such as PSNR, SSIM, and LPIPS. It is employed to assess the quality and perceptual similarity of restored images, particularly in GAN-based and sparse-representation techniques. The dataset enables researchers to compare and validate the effectiveness of different super-resolution approaches, emphasizing high-quality image restoration and low-complexity methods.		
cited_context	Set14 × 2	https://doi.org/10.1007/s11263-023-01843-5 (2023)	https://doi.org/10.1109/CVPR.2019.01132 (2019)	The Set14 × 2 dataset is primarily used for performance comparison in single image super-resolution tasks. Researchers employ this dataset to evaluate and benchmark the efficiency of different algorithms, focusing on metrics such as FLOPs, running time, and peak memory consumption. This enables a detailed analysis of computational efficiency and resource utilization, facilitating advancements in algorithm optimization.		
cited_context	Set5	https://doi.org/10.1109/ICCVW54120.2021.00210 (2021), https://doi.org/10.1109/CVPRW.2018.00121 (2018), https://doi.org/10.48550/arXiv.2210.01427 (2022) (+15)	https://doi.org/10.1007/978-3-642-27413-8_47 (2010)	The Set5 dataset is primarily used to evaluate super-resolution methods, focusing on performance metrics such as PSNR, SSIM, and LPIPS. It is employed to assess the quality and effectiveness of GAN-based and low-complexity single-image super-resolution techniques, particularly in enhancing image resolution at various scaling factors. The dataset also tests the generalization ability of networks on images with blended distortions and different resolutions, contributing to the broader field of image restoration.		
cited_context	Set6	https://doi.org/10.1109/TPAMI.2021.3088914 (2020)	https://doi.org/10.1109/ICCV.2011.6126278 (2011)	Set6 is used in research to evaluate the effectiveness of image restoration methods, particularly focusing on performance metrics such as PSNR. It is employed to compare the proposed DPIR method against other established techniques like EPLL, FDN, DMPHN, IRCNN, and IR-CNN+. The dataset enables researchers to quantitatively assess and benchmark the performance of these methods in image restoration tasks.		
cited_context	Set68	https://www.semanticscholar.org/paper/cbd65255f6266f798326acc8e0654e40ea0918cf (2018)	https://doi.org/10.1109/ICCV.2001.937655 (2001)	Set68 is used for testing image restoration and denoising models, focusing on performance evaluation at specific noise levels (σ = 15, 25, 50). The dataset enables researchers to assess model effectiveness under controlled conditions, providing a standardized benchmark for comparing different image restoration techniques.		
cited_context	SHDD	https://doi.org/10.1109/CVPRW56347.2022.00069 (2022)	https://doi.org/10.1109/CVPR.2018.00259 (2018)	The SHDD dataset is used to evaluate image restoration methods by comparing their performance against mixed distortions and original images. Researchers employ deep reinforcement learning to assess these methods, focusing on how effectively they can restore images degraded by various distortions. This dataset enables rigorous testing and benchmarking of restoration algorithms, ensuring they can handle real-world image degradation scenarios.		
citing_context	SICE	https://doi.org/10.48550/arXiv.2411.17687 (2024), https://doi.org/10.48550/arXiv.2505.18047 (2025)	https://doi.org/10.1109/CVPR46437.2021.00912 (2021)	The SICE dataset is used in research for single-image color and contrast enhancement. It provides multi-exposure images to train and evaluate models, focusing on color correction, detail preservation, and contrast improvement. This dataset enables researchers to assess the generalization of their models on real-world images, enhancing both visual quality and perceptual accuracy.		
cited_context | citing_context	Sid	https://doi.org/10.48550/arXiv.2501.12981 (2025)	https://doi.org/10.1016/j.cviu.2022.103403 (2022)	The SID dataset is used to validate the performance of UniUIR on low-light image enhancement, specifically focusing on Sony camera images. It enables researchers to assess and improve the effectiveness of image restoration techniques under low-light conditions, leveraging the dataset's characteristic focus on Sony camera data.; The SID dataset is used to validate the performance of UniUIR on low-light image enhancement, specifically focusing on images captured by Sony cameras. This dataset enables researchers to assess and improve the effectiveness of image restoration techniques under low-light conditions, ensuring that the enhancements are robust and reliable for Sony camera images.	SID	
cited_context	SID Sony dataset	https://doi.org/10.1109/CVPR46437.2021.00349 (2021)	https://doi.org/10.1007/978-3-319-10602-1_48 (2014)	The SID Sony dataset is mentioned in the citation context but lacks detailed descriptions of its usage in specific research studies. Therefore, there is no evidence to describe its application, methodology, research questions, or enabling capabilities in any particular research area.		
cited_context | citing_context	Sidd	https://doi.org/10.48550/arXiv.2506.16960 (2025), https://doi.org/10.48550/arXiv.2410.15067 (2024), https://doi.org/10.48550/arXiv.2503.09403 (2025) (+2), https://doi.org/10.1109/CVPRW53098.2021.00027 (2021), https://doi.org/10.48550/arXiv.2203.06074 (2022), https://doi.org/10.1109/CVPR52688.2022.00564 (2021) (+8)	https://doi.org/10.1109/CVPR52729.2023.02083 (2023), https://doi.org/10.1109/CVPR.2018.00182 (2018)	The SIDD dataset is primarily used for real denoising in image restoration research, focusing on reducing noise in real-world images, particularly from smartphone cameras. It provides 20 paired noisy-clean images for training and testing denoising models. The dataset is crucial for evaluating the effectiveness of denoising algorithms, especially in handling realistic noise patterns while preserving important image details.; The SIDD dataset is primarily used for denoising experiments, particularly focusing on real-world noise reduction in smartphone camera images. It is employed to train and evaluate denoising algorithms, providing a benchmark for assessing restoration quality and performance metrics such as PSNR. The dataset's high-quality, real-world noisy images enable researchers to test and improve denoising techniques, enhancing image fidelity and addressing specific noise patterns found in smartphone cameras.	SIDD	
citing_context	SIDD val	https://doi.org/10.48550/arXiv.2408.15994 (2024)	https://doi.org/10.48550/arXiv.2206.05514 (2022)	The SIDD val dataset is used to evaluate noise reduction in images, specifically focusing on real-world noise patterns and restoration quality. Researchers employ this dataset to assess the performance of image restoration techniques, ensuring they effectively handle authentic noise scenarios. This enables the development and refinement of algorithms that improve image clarity and fidelity in practical applications.		
cited_context | citing_context	Sidd Validation Set	https://doi.org/10.48550/arXiv.2310.10123 (2023), https://doi.org/10.48550/arXiv.2310.10123 (2023), https://doi.org/10.48550/arXiv.2211.13654 (2022)	https://doi.org/10.1109/CVPR.2017.35 (2016)	The SIDD validation set is used for validating deblurring algorithms, specifically focusing on real-world image degradation and noise. This dataset enables researchers to assess the performance of deblurring techniques by providing realistic and diverse examples of degraded images, ensuring that the algorithms can handle various types of noise and blur effectively.; The SIDD validation set is used in research to validate deblurring and denoising algorithms, focusing on real-world image degradation and photographic noise reduction. It enables researchers to evaluate the performance of these algorithms by providing realistic scenarios, ensuring that the methods can effectively handle practical issues in image restoration.	SIDD validation set	
citing_context	SIDD-Val	https://doi.org/10.48550/arXiv.2503.09403 (2025)	https://doi.org/10.1109/CVPR.2018.00182 (2018)	The SIDD-Val dataset is primarily used for validating image denoising models by providing noisy and clean image pairs. Researchers employ this dataset to assess the performance and accuracy of their denoising algorithms, ensuring they effectively reduce noise while preserving image details. This validation process is crucial for advancing image restoration techniques.		
citing_context	Singapore maritime dataset (SMD)	https://doi.org/10.48550/arXiv.2402.03738 (2024)	https://doi.org/10.1109/TITS.2016.2634580 (2016)	The Singapore maritime dataset (SMD) is used for water scene restoration and evaluating image restoration methods in maritime environments. It provides maritime images to test and improve restoration algorithms, particularly focusing on addressing visibility issues in hazy conditions. This dataset enables researchers to assess and enhance the performance of restoration techniques in challenging maritime settings.		
cited_context	Smartphone Image Denoising Dataset (SIDD)	https://doi.org/10.48550/arXiv.2306.05390 (2023)	https://www.semanticscholar.org/paper/fb52f9294fa1cd83660eccac486367eac01ea32d (2018)	The Smartphone Image Denoising Dataset (SIDD) is primarily used to train and evaluate color denoising models, focusing on noise reduction in images captured by smartphone cameras. This dataset enables researchers to develop and test algorithms that enhance image quality by specifically addressing the unique noise characteristics of smartphone camera sensors.		
cited_context | citing_context	Smd	https://doi.org/10.48550/arXiv.2402.03738 (2024), https://doi.org/10.1109/TIV.2023.3347952 (2024)	https://doi.org/10.1109/TITS.2016.2634580 (2016), https://doi.org/10.1109/TMM.2018.2865686 (2018)	The SMD dataset is used to evaluate dehazing methods in image restoration research. It focuses on assessing image quality improvement through metrics such as PSNR, SSIM, and NIQE. This dataset enables researchers to quantitatively compare the effectiveness of different dehazing techniques, providing a standardized benchmark for performance evaluation.; The SMD dataset is primarily used for evaluating and comparing various image restoration techniques, including dehazing, deraining, desnowing, and low-light enhancement. It serves as a benchmark for assessing performance through metrics like PSNR, SSIM, FSIM, and NIQE, and for visual comparisons. The dataset includes real-world and synthetic images, particularly emphasizing diverse and challenging scenarios, such as those found in maritime environments.	SMD	
cited_context | citing_context	Snow 100K	https://doi.org/10.1109/TCSVT.2024.3429557 (2024)	https://doi.org/10.1109/TIP.2018.2806202 (2017)	The SNOW 100K dataset is used to evaluate model performance in complex snow scenarios, focusing on the effectiveness of models in addressing intricate snow degradations. This dataset enables researchers to test and validate their models against challenging snow conditions, ensuring robustness and reliability in real-world applications.; The SNOW 100K dataset is used to evaluate model performance in complex snow scenarios, focusing on the effectiveness of models in addressing intricate snow degradations. This dataset enables researchers to test and validate their models against challenging snow conditions, ensuring robustness and reliability in image restoration tasks.	SNOW 100K	
cited_context	Snow realistic	https://doi.org/10.1109/ICCV51070.2023.01983 (2023)	https://doi.org/10.1109/TIP.2018.2806202 (2017)	The 'Snow realistic' dataset is used to test and validate snow removal algorithms by providing realistic synthetic snow images. It is employed in real-world experimental analysis to assess the effectiveness of desnowing methods, enabling researchers to evaluate how well these algorithms perform on realistic snow scenes.		
cited_context | citing_context	Snow100K	https://doi.org/10.48550/arXiv.2506.16960 (2025), https://doi.org/10.3390/electronics13142817 (2024), https://doi.org/10.1109/TCSVT.2024.3429557 (2024) (+11), https://doi.org/10.1109/TCSVT.2024.3429557 (2024), https://doi.org/10.1109/TCSVT.2023.3299324 (2024), https://doi.org/10.48550/arXiv.2409.00263 (2024) (+14)	https://doi.org/10.1109/ICCV48922.2021.00416 (2021), https://doi.org/10.1109/TIP.2018.2806202 (2017)	The Snow100K dataset is primarily used for desnowing, which involves removing snowflakes and enhancing visual clarity in images. It is utilized for training and evaluating snow removal algorithms, particularly focusing on diverse snow patterns and intensities. The dataset supports qualitative comparisons and performance assessments of various desnowing methods, including the use of hierarchical dual-tree complex wavelet representation and contradict channel loss. Its large-scale synthetic nature makes it suitable for optimizing and testing image desnowing models.; The Snow100K dataset is primarily used for training and evaluating snow removal algorithms, focusing on synthetic snow images. It is utilized to assess the performance of various models, including hierarchical dual-tree complex wavelet representation, context-aware deep networks, and OneRestore, on tasks such as single image desnowing and restoring underlying scene details. The dataset's large size, particularly its 50,000 paired images in the test split, enables robust training and evaluation of these image restoration techniques.	Snow100K	
cited_context	Snow100k dataset	https://doi.org/10.48550/arXiv.2407.04621 (2024)	https://doi.org/10.1109/TIP.2016.2639450 (2017)	The Snow100k dataset is used for evaluating snow removal techniques in image restoration. It features realistic snowy images that help researchers assess the effectiveness of these techniques under various conditions. The dataset enables the development and testing of algorithms designed to remove snow from images, enhancing their clarity and usability.		
citing_context	Snow100K L	https://doi.org/10.48550/arXiv.2406.18242 (2024)	https://doi.org/10.1109/TIP.2018.2806202 (2017)	The Snow100K L dataset is primarily used for desnowing research, specifically to test and validate the effectiveness of snow removal techniques on large-scale images. It focuses on assessing the robustness of these methods, leveraging the dataset's large-sized images to ensure that desnowing algorithms perform reliably under various conditions.		
citing_context	Snow100K M	https://doi.org/10.48550/arXiv.2406.18242 (2024)	https://doi.org/10.1109/TIP.2018.2806202 (2017)	The Snow100K M dataset is primarily used for desnowing research, specifically to assess the performance of snow removal algorithms on medium-scale images. Studies employ this dataset to evaluate the effectiveness of desnowing techniques, focusing on the quality and efficiency of image restoration in snowy conditions. The dataset's medium-sized images are crucial for testing and validating these algorithms.		
citing_context	Snow100K S	https://doi.org/10.48550/arXiv.2406.18242 (2024)	https://doi.org/10.1109/TIP.2018.2806202 (2017)	The Snow100K S dataset is primarily used for desnowing research, focusing on small-scale and small-sized images. It evaluates the effectiveness and performance of snow removal methods, enabling researchers to assess the quality of desnowing techniques on detailed, smaller image sets. This dataset facilitates the development and testing of algorithms designed to improve image clarity in snowy conditions.		
cited_context | citing_context	Snow100K-L	https://doi.org/10.1109/TMM.2025.3535316 (2024), https://doi.org/10.1109/TCSVT.2024.3516074 (2024), https://doi.org/10.48550/arXiv.2410.08177 (2024) (+2), https://doi.org/10.1109/TIP.2024.3368961 (2023), https://doi.org/10.1109/ACCESS.2025.3526168 (2023), https://doi.org/10.48550/arXiv.2207.04754 (2022) (+1)	https://doi.org/10.1109/CVPR.2019.00173 (2019), https://doi.org/10.1109/TIP.2018.2806202 (2017)	The Snow100K-L dataset is primarily used for evaluating snow removal techniques in synthetic images. Researchers employ it to test and assess the effectiveness of desnowing algorithms, focusing on improving image visibility and restoration under snow degradation. The dataset's large scale and high-resolution synthetic snowy images enable robust testing of various restoration methods, including hierarchical dual-tree complex wavelet representation and contradict channel loss.; The Snow100K-L dataset is used to evaluate and train image restoration methods, particularly for removing snow from images. It contains 16,801 images and is utilized to assess desnowing performance, focusing on larger-scale images with moderate PSNR/SSIM metrics. The dataset enables researchers to test and enhance algorithms for improving image quality in snowy environments, addressing the most challenging subsets of images.	Snow100K-L	
cited_context | citing_context	Snow100K-L Test Set	https://doi.org/10.1109/CVPR52688.2022.00239 (2021), https://doi.org/10.48550/arXiv.2504.05135 (2025), https://doi.org/10.1109/CVPR52688.2022.00239 (2021)	https://doi.org/10.1109/CVPR.2018.00263 (2017)	The Snow100K-L test set is used to evaluate the effectiveness of snow removal algorithms, specifically focusing on clarity and detail preservation in snowy conditions. It consists of 16,081 images and is employed to assess methods' performance in removing snow, ensuring that restored images maintain high quality and detail. This dataset enables researchers to rigorously test and compare different snow removal techniques.; The Snow100k-L test set is used to evaluate methods for removing snow from images, focusing on the reduction of noise and enhancement of visibility. This dataset enables researchers to assess the effectiveness of their techniques in improving image quality under snowy conditions, providing a benchmark for performance comparison.	Snow100K-L test set, Snow100k-L test set	
cited_context	Snow100K-M	https://doi.org/10.48550/arXiv.2207.04754 (2022)	https://doi.org/10.1109/TCSVT.2020.3003025 (2021)	The Snow100K-M dataset is used to evaluate the performance of SMGARN in de-snowing images, focusing on PSNR improvements compared to DS-GAN. This dataset enables researchers to quantitatively assess and compare the effectiveness of different de-snowing algorithms, specifically in enhancing image quality metrics.		
citing_context	Snow100k-R	https://doi.org/10.48550/arXiv.2411.10708 (2024)	https://doi.org/10.1109/TIP.2018.2806202 (2017)	The Snow100k-R dataset is used to evaluate the effectiveness of AllRestorer in snow removal, focusing on enhancing clarity and detail in snowy scenes. This dataset enables researchers to assess the performance of image restoration techniques specifically designed to handle snow-related distortions, ensuring that restored images are clearer and more detailed.		
cited_context	Snow100K-real	https://doi.org/10.1109/TIP.2024.3501855 (2024)	https://doi.org/10.1109/CVPR52688.2022.01693 (2022)	The Snow100K-real dataset is used to evaluate and compare the performance of MWFormer and other models on real weather-degraded images, specifically focusing on restoration under unknown corruptions. This dataset enables researchers to assess the effectiveness of image restoration techniques in realistic conditions, providing a benchmark for model accuracy and robustness.		
cited_context | citing_context	Snow100K-S	https://doi.org/10.3390/electronics13142817 (2024), https://doi.org/10.48550/arXiv.2504.09973 (2025), https://doi.org/10.48550/arXiv.2207.04754 (2022), https://doi.org/10.1109/TPAMI.2023.3238179 (2022)	https://doi.org/10.1109/ICCV48922.2021.00416 (2021), https://doi.org/10.1109/TCSVT.2020.3003025 (2021)	The Snow100K-S dataset is used for evaluating and testing single image desnowing algorithms. It focuses on smaller-scale synthetic snowy images with varying intensities and complexities to assess algorithm performance. This dataset enables researchers to systematically analyze the effectiveness of snow removal techniques, providing a standardized benchmark for comparing different methods.; The Snow100K-S dataset is used to evaluate the performance of image desnowing techniques, particularly in smaller-scale images. It focuses on comparing methods like SMGARN and DS-GAN, emphasizing metrics such as PSNR and SSIM to assess improvements in de-snowing effectiveness. This dataset enables researchers to benchmark and refine algorithms for better image restoration in snowy conditions.	Snow100K-S	
cited_context	Snow10k	https://doi.org/10.48550/arXiv.2305.17863 (2023)	https://doi.org/10.1109/CVPR.2019.00173 (2019)	The Snow10k dataset is mentioned in the citation context but lacks detailed descriptions of its usage in research. There is no explicit information on its application, methodology, research questions, or specific characteristics. Therefore, it cannot be accurately described as being used for All-in-One Image Restoration or any other specific research area based on the provided evidence.		
citing_context	SnowCityScapes	https://doi.org/10.1109/TCSVT.2024.3519352 (2025)	https://doi.org/10.1109/CVPRW.2018.00119 (2018)	The SnowCityScapes dataset is used for evaluating the desnowing performance of all-in-one image restoration models. It provides images with snow, enabling researchers to test the effectiveness of these models in removing snow from images. This dataset facilitates the assessment of desnowing algorithms by offering realistic, snowy urban scenes, which are crucial for validating the robustness and accuracy of image restoration techniques.		
cited_context	SnowTest100K	https://doi.org/10.1109/TPAMI.2023.3238179 (2022)	https://doi.org/10.1109/TIP.2021.3104166 (2021)	The SnowTest100K dataset is used to qualitatively evaluate and compare the performance of different models, specifically the best model, DesnowNet, and DDMSNet, in snow removal tasks. It leverages semantic and depth priors to enhance the accuracy and effectiveness of snow removal techniques. This dataset enables researchers to assess the visual quality and robustness of these models in handling snow-related image restoration challenges.		
cited_context	Solid dataset	https://doi.org/10.1109/TCSVT.2023.3286405 (2024)	https://doi.org/10.1109/ICCV.2017.423 (2017)	The Solid dataset is used to evaluate image restoration techniques, particularly on indoor solid object scenes. It contains 200 image triplets and is employed to assess the robustness, accuracy, and quality of restored images in controlled environments. The dataset supports research in reflection removal and general image restoration, enabling detailed performance comparisons of different algorithms.		
cited_context | citing_context	Sots	https://doi.org/10.1109/TPAMI.2025.3562211 (2024), https://doi.org/10.1109/CVPR52733.2024.00277 (2023), https://doi.org/10.48550/arXiv.2404.02154 (2024) (+11), https://doi.org/10.24963/ijcai.2024/80 (2024), https://doi.org/10.48550/arXiv.2312.05038 (2023), https://doi.org/10.48550/arXiv.2410.08688 (2024) (+11)	https://doi.org/10.1109/TIP.2018.2867951 (2017)	The SOTS dataset is primarily used for image dehazing research, focusing on improving visibility by removing atmospheric haze effects from both synthetic and real-world images. It contains 72,135 training images and 500 testing images, enabling the training and evaluation of dehazing algorithms. Researchers use SOTS to benchmark single-image dehazing methods, conduct visual comparisons, and measure performance improvements, such as PSNR enhancements, demonstrating its utility in advancing dehazing techniques.; The SOTS dataset is primarily used for evaluating and benchmarking single-image dehazing algorithms. It consists of 500 outdoor images and is utilized to test the performance and effectiveness of dehazing techniques, both in synthetic and real-world conditions. Researchers use it to compare their proposed methods against state-of-the-art algorithms, focusing on metrics like PSNR and visual quality improvements. The dataset's controlled conditions and ground truth data enable rigorous quantitative analysis and validation of dehazing models.	SOTS	
cited_context | citing_context	Sots (Outdoor)	https://doi.org/10.1007/978-3-031-72764-1_1 (2024)	https://doi.org/10.1109/TIP.2018.2867951 (2017)	The SOTS (outdoor) dataset is used to evaluate dehazing methods, specifically focusing on outdoor scenes. It enables researchers to assess the performance of these methods in realistic conditions, ensuring that the dehazing techniques are effective in practical scenarios. The dataset's emphasis on outdoor environments provides a robust testbed for dehazing algorithms.	SOTS (outdoor)	
cited_context | citing_context	Sots Outdoor Dataset	https://doi.org/10.1007/978-3-031-72764-1_1 (2024)	https://doi.org/10.1109/TIP.2018.2867951 (2017)	The SOTS outdoor dataset is used for dehazing comparisons in all-in-one image restoration methods, specifically to enhance outdoor image quality and improve clarity. Researchers employ this dataset to evaluate and compare the effectiveness of different dehazing techniques, focusing on the visual quality and clarity of restored images.	SOTS outdoor dataset	
citing_context	SOTS outdoors	https://doi.org/10.48550/arXiv.2406.18242 (2024)	https://doi.org/10.1007/978-3-030-58607-2_7 (2020)	The SOTS outdoors dataset is primarily used for evaluating dehazing algorithms. It provides a collection of outdoor scenes with both synthetic and real haze, enabling researchers to test and compare the effectiveness of different dehazing techniques. This dataset facilitates the assessment of algorithm performance in realistic conditions, enhancing the robustness and reliability of dehazing methods.		
cited_context | citing_context	Sots-Indoor	https://doi.org/10.1109/TCSVT.2024.3429557 (2024), https://doi.org/10.1109/TPAMI.2024.3419007 (2024), https://doi.org/10.48550/arXiv.2305.17863 (2023), https://doi.org/10.1109/TPAMI.2023.3330416 (2023) (+6)	https://doi.org/10.1109/TIP.2023.3256763 (2022), https://doi.org/10.1109/TIP.2018.2867951 (2017)	The SOTS-Indoor dataset is used to evaluate the performance of single image dehazing methods, specifically comparing the proposed method against DehazeFormer-L. Research focuses on metrics such as PSNR and computational efficiency (FLOPs). This dataset enables researchers to benchmark and validate the effectiveness and efficiency of their dehazing algorithms in indoor environments.; The SOTS-Indoor dataset is primarily used for evaluating and benchmarking single-image dehazing algorithms. It focuses on assessing the trade-offs between Peak Signal-to-Noise Ratio (PSNR) and computational efficiency (FLOPs). Researchers use this dataset to compare the performance of various models, such as FSNet-S, ConvIR-B, MB-TaylorFormer-L, DeHamer, OKNet, and DehazeFormer, in terms of dehazing effectiveness and computational complexity. The dataset is particularly useful for visual comparisons and performance evaluations in indoor synthetic daytime scenes, enabling detailed analysis of dehazing techniques.	SOTS-Indoor	
cited_context | citing_context	Sots-Outdoor	https://doi.org/10.48550/arXiv.2310.10123 (2023), https://doi.org/10.48550/arXiv.2409.19403 (2024), https://doi.org/10.48550/arXiv.2411.18412 (2024) (+4), https://doi.org/10.1109/TPAMI.2024.3419007 (2024), https://doi.org/10.1109/TPAMI.2023.3330416 (2023), https://doi.org/10.48550/arXiv.2409.19403 (2024) (+4)	https://www.semanticscholar.org/paper/c7798bbbcd2e85593a2b2d37b92dabac63420cd8 (2018), https://doi.org/10.1109/TIP.2018.2867951 (2017)	The SOTS-Outdoor dataset is primarily used for evaluating dehazing algorithms in outdoor scenes, focusing on enhancing visibility and restoring clear images from hazy environments. It is also utilized for testing image deraining, assessing the model's ability to remove rain from outdoor images. The dataset consists of real-world outdoor scenes, enabling researchers to evaluate the performance of dehazing and deraining methods under varying atmospheric conditions.; The SOTS-Outdoor dataset is primarily used to evaluate and benchmark dehazing methods, focusing on outdoor scenes. It is employed to assess the performance of dehazing algorithms in removing atmospheric haze, with metrics such as PSNR and visual comparisons. The dataset emphasizes real-world hazy conditions and is used to compare proposed models against state-of-the-art methods, highlighting parameter efficiency and performance in realistic outdoor scenarios.	SOTS-Outdoor	
cited_context | citing_context	Spa	https://doi.org/10.48550/arXiv.2410.15067 (2024), https://doi.org/10.48550/arXiv.2410.15067 (2024), https://doi.org/10.1109/CVPR52729.2023.02083 (2023), https://doi.org/10.48550/arXiv.2309.06023 (2023)	https://doi.org/10.1109/ICCV.2001.937655 (2001), https://doi.org/10.1109/CVPR.2019.01255 (2019)	The SPA dataset is primarily used for rain removal tasks in image restoration research. It features both synthetic and real rain images, which are utilized to assess and improve the performance of de-raining techniques. Researchers employ this dataset to evaluate the effectiveness of various rain removal methods, focusing on enhancing image quality and de-raining performance.; The SPA dataset is primarily used for rain removal tasks in image restoration research, focusing on both synthetic and real rain images. It serves as a foundational resource for constructing enhanced datasets like SPA+ and is utilized to improve and evaluate the performance of deraining algorithms, particularly in single-image deraining scenarios. The dataset's high-quality real rain data is crucial for assessing and enhancing rain removal accuracy and demonstrating significant PSNR improvements in retrained models.	SPA	
cited_context | citing_context	Spa+	https://doi.org/10.48550/arXiv.2410.15067 (2024), https://doi.org/10.1109/WACV61041.2025.00069 (2025), https://doi.org/10.1145/3664647.3680762 (2024), https://doi.org/10.48550/arXiv.2410.15067 (2024), https://doi.org/10.1145/3664647.3680762 (2024), https://doi.org/10.1109/CVPR52729.2023.02083 (2023)	https://doi.org/10.1109/CVPR46437.2021.00912 (2021), https://doi.org/10.1109/CVPR52729.2023.02083 (2023)	The SPA+ dataset is primarily used for evaluating and comparing methods in all-weather image restoration, specifically focusing on rain, haze, and snow removal. It is employed to test the effectiveness of models in handling synthetic and real-world weather conditions, particularly in removing rain streaks, water droplets, and other artifacts. This dataset enables researchers to conduct qualitative assessments and benchmark the performance of image deraining techniques.; The SPA+ dataset is primarily used for image deraining, enhancing and expanding the original SPA dataset to improve rain removal performance. It is employed in training and evaluating image restoration models, focusing on removing rain, haze, and snow to restore clear images. The dataset supports research on spatial attention mechanisms and assesses model effectiveness in various adverse weather conditions.	SPA+	
cited_context | citing_context	Spa-Data	https://doi.org/10.48550/arXiv.2411.10708 (2024), https://doi.org/10.1007/978-3-031-20071-7_4 (2021)	https://doi.org/10.1109/TIP.2018.2806202 (2017), https://doi.org/10.1109/CVPR.2019.01255 (2019)	The SPA-Data dataset is used to evaluate the performance of AllRestorer in various real-world scenarios, focusing on multiple types of image degradation and restoration challenges. This dataset enables researchers to assess the effectiveness of image restoration techniques under diverse conditions, ensuring robustness and versatility in practical applications.; The SPA-Data dataset is primarily used to evaluate and benchmark deraining algorithms, particularly focusing on single-image deraining performance. It is utilized to assess the effectiveness of SPDNet and SPDNet-local in removing rain from images, comparing different inference methods. The dataset's high-quality real rain data enables researchers to rigorously test and improve deraining techniques.	SPA-Data	
cited_context	SPACE	https://doi.org/10.1109/TGRS.2024.3378828 (2024)	https://doi.org/10.1109/LGRS.2017.2771212 (2017)	The SPACE dataset is primarily used for hyperspectral image reconstruction, employing methodologies such as compressive sensing and tensor decomposition. It addresses research questions related to efficient and accurate reconstruction of hyperspectral images from compressed data. The dataset's characteristics enable researchers to test and validate advanced reconstruction algorithms, enhancing the quality and resolution of hyperspectral imagery.		
citing_context	SPAD	https://doi.org/10.1109/TIP.2025.3572788 (2025)	https://doi.org/10.1109/CVPR.2019.01255 (2019)	The SPAD dataset is used for conducting image restoration experiments, particularly focusing on image deraining. Researchers employ this dataset to evaluate the effectiveness of various image restoration techniques. The dataset's specific characteristics and images enable rigorous testing and validation of deraining algorithms, contributing to advancements in image restoration methodologies.		
cited_context | citing_context	Spanet	https://doi.org/10.48550/arXiv.2505.21637 (2025), https://doi.org/10.1109/TPAMI.2025.3562211 (2024), https://doi.org/10.48550/arXiv.2405.02843 (2024)	https://doi.org/10.1109/CVPRW.2018.00119 (2018), https://doi.org/10.1109/CVPR.2018.00079 (2018)	The SPANet dataset is primarily used for evaluating and training image restoration models, particularly in deraining tasks. It is employed to assess the performance of various algorithms, such as BaryIR and DA-RCOT, on real-world rain degradation, emphasizing robustness and generalization on unseen data. The dataset facilitates comparisons with state-of-the-art methods, focusing on improving image clarity and removing rain streaks, and is used to measure performance metrics like PSNR.; The SPANet dataset is used to evaluate the performance of deraining models on real-world rainy images. Researchers employ this dataset to test and assess the effectiveness of trained models, particularly those using GAN-based methods, in practical deraining scenarios. This evaluation helps in understanding the practical applicability and robustness of these models in real-world conditions.	SPANet	
citing_context	SPANet [48]	https://doi.org/10.48550/arXiv.2505.21637 (2025)	https://doi.org/10.1109/NCC.2015.7084843 (2015)	The SPANet dataset is used to evaluate image restoration performance, specifically addressing issues like rain and haze. Researchers employ metrics such as PSNR, SSIM, LPIPS, and FID to assess the effectiveness of restoration techniques. This dataset enables rigorous quantitative analysis, facilitating comparisons between different image restoration methods and enhancing the reliability of performance evaluations.		
cited_context | citing_context	Squid-16	https://doi.org/10.48550/arXiv.2501.12981 (2025)	https://doi.org/10.1109/TCSVT.2019.2963772 (2019)	The SQUID-16 dataset is used for testing and visual comparison of underwater image enhancement techniques, particularly focusing on natural lighting conditions and real-world scenarios. It enables researchers to evaluate the effectiveness of restoration methods in improving image quality under natural light, facilitating advancements in underwater imaging.; The SQUID-16 dataset is used for evaluating and comparing image restoration techniques, particularly in enhancing structural details under natural light conditions. It is specifically applied to assess underwater image enhancement methods, addressing the unique challenges posed by natural lighting. This dataset facilitates visual comparisons and performance evaluations, enabling researchers to refine and validate their restoration algorithms in realistic scenarios.	SQUID-16	
cited_context | citing_context	Srrs	https://doi.org/10.1109/TCSVT.2024.3429557 (2024), https://doi.org/10.1109/TIP.2025.3572788 (2025), https://doi.org/10.48550/arXiv.2411.18412 (2024), https://doi.org/10.1109/TCSVT.2024.3429557 (2024), https://doi.org/10.1109/TPAMI.2023.3330416 (2023), https://www.semanticscholar.org/paper/6af728164e4424218967fd356b3448f5037acd88 (2023) (+3)	https://doi.org/10.1109/TIP.2018.2806202 (2017), https://doi.org/10.1007/978-3-030-58589-1_45 (2020)	The SRRS dataset is primarily used to train and evaluate image desnowing models, focusing on the removal of snow from images. It is employed to assess the performance of algorithms in handling varying snow intensities, textures, and complex scenarios, emphasizing size and transparency-aware techniques. The dataset supports research by providing a benchmark for comparing the effectiveness of different models, such as PoolNet-S and FocalNet, using metrics like PSNR.; The SRRS dataset is primarily used for evaluating and training image desnowing models, focusing on the removal of snow from images. It employs methodologies such as hierarchical dual-tree complex wavelet representation, context-aware deep networks, and size and transparency-aware algorithms. The dataset is also used to assess the effectiveness of desnowing techniques in diverse and realistic image conditions, enhancing visual quality and performance.	SRRS	
citing_context	SRRS dataset	https://doi.org/10.48550/arXiv.2411.18412 (2024)	https://doi.org/10.1109/CVPR.2017.35 (2016)	The SRRS dataset is used to evaluate image restoration approaches, particularly focusing on the robustness of methods in complex weather conditions such as haze and snow. Researchers employ this dataset to test and validate their algorithms, ensuring they can effectively handle multiple degradations simultaneously. This enables the assessment of performance in realistic, challenging environments.		
cited_context	standard test dataset of 68 natural images	https://doi.org/10.1109/TPAMI.2016.2596743 (2015)	https://doi.org/10.1007/s11263-008-0197-6 (2009)	The standard test dataset of 68 natural images is primarily used to evaluate the denoising performance of trained models, specifically focusing on Gaussian denoising. This dataset enables researchers to test and compare the effectiveness of their models in removing noise from natural images, providing a standardized benchmark for performance assessment.		
cited_context	stereo super-resolution validation dataset	https://doi.org/10.1109/CVPRW59228.2023.00169 (2023)	https://doi.org/10.48550/arXiv.2301.11699 (2023)	The stereo super-resolution validation dataset is used to assess the performance and restoration quality of stereo super-resolution methods. Researchers employ mean squared error as a primary metric to validate these methods, ensuring they effectively enhance image resolution while maintaining accuracy and detail. This dataset enables rigorous evaluation and comparison of different stereo super-resolution techniques.		
cited_context	SUNRGB-D	https://doi.org/10.48550/arXiv.2409.15278 (2024)	https://doi.org/10.1007/978-3-642-33715-4_54 (2012)	The SUNRGB-D dataset is primarily used for monocular depth estimation, leveraging its large collection of RGB-D images with depth annotations. Researchers employ this dataset to train and evaluate models that predict depth from single RGB images. The dataset's rich annotations enable the development and testing of algorithms aimed at improving depth perception in computer vision tasks.		
citing_context	Synthetic Objective Testing Set (SOTS)	https://doi.org/10.1109/TCSVT.2024.3519352 (2025)	https://doi.org/10.1109/TIP.2018.2867951 (2017)	The Synthetic Objective Testing Set (SOTS) is used for the objective evaluation of dehazing algorithms. It contains synthetic images with controlled parameters, enabling researchers to assess the performance of dehazing techniques systematically. The dataset's controlled environment facilitates precise performance measurement, making it a valuable resource for evaluating and comparing dehazing methods.		
cited_context	synthetic rain (Zamir et al. 2021) datasets (SRD)	https://doi.org/10.48550/arXiv.2409.00263 (2024)	https://doi.org/10.1109/TIP.2018.2806202 (2017)	The synthetic rain (Zamir et al. 2021) datasets (SRD) are used to train methods for rain removal, specifically evaluating the effectiveness of synthetic rain data in enhancing all-weather image restoration. This dataset enables researchers to test and improve algorithms by providing a controlled environment with synthetic rain conditions, facilitating the development of more robust image restoration techniques.		
citing_context	T-OLED-Val	https://doi.org/10.48550/arXiv.2503.09403 (2025)	https://doi.org/10.1109/CVPR.2018.00182 (2018)	The T-OLED-Val dataset is used for validating image restoration models, particularly focusing on low-resolution (LQ) and high-resolution (HQ) image pairs from OLED displays and under-display cameras. It enables researchers to assess the performance of restoration models in enhancing image quality, ensuring that the models effectively handle the specific characteristics of T-OLED technology.		
cited_context | citing_context	T90	https://doi.org/10.48550/arXiv.2501.12981 (2025)	https://doi.org/10.1109/ICCV51070.2023.00371 (2023)	The T90 dataset is used to assess the effectiveness of underwater image restoration techniques on segmentation performance. Researchers apply the Segment Anything Model (SAM) to restored images from various underwater image restoration (UIR) methods, evaluating how these enhancements improve segmentation accuracy. This dataset enables the comparison of different UIR approaches and their impact on downstream computer vision tasks.	T90	
cited_context	TDD	https://doi.org/10.1109/CVPR52733.2024.01482 (2024)	https://doi.org/10.1109/TPAMI.2020.3022406 (2020)	The TDD dataset is used to train models for deblurring photographed document images, specifically enhancing document quality. Researchers employ generative adversarial networks (GANs) to improve image clarity. This dataset enables the development and evaluation of deblurring techniques, addressing the challenge of restoring degraded document images.		
citing_context	TE42	https://doi.org/10.48550/arXiv.2506.16960 (2025)	https://doi.org/10.1007/978-981-15-5873-3_2 (2021)	The TE42 dataset is used to construct a pool of visual grounds for image restoration, specifically for camera testing and visual analysis. It provides a standardized set of images that enable researchers to evaluate and compare the performance of different image restoration techniques under various conditions. This dataset facilitates the development and refinement of algorithms by offering a consistent benchmark for visual quality assessment.		
cited_context	TEST 100	https://doi.org/10.1109/TPAMI.2023.3330416 (2023)	https://doi.org/10.1109/TCSVT.2019.2920407 (2017)	The TEST 100 dataset is used to evaluate the performance of rain removal algorithms on a controlled set of test images. Researchers employ this dataset to assess algorithm effectiveness, focusing on specific image restoration tasks such as rain removal. The dataset's smaller, controlled nature allows for precise and manageable testing environments, enabling detailed analysis of algorithmic performance in rain removal scenarios.		
cited_context	TEST 1200	https://doi.org/10.1109/TPAMI.2023.3330416 (2023)	https://doi.org/10.1109/TCSVT.2019.2920407 (2017)	The TEST 1200 dataset is used to evaluate the robustness of rain removal methods by applying these techniques to a large, diverse set of test images. This evaluation helps researchers assess the effectiveness and generalizability of rain removal algorithms across various image conditions. The dataset's diversity and size enable comprehensive testing, ensuring that the methods can handle a wide range of real-world scenarios.		
cited_context	TEST 2800	https://doi.org/10.1109/TPAMI.2023.3330416 (2023)	https://doi.org/10.1109/TCSVT.2019.2920407 (2017)	The TEST 2800 dataset is used to evaluate the scalability and generalization of rain removal techniques on a large set of test images. It provides a comprehensive benchmark for assessing the performance of these techniques under diverse conditions, enabling researchers to validate the robustness and effectiveness of their methods in removing rain from images.		
cited_context | citing_context	Test1	https://doi.org/10.1109/TMM.2025.3535316 (2024), https://doi.org/10.1109/TCSVT.2023.3299324 (2024), https://doi.org/10.1109/CVPR52688.2022.00239 (2021) (+1), https://doi.org/10.1109/TCSVT.2023.3299324 (2024), https://doi.org/10.1109/TIP.2024.3368961 (2023), https://doi.org/10.1109/TPAMI.2023.3238179 (2022)	https://doi.org/10.1109/CVPR.2019.00173 (2019)	The Test1 dataset is primarily used to evaluate the performance of image restoration methods, particularly in removing rain and haze from images. It contains 750 images and is employed to assess the effectiveness of models in enhancing image quality under various degradations, including rain streaks and complex weather conditions. This dataset enables researchers to test and compare different restoration techniques, focusing on both specific and general restoration capabilities.; The Test1 dataset is used to evaluate the performance of image restoration models, particularly in deraining tasks. It includes 750 images and is employed for both qualitative and quantitative assessments, focusing on the effectiveness of models in removing rain streaks and restoring image quality under heavy rain conditions. This dataset enables researchers to comprehensively test and compare the general restoration capabilities of their methods.	Test1	
cited_context	Test1 dataset	https://doi.org/10.1109/CVPR52688.2022.00239 (2021)	https://doi.org/10.1109/CVPR.2018.00263 (2017)	The Test1 dataset is used for evaluating the performance of image restoration methods, specifically focusing on general image quality metrics. Researchers employ this dataset to assess and compare the effectiveness of different restoration techniques, ensuring that the restored images meet high-quality standards. This dataset enables rigorous testing and validation of image restoration algorithms, contributing to advancements in the field.		
cited_context | citing_context	Test100	https://doi.org/10.1109/TCSVT.2024.3398810 (2023), https://www.semanticscholar.org/paper/6af728164e4424218967fd356b3448f5037acd88 (2023), https://doi.org/10.1109/TMM.2024.3407656 (2024), https://doi.org/10.1109/TPAMI.2024.3419007 (2024) (+7)	https://doi.org/10.1109/CVPR.2018.00079 (2018), https://doi.org/10.1109/TCSVT.2019.2920407 (2017)	The Test100 dataset is used for evaluating the generalization capabilities of image restoration models, particularly focusing on their performance with diverse rain patterns. It serves as a smaller test set to provide insights into how well these models can handle varied conditions, enhancing the understanding of model robustness and adaptability.; The Test100 dataset is primarily used for evaluating image de-raining methods, focusing on performance metrics such as PSNR and visual quality. It is employed to test the generalization and robustness of deraining algorithms, including conditional generative adversarial networks, on a smaller set of diverse rainy images. This dataset enables researchers to compare and assess the effectiveness of different deraining techniques, providing insights into model accuracy and reliability.	Test100	
cited_context | citing_context	Test1200	https://doi.org/10.48550/arXiv.2406.18242 (2024), https://doi.org/10.1109/TCSVT.2024.3398810 (2023), https://doi.org/10.1109/TMM.2024.3407656 (2024), https://www.semanticscholar.org/paper/6af728164e4424218967fd356b3448f5037acd88 (2023), https://doi.org/10.1109/CVPRW53098.2021.00027 (2021) (+5)	https://doi.org/10.1109/TIP.2018.2806202 (2017), https://doi.org/10.1109/CVPR.2017.186 (2017)	The Test1200 dataset is primarily used for evaluating deraining algorithms, focusing on the removal of rain streaks from images. It provides a large set of images with synthetic rain, enabling comprehensive testing across various rain intensities. This dataset facilitates the validation of algorithm robustness and reliability, ensuring effective deraining performance in diverse conditions.; The Test1200 dataset is primarily used for evaluating and validating deraining methods in image restoration. It contains a moderate to large set of images with synthetic rain, enabling researchers to assess the performance, robustness, and scalability of restoration algorithms using metrics like PSNR and SSIM. The dataset supports comprehensive evaluations of model generalization and consistency across diverse rainy conditions, making it suitable for comparing different deraining techniques and ensuring reliable performance in varied scenarios.	Test1200	
cited_context | citing_context	Test2800	https://doi.org/10.48550/arXiv.2406.18242 (2024), https://doi.org/10.1109/TMM.2024.3407656 (2024), https://www.semanticscholar.org/paper/6af728164e4424218967fd356b3448f5037acd88 (2023), https://doi.org/10.1109/CVPRW53098.2021.00027 (2021) (+6)	https://doi.org/10.1109/TIP.2018.2806202 (2017), https://doi.org/10.1109/CVPR.2017.186 (2017)	The Test2800 dataset is primarily used for deraining research, specifically to assess and validate the performance of deraining algorithms. It provides a large set of images with synthetic rain, enabling researchers to evaluate these methods across diverse weather conditions and a broader range of images. This dataset enhances the robustness and reliability of deraining algorithm evaluations by offering a more extensive and varied test set.; The Test2800 dataset is primarily used for evaluating and training deraining methods, containing a large set of synthetic rainy images. It is employed to assess the performance of restoration algorithms using metrics like PSNR and SSIM, focusing on the effectiveness of techniques such as conditional generative adversarial networks. The dataset supports comprehensive benchmarking, scalability testing, and validation of model robustness and generalization across diverse image conditions.	Test2800	
cited_context	Text Deblur Dataset (TDD)	https://doi.org/10.1109/CVPR52733.2024.01482 (2024)	https://doi.org/10.5244/C.29.6 (2015)	The Text Deblur Dataset (TDD) is used to train models specifically for text deblurring. Researchers select 40K samples from the 66K available training samples to develop and refine deblurring algorithms. This dataset enables the enhancement of text clarity in images, addressing the challenge of restoring legibility in blurred text regions.		
cited_context | citing_context	Textpromptir	https://doi.org/10.48550/arXiv.2410.15067 (2024)	https://doi.org/10.48550/arXiv.2401.13221 (2024), https://doi.org/10.1007/978-3-031-72764-1_1 (2024)	The TextpromptIR dataset is used to test and evaluate text-prompt guided image restoration methods, focusing on the integration of textual information into the image restoration process. This dataset enables researchers to assess how effectively textual prompts can enhance or guide the restoration of images, addressing specific research questions related to the performance and accuracy of these methods.; The TextpromptIR dataset is used to evaluate text-guided image restoration techniques, focusing on enhancing image quality through textual prompts. It supports research in image restoration by providing a framework to assess methods that use text-based guidance, enabling the development and comparison of algorithms designed to restore images based on specific textual inputs.	TextpromptIR	
cited_context	TIP2018	https://doi.org/10.48550/arXiv.2203.06074 (2022)	https://doi.org/10.1109/CVPR.2018.00182 (2018)	The TIP2018 dataset is used to evaluate and compare the performance of various image restoration techniques, focusing on a diverse set of image degradations such as noise, rain, and moiré patterns. It provides a comprehensive set of challenges, enabling researchers to assess methods using metrics like PSNR and to address specific issues like denoising, de-raining, and demoireing. The dataset's diverse and varied image conditions facilitate robust evaluation across multiple restoration tasks.		
citing_context	TOLED	https://doi.org/10.48550/arXiv.2503.09403 (2025), https://doi.org/10.48550/arXiv.2505.18047 (2025), https://doi.org/10.48550/arXiv.2412.20157 (2024) (+1)	https://doi.org/10.1109/CVPR46437.2021.00906 (2020)	The TOLED dataset is used for evaluating and enhancing the quality of images captured by under-display cameras, particularly in OLED displays. It provides 10 paired images to assess restoration techniques, focusing on improving image clarity and reducing noise. The dataset supports research on generalizing image restoration models to unseen tasks, ensuring robust performance on real-world under-display camera images.		
citing_context	Transweather	https://doi.org/10.48550/arXiv.2410.15067 (2024)	https://doi.org/10.1109/CVPR52729.2023.02083 (2023)	The Transweather dataset is used for all-weather image restoration, specifically addressing rain, fog, and snow conditions. Researchers employ this dataset to develop and evaluate algorithms that enhance image quality under adverse weather conditions. The dataset's focus on diverse weather scenarios enables robust testing and validation of image restoration techniques, ensuring they perform effectively in real-world applications.		
citing_context	TURBID	https://doi.org/10.48550/arXiv.2506.16960 (2025)	https://doi.org/10.1109/LRA.2020.2974710 (2019)	The TURBID dataset is used to evaluate underwater image enhancement techniques, specifically focusing on improving visual perception. It is employed in methodologies that assess the effectiveness of these techniques, as evidenced by the use of Table 1 for performance evaluation. This dataset enables researchers to address the challenge of enhancing image clarity in underwater environments, which is crucial for various underwater imaging applications.		
cited_context | citing_context	U-Wadn	https://doi.org/10.48550/arXiv.2410.15067 (2024)	https://doi.org/10.48550/arXiv.2401.13221 (2024)	The U-WADN dataset is used to test and evaluate image restoration methods, particularly focusing on unified-width adaptive dynamic networks for all-in-one image restoration. It is employed to assess the performance of these methods in handling diverse image degradations, enabling researchers to compare and refine techniques for robust image restoration.; The U-WADN dataset is used for training and evaluating wide-adaptive dynamic networks, specifically to enhance all-in-one image restoration capabilities. It is particularly effective in handling underwater images, allowing researchers to assess and improve image restoration methods in challenging aquatic environments. This dataset enables the development and testing of algorithms that can adapt to varying image conditions, thereby advancing the field of image restoration.	U-WADN	
cited_context | citing_context	U45	https://doi.org/10.48550/arXiv.2501.12981 (2025)	https://doi.org/10.1109/TCSVT.2019.2963772 (2019)	The U45 dataset is used for evaluating and visually comparing underwater image enhancement algorithms, particularly focusing on real-world conditions under natural light. It enables researchers to assess the effectiveness of restoration techniques in enhancing underwater images, ensuring they perform well in natural lighting scenarios.; The U45 dataset is used for visual and real-world underwater image enhancement, specifically focusing on improving structural details under natural light conditions. It facilitates the testing and comparison of restoration techniques, enabling researchers to address the unique challenges of underwater imaging. This dataset supports the evaluation of methods designed to enhance image quality in natural lighting scenarios.	U45	
citing_context	UAVDT	https://doi.org/10.1109/TIP.2025.3572788 (2025)	https://doi.org/10.1007/978-3-030-01249-6_23 (2018)	The UAVDT dataset is used to evaluate the generalization capability of models in image restoration, particularly focusing on the unique degradation aspects of UAV-captured images, such as varying levels of haze. This dataset enables researchers to test and improve restoration techniques tailored to aerial imagery, enhancing the clarity and usability of UAV-captured images.		
cited_context | citing_context	Uccs	https://doi.org/10.48550/arXiv.2501.12981 (2025)	https://doi.org/10.1109/TCSVT.2019.2963772 (2019)	The UCCS dataset is used for evaluating and comparing underwater image enhancement techniques, particularly focusing on natural light conditions. It is employed to assess the effectiveness of methods like NU2Net in addressing color casts and improving overall image quality. The dataset enables researchers to visually compare restoration results and evaluate the performance of different enhancement algorithms in realistic underwater environments.; The UCCS dataset is used for evaluating and comparing the performance of underwater image enhancement algorithms. It focuses on enhancing structural details and correcting color casts under natural light conditions. Researchers use it to test the effectiveness of methods like NU2Net in improving overall image quality, making it valuable for visual assessments and algorithmic comparisons in underwater imaging.	UCCS	
cited_context	UCID	https://doi.org/10.48550/arXiv.2404.12091 (2024)	https://doi.org/10.1117/12.525375 (2003)	The UCID dataset is used to illustrate the potential overlap in ground truths with the BSD dataset, impacting the separation of dataset-level or density-level properties in instance-level representations. This highlights issues in embedding space separation, aiding researchers in understanding and addressing these overlaps in their methodologies.		
citing_context	UHD-LOL	https://doi.org/10.1109/TCSVT.2024.3516074 (2024)	https://doi.org/10.48550/arXiv.2212.11548 (2022)	The UHD-LOL dataset is used to enhance low-light images in ultra-high-definition settings, specifically addressing complex lighting conditions. Researchers apply this dataset to improve image quality under challenging lighting scenarios, focusing on methodologies that enhance visibility and detail in low-light environments. This dataset enables the development and evaluation of advanced image enhancement techniques, ensuring better performance in real-world applications.		
cited_context | citing_context	Uieb	https://doi.org/10.48550/arXiv.2501.12981 (2025), https://doi.org/10.48550/arXiv.2501.12981 (2025), https://doi.org/10.1145/3664647.3681621 (2024)	https://doi.org/10.1016/j.image.2020.115978 (2019)	The UIEB dataset is used in research to evaluate and compare the visual quality and restoration effectiveness of enhanced underwater images against reference images. This involves assessing the performance of image enhancement techniques, focusing on how well they improve the clarity and detail of underwater scenes. The dataset enables researchers to quantitatively and qualitatively analyze the outcomes of different restoration methods, ensuring that enhancements are both visually appealing and scientifically valid.; The UIEB dataset is used for evaluating and comparing underwater image enhancement algorithms. It focuses on assessing the visual quality and restoration effectiveness of enhanced underwater images against reference images. This dataset enables researchers to improve visual clarity and quality in underwater environments, supporting the development and refinement of image enhancement techniques.	UIEB	
cited_context	UIRD-12	https://doi.org/10.48550/arXiv.2410.08688 (2024)	https://doi.org/10.48550/arXiv.2407.04621 (2024)	The UIRD-12 dataset is used to evaluate image restoration methods, specifically One-to-Many and One-to-Composite techniques, as well as the CoR method. It focuses on assessing performance across various degradation types, enabling researchers to compare and analyze the effectiveness of different restoration approaches.		
cited_context | citing_context	Unseen Under-Display-Camera Dataset	https://doi.org/10.48550/arXiv.2310.10123 (2023)	https://doi.org/10.48550/arXiv.2309.06380 (2023)	The unseen under-display-camera dataset is used to test image restoration techniques, specifically focusing on reducing artifacts and enhancing image quality in under-display-camera images. Researchers employ this dataset to evaluate and improve the performance of image restoration algorithms, addressing challenges unique to under-display-camera technology.; The unseen under-display-camera dataset is used to test image restoration techniques, specifically focusing on artifact removal and enhancing image quality from under-display-camera images. This dataset enables researchers to evaluate and improve the performance of image restoration algorithms in addressing unique artifacts and quality issues associated with under-display-camera technology.	unseen under-display-camera dataset	
cited_context	URBAN10	https://doi.org/10.1007/978-3-030-58523-5_36 (2020)	https://doi.org/10.1007/978-3-642-27413-8_47 (2010)	The URBAN10 dataset is used for assessing image restoration in urban scenes, specifically focusing on the preservation of texture and structure in complex environments. Researchers employ this dataset to evaluate the effectiveness of image restoration techniques in maintaining fine details and structural integrity, which are crucial for applications in urban scene analysis and visualization.		
cited_context | citing_context	Urban100	https://doi.org/10.48550/arXiv.2410.15067 (2024), https://doi.org/10.48550/arXiv.2406.18242 (2024), https://doi.org/10.1109/CVPR52688.2022.01693 (2022) (+18), https://doi.org/10.48550/arXiv.2410.15067 (2024), https://doi.org/10.1109/ICCVW54120.2021.00210 (2021), https://doi.org/10.1109/CVPR52688.2022.00564 (2021) (+36)	https://doi.org/10.1109/ICCV.2001.937655 (2001)	The Urban100 dataset is primarily used to evaluate image restoration techniques, particularly in urban scenes with complex structures and high-resolution details. It is employed to assess the performance of various restoration methods, including denoising, deblurring, and handling issues like rain, haze, and low-light conditions. The dataset's 100 diverse urban images enable researchers to measure PSNR improvements and the effectiveness of algorithms in preserving architectural details and textures.; The Urban100 dataset is primarily used to evaluate image restoration methods, particularly in urban scenes. It focuses on assessing the performance of GAN-based super-resolution and denoising algorithms using metrics like PSNR and LPIPS. The dataset emphasizes architectural details, textures, and fine details, making it suitable for testing under various noise levels and conditions such as rain, blur, and low light.	Urban100	
cited_context	USC-SIPI	https://www.semanticscholar.org/paper/3d3c5fcbc40aadccceda58d3d9c5cd00588ea0b7 (2022), https://doi.org/10.1109/CVPR52729.2023.00958 (2023)	https://www.semanticscholar.org/paper/744fe47157477235032f7bb3777800f9f2f45e52 (2017)	The USC-SIPI dataset is used to evaluate image restoration models, specifically focusing on 256 × 256 images. It assesses the restoration quality of models like DDRM and GDP, emphasizing their effectiveness on diverse image content beyond ImageNet classes. The dataset enables researchers to test and validate the performance of these models on a wide range of image types, ensuring robustness and generalizability in image restoration tasks.		
cited_context	VE-LOL	https://doi.org/10.1109/CVPR52729.2023.00958 (2023)	https://doi.org/10.1109/TPAMI.2021.3126387 (2021)	The VE-LOL dataset is used to assess video enhancement in low-light scenarios, focusing on temporal consistency and noise reduction. Researchers employ this dataset to evaluate and improve algorithms designed to enhance video quality under low-light conditions, ensuring that the enhanced videos maintain temporal coherence and reduced noise levels. This dataset enables the development and testing of advanced video processing techniques specifically tailored for challenging lighting environments.		
cited_context	VE-LOL-L	https://doi.org/10.1109/CVPR52729.2023.00958 (2023)	https://doi.org/10.1109/TPAMI.2021.3126387 (2021)	The VE-LOL-L dataset is used for evaluating low-light video enhancement, specifically focusing on temporal consistency and visual quality in dark videos. It assesses the performance of methods like GDP in enhancing video content under low-light conditions, ensuring that the enhanced videos maintain both clarity and temporal coherence. This dataset enables researchers to rigorously test and compare different enhancement techniques in challenging lighting scenarios.		
cited_context	VKITTI 2	https://doi.org/10.48550/arXiv.2409.15278 (2024)	https://www.semanticscholar.org/paper/c6e57b2d36a4231cef82d7e0e27413ae4bb4dee5 (2020)	The VKITTI 2 dataset is utilized for evaluating image restoration techniques, providing synthetic data with controlled variations. Researchers use this dataset to assess the performance of models under different conditions, focusing on the accuracy and robustness of image restoration methods. The controlled nature of the synthetic data allows for a rigorous evaluation of these techniques, enabling researchers to identify strengths and weaknesses in their models.		
cited_context	Waterloo Exploration	https://doi.org/10.48550/arXiv.2301.11699 (2023)	https://doi.org/10.1109/TPAMI.2010.161 (2011)	The Waterloo Exploration dataset is used to train models for image denoising, providing a rich set of high-quality images that enhance the comprehensiveness of the training process. This dataset supports the development of robust image restoration techniques by offering diverse and high-fidelity images, enabling researchers to improve model performance in denoising tasks.		
cited_context	Waterloo Exploration Database	https://doi.org/10.1109/TPAMI.2021.3088914 (2020)	https://doi.org/10.1109/CVPRW.2017.150 (2017)	The Waterloo Exploration Database contributes 4,744 images to enhance the diversity and scale of image restoration training sets. This dataset is specifically used to improve the robustness and generalization of image restoration models by providing a wide range of image types and conditions. It supports research in developing more effective and versatile image restoration techniques.		
cited_context	Waterloo Exploration Database (WED)	https://doi.org/10.1109/CVPRW.2018.00121 (2018), https://doi.org/10.48550/arXiv.2306.05390 (2023)	https://doi.org/10.1109/TIP.2016.2631888 (2017)	The Waterloo Exploration Database (WED) is used in image restoration research, specifically for denoising and dejpeg tasks. It contains 4,744 images and is utilized to train models like MWCNN, introducing challenges for image quality assessment and restoration. The dataset's diverse image content and degradation types enable robust evaluation and development of restoration algorithms.		
cited_context	Waterloo Exploration Dataset (WED)	https://doi.org/10.48550/arXiv.2412.20066 (2024)	https://doi.org/10.1109/TIP.2016.2631888 (2017)	The Waterloo Exploration Dataset (WED) is used for training models like MaIR on synthetic noise removal tasks. It introduces new challenges for image quality assessment, enhancing the robustness of restoration algorithms. The dataset's synthetic noise characteristics enable researchers to evaluate and improve the performance of image restoration techniques under controlled conditions.		
cited_context	WaterlooED	https://doi.org/10.48550/arXiv.2407.20928 (2024)	https://doi.org/10.1109/ICCV.2001.937655 (2001)	The WaterlooED dataset is used for training in image restoration, providing a diverse set of images to enhance model robustness. It is specifically employed to improve the performance of restoration models by exposing them to a wide range of image conditions, thereby addressing research questions related to the effectiveness and generalizability of these models.		
citing_context	WDC	https://doi.org/10.48550/arXiv.2503.09131 (2025)		The WDC dataset is used to evaluate all-in-one image restoration methods, particularly focusing on the effectiveness of partitioning strategies in urban areas. This involves assessing how well these methods can restore images by dividing them into manageable sections, enhancing the accuracy and efficiency of restoration in complex urban environments.		
cited_context | citing_context	Weatherstream	https://doi.org/10.48550/arXiv.2410.08177 (2024), https://doi.org/10.48550/arXiv.2410.15067 (2024), https://doi.org/10.1109/TIP.2024.3501855 (2024), https://doi.org/10.48550/arXiv.2410.15067 (2024)	https://doi.org/10.1109/CVPR52729.2023.01297 (2023)	The WeatherStream dataset is used to evaluate and test weather condition restoration models, specifically focusing on dehazing, deraining, and desnowing algorithms. It provides a large set of paired images: 4,500 for dehazing, 4,800 for deraining, and 3,960 for desnowing. This extensive dataset enables researchers to train and validate models, ensuring robust performance across various weather conditions.; The WeatherStream dataset is used to train and evaluate image restoration models, specifically focusing on realistic weather degradation scenarios such as rain, fog, and snow. It enhances the robustness of models like MWFormer-real and TransWeather-real by providing real-world frames, enabling researchers to assess and improve the performance of single image deweathering techniques.	WeatherStream	
cited_context | citing_context	Web	https://doi.org/10.1109/TIP.2024.3456583 (2023)	https://doi.org/10.1109/TIP.2016.2631888 (2017)	The WEB dataset is used for training denoising models, providing web-based images that enhance the real-world applicability of these models. This dataset enables researchers to develop and refine algorithms that can effectively reduce noise in images sourced from the internet, improving their quality and usability in various applications.; The WEB dataset is used for training denoising models, providing web images that enhance the real-world applicability of these models. This dataset enables researchers to develop and refine algorithms that can effectively reduce noise in images sourced from the internet, improving their quality and usability in various applications.	WEB	
cited_context | citing_context	Wed	https://doi.org/10.48550/arXiv.2410.15067 (2024), https://doi.org/10.1109/CVPR52733.2024.00277 (2023), https://doi.org/10.48550/arXiv.2411.18466 (2024) (+25), https://doi.org/10.48550/arXiv.2410.15067 (2024), https://doi.org/10.1109/CVPR52688.2022.01693 (2022), https://doi.org/10.48550/arXiv.2410.08688 (2024) (+24)	https://doi.org/10.1109/TIP.2016.2631888 (2017)	The WED dataset is primarily used for image denoising research, providing a diverse set of images to train and evaluate denoising algorithms. It is employed in both training and testing phases, often with synthetic Gaussian noise, to enhance and assess the performance of image restoration models. The dataset complements other benchmarks like BSD400, offering additional challenges and variations in image quality, which helps in evaluating the robustness and effectiveness of denoising techniques under various degradations.; The WED dataset is primarily used for training and evaluating image restoration models, focusing on various degradations such as rain, noise, blur, and low-light conditions. It provides 4,744 high-quality images, enabling researchers to assess and enhance model performance in removing watermarks, denoising, and restoring images under multiple degradation scenarios. The dataset serves as a benchmark for testing the robustness and effectiveness of image restoration techniques, particularly in challenging conditions.	WED	
cited_context	WIDER-Test	https://doi.org/10.48550/arXiv.2308.15070 (2023)	https://doi.org/10.1109/ICCV.2015.425 (2014)	The WIDER-Test dataset is used to evaluate the BFR method on real-world face images, focusing on face detection and restoration capabilities. It assesses performance in complex scenarios with varying scales and occlusions, enabling researchers to test and improve face detection and restoration algorithms in practical conditions.		
cited_context	Wild dataset	https://doi.org/10.1109/TCSVT.2023.3286405 (2024)	https://doi.org/10.1109/ICCV.2017.423 (2017)	The Wild dataset is used to evaluate image restoration algorithms by testing their performance on 55 image triplets from diverse real-world scenes. It assesses the generalization and robustness of these algorithms across various conditions, emphasizing their effectiveness in wild settings. This dataset enables researchers to validate the reliability of image restoration techniques in practical, uncontrolled environments.		
cited_context	WorldView II	https://www.semanticscholar.org/paper/4b688c486f2b223212682fd2b3f72cc0caa94b1b (2023)	https://doi.org/10.1007/978-3-031-19800-7_10 (2022)	The WorldView II dataset is used for evaluating image restoration techniques, particularly in enhancing the clarity and detail of high-resolution satellite imagery. Researchers employ this dataset to assess the effectiveness of various restoration methods, focusing on improving image quality and detail. This dataset's high-resolution characteristics make it suitable for testing and validating advanced image restoration algorithms.		
cited_context	WorldView III	https://www.semanticscholar.org/paper/4b688c486f2b223212682fd2b3f72cc0caa94b1b (2023)	https://doi.org/10.1007/978-3-031-19800-7_10 (2022)	The WorldView III dataset is used for evaluating image restoration techniques, particularly focusing on high-resolution satellite imagery. Researchers employ this dataset to enhance the clarity and detail of satellite images, addressing specific challenges in image restoration. The high resolution and detailed nature of the dataset enable precise evaluation and improvement of restoration algorithms.		
cited_context	Yang’s dataset (RS)	https://doi.org/10.48550/arXiv.2407.04621 (2024)	https://doi.org/10.1109/TIP.2018.2806202 (2017)	Yang’s dataset (RS) is primarily used for image deraining research, featuring both synthetic and real rain-streak images. It is employed to test and evaluate the performance of deraining algorithms, focusing on the effectiveness of removing rain streaks from images. The dataset's inclusion of diverse rain conditions enables researchers to assess algorithm robustness and accuracy in various scenarios.		
cited_context	Yang’s rainy dataset	https://doi.org/10.48550/arXiv.2407.04621 (2024)	https://doi.org/10.1109/TIP.2016.2639450 (2017)	Yang’s rainy dataset is used for evaluating rain removal techniques, featuring both synthetic and real rainy images. Researchers employ this dataset to assess the performance of rain detection and removal methods, focusing on improving image clarity and quality in rainy conditions. The dataset's inclusion of diverse rainy scenarios enhances its utility for benchmarking and validating these methods.		
