{
  "results": [
    {
      "context_text": "Additionally, [2],[14].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not provide any specific information about datasets or their usage. No dataset names are mentioned.",
      "processing_time": 15.590540170669556,
      "citing_paper_id": "269720399",
      "cited_paper_id": 4252896
    },
    {
      "context_text": "Given an event stream with N events ε = , where each event contains the position ( x k , y k ) , the timestamp t k and the polarity p k , we represent the event stream in the voxel grid format following the approach outlined in [16].",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific dataset names, only a method for representing event streams in a voxel grid format.",
      "processing_time": 15.02682638168335,
      "citing_paper_id": "269720399",
      "cited_paper_id": 56475917
    },
    {
      "context_text": "Datasets: We conduct evaluations of our proposed method on two stereo event camera datasets, the Multi-Vehicle Stereo Camera Dataset (MVSEC) [23] and the stereo event camera dataset (DSEC) [24].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Multi-Vehicle Stereo Camera Dataset (MVSEC)",
        "stereo event camera dataset (DSEC)"
      ],
      "dataset_descriptions": {
        "Multi-Vehicle Stereo Camera Dataset (MVSEC)": "Used to evaluate the proposed method for stereo depth estimation using event cameras, focusing on multi-vehicle scenarios.",
        "stereo event camera dataset (DSEC)": "Used to evaluate the proposed method for stereo depth estimation using event cameras, specifically in driving scenarios."
      },
      "confidence_score": 1.0,
      "reasoning": "The citation clearly mentions two specific datasets used for evaluating the proposed method in the context of stereo depth estimation using event cameras.",
      "processing_time": 40.18294095993042,
      "citing_paper_id": "269720399",
      "cited_paper_id": 232170230
    },
    {
      "context_text": "Compared to previous methods of converting the overall voxel features into token [18], [19], we design an event-based temporal aggregation module particularly for the voxel representation of events as shown in Fig.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only a methodological comparison. No dataset names are present in the text.",
      "processing_time": 25.40410828590393,
      "citing_paper_id": "269720399",
      "cited_paper_id": 244707609
    },
    {
      "context_text": "Compared to previous methods of converting the overall voxel features into token [18], [19], we design an event-based temporal aggregation module particularly for the voxel representation of events as shown in Fig.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only a methodological comparison. No dataset names are present in the text.",
      "processing_time": 25.40410828590393,
      "citing_paper_id": "269720399",
      "cited_paper_id": 254564733
    },
    {
      "context_text": "We quantitatively compare the performance of our model with previous state-of-the-art methods, including DDES [9], E-Stereo [26], EITNet [25], Se-cff [10], DTC-SPADE [11], and DTC-PDS [11].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only models and methods. There are no clear identifiers for datasets within the text.",
      "processing_time": 14.236695766448975,
      "citing_paper_id": "269720399",
      "cited_paper_id": 262638843
    },
    {
      "context_text": "We quantitatively compare the performance of our model with previous state-of-the-art methods, including DDES [9], E-Stereo [26], EITNet [25], Se-cff [10], DTC-SPADE [11], and DTC-PDS [11].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only models and methods. There are no clear identifiers for datasets within the text.",
      "processing_time": 14.236695766448975,
      "citing_paper_id": "269720399",
      "cited_paper_id": null
    },
    {
      "context_text": "We quantitatively compare the performance of our model with previous state-of-the-art methods, including DDES [9], E-Stereo [26], EITNet [25], Se-cff [10], DTC-SPADE [11], and DTC-PDS [11].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only models and methods. There are no clear identifiers for datasets within the text.",
      "processing_time": 14.236695766448975,
      "citing_paper_id": "269720399",
      "cited_paper_id": null
    },
    {
      "context_text": "On the MVSEC dataset, we perform dense disparity estimation using the entire ground truth for evaluation, following [9], [11], [25].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "MVSEC"
      ],
      "dataset_descriptions": {
        "MVSEC": "Used for dense disparity estimation in event-based stereo depth estimation, leveraging the entire ground truth for evaluation."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions the MVSEC dataset, which is a specific dataset used for event-based stereo depth estimation. The dataset is used for dense disparity estimation with full ground truth for evaluation.",
      "processing_time": 33.40014696121216,
      "citing_paper_id": "269720399",
      "cited_paper_id": 262638843
    },
    {
      "context_text": "On the MVSEC dataset, we perform dense disparity estimation using the entire ground truth for evaluation, following [9], [11], [25].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "MVSEC"
      ],
      "dataset_descriptions": {
        "MVSEC": "Used for dense disparity estimation in event-based stereo depth estimation, leveraging the entire ground truth for evaluation."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions the MVSEC dataset, which is a specific dataset used for event-based stereo depth estimation. The dataset is used for dense disparity estimation with full ground truth for evaluation.",
      "processing_time": 33.40014696121216,
      "citing_paper_id": "269720399",
      "cited_paper_id": null
    },
    {
      "context_text": "For MVSEC, following the previous setting as [9], [11], [17], [25], [26], we divide the Indoor Flying dataset into three subsets and use split 1 and split 3.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Indoor Flying dataset"
      ],
      "dataset_descriptions": {
        "Indoor Flying dataset": "Used to evaluate event-based stereo depth estimation methods, specifically dividing the dataset into three subsets and using split 1 and split 3 for training and evaluation."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions 'Indoor Flying dataset' which is a specific dataset used in event-based stereo depth estimation research. It is divided into subsets and specific splits are used.",
      "processing_time": 34.12636113166809,
      "citing_paper_id": "269720399",
      "cited_paper_id": 262638843
    },
    {
      "context_text": "For MVSEC, following the previous setting as [9], [11], [17], [25], [26], we divide the Indoor Flying dataset into three subsets and use split 1 and split 3.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Indoor Flying dataset"
      ],
      "dataset_descriptions": {
        "Indoor Flying dataset": "Used to evaluate event-based stereo depth estimation methods, specifically dividing the dataset into three subsets and using split 1 and split 3 for training and evaluation."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions 'Indoor Flying dataset' which is a specific dataset used in event-based stereo depth estimation research. It is divided into subsets and specific splits are used.",
      "processing_time": 34.12636113166809,
      "citing_paper_id": "269720399",
      "cited_paper_id": null
    },
    {
      "context_text": "For MVSEC, following the previous setting as [9], [11], [17], [25], [26], we divide the Indoor Flying dataset into three subsets and use split 1 and split 3.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Indoor Flying dataset"
      ],
      "dataset_descriptions": {
        "Indoor Flying dataset": "Used to evaluate event-based stereo depth estimation methods, specifically dividing the dataset into three subsets and using split 1 and split 3 for training and evaluation."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions 'Indoor Flying dataset' which is a specific dataset used in event-based stereo depth estimation research. It is divided into subsets and specific splits are used.",
      "processing_time": 34.12636113166809,
      "citing_paper_id": "269720399",
      "cited_paper_id": null
    },
    {
      "context_text": "DDES [9] was the ﬁrst learning-based method that proposed a module for event sequence embedding, which stores events in a First-in First-out (FIFO) queue to preserve both temporal and spatial information concurrently.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (DDES) and its functionality. No verifiable resources are identified.",
      "processing_time": 26.92885971069336,
      "citing_paper_id": "269720399",
      "cited_paper_id": 262638843
    },
    {
      "context_text": "Given the disparities in data characteristics between the training and testing sets in split 1, coupled with the smaller sample size of the split 1 test set compared to split 3, averaging errors becomes ineffective [26].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only generic references to training and testing sets. No clear, verifiable resource names are provided.",
      "processing_time": 16.538368940353394,
      "citing_paper_id": "269720399",
      "cited_paper_id": null
    },
    {
      "context_text": "The advantages of stereo over monocular (e.g., [29] vs [121]) due to exploiting spatial parallax are consistently clear on both tables: mean errors decrease by 30–45%, and outliers also decrease (by more than half) while the number of recovered points remains.",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only a comparison between stereo and monocular methods. The cited papers' titles suggest they deal with event-based stereo depth estimation but do not explicitly name datasets.",
      "processing_time": 18.610233783721924,
      "citing_paper_id": "272911351",
      "cited_paper_id": 1082643
    },
    {
      "context_text": "The advantages of stereo over monocular (e.g., [29] vs [121]) due to exploiting spatial parallax are consistently clear on both tables: mean errors decrease by 30–45%, and outliers also decrease (by more than half) while the number of recovered points remains.",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only a comparison between stereo and monocular methods. The cited papers' titles suggest they deal with event-based stereo depth estimation but do not explicitly name datasets.",
      "processing_time": 18.610233783721924,
      "citing_paper_id": "272911351",
      "cited_paper_id": 250918780
    },
    {
      "context_text": "Images courtesy of [29, 121].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only images from two sources. The titles of the cited papers suggest they are relevant to event-based stereo depth estimation but do not specify datasets.",
      "processing_time": 27.7834529876709,
      "citing_paper_id": "272911351",
      "cited_paper_id": 1082643
    },
    {
      "context_text": "Images courtesy of [29, 121].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only images from two sources. The titles of the cited papers suggest they are relevant to event-based stereo depth estimation but do not specify datasets.",
      "processing_time": 27.7834529876709,
      "citing_paper_id": "272911351",
      "cited_paper_id": 250918780
    },
    {
      "context_text": "The small parallax motion (except during turning) also makes it difficult to estimate depth with monocular methods like EMVS [121].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a method (EMVS) which is excluded according to the rules.",
      "processing_time": 25.687989234924316,
      "citing_paper_id": "272911351",
      "cited_paper_id": 1082643
    },
    {
      "context_text": "The monocular method Event-based Multi-View Stereo ( EMVS ) [121] uses known camera poses to shoot rays from event pixels through 3D space (Fig.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions 'Event-based Multi-View Stereo (EMVS)', but it is described as a method rather than a dataset. No specific dataset is mentioned.",
      "processing_time": 26.344356775283813,
      "citing_paper_id": "272911351",
      "cited_paper_id": 1082643
    },
    {
      "context_text": "…reason for the prevalence of cooperative networks for event-based stereo matching is that they can be realized as Spiking Neural Networks ( SNN s), which can be implemented on highly efficient neuromorphic hardware like SpiNNaker [110], ROLLS [111], Loihi [112], DYNAP [113] and TrueNorth [114].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only hardware platforms for neuromorphic computing. No verifiable datasets are referenced.",
      "processing_time": 25.110438346862793,
      "citing_paper_id": "272911351",
      "cited_paper_id": 3608458
    },
    {
      "context_text": "…reason for the prevalence of cooperative networks for event-based stereo matching is that they can be realized as Spiking Neural Networks ( SNN s), which can be implemented on highly efficient neuromorphic hardware like SpiNNaker [110], ROLLS [111], Loihi [112], DYNAP [113] and TrueNorth [114].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only hardware platforms for neuromorphic computing. No verifiable datasets are referenced.",
      "processing_time": 25.110438346862793,
      "citing_paper_id": "272911351",
      "cited_paper_id": 25268038
    },
    {
      "context_text": "Values for CopNet [62] and TSES [57] are taken from [57], whereas the others are from [29].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only references to values from other works. No clear, verifiable datasets are identified.",
      "processing_time": 14.590923070907593,
      "citing_paper_id": "272911351",
      "cited_paper_id": 4412139
    },
    {
      "context_text": "Values for CopNet [62] and TSES [57] are taken from [57], whereas the others are from [29].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only references to values from other works. No clear, verifiable datasets are identified.",
      "processing_time": 14.590923070907593,
      "citing_paper_id": "272911351",
      "cited_paper_id": 250918780
    },
    {
      "context_text": "14: Stereo fusion architecture employed in TSES [57].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a stereo fusion architecture. No dataset names are present in the citation span.",
      "processing_time": 24.576656818389893,
      "citing_paper_id": "272911351",
      "cited_paper_id": 4412139
    },
    {
      "context_text": "The indoor drone sequences being widely used, but the outdoor driving sequences are not suitable for stereo due to the small baseline [57].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.0,
      "reasoning": "The context mentions 'indoor drone sequences' and 'outdoor driving sequences', which could imply datasets, but does not provide specific names. No clear, verifiable datasets are mentioned.",
      "processing_time": 16.168872117996216,
      "citing_paper_id": "272911351",
      "cited_paper_id": 4412139
    },
    {
      "context_text": "Similarly, Time Synchronized Event-based Stereo ( TSES ) [57] uses known and constant camera velocity to warp events during a short time interval.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (TSES) for event-based stereo depth estimation.",
      "processing_time": 15.570721626281738,
      "citing_paper_id": "272911351",
      "cited_paper_id": 4412139
    },
    {
      "context_text": "Image courtesy of [57]. non-spiking event stereo methods.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.0,
      "reasoning": "The citation does not mention any specific datasets, only a method or approach. The context is too vague to identify a dataset.",
      "processing_time": 24.76563549041748,
      "citing_paper_id": "272911351",
      "cited_paper_id": 4412139
    },
    {
      "context_text": "Grid-like 3D event representations [29, 57, 115] like DSIs have been shown to preserve finer details in depth estimation.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.2,
      "reasoning": "The context mentions 'DSIs' which could be a dataset, but there is no explicit mention of a dataset name. The context is more about the representation method and its effectiveness in depth estimation.",
      "processing_time": 29.97998309135437,
      "citing_paper_id": "272911351",
      "cited_paper_id": 4412139
    },
    {
      "context_text": "Grid-like 3D event representations [29, 57, 115] like DSIs have been shown to preserve finer details in depth estimation.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.2,
      "reasoning": "The context mentions 'DSIs' which could be a dataset, but there is no explicit mention of a dataset name. The context is more about the representation method and its effectiveness in depth estimation.",
      "processing_time": 29.97998309135437,
      "citing_paper_id": "272911351",
      "cited_paper_id": 250918780
    },
    {
      "context_text": "This has connections with the Contrast Maximization ( CMax ) framework [123, 124]: the depth slices of the DSI can be interpreted as Images of Warped Events ( IWEs ) and recovery of the 3D scene structure amounts to finding the depth map whose IWE has maximum contrast (i.e., best event alignment,…",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only methods and frameworks. The cited papers' titles do not provide additional dataset information.",
      "processing_time": 26.910343647003174,
      "citing_paper_id": "272911351",
      "cited_paper_id": 4597042
    },
    {
      "context_text": "This has connections with the Contrast Maximization ( CMax ) framework [123, 124]: the depth slices of the DSI can be interpreted as Images of Warped Events ( IWEs ) and recovery of the 3D scene structure amounts to finding the depth map whose IWE has maximum contrast (i.e., best event alignment,…",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only methods and frameworks. The cited papers' titles do not provide additional dataset information.",
      "processing_time": 26.910343647003174,
      "citing_paper_id": "272911351",
      "cited_paper_id": 119309624
    },
    {
      "context_text": "On the other hand, Dikov et al. [63] extended their previous cooperative method [69] by implementing it on SpiNNaker digital processor boards.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and hardware implementations.",
      "processing_time": 23.845133543014526,
      "citing_paper_id": "272911351",
      "cited_paper_id": 4833834
    },
    {
      "context_text": "On the other hand, Dikov et al. [63] extended their previous cooperative method [69] by implementing it on SpiNNaker digital processor boards.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and hardware implementations.",
      "processing_time": 23.845133543014526,
      "citing_paper_id": "272911351",
      "cited_paper_id": 34855834
    },
    {
      "context_text": "Firouzi et al. [69] also proposed a cooperative algorithm for events which employed an additional second pattern of inhibitory connections in their dynamic cooperative network to suppress ambiguous matches.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method or algorithm. The context focuses on the cooperative algorithm and its use of inhibitory connections.",
      "processing_time": 27.767937183380127,
      "citing_paper_id": "272911351",
      "cited_paper_id": 4833834
    },
    {
      "context_text": "Later, Piatkowska et al. [82] implemented a cooperative technique for depth computation using a winner-take-all mechanism to match temporally-close and spatially-constrained events.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for depth computation using event-driven sensors.",
      "processing_time": 12.042008876800537,
      "citing_paper_id": "272911351",
      "cited_paper_id": 6079544
    },
    {
      "context_text": "[129]), the scale invariant depth error (SILog Err), the sum of absolute value of relative differences in depth (AErrR), and δ -accuracy values on the percentage of points whose depth ratio with respect to GT is within some threshold (see [130]).",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only evaluation metrics. The cited paper title suggests a benchmark suite, but no specific dataset is named in the citation context.",
      "processing_time": 16.508806943893433,
      "citing_paper_id": "272911351",
      "cited_paper_id": 6724907
    },
    {
      "context_text": "It was then implemented in software by Delbruck’s student Hess [99].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method or tool developed by Delbruck’s student Hess.",
      "processing_time": 24.212051391601562,
      "citing_paper_id": "272911351",
      "cited_paper_id": 7083033
    },
    {
      "context_text": "Initial works [92, 97, 98, 105, 106] accumulated events into frames for compatibility with standard binocular vision techniques.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only a general approach of accumulating events into frames. No verifiable resources are identified.",
      "processing_time": 25.671422481536865,
      "citing_paper_id": "272911351",
      "cited_paper_id": 7224209
    },
    {
      "context_text": "Initial works [92, 97, 98, 105, 106] accumulated events into frames for compatibility with standard binocular vision techniques.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only a general approach of accumulating events into frames. No verifiable resources are identified.",
      "processing_time": 25.671422481536865,
      "citing_paper_id": "272911351",
      "cited_paper_id": 27059477
    },
    {
      "context_text": "Carneiro et al. [81] demonstrated that adding cameras to the stereo setup (e.g., trinocular vision, with more temporal and epipolar constraints), could further disambiguate and produce more reliable matches.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a methodological improvement in stereo vision setups.",
      "processing_time": 24.560482501983643,
      "citing_paper_id": "272911351",
      "cited_paper_id": 10712214
    },
    {
      "context_text": "To emphasize the benefits of accurate time information, [86] proposed a purely event-driven matching procedure using time-based and polarity-based correlation of the events, without aggregation.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for event-based stereo matching.",
      "processing_time": 23.99226975440979,
      "citing_paper_id": "272911351",
      "cited_paper_id": 11177597
    },
    {
      "context_text": "Besides timestamp and polarity, the orientation of object edges using Gabor filters has also been used as a supplementary signal for cooperative stereo [78].",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method involving Gabor filters for 3D reconstruction in event-driven stereo vision.",
      "processing_time": 13.47240662574768,
      "citing_paper_id": "272911351",
      "cited_paper_id": 12047627
    },
    {
      "context_text": "This is a challenging VIO dataset (EVO [156], ESVO [41], Ultimate SLAM [157] failed in most sequences).",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.3,
      "reasoning": "The citation mentions 'VIO dataset' but does not provide a specific name. The context suggests a dataset used for evaluating Visual-Inertial Odometry systems, but it lacks a clear identifier.",
      "processing_time": 30.35480499267578,
      "citing_paper_id": "272911351",
      "cited_paper_id": 16588072
    },
    {
      "context_text": "It is an evolved version of previous datasets recorded by the same lab [152], [153].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.3,
      "reasoning": "The citation mentions an evolved version of previous datasets but does not specify the names of these datasets. The context is too vague to confidently extract specific dataset names.",
      "processing_time": 28.381078243255615,
      "citing_paper_id": "272911351",
      "cited_paper_id": 17272393
    },
    {
      "context_text": "To reduce false stereo correspondences and therefore enhance matching quality, temporal matching was aided by additional constraints (i.e., equations), such as epipolar and ordering constraints [85].",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods or constraints used in stereo matching.",
      "processing_time": 12.362958669662476,
      "citing_paper_id": "272911351",
      "cited_paper_id": 17693733
    },
    {
      "context_text": "Accurate stereo correspondence between the event and frame-based camera would enable better alignment between them, thus producing a similar effect as a DAVIS camera [151] but with higher image quality and resolution.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a comparison between different types of cameras. No verifiable dataset names are present.",
      "processing_time": 14.196958065032959,
      "citing_paper_id": "272911351",
      "cited_paper_id": 19091270
    },
    {
      "context_text": "Recent works have proven this idea on small-scale scenes with stationary cameras [43, 49, 63, 64].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to other works. No dataset names are provided in the context.",
      "processing_time": 24.944710731506348,
      "citing_paper_id": "272911351",
      "cited_paper_id": 34855834
    },
    {
      "context_text": "Recent works have proven this idea on small-scale scenes with stationary cameras [43, 49, 63, 64].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to other works. No dataset names are provided in the context.",
      "processing_time": 24.944710731506348,
      "citing_paper_id": "272911351",
      "cited_paper_id": 205698386
    },
    {
      "context_text": "…(i.e., frame-based) paradigm that decouples the stereo depth estimation problem in two sequential steps: first, establishing stereo correspondences across image planes (“stereo matching”), and then back-projecting the correspondences to compute the associated 3D point (“triangulation”) [101].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only general techniques and steps in stereo depth estimation.",
      "processing_time": 12.571349382400513,
      "citing_paper_id": "272911351",
      "cited_paper_id": 44969055
    },
    {
      "context_text": "Under the static scene assumption, the warp is approximated by the motion field [101] produced by the camera velocity and candidate scene depth (per pixel).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method or technique for approximating the warp under a static scene assumption.",
      "processing_time": 25.078367710113525,
      "citing_paper_id": "272911351",
      "cited_paper_id": 44969055
    },
    {
      "context_text": "For example, the DVS stereo dataset from Andreopoulos et al. [55] comprises a setup of stereo DAVIS240C cameras that is stationary, and therefore only perceives dynamic IMOs in the scene like a rotating fan and a toy butterfly.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "DVS stereo dataset"
      ],
      "dataset_descriptions": {
        "DVS stereo dataset": "Used to capture dynamic scenes with stationary stereo DAVIS240C cameras, focusing on event-based perception of moving objects like a rotating fan and a toy butterfly."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions a specific dataset, 'DVS stereo dataset', which is used for event-based stereo depth estimation with a stationary setup of stereo DAVIS240C cameras.",
      "processing_time": 35.44199824333191,
      "citing_paper_id": "272911351",
      "cited_paper_id": 46937991
    },
    {
      "context_text": "While the methods discussed so far employed some event pre-processing (like rectification) on conventional hardware, Andreopoulos et al. [55] implemented a full event-driven non-cooperative local stereo matching pipeline on IBM TrueNorth digital neuromorphic processors.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method implemented on a particular hardware platform.",
      "processing_time": 11.849876403808594,
      "citing_paper_id": "272911351",
      "cited_paper_id": 46937991
    },
    {
      "context_text": "This need has been identified in the past [55, 135], leading to proposal of a benchmark [135], but it has not been widely adopted.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a benchmark which is excluded as per instructions.",
      "processing_time": 24.938403129577637,
      "citing_paper_id": "272911351",
      "cited_paper_id": 46937991
    },
    {
      "context_text": "…resolution rather than HD cameras because they observed a “smearing effect” on top of the surface of active events, similar to problems reported by [126, 155], where motion blur in the event stream or timestamp delays was observed from sudden and significant contrast changes on DAVIS event cameras.",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation discusses issues with event cameras, particularly motion blur and timestamp delays, but does not mention any specific datasets.",
      "processing_time": 13.455087423324585,
      "citing_paper_id": "272911351",
      "cited_paper_id": 49864158
    },
    {
      "context_text": "Then, corners are tracked on the motion-compensated time surfaces using Arc ∗ [126], and used to triangulate for instantaneous stereo matching, as well as for temporal camera tracking.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for corner detection and tracking using event cameras.",
      "processing_time": 12.018205165863037,
      "citing_paper_id": "272911351",
      "cited_paper_id": 49864158
    },
    {
      "context_text": "Time-surface matching subsumes the temporal coincidence assumption of stereo events and is a building block for finding stereo matches in established event-only stereo methods for VO/SLAM like [41, 58].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and approaches. The context focuses on the technique of time-surface matching and its application in stereo event cameras.",
      "processing_time": 16.80485963821411,
      "citing_paper_id": "272911351",
      "cited_paper_id": 49877954
    },
    {
      "context_text": "This event stereo dataset [58] (Fig.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.3,
      "reasoning": "The context mentions 'event stereo dataset' which is likely a specific dataset used for stereo depth estimation with event cameras. However, the name is not fully specified in the citation span.",
      "processing_time": 27.173877000808716,
      "citing_paper_id": "272911351",
      "cited_paper_id": 49877954
    },
    {
      "context_text": "R P G [ 58 ] N/A M V S E C [ 136 ] D S E C [ 45 ] T U M -",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "M V S E C",
        "D S E C"
      ],
      "dataset_descriptions": {
        "M V S E C": "Used for evaluating event-based stereo depth estimation algorithms, focusing on semi-dense 3D reconstruction with stereo event cameras.",
        "D S E C": "Used for evaluating event-based stereo depth estimation algorithms, focusing on dense 3D reconstruction with stereo event cameras."
      },
      "confidence_score": 0.7,
      "reasoning": "The citation mentions multiple datasets, including M V S E C and D S E C, which are likely event-based stereo datasets. However, the context does not provide explicit details on their usage.",
      "processing_time": 41.893598556518555,
      "citing_paper_id": "272911351",
      "cited_paper_id": 49877954
    },
    {
      "context_text": "Its mapping module is an improved version of the method in [58], where instantaneous depth is estimated by matching time surfaces across cameras (Fig.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for estimating depth using stereo event cameras.",
      "processing_time": 11.83895754814148,
      "citing_paper_id": "272911351",
      "cited_paper_id": 49877954
    },
    {
      "context_text": "However, it does not contain GT depth, so only qualitative evaluation of 3D reconstruction algorithms is possible.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific dataset names, only a general limitation about the lack of ground truth depth data.",
      "processing_time": 13.24707841873169,
      "citing_paper_id": "272911351",
      "cited_paper_id": 73729084
    },
    {
      "context_text": "However, it does not contain GT depth, so only qualitative evaluation of 3D reconstruction algorithms is possible.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific dataset names, only a general limitation about the lack of ground truth depth data.",
      "processing_time": 13.24707841873169,
      "citing_paper_id": "272911351",
      "cited_paper_id": 274611240
    },
    {
      "context_text": "However, it does not contain GT depth, so only qualitative evaluation of 3D reconstruction algorithms is possible.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific dataset names, only a general limitation about the lack of ground truth depth data.",
      "processing_time": 13.24707841873169,
      "citing_paper_id": "272911351",
      "cited_paper_id": 276652376
    },
    {
      "context_text": "6: Results of line-based stereo matching proposed in [54].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method or results from another paper.",
      "processing_time": 12.548550367355347,
      "citing_paper_id": "272911351",
      "cited_paper_id": 84182058
    },
    {
      "context_text": "Image courtesy of [54].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only an image courtesy. No verifiable resources are identified.",
      "processing_time": 24.36882185935974,
      "citing_paper_id": "272911351",
      "cited_paper_id": 84182058
    },
    {
      "context_text": "In this regard, a line-based feature matching algorithm for depth estimation was proposed in [54] which showed promising results on short real-world scenarios with moving cameras (Fig.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for depth estimation using stereo dynamic vision sensors.",
      "processing_time": 12.340273380279541,
      "citing_paper_id": "272911351",
      "cited_paper_id": 84182058
    },
    {
      "context_text": "Zou et al. [65] also demonstrated decent depth estimation results on sequences with ego-motion by stereo-matching sharp event images obtained by pixel-wise accumulation of events over their lifetime [108].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific dataset names, only a method for stereo-matching sharp event images. No clear, verifiable dataset is referenced.",
      "processing_time": 16.121339082717896,
      "citing_paper_id": "272911351",
      "cited_paper_id": 157060825
    },
    {
      "context_text": "2019 [3] Bio-inspired stereo vision with event cameras; focus on cooperative networks.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general reference to bio-inspired sensors and algorithms. No clear, verifiable datasets are identified.",
      "processing_time": 15.217703580856323,
      "citing_paper_id": "272911351",
      "cited_paper_id": 167210006
    },
    {
      "context_text": "Steffen et al [3] review various bio-inspired aspects of event cameras and the stereo algorithms that may benefit from their novel data.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a review of bio-inspired aspects of event cameras and stereo algorithms.",
      "processing_time": 13.62494444847107,
      "citing_paper_id": "272911351",
      "cited_paper_id": 167210006
    },
    {
      "context_text": "Steffen et al [3] review various bio-inspired aspects of event cameras and the stereo algorithms that may benefit from their novel data.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a review of bio-inspired aspects of event cameras and stereo algorithms.",
      "processing_time": 13.62494444847107,
      "citing_paper_id": "272911351",
      "cited_paper_id": null
    },
    {
      "context_text": "For example, in [64], the cooperative method is realized with a hierarchical SNN architecture of coincidence and disparity detector layers (Fig.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method and architecture. The context is too limited to infer any dataset usage.",
      "processing_time": 14.539273023605347,
      "citing_paper_id": "272911351",
      "cited_paper_id": 205698386
    },
    {
      "context_text": "Risi et al [43] further demonstrated the efficiency and performance of the DYNAP implementation of the cooperative network in [64] by evaluating it on complex real-world scenes from the DHP19 3D human pose tracking dataset.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "DHP19 3D human pose tracking dataset"
      ],
      "dataset_descriptions": {
        "DHP19 3D human pose tracking dataset": "Used to evaluate the performance of a spiking neural network model in complex real-world scenes, focusing on 3D human pose tracking."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions the DHP19 3D human pose tracking dataset, which is a specific, verifiable dataset used for evaluating the performance of a spiking neural network model.",
      "processing_time": 37.2957763671875,
      "citing_paper_id": "272911351",
      "cited_paper_id": 205698386
    },
    {
      "context_text": "Recently, Kim et al [34] demonstrated improved power and hardware efficiency by trading off latency and accuracy while implementing the architecture of [64] entirely on Field Programmable Gate Arrays ( FPGA ).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets. It focuses on the implementation of an architecture on FPGA, discussing power, hardware efficiency, latency, and accuracy.",
      "processing_time": 15.83686375617981,
      "citing_paper_id": "272911351",
      "cited_paper_id": 205698386
    },
    {
      "context_text": "Recently, Kim et al [34] demonstrated improved power and hardware efficiency by trading off latency and accuracy while implementing the architecture of [64] entirely on Field Programmable Gate Arrays ( FPGA ).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets. It focuses on the implementation of an architecture on FPGA, discussing power, hardware efficiency, latency, and accuracy.",
      "processing_time": 15.83686375617981,
      "citing_paper_id": "272911351",
      "cited_paper_id": 254531210
    },
    {
      "context_text": "7: Asynchronous cooperative stereo method in [64].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.0,
      "reasoning": "The citation does not mention any specific datasets, only a method or model. The context is too limited to infer the use of a dataset.",
      "processing_time": 14.949941635131836,
      "citing_paper_id": "272911351",
      "cited_paper_id": 205698386
    },
    {
      "context_text": "Image courtesy of [64]. of stereo algorithms known as “cooperative stereo” uses a dynamic network-based computational model of binocular stereopsis for finding correspondences, making it highly efficient for sparse asynchronous event-driven processing on specialized neuromorphic hardware (Fig.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method or model for event-based neuromorphic stereo vision systems.",
      "processing_time": 13.893925905227661,
      "citing_paper_id": "272911351",
      "cited_paper_id": 205698386
    },
    {
      "context_text": "StereoSpike [37] is the only SNN that effectively uses deep learning for stereo depth estimation.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions StereoSpike as a method for stereo depth estimation using spiking neural networks, but does not refer to a specific dataset.",
      "processing_time": 14.948047637939453,
      "citing_paper_id": "272911351",
      "cited_paper_id": 238198645
    },
    {
      "context_text": "Image courtesy of [42]. frames are missing, it can still output disparity.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.2,
      "reasoning": "The citation does not mention any specific datasets, only a method or approach. The context is too vague to identify a dataset.",
      "processing_time": 14.173390865325928,
      "citing_paper_id": "272911351",
      "cited_paper_id": 244306440
    },
    {
      "context_text": "11: Network structure of EI-Stereo [42].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a network structure. The title of the cited paper suggests a method or approach rather than a dataset.",
      "processing_time": 15.5107581615448,
      "citing_paper_id": "272911351",
      "cited_paper_id": 244306440
    },
    {
      "context_text": "While the former is more efficient and was also used in EI-Stereo [42] and Conc-Net [30], the latter produced better overall performance on the DSEC disparity benchmark.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "DSEC disparity benchmark"
      ],
      "dataset_descriptions": {
        "DSEC disparity benchmark": "Used to evaluate the performance of stereo depth estimation methods, specifically comparing event-based and intensity-based approaches."
      },
      "confidence_score": 1.0,
      "reasoning": "The citation mentions the 'DSEC disparity benchmark' which is a specific dataset used for evaluating stereo depth estimation methods.",
      "processing_time": 20.370365619659424,
      "citing_paper_id": "272911351",
      "cited_paper_id": 244306440
    },
    {
      "context_text": "The first learning-based stereo depth estimation using both frames and events was proposed in Event-Intensity Stereo ( EI-Stereo or EIS ) [42] (Fig.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions 'Event-Intensity Stereo (EI-Stereo or EIS)', but it does not refer to a dataset. It is a method or model for stereo depth estimation using both frames and events.",
      "processing_time": 18.6960551738739,
      "citing_paper_id": "272911351",
      "cited_paper_id": 244306440
    },
    {
      "context_text": "Recently, Furmonas et al published a review article [4] that dives into various event-based depth estimation meth-ods, both monocular and stereo.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a review of methods. No dataset names are present in the citation context.",
      "processing_time": 14.16762661933899,
      "citing_paper_id": "272911351",
      "cited_paper_id": 246656358
    },
    {
      "context_text": "2022 [4] Event-based monocular and stereo depth.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a review of methods and systems for event-based camera depth estimation.",
      "processing_time": 13.602654457092285,
      "citing_paper_id": "272911351",
      "cited_paper_id": 246656358
    },
    {
      "context_text": "However, this limits the FOV of the sensors (59 ◦ HFOV/ 34 ◦ VFOV in [127]) and the light incident on each pixel.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only sensor limitations. No dataset names are present in the citation context.",
      "processing_time": 26.27321219444275,
      "citing_paper_id": "272911351",
      "cited_paper_id": 248227281
    },
    {
      "context_text": "An alternative solution is using a beam splitter setup with separate frame-based and event-based cameras, like in [127, 145, 148, 149].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.0,
      "reasoning": "The context mentions a beam splitter setup with separate frame-based and event-based cameras but does not specify any datasets. The cited papers' titles do not clearly indicate the presence of a dataset.",
      "processing_time": 17.237552881240845,
      "citing_paper_id": "272911351",
      "cited_paper_id": 248227281
    },
    {
      "context_text": "An alternative solution is using a beam splitter setup with separate frame-based and event-based cameras, like in [127, 145, 148, 149].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.0,
      "reasoning": "The context mentions a beam splitter setup with separate frame-based and event-based cameras but does not specify any datasets. The cited papers' titles do not clearly indicate the presence of a dataset.",
      "processing_time": 17.237552881240845,
      "citing_paper_id": "272911351",
      "cited_paper_id": 250607506
    },
    {
      "context_text": "An alternative solution is using a beam splitter setup with separate frame-based and event-based cameras, like in [127, 145, 148, 149].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.0,
      "reasoning": "The context mentions a beam splitter setup with separate frame-based and event-based cameras but does not specify any datasets. The cited papers' titles do not clearly indicate the presence of a dataset.",
      "processing_time": 17.237552881240845,
      "citing_paper_id": "272911351",
      "cited_paper_id": 271892156
    },
    {
      "context_text": "More-over, only qualitative results are shown on depth estimation, which do not look accurate compared to monocular meth-ods like [127].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a comparison to monocular methods. No verifiable resources are identified.",
      "processing_time": 13.419249296188354,
      "citing_paper_id": "272911351",
      "cited_paper_id": 248227281
    },
    {
      "context_text": "I M O 2 [ 139 ] V E C t o r [ 138 ] H K U V I O [ 25 ] N/A M 3 E D [ 140 ] S H E F [ 39 ] (a) Sensors National University targets the problem of stereo 3D reconstruction between a frame-based and an event-based camera.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "EVIMO2",
        "VECtor"
      ],
      "dataset_descriptions": {
        "EVIMO2": "Used for motion segmentation, optical flow, structure from motion, and visual inertial odometry in indoor scenes, providing a rich dataset for event-based camera research.",
        "VECtor": "A versatile event-centric benchmark for multi-sensor SLAM, used to evaluate algorithms in various scenarios, enhancing the robustness of event-based stereo depth estimation."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions 'EVIMO2' and 'VECtor', which are specific datasets used in the field of event-based vision and multi-sensor SLAM. These datasets are relevant to the topic of event-based stereo depth estimation.",
      "processing_time": 32.30830144882202,
      "citing_paper_id": "272911351",
      "cited_paper_id": 248572428
    },
    {
      "context_text": "I M O 2 [ 139 ] V E C t o r [ 138 ] H K U V I O [ 25 ] N/A M 3 E D [ 140 ] S H E F [ 39 ] (a) Sensors National University targets the problem of stereo 3D reconstruction between a frame-based and an event-based camera.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "EVIMO2",
        "VECtor"
      ],
      "dataset_descriptions": {
        "EVIMO2": "Used for motion segmentation, optical flow, structure from motion, and visual inertial odometry in indoor scenes, providing a rich dataset for event-based camera research.",
        "VECtor": "A versatile event-centric benchmark for multi-sensor SLAM, used to evaluate algorithms in various scenarios, enhancing the robustness of event-based stereo depth estimation."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions 'EVIMO2' and 'VECtor', which are specific datasets used in the field of event-based vision and multi-sensor SLAM. These datasets are relevant to the topic of event-based stereo depth estimation.",
      "processing_time": 32.30830144882202,
      "citing_paper_id": "272911351",
      "cited_paper_id": 250127779
    },
    {
      "context_text": "The EV-IMO2 dataset [139] (Fig.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "EV-IMO2"
      ],
      "dataset_descriptions": {
        "EV-IMO2": "Used for motion segmentation, optical flow, structure from motion, and visual inertial odometry in indoor scenes, providing a benchmark for event-based stereo depth estimation."
      },
      "confidence_score": 1.0,
      "reasoning": "The citation mentions the EV-IMO2 dataset, which is a specific, verifiable dataset used for motion segmentation, optical flow, structure from motion, and visual inertial odometry in indoor scenes.",
      "processing_time": 23.475000143051147,
      "citing_paper_id": "272911351",
      "cited_paper_id": 248572428
    },
    {
      "context_text": "The VECtor benchmark dataset [138] (Fig.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "VECtor benchmark dataset"
      ],
      "dataset_descriptions": {
        "VECtor benchmark dataset": "Used to evaluate multi-sensor SLAM algorithms, focusing on event-based stereo depth estimation. The dataset provides versatile event-centric data for robust algorithm testing."
      },
      "confidence_score": 0.9,
      "reasoning": "The context mentions 'VECtor benchmark dataset' which is a specific, verifiable dataset. The title confirms it is a benchmark dataset for multi-sensor SLAM, which is relevant to event-based stereo depth estimation.",
      "processing_time": 23.475919246673584,
      "citing_paper_id": "272911351",
      "cited_paper_id": 250127779
    },
    {
      "context_text": "Moreover, it is inherently affected by shutter noise due to the DAVIS’ hybrid event-frame mode [138].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a characteristic of the DAVIS sensor. No verifiable resources are identified.",
      "processing_time": 13.007429599761963,
      "citing_paper_id": "272911351",
      "cited_paper_id": 250127779
    },
    {
      "context_text": "Specifically, it proposes a temporal disparity consistency loss to align GT disparity maps from different time steps, on top of established paradigms of L 1 disparity loss and Contrast Maximization losses [12, 115, 116].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only loss functions and methods. No verifiable resources are identified.",
      "processing_time": 12.313051700592041,
      "citing_paper_id": "272911351",
      "cited_paper_id": 250699235
    },
    {
      "context_text": "Recently, Shiba et al. [12] extended the CMax framework to simultaneously estimate depth and ego-motion using an optical flow warp.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (CMax framework) and a task (depth and ego-motion estimation).",
      "processing_time": 13.8729829788208,
      "citing_paper_id": "272911351",
      "cited_paper_id": 250699235
    },
    {
      "context_text": "However, as noticed in [29], the baseline of 11.84 cm is too big for the small depth range (nearest object ≈ 40cm away) in these sequences.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a baseline measurement and depth range. No verifiable resources are identified.",
      "processing_time": 12.513165712356567,
      "citing_paper_id": "272911351",
      "cited_paper_id": 250918780
    },
    {
      "context_text": "Event-based stereo depth estimation has notable advantages over monocular (temporal) stereo [29]: higher accurate, faster mapping, outlier removal and absolute scale recovery.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only discusses the advantages of event-based stereo depth estimation over monocular methods.",
      "processing_time": 13.869606256484985,
      "citing_paper_id": "272911351",
      "cited_paper_id": 250918780
    },
    {
      "context_text": "The calib A sequences were better calibrated than the calib B ones [29].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only calibration sequences which are not considered datasets according to the rules.",
      "processing_time": 11.97145676612854,
      "citing_paper_id": "272911351",
      "cited_paper_id": 250918780
    },
    {
      "context_text": "More details are provided in [29].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not provide any specific dataset names or usage details. It only references another paper for more details.",
      "processing_time": 12.15022611618042,
      "citing_paper_id": "272911351",
      "cited_paper_id": 250918780
    },
    {
      "context_text": "EMVS has been extended to the stereo setup in Multi-Camera ( MC )- EMVS [29], where DSIs computed from individual cameras are fused using element-wise operations like the harmonic mean, and the local maxima of the fused DSI yields the depth map (Fig.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific, verifiable datasets. It discusses a method (MC-EMVS) and its application in stereo depth estimation but does not reference any dataset.",
      "processing_time": 17.221851110458374,
      "citing_paper_id": "272911351",
      "cited_paper_id": 250918780
    },
    {
      "context_text": "To eliminate the cost of using two separate event cameras for stereo matching, [33] mounted a stereo lens on a single event camera (Fig.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for mounting a stereo lens on a single event camera.",
      "processing_time": 12.512754201889038,
      "citing_paper_id": "272911351",
      "cited_paper_id": 252476994
    },
    {
      "context_text": "8: Setup using a stereo lens and an event camera [33]. approximating performance metrics (from data reported in the original papers on different evaluation datasets).",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific dataset names, only a general reference to 'different evaluation datasets'. No multi-word proper nouns, acronyms, or hyphenated names with digits are present.",
      "processing_time": 17.96828818321228,
      "citing_paper_id": "272911351",
      "cited_paper_id": 252476994
    },
    {
      "context_text": "A stereo matching algorithm between a frame-based and an event camera has been proposed in [36].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a stereo matching algorithm. No verifiable resources are identified.",
      "processing_time": 12.509865522384644,
      "citing_paper_id": "272911351",
      "cited_paper_id": 253651036
    },
    {
      "context_text": "This architecture was also subsequently adapted and implemented in neuromorphic hardware platforms [43, 49], and on FPGA [34].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only hardware implementations. No dataset names are present in the citation span.",
      "processing_time": 13.57851266860962,
      "citing_paper_id": "272911351",
      "cited_paper_id": 254531210
    },
    {
      "context_text": "Hence, Liu et al [22] have proposed a direct method of state estimation using stereo events and IMU without explicit feature tracking, referred to as direct-ESVIO .",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (direct-ESVIO) for state estimation using stereo events and IMU.",
      "processing_time": 15.478160858154297,
      "citing_paper_id": "272911351",
      "cited_paper_id": 255125395
    },
    {
      "context_text": "Event-based Stereo Visual Odometry ( ESVO ) [41] is an established system on which others build upon [19, 22, 125].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (ESVO) and its usage in stereo visual odometry. No verifiable datasets are referenced.",
      "processing_time": 16.418392181396484,
      "citing_paper_id": "272911351",
      "cited_paper_id": 255125395
    },
    {
      "context_text": "2023 [5] Deep-learning–based event-vision applications; benchmarking on image reconstruction and optical flow.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only general applications and benchmarks. No clear, verifiable datasets are identified.",
      "processing_time": 13.856194972991943,
      "citing_paper_id": "272911351",
      "cited_paper_id": 257019827
    },
    {
      "context_text": "However, the primary focus of [5] seems to be on learning methods for image reconstruction and optical flow estimation (probably due to these problems being more investigated than depth estimation in the context of learning).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only learning methods and problems such as image reconstruction and optical flow estimation.",
      "processing_time": 13.579294919967651,
      "citing_paper_id": "272911351",
      "cited_paper_id": 257019827
    },
    {
      "context_text": "A survey on deep learning methods for event-based vision by Zheng et al. [5] dedicates a section to stereo depth estimation methods.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a survey on deep learning methods for event-based vision. No verifiable resources are identified.",
      "processing_time": 14.12607479095459,
      "citing_paper_id": "272911351",
      "cited_paper_id": 257019827
    },
    {
      "context_text": "Also, a survey on SLAM using event cameras by Wang et al. [6] discusses depth estimation and camera tracking using monocular and stereo setups; the main focus is on the monocular setup.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a survey on SLAM using event cameras. No verifiable datasets are referenced.",
      "processing_time": 14.90766954421997,
      "citing_paper_id": "272911351",
      "cited_paper_id": 258213006
    },
    {
      "context_text": "2023 [6] Event-based SLAM.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general topic of event-based SLAM.",
      "processing_time": 12.979231834411621,
      "citing_paper_id": "272911351",
      "cited_paper_id": 258213006
    },
    {
      "context_text": "Finally, Deep Hybrid Parallel Tracking and Mapping ( DH-PTAM ) [21] uses a hybrid approach, combining model-based estimation with deep learning, for solving SLAM with stereo pairs of event and frame-based cameras.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (DH-PTAM) for solving SLAM with stereo pairs of event and frame-based cameras.",
      "processing_time": 16.741968393325806,
      "citing_paper_id": "272911351",
      "cited_paper_id": 259075396
    },
    {
      "context_text": "Even though the dataset is aimed for robot control learning along with SLAM, its size is small compared to other such datasets recorded in constrained setups [159].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.0,
      "reasoning": "The context mentions a dataset but does not provide a specific name. It only refers to a 'small' dataset compared to others, which is too generic.",
      "processing_time": 16.074912548065186,
      "citing_paper_id": "272911351",
      "cited_paper_id": 260293142
    },
    {
      "context_text": "Events (on frames) DDES [51] DTC-SPADE [31] StereoFlow-Net[11] GT Events DTC-SPADE [31] Conc-Net[30] StereoFlow-Net[11] DTC-SPADE Se-CFF Ours Events Image Fig.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only models and methods. There are no clear identifiers for datasets within the text.",
      "processing_time": 14.893993854522705,
      "citing_paper_id": "272911351",
      "cited_paper_id": 262638843
    },
    {
      "context_text": ") [51] was the first event-only stereo approach using deep learning.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method or approach.",
      "processing_time": 12.128328800201416,
      "citing_paper_id": "272911351",
      "cited_paper_id": 262638843
    },
    {
      "context_text": "The Event-Image-Translation-Network ( EIT-Net ) [40] improves upon DDES [51] by reconstructing images from events and using them as a guiding signal for stereo matching.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models and methods. The context focuses on the improvement of a network for event-based stereo depth estimation.",
      "processing_time": 16.06422781944275,
      "citing_paper_id": "272911351",
      "cited_paper_id": 262638843
    },
    {
      "context_text": "Many of the methods do not report results on split 2, citing poor generalization because of the difference in dynamic characteristics in training and testing events on that split, as mentioned in [40, 51].",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.2,
      "reasoning": "The citation does not mention any specific datasets, only a general issue with a particular split in some unspecified dataset. No clear, verifiable resource names are provided.",
      "processing_time": 16.734025239944458,
      "citing_paper_id": "272911351",
      "cited_paper_id": 262638843
    },
    {
      "context_text": "The output is then fed to the spatial embedding module, followed by the matching and regularization modules as in DDES [51].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (DDES) which is not a dataset.",
      "processing_time": 12.716448068618774,
      "citing_paper_id": "272911351",
      "cited_paper_id": 262638843
    },
    {
      "context_text": "In contrast to SHEF [39], which uses simple model-based stereo matching on edge maps from both sensors, HDES aims to learn stereo depth by employing spatio-temporal input representations (same as [51]) and a novel hybrid pyramid attention module.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models and methods. The context focuses on comparing different approaches to stereo depth estimation.",
      "processing_time": 13.565659523010254,
      "citing_paper_id": "272911351",
      "cited_paper_id": 262638843
    },
    {
      "context_text": "4) is based on the protocol proposed in [51].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a protocol. No dataset names are present in the citation span.",
      "processing_time": 12.970871210098267,
      "citing_paper_id": "272911351",
      "cited_paper_id": 262638843
    },
    {
      "context_text": "Comparing the outputs of DDES [51] (2019) to StereoFlow-Net [11] (2024) we observe a notable improvement in depth accuracy and sharpness of object boundaries through the years.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only comparisons between two methods. No dataset names are present in the citation span.",
      "processing_time": 14.468219757080078,
      "citing_paper_id": "272911351",
      "cited_paper_id": 262638843
    },
    {
      "context_text": "Mean depth error values are provided individually for flying1 , flying2 and flying3 sequences, as in [51].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "flying1",
        "flying2",
        "flying3"
      ],
      "dataset_descriptions": {
        "flying1": "Used to evaluate mean depth error in event-based stereo depth estimation, focusing on specific sequences to assess performance.",
        "flying2": "Used to evaluate mean depth error in event-based stereo depth estimation, focusing on specific sequences to assess performance.",
        "flying3": "Used to evaluate mean depth error in event-based stereo depth estimation, focusing on specific sequences to assess performance."
      },
      "confidence_score": 0.8,
      "reasoning": "The citation mentions 'flying1', 'flying2', and 'flying3' sequences, which appear to be specific datasets or sequences used for evaluating depth error in event-based stereo depth estimation.",
      "processing_time": 33.39601421356201,
      "citing_paper_id": "272911351",
      "cited_paper_id": 262638843
    },
    {
      "context_text": "Another method for stereo depth in this category is proposed in Stereo Cross-Modality network ( SCM-Net ) [120], where an event-intensity fusion network is used to combine intensity features and event embeddings learned from the method proposed in DDES [51].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions SCM-Net and DDES, but neither are datasets. The context does not provide any specific dataset names.",
      "processing_time": 14.474821329116821,
      "citing_paper_id": "272911351",
      "cited_paper_id": 262638843
    },
    {
      "context_text": "The ConvLSTM Event Stereo Network ( CES-Net ) [47] achieved the best results in mean average disparity error (MAE) in the DSEC disparity benchmark among the event-only methods at the CVPR Event Vision Workshop 2023.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "DSEC disparity benchmark"
      ],
      "dataset_descriptions": {
        "DSEC disparity benchmark": "Used to evaluate the performance of event-based stereo depth estimation methods, specifically focusing on mean average disparity error (MAE) in the CVPR Event Vision Workshop 2023."
      },
      "confidence_score": 1.0,
      "reasoning": "The citation mentions the DSEC disparity benchmark, which is a specific dataset used for evaluating event-based stereo depth estimation methods.",
      "processing_time": 22.403611421585083,
      "citing_paper_id": "272911351",
      "cited_paper_id": 263339606
    },
    {
      "context_text": "It is similar to ASNet [17], except that the event stack lengths are decided on a per-pixel basis, giving it more flexibility.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (ASNet) which is not a dataset.",
      "processing_time": 12.713833093643188,
      "citing_paper_id": "272911351",
      "cited_paper_id": 265257632
    },
    {
      "context_text": "The Adaptive Stacks Depth Estimation Network ( ASNet ) [17] proposes the use of event-histogram stacks as input to an image-based stereo matching network (Mo-bileStereoNet).",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.3,
      "reasoning": "The context mentions 'event-histogram stacks' as input to a stereo matching network, which suggests the use of event-based data. However, no specific dataset name is provided.",
      "processing_time": 17.72263813018799,
      "citing_paper_id": "272911351",
      "cited_paper_id": 265257632
    },
    {
      "context_text": "Recently, Ghosh et al. [14] improved upon the SPADE framework by using a novel two-stage coarse-to-fine strategy for stereo matching.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method or framework improvement.",
      "processing_time": 12.268535375595093,
      "citing_paper_id": "272911351",
      "cited_paper_id": 265479838
    },
    {
      "context_text": "NSAVP [143] is a stereo event dataset that focuses on autonomous driving.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "NSAVP"
      ],
      "dataset_descriptions": {
        "NSAVP": "Used to evaluate stereo event-based depth estimation methods, focusing on scenarios relevant to autonomous driving. The dataset provides synchronized stereo event data."
      },
      "confidence_score": 1.0,
      "reasoning": "The citation mentions NSAVP as a stereo event dataset focused on autonomous driving, which is directly relevant to the topic of event-based stereo depth estimation.",
      "processing_time": 21.68132519721985,
      "citing_paper_id": "272911351",
      "cited_paper_id": 267212137
    },
    {
      "context_text": "FusionPortablev2 [144] is a multi-sensor dataset for generalized SLAM across diverse environments and platforms.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "FusionPortablev2"
      ],
      "dataset_descriptions": {
        "FusionPortablev2": "Used to develop and evaluate generalized SLAM algorithms, focusing on multi-sensor integration across various environments and platforms."
      },
      "confidence_score": 1.0,
      "reasoning": "The citation mentions a specific dataset, FusionPortablev2, which is used for generalized SLAM across diverse environments and platforms.",
      "processing_time": 20.787250518798828,
      "citing_paper_id": "272911351",
      "cited_paper_id": 269137093
    },
    {
      "context_text": "The latest work [15] in this category is a follow-up from the first author of the ESVO paper.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a follow-up work. No verifiable resources are identified.",
      "processing_time": 12.706729888916016,
      "citing_paper_id": "272911351",
      "cited_paper_id": 269614135
    },
    {
      "context_text": "A motion-decoupled event camera [133] that always produces sharp, high frame-rate, HDR edge maps may be a better fit for such architectures.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a type of camera technology. No dataset names are present in the citation context.",
      "processing_time": 14.870794773101807,
      "citing_paper_id": "272911351",
      "cited_paper_id": 270068050
    },
    {
      "context_text": "CoSEC [145] uses a pair of beam splitters to perfectly align pixels between frame-based and event cameras (1-Mpx EVK4s) for multi-modal fusion.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "CoSEC"
      ],
      "dataset_descriptions": {
        "CoSEC": "Used for aligning pixels between frame-based and event cameras for multi-modal fusion in autonomous driving, focusing on the technical integration of different camera types."
      },
      "confidence_score": 1.0,
      "reasoning": "The citation mentions a specific dataset, CoSEC, which is used for aligning pixels between frame-based and event cameras for multi-modal fusion in autonomous driving.",
      "processing_time": 21.44094181060791,
      "citing_paper_id": "272911351",
      "cited_paper_id": 271892156
    },
    {
      "context_text": "Since the seminal work [2] (2008) they have gained increasing interest due to their appealing properties, which allow them to perform well in challenging scenarios for traditional cameras, such as high-speed motion, high dynamic range (HDR) illumination, and low-power consumption.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only general properties of event-based sensors.",
      "processing_time": 11.751405715942383,
      "citing_paper_id": "272911351",
      "cited_paper_id": null
    },
    {
      "context_text": "Over the years, several event camera designs have been investigated inspired by biological retinas.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only a general statement about event camera designs.",
      "processing_time": 12.94837212562561,
      "citing_paper_id": "272911351",
      "cited_paper_id": null
    },
    {
      "context_text": "Graph created from a compiled list of papers, which is accessible in this spreadsheet.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, models, or methods. It only refers to a compiled list of papers in a spreadsheet.",
      "processing_time": 15.438528776168823,
      "citing_paper_id": "272911351",
      "cited_paper_id": null
    },
    {
      "context_text": "While NN methods like E2VID provide best reconstruction quality, less accurate but faster reconstruction techniques like simple image recon [160] are equality good at detecting high-contrast corners.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and techniques. No verifiable resources are identified.",
      "processing_time": 12.692264795303345,
      "citing_paper_id": "272911351",
      "cited_paper_id": null
    },
    {
      "context_text": "Most of the methods in this category are still model-based.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The provided context does not mention any specific datasets, models, or other resources. It only discusses a general category of methods.",
      "processing_time": 14.438194990158081,
      "citing_paper_id": "272911351",
      "cited_paper_id": null
    },
    {
      "context_text": "Most of the methods in this category are still model-based.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The provided context does not mention any specific datasets, models, or other resources. It only discusses a general category of methods.",
      "processing_time": 14.438194990158081,
      "citing_paper_id": "272911351",
      "cited_paper_id": null
    },
    {
      "context_text": "For deep learning-based approaches [37], [38], [39], the left and right images are used as feature maps in the network to construct the cost volume.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only the use of left and right images as feature maps in deep learning networks for stereo matching.",
      "processing_time": 16.03448247909546,
      "citing_paper_id": "278355101",
      "cited_paper_id": 4252896
    },
    {
      "context_text": "For deep learning-based approaches [37], [38], [39], the left and right images are used as feature maps in the network to construct the cost volume.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only the use of left and right images as feature maps in deep learning networks for stereo matching.",
      "processing_time": 16.03448247909546,
      "citing_paper_id": "278355101",
      "cited_paper_id": 50773155
    },
    {
      "context_text": "Unlike global attention in [41], deformable attention only samples K points around the reference point to calculate the attention result.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (deformable attention) compared to another method (global attention).",
      "processing_time": 15.434024810791016,
      "citing_paper_id": "278355101",
      "cited_paper_id": 13756489
    },
    {
      "context_text": "The event camera simulates the functionality of the transient visual pathway of the biological retina, where each pixel independently and asynchronously responds to relative brightness changes with a time resolution at the microsecond-level [30], [31].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only the functionality of event cameras. No verifiable resources are identified.",
      "processing_time": 14.073641777038574,
      "citing_paper_id": "278355101",
      "cited_paper_id": 22889967
    },
    {
      "context_text": "The event camera simulates the functionality of the transient visual pathway of the biological retina, where each pixel independently and asynchronously responds to relative brightness changes with a time resolution at the microsecond-level [30], [31].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only the functionality of event cameras. No verifiable resources are identified.",
      "processing_time": 14.073641777038574,
      "citing_paper_id": "278355101",
      "cited_paper_id": null
    },
    {
      "context_text": "Finally, a refinement module [29] is utilized to recover the multiscale predicted disparity maps scale.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a refinement module used in a stereo depth estimation method.",
      "processing_time": 13.536203622817993,
      "citing_paper_id": "278355101",
      "cited_paper_id": 102352684
    },
    {
      "context_text": "Finally, the winner-take-all (WTA) [28] and the refinement module [29] are employed to estimate the disparity.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets. It only refers to methods (WTA and refinement module) used for disparity estimation.",
      "processing_time": 15.136569261550903,
      "citing_paper_id": "278355101",
      "cited_paper_id": 102352684
    },
    {
      "context_text": "Finally, the winner-take-all (WTA) [28] and the refinement module [29] are employed to estimate the disparity.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets. It only refers to methods (WTA and refinement module) used for disparity estimation.",
      "processing_time": 15.136569261550903,
      "citing_paper_id": "278355101",
      "cited_paper_id": 195859047
    },
    {
      "context_text": "Event cameras [7], [8], [9], [10], [11], [12] asynchronously capture submicrosecond brightness changes at each pixel, outputting pixel coordinates, timestamps, and polarities of these brightness changes.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only describes the functionality of event cameras. No dataset names are present in the text.",
      "processing_time": 15.134748458862305,
      "citing_paper_id": "278355101",
      "cited_paper_id": 118684904
    },
    {
      "context_text": "Event cameras [7], [8], [9], [10], [11], [12] asynchronously capture submicrosecond brightness changes at each pixel, outputting pixel coordinates, timestamps, and polarities of these brightness changes.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only describes the functionality of event cameras. No dataset names are present in the text.",
      "processing_time": 15.134748458862305,
      "citing_paper_id": "278355101",
      "cited_paper_id": 220314130
    },
    {
      "context_text": "Event cameras [7], [8], [9], [10], [11], [12] asynchronously capture submicrosecond brightness changes at each pixel, outputting pixel coordinates, timestamps, and polarities of these brightness changes.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only describes the functionality of event cameras. No dataset names are present in the text.",
      "processing_time": 15.134748458862305,
      "citing_paper_id": "278355101",
      "cited_paper_id": 246026914
    },
    {
      "context_text": "Event cameras [7], [8], [9], [10], [11], [12] asynchronously capture submicrosecond brightness changes at each pixel, outputting pixel coordinates, timestamps, and polarities of these brightness changes.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only describes the functionality of event cameras. No dataset names are present in the text.",
      "processing_time": 15.134748458862305,
      "citing_paper_id": "278355101",
      "cited_paper_id": 248304816
    },
    {
      "context_text": "2) Total Loss Function: Both the smooth L 1 loss function [43] and the left–right consistency census loss are used to construct the total loss function.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only loss functions. No dataset names are present in the citation span.",
      "processing_time": 13.806305170059204,
      "citing_paper_id": "278355101",
      "cited_paper_id": 206770307
    },
    {
      "context_text": "Next, a cost volume pyramid is constructed using the similarity (correlation) method, and the cost volumes are aggregated through the ISA and CSA [27].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and approaches for stereo matching.",
      "processing_time": 12.687507390975952,
      "citing_paper_id": "278355101",
      "cited_paper_id": 216036364
    },
    {
      "context_text": "The intrascale aggregation (ISA) module and cross-scale aggregation (CSA) module proposed in [27] are utilized to construct the cost aggregation module.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only modules and methods. The context is focused on the methodology and architecture of the network.",
      "processing_time": 15.13023829460144,
      "citing_paper_id": "278355101",
      "cited_paper_id": 216036364
    },
    {
      "context_text": "They improve the disparity estimation performance using knowledge distillation [23], [24], [25], [26] by incorporating future event information to reinforce the disparity estimation results based on only past event information.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only methods and approaches. The cited papers' titles also do not indicate the use of specific datasets.",
      "processing_time": 16.382587671279907,
      "citing_paper_id": "278355101",
      "cited_paper_id": 226293853
    },
    {
      "context_text": "They improve the disparity estimation performance using knowledge distillation [23], [24], [25], [26] by incorporating future event information to reinforce the disparity estimation results based on only past event information.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only methods and approaches. The cited papers' titles also do not indicate the use of specific datasets.",
      "processing_time": 16.382587671279907,
      "citing_paper_id": "278355101",
      "cited_paper_id": 253121300
    },
    {
      "context_text": "Regarding the uncertainty, the method [49] is followed by applying dropout to the feature extraction component at run-time and measuring the mean µ uncert and variance σ uncert as the uncertainty.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for uncertainty quantification.",
      "processing_time": 12.450720071792603,
      "citing_paper_id": "278355101",
      "cited_paper_id": 234788196
    },
    {
      "context_text": "A S A fundamental and crucial measurement task, depth estimation [2], [3], [4], [5], [6] has attracted significant research attention.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general references to depth estimation research.",
      "processing_time": 12.932919025421143,
      "citing_paper_id": "278355101",
      "cited_paper_id": 247109597
    },
    {
      "context_text": "A S A fundamental and crucial measurement task, depth estimation [2], [3], [4], [5], [6] has attracted significant research attention.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general references to depth estimation research.",
      "processing_time": 12.932919025421143,
      "citing_paper_id": "278355101",
      "cited_paper_id": 247143537
    },
    {
      "context_text": "A S A fundamental and crucial measurement task, depth estimation [2], [3], [4], [5], [6] has attracted significant research attention.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general references to depth estimation research.",
      "processing_time": 12.932919025421143,
      "citing_paper_id": "278355101",
      "cited_paper_id": 271736313
    },
    {
      "context_text": "Similar to frame-based stereo disparity learning [19], [20], the methods proposed in the above works generally adhere to the following pipeline: first, stereo features are extracted through a feature extractor.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general pipeline for stereo disparity learning. No verifiable resources are identified.",
      "processing_time": 14.855517387390137,
      "citing_paper_id": "278355101",
      "cited_paper_id": 247213634
    },
    {
      "context_text": "Previous event-based stereo disparity estimation pipelines using deep learning [13], [14], [15], [16], [17], [18] have made significant contributions to this emerging field, achieving outstanding results.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to previous work in event-based stereo disparity estimation.",
      "processing_time": 13.788725852966309,
      "citing_paper_id": "278355101",
      "cited_paper_id": 249980412
    },
    {
      "context_text": "Nam et al. [16] proposed a learning-based approach to generate event representations by learning from multiple event frames within various time windows.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for generating event representations.",
      "processing_time": 12.677239656448364,
      "citing_paper_id": "278355101",
      "cited_paper_id": 249980412
    },
    {
      "context_text": "3) Evaluation Metrics: Prior works are referenced [13], [14], [16], [17], and evaluation metrics such as RMSE, MAE, 1-pixel error (1PE), and 2-pixel error (2PE) are utilized.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only evaluation metrics. No dataset names are present in the text.",
      "processing_time": 13.785800695419312,
      "citing_paper_id": "278355101",
      "cited_paper_id": 249980412
    },
    {
      "context_text": "First, events are converted into mixed-density event stacking (MES) [16].",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method or technique called 'mixed-density event stacking'. No verifiable datasets are referenced.",
      "processing_time": 15.414558410644531,
      "citing_paper_id": "278355101",
      "cited_paper_id": 249980412
    },
    {
      "context_text": "Smaller time windows offer more accurate edges but contain less event information and require higher computational resources [16].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only discusses the trade-offs of using smaller time windows in event-based stereo depth estimation.",
      "processing_time": 13.793021440505981,
      "citing_paper_id": "278355101",
      "cited_paper_id": 249980412
    },
    {
      "context_text": "Learning-based event representation methods [14], [16], [17] can overcome the aforementioned limitations, yielding more compact representations.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to learning-based event representation methods. No verifiable resources are identified.",
      "processing_time": 15.412531852722168,
      "citing_paper_id": "278355101",
      "cited_paper_id": 249980412
    },
    {
      "context_text": "Nam et al. [16] utilized learning-based representation and employed knowledge transfer techniques to learn a model that incorporates both past and future information.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a methodological approach involving learning-based representation and knowledge transfer.",
      "processing_time": 13.127842426300049,
      "citing_paper_id": "278355101",
      "cited_paper_id": 249980412
    },
    {
      "context_text": "Since a single event contains very little information and the event camera does not produce data when it is stationary, the MES [16] is used as the input to the network.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.2,
      "reasoning": "The context mentions 'MES' but does not provide enough information to determine if it is a dataset, method, or other resource. No other specific datasets are mentioned.",
      "processing_time": 17.14562678337097,
      "citing_paper_id": "278355101",
      "cited_paper_id": 249980412
    },
    {
      "context_text": "Deep learning-based event stereo disparity estimation algorithms [6], [13], [14], [15], [16], [17], [18] typically extract features from event streams through a neural network and represent them as feature maps.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general methods and approaches. No dataset names are present in the text.",
      "processing_time": 15.112645626068115,
      "citing_paper_id": "278355101",
      "cited_paper_id": 249980412
    },
    {
      "context_text": "Deep learning-based event stereo disparity estimation algorithms [6], [13], [14], [15], [16], [17], [18] typically extract features from event streams through a neural network and represent them as feature maps.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general methods and approaches. No dataset names are present in the text.",
      "processing_time": 15.112645626068115,
      "citing_paper_id": "278355101",
      "cited_paper_id": 271736313
    },
    {
      "context_text": "1) Quantitative Analysis: The proposed method is compared with SOTA algorithms (DDES [13], E-Stereo [14], Concentration Net [16], and DTC-PDS [17]) on 1PE, 2PE, MAE, and RMSE metrics using the DSEC disparity benchmark dataset.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "DSEC disparity benchmark"
      ],
      "dataset_descriptions": {
        "DSEC disparity benchmark": "Used to evaluate the proposed method against state-of-the-art algorithms, focusing on performance metrics such as 1PE, 2PE, MAE, and RMSE."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions the use of the DSEC disparity benchmark dataset for evaluating the proposed method against state-of-the-art algorithms. The dataset is clearly identified and used for performance evaluation.",
      "processing_time": 23.383169412612915,
      "citing_paper_id": "278355101",
      "cited_paper_id": 249980412
    },
    {
      "context_text": "In the future, frame-event fusion methods will be explored to achieve higher performance [50], as well as deep spike-based stereo depth estimation network architectures suitable for energy-efficient inference [51], [52], [53].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only future research directions and methods. No verifiable resources are identified.",
      "processing_time": 14.415317296981812,
      "citing_paper_id": "278355101",
      "cited_paper_id": 267740607
    },
    {
      "context_text": "The datasets can be used not only for stereo but also for scene ﬂow, SLAM, and other event-based applications.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.0,
      "reasoning": "The context does not mention any specific dataset names, only generic references to datasets used for various event-based applications.",
      "processing_time": 13.78256344795227,
      "citing_paper_id": "22158024",
      "cited_paper_id": 1686141
    },
    {
      "context_text": "The datasets can be used not only for stereo but also for scene ﬂow, SLAM, and other event-based applications.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.0,
      "reasoning": "The context does not mention any specific dataset names, only generic references to datasets used for various event-based applications.",
      "processing_time": 13.78256344795227,
      "citing_paper_id": "22158024",
      "cited_paper_id": 6258804
    },
    {
      "context_text": "Recently, some datasets for event-based simultaneous localization and mapping (SLAM) (Kogler et al., 2013; Serrano-Gotarredona et al., 2013) have become available, but none of those are created for event based stereo matching and the above previous works do not release their test datasets.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.3,
      "reasoning": "The context mentions datasets for event-based SLAM but specifies that none are created for event-based stereo matching. No specific dataset names are provided.",
      "processing_time": 16.354040145874023,
      "citing_paper_id": "22158024",
      "cited_paper_id": 1686141
    },
    {
      "context_text": "Recently, some datasets for event-based simultaneous localization and mapping (SLAM) (Kogler et al., 2013; Serrano-Gotarredona et al., 2013) have become available, but none of those are created for event based stereo matching and the above previous works do not release their test datasets.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.3,
      "reasoning": "The context mentions datasets for event-based SLAM but specifies that none are created for event-based stereo matching. No specific dataset names are provided.",
      "processing_time": 16.354040145874023,
      "citing_paper_id": "22158024",
      "cited_paper_id": 6258804
    },
    {
      "context_text": "Recently, some datasets for eventbased simultaneous localization and mapping (SLAM) (Kogler et al., 2013; Serrano-Gotarredona et al., 2013) have become available, but none of those are created for event based stereo matching and the above previous works do not release their test datasets.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions datasets for event-based SLAM but specifies that none are for event-based stereo matching. No specific dataset names are provided.",
      "processing_time": 14.827196598052979,
      "citing_paper_id": "22158024",
      "cited_paper_id": 1686141
    },
    {
      "context_text": "The datasets used in previous works (Rogister et al., 2012; Camuñasmesa et al., 2014; Firouzi and Conradt, 2016) both assume the cameras are static.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.3,
      "reasoning": "The context mentions datasets used in previous works but does not specify their names. The cited papers' titles suggest the datasets are related to event-based stereo matching, but no specific dataset names are provided.",
      "processing_time": 18.134566068649292,
      "citing_paper_id": "22158024",
      "cited_paper_id": 4833834
    },
    {
      "context_text": "The datasets used in previous works (Rogister et al., 2012; Camuñasmesa et al., 2014; Firouzi and Conradt, 2016) both assume the cameras are static.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.3,
      "reasoning": "The context mentions datasets used in previous works but does not specify their names. The cited papers' titles suggest the datasets are related to event-based stereo matching, but no specific dataset names are provided.",
      "processing_time": 18.134566068649292,
      "citing_paper_id": "22158024",
      "cited_paper_id": 17693733
    },
    {
      "context_text": "Inthis paper we have replicated Rogister’s and Firouzi’s algorithms.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only algorithms. No dataset names are present in the text.",
      "processing_time": 14.049618244171143,
      "citing_paper_id": "22158024",
      "cited_paper_id": 4833834
    },
    {
      "context_text": "Inthis paper we have replicated Rogister’s and Firouzi’s algorithms.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only algorithms. No dataset names are present in the text.",
      "processing_time": 14.049618244171143,
      "citing_paper_id": "22158024",
      "cited_paper_id": 17693733
    },
    {
      "context_text": "Firouzi and Conradt (2016) came up with the dynamic cooperative neural network to make use of the stream of the events.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (dynamic cooperative neural network).",
      "processing_time": 12.42455267906189,
      "citing_paper_id": "22158024",
      "cited_paper_id": 4833834
    },
    {
      "context_text": "Cop-net is used to denote Firouzi’s cooperative network approach, and EMP is used to denote our Event-based Message Passing approach.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods and approaches. No verifiable resources are identified.",
      "processing_time": 14.057372570037842,
      "citing_paper_id": "22158024",
      "cited_paper_id": 4833834
    },
    {
      "context_text": "There are some state-of-art event-based stereo matching algorithms like Rogister’s ( Rogister et al., 2012), Camuñasmesa (Camuñasmesa et al., 2014), and Firouzi’s (Firouzi and Conradt, 2016).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only algorithms and methods. No verifiable resources are identified.",
      "processing_time": 14.044919967651367,
      "citing_paper_id": "22158024",
      "cited_paper_id": 4833834
    },
    {
      "context_text": "There are some state-of-art event-based stereo matching algorithms like Rogister’s ( Rogister et al., 2012), Camuñasmesa (Camuñasmesa et al., 2014), and Firouzi’s (Firouzi and Conradt, 2016).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only algorithms and methods. No verifiable resources are identified.",
      "processing_time": 14.044919967651367,
      "citing_paper_id": "22158024",
      "cited_paper_id": 17693733
    },
    {
      "context_text": "A schematic of the MRF connectivity and messages are shown in Figure 4C .",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (Belief Propagation) and a figure. No verifiable datasets are referenced.",
      "processing_time": 16.33269762992859,
      "citing_paper_id": "22158024",
      "cited_paper_id": 7495827
    },
    {
      "context_text": "Finding labels that minimize the cost corresponds to a maximum a posteriori (MAP) estimation problem in an appropriately deﬁned MRF (Sun et al., 2003 Felzenszwalb and Huttenlocher, 2004).",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only methods and algorithms. The cited papers are about belief propagation in stereo matching and early vision, which are methods, not datasets.",
      "processing_time": 17.87613558769226,
      "citing_paper_id": "22158024",
      "cited_paper_id": 7495827
    },
    {
      "context_text": "Finding labels that minimize the cost corresponds to a maximum a posteriori (MAP) estimation problem in an appropriately deﬁned MRF (Sun et al., 2003 Felzenszwalb and Huttenlocher, 2004).",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only methods and algorithms. The cited papers are about belief propagation in stereo matching and early vision, which are methods, not datasets.",
      "processing_time": 17.87613558769226,
      "citing_paper_id": "22158024",
      "cited_paper_id": 8702465
    },
    {
      "context_text": "BP is a message passing algorithm for performing inference on graphical models, such as Bayesian networks and Markov random ﬁelds (MRF).",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (Belief Propagation) used for stereo matching.",
      "processing_time": 13.102559804916382,
      "citing_paper_id": "22158024",
      "cited_paper_id": 7495827
    },
    {
      "context_text": "Our goal is to ﬁnd proper label for each pixel to minimize the cost, which corresponds to a maximum a posteriori estimation problem in an appropriately deﬁned Markov Random Field(MAP-MRF).",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only a method for stereo matching using belief propagation.",
      "processing_time": 13.30970573425293,
      "citing_paper_id": "22158024",
      "cited_paper_id": 7495827
    },
    {
      "context_text": "However, as shown in Figure 1 , the classical BP does not work for event accumulated frames, so we have to construct a modiﬁed MRF to manage the event-driven input and formulate a dynamic updating mechanism to deal with the temporal correlation of the event stream.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a method (Belief Propagation) and a modified approach for event-driven input. No verifiable datasets are referenced.",
      "processing_time": 17.4035804271698,
      "citing_paper_id": "22158024",
      "cited_paper_id": 7495827
    },
    {
      "context_text": "The max-product BP algorithm can be used to solve the MAP-MRF problem eﬃciently (Felzenszwalb and Huttenlocher, 2004).",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions the max-product BP algorithm but does not reference any specific dataset. The cited papers are about belief propagation methods, which are algorithms, not datasets.",
      "processing_time": 17.12220788002014,
      "citing_paper_id": "22158024",
      "cited_paper_id": 7495827
    },
    {
      "context_text": "The max-product BP algorithm can be used to solve the MAP-MRF problem eﬃciently (Felzenszwalb and Huttenlocher, 2004).",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions the max-product BP algorithm but does not reference any specific dataset. The cited papers are about belief propagation methods, which are algorithms, not datasets.",
      "processing_time": 17.12220788002014,
      "citing_paper_id": "22158024",
      "cited_paper_id": 8702465
    },
    {
      "context_text": "Finally, the disparity output stage estimates the disparity of each event and generates a semi-dense disparity map with the updated MRF.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for estimating disparity using an MRF.",
      "processing_time": 13.48904824256897,
      "citing_paper_id": "22158024",
      "cited_paper_id": 7495827
    },
    {
      "context_text": "Inspired by Cook et al. (2011) who was using message passing algorithm to jointly estimate ego-motion intensity and optical ﬂow, we explore message passing for stereo depth estimation.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (message passing algorithm) and its application. No verifiable resources are identified.",
      "processing_time": 16.342592239379883,
      "citing_paper_id": "22158024",
      "cited_paper_id": 8385399
    },
    {
      "context_text": "For the event-driven message passing framework, we follow the idea from Felzenszwalb and Huttenlocher (2004), which deﬁnes stereo matching as a labeling problem.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for stereo matching. The context is focused on the methodology rather than the data used.",
      "processing_time": 16.3408100605011,
      "citing_paper_id": "22158024",
      "cited_paper_id": 8702465
    },
    {
      "context_text": "One possible method for estimating stereo is to construct frames by accumulating events over a period of time, and then use the BPmethod (Felzenszwalb and Huttenlocher, 2004) to process in a frame-based manner.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for stereo estimation using event accumulation and belief propagation.",
      "processing_time": 14.043063640594482,
      "citing_paper_id": "22158024",
      "cited_paper_id": 8702465
    },
    {
      "context_text": "We the min convolution algorithm from Felzenszwalb and Huttenlocher (2004) to reduce the complexity of message updating to be linear rather than quadratic in the number of labels.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions an algorithm but does not refer to any specific dataset. The citation is about a method to improve computational efficiency.",
      "processing_time": 15.084936141967773,
      "citing_paper_id": "22158024",
      "cited_paper_id": 8702465
    },
    {
      "context_text": "The max-product BP algorithm can be used to solve the MAP-MRF problem efficiently (Felzenszwalb and Huttenlocher, 2004).",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions the max-product BP algorithm but does not refer to any specific dataset. The context is about the algorithm's application to solve the MAP-MRF problem.",
      "processing_time": 17.401515245437622,
      "citing_paper_id": "22158024",
      "cited_paper_id": 8702465
    },
    {
      "context_text": "One possible method for estimating stereo is to construct frames by accumulating events over a period of time, and then use the BP method (Felzenszwalb and Huttenlocher, 2004) to process in a frame-based manner.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for stereo estimation using event accumulation and belief propagation.",
      "processing_time": 14.038323879241943,
      "citing_paper_id": "22158024",
      "cited_paper_id": 8702465
    },
    {
      "context_text": "BP as a global cost optimization method is used by some state-of-art frame-based stereo methods on the Middlebury and KITTI benchmarks.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Middlebury",
        "KITTI"
      ],
      "dataset_descriptions": {
        "Middlebury": "Used to evaluate frame-based stereo methods, focusing on global cost optimization using belief propagation. Investigates accuracy and robustness in depth estimation.",
        "KITTI": "Used to evaluate stereo methods in the context of autonomous vehicles, focusing on object scene flow. Investigates performance in real-world driving scenarios."
      },
      "confidence_score": 1.0,
      "reasoning": "The citation mentions 'Middlebury' and 'KITTI' as benchmarks, which are commonly used datasets in stereo vision and autonomous driving research.",
      "processing_time": 26.721144199371338,
      "citing_paper_id": "22158024",
      "cited_paper_id": 12986049
    },
    {
      "context_text": "Traditional frame-based stereo vision systems continue to steadily mature, in part thanks to publicly available datasets, such as the Middlebury (Scharstein and Szeliski, 2002) and KITTI (Menze and Geiger, 2015) benchmarks.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Middlebury",
        "KITTI"
      ],
      "dataset_descriptions": {
        "Middlebury": "Used to evaluate traditional frame-based stereo vision systems, providing ground truth disparity maps for benchmarking accuracy and performance.",
        "KITTI": "Used to evaluate stereo vision systems in real-world driving scenarios, providing stereo image pairs and ground truth data for depth estimation."
      },
      "confidence_score": 1.0,
      "reasoning": "The citation mentions two well-known datasets, Middlebury and KITTI, which are used for evaluating stereo vision systems. These datasets are relevant to the topic of event-based stereo depth estimation.",
      "processing_time": 27.28316354751587,
      "citing_paper_id": "22158024",
      "cited_paper_id": 12986049
    },
    {
      "context_text": "In the experiments, ST is used to denote Rogister’s method which enforces Space (epipolar) and Time constraints for stereo matching.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (Rogister’s method) used for stereo matching. No verifiable resources are identified.",
      "processing_time": 16.89003825187683,
      "citing_paper_id": "22158024",
      "cited_paper_id": 17693733
    },
    {
      "context_text": "Rogister et al. used one moving pen and two simultaneously moving pens as stimulus and showed the detected disparity (Rogister et al., 2012), but the accuracy of the algorithm is not quantitatively analyzed.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method or experiment setup. The context focuses on the use of moving pens as stimuli, which is not a verifiable dataset.",
      "processing_time": 18.283819437026978,
      "citing_paper_id": "22158024",
      "cited_paper_id": 17693733
    },
    {
      "context_text": "However, matching using temporal and polarity criterion alone is prone to errors because the latency of events varies (jitter) (Rogister et al., 2012).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only discusses a methodological issue in event-based stereo matching.",
      "processing_time": 13.08819031715393,
      "citing_paper_id": "22158024",
      "cited_paper_id": 17693733
    },
    {
      "context_text": "Event-based vision sensors loosely mimic biological retinas, asynchronously generating events in response to relative light intensity changes rather than absolute image intensity (Posch et al., 2011).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only describes the functionality of event-based vision sensors.",
      "processing_time": 12.629522800445557,
      "citing_paper_id": "22158024",
      "cited_paper_id": 21317717
    },
    {
      "context_text": "Many researchers have explored event-based matching criterions for event-based cameras such as ATIS (Posch et al., 2011) and DVS (Lichtsteiner et al., 2008).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions event-based cameras ATIS and DVS but does not refer to them as datasets. They are hardware devices, not datasets.",
      "processing_time": 16.318479537963867,
      "citing_paper_id": "22158024",
      "cited_paper_id": 21317717
    },
    {
      "context_text": "For the event-based stereo setup, we rely on two DAVIS240C (Brandli et al., 2014) sensors.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions hardware sensors but does not refer to any specific dataset. The context is about the setup and not about data collection or analysis.",
      "processing_time": 15.072599649429321,
      "citing_paper_id": "22158024",
      "cited_paper_id": 24007071
    },
    {
      "context_text": "Calibration is performed by using the frame-capture capability of the DAVIS240C to simultaneously record frames from both sensors, which can then be used with OpenCV to calibrate.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions the DAVIS240C sensor but does not refer to it as a dataset. It is used for calibration purposes, which is a methodological step rather than a reusable dataset.",
      "processing_time": 18.100327491760254,
      "citing_paper_id": "22158024",
      "cited_paper_id": 24007071
    },
    {
      "context_text": "Data from the DAVIS240C, ZED, and Vicon are simultaneously recorded using the Robot Operating System (ROS) 3 .",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions specific hardware (DAVIS240C, ZED, Vicon) used for recording data, but does not specify a named dataset. The context is about the recording process, not a reusable dataset.",
      "processing_time": 18.85079336166382,
      "citing_paper_id": "22158024",
      "cited_paper_id": 24007071
    },
    {
      "context_text": "Events are read out from each DAVIS240C sensor independently over two separate USB cables, but their timestamps are synchronized using the standard timestamp synchronization feature of the sensors (which relies on the audio cable seen in the ﬁgure).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context describes the setup and synchronization of DAVIS240C sensors but does not mention any specific dataset. The focus is on the technical details of the sensors rather than a dataset.",
      "processing_time": 17.841548681259155,
      "citing_paper_id": "22158024",
      "cited_paper_id": 24007071
    },
    {
      "context_text": "It consists of the ZED frame-based stereo sensor mounted below two event-based DAVIS240C sensors, all of which are held together with a 3D printed plastic mounting.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only hardware components. There are no verifiable resources that meet the criteria.",
      "processing_time": 14.795475959777832,
      "citing_paper_id": "22158024",
      "cited_paper_id": 24007071
    },
    {
      "context_text": "Additionally, we demonstrate the advantage of using events over standard frames for tackling HDR scenes in Fig.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only a figure demonstrating the advantage of using events over standard frames for HDR scenes.",
      "processing_time": 15.35872507095337,
      "citing_paper_id": "252918208",
      "cited_paper_id": 9642065
    },
    {
      "context_text": "Additionally, we demonstrate the advantage of using events over standard frames for tackling HDR scenes in Fig.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only a figure demonstrating the advantage of using events over standard frames for HDR scenes.",
      "processing_time": 15.35872507095337,
      "citing_paper_id": "252918208",
      "cited_paper_id": 244920800
    },
    {
      "context_text": "Moreover, the outstanding properties of event cameras, such as high dynamic range (HDR), high temporal resolution (≈ μs) and low power consumption, offer potential to tackle scenarios that are challenging for standard cameras (high speed and/or HDR) [1,3, 6, 7, 15,16,18].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only properties of event cameras. No verifiable resources are identified.",
      "processing_time": 14.369253873825073,
      "citing_paper_id": "252918208",
      "cited_paper_id": 9642065
    },
    {
      "context_text": "Moreover, the outstanding properties of event cameras, such as high dynamic range (HDR), high temporal resolution (≈ μs) and low power consumption, offer potential to tackle scenarios that are challenging for standard cameras (high speed and/or HDR) [1,3, 6, 7, 15,16,18].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only properties of event cameras. No verifiable resources are identified.",
      "processing_time": 14.369253873825073,
      "citing_paper_id": "252918208",
      "cited_paper_id": 244920800
    },
    {
      "context_text": "Moreover, the outstanding properties of event cameras, such as high dynamic range (HDR), high temporal resolution (≈ µs) and low power consumption, offer potential to tackle scenarios that are challenging for standard cameras (high speed and/or HDR) [1,3, 6, 7, 15,16,18].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only properties of event cameras. No verifiable resources are identified.",
      "processing_time": 14.792457580566406,
      "citing_paper_id": "252918208",
      "cited_paper_id": 9642065
    },
    {
      "context_text": "Moreover, the outstanding properties of event cameras, such as high dynamic range (HDR), high temporal resolution (≈ µs) and low power consumption, offer potential to tackle scenarios that are challenging for standard cameras (high speed and/or HDR) [1,3, 6, 7, 15,16,18].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only properties of event cameras. No verifiable resources are identified.",
      "processing_time": 14.792457580566406,
      "citing_paper_id": "252918208",
      "cited_paper_id": 244920800
    },
    {
      "context_text": "Similar to the monocular method in [17], known camera poses are used to back-project events into space in the form of rays, referred to as a Disparity Space Image (DSI).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for back-projecting events into space using known camera poses.",
      "processing_time": 13.995943307876587,
      "citing_paper_id": "252918208",
      "cited_paper_id": 16588072
    },
    {
      "context_text": "Correspondencefree depth estimation for monocular event cameras has been shown to generate state-of-the-art results in visual odometry [17].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for correspondence-free depth estimation using monocular event cameras.",
      "processing_time": 15.060237169265747,
      "citing_paper_id": "252918208",
      "cited_paper_id": 16588072
    },
    {
      "context_text": "Event cameras, such as the Dynamic Vision Sensor [4, 11] (DVS), mimic the transient visual pathway in humans.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions 'Dynamic Vision Sensor' (DVS) but does not refer to it as a dataset. It is described as a sensor, which is a device, not a dataset.",
      "processing_time": 18.688976526260376,
      "citing_paper_id": "252918208",
      "cited_paper_id": 215799961
    },
    {
      "context_text": "It is also the first public visual-inertial dataset with 1 Megapixel stereo event cameras [4].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.3,
      "reasoning": "The citation mentions a 'visual-inertial dataset' with specific characteristics, which fits the criteria for a dataset. However, the name is not explicitly provided, making it a generic reference.",
      "processing_time": 18.83858323097229,
      "citing_paper_id": "252918208",
      "cited_paper_id": 215799961
    },
    {
      "context_text": "1 on a variety of datasets, here we present depth estimation results on the egocentric TUM-VIE dataset [10] for the sake of brevity.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "TUM-VIE"
      ],
      "dataset_descriptions": {
        "TUM-VIE": "Used to evaluate depth estimation algorithms in an egocentric setting, focusing on visual-inertial event data for stereo vision."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions the TUM-VIE dataset, which is a specific dataset used for depth estimation in the context of event-based stereo vision.",
      "processing_time": 21.464463233947754,
      "citing_paper_id": "252918208",
      "cited_paper_id": 237142365
    },
    {
      "context_text": "1 on TUM-VIE data [10] (1Mpix cameras).",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "TUM-VIE"
      ],
      "dataset_descriptions": {
        "TUM-VIE": "Used to evaluate stereo depth estimation methods using event-based cameras, focusing on visual-inertial event data with 1Mpix resolution."
      },
      "confidence_score": 1.0,
      "reasoning": "The citation mentions 'TUM-VIE data', which is a specific dataset used for stereo visual-inertial event processing. The title confirms it is a dataset.",
      "processing_time": 22.597411632537842,
      "citing_paper_id": "252918208",
      "cited_paper_id": 237142365
    },
    {
      "context_text": "Based on event cameras, there has been considerable research on computer vision algorithms, such as image reconstruction [9, 10], optical flow estimation [11, 12], depth estimation [13, 14], etc.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only general research areas and algorithms. The cited papers' titles do not provide additional information about datasets.",
      "processing_time": 17.073707103729248,
      "citing_paper_id": "276248927",
      "cited_paper_id": 1082643
    },
    {
      "context_text": "Based on event cameras, there has been considerable research on computer vision algorithms, such as image reconstruction [9, 10], optical flow estimation [11, 12], depth estimation [13, 14], etc.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only general research areas and algorithms. The cited papers' titles do not provide additional information about datasets.",
      "processing_time": 17.073707103729248,
      "citing_paper_id": "276248927",
      "cited_paper_id": 4597042
    },
    {
      "context_text": "Based on event cameras, there has been considerable research on computer vision algorithms, such as image reconstruction [9, 10], optical flow estimation [11, 12], depth estimation [13, 14], etc.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only general research areas and algorithms. The cited papers' titles do not provide additional information about datasets.",
      "processing_time": 17.073707103729248,
      "citing_paper_id": "276248927",
      "cited_paper_id": 10280488
    },
    {
      "context_text": "Based on event cameras, there has been considerable research on computer vision algorithms, such as image reconstruction [9, 10], optical flow estimation [11, 12], depth estimation [13, 14], etc.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only general research areas and algorithms. The cited papers' titles do not provide additional information about datasets.",
      "processing_time": 17.073707103729248,
      "citing_paper_id": "276248927",
      "cited_paper_id": 189998802
    },
    {
      "context_text": "For the latter, the objective of vision tasks is to comprehend the structure of every 3D object, such as stereo matching [19–21], depth estimation [14].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only general tasks such as stereo matching and depth estimation. The cited papers' titles do not provide additional dataset names.",
      "processing_time": 17.831511974334717,
      "citing_paper_id": "276248927",
      "cited_paper_id": 1082643
    },
    {
      "context_text": "For the latter, the objective of vision tasks is to comprehend the structure of every 3D object, such as stereo matching [19–21], depth estimation [14].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only general tasks such as stereo matching and depth estimation. The cited papers' titles do not provide additional dataset names.",
      "processing_time": 17.831511974334717,
      "citing_paper_id": "276248927",
      "cited_paper_id": 262638843
    },
    {
      "context_text": "And many works propose their own insights [31–33] to improve model performance.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only general improvements to model performance.",
      "processing_time": 13.062729597091675,
      "citing_paper_id": "276248927",
      "cited_paper_id": 1151030
    },
    {
      "context_text": "Traditionally, optical flow estimation relies on variational approaches [1, 2], where it is commonly tackled as an energy minimization problem.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only references to variational approaches and energy minimization problems in optical flow estimation.",
      "processing_time": 15.664279222488403,
      "citing_paper_id": "276248927",
      "cited_paper_id": 2121536
    },
    {
      "context_text": "Benosman et al. [29] design an event-based method to estimate optical flow with inspiration from the Lucas-Kanade algorithm [2].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and algorithms. The context focuses on the design of an event-based method for optical flow estimation.",
      "processing_time": 17.079415321350098,
      "citing_paper_id": "276248927",
      "cited_paper_id": 2121536
    },
    {
      "context_text": "Benosman et al. [29] design an event-based method to estimate optical flow with inspiration from the Lucas-Kanade algorithm [2].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and algorithms. The context focuses on the design of an event-based method for optical flow estimation.",
      "processing_time": 17.079415321350098,
      "citing_paper_id": "276248927",
      "cited_paper_id": 17407641
    },
    {
      "context_text": "Based on different visual tasks, there are many classic vision algorithms, such as optical flow estimation [1, 2], stereo matching [3–5], depth estimation[6], etc.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only general visual tasks and algorithms. No dataset names are present in the text.",
      "processing_time": 15.927923679351807,
      "citing_paper_id": "276248927",
      "cited_paper_id": 2121536
    },
    {
      "context_text": "Based on different visual tasks, there are many classic vision algorithms, such as optical flow estimation [1, 2], stereo matching [3–5], depth estimation[6], etc.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only general visual tasks and algorithms. No dataset names are present in the text.",
      "processing_time": 15.927923679351807,
      "citing_paper_id": "276248927",
      "cited_paper_id": 11008141
    },
    {
      "context_text": "For example, EV-FlowNet [15, 34] applies a deep neural network to process two consecutive event streams as static frames, just like FlowNet [24].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets. It only refers to methods (EV-FlowNet and FlowNet) and their application to event streams.",
      "processing_time": 17.362754821777344,
      "citing_paper_id": "276248927",
      "cited_paper_id": 3396150
    },
    {
      "context_text": "For example, EV-FlowNet [15, 34] applies a deep neural network to process two consecutive event streams as static frames, just like FlowNet [24].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets. It only refers to methods (EV-FlowNet and FlowNet) and their application to event streams.",
      "processing_time": 17.362754821777344,
      "citing_paper_id": "276248927",
      "cited_paper_id": 12552176
    },
    {
      "context_text": "For example, EV-FlowNet [15, 34] applies a deep neural network to process two consecutive event streams as static frames, just like FlowNet [24].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets. It only refers to methods (EV-FlowNet and FlowNet) and their application to event streams.",
      "processing_time": 17.362754821777344,
      "citing_paper_id": "276248927",
      "cited_paper_id": 56475917
    },
    {
      "context_text": "For the former, the purpose of vision algorithms is to understand motion in 3D scenes, such as optical flow [15–17], object tracking [18].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only general concepts like optical flow and object tracking. No verifiable resources are identified.",
      "processing_time": 16.288229942321777,
      "citing_paper_id": "276248927",
      "cited_paper_id": 3396150
    },
    {
      "context_text": "For the former, the purpose of vision algorithms is to understand motion in 3D scenes, such as optical flow [15–17], object tracking [18].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only general concepts like optical flow and object tracking. No verifiable resources are identified.",
      "processing_time": 16.288229942321777,
      "citing_paper_id": "276248927",
      "cited_paper_id": 219037720
    },
    {
      "context_text": "Gallego et al. [12, 30] propose a framework with the principle of contrast maximization.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a framework and its applications. No verifiable resources are identified.",
      "processing_time": 15.045961141586304,
      "citing_paper_id": "276248927",
      "cited_paper_id": 4597042
    },
    {
      "context_text": "Gallego et al. [12, 47] have proposed a unifying framework with contrast maximization (CM) strategy, whose main idea is to find the point trajectories on the image plane that are best aligned with the event data.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (contrast maximization) and its applications. No verifiable datasets are referenced.",
      "processing_time": 16.60540747642517,
      "citing_paper_id": "276248927",
      "cited_paper_id": 4597042
    },
    {
      "context_text": "Gallego et al. [12, 47] have proposed a unifying framework with contrast maximization (CM) strategy, whose main idea is to find the point trajectories on the image plane that are best aligned with the event data.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (contrast maximization) and its applications. No verifiable datasets are referenced.",
      "processing_time": 16.60540747642517,
      "citing_paper_id": "276248927",
      "cited_paper_id": 269525186
    },
    {
      "context_text": "Multi-scale feature pyramid is widely used in the field of computer vision, such as optical flow estimation [25] and object detection [52].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and applications. The context is about the use of multi-scale feature pyramids in various computer vision tasks.",
      "processing_time": 17.81929063796997,
      "citing_paper_id": "276248927",
      "cited_paper_id": 10716717
    },
    {
      "context_text": "Since the introduction of FlowNet [24], which pioneers the application of deep-learning methods for optical flow estimation, neural networks [25–28] trained on large-scale datasets have emerged as a new paradigm.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions 'large-scale datasets' but does not specify any particular dataset name. The reference to FlowNet is about a method, not a dataset.",
      "processing_time": 17.603739976882935,
      "citing_paper_id": "276248927",
      "cited_paper_id": 12552176
    },
    {
      "context_text": "SGM[3], PatchMatch[4], and ADCensus[5] are some classic algorithms among them, whose core idea is matching and optimization.",
      "catation_intent": "none",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only algorithms. There are no verifiable resources that meet the criteria.",
      "processing_time": 15.332279682159424,
      "citing_paper_id": "276248927",
      "cited_paper_id": 26169625
    },
    {
      "context_text": "HD3 [45] utilizes the match density estimation method to design task-specific decoders for optical flow and stereo matching.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for task-specific decoders. No dataset names are present in the citation span.",
      "processing_time": 16.599782466888428,
      "citing_paper_id": "276248927",
      "cited_paper_id": 56366093
    },
    {
      "context_text": "EV-FlowNet [34], an early deep learning network for event cameras, has been exploring the unification of flow, depth, and egomotion.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (EV-FlowNet) and its capabilities. The context focuses on the method's ability to unify flow, depth, and egomotion.",
      "processing_time": 19.20059561729431,
      "citing_paper_id": "276248927",
      "cited_paper_id": 56475917
    },
    {
      "context_text": "In order to feed event data into a neural network, we divide event sequence E ( t s , t e ) into B temporal bins as the channel dimension of an event voxel[34].",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for processing event data. No verifiable dataset names are present in the citation span.",
      "processing_time": 15.639089345932007,
      "citing_paper_id": "276248927",
      "cited_paper_id": 56475917
    },
    {
      "context_text": "If multi-scale iterations are adopted, the feature extraction network can also output multi-scale features such as 1/4 and 1/16, using a weight-sharing convolution with different strides (similar to TridentNet [50]).",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (TridentNet) for feature extraction in object detection.",
      "processing_time": 15.32625436782837,
      "citing_paper_id": "276248927",
      "cited_paper_id": 57573786
    },
    {
      "context_text": "Event camera is a novel neuromorphic visual sensor with the advantages of low latency, high temporal resolution, and high dynamic range [7, 8].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only the characteristics of event cameras. No dataset names are present in the citation span.",
      "processing_time": 16.276199340820312,
      "citing_paper_id": "276248927",
      "cited_paper_id": 118684904
    },
    {
      "context_text": "As shown in Figure 4(b), the head is a GRU iterative structure similar to RAFT [26], which generates optimized residuals based on the current optical flow query and the corresponding cost volume.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (RAFT) for optical flow. No datasets are referenced for training or evaluation.",
      "processing_time": 16.595165014266968,
      "citing_paper_id": "276248927",
      "cited_paper_id": 214667893
    },
    {
      "context_text": "Then, with the inspiration of RAFT [26], Gehrig et al. [16] introduce correlation volume into the event-based optical flow model.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only models and methods. The citation is focused on the introduction of a technique into an existing model.",
      "processing_time": 17.059185028076172,
      "citing_paper_id": "276248927",
      "cited_paper_id": 214667893
    },
    {
      "context_text": "Then, with the inspiration of RAFT [26], Gehrig et al. [16] introduce correlation volume into the event-based optical flow model.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only models and methods. The citation is focused on the introduction of a technique into an existing model.",
      "processing_time": 17.059185028076172,
      "citing_paper_id": "276248927",
      "cited_paper_id": 239049376
    },
    {
      "context_text": "But now deep learning-based methods [39–42] have gradually moved away from this framework.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only references to deep learning-based methods. There is no indication of dataset usage.",
      "processing_time": 15.926513433456421,
      "citing_paper_id": "276248927",
      "cited_paper_id": 216036364
    },
    {
      "context_text": "We assess the performance of several state-of-the-art methods for event-based optical flow estimation, including E-RAFT [16], TMA [17] and IDNet [54].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets. It only refers to methods for event-based optical flow estimation.",
      "processing_time": 15.01449704170227,
      "citing_paper_id": "276248927",
      "cited_paper_id": 239049376
    },
    {
      "context_text": "We assess the performance of several state-of-the-art methods for event-based optical flow estimation, including E-RAFT [16], TMA [17] and IDNet [54].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets. It only refers to methods for event-based optical flow estimation.",
      "processing_time": 15.01449704170227,
      "citing_paper_id": "276248927",
      "cited_paper_id": 257631432
    },
    {
      "context_text": "For optical flow, we use the l 1 loss [16].",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific dataset, only a method (l1 loss) used for optical flow. The cited paper title 'E-RAFT: Dense Optical Flow from Event Cameras' does not introduce a dataset.",
      "processing_time": 19.400789976119995,
      "citing_paper_id": "276248927",
      "cited_paper_id": 239049376
    },
    {
      "context_text": "Compared with ERAFT [16] and TMA [17], our model can capture and predict more detailed optical flow edges.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models (ERAFT and TMA). The context focuses on comparing the performance of different models in capturing and predicting optical flow edges.",
      "processing_time": 18.503466844558716,
      "citing_paper_id": "276248927",
      "cited_paper_id": 239049376
    },
    {
      "context_text": "For ERAFT[16] and IDNet[54] with the warm start strategy, our model also has advantages on 1PE and is competitive on EPE.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models and performance metrics. The context is focused on comparing model performance.",
      "processing_time": 15.637624502182007,
      "citing_paper_id": "276248927",
      "cited_paper_id": 239049376
    },
    {
      "context_text": "For ERAFT[16] and IDNet[54] with the warm start strategy, our model also has advantages on 1PE and is competitive on EPE.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models and performance metrics. The context is focused on comparing model performance.",
      "processing_time": 15.637624502182007,
      "citing_paper_id": "276248927",
      "cited_paper_id": 257631432
    },
    {
      "context_text": "…matching is that the corresponding pixels must exist in both reference and target inputs, so matching may fail for occluded and out-of-boundary For each two columns, we display reference images and event frames, and compare our method with the state-of-the-art baseline E-RAFT [16] and TMA [17].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and models. The context focuses on comparing the performance of different algorithms using reference images and event frames.",
      "processing_time": 17.58965301513672,
      "citing_paper_id": "276248927",
      "cited_paper_id": 239049376
    },
    {
      "context_text": "We also select several state-of-the-art methods for comparison, including DDES [19], E-stereo [55] and Concentration Net [21].",
      "catation_intent": "reusable resource",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context mentions methods (DDES, E-stereo, Concentration Net) but does not reference any specific datasets. No dataset names are present in the context.",
      "processing_time": 18.34445095062256,
      "citing_paper_id": "276248927",
      "cited_paper_id": 244306440
    },
    {
      "context_text": "We also select several state-of-the-art methods for comparison, including DDES [19], E-stereo [55] and Concentration Net [21].",
      "catation_intent": "reusable resource",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context mentions methods (DDES, E-stereo, Concentration Net) but does not reference any specific datasets. No dataset names are present in the context.",
      "processing_time": 18.34445095062256,
      "citing_paper_id": "276248927",
      "cited_paper_id": 262638843
    },
    {
      "context_text": "As shown in Figure 4(a), we choose Transformer as the main architecture for feature enhancement, due to its adeptness in capturing the intricate interplay between two sets using the attention mechanism [22].",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (Transformer) used for feature enhancement.",
      "processing_time": 13.963340997695923,
      "citing_paper_id": "276248927",
      "cited_paper_id": 244709323
    },
    {
      "context_text": "For similarity matching methods [23, 22], the key to optical flow estimation and stereo matching all lies in finding dense correspondence between reference and target inputs.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods for optical flow estimation and stereo matching.",
      "processing_time": 13.960495233535767,
      "citing_paper_id": "276248927",
      "cited_paper_id": 244709323
    },
    {
      "context_text": "Therefore, we have added an additional self-attention layer that can propagate effective matching flows to occluded or out-of-boundary regions [22, 23].",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods or models. The context focuses on adding a self-attention layer for improving optical flow estimation.",
      "processing_time": 15.906147241592407,
      "citing_paper_id": "276248927",
      "cited_paper_id": 244709323
    },
    {
      "context_text": "Inspired by global matching strategy [22, 23], we extract motion and the 3D position of the world from a pair of event streams via feature similarity matching.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for extracting motion and 3D position from event streams.",
      "processing_time": 15.626663208007812,
      "citing_paper_id": "276248927",
      "cited_paper_id": 244709323
    },
    {
      "context_text": "Then, Zhang et al. [20] propose an abstracted model, discrete time convolution (DTC) to encode high dimensional spatial-temporal data.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (DTC) for encoding high-dimensional spatial-temporal data.",
      "processing_time": 14.306782722473145,
      "citing_paper_id": "276248927",
      "cited_paper_id": 250602271
    },
    {
      "context_text": "Recently, there have been several researches on network structures [35, 17] and large-scale datasets [36, 37] to further improve model performance.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to research on network structures and large-scale datasets. No specific dataset names are provided.",
      "processing_time": 16.827686309814453,
      "citing_paper_id": "276248927",
      "cited_paper_id": 257505349
    },
    {
      "context_text": "DDES [19] is the first learning-based stereo method for event cameras, which proposes a learnable representation for event sequences.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions DDES as a method, not a dataset. No specific dataset is named or described in the context.",
      "processing_time": 15.6240234375,
      "citing_paper_id": "276248927",
      "cited_paper_id": 262638843
    },
    {
      "context_text": "Probabilistic filters [27], [28] have been shown to be more suitable for this specific task (the estimated pose is updated every time a new event is triggered), while still preserving the asynchronous nature of the incoming stream of events.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and their suitability for the task.",
      "processing_time": 13.021593809127808,
      "citing_paper_id": "267659950",
      "cited_paper_id": 6178869
    },
    {
      "context_text": "In classical computer vision, the space-sweep approach [31] allows to solve the multi-view stereo (MVS) problem, and a 3D reconstruction can be obtained without the need for matching or data association across cameras.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for solving the multi-view stereo problem.",
      "processing_time": 14.319856405258179,
      "citing_paper_id": "267659950",
      "cited_paper_id": 11008141
    },
    {
      "context_text": "As far as the camera tracking problem is concerned, a variant of the Kalman filter has been used in [14] to predict the 6-DoF camera motion.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (Kalman filter) used for camera motion prediction.",
      "processing_time": 13.961410760879517,
      "citing_paper_id": "267659950",
      "cited_paper_id": 26324573
    },
    {
      "context_text": "In contrast, direct methods process single events, and satisfactory results, in terms of accuracy, are reported in [24]: however, the existing event-feature extractors and trackers [25], [26] cannot be easily adapted to a visual odometry problem, since they suffer from poor accuracy and stability.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and challenges. The context focuses on the limitations of existing event-feature extractors and trackers.",
      "processing_time": 17.320213794708252,
      "citing_paper_id": "267659950",
      "cited_paper_id": 49864158
    },
    {
      "context_text": "Other methods rely on data association [11], [12] (matching to calculate the disparity), or are correspondence-free, but the knowledge of camera’s trajectory is necessary to estimate the depth by performing a re-projection into a reference view [13].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and approaches. No verifiable resources are identified.",
      "processing_time": 15.000877618789673,
      "citing_paper_id": "267659950",
      "cited_paper_id": 65040501
    },
    {
      "context_text": "Belief propagation [11] and event-driven semi-global matching (ESGM) [12] are two methods that have been used to minimize such a cost function.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods (belief propagation and ESGM).",
      "processing_time": 14.304572582244873,
      "citing_paper_id": "267659950",
      "cited_paper_id": 65040501
    },
    {
      "context_text": "This optimization problem is solved using the Lucas-Kanade method [29].",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific dataset, only a method (Lucas-Kanade).",
      "processing_time": 14.313417911529541,
      "citing_paper_id": "267659950",
      "cited_paper_id": 186689463
    },
    {
      "context_text": "The edges in the depth maps generated by our method are sharper, which makes the detection (e.g., by an off-the-shelf deep learning algorithm such as YOLO [32]) of different objects in the scene (traffic light, tree, etc.), easier.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific dataset. It refers to a method (YOLO) for object detection, which is not a dataset.",
      "processing_time": 16.81421685218811,
      "citing_paper_id": "267659950",
      "cited_paper_id": 206594738
    },
    {
      "context_text": "Modern cars are equipped with a suite of advanced sensors, such as multiple color cameras (8 cameras in Tesla Model Y), radars [2], lidars [3], RGB-D cameras [4], and, more recently, event cameras [5].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific, verifiable datasets. It only lists types of sensors used in modern cars.",
      "processing_time": 15.602349281311035,
      "citing_paper_id": "267659950",
      "cited_paper_id": 212837417
    },
    {
      "context_text": "Event cameras are often combined with other incumbent sensors for more accurate and robust pose estimation: for example, an inertial measurement unit (IMU) for visual-inertial odometry [21], or a frame-based camera [22].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only sensors and their combinations for pose estimation.",
      "processing_time": 13.951996326446533,
      "citing_paper_id": "267659950",
      "cited_paper_id": 248227281
    },
    {
      "context_text": "In the near future, we are going to build our own multi-modal dataset to test the proposed pipeline in different traffic and weather conditions, and we would like to fuse the event data with measurements coming from other on-board sensors (e.g., lidar [8], [33], RGB camera, IMU).",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions building a future multi-modal dataset but does not specify any existing datasets. The citation to lidar is for a method, not a dataset.",
      "processing_time": 17.56338596343994,
      "citing_paper_id": "267659950",
      "cited_paper_id": 251765050
    },
    {
      "context_text": "The difference can be defined in different forms, such as absolute gray-scale value, mutual information,(22) and census transform.(23) The penalties depend on the difference to the neighborhood disparities.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods or concepts like 'absolute gray-scale value', 'mutual information', and 'census transform'.",
      "processing_time": 18.02323579788208,
      "citing_paper_id": "65040501",
      "cited_paper_id": 18161107
    },
    {
      "context_text": "The bioinspired vision sensor or called event-based camera mimics retinas to asynchronously generate the response to relative light intensity variations rather than the actual image intensity.(3) Event-based cameras are datadriven sensors that have advantages of low redundancy, high temporal resolution (in the order of microseconds), low latency, and high dynamic range (130 dB compared with 60 dB of standard cameras).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific, verifiable datasets. It describes the functionality and benefits of event-based cameras but does not reference any dataset.",
      "processing_time": 17.301541090011597,
      "citing_paper_id": "65040501",
      "cited_paper_id": 21317717
    },
    {
      "context_text": "In this work, DAVIS is used, which is an extension of the dynamic vision sensor (DVS)(7) with higher resolution (240 180) and an additional frame-based intensity readout (not used in this work).(8) Each event is presented with a quadruplet eðt; x; y; pÞ; t is the time stamp, ðx; yÞ is the image coordinates, and p means polarity (ON/OFF) which indicates the luminance increase (ON) or decrease (OFF).",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "DAVIS"
      ],
      "dataset_descriptions": {
        "DAVIS": "Used to capture event-based data for stereo depth estimation, focusing on high-resolution events with time stamps, image coordinates, and polarity information."
      },
      "confidence_score": 1.0,
      "reasoning": "DAVIS is mentioned as a specific dataset used in the research, with details about its structure and usage in the context of event-based stereo depth estimation.",
      "processing_time": 21.99793815612793,
      "citing_paper_id": "65040501",
      "cited_paper_id": 24007071
    },
    {
      "context_text": "Recently, Eibensteiner et al.15 implemented an event-based stereo matching algorithm in hardware on a field-programmable gate array.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only an implementation of an algorithm. No verifiable resources are identified.",
      "processing_time": 14.293210983276367,
      "citing_paper_id": "65040501",
      "cited_paper_id": 37664826
    },
    {
      "context_text": "Recently, Eibensteiner et al.(15) implemented an event-based stereo matching algorithm in hardware on a field-programmable gate array.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only an implementation of an algorithm. No verifiable resources are identified.",
      "processing_time": 14.29158902168274,
      "citing_paper_id": "65040501",
      "cited_paper_id": 37664826
    },
    {
      "context_text": "Novel local event context descriptors are proposed to robustly match events in a temporal-spatial window [26], [27].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only methods or approaches for event-driven stereo matching and depth map estimation.",
      "processing_time": 15.884931564331055,
      "citing_paper_id": "245300947",
      "cited_paper_id": 1408596
    },
    {
      "context_text": "Novel local event context descriptors are proposed to robustly match events in a temporal-spatial window [26], [27].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only methods or approaches for event-driven stereo matching and depth map estimation.",
      "processing_time": 15.884931564331055,
      "citing_paper_id": "245300947",
      "cited_paper_id": 157060825
    },
    {
      "context_text": "COPNET [48], SEMI-DENSE 3D [7], SGM∗ [19][7], FCV F∗ [49][7]",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods and models. No dataset names are present in the context.",
      "processing_time": 15.87656283378601,
      "citing_paper_id": "245300947",
      "cited_paper_id": 1680724
    },
    {
      "context_text": "Following [8], we furthermore compare against SGM∗ and FCVF∗, two traditional methods [19][49] adjusted",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods. There are no clear identifiers for datasets in the provided context.",
      "processing_time": 16.234759092330933,
      "citing_paper_id": "245300947",
      "cited_paper_id": 1680724
    },
    {
      "context_text": "As can be observed, our proposed method HDES outperforms TSES [30], CopNet [48], Semi-Dense 3D [7], SGM∗ [19][7] and FCV F∗ [49][7].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only comparisons between methods. No dataset names are present in the text.",
      "processing_time": 15.880453824996948,
      "citing_paper_id": "245300947",
      "cited_paper_id": 1680724
    },
    {
      "context_text": "Our architecture is a fully convolutional neural network which is based on the Unet architecture [41].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions a method (Unet architecture) rather than a dataset. No specific dataset is referenced in the citation context.",
      "processing_time": 16.225515127182007,
      "citing_paper_id": "245300947",
      "cited_paper_id": 3719281
    },
    {
      "context_text": "As can be observed, our proposed method HDES outperforms TSES [30], CopNet [48], Semi-Dense 3D [7], SGM ∗ [19][7] and FCVF ∗ [49][7].",
      "catation_intent": "findings",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only comparisons between methods. No dataset names are present in the text.",
      "processing_time": 15.871767044067383,
      "citing_paper_id": "245300947",
      "cited_paper_id": 7151414
    },
    {
      "context_text": "R ESULTS FOR HDES ( OUR PROPOSED METHOD ) TSES [30], C OP N ET [48], S EMI -",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and models. The context focuses on comparing the proposed method (HDES) with other methods (TSES, COPNET, SEMI-).",
      "processing_time": 19.353853702545166,
      "citing_paper_id": "245300947",
      "cited_paper_id": 7151414
    },
    {
      "context_text": "Our ﬁnal experiments compare our hybrid method HDES against the purely event-based methods DDES [8]—a state-of-the-art learning-based method—and Semi-Dense 3D [7], TSES [30], and CopNet [48], which are more traditional methods.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and models. The context focuses on comparing different methods for event-based stereo depth estimation.",
      "processing_time": 17.292041301727295,
      "citing_paper_id": "245300947",
      "cited_paper_id": 7151414
    },
    {
      "context_text": "Rogister et al. [24] ﬁnd matches under the assumption that correlated events are likely to appear within a small time interval and on the same epipolar line.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for finding matches in event-based stereo matching.",
      "processing_time": 13.931430339813232,
      "citing_paper_id": "245300947",
      "cited_paper_id": 17693733
    },
    {
      "context_text": "Original work [21], [22] accumulates events to generate event-images such that a frame-based stereo matching al-gorithm can be applied.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for generating event-images from events. No clear, verifiable dataset names are present.",
      "processing_time": 17.542325019836426,
      "citing_paper_id": "245300947",
      "cited_paper_id": 27059477
    },
    {
      "context_text": "Such methods have been developed to solve many traditional vision problems such as optical flow estimation [31], [32], intensity frame reconstruction [33], [5], action recognition [34], and object tracking [35].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only various computer vision problems and methods. No dataset names are present in the text.",
      "processing_time": 16.991511583328247,
      "citing_paper_id": "245300947",
      "cited_paper_id": 59222403
    },
    {
      "context_text": "Such methods have been developed to solve many traditional vision problems such as optical flow estimation [31], [32], intensity frame reconstruction [33], [5], action recognition [34], and object tracking [35].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only various computer vision problems and methods. No dataset names are present in the text.",
      "processing_time": 16.991511583328247,
      "citing_paper_id": "245300947",
      "cited_paper_id": 226292034
    },
    {
      "context_text": "Event cameras hold several important beneﬁts over standard cameras such as low latency ( ∼ 1 µ s), absence of motion blur, high dynamic range (140 dB vs 60 dB for traditional cameras [1]), and low power consumption.",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only technical advantages of event cameras over traditional cameras.",
      "processing_time": 13.37058973312378,
      "citing_paper_id": "245300947",
      "cited_paper_id": 119309624
    },
    {
      "context_text": "Piatkowska et al. [28] and Firouzi et al. [29] model event-driven stereo matching by a cooperative network for reliable 3D depth perception.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and approaches for event-driven stereo matching.",
      "processing_time": 14.69824481010437,
      "citing_paper_id": "245300947",
      "cited_paper_id": 121601380
    },
    {
      "context_text": "Sharp intensity images are changed into blurry intensity images following [46].",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for transforming sharp intensity images into blurry ones.",
      "processing_time": 13.643116474151611,
      "citing_paper_id": "245300947",
      "cited_paper_id": 131774014
    },
    {
      "context_text": "Such methods have been developed to solve many traditional vision problems such as optical ﬂow estimation [31], [32], intensity frame reconstruction [33], [5], action recognition [34], and object tracking [35].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various computer vision problems and methods. No dataset names are present in the context.",
      "processing_time": 16.996045351028442,
      "citing_paper_id": "245300947",
      "cited_paper_id": 145916256
    },
    {
      "context_text": "Such methods have been developed to solve many traditional vision problems such as optical ﬂow estimation [31], [32], intensity frame reconstruction [33], [5], action recognition [34], and object tracking [35].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various computer vision problems and methods. No dataset names are present in the context.",
      "processing_time": 16.996045351028442,
      "citing_paper_id": "245300947",
      "cited_paper_id": 189998802
    },
    {
      "context_text": "Such methods have been developed to solve many traditional vision problems such as optical ﬂow estimation [31], [32], intensity frame reconstruction [33], [5], action recognition [34], and object tracking [35].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various computer vision problems and methods. No dataset names are present in the context.",
      "processing_time": 16.996045351028442,
      "citing_paper_id": "245300947",
      "cited_paper_id": 226291858
    },
    {
      "context_text": "…on event camera based computer vision, the reader is kindly referred to Gallego et al.’s survey [2] offering a good summary of several important contributions on problems such as object tracking [3], pattern recognition [4], intensity image reconstruction [5] and stereo depth estimation [6], [7].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only references to various applications of event cameras, including stereo depth estimation.",
      "processing_time": 15.864531993865967,
      "citing_paper_id": "245300947",
      "cited_paper_id": 189998802
    },
    {
      "context_text": "DeepPruner [16] improves on the complexity and runtime induced by building the cost volume and the 3D convolutions.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (DeepPruner).",
      "processing_time": 14.277794599533081,
      "citing_paper_id": "245300947",
      "cited_paper_id": 202565789
    },
    {
      "context_text": "Events and frames are utilized as a bi-modal input to recover sharp, high quality images in [36] and [33].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions 'events and frames' as inputs but does not specify any dataset names. The cited papers do not provide additional context to identify specific datasets.",
      "processing_time": 16.992351055145264,
      "citing_paper_id": "245300947",
      "cited_paper_id": 220545977
    },
    {
      "context_text": "Events and frames are utilized as a bi-modal input to recover sharp, high quality images in [36] and [33].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions 'events and frames' as inputs but does not specify any dataset names. The cited papers do not provide additional context to identify specific datasets.",
      "processing_time": 16.992351055145264,
      "citing_paper_id": "245300947",
      "cited_paper_id": 226291858
    },
    {
      "context_text": "’s survey [2] offering a good summary of several important contributions on problems such as object tracking [3], pattern recognition [4], intensity image reconstruction [5] and stereo depth estimation [6], [7].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general research areas and problems. There are no clear identifiers for datasets or other verifiable resources.",
      "processing_time": 16.777527570724487,
      "citing_paper_id": "245300947",
      "cited_paper_id": 221846159
    },
    {
      "context_text": "By providing DVS input data combined with precise, yet sparse, 3D ground-truth information, the DVS 3D Human Pose Dataset (DHP19) [16] offers suitable samples for small-scale neuromorphic architectures of coarse stereo vision.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "DVS 3D Human Pose Dataset (DHP19)"
      ],
      "dataset_descriptions": {
        "DVS 3D Human Pose Dataset (DHP19)": "Used to provide DVS input data and 3D ground-truth information for training small-scale neuromorphic architectures in coarse stereo vision."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions the DVS 3D Human Pose Dataset (DHP19) as a specific dataset used for providing DVS input data and 3D ground-truth information for neuromorphic architectures of coarse stereo vision.",
      "processing_time": 29.878234386444092,
      "citing_paper_id": "233033658",
      "cited_paper_id": 2497402
    },
    {
      "context_text": "By providing DVS input data combined with precise, yet sparse, 3D ground-truth information, the DVS 3D Human Pose Dataset (DHP19) [16] offers suitable samples for small-scale neuromorphic architectures of coarse stereo vision.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "DVS 3D Human Pose Dataset (DHP19)"
      ],
      "dataset_descriptions": {
        "DVS 3D Human Pose Dataset (DHP19)": "Used to provide DVS input data and 3D ground-truth information for training small-scale neuromorphic architectures in coarse stereo vision."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions the DVS 3D Human Pose Dataset (DHP19) as a specific dataset used for providing DVS input data and 3D ground-truth information for neuromorphic architectures of coarse stereo vision.",
      "processing_time": 29.878234386444092,
      "citing_paper_id": "233033658",
      "cited_paper_id": 3416874
    },
    {
      "context_text": "By contrast, biologically inspired neuromorphic event cameras, such as the Dynamic Vision Sensor (DVS) [1], transmit asynchronous streams of events generated by individual pixels in response to perceived brightness changes [2]–[4].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only hardware (event cameras) and their functionality. No verifiable resources are identified.",
      "processing_time": 16.7687509059906,
      "citing_paper_id": "233033658",
      "cited_paper_id": 2497402
    },
    {
      "context_text": "By contrast, biologically inspired neuromorphic event cameras, such as the Dynamic Vision Sensor (DVS) [1], transmit asynchronous streams of events generated by individual pixels in response to perceived brightness changes [2]–[4].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only hardware (event cameras) and their functionality. No verifiable resources are identified.",
      "processing_time": 16.7687509059906,
      "citing_paper_id": "233033658",
      "cited_paper_id": 118684904
    },
    {
      "context_text": "Despite the lack of standard benchmarks for this problem domain, two datasets for event-based stereo have recently been proposed: The Multi Vehicle Stereo Event Camera (MVSEC) Dataset [15], consisting of indoor and outdoor sequences recorded in a variety of illuminations and speeds, and the DVS stereo dataset [10], with two real-world sets of sequences (a fast rotating fan and a rotating toy butterfly).",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Multi Vehicle Stereo Event Camera (MVSEC) Dataset",
        "DVS stereo dataset"
      ],
      "dataset_descriptions": {
        "Multi Vehicle Stereo Event Camera (MVSEC) Dataset": "Used to evaluate event-based stereo algorithms, featuring indoor and outdoor sequences with varying illumination and speeds.",
        "DVS stereo dataset": "Used to test event-based stereo methods, including real-world sequences of a fast rotating fan and a rotating toy butterfly."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions two specific datasets for event-based stereo depth estimation, both of which are relevant to the research topic.",
      "processing_time": 29.1511390209198,
      "citing_paper_id": "233033658",
      "cited_paper_id": 2497402
    },
    {
      "context_text": "Despite the lack of standard benchmarks for this problem domain, two datasets for event-based stereo have recently been proposed: The Multi Vehicle Stereo Event Camera (MVSEC) Dataset [15], consisting of indoor and outdoor sequences recorded in a variety of illuminations and speeds, and the DVS stereo dataset [10], with two real-world sets of sequences (a fast rotating fan and a rotating toy butterfly).",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Multi Vehicle Stereo Event Camera (MVSEC) Dataset",
        "DVS stereo dataset"
      ],
      "dataset_descriptions": {
        "Multi Vehicle Stereo Event Camera (MVSEC) Dataset": "Used to evaluate event-based stereo algorithms, featuring indoor and outdoor sequences with varying illumination and speeds.",
        "DVS stereo dataset": "Used to test event-based stereo methods, including real-world sequences of a fast rotating fan and a rotating toy butterfly."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions two specific datasets for event-based stereo depth estimation, both of which are relevant to the research topic.",
      "processing_time": 29.1511390209198,
      "citing_paper_id": "233033658",
      "cited_paper_id": 3416874
    },
    {
      "context_text": "Unlike datasets such as [10] and [15], which can be used to compute the ground truth on a per-event basis, the Vicon marker-based motion capture system provides ground-truth depth information directly with sparse data linked to point labels attached to specific body parts.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions 'datasets such as [10] and [15]', but does not provide specific names. The Vicon marker-based motion capture system is described, but it is not a dataset.",
      "processing_time": 19.001883506774902,
      "citing_paper_id": "233033658",
      "cited_paper_id": 3416874
    },
    {
      "context_text": "For each column, the top row shows the events of both cameras, depicted as time surfaces [22] with rectified polarities, together with the projected marker locations, and their corresponding disparity over time.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific, verifiable datasets. It describes visualizations and data representations but does not reference a named dataset.",
      "processing_time": 16.986019372940063,
      "citing_paper_id": "233033658",
      "cited_paper_id": 13373696
    },
    {
      "context_text": "By contrast, the stimulus disparity encoded by the SNN was defined as the firing-rate weighted average of the encoded disparity dn for each neuron in C and D, or population Center of Mass (CoM) [21]:",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for calculating stimulus disparity using a Spiking Neural Network (SNN).",
      "processing_time": 15.844767570495605,
      "citing_paper_id": "233033658",
      "cited_paper_id": 14072069
    },
    {
      "context_text": "Digital peripheral asynchronous input/output logic circuits are used to receive and transmit spikes via an Address Event Representation (AER) communication protocol [20].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a communication protocol used in neuromorphic systems.",
      "processing_time": 15.242955446243286,
      "citing_paper_id": "233033658",
      "cited_paper_id": 14878668
    },
    {
      "context_text": "Following the pioneering work of Misha\nMahowald [6], several Spiking Neural Networks (SNNs) that reconstruct 3D information on a per-event basis have been recently deployed on fully digital, as well as mixed-signals neuromorphic architectures: Spinnaker [7], [8], True North [9], [10], ROLLS [11], [12], and DYNAP [13], [14].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only various neuromorphic architectures and SNNs. There are no clear identifiers for datasets or other verifiable resources.",
      "processing_time": 18.4304780960083,
      "citing_paper_id": "233033658",
      "cited_paper_id": 25268038
    },
    {
      "context_text": "Following the pioneering work of Misha Mahowald [6], several Spiking Neural Networks (SNNs) that reconstruct 3D information on a per-event basis have been recently deployed on fully digital, as well as mixed-signals neuromorphic architectures: Spinnaker [7], [8], True North [9], [10], ROLLS [11], [12], and DYNAP [13], [14].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only various neuromorphic architectures and SNNs. No verifiable resources are identified.",
      "processing_time": 17.264841079711914,
      "citing_paper_id": "233033658",
      "cited_paper_id": 25268038
    },
    {
      "context_text": "Following the pioneering work of Misha Mahowald [6], several Spiking Neural Networks (SNNs) that reconstruct 3D information on a per-event basis have been recently deployed on fully digital, as well as mixed-signals neuromorphic architectures: Spinnaker [7], [8], True North [9], [10], ROLLS [11], [12], and DYNAP [13], [14].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only various neuromorphic architectures and SNNs. No verifiable resources are identified.",
      "processing_time": 17.264841079711914,
      "citing_paper_id": "233033658",
      "cited_paper_id": 226308033
    },
    {
      "context_text": "Indeed, a novel class of event-based algorithms for stereo vision, also referred to as instantaneous stereo, extracts depth information by exploiting the inter-ocular spatio-temporal correlation of spike trains from event cameras [2].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a class of algorithms. No verifiable resources are identified.",
      "processing_time": 15.838657855987549,
      "citing_paper_id": "233033658",
      "cited_paper_id": 118684904
    },
    {
      "context_text": "Finally, for each input sample, we estimated the power consumption of the mixed-signal neuromorphic implementation as described in [14].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for estimating power consumption in a neuromorphic implementation.",
      "processing_time": 16.189398527145386,
      "citing_paper_id": "233033658",
      "cited_paper_id": 226308033
    },
    {
      "context_text": "Thus, in this work, we use the DHP19 dataset to assess the robustness of the event-based approach for neuromorphic, on-chip depth estimation recently presented in [14].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "DHP19"
      ],
      "dataset_descriptions": {
        "DHP19": "Used to assess the robustness of an event-based approach for neuromorphic, on-chip depth estimation, focusing on the performance and reliability of the method."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions the DHP19 dataset, which is used to assess the robustness of an event-based approach for neuromorphic, on-chip depth estimation.",
      "processing_time": 23.681954383850098,
      "citing_paper_id": "233033658",
      "cited_paper_id": 226308033
    },
    {
      "context_text": "The Spike-Based Neuromorphic Architecture The spike-based neuromorphic architecture used to extract disparity information is based on the hardwired topology proposed in [14].",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method or architecture. There are no clear identifiers for datasets in the provided context.",
      "processing_time": 16.971582889556885,
      "citing_paper_id": "233033658",
      "cited_paper_id": 226308033
    },
    {
      "context_text": "By contrast, [12] and [14] use mixed-signal analog/digital neuromorphic circuits that directly emulate the dynamics of the neural computing primitives used in biology to perform stereo vision.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and architectures. No verifiable resources are identified.",
      "processing_time": 15.554708242416382,
      "citing_paper_id": "233033658",
      "cited_paper_id": 226308033
    },
    {
      "context_text": "2, is adapted from the structure presented in [12], [14].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to other papers. There is no indication of dataset usage.",
      "processing_time": 16.503411293029785,
      "citing_paper_id": "233033658",
      "cited_paper_id": 226308033
    },
    {
      "context_text": "See [14] for a comprehensive description of the architecture.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a reference to the architecture described in another paper.",
      "processing_time": 15.230103731155396,
      "citing_paper_id": "233033658",
      "cited_paper_id": 226308033
    },
    {
      "context_text": "…camera systems, consisting of an event camera and a frame camera , to solve long-standing challenges in various applications, including de-blur [23], HDR imaging [10], SLAM [30], etc. Recently, depth estimation from SAFE systems [32, 42] has also been explored, which aims to estimate…",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general applications and systems. No clear, verifiable datasets are identified.",
      "processing_time": 16.753013372421265,
      "citing_paper_id": "268277996",
      "cited_paper_id": 3719281
    },
    {
      "context_text": "…of an event camera and a frame camera , to solve long-standing challenges in various applications, including de-blur [23], HDR imaging [10], SLAM [30], etc. Recently, depth estimation from SAFE systems [32, 42] has also been explored, which aims to estimate accurate depth in various conditions.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets by name. It discusses applications and methods but does not reference any particular dataset.",
      "processing_time": 16.49890947341919,
      "citing_paper_id": "268277996",
      "cited_paper_id": 4252896
    },
    {
      "context_text": "…of an event camera and a frame camera , to solve long-standing challenges in various applications, including de-blur [23], HDR imaging [10], SLAM [30], etc. Recently, depth estimation from SAFE systems [32, 42] has also been explored, which aims to estimate accurate depth in various conditions.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets by name. It discusses applications and methods but does not reference any particular dataset.",
      "processing_time": 16.49890947341919,
      "citing_paper_id": "268277996",
      "cited_paper_id": 245300947
    },
    {
      "context_text": "In a SAFE system, a frame camera and an event camera are used to perceive scenes with different modalities.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only describes the use of two types of cameras in a system.",
      "processing_time": 14.247740983963013,
      "citing_paper_id": "268277996",
      "cited_paper_id": 4252896
    },
    {
      "context_text": "These advantages make event cameras a promising alternative to conventional frame cameras in challenging scenarios, such as challenging illumination or high-speed situations.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only discusses the advantages of event cameras over conventional frame cameras.",
      "processing_time": 15.546590089797974,
      "citing_paper_id": "268277996",
      "cited_paper_id": 4252896
    },
    {
      "context_text": "An event is triggered by a pixel intensity change above a certain threshold and characterized by the corresponding pixel location, timestamp, and polarity.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only a general description of how events are triggered in event-based systems.",
      "processing_time": 16.182990312576294,
      "citing_paper_id": "268277996",
      "cited_paper_id": 4252896
    },
    {
      "context_text": "Depth estimation from SAFE systems is expected to be accurate in various scenarios, because the event camera provides high-quality signals even in high dynamic range or high-speed regions while the frame camera provides clear intensity signals in most regions.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only describes the capabilities of event cameras and frame cameras in depth estimation.",
      "processing_time": 15.810076475143433,
      "citing_paper_id": "268277996",
      "cited_paper_id": 4252896
    },
    {
      "context_text": "Experiments",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only 'Experiments'. No verifiable resources are identified.",
      "processing_time": 15.541723251342773,
      "citing_paper_id": "268277996",
      "cited_paper_id": 4712004
    },
    {
      "context_text": "Typically, to boost the estimation of one reference view, multiple source views are matched with the reference jointly [34,35] or respectively [12,18].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only methods and approaches for depth estimation in multi-view stereo. No verifiable resources are identified.",
      "processing_time": 17.700305938720703,
      "citing_paper_id": "268277996",
      "cited_paper_id": 4712004
    },
    {
      "context_text": "Typically, to boost the estimation of one reference view, multiple source views are matched with the reference jointly [34,35] or respectively [12,18].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only methods and approaches for depth estimation in multi-view stereo. No verifiable resources are identified.",
      "processing_time": 17.700305938720703,
      "citing_paper_id": "268277996",
      "cited_paper_id": 53073405
    },
    {
      "context_text": "We expect our method could generalize to other stereo asymmetric systems and leave it as a future work.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general statement about future work. No verifiable resources are identified.",
      "processing_time": 16.490379095077515,
      "citing_paper_id": "268277996",
      "cited_paper_id": 4712004
    },
    {
      "context_text": "We expect our method could generalize to other stereo asymmetric systems and leave it as a future work.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general statement about future work. No verifiable resources are identified.",
      "processing_time": 16.490379095077515,
      "citing_paper_id": "268277996",
      "cited_paper_id": 212675709
    },
    {
      "context_text": "Compared with cross-modal symmetric stereo systems, asymmetric stereo systems with a single modality on one side, e.g., frame-event [9,32,42] and RGB-NIR [16,38] systems, possess the same sensing capabilities and come with half costs.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only types of stereo systems. No verifiable resources are identified.",
      "processing_time": 16.168484926223755,
      "citing_paper_id": "268277996",
      "cited_paper_id": 29158639
    },
    {
      "context_text": "Compared with cross-modal symmetric stereo systems, asymmetric stereo systems with a single modality on one side, e.g., frame-event [9,32,42] and RGB-NIR [16,38] systems, possess the same sensing capabilities and come with half costs.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only types of stereo systems. No verifiable resources are identified.",
      "processing_time": 16.168484926223755,
      "citing_paper_id": "268277996",
      "cited_paper_id": 245300947
    },
    {
      "context_text": "For example, edge images and temporal gradient images are adopted to normalize frame and event images [15, 32], while transformation networks are proposed to make up the photometric inconsistency of RGB and NIR images [16, 38].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only methods and approaches. The cited papers' titles do not provide additional dataset information.",
      "processing_time": 17.491609811782837,
      "citing_paper_id": "268277996",
      "cited_paper_id": 29158639
    },
    {
      "context_text": "For example, edge images and temporal gradient images are adopted to normalize frame and event images [15, 32], while transformation networks are proposed to make up the photometric inconsistency of RGB and NIR images [16, 38].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only methods and approaches. The cited papers' titles do not provide additional dataset information.",
      "processing_time": 17.491609811782837,
      "citing_paper_id": "268277996",
      "cited_paper_id": 253651036
    },
    {
      "context_text": "…camera systems, consisting of an event camera and a frame camera , to solve long-standing challenges in various applications, including de-blur [23], HDR imaging [10], SLAM [30], etc. Recently, depth estimation from SAFE systems [32, 42] has also been explored, which aims to estimate accurate…",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only applications and methods. The cited paper title does not help in identifying a dataset.",
      "processing_time": 17.237674236297607,
      "citing_paper_id": "268277996",
      "cited_paper_id": 53749928
    },
    {
      "context_text": "Due to the unique working principle, event cameras present several attractive advantages, including high dynamic range ( > 120 dB), high temporal resolution (in the order of microsecond), etc [6].",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only general properties of event cameras. No verifiable resources are identified.",
      "processing_time": 16.485198497772217,
      "citing_paper_id": "268277996",
      "cited_paper_id": 118684904
    },
    {
      "context_text": "However, modality asymmetry can not be eliminated or even mitigated in certain challenging regions due to the inevitable information absence of one modality, e.g. , high dynamic range regions for frame cameras and regions inside object contours for event cameras.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "DSEC"
      ],
      "dataset_descriptions": {
        "DSEC": "Used to evaluate stereo depth estimation methods in driving scenarios, focusing on the challenges posed by modality asymmetry in high dynamic range regions and object contours."
      },
      "confidence_score": 0.8,
      "reasoning": "The context does not mention any specific datasets, only discusses challenges in modality asymmetry. However, the cited paper title suggests a relevant dataset.",
      "processing_time": 22.04418134689331,
      "citing_paper_id": "268277996",
      "cited_paper_id": 232170230
    },
    {
      "context_text": "E-SfM with two source views ( E Rt and E Rt − 1 ) could be recognized as a two-view SfM problem [31] with known camera poses ( e.g. , from the inertial measurement unit, IMU).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method or approach. The context is about recognizing a problem as a two-view SfM problem with known camera poses.",
      "processing_time": 18.845890283584595,
      "citing_paper_id": "268277996",
      "cited_paper_id": 232478376
    },
    {
      "context_text": "Therefore, FF-PSMNet can not reveal the 3D geometry in these regions as accurately as SAFE-Ours and EE-DDES.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a comparison between methods. No verifiable resources are identified.",
      "processing_time": 14.919135808944702,
      "citing_paper_id": "268277996",
      "cited_paper_id": 245300947
    },
    {
      "context_text": "More recently, cross-modal symmetric stereo systems with cameras of different modalities on both sides have been proposed and demonstrate distinct performance in various scenarios [4,20,21].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only references to cross-modal symmetric stereo systems. No clear, verifiable datasets are identified.",
      "processing_time": 17.693933486938477,
      "citing_paper_id": "268277996",
      "cited_paper_id": 249980412
    },
    {
      "context_text": "More recently, cross-modal symmetric stereo systems with cameras of different modalities on both sides have been proposed and demonstrate distinct performance in various scenarios [4,20,21].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only references to cross-modal symmetric stereo systems. No clear, verifiable datasets are identified.",
      "processing_time": 17.693933486938477,
      "citing_paper_id": "268277996",
      "cited_paper_id": 253513043
    },
    {
      "context_text": "To conduct stereo matching from a pair of frame and event images (converted from event streams) with signiﬁcant asymmetry, existing methods pro-pose to normalize [32] or transform [15] different modalities to a uniﬁed form.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods for stereo matching. No dataset names are provided in the context.",
      "processing_time": 16.729548931121826,
      "citing_paper_id": "268277996",
      "cited_paper_id": 253651036
    },
    {
      "context_text": "Qualitative comparison of depth estimated using the MC-EMVS method [13], applying it to the new selected pixels F denser and our method DERD-Net, for the MVSEC indoor flying [48] (top 3 rows) and DSEC zurich city 04 a (bottom row) sequences.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "MVSEC indoor flying",
        "DSEC zurich city 04 a"
      ],
      "dataset_descriptions": {
        "MVSEC indoor flying": "Used to qualitatively compare depth estimation methods, focusing on dense pixel selection and performance in indoor flying scenarios.",
        "DSEC zurich city 04 a": "Used to qualitatively compare depth estimation methods, focusing on dense pixel selection and performance in urban outdoor scenarios."
      },
      "confidence_score": 1.0,
      "reasoning": "The citation mentions two specific datasets, 'MVSEC indoor flying' and 'DSEC zurich city 04 a', which are used for qualitative comparison of depth estimation methods.",
      "processing_time": 31.4975688457489,
      "citing_paper_id": "277994207",
      "cited_paper_id": 3416874
    },
    {
      "context_text": "In this section, we describe the experiments conducted on the indoor flying sequences 1,2,3 of the MVSEC dataset [48] to evaluate the performance of the proposed depth estimation method.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "MVSEC dataset"
      ],
      "dataset_descriptions": {
        "MVSEC dataset": "Used to evaluate the performance of the proposed depth estimation method on indoor flying sequences, focusing on 3D perception using event camera data."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions the use of the 'MVSEC dataset' for evaluating the performance of a depth estimation method, which is directly relevant to the research topic of event-based stereo depth estimation.",
      "processing_time": 32.777114391326904,
      "citing_paper_id": "277994207",
      "cited_paper_id": 3416874
    },
    {
      "context_text": "Following prior protocols, we conduct experiments on the MVSEC [48] and the DSEC [10] datasets.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "MVSEC",
        "DSEC"
      ],
      "dataset_descriptions": {
        "MVSEC": "Used to conduct experiments on event-based stereo depth estimation, focusing on 3D perception with event cameras.",
        "DSEC": "Used to conduct experiments on event-based stereo depth estimation, focusing on 3D perception with event cameras."
      },
      "confidence_score": 1.0,
      "reasoning": "The citation mentions two specific datasets, MVSEC and DSEC, which are used for conducting experiments in event-based stereo depth estimation.",
      "processing_time": 34.270638942718506,
      "citing_paper_id": "277994207",
      "cited_paper_id": 3416874
    },
    {
      "context_text": "Moreover, the scarcity of event camera datasets with ground truth depth [14, 48] results in limited training data, which can lead to overfitting [38].",
      "catation_intent": "gap",
      "resource_type": "gap",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions the scarcity of event camera datasets with ground truth depth, indicating a gap in available resources. No specific dataset names are provided.",
      "processing_time": 24.042221307754517,
      "citing_paper_id": "277994207",
      "cited_paper_id": 3416874
    },
    {
      "context_text": "Moreover, the scarcity of event camera datasets with ground truth depth [14, 48] results in limited training data, which can lead to overfitting [38].",
      "catation_intent": "gap",
      "resource_type": "gap",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions the scarcity of event camera datasets with ground truth depth, indicating a gap in available resources. No specific dataset names are provided.",
      "processing_time": 24.042221307754517,
      "citing_paper_id": "277994207",
      "cited_paper_id": 262638843
    },
    {
      "context_text": "• Comprehensive Experiments : We evaluate our model using both monocular and stereo data from standard datasets MVSEC [48] and DSEC [10], employing cross-validation, outperforming the state of the art by a large margin on ten figures of merit.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "MVSEC",
        "DSEC"
      ],
      "dataset_descriptions": {
        "MVSEC": "Used to evaluate the model's performance in event-based stereo depth estimation, employing cross-validation and comparing against state-of-the-art methods.",
        "DSEC": "Used to evaluate the model's performance in event-based stereo depth estimation, employing cross-validation and comparing against state-of-the-art methods."
      },
      "confidence_score": 1.0,
      "reasoning": "The citation mentions two specific datasets, MVSEC and DSEC, which are used for evaluating the model's performance in event-based stereo depth estimation.",
      "processing_time": 36.76177740097046,
      "citing_paper_id": "277994207",
      "cited_paper_id": 3416874
    },
    {
      "context_text": "Recent approaches have addressed stereo event-based 3D reconstruction for VO and SLAM [7, 15, 35, 45–47].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to other papers. No dataset names are provided in the context.",
      "processing_time": 23.853471755981445,
      "citing_paper_id": "277994207",
      "cited_paper_id": 4412139
    },
    {
      "context_text": "Recent approaches have addressed stereo event-based 3D reconstruction for VO and SLAM [7, 15, 35, 45–47].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to other papers. No dataset names are provided in the context.",
      "processing_time": 23.853471755981445,
      "citing_paper_id": "277994207",
      "cited_paper_id": 250699235
    },
    {
      "context_text": "Stereo depth estimation with event cameras has been an intriguing problem since the invention of the first event camera by Mahowald and Mead in the 1990s [9, 14, 26] due to their potential for high temporal resolution and robustness to motion blur.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to the invention of event cameras. No verifiable resources are identified.",
      "processing_time": 24.203834295272827,
      "citing_paper_id": "277994207",
      "cited_paper_id": 6539071
    },
    {
      "context_text": "The Generalized Time-Based Stereovision (GTS) method [18] utilizes a two-step process: first performing stereo matching based on a time-consistency score for each event, followed by depth estimation through triangulation.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation describes a method (GTS) rather than a dataset. No specific dataset is mentioned or used in the described research context.",
      "processing_time": 24.393901109695435,
      "citing_paper_id": "277994207",
      "cited_paper_id": 49554392
    },
    {
      "context_text": "Deep learning has significantly advanced depth estimation in traditional monocular and stereo camera setups, achieving remarkable results [20, 21, 37].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only general advancements in depth estimation using deep learning.",
      "processing_time": 23.351800203323364,
      "citing_paper_id": "277994207",
      "cited_paper_id": 198229801
    },
    {
      "context_text": "Deep learning has significantly advanced depth estimation in traditional monocular and stereo camera setups, achieving remarkable results [20, 21, 37].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only general advancements in depth estimation using deep learning.",
      "processing_time": 23.351800203323364,
      "citing_paper_id": "277994207",
      "cited_paper_id": 219303641
    },
    {
      "context_text": "Unlike conventional cameras, event cameras operate asynchronously, detecting per-pixel brightness changes [8, 9, 22].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only describes the operation of event cameras. No verifiable resources are identified.",
      "processing_time": 24.036351203918457,
      "citing_paper_id": "277994207",
      "cited_paper_id": 215799961
    },
    {
      "context_text": "Unlike conventional cameras, event cameras operate asynchronously, detecting per-pixel brightness changes [8, 9, 22].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only describes the operation of event cameras. No verifiable resources are identified.",
      "processing_time": 24.036351203918457,
      "citing_paper_id": "277994207",
      "cited_paper_id": null
    },
    {
      "context_text": "Finally, we report δ -accuracy values, which indicate the percentage of points whose estimated depth falls within specified limits relative to GT [41].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific dataset names, only a general reference to ground truth (GT) data. No multi-word proper nouns, acronyms, or hyphenated names with digits are present.",
      "processing_time": 27.098435401916504,
      "citing_paper_id": "277994207",
      "cited_paper_id": 226298400
    },
    {
      "context_text": "For stereo depth estimation, [38, 40] present two pio-neering studies.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only references to pioneering studies in event-based stereo depth estimation.",
      "processing_time": 23.630143404006958,
      "citing_paper_id": "277994207",
      "cited_paper_id": 250374739
    },
    {
      "context_text": "For stereo depth estimation, [38, 40] present two pio-neering studies.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only references to pioneering studies in event-based stereo depth estimation.",
      "processing_time": 23.630143404006958,
      "citing_paper_id": "277994207",
      "cited_paper_id": 262638843
    },
    {
      "context_text": "Specifically, DDES [38] introduced the first deep-learning–based supervised stereo-matching method, while [40] proposed the first unsupervised learning framework.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions two papers, one introducing a supervised method and the other an unsupervised method for event-based stereo depth estimation. However, no specific datasets are mentioned.",
      "processing_time": 26.53423833847046,
      "citing_paper_id": "277994207",
      "cited_paper_id": 250374739
    },
    {
      "context_text": "Specifically, DDES [38] introduced the first deep-learning–based supervised stereo-matching method, while [40] proposed the first unsupervised learning framework.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions two papers, one introducing a supervised method and the other an unsupervised method for event-based stereo depth estimation. However, no specific datasets are mentioned.",
      "processing_time": 26.53423833847046,
      "citing_paper_id": "277994207",
      "cited_paper_id": 262638843
    },
    {
      "context_text": "For each interval, we construct two DSIs (one for each camera) and fuse them by applying voxel-wise metrics (e.g., harmonic mean) as described in [13].",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for constructing and fusing depth images from event cameras.",
      "processing_time": 23.166988372802734,
      "citing_paper_id": "277994207",
      "cited_paper_id": 250918780
    },
    {
      "context_text": "MC-EMVS [13] introduced a novel stereo approach for depth estimation which does not require explicit data association, using DSIs generated from stereo events cameras.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions 'DSIs generated from stereo events cameras' but does not specify a dataset name. The context is about a method for depth estimation using event cameras.",
      "processing_time": 26.290956497192383,
      "citing_paper_id": "277994207",
      "cited_paper_id": 250918780
    },
    {
      "context_text": "A confidence map is generated by projecting the DSI onto a 2D grid of size W × H , where each pixel’s value represents the maximum ray density among all depth levels [13] (pixel selection map in Fig.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific dataset names. It describes a method for generating a confidence map using a DSI (Depth Surface Indicator) projected onto a 2D grid.",
      "processing_time": 26.822325706481934,
      "citing_paper_id": "277994207",
      "cited_paper_id": 250918780
    },
    {
      "context_text": "The performance of the networks is evaluated using ten standard metrics commonly employed in depth estimation tasks [13].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only standard metrics for depth estimation tasks.",
      "processing_time": 23.034334897994995,
      "citing_paper_id": "277994207",
      "cited_paper_id": 250918780
    },
    {
      "context_text": "To test this hypothesis, we used a larger filter window size of 9 × 9 px and a subtractive constant of C denser = − 10 , re-5 Scene Pixel selection map MC-EMVS [13] MC-EMVS + F denser DERD-Net (Ours) Ground truth (GT) Figure 3.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific, verifiable datasets. It describes a method and experimental setup but does not reference a named dataset.",
      "processing_time": 25.011046171188354,
      "citing_paper_id": "277994207",
      "cited_paper_id": 250918780
    },
    {
      "context_text": "Subsequently, we retrained it on stereo DSIs fused via the harmonic mean and compared its performance to MC-EMVS [13].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (MC-EMVS) and a general process of retraining. No clear, verifiable dataset names are present.",
      "processing_time": 26.285359859466553,
      "citing_paper_id": "277994207",
      "cited_paper_id": 250918780
    },
    {
      "context_text": "The Multi-Camera Event-based Multi-View Stereo (MC-EMVS) method has recently produced state-of-the-art (SOTA) results, outperforming other techniques in depth benchmarks across several metrics [13].",
      "catation_intent": "findings",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions a method (MC-EMVS) and its performance in depth benchmarks, but does not specify any datasets used.",
      "processing_time": 24.715999364852905,
      "citing_paper_id": "277994207",
      "cited_paper_id": 250918780
    },
    {
      "context_text": "The two closest baseline methods for performance comparison of our method are EMVS for monocular vision [31] and MC-EMVS for stereo vision [13].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods for performance comparison.",
      "processing_time": 22.90836215019226,
      "citing_paper_id": "277994207",
      "cited_paper_id": 250918780
    },
    {
      "context_text": "The objective is to capture local geometrical patterns in the Sub-DSI to extract more relevant depth information from it than the SOTA argmax approach used in [13, 31].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a methodological approach. No dataset names are present in the citation span.",
      "processing_time": 24.70841956138611,
      "citing_paper_id": "277994207",
      "cited_paper_id": 250918780
    },
    {
      "context_text": "Following [13, 31], this stream is sliced by dividing time into intervals.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for slicing a stream of events. No verifiable resources are identified.",
      "processing_time": 24.71500563621521,
      "citing_paper_id": "277994207",
      "cited_paper_id": 250918780
    },
    {
      "context_text": "No other method reports good generalization on “split 2” of MVSEC because of the difference in dynamic characteris-Method tics of events in training and testing on that split (as mentioned in [1, 38]).",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.3,
      "reasoning": "The citation mentions 'split 2' of MVSEC, which is a known dataset in the field of event-based vision. However, the citation does not explicitly state that the dataset is used in the research, only that it is referenced for context.",
      "processing_time": 30.530653715133667,
      "citing_paper_id": "277994207",
      "cited_paper_id": 262638843
    },
    {
      "context_text": "This step is based on the concept of epipolar geometry [1] which constructs an epipolar plane which primarily includes epipolar lines across the x coordinates.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a concept related to epipolar geometry. No verifiable resources are identified.",
      "processing_time": 25.28996253013611,
      "citing_paper_id": "258869142",
      "cited_paper_id": 147709
    },
    {
      "context_text": "The traditional camera based stereo vision [1], [12], has seen more advancements in the recent works assisted by deep learning to achieve high resolution binocular vision",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general advancement in camera-based stereo vision using deep learning.",
      "processing_time": 24.37002730369568,
      "citing_paper_id": "258869142",
      "cited_paper_id": 147709
    },
    {
      "context_text": "The concept of stereo vision deeply relies on epipolar geometry [1] which will be briefly described in the following calibration step.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a theoretical concept related to stereo vision.",
      "processing_time": 23.61435627937317,
      "citing_paper_id": "258869142",
      "cited_paper_id": 147709
    },
    {
      "context_text": "The eDVS setup records the event-based data and yields two files in the aedat4 [2] format, each for the left and right cameras.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions 'event-based data' but does not specify a named dataset. The term 'aedat4' refers to a file format, not a dataset.",
      "processing_time": 26.81067967414856,
      "citing_paper_id": "258869142",
      "cited_paper_id": 2070927
    },
    {
      "context_text": "In contrast, Dynamic Vision Sensors (DVS) [9] encode temporal contrast (TC), i.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a sensor technology. No dataset names are present in the citation context.",
      "processing_time": 24.99469494819641,
      "citing_paper_id": "258869142",
      "cited_paper_id": 15357188
    },
    {
      "context_text": "Stereo vision is the ability to construct a unified view of an object or scene, with two different vision sensors/cameras [11].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general concept of stereo vision. No verifiable resources are identified.",
      "processing_time": 24.994077682495117,
      "citing_paper_id": "258869142",
      "cited_paper_id": 23913692
    },
    {
      "context_text": "This requires identifying and matching features in the left and right images of the same point in the visual scene that helps to derive information about how far away objects are, based solely on the relative positions of the object in the two sensors [11].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general concept of stereo vision. No verifiable resources are identified.",
      "processing_time": 24.363035678863525,
      "citing_paper_id": "258869142",
      "cited_paper_id": 23913692
    },
    {
      "context_text": "[7], render high quality 3-D reconstruction of stereo images [13] or MRI images [6], [10], etc.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general applications of 3D reconstruction techniques.",
      "processing_time": 24.170495748519897,
      "citing_paper_id": "258869142",
      "cited_paper_id": 209202615
    },
    {
      "context_text": "Those can estimate depth from binocular parallax using two cameras [2] or motion [3], [4], from focus [5], [6], from shading [7], from occlusions [8], from linear perspective [9], and many others.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various methods for depth estimation. No dataset names are present in the text.",
      "processing_time": 25.697661876678467,
      "citing_paper_id": "53086261",
      "cited_paper_id": 3494469
    },
    {
      "context_text": ", patches in images [12] or time surfaces in event-based systems [13]).",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only general concepts like 'patches in images' and 'time surfaces in event-based systems'. No verifiable resources are identified.",
      "processing_time": 27.598397254943848,
      "citing_paper_id": "53086261",
      "cited_paper_id": 10712214
    },
    {
      "context_text": "To help placing our system in the vast landscape of eventbased stereo vision approaches that have been devised, note that 1) it does not rely on the precise spatio-temporal matching of event time surfaces such as in [13], [25]–[28], and that 2) even though it is an active approach using the emission of light in the scene, it is not using the idea of analyzing the deformations of an a-priori known pattern of light such as in structured light (that often make use of a single sensor) [29]– [31], nor using any time-of-flight information [32].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods and approaches. No verifiable resources are identified.",
      "processing_time": 24.69652271270752,
      "citing_paper_id": "53086261",
      "cited_paper_id": 10712214
    },
    {
      "context_text": "To help placing our system in the vast landscape of eventbased stereo vision approaches that have been devised, note that 1) it does not rely on the precise spatio-temporal matching of event time surfaces such as in [13], [25]–[28], and that 2) even though it is an active approach using the emission of light in the scene, it is not using the idea of analyzing the deformations of an a-priori known pattern of light such as in structured light (that often make use of a single sensor) [29]– [31], nor using any time-of-flight information [32].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods and approaches. No verifiable resources are identified.",
      "processing_time": 24.69652271270752,
      "citing_paper_id": "53086261",
      "cited_paper_id": 23276048
    },
    {
      "context_text": "Thus, a natural sensor to use to detect such a change is a Dynamic Vision Sensor (DVS) [20]–[22].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a type of sensor (DVS). No dataset names are present in the citation context.",
      "processing_time": 26.261950969696045,
      "citing_paper_id": "53086261",
      "cited_paper_id": 15357188
    },
    {
      "context_text": "In the DAVIS240C sensor we use, each pixel contains both an APS and DVS circuit sharing the same photodiode.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a sensor used in the research. No verifiable resources are identified.",
      "processing_time": 24.355701446533203,
      "citing_paper_id": "53086261",
      "cited_paper_id": 15357188
    },
    {
      "context_text": "Thus, we can calibrate our system using the grayscale images produced by the APS part and use the estimated parameters for further operations involving events produced by the DVS part.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific, verifiable datasets. It only describes a method for calibrating a system using grayscale images and event data.",
      "processing_time": 27.0625216960907,
      "citing_paper_id": "53086261",
      "cited_paper_id": 15357188
    },
    {
      "context_text": "DAVIS sensors contain both a Dynamic Vision Sensor (DVS) and an Active Pixel Sensor (APS) circuit in each pixel.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only hardware components of DAVIS sensors. No verifiable resources are identified.",
      "processing_time": 25.265618324279785,
      "citing_paper_id": "53086261",
      "cited_paper_id": 15357188
    },
    {
      "context_text": "In this paper, we demonstrated how combining a laser that can quickly scan a scene with a pair of DVS allows us to create\na stereo vision system, in which the stereo-matching problem is alleviated.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method involving a laser and DVS sensors for stereo vision.",
      "processing_time": 25.269646644592285,
      "citing_paper_id": "53086261",
      "cited_paper_id": 15357188
    },
    {
      "context_text": "Hence, whenever the laser blinks or moves to a position, it creates events that are captured in the two DVS (as seen in Figure 3).",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only describes the operation of a Dynamic Vision Sensor (DVS).",
      "processing_time": 24.15342879295349,
      "citing_paper_id": "53086261",
      "cited_paper_id": 15357188
    },
    {
      "context_text": "A DVS is a vision chip that reports events indicating changes in brightness for each pixel asynchronously.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a description of a Dynamic Vision Sensor (DVS). No verifiable resources are identified.",
      "processing_time": 26.252851247787476,
      "citing_paper_id": "53086261",
      "cited_paper_id": 15357188
    },
    {
      "context_text": "The difficulty of stereo-matching, whether it is area based or feature based, is illustrated by the numerous hardware accelerators that have been devised to perform these operations [15]–[19].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only hardware accelerators for stereo-matching and image feature extraction.",
      "processing_time": 24.975079774856567,
      "citing_paper_id": "53086261",
      "cited_paper_id": 22296005
    },
    {
      "context_text": "By observing a scene with two sensors separated by some baseline, the third dimension – depth – can be recovered by triangulation [10].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general principle of stereo vision.",
      "processing_time": 23.428565979003906,
      "citing_paper_id": "53086261",
      "cited_paper_id": 261497446
    },
    {
      "context_text": "Triangulation is performed using a Direct Linear Transform [10] after refining the twodimensional coordinates of our points using constraints imposed by the geometry of our setup (epipolar constraints).",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for triangulation using geometric constraints.",
      "processing_time": 23.589290857315063,
      "citing_paper_id": "53086261",
      "cited_paper_id": 261497446
    },
    {
      "context_text": "Patch based techniques have also been used in Popham et al. (2010) and Cagniart et al. (2010) to split complex surfaces into simpler ones.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and approaches. No verifiable resources are identified.",
      "processing_time": 24.955798149108887,
      "citing_paper_id": "8305136",
      "cited_paper_id": 1846045
    },
    {
      "context_text": "Scene ﬂow can also be computed from local descriptors of reconstructed surfaces such as surfel that encodes the local geometry and the reﬂectance information of the shapes (Carceroni and Kutulakos, 2002).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for computing scene flow using local descriptors of reconstructed surfaces.",
      "processing_time": 25.490782499313354,
      "citing_paper_id": "8305136",
      "cited_paper_id": 1864608
    },
    {
      "context_text": "Regularization is often performed by minimizing an energy function with variational formulations (Zhang et al., 2001; Min and Sohn, 2006; Huguet and Devernay, 2007).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only references to methods and regularization techniques.",
      "processing_time": 23.42298197746277,
      "citing_paper_id": "8305136",
      "cited_paper_id": 2610586
    },
    {
      "context_text": "However, SFM’s high vulnerability to images’ noise and to camera calibration errors raised questions regarding its applicability in real-world scenarios (Tomasi and Zhang, 1995).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (SFM) and its limitations. There are no verifiable resources or datasets mentioned.",
      "processing_time": 26.238001108169556,
      "citing_paper_id": "8305136",
      "cited_paper_id": 9208584
    },
    {
      "context_text": "The algorithm is applied to two sources of 3D data: a Microsoft Kinect (an RGBD sensor that outputs frames of 3D points aligned with RGB information) and an asynchronous event-based 3D reconstruction system as introduced in Carneiro et al. (2013).",
      "catation_intent": "reusable resource",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions two sources of 3D data but does not specify any named datasets. The Kinect and the asynchronous event-based 3D reconstruction system are described as data sources, not datasets.",
      "processing_time": 29.55410361289978,
      "citing_paper_id": "8305136",
      "cited_paper_id": 10712214
    },
    {
      "context_text": "This subsection provides the 3D scene ﬂow using event-based cameras (DVS) as described in Carneiro et al. (2013).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method or approach for 3D scene flow using event-based cameras.",
      "processing_time": 24.955716609954834,
      "citing_paper_id": "8305136",
      "cited_paper_id": 10712214
    },
    {
      "context_text": "As introduced in Rogister et al. (2012) and Carneiro et al. (2013), event-based cameras allow to estimate depth and produce 3D point clouds at unprecedented accuracy ( > 1 kHz in real-time) at very low computational and energy cost using conventional processing hardware.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only discusses the capabilities of event-based cameras.",
      "processing_time": 24.33498787879944,
      "citing_paper_id": "8305136",
      "cited_paper_id": 10712214
    },
    {
      "context_text": "This approach parametrizes the motion problem on the image plane, i.e., in 2D and is the most commonly found in the existing literature (Vedula et al., 1999; Zhang et al., 2001; Isard and MacCormick, 2006; Wedel et al., 2011).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only references to methods and approaches in the literature.",
      "processing_time": 23.790275812149048,
      "citing_paper_id": "8305136",
      "cited_paper_id": 18164747
    },
    {
      "context_text": "It is also called the mean closest point between both points clouds and is a dissimilarity measure often used for example in the Iterative Closest Point (ICP) problem (Besl and McKay, 1992).",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific dataset, only a method (ICP) and a dissimilarity measure. No dataset names are present in the citation span.",
      "processing_time": 26.774592876434326,
      "citing_paper_id": "8305136",
      "cited_paper_id": 21874346
    },
    {
      "context_text": "DVS yield sparse event data in case of a motion occurring within the area of very similar brightness intensity, which the sensor is unable to discern, resulting in poor results of event lifetime and disparity estimation.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only discusses limitations of DVS sensors in certain conditions.",
      "processing_time": 23.78307819366455,
      "citing_paper_id": "197431184",
      "cited_paper_id": 2497402
    },
    {
      "context_text": "Event-based vision sensors [1], such as the dynamic vision sensor (DVS) [2] and dynamic and active-pixel vision sensor (DAVIS) [3], are relatively novel biologically inspired cameras that output pixel-wise changes in brightness intensity in the scene, which are referred to as events due to their asynchronous nature.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only types of event-based vision sensors. No verifiable resources are identified.",
      "processing_time": 25.893153429031372,
      "citing_paper_id": "197431184",
      "cited_paper_id": 2497402
    },
    {
      "context_text": "Event-based vision sensors [1], such as the dynamic vision sensor (DVS) [2] and dynamic and active-pixel vision sensor (DAVIS) [3], are relatively novel biologically inspired cameras that output pixel-wise changes in brightness intensity in the scene, which are referred to as events due to their asynchronous nature.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only types of event-based vision sensors. No verifiable resources are identified.",
      "processing_time": 25.893153429031372,
      "citing_paper_id": "197431184",
      "cited_paper_id": 24007071
    },
    {
      "context_text": "Index Terms—event-based cameras, stereo vision, event lifetime estimation, disparity estimation\nI. INTRODUCTION\nEvent-based vision sensors [1], such as the dynamic vision sensor (DVS) [2] and dynamic and active-pixel vision sensor (DAVIS) [3], are relatively novel biologically inspired cameras that output pixel-wise changes in brightness intensity in the scene, which are referred to as events due to their asynchronous nature.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only types of event-based vision sensors. No verifiable resources are identified.",
      "processing_time": 25.891303300857544,
      "citing_paper_id": "197431184",
      "cited_paper_id": 2497402
    },
    {
      "context_text": "Index Terms—event-based cameras, stereo vision, event lifetime estimation, disparity estimation\nI. INTRODUCTION\nEvent-based vision sensors [1], such as the dynamic vision sensor (DVS) [2] and dynamic and active-pixel vision sensor (DAVIS) [3], are relatively novel biologically inspired cameras that output pixel-wise changes in brightness intensity in the scene, which are referred to as events due to their asynchronous nature.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only types of event-based vision sensors. No verifiable resources are identified.",
      "processing_time": 25.891303300857544,
      "citing_paper_id": "197431184",
      "cited_paper_id": 24007071
    },
    {
      "context_text": "A recent survey on event-based vision applications and methods can be found in [4] and an example of a processed DVS output can be seen in Fig.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a reference to a survey and a figure. No verifiable resources are identified.",
      "processing_time": 26.474249362945557,
      "citing_paper_id": "197431184",
      "cited_paper_id": 2497402
    },
    {
      "context_text": "The implementation of the proposed method is done in C++ in ROS environment [20].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only the implementation environment.",
      "processing_time": 23.57065486907959,
      "citing_paper_id": "197431184",
      "cited_paper_id": 6324125
    },
    {
      "context_text": "Accumulation can be done directly by choosing a fixed time interval and this approach has been used for tracking and optical flow estimation [7], [8].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only general approaches for tracking and optical flow estimation.",
      "processing_time": 23.965934991836548,
      "citing_paper_id": "197431184",
      "cited_paper_id": 10280488
    },
    {
      "context_text": "We use an event feature descriptor introduced by [16] that supports event-driven stereo matching based on distance transform [17] which describes the context of a pixel in binary images by calculating the distance from the pixel to the nearest active pixel.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific, verifiable datasets. It discusses methods and techniques but does not reference any named datasets.",
      "processing_time": 25.886123180389404,
      "citing_paper_id": "197431184",
      "cited_paper_id": 12212153
    },
    {
      "context_text": "We use an event feature descriptor introduced by [16] that supports event-driven stereo matching based on distance transform [17] which describes the context of a pixel in binary images by calculating the distance from the pixel to the nearest active pixel.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific, verifiable datasets. It discusses methods and techniques but does not reference any named datasets.",
      "processing_time": 25.886123180389404,
      "citing_paper_id": "197431184",
      "cited_paper_id": 157060825
    },
    {
      "context_text": "Experiments were conducted with two DAVIS240 sensors mounted on a stereo rig with a baseline of 10 cm as shown in Fig.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific, verifiable datasets. It only describes the experimental setup using DAVIS240 sensors.",
      "processing_time": 26.226399660110474,
      "citing_paper_id": "197431184",
      "cited_paper_id": 24007071
    },
    {
      "context_text": "Stream of events from DAVIS240 is fetched in grouped sequences through a ROS topic.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions 'DAVIS240' which is a specific event camera sensor, not a dataset. No other specific datasets are mentioned.",
      "processing_time": 25.88419198989868,
      "citing_paper_id": "197431184",
      "cited_paper_id": 24007071
    },
    {
      "context_text": "We used the DAVIS240 standard camera frames for calibration, but our proposed method operates on events only.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "DAVIS240"
      ],
      "dataset_descriptions": {
        "DAVIS240": "Used for calibration purposes, providing standard camera frames to ensure accurate setup before applying the event-only processing method."
      },
      "confidence_score": 0.8,
      "reasoning": "The context mentions 'DAVIS240 standard camera frames' which is a specific dataset used for calibration. However, the primary focus is on event-based processing, which is not directly related to the dataset.",
      "processing_time": 34.629485845565796,
      "citing_paper_id": "197431184",
      "cited_paper_id": 24007071
    },
    {
      "context_text": "In [12], [13] authors used a fixed event accumulation interval to generate artificial frames, while [14] generated different frames based on event timestamps.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only methods for generating frames from events. No clear, verifiable datasets are identified.",
      "processing_time": 26.760247707366943,
      "citing_paper_id": "197431184",
      "cited_paper_id": 24236495
    },
    {
      "context_text": "In [12], [13] authors used a fixed event accumulation interval to generate artificial frames, while [14] generated different frames based on event timestamps.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only methods for generating frames from events. No clear, verifiable datasets are identified.",
      "processing_time": 26.760247707366943,
      "citing_paper_id": "197431184",
      "cited_paper_id": 44623261
    },
    {
      "context_text": "Recently, significant efforts have been made in the area of intensity image reconstruction from events [5], [6]; however, for some applications complex image intensity reconstruction",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only general research areas and applications.",
      "processing_time": 24.317522764205933,
      "citing_paper_id": "197431184",
      "cited_paper_id": 115151433
    },
    {
      "context_text": "We compare the proposed method in terms of both execution time and accuracy with the method proposed in [16] that first executes lifetime estimation separately for both sensors and then follows up with disparity estimation (referred to as the decoupled method).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a comparison of methods. No verifiable resources are identified.",
      "processing_time": 24.655522346496582,
      "citing_paper_id": "197431184",
      "cited_paper_id": 157060825
    },
    {
      "context_text": "However, opposed to choosing a fixed time event accumulation interval, enhancing the event stream with event lifetimes yields more accurate disparity maps [16].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific dataset names. It discusses a method for enhancing event streams with event lifetimes to improve disparity maps, which is relevant to event-based stereo depth estimation.",
      "processing_time": 29.294447898864746,
      "citing_paper_id": "197431184",
      "cited_paper_id": 157060825
    },
    {
      "context_text": "Although [16] proposed a descriptor invariant to scale and rotation, we do not implement those features since they are not needed for stereo matching on rectified images.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method or feature that was not implemented.",
      "processing_time": 24.93097186088562,
      "citing_paper_id": "197431184",
      "cited_paper_id": 157060825
    },
    {
      "context_text": "disparity estimation method is computationally more efficient and accurate in comparison to the method that decouples event lifetime and disparity estimation [16].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a comparison between methods for disparity estimation.",
      "processing_time": 24.648606061935425,
      "citing_paper_id": "197431184",
      "cited_paper_id": 157060825
    },
    {
      "context_text": "Cost aggregation is performed as suggested in [16], in a fixed Algorithm 1 Stereo event lifetime (sensorA, sensorB)",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for cost aggregation in stereo depth estimation.",
      "processing_time": 24.92877197265625,
      "citing_paper_id": "197431184",
      "cited_paper_id": 157060825
    },
    {
      "context_text": "The camera intrinsics and extrinsics are estimated using a grid of AprilTags [35] that is moved in front of the sensor rig and calibrated using Kalibr.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions AprilTags but does not refer to it as a dataset. It is used as a tool for calibration, which is outside the scope of the dataset extraction rules.",
      "processing_time": 28.57188081741333,
      "citing_paper_id": "3416874",
      "cited_paper_id": 277804
    },
    {
      "context_text": "[5] provide a large dataset of a DAVIS 346B mounted behind the windshield of a car, with 12 hours of driving, intended for end to end learning of various driving related tasks.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "DDD17"
      ],
      "dataset_descriptions": {
        "DDD17": "Used to provide a large dataset of a DAVIS 346B sensor mounted in a car, capturing 12 hours of driving data for end-to-end learning of driving-related tasks."
      },
      "confidence_score": 1.0,
      "reasoning": "The citation mentions a specific dataset with a clear name and purpose, which is relevant to the topic of event-based stereo depth estimation.",
      "processing_time": 34.46494245529175,
      "citing_paper_id": "3416874",
      "cited_paper_id": 396580
    },
    {
      "context_text": "In [15], the authors propose a novel context descriptor to perform matching, and the authors in [16] use a stereo event camera undergoing pure rotation to perform depth estimation and panoramic stitching.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and approaches. The context is about stereo depth estimation using event cameras, but no dataset names are provided.",
      "processing_time": 28.19579768180847,
      "citing_paper_id": "3416874",
      "cited_paper_id": 1408596
    },
    {
      "context_text": "Later works in [8], [9] and [10] have adapted cooperative methods for stereo depth to event based cameras, due to their applicability to asynchronous, point based measurements.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only cooperative methods adapted for stereo depth estimation with event-based cameras.",
      "processing_time": 24.932663202285767,
      "citing_paper_id": "3416874",
      "cited_paper_id": 7151414
    },
    {
      "context_text": "The authors in [17] and [18] proposed novel methods to perform feature tracking in the event space, which they extended in [19] and [20] to perform visual and visual inertial odometry, respectively.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods for feature tracking and odometry.",
      "processing_time": 24.106298208236694,
      "citing_paper_id": "3416874",
      "cited_paper_id": 7884141
    },
    {
      "context_text": "Early works in [6], [7] present stereo depth estimation results with a number of spatial and temporal costs.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only general methods and results. No clear, verifiable resource names are provided.",
      "processing_time": 25.642561674118042,
      "citing_paper_id": "3416874",
      "cited_paper_id": 11177597
    },
    {
      "context_text": "The transformation that takes a point from the lidar frame to the left DAVIS frame was initially calibrated using the Camera and Range Calibration Toolbox [33].",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions a toolbox, which is a method or tool, not a dataset. No specific dataset is mentioned.",
      "processing_time": 24.92837691307068,
      "citing_paper_id": "3416874",
      "cited_paper_id": 12339854
    },
    {
      "context_text": "lidar are calibrated using the Camera and Range Calibration Toolbox4 [33], and fine tuned manually, and the hand eye calibration between the mocap model pose in the motion capture",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only tools and methods for calibration. There are no clear identifiers for datasets in the provided context.",
      "processing_time": 27.306106090545654,
      "citing_paper_id": "3416874",
      "cited_paper_id": 12339854
    },
    {
      "context_text": "The camera intrinsics, stereo extrinsics, and camera-IMU extrinsics are calibrated using the Kalibr toolbox3 [30], [31], [32], the extrinsics between the left DAVIS camera and Velodyne lidar are calibrated using the Camera and Range Calibration Toolbox4 [33], and fine tuned manually, and the hand eye calibration between the mocap model pose in the motion capture world frame and the left DAVIS camera pose is performed using CamOdoCal5 [34].",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only tools and methods for calibration. No verifiable datasets are referenced.",
      "processing_time": 26.19560432434082,
      "citing_paper_id": "3416874",
      "cited_paper_id": 12339854
    },
    {
      "context_text": "The camera intrinsics, stereo extrinsics, and camera-IMU extrinsics are calibrated using the Kalibr toolbox3 [30], [31], [32], the extrinsics between the left DAVIS camera and Velodyne",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions the use of the Kalibr toolbox for calibration purposes, but does not refer to any specific dataset. The Kalibr toolbox is a method or tool, not a dataset.",
      "processing_time": 29.273466110229492,
      "citing_paper_id": "3416874",
      "cited_paper_id": 15778738
    },
    {
      "context_text": "The camera intrinsics, stereo extrinsics, and camera-IMU extrinsics are calibrated using the Kalibr toolbox3 [30], [31], [32], the extrinsics between the left DAVIS camera and Velodyne",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions the use of the Kalibr toolbox for calibration purposes, but does not refer to any specific dataset. The Kalibr toolbox is a method or tool, not a dataset.",
      "processing_time": 29.273466110229492,
      "citing_paper_id": "3416874",
      "cited_paper_id": 120110206
    },
    {
      "context_text": "Similarly, [11] and [12] apply a set of temporal, epipolar, ordering and polarity constraints to determine matches, while [13] compare this with matching based on the output of a bank of orientation filters.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and approaches. The context is focused on comparing different techniques for event-based stereo matching.",
      "processing_time": 27.535000324249268,
      "citing_paper_id": "3416874",
      "cited_paper_id": 17693733
    },
    {
      "context_text": "For each sequence with lidar measurements, we run the Lidar Odometry and Mapping (LOAM) algorithm [29] to generate dense 3D local maps, which are projected into each DAVIS camera to generate dense depth images at 20 Hz, and to provide 3D pose for the handheld sequences.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only the use of a method (LOAM) for generating dense 3D local maps and depth images. No verifiable dataset names are present.",
      "processing_time": 28.83028745651245,
      "citing_paper_id": "3416874",
      "cited_paper_id": 18612391
    },
    {
      "context_text": "The authors in [21] use events to estimate angular velocity of a camera, while [22] and [23] perform visual odometry by building an up to a scale map.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and approaches. The context focuses on the use of event cameras for various tasks, but no dataset names are provided.",
      "processing_time": 27.926195859909058,
      "citing_paper_id": "3416874",
      "cited_paper_id": 26324573
    },
    {
      "context_text": "• Images and IMU measurements from the VI Sensor.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions 'Images and IMU measurements from the VI Sensor' but does not specify a named dataset. The VI Sensor is likely a device or sensor setup rather than a dataset.",
      "processing_time": 29.268296718597412,
      "citing_paper_id": "3416874",
      "cited_paper_id": 109416659
    },
    {
      "context_text": "The timestamps for the VI Sensor messages in the dataset are then modified to compensate for this offset.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.3,
      "reasoning": "The context mentions a dataset but does not specify a name. The modification of timestamps suggests a specific dataset is being used, but without a clear identifier, it cannot be included.",
      "processing_time": 28.557331800460815,
      "citing_paper_id": "3416874",
      "cited_paper_id": 109416659
    },
    {
      "context_text": "In comparison, this dataset provides event streams from two synchronized and calibrated Dynamic Vision and Active Pixel Sensors (DAVISm346b), with long indoor and outdoor sequences in a variety of illuminations and speeds, along with accurate depth images and pose at up to 100Hz, generated from a lidar system rigidly mounted on top of the cameras, as in Fig 1, along with motion capture and GPS.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "DAVISm346b"
      ],
      "dataset_descriptions": {
        "DAVISm346b": "Used for event-based stereo depth estimation, providing synchronized event streams, depth images, and pose data from two calibrated DAVISm346b sensors, suitable for indoor and outdoor sequences."
      },
      "confidence_score": 0.9,
      "reasoning": "The context describes a specific dataset with detailed characteristics, including event streams from DAVIS sensors, depth images, and pose data. The dataset is used for event-based stereo depth estimation.",
      "processing_time": 37.28517413139343,
      "citing_paper_id": "3416874",
      "cited_paper_id": 109416659
    },
    {
      "context_text": "In addition, we have mounted a VI Sensor [27], originally",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a sensor system. No verifiable resources are identified.",
      "processing_time": 24.91574478149414,
      "citing_paper_id": "3416874",
      "cited_paper_id": 109416659
    },
    {
      "context_text": "In addition, we calibrate the temporal offset between the DAVIS stereo pair and the VI Sensor by finding the temporal offset that maximizes the cross correlation between the magnitude of the gyroscope angular velocities from the IMUs of the left DAVIS and the VI Sensor.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only sensors and their synchronization. No verifiable dataset names are present.",
      "processing_time": 25.202542304992676,
      "citing_paper_id": "3416874",
      "cited_paper_id": 109416659
    },
    {
      "context_text": "In this section, we describe the various steps performed to calibrate the intrinsic parameters of each DAVIS and VI-Sensor camera, as well as the extrinsic transformations between each of the cameras, IMUs and the lidar.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only hardware and calibration processes.",
      "processing_time": 23.739667415618896,
      "citing_paper_id": "3416874",
      "cited_paper_id": 109416659
    },
    {
      "context_text": "For high speed sequences, the DAVIS stereo rig and VI Sensor are mounted on the handlebar of a motorcycle (Fig 2d), along with the GPS device.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions specific hardware setups but does not refer to any named datasets. The citation is more about the setup and equipment used rather than a specific dataset.",
      "processing_time": 28.333731412887573,
      "citing_paper_id": "3416874",
      "cited_paper_id": 109416659
    },
    {
      "context_text": "In addition, we have mounted a VI Sensor [27], originally developed by Skybotix for comparison with frame based methods.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a sensor system. No verifiable resources are identified.",
      "processing_time": 24.908873796463013,
      "citing_paper_id": "3416874",
      "cited_paper_id": 109416659
    },
    {
      "context_text": "In addition, [24] and [25] also fuse events with measurements from an IMU to perform visual inertial odometry.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods or approaches. The context is about fusing events with IMU measurements for visual inertial odometry.",
      "processing_time": 27.697237253189087,
      "citing_paper_id": "3416874",
      "cited_paper_id": 204780933
    },
    {
      "context_text": "The work in [21] aims to reduce the disparity search space based on the prediction and use pixel-wise Kalman filtering to combine the predicted disparity and the newly matched one, whereas [22] extends the method by exploiting the difference between the predicted and matched disparity to detect moving objects.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and approaches. The context focuses on disparity estimation and object detection techniques.",
      "processing_time": 25.8337459564209,
      "citing_paper_id": "221543284",
      "cited_paper_id": 3892441
    },
    {
      "context_text": "To demonstrate the performance of our method in a real-world scenario, we used the well-known KITTI dataset [28].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "KITTI"
      ],
      "dataset_descriptions": {
        "KITTI": "Used to demonstrate the performance of the proposed method in a real-world scenario, focusing on event-based stereo depth estimation using the KITTI vision benchmark suite."
      },
      "confidence_score": 1.0,
      "reasoning": "The KITTI dataset is explicitly mentioned and is a well-known resource in the field of computer vision and robotics, particularly for autonomous driving scenarios.",
      "processing_time": 33.739935874938965,
      "citing_paper_id": "221543284",
      "cited_paper_id": 6724907
    },
    {
      "context_text": "Prior to synthesizing events, the standard frames of the KITTI dataset that were captured with the frequency of 10 Hz were upsampled with a frame interpolation package (also available in the said tool) based on [30] to achieve better results.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "KITTI"
      ],
      "dataset_descriptions": {
        "KITTI": "Used to provide standard frames for upsampling experiments, enhancing the quality of intermediate frames for video interpolation in the context of event-based stereo depth estimation."
      },
      "confidence_score": 1.0,
      "reasoning": "The KITTI dataset is mentioned as the source of standard frames used for upsampling. The dataset is relevant to the research on event-based stereo depth estimation.",
      "processing_time": 33.85655999183655,
      "citing_paper_id": "221543284",
      "cited_paper_id": 10817557
    },
    {
      "context_text": "…4 × 4 transformation matrix deﬁned as: where Γ is the projection matrix that transforms the coordinates ω between the disparity space and the coordinates M in the Euclidean space as follows: with f and b being the focal length of the cameras and the baseline of the stereo rig, respectively [24].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only mathematical concepts and transformations. No verifiable resources are identified.",
      "processing_time": 25.843810319900513,
      "citing_paper_id": "221543284",
      "cited_paper_id": 15002233
    },
    {
      "context_text": "We used our implementation of semiglobal matching (SGM) for a stereo setup presented in [22], which supports AVX2 instructions and multithreading to support real-time performance.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific, verifiable datasets. It focuses on the method (SGM) and its implementation details.",
      "processing_time": 25.835317134857178,
      "citing_paper_id": "221543284",
      "cited_paper_id": 18327083
    },
    {
      "context_text": ", SGM [18], and use events to track the disparity asynchronously between the frames, an illus-",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and approaches. The context is too limited to infer the use of any particular dataset.",
      "processing_time": 27.900887489318848,
      "citing_paper_id": "221543284",
      "cited_paper_id": 18327083
    },
    {
      "context_text": "We suggest to use standard frames to perform dense disparity estimation with any standard stereo matching method, e.g., SGM [18], and use events to track the disparity asynchronously between the frames, an illus-\ntration of which is shown in Figure 1.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and approaches. The context focuses on the use of standard frames and event-based tracking for disparity estimation.",
      "processing_time": 28.53899645805359,
      "citing_paper_id": "221543284",
      "cited_paper_id": 18327083
    },
    {
      "context_text": "Dense disparity was estimated using SGM, shown in (a) and (f).",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (SGM) for estimating dense disparity. No verifiable resources are identified.",
      "processing_time": 27.690627336502075,
      "citing_paper_id": "221543284",
      "cited_paper_id": 18327083
    },
    {
      "context_text": "SGM introduces parameters P1 and P2 as discontinuity penalties in order to obtain smooth disparity maps.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only parameters and a method for obtaining smooth disparity maps.",
      "processing_time": 25.603259801864624,
      "citing_paper_id": "221543284",
      "cited_paper_id": 18327083
    },
    {
      "context_text": "The method performs in real-time in a single thread, with the ability to run our implementation of SGM multithreaded, making it suitable for implementation in autonomous systems that operate in highly dynamic scenarios or have significant computational constraints.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only discusses the performance and implementation details of a method.",
      "processing_time": 25.82922649383545,
      "citing_paper_id": "221543284",
      "cited_paper_id": 18327083
    },
    {
      "context_text": "We have demonstrated the performance of the method through several experiments that cover different scenarios, including synthetic data from the event simulator, KITTI dataset enhanced with events, MVSEC dataset, and our own stereo DAVIS recordings.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "KITTI dataset",
        "MVSEC dataset",
        "stereo DAVIS recordings"
      ],
      "dataset_descriptions": {
        "KITTI dataset": "Enhanced with events, used to evaluate the method's performance in real-world driving scenarios, focusing on stereo depth estimation accuracy.",
        "MVSEC dataset": "Used to assess the method's performance in various dynamic environments, emphasizing robustness and accuracy in event-based stereo depth estimation.",
        "stereo DAVIS recordings": "Custom stereo DAVIS recordings used to test the method in controlled settings, focusing on latency and accuracy in event-based stereo depth estimation."
      },
      "confidence_score": 0.9,
      "reasoning": "The context mentions multiple datasets used for evaluating the performance of the method in different scenarios. These datasets are specific and relevant to the topic of event-based stereo depth estimation.",
      "processing_time": 44.65066576004028,
      "citing_paper_id": "221543284",
      "cited_paper_id": 24007071
    },
    {
      "context_text": "4.4 Stereo DAVIS346 Experiments\nExperimental data was collected with two DAVIS346 sensors mounted on a stereo rig with a baseline of 10 cm.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.2,
      "reasoning": "The context mentions experimental data collected using DAVIS346 sensors, but does not specify a named dataset. The description is too generic and lacks a specific, identifiable dataset name.",
      "processing_time": 29.86623740196228,
      "citing_paper_id": "221543284",
      "cited_paper_id": 24007071
    },
    {
      "context_text": "This dataset provides multiple stereo event sequences collected with two DAVIS346 cameras, including ground truth information about pose and depth obtained by lidar SLAM.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "DAVIS346"
      ],
      "dataset_descriptions": {
        "DAVIS346": "Used to collect stereo event sequences with ground truth pose and depth information for evaluating event-based stereo depth estimation algorithms."
      },
      "confidence_score": 0.9,
      "reasoning": "The context describes a specific dataset with stereo event sequences and ground truth information, which is highly relevant to event-based stereo depth estimation.",
      "processing_time": 33.151734590530396,
      "citing_paper_id": "221543284",
      "cited_paper_id": 24007071
    },
    {
      "context_text": "Event cameras, such as the dynamic and active-pixel vision sensor (DAVIS) [1], are biologically inspired sensors that operate fundamentally different than standard cameras.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a type of sensor (DAVIS). No verifiable datasets are referenced.",
      "processing_time": 27.262104272842407,
      "citing_paper_id": "221543284",
      "cited_paper_id": 24007071
    },
    {
      "context_text": "A similar method that rejects unreliable predictions in the areas where moving objects are detected was proposed in [20].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for rejecting unreliable predictions in dynamic scenes.",
      "processing_time": 24.88800573348999,
      "citing_paper_id": "221543284",
      "cited_paper_id": 38870956
    },
    {
      "context_text": "Experiments were conducted on the data obtained with ESIM: an Open Event Camera Simulator [27].",
      "catation_intent": "reusable resource",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions 'data obtained with ESIM', which is an open event camera simulator. However, ESIM is a tool, not a dataset. No specific dataset is mentioned.",
      "processing_time": 29.238866090774536,
      "citing_paper_id": "221543284",
      "cited_paper_id": 53107219
    },
    {
      "context_text": "In order to get smoother depth maps, global approaches incorporate smoothness constraints across neighboring points, typically demonstrated in scenes with static cameras and few moving objects [7, 8].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only general methods and approaches. No verifiable resources are identified.",
      "processing_time": 25.59760355949402,
      "citing_paper_id": "221543284",
      "cited_paper_id": 65040501
    },
    {
      "context_text": "A recent survey on event cameras oﬀers an exhausting overview of all state-of-the-art event-based applications and methods [2].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a survey of methods and applications.",
      "processing_time": 25.403273582458496,
      "citing_paper_id": "221543284",
      "cited_paper_id": 118684904
    },
    {
      "context_text": "The work in [14] attempts to recover semi-dense disparity by fusing information from multiple camera viewpoints, while [5] proposes to estimate dense disparity by performing interpolation in the areas uncovered by events.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and approaches for disparity estimation.",
      "processing_time": 25.40142059326172,
      "citing_paper_id": "221543284",
      "cited_paper_id": 157060825
    },
    {
      "context_text": "Some works proposed to estimate event lifetimes prior to event aggregation in order to yield more accurate results [5, 6].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods or approaches for estimating event lifetimes.",
      "processing_time": 25.82194709777832,
      "citing_paper_id": "221543284",
      "cited_paper_id": 157060825
    },
    {
      "context_text": "However, we generated synthetic event data using the open-source Video to Events tool provided by [29].",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions generating synthetic event data using a tool, but does not reference a specific dataset. The cited paper title confirms the tool's purpose but does not introduce a dataset.",
      "processing_time": 28.519176721572876,
      "citing_paper_id": "221543284",
      "cited_paper_id": 214743146
    },
    {
      "context_text": "We trained and tested our network on the Multi Vehicle Stereo Event Camera (MVSEC) dataset (Zhu et al. 2018b).",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Multi Vehicle Stereo Event Camera (MVSEC)"
      ],
      "dataset_descriptions": {
        "Multi Vehicle Stereo Event Camera (MVSEC)": "Used to train and test a network for event-based stereo depth estimation, focusing on 3D perception tasks with event camera data."
      },
      "confidence_score": 1.0,
      "reasoning": "The MVSEC dataset is explicitly mentioned and used for training and testing a network for event-based stereo depth estimation.",
      "processing_time": 34.118409633636475,
      "citing_paper_id": "238198645",
      "cited_paper_id": 3416874
    },
    {
      "context_text": "Implemented on dedicated chips such as Intel Loihi [5], IBM TrueNorth [2], Brainchip Akida [40] or Tianjic [34], these models could become a new paradigm for ultra-low power computation in the coming years.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only hardware platforms. No dataset names are present in the text.",
      "processing_time": 26.690977334976196,
      "citing_paper_id": "238198645",
      "cited_paper_id": 3608458
    },
    {
      "context_text": "Implemented on dedicated chips such as Intel Loihi [5], IBM TrueNorth [2], Brainchip Akida [40] or Tianjic [34], these models could become a new paradigm for ultra-low power computation in the coming years.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only hardware platforms. No dataset names are present in the text.",
      "processing_time": 26.690977334976196,
      "citing_paper_id": "238198645",
      "cited_paper_id": 207935891
    },
    {
      "context_text": "As a result, it is essentially implementable on dedicated neuromorphic hardware, such as Intel Loihi [7], IBM TrueNorth [8].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only hardware platforms. There are no verifiable resources that meet the criteria for inclusion.",
      "processing_time": 26.393192529678345,
      "citing_paper_id": "238198645",
      "cited_paper_id": 3608458
    },
    {
      "context_text": "Implemented on dedicated chips such as Intel Loihi [7], IBM TrueNorth [8], Brainchip Akida [9] or",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only hardware platforms. There are no verifiable resources that meet the criteria.",
      "processing_time": 26.149646520614624,
      "citing_paper_id": "238198645",
      "cited_paper_id": 3608458
    },
    {
      "context_text": "As a result, it is essentially implementable on dedicated neuromorphic hardware, such as Intel Loihi [5], IBM TrueNorth [2].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions hardware platforms but does not refer to any specific datasets, models, or methods.",
      "processing_time": 24.576013565063477,
      "citing_paper_id": "238198645",
      "cited_paper_id": 3608458
    },
    {
      "context_text": "The model in [7] was the ﬁrst successful multi-scale architecture designed for depth estimation from RGB images, and was consequently followed by advances based on similar approaches [24] [23] [14].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only models and methods for depth estimation from RGB images.",
      "processing_time": 26.38981342315674,
      "citing_paper_id": "238198645",
      "cited_paper_id": 4572038
    },
    {
      "context_text": "The model in [7] was the ﬁrst successful multi-scale architecture designed for depth estimation from RGB images, and was consequently followed by advances based on similar approaches [24] [23] [14].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only models and methods for depth estimation from RGB images.",
      "processing_time": 26.38981342315674,
      "citing_paper_id": "238198645",
      "cited_paper_id": 102496818
    },
    {
      "context_text": "Depth is an important feature of the surrounding space whose estimation finds its place in various tasks across many different fields [1].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general statement about depth estimation. No verifiable resources are identified.",
      "processing_time": 25.811540365219116,
      "citing_paper_id": "238198645",
      "cited_paper_id": 4694685
    },
    {
      "context_text": "Such neurons are inexpensive to simulate and can be deployed in large models, whereas more complex ones such as Hodgkin-Huxley [16], Izhikevich [18] or even SRM [13] are still too computationally expensive to be trained on modern hardware.",
      "catation_intent": "none",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only neuron models. No datasets are referenced or used in the context provided.",
      "processing_time": 26.942056894302368,
      "citing_paper_id": "238198645",
      "cited_paper_id": 6195748
    },
    {
      "context_text": "7 TSES* [38] 36 44 36 CopNet* [31] 61 100 64",
      "catation_intent": "findings",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not contain any specific dataset names or verifiable resources. It appears to be a table of results comparing different methods.",
      "processing_time": 26.940443992614746,
      "citing_paper_id": "238198645",
      "cited_paper_id": 7151414
    },
    {
      "context_text": "StereoSpike also outperforms TSES [38] and CopNet [31], which only predict depth at event positions.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only comparisons with other methods. No verifiable resources are identified.",
      "processing_time": 25.577611446380615,
      "citing_paper_id": "238198645",
      "cited_paper_id": 7151414
    },
    {
      "context_text": "DTC-SPADE [44] is not represented here as the authors did not use the same colormap as other studies and did not publish their code.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (DTC-SPADE) and a reason why it was not included in the study.",
      "processing_time": 28.937201023101807,
      "citing_paper_id": "238198645",
      "cited_paper_id": 81981856
    },
    {
      "context_text": "The model in [22] used the same input embedding and backbone as [21], but proposed a preliminary network using spatially-adaptive normalization (SPADE) [23] to reconstruct grayscale intensity images jointly with depth maps.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models and methods. The context focuses on the use of SPADE for image synthesis and depth map reconstruction.",
      "processing_time": 28.93041157722473,
      "citing_paper_id": "238198645",
      "cited_paper_id": 81981856
    },
    {
      "context_text": "DTC-SPADE [44] has managed to incorporate elements of the latter for the beneﬁt of accuracy and without exploding algorithmic size.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (DTC-SPADE) and a reference to another paper. No verifiable resources are identified.",
      "processing_time": 29.44647240638733,
      "citing_paper_id": "238198645",
      "cited_paper_id": 81981856
    },
    {
      "context_text": "[44] also used a similar matching backbone as well as SPADE, but differed in its input encoding.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (SPADE) and a general reference to input encoding. No verifiable resources are identified.",
      "processing_time": 28.932634115219116,
      "citing_paper_id": "238198645",
      "cited_paper_id": 81981856
    },
    {
      "context_text": "The model in [1] used the same input embedding and backbone as [39], but proposed a preliminary network using spatially-adaptive normalization (SPADE) [33] to reconstruct grayscale intensity images jointly with depth maps.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models and methods. The context focuses on the use of SPADE for image synthesis and depth map reconstruction.",
      "processing_time": 28.93392586708069,
      "citing_paper_id": "238198645",
      "cited_paper_id": 81981856
    },
    {
      "context_text": "From a visual neuroscience perspective, there is strong evidence that binocular cues are mixed in a similar manner at very early stages of the visual system [41].",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a reference to a finding about binocular cues in the visual system.",
      "processing_time": 27.469687700271606,
      "citing_paper_id": "238198645",
      "cited_paper_id": 86423050
    },
    {
      "context_text": "Dynamic Vision Sensors (DVS) have recently gathered the interest of scientists and industrial actors, thanks to a growing number of research papers explaining how to process their output [11].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general interest in Dynamic Vision Sensors (DVS).",
      "processing_time": 26.93636727333069,
      "citing_paper_id": "238198645",
      "cited_paper_id": 118684904
    },
    {
      "context_text": "We therefore consider that Brainchip’s Akida chip [35] is a good fit.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a hardware solution. There are no verifiable resources that meet the criteria.",
      "processing_time": 26.664203882217407,
      "citing_paper_id": "238198645",
      "cited_paper_id": 207935891
    },
    {
      "context_text": "Implemented on dedicated chips such as Intel Loihi [3], IBM TrueNorth [1], Brainchip Akida [35] or Tianjic [29], these models could become a new paradigm for ultra-low power computation in the coming years.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only hardware platforms. There are no verifiable resources that meet the criteria.",
      "processing_time": 26.371185302734375,
      "citing_paper_id": "238198645",
      "cited_paper_id": 207935891
    },
    {
      "context_text": "A notable exception is (Gehrig et al. 2020), but they only regressed 3 variables, while we propose here to regress the values of 260× 346 = 89960 pixels.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a comparison of regression variable counts.",
      "processing_time": 25.564239740371704,
      "citing_paper_id": "238198645",
      "cited_paper_id": 212414806
    },
    {
      "context_text": "We then investigated outdoor scenarios by following the same training, test and validation splits as [19].",
      "catation_intent": "reusable resource",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.3,
      "reasoning": "The citation does not mention any specific dataset names, only referring to splits used in another paper. No clear, verifiable dataset is identified.",
      "processing_time": 27.644097805023193,
      "citing_paper_id": "238198645",
      "cited_paper_id": 223957202
    },
    {
      "context_text": "In outdoor scenarios, our model outperforms E2Depth [19] by a large margin at all cutoff distances, and with 25× fewer parameters.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a comparison with another model (E2Depth).",
      "processing_time": 25.56175470352173,
      "citing_paper_id": "238198645",
      "cited_paper_id": 223957202
    },
    {
      "context_text": "divide by the maximum number of spike count) for an easier-to-learn distribution of data and better generalization [19].",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.2,
      "reasoning": "The citation does not mention any specific dataset names, only a method for data distribution. The context is too vague to identify a specific dataset.",
      "processing_time": 28.108787059783936,
      "citing_paper_id": "238198645",
      "cited_paper_id": 223957202
    },
    {
      "context_text": "However, we argue that the task of depth recovery from events has a minor temporal component and can be solved by a fully feedforward model with minimal temporal knowledge; therefore the use of convLSTMs in [19] is suboptimal and unnecessarily costly in terms of computation.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (convLSTMs) and a critique of its use. No verifiable resources are identified.",
      "processing_time": 28.26626968383789,
      "citing_paper_id": "238198645",
      "cited_paper_id": 223957202
    },
    {
      "context_text": "Finally, in [19], dense metric depth was recovered from only one camera, and showed good performances with a recurrent, monocular encoder-decoder architecture on outdoor sequences.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for recovering dense metric depth using a monocular encoder-decoder architecture.",
      "processing_time": 26.925628185272217,
      "citing_paper_id": "238198645",
      "cited_paper_id": 223957202
    },
    {
      "context_text": "As a counterbalance measure, we used data augmentation; as [19].",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific dataset names, only a general reference to data augmentation. No multi-word proper nouns, acronyms, or hyphenated names with digits are present.",
      "processing_time": 29.42843747138977,
      "citing_paper_id": "238198645",
      "cited_paper_id": 223957202
    },
    {
      "context_text": "As in [19], we used a combination of a regression loss with a regularization loss.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a combination of losses used in training. No verifiable resources are identified.",
      "processing_time": 27.847359657287598,
      "citing_paper_id": "238198645",
      "cited_paper_id": 223957202
    },
    {
      "context_text": "Please note that [19] also used simulated data that did not come from MVSEC, but this does not prevent StereoSpike from outperforming this competitor by a large margin with substantially less parameters (cf.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions 'simulated data' but does not specify a dataset name. MVSEC is mentioned but not as a source of the simulated data. No other specific datasets are named.",
      "processing_time": 29.81844425201416,
      "citing_paper_id": "238198645",
      "cited_paper_id": 223957202
    },
    {
      "context_text": "According to [19], the minimization of this term encourages smooth depth changes as well as sharp depth discontinuities in the depth map prediction, hence helping the network to represent objects that stand out of the background of the scene (e.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a methodological approach to depth estimation.",
      "processing_time": 25.129034757614136,
      "citing_paper_id": "238198645",
      "cited_paper_id": 223957202
    },
    {
      "context_text": "This is not due to the fact that [19] takes input from only one camera as the monocular version of StereoSpike still remains consistently superior to this competitor.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a comparison between methods. No dataset names are present in the citation span.",
      "processing_time": 26.918392181396484,
      "citing_paper_id": "238198645",
      "cited_paper_id": 223957202
    },
    {
      "context_text": "While SNNs generally remain less accurate than their analog counterpart (i.e., Analog Neural Networks or ANNs), the gap in accuracy is decreasing, even on challenging problems like ImageNet [9].",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions 'ImageNet' but does not indicate that it is used as a dataset in the research. It is referenced to describe the difficulty of a problem, not as a resource used.",
      "processing_time": 30.57748556137085,
      "citing_paper_id": "238198645",
      "cited_paper_id": 226976144
    },
    {
      "context_text": "While SNNs generally remain less accurate than their analog counterpart (i.e., Analog Neural Networks or ANNs), the gap in accuracy is decreasing, even on challenging problems like ImageNet [9].",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions 'ImageNet' but does not indicate that it is used as a dataset in the research. It is referenced to describe the difficulty of a problem, not as a resource used.",
      "processing_time": 30.57748556137085,
      "citing_paper_id": "238198645",
      "cited_paper_id": 235359262
    },
    {
      "context_text": "While SNNs generally remain less accurate than their analog counterpart (i.e., Analog Neural Networks or ANNs), the gap in accuracy is decreasing, even on challenging problems like ImageNet [9].",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions 'ImageNet' but does not indicate that it is used as a dataset in the research. It is referenced to describe the difficulty of a problem, not as a resource used.",
      "processing_time": 30.57748556137085,
      "citing_paper_id": "238198645",
      "cited_paper_id": 241440878
    },
    {
      "context_text": "So far, SNNs have been used for classification tasks like image recognition [8], object detection [18], or motion segmentation [27].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only general tasks such as image recognition, object detection, and motion segmentation.",
      "processing_time": 27.628888607025146,
      "citing_paper_id": "238198645",
      "cited_paper_id": 226976144
    },
    {
      "context_text": "We use the derivative of the arctan function as our surrogate gradient in this paper, as suggested in [8].",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for using a surrogate gradient.",
      "processing_time": 24.835646867752075,
      "citing_paper_id": "238198645",
      "cited_paper_id": 226976144
    },
    {
      "context_text": ", Analog Neural Networks or ANNs), the gap in accuracy is decreasing, even on challenging problems like ImageNet [8].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific dataset used for research. It only references a benchmark (ImageNet) which is excluded as per instructions.",
      "processing_time": 28.25243043899536,
      "citing_paper_id": "238198645",
      "cited_paper_id": 226976144
    },
    {
      "context_text": "The bottleneck consisted in 2 SEWResBlocks [9] following each other and with ADD connect function.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (SEWResBlocks) used in the architecture.",
      "processing_time": 27.20512890815735,
      "citing_paper_id": "238198645",
      "cited_paper_id": 235359262
    },
    {
      "context_text": "So far, SNNs have been used for classification tasks like image recognition [9,10], object detection [3,20], or motion segmentation [31].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only general tasks such as image recognition, object detection, and motion segmentation.",
      "processing_time": 27.619073629379272,
      "citing_paper_id": "238198645",
      "cited_paper_id": 235359262
    },
    {
      "context_text": ", Analog Neural Networks or ANNs), the gap in accuracy is decreasing, even on challenging problems like ImageNet [9].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific dataset names, only a reference to 'challenging problems like ImageNet'. However, ImageNet is not used in the specific research context described.",
      "processing_time": 30.35802173614502,
      "citing_paper_id": "238198645",
      "cited_paper_id": 235359262
    },
    {
      "context_text": "So far, SNNs have been used for classiﬁcation tasks like image recognition [9,10], object detection [3,20], or motion segmentation [31].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general tasks and applications of SNNs. No verifiable resources are identified.",
      "processing_time": 28.468005180358887,
      "citing_paper_id": "238198645",
      "cited_paper_id": 241440878
    },
    {
      "context_text": "For deep learning-based approaches [40]–[42], the left and right images are used as feature maps in the network to construct the cost volume.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for constructing a cost volume using left and right images in a stereo matching network.",
      "processing_time": 28.46118927001953,
      "citing_paper_id": "271855627",
      "cited_paper_id": 4252896
    },
    {
      "context_text": "This method is inspired by [38], but differs in the definition of t max , which represents the maximum timestamp in the event stream.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method and a timestamp definition. No verifiable resources are identified.",
      "processing_time": 27.61779022216797,
      "citing_paper_id": "271855627",
      "cited_paper_id": 13373696
    },
    {
      "context_text": "Unlike global attention in [44], deformable attention only samples K points around the reference point to calculate the attention result.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (deformable attention) compared to another method (global attention).",
      "processing_time": 28.462905645370483,
      "citing_paper_id": "271855627",
      "cited_paper_id": 13756489
    },
    {
      "context_text": "It possesses extensive applications in the tasks of robotics [13], autonomous driving [14], [15] and virtual augmented reality (VR/AR) [16], offering a broad range of possibilities.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general applications of stereo vision in various fields. No verifiable resources are identified.",
      "processing_time": 28.24549627304077,
      "citing_paper_id": "271855627",
      "cited_paper_id": 14193490
    },
    {
      "context_text": "It possesses extensive applications in the tasks of robotics [13], autonomous driving [14], [15] and virtual augmented reality (VR/AR) [16], offering a broad range of possibilities.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general applications of stereo vision in various fields. No verifiable resources are identified.",
      "processing_time": 28.24549627304077,
      "citing_paper_id": "271855627",
      "cited_paper_id": 54115081
    },
    {
      "context_text": "The underlying principle of stereo disparity estimation relies on the assumption that the left and right images demonstrate left-right consistency [37].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general principle of stereo disparity estimation.",
      "processing_time": 26.332660913467407,
      "citing_paper_id": "271855627",
      "cited_paper_id": 20954901
    },
    {
      "context_text": "In stereo disparity estimation, accurate disparity estimation relies on input images with clear object boundaries [35]– [37].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only general requirements for stereo disparity estimation.",
      "processing_time": 25.339139699935913,
      "citing_paper_id": "271855627",
      "cited_paper_id": 20954901
    },
    {
      "context_text": "Finally, we utilize the winner-take-all (WTA) [12] and refinement module [32] to estimate disparity.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only methods and modules. No dataset names are present in the text.",
      "processing_time": 27.818621158599854,
      "citing_paper_id": "271855627",
      "cited_paper_id": 102352684
    },
    {
      "context_text": "Finally, we utilize the winner-take-all (WTA) [12] and refinement module [32] to estimate disparity.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only methods and modules. No dataset names are present in the text.",
      "processing_time": 27.818621158599854,
      "citing_paper_id": "271855627",
      "cited_paper_id": 195859047
    },
    {
      "context_text": "Finally, we utilize a refinement module [32] to recover the multi-scale predicted disparity maps scale.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a refinement module for stereo depth estimation.",
      "processing_time": 26.32792568206787,
      "citing_paper_id": "271855627",
      "cited_paper_id": 102352684
    },
    {
      "context_text": "E VENT cameras [2]–[7], inspired by the neural structure of the human eye in response to the changes of illumination, are a type of neuromorphic vision sensor [8].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only a type of camera technology. There are no clear identifiers for datasets.",
      "processing_time": 28.237611770629883,
      "citing_paper_id": "271855627",
      "cited_paper_id": 118684904
    },
    {
      "context_text": "Stereo depth estimation [9], [10] stands as a fundamental task in computer vision [11] and robotics [12].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general tasks in computer vision and robotics. No verifiable resources are identified.",
      "processing_time": 28.2367422580719,
      "citing_paper_id": "271855627",
      "cited_paper_id": 195859047
    },
    {
      "context_text": "Stereo depth estimation [9], [10] stands as a fundamental task in computer vision [11] and robotics [12].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general tasks in computer vision and robotics. No verifiable resources are identified.",
      "processing_time": 28.2367422580719,
      "citing_paper_id": "271855627",
      "cited_paper_id": 246834559
    },
    {
      "context_text": "2) Total Loss Function: We utilize both the smooth L 1 loss function [46] and the left-right consistency census loss to construct the total loss function.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only loss functions. The context is about constructing a total loss function using smooth L1 loss and left-right consistency census loss.",
      "processing_time": 30.030345916748047,
      "citing_paper_id": "271855627",
      "cited_paper_id": 206770307
    },
    {
      "context_text": "In Table IV, we employ two fundamental metrics, namely Peak Signal-to-Noise Ratio (PSNR) and Structural Similarity (SSIM) [47], to evaluate the efficiency of the different configurations which sequentially remove the left-right consistency census loss and EAA modules from EV-MGDispNet.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only evaluation metrics (PSNR and SSIM).",
      "processing_time": 26.890622854232788,
      "citing_paper_id": "271855627",
      "cited_paper_id": 207761262
    },
    {
      "context_text": "Then, we input the X into the deformable encoder, which is applied as follows: where f deform-attn ( · ) denotes deformable attention [43], f LN ( · ) denotes layernorm layer and f FFN ( · ) denotes feed-forward network consisting of two MLP layers.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only components of a method (deformable attention, layernorm, feed-forward network).",
      "processing_time": 28.88483476638794,
      "citing_paper_id": "271855627",
      "cited_paper_id": 222208633
    },
    {
      "context_text": "[19] reconstructs event features as images and combines them with frame image features using the conditional normalization method.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for combining event features with frame image features.",
      "processing_time": 27.18662929534912,
      "citing_paper_id": "271855627",
      "cited_paper_id": 235306612
    },
    {
      "context_text": "[17], [19], [22] transforms events into an event queue, initially proposed by Tulyakov et al. [17] , for stereo disparity estimation.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for transforming events into an event queue for stereo disparity estimation.",
      "processing_time": 27.812170267105103,
      "citing_paper_id": "271855627",
      "cited_paper_id": 235306612
    },
    {
      "context_text": "Hand-crafted event representations can preserve a greater amount of raw event data and exhibit more stable outcomes compared to reconstructed images [17], [19].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only discusses methods and outcomes.",
      "processing_time": 25.746996641159058,
      "citing_paper_id": "271855627",
      "cited_paper_id": 235306612
    },
    {
      "context_text": "Previous works [19], [21] introduce conditional normalization to address this issue by adjusting the distribution of feature maps to better align with the edges of scenes.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (conditional normalization).",
      "processing_time": 26.080561637878418,
      "citing_paper_id": "271855627",
      "cited_paper_id": 235306612
    },
    {
      "context_text": "A naive approach to represent event streams is to reconstruct the events into images [18], [19], and yet the quality of reconstructed images significantly impacts the accuracy of stereo disparity estimation.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only a general approach to representing event streams. No clear, verifiable datasets are identified.",
      "processing_time": 29.154775381088257,
      "citing_paper_id": "271855627",
      "cited_paper_id": 235306612
    },
    {
      "context_text": "A naive approach to represent event streams is to reconstruct the events into images [18], [19], and yet the quality of reconstructed images significantly impacts the accuracy of stereo disparity estimation.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only a general approach to representing event streams. No clear, verifiable datasets are identified.",
      "processing_time": 29.154775381088257,
      "citing_paper_id": "271855627",
      "cited_paper_id": 244306440
    },
    {
      "context_text": "Previous works [18], [20] generate event representation through neural networks, but the generated event representation suffers from pixel shifts.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a methodological issue with event representation generation.",
      "processing_time": 27.179298639297485,
      "citing_paper_id": "271855627",
      "cited_paper_id": 244306440
    },
    {
      "context_text": "[18] proposes the earliest deep learning-based method that fuses event and frame image data for stereo disparity estimation.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for stereo disparity estimation using event and frame image data.",
      "processing_time": 28.060483932495117,
      "citing_paper_id": "271855627",
      "cited_paper_id": 244306440
    },
    {
      "context_text": "3) Evaluation Metrics: we reference prior works [17], [18], [20], [21] and utilize evaluation metrics such as Root Mean Squared Error (RMSE), Mean Absolute Error (MAE), 1-Pixel Error (1PE) and 2-Pixel Error (2PE).",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only evaluation metrics. No dataset names are present in the text.",
      "processing_time": 27.804235219955444,
      "citing_paper_id": "271855627",
      "cited_paper_id": 244306440
    },
    {
      "context_text": "Learning-based event representation methods [18], [20], [21] can overcome the aforementioned limitations, yielding more compact representations.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods for event representation. There are no clear identifiers for datasets in the provided context.",
      "processing_time": 28.864408493041992,
      "citing_paper_id": "271855627",
      "cited_paper_id": 244306440
    },
    {
      "context_text": "We believe that the improved accuracy [17], [18], [20], [21] ON THE DSEC [1] of fine structures may be attributed to the integration of motion information in the EAA module, which allows the generated aggregated edge-modulated event frame to possess more accurate edges and contours.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "DSEC"
      ],
      "dataset_descriptions": {
        "DSEC": "Used to evaluate the accuracy of fine structures in event-based stereo depth estimation, focusing on the integration of motion information in the EAA module to improve edge and contour accuracy."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions 'DSEC' as a dataset used to evaluate the accuracy of fine structures in the research. The dataset is relevant to the topic of event-based stereo depth estimation.",
      "processing_time": 36.529468297958374,
      "citing_paper_id": "271855627",
      "cited_paper_id": 244306440
    },
    {
      "context_text": "1) Quantitative Analysis: We compare our EV-MGDispNet with state-of-the-art algorithms (DDES [17], E-Stereo [18], Concentration Net [20], DTC-PDS [21]) on 1PE, 2PE, MAE and RMSE metrics using the DSEC disparity benchmark dataset.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "DSEC disparity benchmark"
      ],
      "dataset_descriptions": {
        "DSEC disparity benchmark": "Used to compare performance metrics (1PE, 2PE, MAE, RMSE) of event-based stereo depth estimation algorithms, specifically evaluating the proposed EV-MGDispNet against state-of-the-art methods."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions the use of the DSEC disparity benchmark dataset for comparing performance metrics of different algorithms. This is a specific, verifiable dataset used in the research.",
      "processing_time": 38.84217643737793,
      "citing_paper_id": "271855627",
      "cited_paper_id": 244306440
    },
    {
      "context_text": "The existing works [18], [20], [21] mainly rely on components from the field of image-based stereo disparity estimation to perform basic functionalities such as cost volume computation, volume aggregation and disparity estimation.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to methods and approaches from the field of image-based stereo disparity estimation.",
      "processing_time": 28.70198392868042,
      "citing_paper_id": "271855627",
      "cited_paper_id": 244306440
    },
    {
      "context_text": "Mostafavi et al. [18] proposes a method that combines event stream within a time window with images using image reconstruction to generate reconstructed images.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method combining event streams and images for depth estimation.",
      "processing_time": 26.87109398841858,
      "citing_paper_id": "271855627",
      "cited_paper_id": 244306440
    },
    {
      "context_text": "They improve the disparity estimation performance using knowledge distillation [27]–[30] by incorporating future event information to reinforce the disparity estimation results based on only past event information.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for improving disparity estimation using knowledge distillation.",
      "processing_time": 27.583125114440918,
      "citing_paper_id": "271855627",
      "cited_paper_id": 247593948
    },
    {
      "context_text": "The event camera simulates the functionality of the transient visual pathway of the biological retina, where each pixel independently and asynchronously responds to relative brightness changes with a time resolution at the microsecond-level [33], [34].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only describes the functionality of event cameras.",
      "processing_time": 25.731638431549072,
      "citing_paper_id": "271855627",
      "cited_paper_id": null
    },
    {
      "context_text": "Similar to the classification employed for frame-based systems [9], they can be either feature-based (indirect) [18,33] or direct [8,15,37].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only methods and approaches. The cited papers' titles do not provide additional information about datasets.",
      "processing_time": 29.37479519844055,
      "citing_paper_id": "271974279",
      "cited_paper_id": 3299195
    },
    {
      "context_text": "Similar to the classification employed for frame-based systems [9], they can be either feature-based (indirect) [18,33] or direct [8,15,37].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only methods and approaches. The cited papers' titles do not provide additional information about datasets.",
      "processing_time": 29.37479519844055,
      "citing_paper_id": "271974279",
      "cited_paper_id": 250918780
    },
    {
      "context_text": "Similar to the classification employed for frame-based systems [9], they can be either feature-based (indirect) [18,33] or direct [8,15,37].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only methods and approaches. The cited papers' titles do not provide additional information about datasets.",
      "processing_time": 29.37479519844055,
      "citing_paper_id": "271974279",
      "cited_paper_id": 260164484
    },
    {
      "context_text": "2) via ROS [27].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific dataset, only a reference to ROS, which is a software framework. No dataset names or specific resources are identified.",
      "processing_time": 29.58430027961731,
      "citing_paper_id": "271974279",
      "cited_paper_id": 6324125
    },
    {
      "context_text": "Adding more than two cameras to a sensor setup can improve 3D reconstruction by better handling occlusions and removing ambiguities [5].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general improvement in 3D reconstruction with additional cameras.",
      "processing_time": 27.163305282592773,
      "citing_paper_id": "271974279",
      "cited_paper_id": 10712214
    },
    {
      "context_text": "Events are back-projected through 3D space at their corresponding camera location by shooting fictitious rays through 3D space following a space-sweep approach [7].",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for back-projecting events through 3D space.",
      "processing_time": 27.393951416015625,
      "citing_paper_id": "271974279",
      "cited_paper_id": 11008141
    },
    {
      "context_text": "Since we aim to demonstrate the effectiveness of the fusion-based mapping module for visual odometry, we use an event-based camera tracker that works on the produced semi-dense 3D maps, by leveraging the idea of edge-map alignment [29].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for event-based tracking and mapping. No dataset names are provided in the context.",
      "processing_time": 29.58043098449707,
      "citing_paper_id": "271974279",
      "cited_paper_id": 16588072
    },
    {
      "context_text": "Progress in event-based VO/SLAM has been witnessed in monocular [20, 22, 29], and stereo setups [8, 15, 18, 33, 37, 39], possibly aided by other sensors (e.g., depth and/or IMU sensor fusion) [19,30,42].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to progress in event-based VO/SLAM systems. No dataset names are explicitly mentioned.",
      "processing_time": 30.000171899795532,
      "citing_paper_id": "271974279",
      "cited_paper_id": 16588072
    },
    {
      "context_text": "Progress in event-based VO/SLAM has been witnessed in monocular [20, 22, 29], and stereo setups [8, 15, 18, 33, 37, 39], possibly aided by other sensors (e.g., depth and/or IMU sensor fusion) [19,30,42].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to progress in event-based VO/SLAM systems. No dataset names are explicitly mentioned.",
      "processing_time": 30.000171899795532,
      "citing_paper_id": "271974279",
      "cited_paper_id": 250918780
    },
    {
      "context_text": "Progress in event-based VO/SLAM has been witnessed in monocular [20, 22, 29], and stereo setups [8, 15, 18, 33, 37, 39], possibly aided by other sensors (e.g., depth and/or IMU sensor fusion) [19,30,42].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to progress in event-based VO/SLAM systems. No dataset names are explicitly mentioned.",
      "processing_time": 30.000171899795532,
      "citing_paper_id": "271974279",
      "cited_paper_id": 260164484
    },
    {
      "context_text": "Progress in event-based VO/SLAM has been witnessed in monocular [20, 22, 29], and stereo setups [8, 15, 18, 33, 37, 39], possibly aided by other sensors (e.g., depth and/or IMU sensor fusion) [19,30,42].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to progress in event-based VO/SLAM systems. No dataset names are explicitly mentioned.",
      "processing_time": 30.000171899795532,
      "citing_paper_id": "271974279",
      "cited_paper_id": 266335686
    },
    {
      "context_text": "In this work, we compare the camera tracking performance of our system with the state-of-the-art direct event-only stereo visual odometry method ESVO [37] and with monocular system EVO [29].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods (ESVO and EVO) for comparison. No verifiable datasets are referenced.",
      "processing_time": 29.575103044509888,
      "citing_paper_id": "271974279",
      "cited_paper_id": 16588072
    },
    {
      "context_text": "All the values for EVO [29] are taken from [22].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific dataset names, only a reference to values from another paper. No clear dataset usage is described.",
      "processing_time": 28.850885152816772,
      "citing_paper_id": "271974279",
      "cited_paper_id": 16588072
    },
    {
      "context_text": "All the values for EVO [29] are taken from [22].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific dataset names, only a reference to values from another paper. No clear dataset usage is described.",
      "processing_time": 28.850885152816772,
      "citing_paper_id": "271974279",
      "cited_paper_id": 266335686
    },
    {
      "context_text": "We combine this method with a simple, fast and effective event camera tracker grounded on the idea of (semi-dense) edge-map alignment [29].",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for event camera tracking. No dataset names are present in the citation context.",
      "processing_time": 29.129416704177856,
      "citing_paper_id": "271974279",
      "cited_paper_id": 16588072
    },
    {
      "context_text": "One possibility is, as in [29], initialization by classical epipolar-geometry methods on event images.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for initialization using classical epipolar-geometry methods on event images.",
      "processing_time": 28.84760594367981,
      "citing_paper_id": "271974279",
      "cited_paper_id": 16588072
    },
    {
      "context_text": "The tracking module is derived from EVO [29], which is grounded on the idea of global image alignment [1].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions EVO but does not refer to it as a dataset. It is described as a method or tool for tracking and mapping.",
      "processing_time": 29.74908423423767,
      "citing_paper_id": "271974279",
      "cited_paper_id": 16588072
    },
    {
      "context_text": "Whereas the map in EVO [29] is given by EMVS [28], our depth estimates come from a mapping module which extends [28] to multi-camera data fusion via the harmonic mean.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and approaches. The context focuses on the methodology and extensions to multi-camera data fusion.",
      "processing_time": 29.74981141090393,
      "citing_paper_id": "271974279",
      "cited_paper_id": 16588072
    },
    {
      "context_text": "We evaluate the performance of our stereo VO pipeline on sequences from five publicly available datasets [3,14,21,36,40] with varying camera resolutions depicting a wide range of scenarios on different mobile platforms.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "DSEC",
        "EVIMO2"
      ],
      "dataset_descriptions": {
        "DSEC": "Used to evaluate stereo event camera performance in driving scenarios, focusing on 3D reconstruction and visual odometry.",
        "EVIMO2": "Used to assess motion segmentation, optical flow, structure from motion, and visual inertial odometry in indoor scenes with monocular event cameras."
      },
      "confidence_score": 0.8,
      "reasoning": "The context mentions 'five publicly available datasets' but does not specify their names. However, the cited paper titles suggest specific datasets that could be relevant.",
      "processing_time": 41.4911892414093,
      "citing_paper_id": "271974279",
      "cited_paper_id": 49877954
    },
    {
      "context_text": "We evaluate the performance of our stereo VO pipeline on sequences from five publicly available datasets [3,14,21,36,40] with varying camera resolutions depicting a wide range of scenarios on different mobile platforms.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "DSEC",
        "EVIMO2"
      ],
      "dataset_descriptions": {
        "DSEC": "Used to evaluate stereo event camera performance in driving scenarios, focusing on 3D reconstruction and visual odometry.",
        "EVIMO2": "Used to assess motion segmentation, optical flow, structure from motion, and visual inertial odometry in indoor scenes with monocular event cameras."
      },
      "confidence_score": 0.8,
      "reasoning": "The context mentions 'five publicly available datasets' but does not specify their names. However, the cited paper titles suggest specific datasets that could be relevant.",
      "processing_time": 41.4911892414093,
      "citing_paper_id": "271974279",
      "cited_paper_id": 232170230
    },
    {
      "context_text": "We evaluate the performance of our stereo VO pipeline on sequences from five publicly available datasets [3,14,21,36,40] with varying camera resolutions depicting a wide range of scenarios on different mobile platforms.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "DSEC",
        "EVIMO2"
      ],
      "dataset_descriptions": {
        "DSEC": "Used to evaluate stereo event camera performance in driving scenarios, focusing on 3D reconstruction and visual odometry.",
        "EVIMO2": "Used to assess motion segmentation, optical flow, structure from motion, and visual inertial odometry in indoor scenes with monocular event cameras."
      },
      "confidence_score": 0.8,
      "reasoning": "The context mentions 'five publicly available datasets' but does not specify their names. However, the cited paper titles suggest specific datasets that could be relevant.",
      "processing_time": 41.4911892414093,
      "citing_paper_id": "271974279",
      "cited_paper_id": 248572428
    },
    {
      "context_text": "This dataset was recorded with stereo DAVIS cameras with 240 × 180 px resolution mounted on a handheld rig [36].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.3,
      "reasoning": "The context mentions a dataset recorded with stereo DAVIS cameras, which is relevant to event-based stereo depth estimation. However, the dataset name is not explicitly provided.",
      "processing_time": 30.143508195877075,
      "citing_paper_id": "271974279",
      "cited_paper_id": 49877954
    },
    {
      "context_text": "Our work deviates from Zhu et al. [41], which proposes a supervised learning-based system that is trained on stereo data to predict ego-motion and depth, but that at inference time works only on monocular setups.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions 'stereo data' but does not specify a named dataset. The context focuses on the method and its limitations rather than a specific dataset.",
      "processing_time": 29.986979722976685,
      "citing_paper_id": "271974279",
      "cited_paper_id": 56475917
    },
    {
      "context_text": "In such cases, initial poses may be obtained from running a frame-based method (e.g., SVO [10]) on the DAVIS frames, by IMU dead-reckoning or by a VIO method (e.g., EVIO [38], ROVIO [2]).",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only methods and tools. No verifiable resources are identified.",
      "processing_time": 28.4074387550354,
      "citing_paper_id": "271974279",
      "cited_paper_id": 206775501
    },
    {
      "context_text": "In such cases, initial poses may be obtained from running a frame-based method (e.g., SVO [10]) on the DAVIS frames, by IMU dead-reckoning or by a VIO method (e.g., EVIO [38], ROVIO [2]).",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only methods and tools. No verifiable resources are identified.",
      "processing_time": 28.4074387550354,
      "citing_paper_id": "271974279",
      "cited_paper_id": null
    },
    {
      "context_text": "Dealing with them, possibly via a combination with motion segmentation [35], is still an unexplored topic in the incipient field of event-based SLAM.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for event-based motion segmentation. The context is about an unexplored topic in event-based SLAM.",
      "processing_time": 30.844439029693604,
      "citing_paper_id": "271974279",
      "cited_paper_id": 229211559
    },
    {
      "context_text": "Where camera poses are not available, like in DSEC [14], we use LiDAR-inertial odometry data provided by the authors as ground truth.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "DSEC"
      ],
      "dataset_descriptions": {
        "DSEC": "Used to provide ground truth for camera poses in driving scenarios where such information is not directly available, enhancing the evaluation of event-based stereo depth estimation methods."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions DSEC, which is a dataset for stereo event camera data in driving scenarios. It is used as a source of ground truth for camera poses.",
      "processing_time": 35.43264293670654,
      "citing_paper_id": "271974279",
      "cited_paper_id": 232170230
    },
    {
      "context_text": "Figure 3 compares the proposed strategy to that in MC-EMVS [15] on a section of the DSEC driving dataset [14], showing a clear improvement.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "DSEC driving dataset"
      ],
      "dataset_descriptions": {
        "DSEC driving dataset": "Used to compare the proposed event-based stereo depth estimation strategy with MC-EMVS, focusing on performance improvements in driving scenarios."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions the DSEC driving dataset, which is a specific, verifiable dataset used for comparing the proposed strategy with MC-EMVS.",
      "processing_time": 35.43096399307251,
      "citing_paper_id": "271974279",
      "cited_paper_id": 232170230
    },
    {
      "context_text": "1a), which is particularly suitable for automotive applications, supported by the introduction of new datasets [6,14,24,40], because it can recover the absolute scale of the scene and produces fast depth estimates due to spatial parallax.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "DSEC",
        "M3ED"
      ],
      "dataset_descriptions": {
        "DSEC": "Used to evaluate event-based stereo depth estimation methods in driving scenarios, providing synchronized event and intensity data for robust testing.",
        "M3ED": "Utilized to assess multi-robot, multi-sensor, and multi-environment event-based systems, offering diverse data for comprehensive evaluation."
      },
      "confidence_score": 0.9,
      "reasoning": "The context mentions 'new datasets' but does not specify names. However, the cited papers provide specific dataset names that are relevant to event-based stereo depth estimation.",
      "processing_time": 41.991960287094116,
      "citing_paper_id": "271974279",
      "cited_paper_id": 232170230
    },
    {
      "context_text": "1a), which is particularly suitable for automotive applications, supported by the introduction of new datasets [6,14,24,40], because it can recover the absolute scale of the scene and produces fast depth estimates due to spatial parallax.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "DSEC",
        "M3ED"
      ],
      "dataset_descriptions": {
        "DSEC": "Used to evaluate event-based stereo depth estimation methods in driving scenarios, providing synchronized event and intensity data for robust testing.",
        "M3ED": "Utilized to assess multi-robot, multi-sensor, and multi-environment event-based systems, offering diverse data for comprehensive evaluation."
      },
      "confidence_score": 0.9,
      "reasoning": "The context mentions 'new datasets' but does not specify names. However, the cited papers provide specific dataset names that are relevant to event-based stereo depth estimation.",
      "processing_time": 41.991960287094116,
      "citing_paper_id": "271974279",
      "cited_paper_id": 259380779
    },
    {
      "context_text": "The sequences in the DSEC dataset [14] were recorded with VGA resolution (640 × 480 px) stereo event cameras on a car driven through the streets of Switzerland.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "DSEC"
      ],
      "dataset_descriptions": {
        "DSEC": "Used to record sequences with stereo event cameras for driving scenarios, focusing on VGA resolution data to enhance event-based stereo depth estimation."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions the DSEC dataset, which is a specific dataset used for recording sequences with stereo event cameras in driving scenarios.",
      "processing_time": 34.29031538963318,
      "citing_paper_id": "271974279",
      "cited_paper_id": 232170230
    },
    {
      "context_text": "We present results on the only existing trinocular event dataset EVIMO2 [3], recorded using a handheld rig with three VGA resolution event cameras in an indoor environment.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "EVIMO2"
      ],
      "dataset_descriptions": {
        "EVIMO2": "Used to evaluate trinocular event-based algorithms in indoor environments, focusing on motion segmentation, optical flow, and visual inertial odometry with a handheld rig."
      },
      "confidence_score": 1.0,
      "reasoning": "The citation mentions the use of the EVIMO2 dataset, which is a specific, verifiable resource relevant to event-based stereo depth estimation.",
      "processing_time": 35.742093324661255,
      "citing_paper_id": "271974279",
      "cited_paper_id": 248572428
    },
    {
      "context_text": "Among the multiple fusion schemes available in MC-EMVS we choose a point-wise (i.e., voxel-wise) harmonic mean fusion across cameras and no extra fusion in time due to its high accuracy and speed [15].",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for depth estimation using event cameras.",
      "processing_time": 28.830862998962402,
      "citing_paper_id": "271974279",
      "cited_paper_id": 250918780
    },
    {
      "context_text": "MC-EMVS [15] circumvented this issue by always setting the DSI RV at the end of the camera trajectory in forward moving scenes, but this strategy does not work for arbitrary motions (e.g., moving backwards).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (MC-EMVS) and its limitations. The context is about a technical approach to depth estimation using event cameras.",
      "processing_time": 31.220460653305054,
      "citing_paper_id": "271974279",
      "cited_paper_id": 250918780
    },
    {
      "context_text": "In contrast to the pioneering work Event-based Stereo Visual Odometry (ESVO) [37], the MC-EMVS mapper offers several advantages, such as sharper edges, higher accuracy and improved depth map completion [15].",
      "catation_intent": "findings",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and findings. The context focuses on comparing the MC-EMVS mapper to ESVO, highlighting improvements in depth map quality.",
      "processing_time": 31.889309644699097,
      "citing_paper_id": "271974279",
      "cited_paper_id": 250918780
    },
    {
      "context_text": "Specifically, we improve upon the state-of-the-art mapping method (Multi-Camera Event-based Multi-View Stereo – MC-EMVS) [15], capable of generating semi-dense depth maps from synchronized event cameras.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (MC-EMVS) for generating depth maps from event cameras.",
      "processing_time": 30.832436323165894,
      "citing_paper_id": "271974279",
      "cited_paper_id": 250918780
    },
    {
      "context_text": "The mapper processes events in batches of fixed number of events N e (instead of a constant time window [15]).",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for processing events in batches. No verifiable resources are identified.",
      "processing_time": 30.12605571746826,
      "citing_paper_id": "271974279",
      "cited_paper_id": 250918780
    },
    {
      "context_text": "The mapping module is based on MC-EMVS [15], which demonstrated the state-of-the-art depth estimation capabilities when using ground truth (GT) poses.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (MC-EMVS) and the use of ground truth poses. Ground truth poses are not considered a dataset under the given rules.",
      "processing_time": 33.34922933578491,
      "citing_paper_id": "271974279",
      "cited_paper_id": 250918780
    },
    {
      "context_text": "Improvements with respect to MC-EMVS [15].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a reference to another paper. No verifiable resources are identified.",
      "processing_time": 31.579108715057373,
      "citing_paper_id": "271974279",
      "cited_paper_id": 250918780
    },
    {
      "context_text": "Our mapper works on the principle of ray density fusion across stereo cameras [15].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method or principle for depth estimation using stereo cameras.",
      "processing_time": 32.218441009521484,
      "citing_paper_id": "271974279",
      "cited_paper_id": 250918780
    },
    {
      "context_text": "1 because code was unavailable [8, 18, 33] and/or trajectory errors were either not reported [8], or did not use the same standard metrics as in ESVO (e.g., absolute errors) [18].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only issues with code availability and metric standards.",
      "processing_time": 32.337294578552246,
      "citing_paper_id": "271974279",
      "cited_paper_id": 260164484
    },
    {
      "context_text": "Similar to recent event-based VO methods in the literature [22,25,37], we perform quantitative evaluation by computing the root-mean-square (RMSE) Absolute Trajectory Errors (ATE) and Absolute Rotation Errors (ARE) on tracked camera poses using the tool in [34] (Tab.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only evaluation metrics and tools. The context is focused on methodological aspects rather than data sources.",
      "processing_time": 33.84501838684082,
      "citing_paper_id": "271974279",
      "cited_paper_id": 266335686
    },
    {
      "context_text": "Moreover, we consider the event-only setting, as our objective is to advance knowledge in purely event-based processing systems, expanding their capabilities while avoiding potential bottlenecks associated with paired sensors [22].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general research direction. No verifiable resources are identified.",
      "processing_time": 33.22189235687256,
      "citing_paper_id": "271974279",
      "cited_paper_id": 266335686
    },
    {
      "context_text": "Since ARE is missing from that paper, we take them from [22].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a reference to another paper for additional information.",
      "processing_time": 32.85408067703247,
      "citing_paper_id": "271974279",
      "cited_paper_id": 266335686
    },
    {
      "context_text": "Rotation errors on the RPG dataset were taken from [22].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "RPG dataset"
      ],
      "dataset_descriptions": {
        "RPG dataset": "Used to evaluate rotation errors in deep event visual odometry, focusing on the accuracy of orientation estimation in dynamic environments."
      },
      "confidence_score": 0.9,
      "reasoning": "The context mentions the 'RPG dataset' which is a specific dataset used for evaluating rotation errors in the context of visual odometry.",
      "processing_time": 41.637104749679565,
      "citing_paper_id": "271974279",
      "cited_paper_id": 266335686
    },
    {
      "context_text": "…at a high temporal resolution ( µ s), offering low power consumption, high dynamic range and sparsity for robust and efficient VO/SLAM [11,13,23]. for the TUM-VIE mocap-desk sequence [21], along with a snapshot of events, confidence map and the projected point cloud (overlaid on the events).",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "TUM-VIE mocap-desk sequence"
      ],
      "dataset_descriptions": {
        "TUM-VIE mocap-desk sequence": "Used to evaluate event-based stereo depth estimation, providing high-resolution event data, confidence maps, and point clouds for robust and efficient visual odometry and SLAM."
      },
      "confidence_score": 0.9,
      "reasoning": "The citation mentions the TUM-VIE mocap-desk sequence, which appears to be a specific dataset used for evaluation in the context of event-based stereo depth estimation.",
      "processing_time": 44.33955526351929,
      "citing_paper_id": "271974279",
      "cited_paper_id": null
    },
    {
      "context_text": "Event cameras, such as the Dynamic Vision Sensor (DVS) [23], transmit pixel-wise brightness changes asynchronously, in the form of “events”.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a type of sensor technology. No verifiable resources are identified.",
      "processing_time": 34.14916515350342,
      "citing_paper_id": "271974279",
      "cited_paper_id": null
    },
    {
      "context_text": "We first evaluate our proposed Mixed-EF2DNet on MVSEC Dataset to validate the effectiveness of our method in real-world environments.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "MVSEC Dataset"
      ],
      "dataset_descriptions": {
        "MVSEC Dataset": "Used to evaluate Mixed-EF2DNet for event-based stereo depth estimation, validating the method's effectiveness in real-world scenarios."
      },
      "confidence_score": 1.0,
      "reasoning": "The citation mentions the 'MVSEC Dataset' which is a specific dataset used for evaluating the proposed method in real-world environments.",
      "processing_time": 41.883662939071655,
      "citing_paper_id": "259338957",
      "cited_paper_id": 3416874
    },
    {
      "context_text": "Qualitative comparison for MVSEC dataset.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "MVSEC"
      ],
      "dataset_descriptions": {
        "MVSEC": "Used for qualitative comparison in event-based stereo depth estimation, focusing on 3D perception with event cameras."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions 'MVSEC dataset', which is a specific dataset used for 3D perception with event cameras. The cited paper title confirms it is a dataset.",
      "processing_time": 42.049551248550415,
      "citing_paper_id": "259338957",
      "cited_paper_id": 3416874
    },
    {
      "context_text": "The representative outdoor day1 and outdoor night1 sequences of MVSEC are used for testing to verify the effectiveness of the depth estimation algorithm in the day and night environments.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "MVSEC"
      ],
      "dataset_descriptions": {
        "MVSEC": "Used to test the depth estimation algorithm in day and night environments, focusing on outdoor sequences to verify algorithm effectiveness."
      },
      "confidence_score": 1.0,
      "reasoning": "The citation mentions specific sequences from the MVSEC dataset, which is relevant to event-based stereo depth estimation.",
      "processing_time": 41.08661437034607,
      "citing_paper_id": "259338957",
      "cited_paper_id": 3416874
    },
    {
      "context_text": "Our proposed method provides a more complete and accurate depth estimation of objects, such as trees and bus stations. samples of DENSE and then finetune the network by the combination train split of MVSEC and DENSE.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "DENSE",
        "MVSEC"
      ],
      "dataset_descriptions": {
        "DENSE": "Used for initial training of the network, focusing on dense depth estimation from event-based cameras.",
        "MVSEC": "Used in combination with DENSE for fine-tuning the network, enhancing accuracy in 3D perception tasks."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions 'DENSE' and 'MVSEC', which are specific datasets used for training and fine-tuning the network. These datasets are relevant to event-based stereo depth estimation.",
      "processing_time": 44.721864223480225,
      "citing_paper_id": "259338957",
      "cited_paper_id": 3416874
    },
    {
      "context_text": "In this section, we implement the experiments on the real-world event dataset MVSEC [24] and synthetic event dataset DENSE [18] to prove the effectiveness of our proposed method.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "MVSEC",
        "DENSE"
      ],
      "dataset_descriptions": {
        "MVSEC": "Used to validate the proposed method on real-world event data, focusing on 3D perception tasks using event cameras.",
        "DENSE": "Used to validate the proposed method on synthetic event data, focusing on monocular dense depth estimation from events."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions two specific datasets, MVSEC and DENSE, which are used to validate the proposed method in the context of event-based stereo depth estimation.",
      "processing_time": 44.56768226623535,
      "citing_paper_id": "259338957",
      "cited_paper_id": 3416874
    },
    {
      "context_text": "In this section, we implement the experiments on the real-world event dataset MVSEC [24] and synthetic event dataset DENSE [18] to prove the effectiveness of our proposed method.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "MVSEC",
        "DENSE"
      ],
      "dataset_descriptions": {
        "MVSEC": "Used to validate the proposed method on real-world event data, focusing on 3D perception tasks using event cameras.",
        "DENSE": "Used to validate the proposed method on synthetic event data, focusing on monocular dense depth estimation from events."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions two specific datasets, MVSEC and DENSE, which are used to validate the proposed method in the context of event-based stereo depth estimation.",
      "processing_time": 44.56768226623535,
      "citing_paper_id": "259338957",
      "cited_paper_id": 223957202
    },
    {
      "context_text": "To further demonstrate the effectiveness of the proposed method, we conduct an ablation study on the MVSEC dataset.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "MVSEC"
      ],
      "dataset_descriptions": {
        "MVSEC": "Used to conduct an ablation study, evaluating the effectiveness of the proposed method in 3D perception using event camera data."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions the MVSEC dataset, which is a specific dataset used for 3D perception with event cameras. The cited paper title confirms it is a dataset.",
      "processing_time": 42.46932625770569,
      "citing_paper_id": "259338957",
      "cited_paper_id": 3416874
    },
    {
      "context_text": "The Multi-Vehicle Stereo Event Camera Dataset (MVSEC) is a popular real-world dataset recorded with mDAVIS346 sensors [24].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Multi-Vehicle Stereo Event Camera Dataset"
      ],
      "dataset_descriptions": {
        "Multi-Vehicle Stereo Event Camera Dataset": "Used to evaluate event-based stereo depth estimation methods, providing real-world recordings from mDAVIS346 sensors for 3D perception tasks."
      },
      "confidence_score": 1.0,
      "reasoning": "The citation clearly mentions the MVSEC dataset, which is a real-world dataset recorded with mDAVIS346 sensors, making it highly relevant to event-based stereo depth estimation.",
      "processing_time": 43.93518352508545,
      "citing_paper_id": "259338957",
      "cited_paper_id": 3416874
    },
    {
      "context_text": "world event dataset MVSEC [24] and synthetic event dataset DENSE [18] to prove the effectiveness of our proposed method.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "MVSEC",
        "DENSE"
      ],
      "dataset_descriptions": {
        "MVSEC": "Used to validate the proposed method for 3D perception using real-world event data from stereo event cameras.",
        "DENSE": "Used to validate the proposed method for 3D perception using synthetic event data."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions two specific datasets, MVSEC and DENSE, which are used to validate the proposed method in the research.",
      "processing_time": 43.696229219436646,
      "citing_paper_id": "259338957",
      "cited_paper_id": 3416874
    },
    {
      "context_text": "We further evaluate the models trained on DENSE in the testing split of MVSEC to investigate the generalization of methods.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "MVSEC"
      ],
      "dataset_descriptions": {
        "MVSEC": "Used to evaluate the generalization of models trained on DENSE, focusing on 3D perception tasks using event camera data."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions 'MVSEC' as a dataset used for evaluating the generalization of models trained on another dataset ('DENSE'). The cited paper title confirms 'MVSEC' is a dataset.",
      "processing_time": 43.04739761352539,
      "citing_paper_id": "259338957",
      "cited_paper_id": 3416874
    },
    {
      "context_text": "The metrics values for MonoDepth and Zhu et al. method are taken from [15] and those for MegaDepth, MegaDepth+, and E2Depth from [18].",
      "catation_intent": "findings",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only methods and their performance metrics. The cited papers' titles do not provide additional dataset information.",
      "processing_time": 36.93570160865784,
      "citing_paper_id": "259338957",
      "cited_paper_id": 4572038
    },
    {
      "context_text": "The metrics values for MonoDepth and Zhu et al. method are taken from [15] and those for MegaDepth, MegaDepth+, and E2Depth from [18].",
      "catation_intent": "findings",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only methods and their performance metrics. The cited papers' titles do not provide additional dataset information.",
      "processing_time": 36.93570160865784,
      "citing_paper_id": "259338957",
      "cited_paper_id": 223957202
    },
    {
      "context_text": "Besides, Mixed-EF2DNet also compares against well known image-based depth estimation algorithms, MonoDepth [8] 1 and MegaDepth [9], to verify the superiority of algorithms specifically designed for events.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only algorithms. The context focuses on comparing algorithms, not using datasets.",
      "processing_time": 35.707573890686035,
      "citing_paper_id": "259338957",
      "cited_paper_id": 4572038
    },
    {
      "context_text": "The predictions from MegaDepth might omit the scene information lost by images due to low lighting (e.g., the traffic light), and they have texture edges that break the continuity of depth (e.g., textures on the ground).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a method (MegaDepth). The description focuses on the limitations of the method rather than the use of a dataset.",
      "processing_time": 37.093026876449585,
      "citing_paper_id": "259338957",
      "cited_paper_id": 4572038
    },
    {
      "context_text": "For day environments, predicted depth maps from MegaDepth have artifacts in the sky.",
      "catation_intent": "findings",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions 'MegaDepth' but refers to it as a method for predicting depth maps, not a dataset. No other datasets are mentioned.",
      "processing_time": 36.26628923416138,
      "citing_paper_id": "259338957",
      "cited_paper_id": 4572038
    },
    {
      "context_text": "5, we visualize the depth maps predicted by the image-based method MegaDepth, event-based method E2Depth, and Mixed-EF2DNet.",
      "catation_intent": "none",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions methods (MegaDepth, E2Depth, Mixed-EF2DNet) but does not reference any specific datasets. The context is focused on visualizing depth maps generated by these methods.",
      "processing_time": 39.750487089157104,
      "citing_paper_id": "259338957",
      "cited_paper_id": 4572038
    },
    {
      "context_text": "The quantitative results are presented in Table I. MegaDepth+ indicates that events are reconstructed into images using E2VID [27] then taken by MegaDepth to estimate depth.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions the use of MegaDepth, but it is used as a method for depth estimation, not as a dataset. No specific dataset is mentioned.",
      "processing_time": 37.37632465362549,
      "citing_paper_id": "259338957",
      "cited_paper_id": 4572038
    },
    {
      "context_text": "where ρ(x)= √ x2 +σ2 denotes the Chabonnier loss function [23] and N (u) denotes the 4 neighbouring pixels around the pixel u.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a loss function and neighboring pixels. There are no verifiable resources or datasets mentioned.",
      "processing_time": 36.45077466964722,
      "citing_paper_id": "259338957",
      "cited_paper_id": 38030033
    },
    {
      "context_text": "low power consumption, and no motion blur, event cameras offer a potential choice for the scenarios where conventional cameras are challenged [2], [3].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only discusses the advantages of event cameras.",
      "processing_time": 35.0050630569458,
      "citing_paper_id": "259338957",
      "cited_paper_id": 91183976
    },
    {
      "context_text": "In each SE-resblock, the SE block [22] performs a squeeze and excitation operation on the feature map U output by two convolutions and gets a collection of per-channel modulation weights W .",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a method (SE block) used within a neural network architecture.",
      "processing_time": 35.69638752937317,
      "citing_paper_id": "259338957",
      "cited_paper_id": 140309863
    },
    {
      "context_text": "To quickly verify the effectiveness of introducing optical flow for our task, we use a mature image-based optical flow network RAFT [19] to estimate the optical flow between voxel grids.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions using RAFT for estimating optical flow, but RAFT is a method, not a dataset. No datasets are explicitly mentioned.",
      "processing_time": 36.6628782749176,
      "citing_paper_id": "259338957",
      "cited_paper_id": 214667893
    },
    {
      "context_text": "The central voxel grid V c and a side voxel grid V s i are passed into RAFT to output the optical flow Op s i from the central voxel grid to the side one.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only the use of a method (RAFT) for optical flow estimation.",
      "processing_time": 35.69553780555725,
      "citing_paper_id": "259338957",
      "cited_paper_id": 214667893
    },
    {
      "context_text": "The most related to our method is [18], where a recurrent encoder-decoder style framework is used to perform dense depth estimation from monocular event stream by leveraging the temporal consistency.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for dense depth estimation from monocular event streams.",
      "processing_time": 35.24801301956177,
      "citing_paper_id": "259338957",
      "cited_paper_id": 223957202
    },
    {
      "context_text": "The values for E2Depth here are taken from [18].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.2,
      "reasoning": "The citation does not mention any specific dataset names, only a method or model (E2Depth). The context is too limited to infer the use of a specific dataset.",
      "processing_time": 37.65686368942261,
      "citing_paper_id": "259338957",
      "cited_paper_id": 223957202
    },
    {
      "context_text": "Following the prior work [18], we pre-train Mixed-EF2DNet with the first 1000 training (e) Ours (f) Ground Truth Fig.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (Mixed-EF2DNet) and a general reference to prior work. No verifiable datasets are identified.",
      "processing_time": 38.75497102737427,
      "citing_paper_id": "259338957",
      "cited_paper_id": 223957202
    },
    {
      "context_text": "Our method solves the same task by a supervised joint network that further introduces optical flow compensating temporal relationship lost by event pre-processing than [18].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for solving a task involving event-based depth estimation.",
      "processing_time": 35.24477553367615,
      "citing_paper_id": "259338957",
      "cited_paper_id": 223957202
    },
    {
      "context_text": "We compare our proposed Mixed-EF2DNet with event-based depth estimation algorithms, including the method proposed by Zhu et al. [15] and E2Depth [18].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and algorithms. The context focuses on comparing the proposed method with existing event-based depth estimation algorithms.",
      "processing_time": 37.95708656311035,
      "citing_paper_id": "259338957",
      "cited_paper_id": 223957202
    },
    {
      "context_text": "[17] used events and intensity images to complement each other and further estimated dense depth maps.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method combining events and frames for depth prediction.",
      "processing_time": 35.115119218826294,
      "citing_paper_id": "259338957",
      "cited_paper_id": 231951439
    },
    {
      "context_text": "Bio-inspired dynamic vision sensors called event cameras, such as Dynamic Vision Sensor (DVS) [1], are novel eventdriven devices that only report the pixel where illumination intensity has changed beyond a set threshold.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a type of sensor technology. No verifiable resources are identified.",
      "processing_time": 35.500096797943115,
      "citing_paper_id": "259338957",
      "cited_paper_id": null
    },
    {
      "context_text": "Bio-inspired dynamic vision sensors called event cameras, such as Dynamic Vision Sensor (DVS) [1], are novel event-driven devices that only report the pixel where illumination intensity has changed beyond a set threshold.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a type of sensor technology. No verifiable resources are identified.",
      "processing_time": 35.50090265274048,
      "citing_paper_id": "259338957",
      "cited_paper_id": null
    },
    {
      "context_text": "This process is iterated until convergence to find the velocity best describing the initial inlier set found by fast MC-RANSAC, ¯ ϖ ← ¯ ϖ + δ ϖ ∗ .",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (MC-RANSAC) for processing data. No verifiable datasets are referenced.",
      "processing_time": 37.35285472869873,
      "citing_paper_id": "260164484",
      "cited_paper_id": 837271
    },
    {
      "context_text": "The fast version of MC-RANSAC (Sec.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only a method (MC-RANSAC). No verifiable resources are identified.",
      "processing_time": 36.648077964782715,
      "citing_paper_id": "260164484",
      "cited_paper_id": 837271
    },
    {
      "context_text": "This iterative process uses the velocities found during MC-RANSAC as an initial condition.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (MC-RANSAC).",
      "processing_time": 35.494316816329956,
      "citing_paper_id": "260164484",
      "cited_paper_id": 837271
    },
    {
      "context_text": "The inlier tracklets from MC-RANSAC are used to define the trajectory optimization problem (Fig.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (MC-RANSAC) used for trajectory optimization. No verifiable datasets are referenced.",
      "processing_time": 37.94804620742798,
      "citing_paper_id": "260164484",
      "cited_paper_id": 837271
    },
    {
      "context_text": "It also uses Motion-Compensated RANSAC (MC-RANSAC) [10] to consider the unique measurement times during outlier rejection and independently provide better tracklets and initial conditions for estimation.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (MC-RANSAC).",
      "processing_time": 35.49432897567749,
      "citing_paper_id": "260164484",
      "cited_paper_id": 837271
    },
    {
      "context_text": "The largest inlier set found is used as the initial segmentation for the full iterative MC-RANSAC.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (MC-RANSAC) and its application. No verifiable resources are identified.",
      "processing_time": 37.63780188560486,
      "citing_paper_id": "260164484",
      "cited_paper_id": 837271
    },
    {
      "context_text": "In contrast to these existing works, it maintains temporal resolution with either frame-based or event-based feature detection and tracking and with a RANSAC formulation [10] that separates outlier rejection from estimation.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (RANSAC) and general concepts. No verifiable resources are identified.",
      "processing_time": 37.349995136260986,
      "citing_paper_id": "260164484",
      "cited_paper_id": 837271
    },
    {
      "context_text": "Both versions of MC-RANSAC are compared to traditional RANSAC in [10].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a comparison between different versions of RANSAC algorithms.",
      "processing_time": 36.074331283569336,
      "citing_paper_id": "260164484",
      "cited_paper_id": 837271
    },
    {
      "context_text": "This process is repeated a user-specified number of times and then the largest inlier set is refined using the full iterative MC-RANSAC (Sec.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only a method (MC-RANSAC). There are no verifiable resources or datasets mentioned.",
      "processing_time": 37.63299512863159,
      "citing_paper_id": "260164484",
      "cited_paper_id": 837271
    },
    {
      "context_text": "This assumption is incorrect for asynchronous event tracklets and this paper instead uses MC-RANSAC [10], which makes no assumptions about common state times and uses a constant-velocity model in SE(3).",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (MC-RANSAC) used for processing asynchronous event tracklets.",
      "processing_time": 37.05779504776001,
      "citing_paper_id": "260164484",
      "cited_paper_id": 837271
    },
    {
      "context_text": "This assumption is incorrect for asynchronous event tracklets and this paper instead uses MC-RANSAC [10], which makes no assumptions about common state times and uses a constant-velocity model in SE (3) .",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (MC-RANSAC) used for processing asynchronous event tracklets.",
      "processing_time": 37.34404706954956,
      "citing_paper_id": "260164484",
      "cited_paper_id": 837271
    },
    {
      "context_text": "This is more accurate outlier rejection than using fast MC-RANSAC alone.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (RANSAC) for improving outlier rejection in 3D visual sensors.",
      "processing_time": 37.62936449050903,
      "citing_paper_id": "260164484",
      "cited_paper_id": 837271
    },
    {
      "context_text": "Outlier rejection was done with 10000 iterations of fast MC-RANSAC followed with one call of iterative MC-RANSAC, both using an inlier threshold of 5%.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for outlier rejection using RANSAC variants.",
      "processing_time": 36.22968363761902,
      "citing_paper_id": "260164484",
      "cited_paper_id": 837271
    },
    {
      "context_text": "MC-RANSAC segments these tracklets into inliers and outliers by finding the most tracklets that can be explained by a single velocity.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (MC-RANSAC) used for processing tracklets.",
      "processing_time": 36.06642174720764,
      "citing_paper_id": "260164484",
      "cited_paper_id": 837271
    },
    {
      "context_text": "2) Iterative MC-RANSAC: A more accurate iterative MC-RANSAC approach minimizes the reprojection error of each tracklet in image space with the cost function, where R j,k is the covariance matrix for the measurements of the j th tracklet and M fast is the number of inliers found by the fast MC-RANSAC.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (MC-RANSAC) and its application. No verifiable resources are identified.",
      "processing_time": 38.154475927352905,
      "citing_paper_id": "260164484",
      "cited_paper_id": 837271
    },
    {
      "context_text": "Traditional VO pipeline uses Random Sample Consensus (RANSAC) [33] to remove tracklet outliers before estimation.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific dataset. It refers to a method (RANSAC) used in visual odometry for outlier rejection.",
      "processing_time": 37.931880235672,
      "citing_paper_id": "260164484",
      "cited_paper_id": 1143169
    },
    {
      "context_text": "V ISUAL Odometry (VO) is a technique to estimate egomotion in robotics [1], [2], [3], [4], [5].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only the technique of Visual Odometry. No verifiable resources are identified.",
      "processing_time": 37.33131003379822,
      "citing_paper_id": "260164484",
      "cited_paper_id": 3299195
    },
    {
      "context_text": "V ISUAL Odometry (VO) is a technique to estimate egomotion in robotics [1], [2], [3], [4], [5].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only the technique of Visual Odometry. No verifiable resources are identified.",
      "processing_time": 37.33131003379822,
      "citing_paper_id": "260164484",
      "cited_paper_id": 14925984
    },
    {
      "context_text": "Many pipelines do this by grouping similar feature times to a common time [7], [8], [9].",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only a general method for handling feature times in pipelines. The cited papers' titles suggest a focus on event-based visual SLAM and stereo visual odometry, but no specific datasets are named.",
      "processing_time": 41.11779069900513,
      "citing_paper_id": "260164484",
      "cited_paper_id": 3738244
    },
    {
      "context_text": "Many pipelines do this by grouping similar feature times to a common time [7], [8], [9].",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only a general method for handling feature times in pipelines. The cited papers' titles suggest a focus on event-based visual SLAM and stereo visual odometry, but no specific datasets are named.",
      "processing_time": 41.11779069900513,
      "citing_paper_id": "260164484",
      "cited_paper_id": 220870707
    },
    {
      "context_text": "Ultimate-SLAM [7] extends [17] to use the IMU to generate motion-compensated event frames and reformulates the cost function for camera egomotion estimation.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and approaches. The context focuses on extending a method to incorporate IMU data for motion compensation and egomotion estimation.",
      "processing_time": 38.718501806259155,
      "citing_paper_id": "260164484",
      "cited_paper_id": 3738244
    },
    {
      "context_text": "[18] extract features using an asynchronous feature detector (Arc* [19]) from a stereo pair of event cameras and adopt an estimation pipeline similar to [7].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and tools. The context focuses on the use of an asynchronous feature detector and event cameras for stereo depth estimation.",
      "processing_time": 39.13790702819824,
      "citing_paper_id": "260164484",
      "cited_paper_id": 3738244
    },
    {
      "context_text": "[18] extract features using an asynchronous feature detector (Arc* [19]) from a stereo pair of event cameras and adopt an estimation pipeline similar to [7].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and tools. The context focuses on the use of an asynchronous feature detector and event cameras for stereo depth estimation.",
      "processing_time": 39.13790702819824,
      "citing_paper_id": "260164484",
      "cited_paper_id": 49864158
    },
    {
      "context_text": "Mueggler et al. [27] use a continuous-time pose estimation framework that uses IMU measurements and represents the trajectory as cumulative cubic B-splines.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for continuous-time pose estimation using IMU measurements and cumulative cubic B-splines.",
      "processing_time": 38.502471923828125,
      "citing_paper_id": "260164484",
      "cited_paper_id": 9729856
    },
    {
      "context_text": "Features are detected and tracked in the event frames and each feature is assigned an event time from the SAE.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a method for detecting and tracking features in event frames. No verifiable resource names are present.",
      "processing_time": 38.334360122680664,
      "citing_paper_id": "260164484",
      "cited_paper_id": 12475678
    },
    {
      "context_text": "1) Event Clustering: The stereo event stream is rectified and clustered to construct new SAEs and new binary event frames (Fig.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for processing stereo event streams.",
      "processing_time": 35.81425213813782,
      "citing_paper_id": "260164484",
      "cited_paper_id": 12475678
    },
    {
      "context_text": "The system takes an asynchronous stereo event stream and clusters the events into event frames and SAEs.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a method for processing event streams.",
      "processing_time": 35.81103253364563,
      "citing_paper_id": "260164484",
      "cited_paper_id": 12475678
    },
    {
      "context_text": "The timestamp of each tracklet state is assigned from the nearest event in the associated SAE.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a method or process involving 'SAE' (Sparse Asynchronous Events). No verifiable dataset names are present.",
      "processing_time": 38.33008933067322,
      "citing_paper_id": "260164484",
      "cited_paper_id": 12475678
    },
    {
      "context_text": "The temporal resolution of event cameras is maintained by assigning (possibly unique) times to each feature from the corresponding event in a Surface of Active Events (SAE) [31].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method or concept (Surface of Active Events).",
      "processing_time": 35.808528900146484,
      "citing_paper_id": "260164484",
      "cited_paper_id": 12475678
    },
    {
      "context_text": "The SAE records the most recent event timestamp of each pixel location and is used to maintain the asynchronous nature of the event camera.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a method or concept (SAE) related to event cameras.",
      "processing_time": 37.03218126296997,
      "citing_paper_id": "260164484",
      "cited_paper_id": 12475678
    },
    {
      "context_text": "[23] present a feature-based stereo VO pipeline using events, which group events into frames and then adopts a similar estimation framework to a traditional frame-based stereo VO pipeline [3].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method or pipeline. The context is about a feature-based stereo visual odometry pipeline using events, which is a method rather than a dataset.",
      "processing_time": 39.90426278114319,
      "citing_paper_id": "260164484",
      "cited_paper_id": 14925984
    },
    {
      "context_text": "The sliding window width is five and LIBVISO2 is run on both full-and half-resolution images.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (LIBVISO2) and a parameter setting. No verifiable resources are identified.",
      "processing_time": 37.608726263046265,
      "citing_paper_id": "260164484",
      "cited_paper_id": 16284071
    },
    {
      "context_text": "A sliding-window version of the system is implemented in MATLAB using LIBVISO2 [36] for feature detection and tracking.",
      "catation_intent": "none",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only tools and methods. The cited paper title does not provide additional information about datasets.",
      "processing_time": 37.60448932647705,
      "citing_paper_id": "260164484",
      "cited_paper_id": 16284071
    },
    {
      "context_text": "[21] estimate SE(3) motion using only eventbased cameras.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for estimating SE(3) motion using event-based cameras.",
      "processing_time": 36.3881778717041,
      "citing_paper_id": "260164484",
      "cited_paper_id": 26324573
    },
    {
      "context_text": "It is compared quantitatively to other estimation techniques in [24, 35].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a comparison to other estimation techniques. No verifiable resources are identified.",
      "processing_time": 37.31452989578247,
      "citing_paper_id": "260164484",
      "cited_paper_id": 27987704
    },
    {
      "context_text": "This interpolation can also be used to define the estimation states at a subset of the measurement times [24, 35].",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for interpolation. No dataset names are present in the citation context.",
      "processing_time": 37.313689947128296,
      "citing_paper_id": "260164484",
      "cited_paper_id": 27987704
    },
    {
      "context_text": "The system’s computational performance should be improved by implementing it in a more efficient language, such as C++, and using keytimes to reduce the number of the estimation states [24, 35].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only suggestions for improving computational performance.",
      "processing_time": 35.45418071746826,
      "citing_paper_id": "260164484",
      "cited_paper_id": 27987704
    },
    {
      "context_text": "…γ k , can be defined as a continuous-time function with respect to the global trajectory state, , where T k ( τ ) is the pose at time t k ≤ τ ≤ t k +1 , J ( · ) − 1 is the inverse left Jacobian function, ln( · ) is the inverse exponential map, and ( · ) ∨ is the inverse lifting operator [34].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only mathematical functions and operations used in state estimation for robotics.",
      "processing_time": 36.59828329086304,
      "citing_paper_id": "260164484",
      "cited_paper_id": 65172180
    },
    {
      "context_text": "…result equal to zero gives the perturbation that minimizes the linearization, where H j,k is the Jacobian of error function in (7), where the partial derivative of the sensor model is evaluated at the nominal value, T k,k ′ is the adjoint of SE (3) and J k,k ′ is the left Jacobian of SE (3) [34].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only mathematical concepts and equations. There are no clear identifiers for datasets or other verifiable resources.",
      "processing_time": 38.48140835762024,
      "citing_paper_id": "260164484",
      "cited_paper_id": 65172180
    },
    {
      "context_text": "…δ x = { δ ξ , δ ϖ , δ ζ } , linearizing (10) as where where P j,k and P k are matrices to pick the specific components of the total perturbation, G j,k is the Jacobian of (13), and E k is the Jacobian of the prior error function in (11), where and ( · ) ⋏ is the R 6 × 1 to R 6 × 6 operator [34].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only mathematical operations and matrices. There are no clear identifiers for datasets or other verifiable resources.",
      "processing_time": 38.47981858253479,
      "citing_paper_id": "260164484",
      "cited_paper_id": 65172180
    },
    {
      "context_text": "Substituting (3) into (2) approximates the error term as, where [34].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, models, or methods. It only refers to mathematical equations.",
      "processing_time": 36.034045696258545,
      "citing_paper_id": "260164484",
      "cited_paper_id": 65172180
    },
    {
      "context_text": "This constant velocity over a time, ∆ t , gives the relative SE (3) transformation, where exp( · ) is the matrix exponential and ( · ) ∧ is the lifting operator that converts R 6 × 1 to R 4 × 4 [34].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only mathematical concepts and operators. There are no verifiable resources or datasets mentioned.",
      "processing_time": 37.89963459968567,
      "citing_paper_id": "260164484",
      "cited_paper_id": 65172180
    },
    {
      "context_text": "The trajectory is initialized with an Ackermann motion model [29], and globally optimized with a B-spline-based continuous-time estimation framework.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and models. There are no verifiable resources that meet the criteria for inclusion.",
      "processing_time": 38.11897850036621,
      "citing_paper_id": "260164484",
      "cited_paper_id": 195496021
    },
    {
      "context_text": "It is evaluated on the publicly available Multi Vehicle Stereo Event Camera (MVSEC) dataset [11], where it obtains a more accurate and smoother trajectory estimate than the state-of-theart Event-based Stereo Visual Odometry (ESVO) [9].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Multi Vehicle Stereo Event Camera (MVSEC)"
      ],
      "dataset_descriptions": {
        "Multi Vehicle Stereo Event Camera (MVSEC)": "Used to evaluate the accuracy and smoothness of trajectory estimates in event-based stereo depth estimation, comparing performance against state-of-the-art methods."
      },
      "confidence_score": 1.0,
      "reasoning": "The MVSEC dataset is explicitly mentioned and used for evaluating the accuracy and smoothness of trajectory estimates in event-based stereo depth estimation.",
      "processing_time": 43.849058866500854,
      "citing_paper_id": "260164484",
      "cited_paper_id": 220870707
    },
    {
      "context_text": "The presented pipeline is evaluated on the MVSEC dataset [11] and compared against the publicly available ESVO [9], a discrete event-based stereo VO pipeline.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "MVSEC"
      ],
      "dataset_descriptions": {
        "MVSEC": "Used to evaluate the presented event-based stereo depth estimation pipeline, comparing performance against the ESVO method. Focuses on visual odometry accuracy and robustness."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions the MVSEC dataset, which is a specific dataset used for evaluating the presented pipeline. The dataset is clearly identified and used for evaluation purposes.",
      "processing_time": 43.52728247642517,
      "citing_paper_id": "260164484",
      "cited_paper_id": 220870707
    },
    {
      "context_text": "ESVO [9] uses parallel tracking and mapping to estimate the egomotion trajectory and a semidense 3D scene reconstruction.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (ESVO) and its capabilities. No verifiable resources are identified.",
      "processing_time": 37.58194136619568,
      "citing_paper_id": "260164484",
      "cited_paper_id": 220870707
    },
    {
      "context_text": "IMU Dynamic Vision Sensor Odometry using Lines (IDOL) [20] uses an alternative VIO paradigm.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method or framework called IDOL. There are no clear identifiers for datasets in the provided context.",
      "processing_time": 39.10532522201538,
      "citing_paper_id": "260164484",
      "cited_paper_id": 221112528
    },
    {
      "context_text": "It could also be directly replaced with event-based methods, e.g., Arc* [19] or HASTE [32].",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods (Arc* and HASTE).",
      "processing_time": 36.374064207077026,
      "citing_paper_id": "260164484",
      "cited_paper_id": 221670108
    },
    {
      "context_text": "Hadviger et al. [23] present a feature-based stereo VO pipeline using events, which group events into frames and then adopts a similar estimation framework to a traditional frame-based stereo VO pipeline [3].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method or pipeline. The context focuses on the methodology and approach rather than the use of a particular dataset.",
      "processing_time": 39.88095021247864,
      "citing_paper_id": "260164484",
      "cited_paper_id": 235794981
    },
    {
      "context_text": "[42] ++ + + + +++ [43] + ++ +++ + +++ [44] ++ +++ [51] + ++ + ++ [35] + + +++ ++ [49] + +++ [48] 1 + + +++ [47] 2 ++ [40] + + ++ + + [53] + ++ + +++ + [45] + ++ + +++ [55] + + ++ ++ [41] + ++ ++ ++ + [52] +++ +++ + +++ [28] ++ + ++ +++ + [57] +++ +++ + [50] ++ ++ + ++",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not contain any specific dataset names or verifiable resources. The content appears to be a series of symbols and numbers, which do not provide context for dataset usage.",
      "processing_time": 39.40259909629822,
      "citing_paper_id": "246656358",
      "cited_paper_id": 185541
    },
    {
      "context_text": "Neuromorphic computing consists of a variety of brain-inspired computers, devices, and models that contrast the pervasive von Neumann computer architecture [3].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general overview of neuromorphic computing. No verifiable resources are identified.",
      "processing_time": 37.57906746864319,
      "citing_paper_id": "246656358",
      "cited_paper_id": 605892
    },
    {
      "context_text": "Moreover, neuromorphic computing has reached some significant milestones, which have been well summarized in [3].",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a summary of milestones in neuromorphic computing.",
      "processing_time": 36.18037247657776,
      "citing_paper_id": "246656358",
      "cited_paper_id": 605892
    },
    {
      "context_text": "An intermediate complexity choice is the Izhikevich model [31].",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific dataset, only a model. The context is about the Izhikevich model, which is a method, not a dataset.",
      "processing_time": 40.066253662109375,
      "citing_paper_id": "246656358",
      "cited_paper_id": 814743
    },
    {
      "context_text": "It eliminates the dependency on absolute lighting level and instead the receptors respond to changes in the incident light (also known as temporal contrast) [13,34].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a biological concept related to neural computations in the retina.",
      "processing_time": 37.289790391922,
      "citing_paper_id": "246656358",
      "cited_paper_id": 1226657
    },
    {
      "context_text": "The latency between triggered events has dropped to less than 0.5 µ s [5,6].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only technical details about event-based sensors.",
      "processing_time": 36.174689292907715,
      "citing_paper_id": "246656358",
      "cited_paper_id": 2792722
    },
    {
      "context_text": "The latency between triggered events has dropped to less than 0.5 µ s [5,6].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only technical details about event-based sensors.",
      "processing_time": 36.174689292907715,
      "citing_paper_id": "246656358",
      "cited_paper_id": 197634653
    },
    {
      "context_text": "The depth prediction is done with a fully convolutional neural network, based on the UNet architecture [59].",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific dataset. It only refers to a method (UNet architecture) used for depth prediction.",
      "processing_time": 38.10172939300537,
      "citing_paper_id": "246656358",
      "cited_paper_id": 3719281
    },
    {
      "context_text": "Later, time surfaces (hierarchy of time surfaces (HOTS) and histograms of averaged time surfaces (HATS)) were presented, which used the spatiotemporal resolution provided by the event-based cameras to generate 3D maps of event data [26,27].",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions 'event data' but does not specify a dataset name. The cited paper title suggests a method (HATS) rather than a dataset.",
      "processing_time": 39.65750050544739,
      "citing_paper_id": "246656358",
      "cited_paper_id": 3993392
    },
    {
      "context_text": "The authors of [52] showed that motion blur can be solved by having a range of disparities to synchronize the position of events.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for solving motion blur using event-based stereo. No verifiable resources are identified.",
      "processing_time": 38.28753662109375,
      "citing_paper_id": "246656358",
      "cited_paper_id": 4412139
    },
    {
      "context_text": "Going further, a spatiotemporal adaptive cooperative approach was used to calculate disparities for each incoming event in [44,45].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and approaches. The context focuses on the methodology used for calculating disparities in event-driven stereo vision.",
      "processing_time": 38.67055058479309,
      "citing_paper_id": "246656358",
      "cited_paper_id": 6079544
    },
    {
      "context_text": "Going further, a spatiotemporal adaptive cooperative approach was used to calculate disparities for each incoming event in [44,45].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and approaches. The context focuses on the methodology used for calculating disparities in event-driven stereo vision.",
      "processing_time": 38.67055058479309,
      "citing_paper_id": "246656358",
      "cited_paper_id": 7151414
    },
    {
      "context_text": "[42] [63] Embedded Blackfin BF537 DSP Stereo 3.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only hardware components. There is no clear indication of a reusable dataset.",
      "processing_time": 38.09300136566162,
      "citing_paper_id": "246656358",
      "cited_paper_id": 15357188
    },
    {
      "context_text": "An event-based camera combined with TrueNorth has successfully demonstrated the ability of a fully event-based stereo system [41].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific dataset. It refers to a hardware demonstration using TrueNorth, which is a neurosynaptic chip, not a dataset.",
      "processing_time": 39.86468958854675,
      "citing_paper_id": "246656358",
      "cited_paper_id": 15373627
    },
    {
      "context_text": "In [41], the program that is running on TrueNorth was written in the Corelet programming language that was speciﬁcally created by IBM for this hardware platform.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a programming language and hardware platform.",
      "processing_time": 35.76443004608154,
      "citing_paper_id": "246656358",
      "cited_paper_id": 15373627
    },
    {
      "context_text": "At the time of publication for this paper, a few neuromorphic processors have already been manufactured and well-utilized: TrueNorth [36], Loihi [37], SpiNNaker [38], and some others still in development [39].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions neuromorphic processors but does not refer to any specific datasets. The names mentioned are models or hardware, not datasets.",
      "processing_time": 38.66553044319153,
      "citing_paper_id": "246656358",
      "cited_paper_id": 15373627
    },
    {
      "context_text": "At the time of publication for this paper, a few neuromorphic processors have already been manufactured and well-utilized: TrueNorth [36], Loihi [37], SpiNNaker [38], and some others still in development [39].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions neuromorphic processors but does not refer to any specific datasets. The names mentioned are models or hardware, not datasets.",
      "processing_time": 38.66553044319153,
      "citing_paper_id": "246656358",
      "cited_paper_id": 25268038
    },
    {
      "context_text": "This neuromorphic chip consists of systems of equations that deﬁne the behaviour of TrueNorth’s neurons.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a neuromorphic chip called TrueNorth. The citation is about the design and tool flow of the chip, not a dataset.",
      "processing_time": 40.05514144897461,
      "citing_paper_id": "246656358",
      "cited_paper_id": 15373627
    },
    {
      "context_text": "The second method utilizing a TrueNorth processor realized a fully event-based system that achieved high computational efﬁciency and low power consumption.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method involving a TrueNorth processor. No verifiable resources are identified.",
      "processing_time": 38.445101261138916,
      "citing_paper_id": "246656358",
      "cited_paper_id": 15373627
    },
    {
      "context_text": "It used an event-based camera, and its output data was connected to nine TrueNorth chips.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific dataset, only hardware components. The citation is about using an event-based camera and TrueNorth chips, which are not datasets.",
      "processing_time": 39.07848906517029,
      "citing_paper_id": "246656358",
      "cited_paper_id": 15373627
    },
    {
      "context_text": "Table 3 presents seventeen methods that use standard CPUs, GPUs, or embedded platforms, only two methods that use FPGAs and one method that utilizes a TrueNorth processor.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only hardware platforms. The cited paper title is about a neurosynaptic chip, which is not a dataset.",
      "processing_time": 38.66075539588928,
      "citing_paper_id": "246656358",
      "cited_paper_id": 15373627
    },
    {
      "context_text": "Some solutions exist that can remove or mitigate motion blur or substitute it with motion flow using deep neural networks [2].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for removing motion blur using deep neural networks.",
      "processing_time": 36.561118841171265,
      "citing_paper_id": "246656358",
      "cited_paper_id": 18123440
    },
    {
      "context_text": "This is one of the ﬁrst models that describes the behaviour of biological neurons [30].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific dataset, only a model describing biological neurons.",
      "processing_time": 36.34553050994873,
      "citing_paper_id": "246656358",
      "cited_paper_id": 20873334
    },
    {
      "context_text": "The non-linear dynamic range is fairly the same in all the presented research papers with a maximum value up to 143 dB [7].",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a technical specification of a sensor.",
      "processing_time": 36.55791759490967,
      "citing_paper_id": "246656358",
      "cited_paper_id": 21317717
    },
    {
      "context_text": "In another work, a belief propagation neural network, which is deﬁned in [54], was used and a mean depth error of 61.14–92%, with a maximum depth of 5 m was reported [55].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific dataset names, only a method and performance metrics.",
      "processing_time": 36.81806468963623,
      "citing_paper_id": "246656358",
      "cited_paper_id": 22158024
    },
    {
      "context_text": "Monocular depth estimation meth-ods [55,57] have shown good performance in outdoor scenes up to thirty meters.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods for monocular depth estimation. No dataset names are provided in the context.",
      "processing_time": 38.88648009300232,
      "citing_paper_id": "246656358",
      "cited_paper_id": 22158024
    },
    {
      "context_text": "The study in [47] was the ﬁrst (to the best of authors’ knowledge) to provide a 3D reconstruction of a scene based on 6-DOF camera tracking.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method or approach for 3D reconstruction and 6-DOF tracking using an event camera.",
      "processing_time": 39.06946539878845,
      "citing_paper_id": "246656358",
      "cited_paper_id": 26324573
    },
    {
      "context_text": "Nevertheless, [47] trail-blazed monocular event-based depth perception and 3D reconstruction for the next generation of efforts in this area.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method or approach. The cited paper is about real-time 3D reconstruction and tracking using an event camera, which is relevant to the topic but does not specify a dataset.",
      "processing_time": 41.498151540756226,
      "citing_paper_id": "246656358",
      "cited_paper_id": 26324573
    },
    {
      "context_text": "One of the ﬁrst to solve the correspondence problem with event-based vision sensors was [42].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific dataset, only a method or system for solving the correspondence problem with event-based vision sensors.",
      "processing_time": 38.64544200897217,
      "citing_paper_id": "246656358",
      "cited_paper_id": 30913835
    },
    {
      "context_text": "The researchers in [42] used an embedded device with a DSP, which utilized a simple algo-rithm for stereo processing— camera calibration and rectiﬁcation, stereo correspondence calculation and reconstruction (disparity map).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for stereo processing using an embedded device.",
      "processing_time": 36.54566478729248,
      "citing_paper_id": "246656358",
      "cited_paper_id": 30913835
    },
    {
      "context_text": "The correspondence problem with a neuromorphic computing platform, SpiNNaker, was ﬁrst solved by [40].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method or system (SpiNNaker) used to solve the correspondence problem.",
      "processing_time": 38.07317852973938,
      "citing_paper_id": "246656358",
      "cited_paper_id": 34855834
    },
    {
      "context_text": "However, this noise can be filtered out using background filtering techniques [32].",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a technique for filtering noise in event-based systems.",
      "processing_time": 36.330973386764526,
      "citing_paper_id": "246656358",
      "cited_paper_id": 203162947
    },
    {
      "context_text": "The results from the MVSEC dataset are compared with frame-based state-of-the-art methods [61,62], and with another monocular event-based method [58].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "MVSEC"
      ],
      "dataset_descriptions": {
        "MVSEC": "Used to compare results of event-based stereo depth estimation with frame-based and monocular event-based methods, focusing on performance metrics and consistency."
      },
      "confidence_score": 1.0,
      "reasoning": "The citation mentions the MVSEC dataset, which is a specific dataset used for event-based stereo depth estimation. The dataset is used to compare results with other methods.",
      "processing_time": 43.56352996826172,
      "citing_paper_id": "246656358",
      "cited_paper_id": 206596513
    },
    {
      "context_text": "The main differences between SNNs and ANNs are well summarized in [29]:",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a comparison between SNNs and ANNs.",
      "processing_time": 37.84523940086365,
      "citing_paper_id": "246656358",
      "cited_paper_id": 208098355
    },
    {
      "context_text": "Hence, it can be extensively used in safety-critical applications like those related to the automotive industry [33].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general application area. There are no clear identifiers for datasets or other verifiable resources.",
      "processing_time": 39.35725927352905,
      "citing_paper_id": "246656358",
      "cited_paper_id": 220314130
    },
    {
      "context_text": "5 [24] 320 × 262 1000 >100 13 × 13 70 [8] 132 × 1024 10 × 10 0.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not contain any specific, verifiable dataset names or other resources. It appears to be a series of numbers and symbols without context.",
      "processing_time": 38.8735294342041,
      "citing_paper_id": "246656358",
      "cited_paper_id": null
    },
    {
      "context_text": "For the new arrival event streams, the existing event-corner features are firstly temporally tracked by the LK optical approach [20] and then spatially matched in left and right event streams.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and approaches. No verifiable resources are identified.",
      "processing_time": 37.840811014175415,
      "citing_paper_id": "255125395",
      "cited_paper_id": 2121536
    },
    {
      "context_text": "What’s more, as can be seen from the video record of the evaluation using our ESVIO (take the school-scooter 3 in Vector and indoor flying 1 4 in MVSEC [7] as examples).",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "MVSEC"
      ],
      "dataset_descriptions": {
        "MVSEC": "Used to evaluate stereo depth estimation methods, specifically focusing on indoor flying scenarios and school-scooter sequences to assess performance in dynamic environments."
      },
      "confidence_score": 1.0,
      "reasoning": "The citation mentions 'MVSEC' which is a specific dataset used for 3D perception with event cameras. The context indicates that it is used for evaluation, specifically for stereo depth estimation.",
      "processing_time": 44.42404508590698,
      "citing_paper_id": "255125395",
      "cited_paper_id": 3416874
    },
    {
      "context_text": "In Section IV-B, we compare our methods with other event-based and image-based methods on two publicly available datasets: MVSEC [23] and VECtor [24].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "MVSEC",
        "VECtor"
      ],
      "dataset_descriptions": {
        "MVSEC": "Used to compare event-based and image-based methods for 3D perception, focusing on stereo depth estimation using event cameras.",
        "VECtor": "Used to evaluate multi-sensor SLAM algorithms, providing a versatile benchmark for event-centric data in the context of stereo depth estimation."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions two specific datasets, MVSEC and VECtor, which are relevant to the research topic of event-based stereo depth estimation.",
      "processing_time": 45.93324422836304,
      "citing_paper_id": "255125395",
      "cited_paper_id": 3416874
    },
    {
      "context_text": "In Section IV-B, we compare our methods with other event-based and image-based methods on two publicly available datasets: MVSEC [23] and VECtor [24].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "MVSEC",
        "VECtor"
      ],
      "dataset_descriptions": {
        "MVSEC": "Used to compare event-based and image-based methods for 3D perception, focusing on stereo depth estimation using event cameras.",
        "VECtor": "Used to evaluate multi-sensor SLAM algorithms, providing a versatile benchmark for event-centric data in the context of stereo depth estimation."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions two specific datasets, MVSEC and VECtor, which are relevant to the research topic of event-based stereo depth estimation.",
      "processing_time": 45.93324422836304,
      "citing_paper_id": "255125395",
      "cited_paper_id": 250127779
    },
    {
      "context_text": "In subsection IV-B, we compare our methods with other event-based and image-based methods on two publicly available datasets: MVSEC [23] and VECtor [24].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "MVSEC",
        "VECtor"
      ],
      "dataset_descriptions": {
        "MVSEC": "Used to compare event-based and image-based methods for 3D perception, focusing on stereo depth estimation using event cameras.",
        "VECtor": "Used to evaluate multi-sensor SLAM techniques, including event-based stereo depth estimation, providing a versatile benchmark for performance comparison."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions two specific datasets, MVSEC and VECtor, which are relevant to the research topic of event-based stereo depth estimation.",
      "processing_time": 45.87828826904297,
      "citing_paper_id": "255125395",
      "cited_paper_id": 3416874
    },
    {
      "context_text": "In subsection IV-B, we compare our methods with other event-based and image-based methods on two publicly available datasets: MVSEC [23] and VECtor [24].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "MVSEC",
        "VECtor"
      ],
      "dataset_descriptions": {
        "MVSEC": "Used to compare event-based and image-based methods for 3D perception, focusing on stereo depth estimation using event cameras.",
        "VECtor": "Used to evaluate multi-sensor SLAM techniques, including event-based stereo depth estimation, providing a versatile benchmark for performance comparison."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions two specific datasets, MVSEC and VECtor, which are relevant to the research topic of event-based stereo depth estimation.",
      "processing_time": 45.87828826904297,
      "citing_paper_id": "255125395",
      "cited_paper_id": 250127779
    },
    {
      "context_text": "For the MVSEC [23], we select the sequence captured in the indoor flying room.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "MVSEC"
      ],
      "dataset_descriptions": {
        "MVSEC": "Used to select a sequence captured in the indoor flying room for 3D perception experiments with event cameras, focusing on stereo depth estimation."
      },
      "confidence_score": 1.0,
      "reasoning": "The citation mentions 'MVSEC' which is a specific dataset used for 3D perception with event cameras. The context indicates it is used for selecting a specific sequence for experimentation.",
      "processing_time": 43.94631481170654,
      "citing_paper_id": "255125395",
      "cited_paper_id": 3416874
    },
    {
      "context_text": "Available: https://github.com/MichaelGrupp /evo [26] C. Campos, R. Elvira, J. J. G. Rodríguez, J. M. Montiel, and J. D. Tars, “ORB-SLAM3: An accurate open-source library for visual, visual– inertial, and multimap SLAM,” IEEE Trans.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (ORB-SLAM3) and a GitHub repository (evo). The context is about visual and inertial SLAM, which is related to the topic but does not specify any datasets.",
      "processing_time": 42.322110652923584,
      "citing_paper_id": "255125395",
      "cited_paper_id": 3738244
    },
    {
      "context_text": "While the traditional event-based methods [2], [3], [5] failed in most of the sequences in these two datasets.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.2,
      "reasoning": "The context mentions 'two datasets' but does not provide specific names. The cited paper title does not help in identifying specific datasets.",
      "processing_time": 38.863396883010864,
      "citing_paper_id": "255125395",
      "cited_paper_id": 3738244
    },
    {
      "context_text": "Index Terms—Visual-Inertial SLAM, sensor fusion, aerial systems: perception and autonomy.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general terms and concepts related to visual-inertial SLAM and sensor fusion.",
      "processing_time": 39.612690448760986,
      "citing_paper_id": "255125395",
      "cited_paper_id": 3738244
    },
    {
      "context_text": "Available: https://github.\ncom/arclab-hku/Event_based_VO-VIO-SLAM/tree/main/ES VIO/supply.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only a GitHub repository. The title suggests a combination of events, images, and IMU data, but no specific dataset names are provided.",
      "processing_time": 40.84028077125549,
      "citing_paper_id": "255125395",
      "cited_paper_id": 3738244
    },
    {
      "context_text": "[3] A. R. Vidal, H. Rebecq, T. Horstschaefer, and D. Scaramuzza, “Ultimate SLAM? combining events, images, and IMU for robust visual SLAM in HDR and high-speed scenarios,” IEEE Robot.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and approaches. The focus is on combining events, images, and IMU for SLAM.",
      "processing_time": 39.345702171325684,
      "citing_paper_id": "255125395",
      "cited_paper_id": 3738244
    },
    {
      "context_text": "[24] L. Gao et al., “VECtor: A versatile event-centric benchmark for multisensor SLAM,” IEEE Robot.",
      "catation_intent": "reusable resource",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method or benchmark called VECtor. No clear dataset names are provided.",
      "processing_time": 38.62486672401428,
      "citing_paper_id": "255125395",
      "cited_paper_id": 3738244
    },
    {
      "context_text": "Due to motion blur, both ORB-SLAM3 and VINS-Fusion fail to extract reliable features in hku_agg_walk sequence, resulting in system failure.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "hku_agg_walk"
      ],
      "dataset_descriptions": {
        "hku_agg_walk": "Used to evaluate the robustness of ORB-SLAM3 and VINS-Fusion in handling motion blur, focusing on feature extraction and system reliability in high-speed scenarios."
      },
      "confidence_score": 0.6,
      "reasoning": "The context mentions 'hku_agg_walk sequence' which appears to be a specific dataset or sequence used to evaluate the performance of SLAM systems. However, the name does not follow the typical naming conventions for datasets and lacks clear provenance.",
      "processing_time": 45.70745134353638,
      "citing_paper_id": "255125395",
      "cited_paper_id": 3738244
    },
    {
      "context_text": "Most of the existing event-based visual odometers (VO) use monocular event camera [2], [3], [4], while few research on visual odometry based on stereo event cameras [5], [6].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only references to research papers and methods. No verifiable resources are identified.",
      "processing_time": 38.85538458824158,
      "citing_paper_id": "255125395",
      "cited_paper_id": 3738244
    },
    {
      "context_text": "Most of the existing event-based visual odometers (VO) use monocular event camera [2], [3], [4], while few research on visual odometry based on stereo event cameras [5], [6].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only references to research papers and methods. No verifiable resources are identified.",
      "processing_time": 38.85538458824158,
      "citing_paper_id": "255125395",
      "cited_paper_id": 235794981
    },
    {
      "context_text": "As for the MRE evaluation criterion, our ESVIO shows significant improvement compared to other advanced algorithms, e.g. the average MRE of ESVIO is 0.033◦/m while the value of ORB-SLAM3 is 0.12◦/m.",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only compares performance metrics of different algorithms.",
      "processing_time": 37.23164963722229,
      "citing_paper_id": "255125395",
      "cited_paper_id": 3738244
    },
    {
      "context_text": "In the hku_dark_normal sequence, ORB-SLAM3 cannot extract any feature due to poor light conditions (more qualitative details is in the supplementary material).",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.2,
      "reasoning": "The context mentions a specific sequence 'hku_dark_normal' but does not clearly indicate it is a dataset. It is more likely a specific scenario or test case within a broader dataset or experimental setup.",
      "processing_time": 41.10412406921387,
      "citing_paper_id": "255125395",
      "cited_paper_id": 3738244
    },
    {
      "context_text": "[25] M. Grupp, “EVO: Python package for the evaluation of odometry and SLAM,” 2017.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific dataset, only a Python package for evaluating odometry and SLAM. The context is about a method or tool, not a dataset.",
      "processing_time": 40.31386947631836,
      "citing_paper_id": "255125395",
      "cited_paper_id": 3738244
    },
    {
      "context_text": "Available: https://github.com/arclabhku/Event_based_VO-VIO-SLAM.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only a GitHub repository for code. The cited paper title suggests a focus on SLAM techniques but does not specify datasets.",
      "processing_time": 40.91875505447388,
      "citing_paper_id": "255125395",
      "cited_paper_id": 3738244
    },
    {
      "context_text": "Although the MPE criterion of ORB-SLAM3 is slightly better than ours in some sequences (e.g. robot-normal, desk-normal, mountain-normal), our ESVIO provides more reliable and accurate results in most of the sequences under harsh situations with HDR or aggressive motion.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a comparison of methods (ORB-SLAM3 and ESVIO) in various sequences. No clear, verifiable dataset names are provided.",
      "processing_time": 41.00855207443237,
      "citing_paper_id": "255125395",
      "cited_paper_id": 3738244
    },
    {
      "context_text": "However, most of the recent research on stereo event cameras has focused on depth estimation and constructing semi-dense or dense maps [16], [17], with less research on VO/SLAM fields.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only general research areas and methods. No verifiable resources are identified.",
      "processing_time": 39.02849102020264,
      "citing_paper_id": "255125395",
      "cited_paper_id": 3738244
    },
    {
      "context_text": "However, most of the recent research on stereo event cameras has focused on depth estimation and constructing semi-dense or dense maps [16], [17], with less research on VO/SLAM fields.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only general research areas and methods. No verifiable resources are identified.",
      "processing_time": 39.02849102020264,
      "citing_paper_id": "255125395",
      "cited_paper_id": 249980412
    },
    {
      "context_text": "Our ESIO has good performance, especially for the sequence hku_agg_walk and hku_dark_normal, our ESIO still can produce reliable and accurate pose estimation even when the state-of-the-art image-based VIO method, ORB-SLAM3, fails.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.3,
      "reasoning": "The context mentions specific sequences (hku_agg_walk and hku_dark_normal) but does not provide enough information to determine if these are datasets or specific scenarios within a larger dataset. No clear dataset names are identified.",
      "processing_time": 41.5391001701355,
      "citing_paper_id": "255125395",
      "cited_paper_id": 3738244
    },
    {
      "context_text": "Observing this complementarity, leveraging both of the advantages of the aforementioned different sensors in combination with an inertial measurement unit (IMU) results in a robust and accurate visual-inertial odometry (VIO) pipeline [3], [4].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and systems. The context focuses on combining different sensors for visual-inertial odometry.",
      "processing_time": 40.306864500045776,
      "citing_paper_id": "255125395",
      "cited_paper_id": 3738244
    },
    {
      "context_text": "Ultimate SLAM [3] furthered the aforementioned research by combining event streams, image frames, and IMU measurements with nonlinear optimization, which leverages the complementary advantages of event cameras and standard cameras.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and approaches. The focus is on the combination of event streams, image frames, and IMU measurements.",
      "processing_time": 40.00389623641968,
      "citing_paper_id": "255125395",
      "cited_paper_id": 3738244
    },
    {
      "context_text": "[11] adopted a continuous-time framework based on cubic spline for smooth trajectory estimation and fused both event streams and IMU together.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for fusing event streams and IMU data.",
      "processing_time": 37.50580406188965,
      "citing_paper_id": "255125395",
      "cited_paper_id": 9729856
    },
    {
      "context_text": "[14] D. Gehrig, H. Rebecq, G. Gallego, and D. Scaramuzza, “EKLT: Asynchronous photometric feature tracking using events and frames,” Int.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for asynchronous photometric feature tracking.",
      "processing_time": 38.032395362854004,
      "citing_paper_id": "255125395",
      "cited_paper_id": 50775406
    },
    {
      "context_text": "EKLT-VIO [13] integrated an accurate state-of-the-art event-based feature tracker EKLT [14] with EKF backend to achieve event-based state estimation on Mars-like datasets.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.3,
      "reasoning": "The citation mentions 'Mars-like datasets' but does not provide a specific name for the dataset. The term 'Mars-like datasets' is too generic and lacks a clear identifier.",
      "processing_time": 41.41443610191345,
      "citing_paper_id": "255125395",
      "cited_paper_id": 50775406
    },
    {
      "context_text": "EVENT cameras are novel bio-inspired sensors [1], which have a high dynamic range (140 dB compared to 60 dB of standard cameras) to handle broad illumination conditions.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only describes properties of event cameras.",
      "processing_time": 36.46748995780945,
      "citing_paper_id": "255125395",
      "cited_paper_id": 118684904
    },
    {
      "context_text": "[10] obtains a discrete number of states based on a spatio-temporal window of event streams, and introduces virtual event frames to achieve nonlinear optimization that refines estimated poses.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for processing event streams. The cited paper title confirms it is about a method (ORB-SLAM3) rather than a dataset.",
      "processing_time": 41.051008462905884,
      "citing_paper_id": "255125395",
      "cited_paper_id": 220713377
    },
    {
      "context_text": "Sequence ORB-SLAM3 [26] Stereo VIO VINS-Fusion [22] Stereo VIO USLAM [10] Mono EIO USLAM [3] Mono EVIO PL-EVIO [4] Mono EVIO Our ESIO Stereo EIO Our ESIO+ Stereo EIO Our ESVIO Stereo EVIO",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods and systems. There are no clear identifiers for datasets within the provided context.",
      "processing_time": 39.95788502693176,
      "citing_paper_id": "255125395",
      "cited_paper_id": 220713377
    },
    {
      "context_text": "[28] M. Gehrig, W. Aarents, D. Gehrig, and D. Scaramuzza, “DSEC: A stereo event camera dataset for driving scenarios,” IEEE Robot.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "DSEC"
      ],
      "dataset_descriptions": {
        "DSEC": "Used to evaluate stereo event camera algorithms in driving scenarios, focusing on depth estimation under dynamic conditions."
      },
      "confidence_score": 1.0,
      "reasoning": "The citation clearly mentions 'DSEC' as a dataset for driving scenarios using stereo event cameras, which is directly relevant to the topic of event-based stereo depth estimation.",
      "processing_time": 43.23973369598389,
      "citing_paper_id": "255125395",
      "cited_paper_id": 232170230
    },
    {
      "context_text": "1) DSEC Dataset: DSEC [28] is collected by high-resolution stereo event cameras (640 × 480) under driving scenarios, which is challenging for event-based sensors, as forward motions typically produce considerably fewer events at the center.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "DSEC"
      ],
      "dataset_descriptions": {
        "DSEC": "Used to evaluate event-based stereo depth estimation in driving scenarios, focusing on the challenges posed by forward motion and sparse event generation at the image center."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions a specific dataset, DSEC, which is used for driving scenarios with stereo event cameras. The dataset is described as challenging for event-based sensors.",
      "processing_time": 43.954455852508545,
      "citing_paper_id": "255125395",
      "cited_paper_id": 232170230
    },
    {
      "context_text": "Since the DSEC dataset does not provide the ground truth 6-DoF poses, we only show the estimated trajectory and the tracking performance of our event-based and image-based features.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "DSEC"
      ],
      "dataset_descriptions": {
        "DSEC": "Used to evaluate the tracking performance of event-based and image-based features in driving scenarios, focusing on the estimated trajectory without ground truth 6-DoF poses."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions the DSEC dataset, which is used for evaluating the tracking performance of event-based and image-based features in driving scenarios.",
      "processing_time": 43.79295516014099,
      "citing_paper_id": "255125395",
      "cited_paper_id": 232170230
    },
    {
      "context_text": "[6] adopted stereo feature detection and matching with the geometry method, which adopts reprojection error minimization to achieve pose estimation.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and approaches. There are no clear identifiers for datasets in the provided context.",
      "processing_time": 39.75417232513428,
      "citing_paper_id": "255125395",
      "cited_paper_id": 235794981
    },
    {
      "context_text": "The VECtor [24] dataset consists of a hardwaresynchronized sensor suite that includes stereo event cameras, stereo standard cameras, an RGB-D sensor, a LiDAR, and an IMU.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "VECtor"
      ],
      "dataset_descriptions": {
        "VECtor": "Used to evaluate multi-sensor SLAM systems, focusing on the integration of stereo event cameras and other sensors for robust depth estimation."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions the VECtor dataset, which is a multi-sensor dataset including stereo event cameras, suitable for event-based stereo depth estimation.",
      "processing_time": 43.55301570892334,
      "citing_paper_id": "255125395",
      "cited_paper_id": 250127779
    },
    {
      "context_text": "The VECtor [24] dataset consists of a hardware-synchronized sensor suite that includes stereo event cameras, stereo standard cameras, an RGB-D sensor, a LiDAR, and an IMU.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "VECtor"
      ],
      "dataset_descriptions": {
        "VECtor": "Used to evaluate multi-sensor SLAM systems, focusing on the integration of stereo event cameras and other sensors for robust localization and mapping."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions the VECtor dataset, which is a multi-sensor dataset including stereo event cameras, suitable for event-based stereo depth estimation.",
      "processing_time": 43.631619930267334,
      "citing_paper_id": "255125395",
      "cited_paper_id": 250127779
    },
    {
      "context_text": "The accuracy is measured with mean position error (MPE, %) and mean rotation error (MRE, ◦/m) aligning the estimated trajectory with ground truth using 6-DOF transformation (in SE3), which is calculated by the tool [25].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific dataset, only tools and metrics. No dataset names are present in the citation span.",
      "processing_time": 38.3332154750824,
      "citing_paper_id": "255125395",
      "cited_paper_id": null
    },
    {
      "context_text": "While the traditional event-based methods [2] [3] [5] failed in most of the sequences in these two datasets.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.0,
      "reasoning": "The context mentions 'these two datasets' but does not provide specific names. Without more information, it is impossible to identify the datasets.",
      "processing_time": 38.96684670448303,
      "citing_paper_id": "255125395",
      "cited_paper_id": null
    },
    {
      "context_text": "Note that we also evaluate EVO [2] and ESVO [5] in our self-collected datasets, but they failed in all sequences.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.3,
      "reasoning": "The context mentions 'self-collected datasets' but does not provide a specific name or identifier. Therefore, it is considered too generic and not included.",
      "processing_time": 40.48072052001953,
      "citing_paper_id": "255125395",
      "cited_paper_id": null
    },
    {
      "context_text": "EVO [2] proposed a monocular event-based parallel tracking-and-mapping philosophy which applies the image-to-model alignment for tracking and Event-based Multi-View Stereo (EMVS) [8] for mapping.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and approaches. There are no clear identifiers for datasets in the provided context.",
      "processing_time": 40.24252533912659,
      "citing_paper_id": "255125395",
      "cited_paper_id": null
    },
    {
      "context_text": "Besides, we emphasize real-time performance when evaluating our methods, while the computational burden of EVO [2] and ESVO [3] is so large that we had to slow down the rosbag such as × 0 .",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and their computational requirements.",
      "processing_time": 36.870933294296265,
      "citing_paper_id": "255125395",
      "cited_paper_id": null
    },
    {
      "context_text": "Most of the existing event-based visual odometers (VO) use monocular event camera [2]–[4], while few research on visual odometry based on stereo event cameras [5] [6].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only references to research papers and general methods.",
      "processing_time": 37.446701526641846,
      "citing_paper_id": "255125395",
      "cited_paper_id": null
    },
    {
      "context_text": "However, the generalization capability of [2] and [5] is slightly poor.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, models, or methods. It only comments on the generalization capability of two unspecified works.",
      "processing_time": 40.47677278518677,
      "citing_paper_id": "255125395",
      "cited_paper_id": null
    },
    {
      "context_text": "The ability to process the stereo correspondence problem in real time, which solves the input data without storing it for later processing, is crucial in machine-vision systems such as robots and autonomous vehicles [1].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only discusses the importance of real-time processing in stereo matching for machine vision systems.",
      "processing_time": 39.93497967720032,
      "citing_paper_id": "254531210",
      "cited_paper_id": 458430
    },
    {
      "context_text": "The disparity maps were computed using frame-based approaches [1], [9].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only frame-based approaches for computing disparity maps. No verifiable resources are identified.",
      "processing_time": 40.23953628540039,
      "citing_paper_id": "254531210",
      "cited_paper_id": 458430
    },
    {
      "context_text": "The disparity maps were computed using frame-based approaches [1], [9].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only frame-based approaches for computing disparity maps. No verifiable resources are identified.",
      "processing_time": 40.23953628540039,
      "citing_paper_id": "254531210",
      "cited_paper_id": 1680724
    },
    {
      "context_text": "In [14], a time-correlated kernel was used for event-driven operations, but this method requires recalculating the synaptic weights for each event input, which leads to excessive computation.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for event-driven operations. No dataset names are present in the citation context.",
      "processing_time": 39.52422595024109,
      "citing_paper_id": "254531210",
      "cited_paper_id": 4833834
    },
    {
      "context_text": "In frame-based stereo systems, most stereo correspondence algorithms ﬁnd a matching point by comparing the similarity between the visual features of the left and right frame images and calculate the distance along the camera geometry through the disparity between the two matching points [2].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general review of stereo vision algorithms. No verifiable resources are identified.",
      "processing_time": 40.23651075363159,
      "citing_paper_id": "254531210",
      "cited_paper_id": 8688550
    },
    {
      "context_text": "A similar idea was adopted for event-based stereo systems in previous studies [3], [4], [5], [6].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to previous studies. No dataset names are provided in the context or titles.",
      "processing_time": 40.65496611595154,
      "citing_paper_id": "254531210",
      "cited_paper_id": 11395394
    },
    {
      "context_text": "A similar idea was adopted for event-based stereo systems in previous studies [3], [4], [5], [6].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to previous studies. No dataset names are provided in the context or titles.",
      "processing_time": 40.65496611595154,
      "citing_paper_id": "254531210",
      "cited_paper_id": 30913835
    },
    {
      "context_text": "The methods reported in [5] and [6] do not generate frame images to compute the matching cost but compare each event with the collected events of another DVS pair within a speciﬁed spatiotemporal window.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and algorithms. No verifiable resources are identified.",
      "processing_time": 39.25584530830383,
      "citing_paper_id": "254531210",
      "cited_paper_id": 11395394
    },
    {
      "context_text": "Thus, stereo systems utilizing DVSs have recently gained popularity and have been reported in the literature [3], [4], [5], [6], [7], [8].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to literature. The cited paper titles do not provide additional context about datasets.",
      "processing_time": 40.651593923568726,
      "citing_paper_id": "254531210",
      "cited_paper_id": 11395394
    },
    {
      "context_text": "Thus, stereo systems utilizing DVSs have recently gained popularity and have been reported in the literature [3], [4], [5], [6], [7], [8].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to literature. The cited paper titles do not provide additional context about datasets.",
      "processing_time": 40.651593923568726,
      "citing_paper_id": "254531210",
      "cited_paper_id": 30913835
    },
    {
      "context_text": "Thus, stereo systems utilizing DVSs have recently gained popularity and have been reported in the literature [3], [4], [5], [6], [7], [8].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to literature. The cited paper titles do not provide additional context about datasets.",
      "processing_time": 40.651593923568726,
      "citing_paper_id": "254531210",
      "cited_paper_id": 205698386
    },
    {
      "context_text": "Compared with other ASIC approaches [3], [17], our FPGA-based implementation achieved considerable accuracy and power performance with lesser hardware.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a comparison with other ASIC approaches and the performance of an FPGA-based implementation.",
      "processing_time": 39.253844022750854,
      "citing_paper_id": "254531210",
      "cited_paper_id": 30913835
    },
    {
      "context_text": "In [3], grayscale images were generated by collecting spike events over 20 ms.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific, verifiable dataset. It only describes a method for generating grayscale images from spike events.",
      "processing_time": 39.926740646362305,
      "citing_paper_id": "254531210",
      "cited_paper_id": 30913835
    },
    {
      "context_text": "Since the disparity data generated from SCN is frameless, the latency was estimated using the amount of SCN spikes and rate of change of edges as proposed in [7].",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific dataset names, only a method for estimating latency using SCN spikes and edge change rates.",
      "processing_time": 39.9256489276886,
      "citing_paper_id": "254531210",
      "cited_paper_id": 205698386
    },
    {
      "context_text": "In most previous studies, a processor was used for SCN design [7], [8], [11], [17].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to previous studies using processors for SCN design. No verifiable resources are identified.",
      "processing_time": 40.83426547050476,
      "citing_paper_id": "254531210",
      "cited_paper_id": 205698386
    },
    {
      "context_text": "In [7], another method was proposed for constructing the network, which separates the SCN into two layers as follows : When a coincidence neuron is activated, it excites the disparity-layer neurons according to the continuity constraint.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for constructing a network. No verifiable resources are identified.",
      "processing_time": 39.92385816574097,
      "citing_paper_id": "254531210",
      "cited_paper_id": 205698386
    },
    {
      "context_text": "In [7] and [8], a cooperative stereo algorithm employing a spiking-neuron model was presented.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method or model. The context focuses on a cooperative stereo algorithm using a spiking-neuron model.",
      "processing_time": 41.19821524620056,
      "citing_paper_id": "254531210",
      "cited_paper_id": 205698386
    },
    {
      "context_text": "Neu-romorphic computing systems utilizing ﬁeld-programmable analog array or ﬁeld-programmable gate array (FPGA) devices have been successfully demonstrated the advantages of asynchronous computations [26], [27], [28].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only hardware and computational methods. There are no clear identifiers for datasets in the text.",
      "processing_time": 40.73932862281799,
      "citing_paper_id": "254531210",
      "cited_paper_id": 232152677
    },
    {
      "context_text": "Inspired by convolutional RNNs [4, 29, 55] and continuous temporal dynamics of biological neuron models [18, 30], we propose a novel event processing method combining merits of both sides.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only references to methods and models. No verifiable resources are identified.",
      "processing_time": 40.45573568344116,
      "citing_paper_id": "250602271",
      "cited_paper_id": 485828
    },
    {
      "context_text": "Inspired by convolutional RNNs [4, 29, 55] and continuous temporal dynamics of biological neuron models [18, 30], we propose a novel event processing method combining merits of both sides.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only references to methods and models. No verifiable resources are identified.",
      "processing_time": 40.45573568344116,
      "citing_paper_id": "250602271",
      "cited_paper_id": 1753085
    },
    {
      "context_text": "However, fully-connected RNNs are not efficient for information extraction of images.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, models, or methods. It only discusses the inefficiency of fully-connected RNNs for image information extraction.",
      "processing_time": 41.194990396499634,
      "citing_paper_id": "250602271",
      "cited_paper_id": 485828
    },
    {
      "context_text": "An alternative encoding method for events is to use recurrent neural networks (RNNs), given their inherent ability to encode temporal sequences.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a method (RNNs) for encoding events. No verifiable resources are identified.",
      "processing_time": 40.8244571685791,
      "citing_paper_id": "250602271",
      "cited_paper_id": 485828
    },
    {
      "context_text": "Different embodiments of RCNN include convolutional long short-term memory (ConvLSTM) [55] and convolutional gated recurrent units (ConvGRU) [4], where additional gating variables were used for memory modulation.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models and methods. No dataset names are present in the citation span.",
      "processing_time": 39.5040488243103,
      "citing_paper_id": "250602271",
      "cited_paper_id": 485828
    },
    {
      "context_text": "A natural thought is to incorporate RNNs into convolutional operations.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only a methodological idea about incorporating RNNs into convolutional operations.",
      "processing_time": 40.729599475860596,
      "citing_paper_id": "250602271",
      "cited_paper_id": 485828
    },
    {
      "context_text": "Different from traditional RNNs constructed with artificial neurons, spiking neural networks (SNNs) [30] uses spiking neuron models inspired by biology with inherent self-recurrence.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (spiking neural networks).",
      "processing_time": 39.49811387062073,
      "citing_paper_id": "250602271",
      "cited_paper_id": 485828
    },
    {
      "context_text": "Different from traditional RNNs constructed with artificial neurons, spiking neural networks (SNNs) [30] uses spiking neuron models inspired by biology with inherent self-recurrence.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (spiking neural networks).",
      "processing_time": 39.49811387062073,
      "citing_paper_id": "250602271",
      "cited_paper_id": 1753085
    },
    {
      "context_text": "There has been an increasing number of applications of SNNs in deep learning [5, 12, 21, 22, 28, 38, 45, 50, 54, 60], and the network’s asynchronous nature makes it an ideal solution for event-based tasks [7, 8, 23, 27, 41, 57].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general applications of SNNs and their suitability for event-based tasks. No verifiable resources are identified.",
      "processing_time": 41.361401319503784,
      "citing_paper_id": "250602271",
      "cited_paper_id": 1234009
    },
    {
      "context_text": "There has been an increasing number of applications of SNNs in deep learning [5, 12, 21, 22, 28, 38, 45, 50, 54, 60], and the network’s asynchronous nature makes it an ideal solution for event-based tasks [7, 8, 23, 27, 41, 57].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general applications of SNNs and their suitability for event-based tasks. No verifiable resources are identified.",
      "processing_time": 41.361401319503784,
      "citing_paper_id": "250602271",
      "cited_paper_id": 203593170
    },
    {
      "context_text": "There has been an increasing number of applications of SNNs in deep learning [5, 12, 21, 22, 28, 38, 45, 50, 54, 60], and the network’s asynchronous nature makes it an ideal solution for event-based tasks [7, 8, 23, 27, 41, 57].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general applications of SNNs and their suitability for event-based tasks. No verifiable resources are identified.",
      "processing_time": 41.361401319503784,
      "citing_paper_id": "250602271",
      "cited_paper_id": 211258776
    },
    {
      "context_text": "There has been an increasing number of applications of SNNs in deep learning [5, 12, 21, 22, 28, 38, 45, 50, 54, 60], and the network’s asynchronous nature makes it an ideal solution for event-based tasks [7, 8, 23, 27, 41, 57].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general applications of SNNs and their suitability for event-based tasks. No verifiable resources are identified.",
      "processing_time": 41.361401319503784,
      "citing_paper_id": "250602271",
      "cited_paper_id": 213704910
    },
    {
      "context_text": "There has been an increasing number of applications of SNNs in deep learning [5, 12, 21, 22, 28, 38, 45, 50, 54, 60], and the network’s asynchronous nature makes it an ideal solution for event-based tasks [7, 8, 23, 27, 41, 57].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general applications of SNNs and their suitability for event-based tasks. No verifiable resources are identified.",
      "processing_time": 41.361401319503784,
      "citing_paper_id": "250602271",
      "cited_paper_id": 226976144
    },
    {
      "context_text": "There has been an increasing number of applications of SNNs in deep learning [5, 12, 21, 22, 28, 38, 45, 50, 54, 60], and the network’s asynchronous nature makes it an ideal solution for event-based tasks [7, 8, 23, 27, 41, 57].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general applications of SNNs and their suitability for event-based tasks. No verifiable resources are identified.",
      "processing_time": 41.361401319503784,
      "citing_paper_id": "250602271",
      "cited_paper_id": 247675601
    },
    {
      "context_text": "[39] applied a modified version of LSTM [19] to event-based recognition, but the model was not specifically designed to preserve spatial information.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (LSTM) used for event-based recognition. No datasets are referenced for extraction.",
      "processing_time": 40.72785019874573,
      "citing_paper_id": "250602271",
      "cited_paper_id": 1561703
    },
    {
      "context_text": "To our best knowledge, our models are the first to perform streaming experiments for dense disparity estimation on the MVSEC dataset.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "MVSEC"
      ],
      "dataset_descriptions": {
        "MVSEC": "Used for streaming experiments in dense disparity estimation, focusing on 3D perception tasks with event cameras."
      },
      "confidence_score": 1.0,
      "reasoning": "The citation mentions the MVSEC dataset, which is a specific, verifiable dataset used for 3D perception tasks, particularly dense disparity estimation in streaming experiments.",
      "processing_time": 43.671939849853516,
      "citing_paper_id": "250602271",
      "cited_paper_id": 3416874
    },
    {
      "context_text": "We demonstrated the superiority of our model over existing state-of-the-art works on both the MVSEC and the DSEC dataset.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "MVSEC",
        "DSEC"
      ],
      "dataset_descriptions": {
        "MVSEC": "Used to evaluate the model's performance in 3D perception tasks using event camera data, focusing on stereo depth estimation.",
        "DSEC": "Used to assess the model's effectiveness in stereo depth estimation with event camera data, providing a benchmark against state-of-the-art methods."
      },
      "confidence_score": 1.0,
      "reasoning": "The citation mentions two specific datasets, MVSEC and DSEC, which are used to evaluate the performance of the model.",
      "processing_time": 45.86124777793884,
      "citing_paper_id": "250602271",
      "cited_paper_id": 3416874
    },
    {
      "context_text": "We demonstrated their advantages over other methods on the event-based stereo matching task on the MVSEC dataset.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "MVSEC"
      ],
      "dataset_descriptions": {
        "MVSEC": "Used to evaluate event-based stereo matching methods, focusing on 3D perception tasks with event camera data."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions the use of the MVSEC dataset for evaluating event-based stereo matching methods. The cited paper title confirms that MVSEC is a dataset.",
      "processing_time": 43.58681869506836,
      "citing_paper_id": "250602271",
      "cited_paper_id": 3416874
    },
    {
      "context_text": "We split and preprocess the Indoor Flying dataset from the MVSEC using the same setting as [2, 51, 62].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Indoor Flying dataset",
        "MVSEC"
      ],
      "dataset_descriptions": {
        "Indoor Flying dataset": "Used for preprocessing and splitting in event-based stereo depth estimation, focusing on indoor flying scenarios with event cameras.",
        "MVSEC": "Used for preprocessing and splitting in event-based stereo depth estimation, providing a multivehicle stereo event camera dataset."
      },
      "confidence_score": 0.9,
      "reasoning": "The context mentions the 'Indoor Flying dataset' from the 'MVSEC'. The cited papers confirm that MVSEC is a dataset, and the context indicates it is used for preprocessing and splitting.",
      "processing_time": 46.43142223358154,
      "citing_paper_id": "250602271",
      "cited_paper_id": 3416874
    },
    {
      "context_text": "We split and preprocess the Indoor Flying dataset from the MVSEC using the same setting as [2, 51, 62].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Indoor Flying dataset",
        "MVSEC"
      ],
      "dataset_descriptions": {
        "Indoor Flying dataset": "Used for preprocessing and splitting in event-based stereo depth estimation, focusing on indoor flying scenarios with event cameras.",
        "MVSEC": "Used for preprocessing and splitting in event-based stereo depth estimation, providing a multivehicle stereo event camera dataset."
      },
      "confidence_score": 0.9,
      "reasoning": "The context mentions the 'Indoor Flying dataset' from the 'MVSEC'. The cited papers confirm that MVSEC is a dataset, and the context indicates it is used for preprocessing and splitting.",
      "processing_time": 46.43142223358154,
      "citing_paper_id": "250602271",
      "cited_paper_id": 4412139
    },
    {
      "context_text": "We conduct our experiments on the MVSEC dataset [63], which contains depth information collected by a LIDAR sensor and event streams obtained from two event cameras with corresponding 20 Hz intensity images at 346×260 resolution.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "MVSEC"
      ],
      "dataset_descriptions": {
        "MVSEC": "Used for experiments involving depth information from a LIDAR sensor and event streams from two event cameras, with 20 Hz intensity images at 346×260 resolution."
      },
      "confidence_score": 1.0,
      "reasoning": "The context clearly mentions the MVSEC dataset, which is used for experiments involving depth information and event streams from event cameras. The dataset is specific and relevant to the research topic.",
      "processing_time": 45.308183431625366,
      "citing_paper_id": "250602271",
      "cited_paper_id": 3416874
    },
    {
      "context_text": "We demonstrate the advantage of CTC and DTC over other event encoding methods on a set of criteria of event-based stereo matching, on the Multi Vehicle Stereo Event Camera (MVSEC) [63] dataset.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Multi Vehicle Stereo Event Camera (MVSEC)"
      ],
      "dataset_descriptions": {
        "Multi Vehicle Stereo Event Camera (MVSEC)": "Used to evaluate event-based stereo matching methods, focusing on criteria such as accuracy and efficiency in 3D perception tasks."
      },
      "confidence_score": 1.0,
      "reasoning": "The MVSEC dataset is explicitly mentioned and is used for evaluating event-based stereo matching methods. The dataset is relevant to the research topic.",
      "processing_time": 44.67764401435852,
      "citing_paper_id": "250602271",
      "cited_paper_id": 3416874
    },
    {
      "context_text": "It also decreases data storage space for potential hardware applications, similar approaches were taken in [62].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general approach to reducing data storage space.",
      "processing_time": 39.895346879959106,
      "citing_paper_id": "250602271",
      "cited_paper_id": 4412139
    },
    {
      "context_text": "TSES [62] utilized the velocity of the camera to approximate optical flow and build time synchronized event disparity volumes.",
      "catation_intent": "reusable resource",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.3,
      "reasoning": "The citation mentions 'TSES' which appears to be a dataset or method used for building time synchronized event disparity volumes. However, without more context, it is unclear if TSES is a dataset or a method.",
      "processing_time": 42.19433259963989,
      "citing_paper_id": "250602271",
      "cited_paper_id": 4412139
    },
    {
      "context_text": "The so-called handcrafted methods [25, 31, 35, 36, 40, 53, 58, 64] directly convert events to event frames based on the four dimensional information of each event.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only methods for converting events to event frames. No verifiable resources are identified.",
      "processing_time": 40.89537763595581,
      "citing_paper_id": "250602271",
      "cited_paper_id": 13373696
    },
    {
      "context_text": "The so-called handcrafted methods [25, 31, 35, 36, 40, 53, 58, 64] directly convert events to event frames based on the four dimensional information of each event.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only methods for converting events to event frames. No verifiable resources are identified.",
      "processing_time": 40.89537763595581,
      "citing_paper_id": "250602271",
      "cited_paper_id": 52814827
    },
    {
      "context_text": "[25, 31] stored histograms of events of different polarities in different channels to avoid information loss due to polarity cancellation.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a method for storing event histograms. No clear identifiers for datasets are present.",
      "processing_time": 40.19659781455994,
      "citing_paper_id": "250602271",
      "cited_paper_id": 13373696
    },
    {
      "context_text": "An extension of our principles to SNNs implemented in neuromorphic hardware [10, 11, 14, 24, 33, 43] could further lead to super fast event-based stereo system [3].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only references to neuromorphic hardware and spiking neural networks. No verifiable datasets are identified.",
      "processing_time": 41.346402168273926,
      "citing_paper_id": "250602271",
      "cited_paper_id": 25268038
    },
    {
      "context_text": "An extension of our principles to SNNs implemented in neuromorphic hardware [10, 11, 14, 24, 33, 43] could further lead to super fast event-based stereo system [3].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only references to neuromorphic hardware and spiking neural networks. No verifiable datasets are identified.",
      "processing_time": 41.346402168273926,
      "citing_paper_id": "250602271",
      "cited_paper_id": 119297695
    },
    {
      "context_text": "An extension of our principles to SNNs implemented in neuromorphic hardware [10, 11, 14, 24, 33, 43] could further lead to super fast event-based stereo system [3].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only references to neuromorphic hardware and spiking neural networks. No verifiable datasets are identified.",
      "processing_time": 41.346402168273926,
      "citing_paper_id": "250602271",
      "cited_paper_id": 235078812
    },
    {
      "context_text": "[65] created voxel grids by interpolation based on timestamps of events.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific dataset names, only a method for creating voxel grids from event timestamps.",
      "processing_time": 39.21417760848999,
      "citing_paper_id": "250602271",
      "cited_paper_id": 56475917
    },
    {
      "context_text": "Most of the works in stereo matching based on deep learning are established on image datasets [20, 32, 52, 56, 59].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions 'image datasets' but does not specify any particular dataset names. The cited paper titles do not provide additional dataset names either.",
      "processing_time": 41.07601809501648,
      "citing_paper_id": "250602271",
      "cited_paper_id": 119304432
    },
    {
      "context_text": "Most of the works in stereo matching based on deep learning are established on image datasets [20, 32, 52, 56, 59].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions 'image datasets' but does not specify any particular dataset names. The cited paper titles do not provide additional dataset names either.",
      "processing_time": 41.07601809501648,
      "citing_paper_id": "250602271",
      "cited_paper_id": 216036364
    },
    {
      "context_text": "The model can be viewed as a non-spiking form of the leaky integrate and fire (LIF) neuron with conductance synapse [44], without specifically defining its synaptic dynamics.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a model or method. There are no verifiable resources that meet the criteria for inclusion.",
      "processing_time": 41.166125774383545,
      "citing_paper_id": "250602271",
      "cited_paper_id": 196016124
    },
    {
      "context_text": "[9] used time-surface with linear time decay to construct event images.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for constructing event images.",
      "processing_time": 38.27603054046631,
      "citing_paper_id": "250602271",
      "cited_paper_id": 211126617
    },
    {
      "context_text": "[34, 47] applied specially designed asynchronous convolution for sparse events data.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions 'sparse events data' which is relevant to event-based stereo depth estimation, but does not specify a dataset name. The cited paper title confirms the focus on event-based data but does not mention a specific dataset.",
      "processing_time": 42.181432008743286,
      "citing_paper_id": "250602271",
      "cited_paper_id": 214605597
    },
    {
      "context_text": "LTC further enhances its ability by integrating f into the time constant of the system:\ndx(t)\ndt = −\n[ 1\nτ + f(x(t), I(t), t, θ)\n] x(t)+\nf(x(t), I(t), t, θ)A\n(3)\nwhere the system time constant becomes an inputdependent term τ1+τf(x(t),I(t),t,θ) and A is a scale parameter.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The provided context does not mention any specific datasets, only a mathematical model and equations. There are no verifiable resources or datasets mentioned.",
      "processing_time": 41.16348338127136,
      "citing_paper_id": "250602271",
      "cited_paper_id": 222319014
    },
    {
      "context_text": "The liquid time-constant network (LTC) [18,26], an expansion of the continuous time RNN [13], circumvents this problem by using continuous valued activation functions for its neuron, whose dynamics is modulated by an inputdependent system time constant.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models and methods. The context is focused on describing the LTC network and its relation to continuous time RNNs.",
      "processing_time": 41.671794414520264,
      "citing_paper_id": "250602271",
      "cited_paper_id": 222319014
    },
    {
      "context_text": "We term both convolution LTC (convLTC) and convolution LTC without reversal potential (convLTCOR) as continuous time convolution (CTC).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only model architectures. There are no verifiable resources or datasets mentioned.",
      "processing_time": 40.79272484779358,
      "citing_paper_id": "250602271",
      "cited_paper_id": 222319014
    },
    {
      "context_text": "Note that for CTC, we use simulation results from the convLTCOR model.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions 'simulation results' but does not specify a dataset. The term 'convLTCOR model' is a method, not a dataset.",
      "processing_time": 41.15810251235962,
      "citing_paper_id": "250602271",
      "cited_paper_id": 222319014
    },
    {
      "context_text": "The dynamics of the convLTCOR model is mainly characterized by its membrane time constant τm, abstracted from this intuition, we develop the discrete time convolution model (DTC), formulated as:\nxtcij = σ(τcx t−1 cij + Icij(t)) (9)\nwhere Icij(t) is defined the same as in Eq.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The provided context does not mention any specific datasets, only a model formulation. There are no clear identifiers for datasets or other verifiable resources.",
      "processing_time": 41.239895820617676,
      "citing_paper_id": "250602271",
      "cited_paper_id": 222319014
    },
    {
      "context_text": "The LTC network [18] is an expansion of continuoustime RNN (CT-RNN) [13], which can be described by an ordinary differential equation (ODE):\ndx(t) dt = −x(t) τ + f(x(t), I(t), t, θ) (2)\nwhere τ characterizes the speed and the coupling sensitivity of the dynamical system, x(t) is the hidden state, I(t) is the input, t represents time and f is a neural network parameterized by θ.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models and equations. There are no verifiable resources that meet the criteria.",
      "processing_time": 40.599181175231934,
      "citing_paper_id": "250602271",
      "cited_paper_id": 222319014
    },
    {
      "context_text": "In the fully connected structure, the synaptic input of an LTC neuron contains inputs from all the other neurons.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, models, or methods. It only describes a neural network structure.",
      "processing_time": 39.876508951187134,
      "citing_paper_id": "250602271",
      "cited_paper_id": 222319014
    },
    {
      "context_text": "The LTC network was only applied for low dimensional temporal sequence modeling.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (LTC network) and its application. There are no verifiable resources or datasets mentioned.",
      "processing_time": 41.578832387924194,
      "citing_paper_id": "250602271",
      "cited_paper_id": 222319014
    },
    {
      "context_text": "Empirically we found that the training of the convLTC model was unstable, during which gradients sometimes tended to vanish.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (convLTC model).",
      "processing_time": 40.40957164764404,
      "citing_paper_id": "250602271",
      "cited_paper_id": 222319014
    },
    {
      "context_text": "The resulting convolution LTC neuron and its simplified version can be formulated as:\ndxcij(t)\ndt =−\n[ 1\nτm,c +\nIcij(t)\nCm,c\n] xcij(t)\n+ Icij(t)\nCm,c Erev,c + Eleak,c τm,c\n(6)\ndxcij(t)\ndt = Eleak,c − xcij(t) τm,c + Icij(t) Cm,c (7) Icij(t) = ∑ h ∑ k wchkP t h+i,k+j (8)\nwith Wg(t) in the previous section specified by Icij(t), which represents the convolution input on channel c at location i, j from the event frame pre-processed by SBT, h and k are spatial coordinates on the input plane.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The provided context does not mention any specific datasets, only mathematical formulations and neural circuit policies. No verifiable resources are identified.",
      "processing_time": 41.05548667907715,
      "citing_paper_id": "250602271",
      "cited_paper_id": 222319014
    },
    {
      "context_text": "However, the LTC was only applied for low dimensional temporal sequence modeling and it lacks the ability to encode high dimensional spatial features.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (LTC) and its limitations. There are no verifiable resources or datasets mentioned.",
      "processing_time": 41.49078106880188,
      "citing_paper_id": "250602271",
      "cited_paper_id": 222319014
    },
    {
      "context_text": "The output of the LTC neuron is normalized by a parametrical sigmoid function σ(xcij) = 1/(1 + exp(γc(μc − xcij))), where γc and μc are trainable parameters that scale and shift xcij .",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only a mathematical function used in a neural network model.",
      "processing_time": 40.6862416267395,
      "citing_paper_id": "250602271",
      "cited_paper_id": 222319014
    },
    {
      "context_text": "In the original work, the LTC neuron was evolved at a frequency six times higher than the input sampling rate, leading to a six times slower output rate for an equal temporal span of the input.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only discusses the evolution of the LTC neuron in relation to input sampling rates.",
      "processing_time": 41.05327296257019,
      "citing_paper_id": "250602271",
      "cited_paper_id": 222319014
    },
    {
      "context_text": "We develop continuous time convolution (CTC), an expansion of LTC, for encoding high dimensional spatial-temporal data.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (CTC) for processing high-dimensional spatial-temporal data.",
      "processing_time": 41.05242347717285,
      "citing_paper_id": "250602271",
      "cited_paper_id": 222319014
    },
    {
      "context_text": "Inspired by recent studies [2,6,42,48], we further develop a dual-path structure for feature embedding fused by SPADE with multi-scale dilated convolution.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and models. The context focuses on the development of a dual-path structure for feature embedding.",
      "processing_time": 41.485530853271484,
      "citing_paper_id": "250602271",
      "cited_paper_id": 231759393
    },
    {
      "context_text": "[59, 57] propose a cost function for the rotating stereo panorama setup in [7] based on temporal event difference.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method or approach for stereo vision using events. The context and titles do not provide specific dataset names.",
      "processing_time": 41.314746618270874,
      "citing_paper_id": "46937991",
      "cited_paper_id": 185541
    },
    {
      "context_text": "[59, 57] propose a cost function for the rotating stereo panorama setup in [7] based on temporal event difference.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method or approach for stereo vision using events. The context and titles do not provide specific dataset names.",
      "processing_time": 41.314746618270874,
      "citing_paper_id": "46937991",
      "cited_paper_id": 8415966
    },
    {
      "context_text": "[59, 57] propose a cost function for the rotating stereo panorama setup in [7] based on temporal event difference.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method or approach for stereo vision using events. The context and titles do not provide specific dataset names.",
      "processing_time": 41.314746618270874,
      "citing_paper_id": "46937991",
      "cited_paper_id": 21539113
    },
    {
      "context_text": "However, most of the recent event-based implementations of the cooperative algorithm do not consider depth gradients [47, 48, 23, 17].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to papers discussing event-based implementations of cooperative algorithms in stereo vision.",
      "processing_time": 41.22252917289734,
      "citing_paper_id": "46937991",
      "cited_paper_id": 6079544
    },
    {
      "context_text": "However, most of the recent event-based implementations of the cooperative algorithm do not consider depth gradients [47, 48, 23, 17].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to papers discussing event-based implementations of cooperative algorithms in stereo vision.",
      "processing_time": 41.22252917289734,
      "citing_paper_id": "46937991",
      "cited_paper_id": 34855834
    },
    {
      "context_text": "However, most of the recent event-based implementations of the cooperative algorithm do not consider depth gradients [47, 48, 23, 17].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to papers discussing event-based implementations of cooperative algorithms in stereo vision.",
      "processing_time": 41.22252917289734,
      "citing_paper_id": "46937991",
      "cited_paper_id": 121601380
    },
    {
      "context_text": "CNNs [35] have been used to learn stereo matching cost [66, 46].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets. It only refers to the use of CNNs for stereo matching cost, which is a methodological approach rather than a dataset.",
      "processing_time": 42.069857597351074,
      "citing_paper_id": "46937991",
      "cited_paper_id": 6913648
    },
    {
      "context_text": "CNNs [35] have been used to learn stereo matching cost [66, 46].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets. It only refers to the use of CNNs for stereo matching cost, which is a methodological approach rather than a dataset.",
      "processing_time": 42.069857597351074,
      "citing_paper_id": "46937991",
      "cited_paper_id": 14542261
    },
    {
      "context_text": "[7] use a rotating pair of event-based line (vertical) sensors in static scenes and render events from each rotation to an edge map [33], which is subsequently processed using a frame-based panoramic stereo algorithm [36].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and tools. The context describes a method for processing events from event-based sensors.",
      "processing_time": 41.04556393623352,
      "citing_paper_id": "46937991",
      "cited_paper_id": 8415966
    },
    {
      "context_text": "While the successful artificial neural networks may not operate the same way as the brain, both of them utilize highly parallel and hierarchical architectures that gradually abstract input data to more meaningful concepts [8, 51, 16].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only discusses the architectural similarities between artificial neural networks and the brain.",
      "processing_time": 40.94937300682068,
      "citing_paper_id": "46937991",
      "cited_paper_id": 8920227
    },
    {
      "context_text": "The proposed method and its FPGA implementations [20, 19] are equivalent to the cooperative stereo algorithm [42] with noisy time difference inputs.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only algorithms and hardware implementations.",
      "processing_time": 40.15825891494751,
      "citing_paper_id": "46937991",
      "cited_paper_id": 11395394
    },
    {
      "context_text": "This consists of systems of equations defining the behavior of TrueNorth neurons, encased in modules called corelets [1], and the subsequent composition of the inputs and outputs of these modules.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method or system (TrueNorth neurons and corelets).",
      "processing_time": 41.03886413574219,
      "citing_paper_id": "46937991",
      "cited_paper_id": 11759366
    },
    {
      "context_text": "TrueNorth programs are written in the Corelet Programming Language — a hierarchical, compositional, object-oriented language [1].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any datasets, only a programming language. There are no verifiable resources that meet the criteria.",
      "processing_time": 41.12810206413269,
      "citing_paper_id": "46937991",
      "cited_paper_id": 11759366
    },
    {
      "context_text": "For autonomous vehicles, drones, and satellites, energy consumption is a challenge [6].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general challenge related to energy consumption in autonomous vehicles, drones, and satellites.",
      "processing_time": 41.12697505950928,
      "citing_paper_id": "46937991",
      "cited_paper_id": 12248226
    },
    {
      "context_text": "Ground truth disparity maps from benchmark frame-based datasets [27, 54, 26, 43] are used to train these Figure 1.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.3,
      "reasoning": "The context mentions 'benchmark frame-based datasets' which are used to train models for disparity maps. However, no specific dataset names are provided in the citation span.",
      "processing_time": 42.05972671508789,
      "citing_paper_id": "46937991",
      "cited_paper_id": 12986049
    },
    {
      "context_text": "Ground truth disparity maps from benchmark frame-based datasets [27, 54, 26, 43] are used to train these Figure 1.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.3,
      "reasoning": "The context mentions 'benchmark frame-based datasets' which are used to train models for disparity maps. However, no specific dataset names are provided in the citation span.",
      "processing_time": 42.05972671508789,
      "citing_paper_id": "46937991",
      "cited_paper_id": 14915763
    },
    {
      "context_text": "Mahowald and Delbrück [41] implemented the Marr and Poggio cooperative stereo algorithm [42], a global approach, in an analog VLSI circuit.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method and its implementation.",
      "processing_time": 39.84578585624695,
      "citing_paper_id": "46937991",
      "cited_paper_id": 15077875
    },
    {
      "context_text": "Mahowald and Delbrück [41] implemented the Marr and Poggio cooperative stereo algorithm [42], a global approach, in an analog VLSI circuit.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method and its implementation.",
      "processing_time": 40.66684627532959,
      "citing_paper_id": "46937991",
      "cited_paper_id": 15077875
    },
    {
      "context_text": "[60, 58] propose DSP implementation of a spatiotemporal similarity method using two live event sensors [37].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and systems. The context is about implementing a spatiotemporal similarity method using event sensors, which does not indicate the use of a specific dataset.",
      "processing_time": 43.04755878448486,
      "citing_paper_id": "46937991",
      "cited_paper_id": 15357188
    },
    {
      "context_text": "[60, 58] propose DSP implementation of a spatiotemporal similarity method using two live event sensors [37].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and systems. The context is about implementing a spatiotemporal similarity method using event sensors, which does not indicate the use of a specific dataset.",
      "processing_time": 43.04755878448486,
      "citing_paper_id": "46937991",
      "cited_paper_id": 24236495
    },
    {
      "context_text": "[60, 58] propose DSP implementation of a spatiotemporal similarity method using two live event sensors [37].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and systems. The context is about implementing a spatiotemporal similarity method using event sensors, which does not indicate the use of a specific dataset.",
      "processing_time": 43.04755878448486,
      "citing_paper_id": "46937991",
      "cited_paper_id": 30913835
    },
    {
      "context_text": "Recently developed event-based cameras such as Dynamic Vision Sensor (DVS) [37, 10] and ATIS [50], inspired by the biological retina, encode pixel illumination changes as events.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only event-based cameras. No verifiable resources are identified.",
      "processing_time": 41.46456289291382,
      "citing_paper_id": "46937991",
      "cited_paper_id": 15357188
    },
    {
      "context_text": "Recently developed event-based cameras such as Dynamic Vision Sensor (DVS) [37, 10] and ATIS [50], inspired by the biological retina, encode pixel illumination changes as events.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only event-based cameras. No verifiable resources are identified.",
      "processing_time": 41.46456289291382,
      "citing_paper_id": "46937991",
      "cited_paper_id": 24007071
    },
    {
      "context_text": "Using a cluster of TrueNorth neurosynaptic processors, we demonstrate their ability to process bilateral event-based inputs streamed live by Dynamic Vision Sensors (DVS), at up to 2,000 disparity maps per second, producing high fidelity disparities which are in turn used to reconstruct, at low power, the depth of events produced from rapidly changing scenes.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific, verifiable datasets. It describes the use of hardware and sensors but does not reference a named dataset.",
      "processing_time": 42.139352798461914,
      "citing_paper_id": "46937991",
      "cited_paper_id": 15357188
    },
    {
      "context_text": "Using a cluster of TrueNorth neurosynaptic processors, we demonstrate their ability to process bilateral event-based inputs streamed live by Dynamic Vision Sensors (DVS), at up to 2,000 disparity maps per second, producing high fidelity disparities which are in turn used to reconstruct, at low power, the depth of events produced from rapidly changing scenes.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific, verifiable datasets. It describes the use of hardware and sensors but does not reference a named dataset.",
      "processing_time": 42.139352798461914,
      "citing_paper_id": "46937991",
      "cited_paper_id": 24007071
    },
    {
      "context_text": "The proposed event-based disparity method is implemented using a stereo pair of DAVIS sensors [10] (a version of DVS) and nine TrueNorth NS1e boards [53].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions hardware components (DAVIS sensors and TrueNorth NS1e boards) but does not refer to any specific datasets. The cited papers are about sensor technology, not datasets.",
      "processing_time": 43.126853704452515,
      "citing_paper_id": "46937991",
      "cited_paper_id": 15357188
    },
    {
      "context_text": "The proposed event-based disparity method is implemented using a stereo pair of DAVIS sensors [10] (a version of DVS) and nine TrueNorth NS1e boards [53].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions hardware components (DAVIS sensors and TrueNorth NS1e boards) but does not refer to any specific datasets. The cited papers are about sensor technology, not datasets.",
      "processing_time": 43.126853704452515,
      "citing_paper_id": "46937991",
      "cited_paper_id": 24007071
    },
    {
      "context_text": "A live-feed version of the system running on nine TrueNorth chips is shown to calculate 400 disparity maps per second, and the ability to increase this up to 2,000 disparities per second (subject to certain trade-offs) is demonstrated, for use with high speed event cameras, such as DVS.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only hardware components and performance metrics. No verifiable resources are identified.",
      "processing_time": 41.88089084625244,
      "citing_paper_id": "46937991",
      "cited_paper_id": 15357188
    },
    {
      "context_text": "A live-feed version of the system running on nine TrueNorth chips is shown to calculate 400 disparity maps per second, and the ability to increase this up to 2,000 disparities per second (subject to certain trade-offs) is demonstrated, for use with high speed event cameras, such as DVS.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only hardware components and performance metrics. No verifiable resources are identified.",
      "processing_time": 41.88089084625244,
      "citing_paper_id": "46937991",
      "cited_paper_id": 24007071
    },
    {
      "context_text": "When the data in a cycle is sparse, as is the case with a DVS sensor, most neurons would not compute for most of the time, resulting in low active power [44].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only discusses the sparsity of data from a DVS sensor. No verifiable resources are identified.",
      "processing_time": 42.63195180892944,
      "citing_paper_id": "46937991",
      "cited_paper_id": 15357188
    },
    {
      "context_text": "When the data in a cycle is sparse, as is the case with a DVS sensor, most neurons would not compute for most of the time, resulting in low active power [44].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only discusses the sparsity of data from a DVS sensor. No verifiable resources are identified.",
      "processing_time": 42.63195180892944,
      "citing_paper_id": "46937991",
      "cited_paper_id": 24007071
    },
    {
      "context_text": "Experiments on real-world sequences demonstrate the ability of the system to take full advantage of the asynchronous and sparse nature of DVS sensors for low power depth reconstruction, in environments where conventional frame-based cameras connected to synchronous processors would be inefficient for rapidly moving objects.",
      "catation_intent": "reusable resource",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only describes experiments using real-world sequences. No clear, verifiable dataset names are provided.",
      "processing_time": 42.717302560806274,
      "citing_paper_id": "46937991",
      "cited_paper_id": 15357188
    },
    {
      "context_text": "Experiments on real-world sequences demonstrate the ability of the system to take full advantage of the asynchronous and sparse nature of DVS sensors for low power depth reconstruction, in environments where conventional frame-based cameras connected to synchronous processors would be inefficient for rapidly moving objects.",
      "catation_intent": "reusable resource",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only describes experiments using real-world sequences. No clear, verifiable dataset names are provided.",
      "processing_time": 42.717302560806274,
      "citing_paper_id": "46937991",
      "cited_paper_id": 24007071
    },
    {
      "context_text": "The system is highly parameterized and can operate with other event based sensors such as ATIS [50] or DVS [37].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions 'ATIS' and 'DVS' as event-based sensors but does not indicate their use as datasets. These are likely sensor types or models, not datasets.",
      "processing_time": 43.36410975456238,
      "citing_paper_id": "46937991",
      "cited_paper_id": 15357188
    },
    {
      "context_text": "The system is highly parameterized and can operate with other event based sensors such as ATIS [50] or DVS [37].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions 'ATIS' and 'DVS' as event-based sensors but does not indicate their use as datasets. These are likely sensor types or models, not datasets.",
      "processing_time": 43.36410975456238,
      "citing_paper_id": "46937991",
      "cited_paper_id": 24007071
    },
    {
      "context_text": "[52, 14] propose to use event-toevent constraints for calculating matching cost, such as time window, distance to the epipolar line, ordering constraint, and polarity.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and approaches for event-based stereo depth estimation.",
      "processing_time": 42.377416133880615,
      "citing_paper_id": "46937991",
      "cited_paper_id": 16160208
    },
    {
      "context_text": "The main advantages of the proposed method, compared to the related work [17, 49, 45, 52, 57], are simultaneous end-to-end neuromorphic disparity calculation, low power, high throughput, low latency (9-11 ms), and linear scalability to multiple neuromorphic processors for larger input sizes.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods and performance metrics. The cited paper titles do not provide additional context about datasets.",
      "processing_time": 43.11748552322388,
      "citing_paper_id": "46937991",
      "cited_paper_id": 21539113
    },
    {
      "context_text": "The main advantages of the proposed method, compared to the related work [17, 49, 45, 52, 57], are simultaneous end-to-end neuromorphic disparity calculation, low power, high throughput, low latency (9-11 ms), and linear scalability to multiple neuromorphic processors for larger input sizes.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods and performance metrics. The cited paper titles do not provide additional context about datasets.",
      "processing_time": 43.11748552322388,
      "citing_paper_id": "46937991",
      "cited_paper_id": 34855834
    },
    {
      "context_text": "Local methods can be parallelized and find corresponding events using either local features over a spatiotemporal window or event-to-event features [13, 58, 52, 32, 57].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only methods and approaches for event-driven stereo systems.",
      "processing_time": 42.54159116744995,
      "citing_paper_id": "46937991",
      "cited_paper_id": 21539113
    },
    {
      "context_text": "Local methods can be parallelized and find corresponding events using either local features over a spatiotemporal window or event-to-event features [13, 58, 52, 32, 57].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only methods and approaches for event-driven stereo systems.",
      "processing_time": 42.54159116744995,
      "citing_paper_id": "46937991",
      "cited_paper_id": 24236495
    },
    {
      "context_text": "To benefit from sparse and asynchronous computation, neuromorphic processors have been developed [44, 24, 30, 9, 56].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only references to neuromorphic processors and their development.",
      "processing_time": 42.70903754234314,
      "citing_paper_id": "46937991",
      "cited_paper_id": 22330500
    },
    {
      "context_text": "Our implementation uses a pair of synchronized DAVIS240C cameras [10], connected via Ethernet to a cluster of TrueNorth NS1e boards (Fig.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only hardware components. The citation is for a camera sensor, which is not a dataset.",
      "processing_time": 42.95091223716736,
      "citing_paper_id": "46937991",
      "cited_paper_id": 24007071
    },
    {
      "context_text": "[17] use six SpiNNaker [24] processor boards to implement the cooperative network for 106 × 106 pixels of stereo event data.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions 'stereo event data' but does not specify a dataset name. The context is about hardware implementation and latency, not a specific dataset.",
      "processing_time": 43.35358643531799,
      "citing_paper_id": "46937991",
      "cited_paper_id": 34855834
    },
    {
      "context_text": "The implemented neuromorphic stereo disparity system achieves these advantages, while consuming ∼ 200× less power per pixel per disparity map compared to the stateof-the-art [17].",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a comparison to the state-of-the-art in terms of power consumption.",
      "processing_time": 43.1898148059845,
      "citing_paper_id": "46937991",
      "cited_paper_id": 34855834
    },
    {
      "context_text": "Most global methods [40, 17, 49, 45] are derived from the Marr and Poggio cooperative stereo algorithm [42].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only references to algorithms and methods.",
      "processing_time": 42.70360469818115,
      "citing_paper_id": "46937991",
      "cited_paper_id": 34855834
    },
    {
      "context_text": "With respect to the most relevant state-of-the-art approach [17], our method uses ∼ 200× less power per pixel per disparity map.",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a comparison of power efficiency. No verifiable resources are identified.",
      "processing_time": 43.351064920425415,
      "citing_paper_id": "46937991",
      "cited_paper_id": 34855834
    },
    {
      "context_text": "The algorithm converges well when object surfaces are fronto-parallel and candidate matches injected to the network are close together [40, 17].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only discusses the performance of an algorithm under certain conditions.",
      "processing_time": 43.185861349105835,
      "citing_paper_id": "46937991",
      "cited_paper_id": 34855834
    },
    {
      "context_text": "models, followed by sparse-to-dense conversions [18, 5].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models and methods. The context is too limited to infer the use of any datasets.",
      "processing_time": 43.82123351097107,
      "citing_paper_id": "46937991",
      "cited_paper_id": 35157264
    },
    {
      "context_text": "For example, the TrueNorth neuromorphic chip [44] has been used for high throughput Convolutional neural networks (CNNs) [22], character recognition [53], optic flow [11], saliency [3], and gesture recognition [2].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only applications of the TrueNorth neuromorphic chip. No dataset names are present in the citation span.",
      "processing_time": 44.13545203208923,
      "citing_paper_id": "46937991",
      "cited_paper_id": 45560068
    },
    {
      "context_text": "Frame-based stereo disparity methods calculate matching cost using a spatial similarity metric [25, 27, 29] or a cost function learned from a dataset (see reviews [62, 55, 34, 63]).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.2,
      "reasoning": "The context mentions 'a dataset' but does not provide a specific name. The cited papers do not clarify a specific dataset name either.",
      "processing_time": 43.976372480392456,
      "citing_paper_id": "46937991",
      "cited_paper_id": 195859047
    },
    {
      "context_text": "The performance is compared to MC3D and Semi-Global Matching (SGM) [9] algorithms: ESL surpasses both MC3D and SGM in static scenes, demonstrating lower error values and superior noise suppression against the baseline.",
      "catation_intent": "findings",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only algorithms (MC3D and SGM) which are excluded according to the rules.",
      "processing_time": 44.05225872993469,
      "citing_paper_id": "259336126",
      "cited_paper_id": 18327083
    },
    {
      "context_text": "The performance is compared to MC3D and Semi-Global Matching (SGM) [9] algorithms: ESL surpasses both MC3D and SGM in static scenes, demonstrating lower error values and superior noise suppression against the baseline.",
      "catation_intent": "findings",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only algorithms (MC3D and SGM) which are excluded according to the rules.",
      "processing_time": 44.05225872993469,
      "citing_paper_id": "259336126",
      "cited_paper_id": 206767633
    },
    {
      "context_text": "tainment [2], industrial applications [1, 6], advertising [19], cultural heritage [22], and healthcare [5].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The provided context does not mention any specific datasets, only general application areas. No dataset names are present in the text.",
      "processing_time": 43.734612703323364,
      "citing_paper_id": "259336126",
      "cited_paper_id": 33388297
    },
    {
      "context_text": "tainment [2], industrial applications [1, 6], advertising [19], cultural heritage [22], and healthcare [5].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The provided context does not mention any specific datasets, only general application areas. No dataset names are present in the text.",
      "processing_time": 43.734612703323364,
      "citing_paper_id": "259336126",
      "cited_paper_id": 53419489
    },
    {
      "context_text": "Camera-projector systems can be calibrated with established methods such as [13].",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for calibration.",
      "processing_time": 42.771552324295044,
      "citing_paper_id": "259336126",
      "cited_paper_id": 206428176
    },
    {
      "context_text": "Building on MC3D’s groundwork, Event-based Structured Light (ESL) [15] introduces time maps to establish the temporal projector-camera correspondences.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets. It refers to methods and approaches rather than datasets.",
      "processing_time": 43.57611274719238,
      "citing_paper_id": "259336126",
      "cited_paper_id": 206767633
    },
    {
      "context_text": "Building on MC3D’s groundwork, Event-based Structured Light (ESL) [15] introduces time maps to establish the temporal projector-camera correspondences.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets. It refers to methods and approaches rather than datasets.",
      "processing_time": 43.57611274719238,
      "citing_paper_id": "259336126",
      "cited_paper_id": 244729216
    },
    {
      "context_text": "If the MC3D measurements are averaged over a period of 1 second (60 frames) in MC3D-1s, the depth maps become more dense, but still differ a lot from the smoothed ESL depth maps.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "MC3D-1s"
      ],
      "dataset_descriptions": {
        "MC3D-1s": "Used to estimate dense depth maps by averaging MC3D measurements over 1 second, comparing the results with ESL depth maps for event-based stereo depth estimation."
      },
      "confidence_score": 0.7,
      "reasoning": "The context mentions 'MC3D-1s', which appears to be a specific dataset or data configuration derived from the MC3D method. However, it is not clear if this is a reusable dataset or just a methodological variation.",
      "processing_time": 47.556918144226074,
      "citing_paper_id": "259336126",
      "cited_paper_id": 206767633
    },
    {
      "context_text": "MC3D is not able to capture full frames with a frequency of 60 Hz .",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (MC3D) and its limitations.",
      "processing_time": 43.80870842933655,
      "citing_paper_id": "259336126",
      "cited_paper_id": 206767633
    },
    {
      "context_text": "Our approach builds on the findings of previous methods [12,15], which combine structured light technology and",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only references to previous methods and technologies. The cited papers' titles do not provide additional dataset information.",
      "processing_time": 44.50836992263794,
      "citing_paper_id": "259336126",
      "cited_paper_id": 206767633
    },
    {
      "context_text": "Our approach builds on the findings of previous methods [12,15], which combine structured light technology and",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only references to previous methods and technologies. The cited papers' titles do not provide additional dataset information.",
      "processing_time": 44.50836992263794,
      "citing_paper_id": "259336126",
      "cited_paper_id": 244729216
    },
    {
      "context_text": "In ESL, results from a time-averaged MC3D calculation are used as a baseline to compare against.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (MC3D) used for comparison. The context is about using results from a time-averaged MC3D calculation as a baseline.",
      "processing_time": 45.60529971122742,
      "citing_paper_id": "259336126",
      "cited_paper_id": 206767633
    },
    {
      "context_text": "Motion Contrast 3D (MC3D) [12] introduces the concept of merging single-shot structured light techniques with event-based cameras and addresses the trade-offs between resolution, robustness, and speed in structured light systems.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (MC3D) and its application in merging single-shot structured light techniques with event-based cameras.",
      "processing_time": 45.15466547012329,
      "citing_paper_id": "259336126",
      "cited_paper_id": 206767633
    },
    {
      "context_text": "In comparison to MC3D, we see that X-maps provides depth maps with much higher fill rate and lower RMSE.",
      "catation_intent": "findings",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only compares two methods (MC3D and X-maps) in terms of performance.",
      "processing_time": 44.885863065719604,
      "citing_paper_id": "259336126",
      "cited_paper_id": 206767633
    },
    {
      "context_text": "At high sampling rates, MC3D’s correspondence search amplifies noise in the event timestamps, resulting in noisy and patchy stereo correspondences.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a method (MC3D) and its performance issues at high sampling rates.",
      "processing_time": 44.50333118438721,
      "citing_paper_id": "259336126",
      "cited_paper_id": 206767633
    },
    {
      "context_text": "We found that the refined ESL results (window size W = 7 and denoised) capture the geometry more cleanly than MC3D.",
      "catation_intent": "findings",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a comparison between methods (ESL and MC3D).",
      "processing_time": 44.34783625602722,
      "citing_paper_id": "259336126",
      "cited_paper_id": 206767633
    },
    {
      "context_text": "MC3D employs a projector that scans scenes using a single laser beam in a raster pattern.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method or system (MC3D) that uses a projector to scan scenes. No verifiable dataset names are present.",
      "processing_time": 45.433613777160645,
      "citing_paper_id": "259336126",
      "cited_paper_id": 206767633
    },
    {
      "context_text": "We compare against MC3D and the initialization step of ESL ( ESL-init ), which uses a row-wise disparity matching and no further optimization.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods or models. The context focuses on comparing against methods, not using datasets.",
      "processing_time": 44.953383684158325,
      "citing_paper_id": "259336126",
      "cited_paper_id": 206767633
    },
    {
      "context_text": "A good discussion of the different sources of noise within the camera timestamps can be found in [17].",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a discussion about noise in camera timestamps.",
      "processing_time": 44.033379554748535,
      "citing_paper_id": "259336126",
      "cited_paper_id": 239050401
    },
    {
      "context_text": "Foveated rendering in VR headsets is adapted for depth sensing in [17], where the authors develop a foveating",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method or technique. The context is too limited to infer the use of a dataset.",
      "processing_time": 45.08520150184631,
      "citing_paper_id": "259336126",
      "cited_paper_id": 239050401
    },
    {
      "context_text": "The increased accuracy in dynamic scenes and robustness of event-based sensing compared to structured light sensors such as RealSense [4] was performed in [15] and is also applicable to our method.",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a comparison between event-based sensing and structured light sensors. No verifiable resources are identified.",
      "processing_time": 44.798884868621826,
      "citing_paper_id": "259336126",
      "cited_paper_id": 244729216
    },
    {
      "context_text": "A rectified time map is used in ESL [15] to match the recorded time in the camera’s time map to the ideal time in a synthesized projector time map, by searching along epipolar lines to minimize the difference in t.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a method (ESL) and a technique (rectified time map). No verifiable dataset names are present.",
      "processing_time": 45.14166474342346,
      "citing_paper_id": "259336126",
      "cited_paper_id": 244729216
    },
    {
      "context_text": "Matching time entries of the map along epipolar lines with an idealized projector time map to compute scene disparity is computationally expensive [15].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for computing scene disparity using event-based structured light.",
      "processing_time": 44.72110939025879,
      "citing_paper_id": "259336126",
      "cited_paper_id": 244729216
    },
    {
      "context_text": "In the data recorded for the ESL [15], the projector displayed a comparatively milder nonlinear behavior, resulting in significantly flatter planes even without time map calibration.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions 'ESL' which is likely a dataset or a method. However, the title 'ESL: Event-based Structured Light' suggests it is a method or system rather than a dataset. No other specific datasets are mentioned.",
      "processing_time": 46.03745484352112,
      "citing_paper_id": "259336126",
      "cited_paper_id": 244729216
    },
    {
      "context_text": "We compare our method to the state of the art approach ESL [15] and use the static scenes of the public dataset provided by them.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "public dataset provided by ESL"
      ],
      "dataset_descriptions": {
        "public dataset provided by ESL": "Used to evaluate the performance of the proposed method compared to the state-of-the-art approach ESL, focusing on static scenes."
      },
      "confidence_score": 0.8,
      "reasoning": "The context mentions a 'public dataset' provided by the authors of the cited paper ESL. This dataset is used to compare the current method to the state-of-the-art approach.",
      "processing_time": 47.151294469833374,
      "citing_paper_id": "259336126",
      "cited_paper_id": 244729216
    },
    {
      "context_text": "We build our demonstrator using the Nebra Anybeam MEMS Laser Projector [18].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any datasets, only a hardware component used in building a demonstrator.",
      "processing_time": 44.102020502090454,
      "citing_paper_id": "259336126",
      "cited_paper_id": null
    },
    {
      "context_text": "L p is the perceptual loss (Zhang et al., 2018). β 1 , β 2 aim to balance the L 1 and perceptual loss, we constantly set them to 0.8 and 0.2 for all situations.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (perceptual loss) and parameters. No dataset names are present in the citation context.",
      "processing_time": 45.31083559989929,
      "citing_paper_id": "270045246",
      "cited_paper_id": 4766599
    },
    {
      "context_text": "Traditional explicit representation methods include point cloud(Achlioptas et al., 2018), mesh(Liu et al., 2020a), and voxel(Lombardi et al., 2019; Sitzmann et al., 2019).",
      "catation_intent": "none",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only methods for representing 3D data. No dataset names are present in the citation span.",
      "processing_time": 45.13285493850708,
      "citing_paper_id": "270045246",
      "cited_paper_id": 23102425
    },
    {
      "context_text": "Traditional explicit representation methods include point cloud(Achlioptas et al., 2018), mesh(Liu et al., 2020a), and voxel(Lombardi et al., 2019; Sitzmann et al., 2019).",
      "catation_intent": "none",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only methods for representing 3D data. No dataset names are present in the citation span.",
      "processing_time": 45.13285493850708,
      "citing_paper_id": "270045246",
      "cited_paper_id": 220978548
    },
    {
      "context_text": "Dataset Existing event-based 3D datasets such as (Rudnev et al., 2023; Zhou et al., 2018) only contain a limited number of objects and lack high-quality intensity, depth, and mask groundtruths because they mainly concentrate on single scene reconstruction or sparse vision tasks.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.3,
      "reasoning": "The context mentions existing event-based 3D datasets but does not provide specific names. It describes limitations of these datasets, which are not clearly identified.",
      "processing_time": 45.361027002334595,
      "citing_paper_id": "270045246",
      "cited_paper_id": 49877954
    },
    {
      "context_text": "In this section, we select three popular image recovery algorithms, i.e. E2VID (Rebecq et al., 2019), FireNet (Scheerlinck et al., 2020), and EVSNN (Barchid et al., 2023).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span mentions three algorithms (E2VID, FireNet, EVSNN) but does not refer to any specific datasets. The context is about selecting algorithms for image recovery, not using datasets.",
      "processing_time": 45.88417863845825,
      "citing_paper_id": "270045246",
      "cited_paper_id": 189998802
    },
    {
      "context_text": "In this section, we select three popular image recovery algorithms, i.e. E2VID (Rebecq et al., 2019), FireNet (Scheerlinck et al., 2020), and EVSNN (Barchid et al., 2023).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span mentions three algorithms (E2VID, FireNet, EVSNN) but does not refer to any specific datasets. The context is about selecting algorithms for image recovery, not using datasets.",
      "processing_time": 45.88417863845825,
      "citing_paper_id": "270045246",
      "cited_paper_id": 210886473
    },
    {
      "context_text": "In this section, we select three popular image recovery algorithms, i.e. E2VID (Rebecq et al., 2019), FireNet (Scheerlinck et al., 2020), and EVSNN (Barchid et al., 2023).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span mentions three algorithms (E2VID, FireNet, EVSNN) but does not refer to any specific datasets. The context is about selecting algorithms for image recovery, not using datasets.",
      "processing_time": 45.88417863845825,
      "citing_paper_id": "270045246",
      "cited_paper_id": 246285530
    },
    {
      "context_text": "E2VID(Rebecq et al., 2019) introduced a ConvLSTM-based model, facilitating the recovery of high-dynamic video.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions E2VID, which is a method, not a dataset. No specific dataset is mentioned or used in the context provided.",
      "processing_time": 45.244309186935425,
      "citing_paper_id": "270045246",
      "cited_paper_id": 189998802
    },
    {
      "context_text": "While the recovery of intensity images is effective when interpolating between the given intensity images (Wang et al., 2020), the performance dramatically degrades when only the event stream is available (Rebecq et al., 2019).",
      "catation_intent": "findings",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only references to methods and their performance.",
      "processing_time": 44.17275929450989,
      "citing_paper_id": "270045246",
      "cited_paper_id": 189998802
    },
    {
      "context_text": "Following (Scheerlinck et al., 2020), we set B = 5 in our experiments.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a parameter setting. No verifiable resources are identified.",
      "processing_time": 44.401346921920776,
      "citing_paper_id": "270045246",
      "cited_paper_id": 210886473
    },
    {
      "context_text": "FireNet(Scheerlinck et al., 2020) employs the GRUs to provide a more rapid and lightweight method for event-based video reconstruction.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (FireNet) and its application to event-based video reconstruction.",
      "processing_time": 45.12324357032776,
      "citing_paper_id": "270045246",
      "cited_paper_id": 210886473
    },
    {
      "context_text": "E2VID and FireNet rely on the recurrent convolution structure while EVSNN is built upon the spiking neural network.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only models and methods. No dataset names are present in the text.",
      "processing_time": 45.06361103057861,
      "citing_paper_id": "270045246",
      "cited_paper_id": 210886473
    },
    {
      "context_text": "E2VID and FireNet rely on the recurrent convolution structure while EVSNN is built upon the spiking neural network.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only models and methods. No dataset names are present in the text.",
      "processing_time": 45.06361103057861,
      "citing_paper_id": "270045246",
      "cited_paper_id": 246285530
    },
    {
      "context_text": "The intensity reconstruction of E2VID is slightly better than FireNet, especially the reconstruction of the ‘Train’ scene, which is quite close to our joint training strategy.",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a comparison between E2VID and FireNet in terms of intensity reconstruction.",
      "processing_time": 44.85516023635864,
      "citing_paper_id": "270045246",
      "cited_paper_id": 210886473
    },
    {
      "context_text": "FireNet did not recover the intensity values correctly, and it can be observed that there are severe color bleeding effects in all five test scenes.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only issues with a method (FireNet).",
      "processing_time": 44.39511752128601,
      "citing_paper_id": "270045246",
      "cited_paper_id": 210886473
    },
    {
      "context_text": "We employ V2E(Hu et al., 2021) to generate synthetic event streams maintaining default noise configurations.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions V2E, which is a tool for generating synthetic event streams, not a dataset. No datasets are explicitly mentioned.",
      "processing_time": 44.99706482887268,
      "citing_paper_id": "270045246",
      "cited_paper_id": 235651771
    },
    {
      "context_text": "NeuRay(Liu et al., 2022b) implicitly models visibility to deal with occlusion issues.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific dataset. It refers to a method (NeuRay) for dealing with occlusion issues in image-based rendering.",
      "processing_time": 45.40428352355957,
      "citing_paper_id": "270045246",
      "cited_paper_id": 236469482
    },
    {
      "context_text": "ET-Net(Weng et al., 2021) employed a vision transformer to reconstruct videos from events.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (vision transformer) used for event-based video reconstruction.",
      "processing_time": 45.05719590187073,
      "citing_paper_id": "270045246",
      "cited_paper_id": 244707609
    },
    {
      "context_text": "The hue of the scenes reconstructed by EVSNN is noticeably lighter compared to the true intensity map.",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a comparison of reconstruction quality. No verifiable resources are identified.",
      "processing_time": 44.77141785621643,
      "citing_paper_id": "270045246",
      "cited_paper_id": 246285530
    },
    {
      "context_text": "EVSNN performs the best among the methods outside of our joint training strategy, recovering the intensity values well in all scenes except for ’Flower’ and ’Doll’.",
      "catation_intent": "findings",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (EVSNN) and some scenes ('Flower' and 'Doll'). These scenes do not meet the criteria for being considered datasets.",
      "processing_time": 46.00673222541809,
      "citing_paper_id": "270045246",
      "cited_paper_id": 246285530
    },
    {
      "context_text": "EVSNN(Zhu et al., 2022) proposes a hybrid potential-assisted spiking neural network to recover images from events efficiently.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions EVSNN, which is a method, not a dataset. No specific dataset is mentioned in the context.",
      "processing_time": 45.171093225479126,
      "citing_paper_id": "270045246",
      "cited_paper_id": 246285530
    },
    {
      "context_text": "However, it can be observed that EVSNN also has the same issue as E2VID with low contrast.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only discusses issues with methods (EVSNN and E2VID).",
      "processing_time": 44.84278082847595,
      "citing_paper_id": "270045246",
      "cited_paper_id": 246285530
    },
    {
      "context_text": "NeuMesh(Yang et al., 2022) distills the neural field into a mesh scaffold, enabling field manipulation with the mesh deformation.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method called NeuMesh. The context focuses on the method's functionality and application.",
      "processing_time": 45.339942932128906,
      "citing_paper_id": "270045246",
      "cited_paper_id": 251040986
    },
    {
      "context_text": "(Rud-nev et al., 2023; Hwang et al., 2023; Klenk et al., 2023; Wang et al., 2024a) build similar pipelines which integrate the event generation model into NeRF.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and models. The context focuses on integrating an event generation model into NeRF, which is a method.",
      "processing_time": 45.612497329711914,
      "citing_paper_id": "270045246",
      "cited_paper_id": 251765179
    },
    {
      "context_text": "( Brebion et al., 2023) fuses information from an event camera and a LiDAR.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only the fusion of data from an event camera and a LiDAR.",
      "processing_time": 44.91109919548035,
      "citing_paper_id": "270045246",
      "cited_paper_id": 257232560
    },
    {
      "context_text": "Ref-NeuS(Ge et al., 2023) model sign distance field by incorporating explicit reflection scores into NeRF.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a model (Ref-NeuS) and its methodology. No verifiable datasets are referenced.",
      "processing_time": 45.50306057929993,
      "citing_paper_id": "270045246",
      "cited_paper_id": 257632404
    },
    {
      "context_text": "(Xu et al., 2022) and (Wang et al., 2023) combine point clouds with NeRF to deliver better reconstruction quality.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and models. The context focuses on combining point clouds with NeRF for better reconstruction quality.",
      "processing_time": 45.444581508636475,
      "citing_paper_id": "270045246",
      "cited_paper_id": 264886560
    },
    {
      "context_text": "[19] proposed an efficient seed-growing algorithm to fuse time-of-flight (ToF) depth data with stereo pairs, while Marin et al .",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for fusing ToF depth data with stereo pairs.",
      "processing_time": 45.04132914543152,
      "citing_paper_id": "271769110",
      "cited_paper_id": 55750
    },
    {
      "context_text": "…a longstanding open problem, with a large body of literature spanning from traditional approaches grounded on handcrafted features and priors [5, 24, 31, 36, 62, 68, 75, 76, 78] to contemporary deep learning approaches that brought significant improvements over previous methods, starting with [79].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only a general reference to a large body of literature. No specific, verifiable datasets are named.",
      "processing_time": 45.49712634086609,
      "citing_paper_id": "271769110",
      "cited_paper_id": 703552
    },
    {
      "context_text": "…a longstanding open problem, with a large body of literature spanning from traditional approaches grounded on handcrafted features and priors [5, 24, 31, 36, 62, 68, 75, 76, 78] to contemporary deep learning approaches that brought significant improvements over previous methods, starting with [79].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only a general reference to a large body of literature. No specific, verifiable datasets are named.",
      "processing_time": 45.49712634086609,
      "citing_paper_id": "271769110",
      "cited_paper_id": 786967
    },
    {
      "context_text": "…a longstanding open problem, with a large body of literature spanning from traditional approaches grounded on handcrafted features and priors [5, 24, 31, 36, 62, 68, 75, 76, 78] to contemporary deep learning approaches that brought significant improvements over previous methods, starting with [79].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only a general reference to a large body of literature. No specific, verifiable datasets are named.",
      "processing_time": 45.49712634086609,
      "citing_paper_id": "271769110",
      "cited_paper_id": 5880703
    },
    {
      "context_text": "…a longstanding open problem, with a large body of literature spanning from traditional approaches grounded on handcrafted features and priors [5, 24, 31, 36, 62, 68, 75, 76, 78] to contemporary deep learning approaches that brought significant improvements over previous methods, starting with [79].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only a general reference to a large body of literature. No specific, verifiable datasets are named.",
      "processing_time": 45.49712634086609,
      "citing_paper_id": "271769110",
      "cited_paper_id": 6262684
    },
    {
      "context_text": "XI to XVIII) different figures from the M3ED [5] dataset, using raw LiDAR measurements as guidance for all fusion frameworks.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "M3ED"
      ],
      "dataset_descriptions": {
        "M3ED": "Used to generate figures for event-based stereo depth estimation, specifically employing raw LiDAR measurements as guidance for fusion frameworks."
      },
      "confidence_score": 0.9,
      "reasoning": "The context mentions the M3ED dataset, which is a specific dataset used for event-based stereo depth estimation. The dataset is used to generate figures using raw LiDAR measurements.",
      "processing_time": 47.349013328552246,
      "citing_paper_id": "271769110",
      "cited_paper_id": 2430892
    },
    {
      "context_text": "For more details, please refer to the original paper [5].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, models, or methods. It only refers readers to the original paper for more details.",
      "processing_time": 45.27568769454956,
      "citing_paper_id": "271769110",
      "cited_paper_id": 2430892
    },
    {
      "context_text": "It is a longstanding open problem, with a large body of literature spanning from traditional approaches grounded on handcrafted features and priors [5, 24, 31, 36, 62, 68, 75, 76, 78] to contemporary deep learning approaches that brought significant improvements over previous methods, starting with…",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only a reference to a large body of literature and approaches. No verifiable resources are identified.",
      "processing_time": 45.4907865524292,
      "citing_paper_id": "271769110",
      "cited_paper_id": 2430892
    },
    {
      "context_text": "In particular, we describe the DSEC [7] search split we used for the hyperparameters search concerning our proposals VSH and BTH, the M3ED [5] evaluation split we selected, and how we managed to process both datasets to extract raw LiDAR and, on M3ED dataset [5], for obtaining misaligned LiDAR…",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "DSEC",
        "M3ED"
      ],
      "dataset_descriptions": {
        "DSEC": "Used for hyperparameter search concerning VSH and BTH proposals, processing raw LiDAR data for event-based stereo depth estimation.",
        "M3ED": "Selected for evaluation, processing raw LiDAR data and obtaining misaligned LiDAR for assessing stereo depth estimation performance."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions two specific datasets, DSEC and M3ED, which are used for hyperparameter search and evaluation, respectively. Both are relevant to event-based stereo depth estimation.",
      "processing_time": 48.71870255470276,
      "citing_paper_id": "271769110",
      "cited_paper_id": 2430892
    },
    {
      "context_text": "In particular, we describe the DSEC [7] search split we used for the hyperparameters search concerning our proposals VSH and BTH, the M3ED [5] evaluation split we selected, and how we managed to process both datasets to extract raw LiDAR and, on M3ED dataset [5], for obtaining misaligned LiDAR…",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "DSEC",
        "M3ED"
      ],
      "dataset_descriptions": {
        "DSEC": "Used for hyperparameter search concerning VSH and BTH proposals, processing raw LiDAR data for event-based stereo depth estimation.",
        "M3ED": "Selected for evaluation, processing raw LiDAR data and obtaining misaligned LiDAR for assessing stereo depth estimation performance."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions two specific datasets, DSEC and M3ED, which are used for hyperparameter search and evaluation, respectively. Both are relevant to event-based stereo depth estimation.",
      "processing_time": 48.71870255470276,
      "citing_paper_id": "271769110",
      "cited_paper_id": 12047627
    },
    {
      "context_text": "Differently from M3ED [5], this dataset does not provide any ground-truth pose.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.3,
      "reasoning": "The context mentions 'this dataset' but does not provide a specific name. It also references M3ED, which is likely a method or another dataset, but there is not enough information to confirm its type.",
      "processing_time": 46.19064259529114,
      "citing_paper_id": "271769110",
      "cited_paper_id": 2430892
    },
    {
      "context_text": "…VSH and BTH, the M3ED [5] evaluation split we selected, and how we managed to process both datasets to extract raw LiDAR and, on M3ED dataset [5], for obtaining misaligned LiDAR measurements with respect to the timestamp at which we estimate disparity maps – and thus, at which we have…",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "M3ED"
      ],
      "dataset_descriptions": {
        "M3ED": "Used to evaluate stereo depth estimation methods, focusing on processing raw LiDAR data and generating misaligned measurements for disparity map estimation."
      },
      "confidence_score": 0.9,
      "reasoning": "The context mentions 'M3ED' as a dataset used for evaluation and describes specific processing steps applied to it. No other datasets are mentioned.",
      "processing_time": 47.22488331794739,
      "citing_paper_id": "271769110",
      "cited_paper_id": 2430892
    },
    {
      "context_text": "In contrast, the latter constructs a feature cost volume from image pair features and estimates the disparity map through 3D convolutions at the cost of substantially higher memory and runtime demands [10,11,13,16,23, 27,28,57,70,73,80].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only methods and approaches for stereo depth estimation.",
      "processing_time": 44.96952724456787,
      "citing_paper_id": "271769110",
      "cited_paper_id": 2658860
    },
    {
      "context_text": "In contrast, the latter constructs a feature cost volume from image pair features and estimates the disparity map through 3D convolutions at the cost of substantially higher memory and runtime demands [10,11,13,16,23, 27,28,57,70,73,80].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only methods and approaches for stereo depth estimation.",
      "processing_time": 44.96952724456787,
      "citing_paper_id": "271769110",
      "cited_paper_id": 4252896
    },
    {
      "context_text": "In contrast, the latter constructs a feature cost volume from image pair features and estimates the disparity map through 3D convolutions at the cost of substantially higher memory and runtime demands [10,11,13,16,23, 27,28,57,70,73,80].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only methods and approaches for stereo depth estimation.",
      "processing_time": 44.96952724456787,
      "citing_paper_id": "271769110",
      "cited_paper_id": 53082511
    },
    {
      "context_text": "In contrast, the latter constructs a feature cost volume from image pair features and estimates the disparity map through 3D convolutions at the cost of substantially higher memory and runtime demands [10,11,13,16,23, 27,28,57,70,73,80].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only methods and approaches for stereo depth estimation.",
      "processing_time": 44.96952724456787,
      "citing_paper_id": "271769110",
      "cited_paper_id": 73729084
    },
    {
      "context_text": "In contrast, the latter constructs a feature cost volume from image pair features and estimates the disparity map through 3D convolutions at the cost of substantially higher memory and runtime demands [10,11,13,16,23, 27,28,57,70,73,80].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only methods and approaches for stereo depth estimation.",
      "processing_time": 44.96952724456787,
      "citing_paper_id": "271769110",
      "cited_paper_id": 119304432
    },
    {
      "context_text": "In contrast, the latter constructs a feature cost volume from image pair features and estimates the disparity map through 3D convolutions at the cost of substantially higher memory and runtime demands [10,11,13,16,23, 27,28,57,70,73,80].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only methods and approaches for stereo depth estimation.",
      "processing_time": 44.96952724456787,
      "citing_paper_id": "271769110",
      "cited_paper_id": 196183868
    },
    {
      "context_text": "In contrast, the latter constructs a feature cost volume from image pair features and estimates the disparity map through 3D convolutions at the cost of substantially higher memory and runtime demands [10,11,13,16,23, 27,28,57,70,73,80].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only methods and approaches for stereo depth estimation.",
      "processing_time": 44.96952724456787,
      "citing_paper_id": "271769110",
      "cited_paper_id": 225072923
    },
    {
      "context_text": "In contrast, the latter constructs a feature cost volume from image pair features and estimates the disparity map through 3D convolutions at the cost of substantially higher memory and runtime demands [10,11,13,16,23, 27,28,57,70,73,80].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only methods and approaches for stereo depth estimation.",
      "processing_time": 44.96952724456787,
      "citing_paper_id": "271769110",
      "cited_paper_id": 233204703
    },
    {
      "context_text": "The former, inspired by the U-Net model [53], adopts an encoder-decoder design [37,42,45,50, 54,59,63,64,74,77].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only models and methods. The cited papers do not provide additional context about datasets.",
      "processing_time": 45.377092361450195,
      "citing_paper_id": "271769110",
      "cited_paper_id": 3719281
    },
    {
      "context_text": "The former, inspired by the U-Net model [53], adopts an encoder-decoder design [37,42,45,50, 54,59,63,64,74,77].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only models and methods. The cited papers do not provide additional context about datasets.",
      "processing_time": 45.377092361450195,
      "citing_paper_id": "271769110",
      "cited_paper_id": 3871029
    },
    {
      "context_text": "The former, inspired by the U-Net model [53], adopts an encoder-decoder design [37,42,45,50, 54,59,63,64,74,77].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only models and methods. The cited papers do not provide additional context about datasets.",
      "processing_time": 45.377092361450195,
      "citing_paper_id": "271769110",
      "cited_paper_id": 31762881
    },
    {
      "context_text": "The former, inspired by the U-Net model [53], adopts an encoder-decoder design [37,42,45,50, 54,59,63,64,74,77].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only models and methods. The cited papers do not provide additional context about datasets.",
      "processing_time": 45.377092361450195,
      "citing_paper_id": "271769110",
      "cited_paper_id": 159040912
    },
    {
      "context_text": "The former, inspired by the U-Net model [53], adopts an encoder-decoder design [37,42,45,50, 54,59,63,64,74,77].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only models and methods. The cited papers do not provide additional context about datasets.",
      "processing_time": 45.377092361450195,
      "citing_paper_id": "271769110",
      "cited_paper_id": 269983050
    },
    {
      "context_text": "Instead, [47] revisited the cooperative network from [41].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method or approach from another paper.",
      "processing_time": 44.66849112510681,
      "citing_paper_id": "271769110",
      "cited_paper_id": 6079544
    },
    {
      "context_text": "We use Adam [29] with beta (0.9, 0.999) and weight decay set to 10 − 4 .",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only the Adam optimization method.",
      "processing_time": 44.66789627075195,
      "citing_paper_id": "271769110",
      "cited_paper_id": 6628106
    },
    {
      "context_text": "However, pseudo-images lose the high temporal resolution of the stream: to face this problem, [8,52] handle events without an intermediate representation using an event-to-event matching approach, where for each reference event, a set of possible matches is given.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods for handling events in event-based systems.",
      "processing_time": 44.74240803718567,
      "citing_paper_id": "271769110",
      "cited_paper_id": 10712214
    },
    {
      "context_text": "[7] Fig.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not contain any specific dataset names or verifiable resources. It only mentions a figure reference.",
      "processing_time": 44.88985848426819,
      "citing_paper_id": "271769110",
      "cited_paper_id": 12047627
    },
    {
      "context_text": "We start with DSEC [7], which we use for i) tuning the hyper-parameters in our solutions, and ii) training the models involved in our experiments.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "DSEC"
      ],
      "dataset_descriptions": {
        "DSEC": "Used for tuning hyperparameters and evaluating models in event-driven stereo vision, focusing on 3D reconstruction using orientation filters."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions 'DSEC' as a dataset used for tuning hyper-parameters and training models in the context of event-driven stereo vision.",
      "processing_time": 47.05706000328064,
      "citing_paper_id": "271769110",
      "cited_paper_id": 12047627
    },
    {
      "context_text": "Similarly to conventional stereo matching, the first approaches focused on developing traditional algorithms by building structured representations, such as voxel grids [56], matched through handcrafted similarity functions [30, 56, 60, 85].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only methods and algorithms. No verifiable resources are identified.",
      "processing_time": 45.1977972984314,
      "citing_paper_id": "271769110",
      "cited_paper_id": 24236495
    },
    {
      "context_text": "Similarly to conventional stereo matching, the first approaches focused on developing traditional algorithms by building structured representations, such as voxel grids [56], matched through handcrafted similarity functions [30, 56, 60, 85].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only methods and algorithms. No verifiable resources are identified.",
      "processing_time": 45.1977972984314,
      "citing_paper_id": "271769110",
      "cited_paper_id": 44623261
    },
    {
      "context_text": "In the latter case, Concat [6] and Guided+Concat [15] can reduce the error by about 40%, yet far behind the improvement yielded by BTH (more than 70% error rate reduction).",
      "catation_intent": "findings",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and their performance comparisons.",
      "processing_time": 44.50837421417236,
      "citing_paper_id": "271769110",
      "cited_paper_id": 34855834
    },
    {
      "context_text": "We report additional details about the deep architectures used in our experiments, starting from the baseline network, SE-CFF [12], and then showing the fusion strategies ported from classical deep stereo literature [6,13,15] Baseline Model: SE-CFF.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only models and methods. The citation intent is to describe the research work and the deep architectures used.",
      "processing_time": 45.58451271057129,
      "citing_paper_id": "271769110",
      "cited_paper_id": 34855834
    },
    {
      "context_text": "We report additional details about the deep architectures used in our experiments, starting from the baseline network, SE-CFF [12], and then showing the fusion strategies ported from classical deep stereo literature [6,13,15] Baseline Model: SE-CFF.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only models and methods. The citation intent is to describe the research work and the deep architectures used.",
      "processing_time": 45.58451271057129,
      "citing_paper_id": "271769110",
      "cited_paper_id": 225072923
    },
    {
      "context_text": "Guided [13] is almost ineffective, while using both Concat [6] and Guided+Concat [15] leads to 20% error reduction.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only methods or approaches. The cited paper titles do not provide additional information about datasets.",
      "processing_time": 45.47717833518982,
      "citing_paper_id": "271769110",
      "cited_paper_id": 34855834
    },
    {
      "context_text": "Guided [13] is almost ineffective, while using both Concat [6] and Guided+Concat [15] leads to 20% error reduction.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only methods or approaches. The cited paper titles do not provide additional information about datasets.",
      "processing_time": 45.47717833518982,
      "citing_paper_id": "271769110",
      "cited_paper_id": 225072923
    },
    {
      "context_text": "The former are often inspired by [41] and typically employ Spiking Neural Networks (SNN) [1, 15, 44].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only references to methods and approaches. No dataset names are present in the citation span.",
      "processing_time": 45.25258755683899,
      "citing_paper_id": "271769110",
      "cited_paper_id": 34855834
    },
    {
      "context_text": "[48] exploited confidence measures.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only the use of confidence measures in depth data fusion.",
      "processing_time": 44.880460023880005,
      "citing_paper_id": "271769110",
      "cited_paper_id": 208828341
    },
    {
      "context_text": "We build our code base starting from SE-CFF [43] – state-of-the-art for event-based stereo – assuming the same stereo backbone as in their experiments, i.e . derived from AANet [72], and run SBN to generate the event history to be stacked.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and models. The context focuses on the code base and stereo backbone used, which are not datasets.",
      "processing_time": 45.73692774772644,
      "citing_paper_id": "271769110",
      "cited_paper_id": 216036364
    },
    {
      "context_text": "In the former case, Guided [13] is nearly ineffective, whereas both VSH and BTH largely improve the results.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods or models. The context is focused on comparing the effectiveness of different approaches.",
      "processing_time": 45.24794316291809,
      "citing_paper_id": "271769110",
      "cited_paper_id": 225072923
    },
    {
      "context_text": "Guided Stereo [13].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method or approach for stereo matching.",
      "processing_time": 45.01161170005798,
      "citing_paper_id": "271769110",
      "cited_paper_id": 225072923
    },
    {
      "context_text": "Following [13], LiDAR points are downsampled through nearest-neighbor interpolation to act at the different resolutions.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for processing LiDAR points. No verifiable resources are identified.",
      "processing_time": 45.411494731903076,
      "citing_paper_id": "271769110",
      "cited_paper_id": 225072923
    },
    {
      "context_text": "A recent trend in this field [34,38,65,71,83,84] introduced innovative deep stereo networks that embrace an iterative refinement paradigm or use Vision Transformers [22,35].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only references to methods and approaches in stereo depth estimation.",
      "processing_time": 45.18443179130554,
      "citing_paper_id": "271769110",
      "cited_paper_id": 226254259
    },
    {
      "context_text": "A recent trend in this field [34,38,65,71,83,84] introduced innovative deep stereo networks that embrace an iterative refinement paradigm or use Vision Transformers [22,35].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only references to methods and approaches in stereo depth estimation.",
      "processing_time": 45.18443179130554,
      "citing_paper_id": "271769110",
      "cited_paper_id": 247596980
    },
    {
      "context_text": "A recent trend in this field [34,38,65,71,83,84] introduced innovative deep stereo networks that embrace an iterative refinement paradigm or use Vision Transformers [22,35].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only references to methods and approaches in stereo depth estimation.",
      "processing_time": 45.18443179130554,
      "citing_paper_id": "271769110",
      "cited_paper_id": 253080413
    },
    {
      "context_text": "A recent trend in this field [34,38,65,71,83,84] introduced innovative deep stereo networks that embrace an iterative refinement paradigm or use Vision Transformers [22,35].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only references to methods and approaches in stereo depth estimation.",
      "processing_time": 45.18443179130554,
      "citing_paper_id": "271769110",
      "cited_paper_id": 257407018
    },
    {
      "context_text": "A recent trend in this field [34,38,65,71,83,84] introduced innovative deep stereo networks that embrace an iterative refinement paradigm or use Vision Transformers [22,35].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only references to methods and approaches in stereo depth estimation.",
      "processing_time": 45.18443179130554,
      "citing_paper_id": "271769110",
      "cited_paper_id": 257495841
    },
    {
      "context_text": "A recent trend in this field [34,38,65,71,83,84] introduced innovative deep stereo networks that embrace an iterative refinement paradigm or use Vision Transformers [22,35].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only references to methods and approaches in stereo depth estimation.",
      "processing_time": 45.18443179130554,
      "citing_paper_id": "271769110",
      "cited_paper_id": 260779095
    },
    {
      "context_text": "Eventually, contemporary approaches integrated depth from sensors with modern stereo networks, either by concatenating them to images as input [12,46,69,81] or by using them to guide the cost optimization process by modulating existing cost volumes [25, 49, 69, 82].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and approaches for integrating depth from sensors with stereo networks.",
      "processing_time": 45.24359583854675,
      "citing_paper_id": "271769110",
      "cited_paper_id": 232104918
    },
    {
      "context_text": "…of i) concatenating the two modalities and processing them as joint inputs with a stereo network [12,46,69,81], ii) modulating the internal cost volume computed by the backbone itself [25,49,69,82] or, more recently, iii) projecting distinctive patterns on images according to depth hints [4].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only methods and approaches for stereo depth estimation.",
      "processing_time": 45.064897298812866,
      "citing_paper_id": "271769110",
      "cited_paper_id": 232104918
    },
    {
      "context_text": "…of i) concatenating the two modalities and processing them as joint inputs with a stereo network [12,46,69,81], ii) modulating the internal cost volume computed by the backbone itself [25,49,69,82] or, more recently, iii) projecting distinctive patterns on images according to depth hints [4].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only methods and approaches for stereo depth estimation.",
      "processing_time": 45.064897298812866,
      "citing_paper_id": "271769110",
      "cited_paper_id": 262083814
    },
    {
      "context_text": "The latter adopts data-driven Convolutional Neural Networks (CNNs) to infer dense depth maps [43, 66, 67].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only the use of CNNs for depth estimation.",
      "processing_time": 44.797046422958374,
      "citing_paper_id": "271769110",
      "cited_paper_id": 250374739
    },
    {
      "context_text": "Although LiDAR sensors and event cameras have been deployed together for some applications [6,14,20,33,55,58,61], this paper represents the first attempt at combining LiDAR with an event stereo framework.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only the combination of LiDAR and event cameras. No verifiable resources are identified.",
      "processing_time": 45.56940197944641,
      "citing_paper_id": "271769110",
      "cited_paper_id": 258187423
    },
    {
      "context_text": "…consisting of i) modulating the cost volume built by the backbone – Guided Stereo Matching [49], ii) concatenating the sparse depth values to the inputs to the stereo network – e.g ., as done by LidarStereoNet [12], iii) a combination of both the previous strategies – in DSEC [21] M3ED [9] Fig.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "M3ED"
      ],
      "dataset_descriptions": {
        "M3ED": "Used to evaluate multi-robot, multi-sensor, multi-environment event-based stereo depth estimation, combining sparse depth values with stereo network inputs to improve depth accuracy."
      },
      "confidence_score": 0.85,
      "reasoning": "The context mentions 'DSEC' and 'M3ED', which are likely datasets given their acronyms and the research topic of event-based stereo depth estimation. However, only 'M3ED' is confirmed as a dataset through the cited paper title.",
      "processing_time": 48.439852237701416,
      "citing_paper_id": "271769110",
      "cited_paper_id": 259380779
    },
    {
      "context_text": "M3ED [9].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "M3ED"
      ],
      "dataset_descriptions": {
        "M3ED": "Used to evaluate multi-robot, multi-sensor systems in various environments, focusing on event detection and processing, which is relevant for event-based stereo depth estimation."
      },
      "confidence_score": 1.0,
      "reasoning": "The citation mentions M3ED, which is a dataset as indicated by the title 'M3ED: Multi-Robot, Multi-Sensor, Multi-Environment Event Dataset'. It is relevant to event-based stereo depth estimation due to its multi-sensor and multi-environment nature.",
      "processing_time": 48.366355419158936,
      "citing_paper_id": "271769110",
      "cited_paper_id": 259380779
    },
    {
      "context_text": "Our strategies outperform existing alternatives inherited from RGB stereo literature on DSEC [21] and M3ED [9] datasets – VSH and BTH can exploit even outdated LiDAR data to increase the event stream distinctiveness and ease matching, preserving the microsecond resolution of event cameras and…",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "DSEC",
        "M3ED"
      ],
      "dataset_descriptions": {
        "DSEC": "Used to evaluate the performance of event-based stereo depth estimation methods, focusing on the distinctiveness of event streams and matching accuracy.",
        "M3ED": "Used to assess the effectiveness of the proposed strategies across multiple robots, sensors, and environments, emphasizing the integration of LiDAR data with event cameras."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions two specific datasets, DSEC and M3ED, which are used to evaluate the performance of the proposed strategies in event-based stereo depth estimation.",
      "processing_time": 48.81511855125427,
      "citing_paper_id": "271769110",
      "cited_paper_id": 259380779
    },
    {
      "context_text": "Even so, complex event representations – e.g ., ERGO-12 [87] – can better generalize.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "ERGO-12"
      ],
      "dataset_descriptions": {
        "ERGO-12": "Mentioned as an example of complex event representations, but the specific usage, research context, and methodology are not detailed in the citation."
      },
      "confidence_score": 0.3,
      "reasoning": "The citation mentions 'ERGO-12' which appears to be a specific dataset or resource. However, the context does not provide enough information about its usage or relevance to the research topic of event-based stereo depth estimation.",
      "processing_time": 48.11072278022766,
      "citing_paper_id": "271769110",
      "cited_paper_id": 261031728
    },
    {
      "context_text": "ERGO-12 [87].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.2,
      "reasoning": "The citation does not provide enough context to identify a specific dataset. The title suggests a method or approach rather than a dataset.",
      "processing_time": 45.23062825202942,
      "citing_paper_id": "271769110",
      "cited_paper_id": 261031728
    },
    {
      "context_text": "Starting from baseline models, we can notice how the different representations have an impact on the accuracy of the stereo backbone, with those modeling complex behaviors – e.g ., Time Surface [32] or ERGO-12 [87] – yielding up to 2% lower 1PE than simpler ones such as Histogram [39].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions 'Time Surface' and 'ERGO-12' as event representations, but does not specify them as datasets. They are likely methods or models. No other specific datasets are mentioned.",
      "processing_time": 46.24207949638367,
      "citing_paper_id": "271769110",
      "cited_paper_id": 261031728
    },
    {
      "context_text": "[4] followed a different path with Virtual Pattern Projection (VPP).",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method called Virtual Pattern Projection (VPP).",
      "processing_time": 45.22629404067993,
      "citing_paper_id": "271769110",
      "cited_paper_id": 262083814
    },
    {
      "context_text": "Inspired by [4], events might be optionally hallucinated in patches rather than single pixels.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.0,
      "reasoning": "The citation does not mention any specific datasets, only a method or approach. The context is too vague to identify a dataset.",
      "processing_time": 45.281726360321045,
      "citing_paper_id": "271769110",
      "cited_paper_id": 262083814
    },
    {
      "context_text": "We deploy a generalized version of the random pattern operator A proposed in [4], agnostic to the stacked representation: with S − and S + the minimum and maximum values appearing across stacks S L , S R and U a uniform random distribution.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method or operator. There are no clear identifiers for datasets in the provided context.",
      "processing_time": 45.55261826515198,
      "citing_paper_id": "271769110",
      "cited_paper_id": 262083814
    },
    {
      "context_text": "Following [4], the pattern can either cover a single pixel or a local window.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method or pattern usage in stereo systems.",
      "processing_time": 44.9251344203949,
      "citing_paper_id": "271769110",
      "cited_paper_id": 262083814
    },
    {
      "context_text": "Inspired by [4], which projects distinctive color patterns on the images consistently with measured depth, we design a hallucination mechanism to generate fictitious events over time to densify the stream collected by the event cameras.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for generating fictitious events to densify the stream collected by event cameras.",
      "processing_time": 45.385780572891235,
      "citing_paper_id": "271769110",
      "cited_paper_id": 262083814
    },
    {
      "context_text": "This strategy alone is sufficient already to ensure distinctiveness and to dramatically ease matching across stacks, even more than with color images [4], since acting on semi-dense structures – i.e ., stacks are uninformative in the absence of events.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method or approach. The context is too vague to identify a reusable resource.",
      "processing_time": 45.60646057128906,
      "citing_paper_id": "271769110",
      "cited_paper_id": 262083814
    },
    {
      "context_text": "According to the RGB stereo literature, fusing color information with sparse depth measurements from an active sensor [4,12,49,82] ( e.g ., a LiDAR) considerably softens the weaknesses of passive depth sensing, despite the much lower resolution at which depth points are provided.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only general methods and concepts. No dataset names are present in the citation span.",
      "processing_time": 45.33226561546326,
      "citing_paper_id": "271769110",
      "cited_paper_id": 262083814
    },
    {
      "context_text": "It also ensures a straight-forward application of the same principles used on RGB images, e.g ., to combine the original content (color) with the virtual projection (pattern) employing alpha blending [4].",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for combining color and pattern using alpha blending.",
      "processing_time": 45.27390241622925,
      "citing_paper_id": "271769110",
      "cited_paper_id": 262083814
    },
    {
      "context_text": "In this paper, starting from the RGB literature [4,12,49,82], we embark on a comprehensive investigation into the fusion of event-based stereo with sparse depth hints from active sensors.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general literature review and a methodological approach.",
      "processing_time": 45.10065937042236,
      "citing_paper_id": "271769110",
      "cited_paper_id": 262083814
    },
    {
      "context_text": "However, the number of events between the two cameras may also differ, and so we apply a similarity loss on the census transforms [21] of the images.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific dataset names, only a method (census transforms) used for computing visual correspondence.",
      "processing_time": 45.486610889434814,
      "citing_paper_id": "56475917",
      "cited_paper_id": 703552
    },
    {
      "context_text": "2, and a robust similarity loss between the census transforms [21, 17] of the deblurred event images.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and techniques. There are no clear identifiers for datasets in the given context.",
      "processing_time": 45.59680986404419,
      "citing_paper_id": "56475917",
      "cited_paper_id": 703552
    },
    {
      "context_text": "[10], there has been a strong interest in the development of algorithms that leverage the benefits provided by these cameras.",
      "catation_intent": [],
      "resource_type": [],
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.3,
      "reasoning": "JSON decode error: Extra data: line 9 column 1 (char 338), response: ```json\n{\n    \"reasoning\": \"The citation span does not mention any specific datasets, models, or met",
      "processing_time": 45.889646768569946,
      "citing_paper_id": "56475917",
      "cited_paper_id": 2497402
    },
    {
      "context_text": "Recently, there have been several works, such as [4, 5, 13, 26, 24], that have shown that optical ﬂow, and other types of motion information, can be estimated from a spatiotemporal volume of events, by propagating the events along the optical ﬂow direction, and attempting to minimize the motion…",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general concepts and methods. There are no clear identifiers for datasets or other verifiable resources.",
      "processing_time": 45.79335808753967,
      "citing_paper_id": "56475917",
      "cited_paper_id": 3328976
    },
    {
      "context_text": "[13], who use a loss which minimizes the sum of squares of the average timestamp at each pixel.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method or approach for event-based moving object detection and tracking.",
      "processing_time": 45.482654094696045,
      "citing_paper_id": "56475917",
      "cited_paper_id": 3845250
    },
    {
      "context_text": "[13] for a neural network, by generating a single fully differentiable loss function that allows our networks to learn optical flow and structure from motion in an unsupervised manner.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for learning optical flow and structure from motion.",
      "processing_time": 45.14908528327942,
      "citing_paper_id": "56475917",
      "cited_paper_id": 3845250
    },
    {
      "context_text": "As the translation predicted by SFMLearner is only up to a scale factor, we present errors in terms of angular error between both the predicted translation and rotations.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only discusses the method's performance metric.",
      "processing_time": 44.97288990020752,
      "citing_paper_id": "56475917",
      "cited_paper_id": 11977588
    },
    {
      "context_text": "As there is currently no public code to the extent of our knowledge for unsupervised deep SFM methods with a stereo loss, we compare our ego-motion results against SFMLearner by Zhou et al. [22], which learns egomotion and depth from monocular grayscale images, while ac-knowledging that our loss has access to an additional stereo image at training time.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (SFMLearner) for comparison. The context focuses on the methodology and results rather than a dataset.",
      "processing_time": 45.99975275993347,
      "citing_paper_id": "56475917",
      "cited_paper_id": 11977588
    },
    {
      "context_text": "We train the SFMLearner models on the VI-Sensor images from the outdoor day2 sequence, once again cropping out the hood of the car.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "VI-Sensor images from the outdoor day2 sequence"
      ],
      "dataset_descriptions": {
        "VI-Sensor images from the outdoor day2 sequence": "Used to train SFMLearner models, focusing on depth and ego-motion estimation from video, with specific attention to cropping out the hood of the car."
      },
      "confidence_score": 0.85,
      "reasoning": "The context mentions using VI-Sensor images from the outdoor day2 sequence, which appears to be a specific dataset used for training models.",
      "processing_time": 47.93788385391235,
      "citing_paper_id": "56475917",
      "cited_paper_id": 11977588
    },
    {
      "context_text": "[23] show that a network can learn a camera’s egomotion and depth using camera reprojection and a photoconsistency loss.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for learning depth and ego-motion.",
      "processing_time": 45.0865843296051,
      "citing_paper_id": "56475917",
      "cited_paper_id": 11977588
    },
    {
      "context_text": "As there is currently no public code to the extent of our knowledge for unsupervised deep SFM methods with a stereo loss, we compare our ego-motion results against SFMLearner [23], and ECN [20], which learn egomotion and depth from monocular images and events.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models and methods. The context focuses on comparing ego-motion results against other methods.",
      "processing_time": 45.691014766693115,
      "citing_paper_id": "56475917",
      "cited_paper_id": 11977588
    },
    {
      "context_text": "1, we can see that our method outperforms EV-FlowNet in almost all experiments, and nears the performance of UnFlow on the short 1 frame sequences.",
      "catation_intent": "findings",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only comparisons between methods. No verifiable resources are identified.",
      "processing_time": 45.423192501068115,
      "citing_paper_id": "56475917",
      "cited_paper_id": 19160323
    },
    {
      "context_text": "1, where we compare our results against EV-FlowNet [24] and the image method UnFlow [12].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only comparisons with other methods.",
      "processing_time": 45.08216738700867,
      "citing_paper_id": "56475917",
      "cited_paper_id": 19160323
    },
    {
      "context_text": "[12] extend this work by applying a bidirectional census loss to improve the quality of the flow.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (bidirectional census loss) applied to improve optical flow quality.",
      "processing_time": 45.63029170036316,
      "citing_paper_id": "56475917",
      "cited_paper_id": 19160323
    },
    {
      "context_text": "[25] use an EM based feature tracking method to perform visual-inertial odometry, while Rebecq et al.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for visual-inertial odometry.",
      "processing_time": 45.41717290878296,
      "citing_paper_id": "56475917",
      "cited_paper_id": 20619009
    },
    {
      "context_text": "[9] demonstrate that a Kalman filter can reconstruct the pose of the camera and a local map.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (Kalman filter) and a general application (3D reconstruction and tracking).",
      "processing_time": 45.678205251693726,
      "citing_paper_id": "56475917",
      "cited_paper_id": 26324573
    },
    {
      "context_text": "Warping-based methods [17], [18], [19], [20] use optical ﬂow that perceives motion information between consecutive frames and captures dense correspondences.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only methods and techniques. No dataset names are present in the text.",
      "processing_time": 45.627657413482666,
      "citing_paper_id": "259937070",
      "cited_paper_id": 4459013
    },
    {
      "context_text": "Warping-based methods [17], [18], [19], [20] use optical ﬂow that perceives motion information between consecutive frames and captures dense correspondences.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only methods and techniques. No dataset names are present in the text.",
      "processing_time": 45.627657413482666,
      "citing_paper_id": "259937070",
      "cited_paper_id": 212675709
    },
    {
      "context_text": "Some techniques and information have been utilized to enhance the interpolation performance, e.g. , forward warping [17], context [19], depth [20], and deformable convolution [21], [22].",
      "catation_intent": [],
      "resource_type": [],
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.3,
      "reasoning": "JSON decode error: Extra data: line 9 column 1 (char 340), response: ```json\n{\n    \"reasoning\": \"The citation context does not mention any specific datasets, only techni",
      "processing_time": 45.85923719406128,
      "citing_paper_id": "259937070",
      "cited_paper_id": 4459013
    },
    {
      "context_text": "Some techniques and information have been utilized to enhance the interpolation performance, e.g. , forward warping [17], context [19], depth [20], and deformable convolution [21], [22].",
      "catation_intent": [],
      "resource_type": [],
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.3,
      "reasoning": "JSON decode error: Extra data: line 9 column 1 (char 340), response: ```json\n{\n    \"reasoning\": \"The citation context does not mention any specific datasets, only techni",
      "processing_time": 45.85923719406128,
      "citing_paper_id": "259937070",
      "cited_paper_id": 212675709
    },
    {
      "context_text": "Some techniques and information have been utilized to enhance the interpolation performance, e.g. , forward warping [17], context [19], depth [20], and deformable convolution [21], [22].",
      "catation_intent": [],
      "resource_type": [],
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.3,
      "reasoning": "JSON decode error: Extra data: line 9 column 1 (char 340), response: ```json\n{\n    \"reasoning\": \"The citation context does not mention any specific datasets, only techni",
      "processing_time": 45.85923719406128,
      "citing_paper_id": "259937070",
      "cited_paper_id": 253761147
    },
    {
      "context_text": "However, due to the lack of motion information between consecutive input keyframes, most Frame-based VFI (F-VFI) methods are built on simpliﬁed assumptions, e.g. , linear motions [17], [18], [19], [20] or local movements [15], [30], leading to performance degradation in real-world scenarios.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only methods and their limitations. No verifiable resources are identified.",
      "processing_time": 45.5719313621521,
      "citing_paper_id": "259937070",
      "cited_paper_id": 4459013
    },
    {
      "context_text": "However, due to the lack of motion information between consecutive input keyframes, most Frame-based VFI (F-VFI) methods are built on simpliﬁed assumptions, e.g. , linear motions [17], [18], [19], [20] or local movements [15], [30], leading to performance degradation in real-world scenarios.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only methods and their limitations. No verifiable resources are identified.",
      "processing_time": 45.5719313621521,
      "citing_paper_id": "259937070",
      "cited_paper_id": 13697803
    },
    {
      "context_text": "However, due to the lack of motion information between consecutive input keyframes, most Frame-based VFI (F-VFI) methods are built on simpliﬁed assumptions, e.g. , linear motions [17], [18], [19], [20] or local movements [15], [30], leading to performance degradation in real-world scenarios.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only methods and their limitations. No verifiable resources are identified.",
      "processing_time": 45.5719313621521,
      "citing_paper_id": "259937070",
      "cited_paper_id": 212675709
    },
    {
      "context_text": "To achieve better visual quality, we add the perceptual loss [52] to Lrec that is formulated as:",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a perceptual loss function. The context is focused on the methodology and loss formulation.",
      "processing_time": 45.769431591033936,
      "citing_paper_id": "259937070",
      "cited_paper_id": 4766599
    },
    {
      "context_text": "2) Benchmark: For VFI task, we compare SEVFI-Net with three open-sourced E-VFI methods, i.e. , CBMNet [38], RE-FID [39] and Time Lens [7]; and seven state-of-the-art F-VFI methods, DAIN [20], EMA-VFI [24], FLAVR [29], RIFE [13], RRIN [16], Super Slomo [57] and TTVFI [25].",
      "catation_intent": "reusable resource",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets. It only lists methods and models used for benchmarking.",
      "processing_time": 45.571836948394775,
      "citing_paper_id": "259937070",
      "cited_paper_id": 10817557
    },
    {
      "context_text": "2) Benchmark: For VFI task, we compare SEVFI-Net with three open-sourced E-VFI methods, i.e. , CBMNet [38], RE-FID [39] and Time Lens [7]; and seven state-of-the-art F-VFI methods, DAIN [20], EMA-VFI [24], FLAVR [29], RIFE [13], RRIN [16], Super Slomo [57] and TTVFI [25].",
      "catation_intent": "reusable resource",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets. It only lists methods and models used for benchmarking.",
      "processing_time": 45.571836948394775,
      "citing_paper_id": "259937070",
      "cited_paper_id": 211731854
    },
    {
      "context_text": "2) Benchmark: For VFI task, we compare SEVFI-Net with three open-sourced E-VFI methods, i.e. , CBMNet [38], RE-FID [39] and Time Lens [7]; and seven state-of-the-art F-VFI methods, DAIN [20], EMA-VFI [24], FLAVR [29], RIFE [13], RRIN [16], Super Slomo [57] and TTVFI [25].",
      "catation_intent": "reusable resource",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets. It only lists methods and models used for benchmarking.",
      "processing_time": 45.571836948394775,
      "citing_paper_id": "259937070",
      "cited_paper_id": 250644220
    },
    {
      "context_text": "2) Benchmark: For VFI task, we compare SEVFI-Net with three open-sourced E-VFI methods, i.e. , CBMNet [38], RE-FID [39] and Time Lens [7]; and seven state-of-the-art F-VFI methods, DAIN [20], EMA-VFI [24], FLAVR [29], RIFE [13], RRIN [16], Super Slomo [57] and TTVFI [25].",
      "catation_intent": "reusable resource",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets. It only lists methods and models used for benchmarking.",
      "processing_time": 45.571836948394775,
      "citing_paper_id": "259937070",
      "cited_paper_id": 255749379
    },
    {
      "context_text": ", forward warping [17], transformer [18], context [19], depth [20], patch-based [21], [22] and deformable convolution [23], [24].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only various methods and techniques. No dataset names are present in the context.",
      "processing_time": 45.71920299530029,
      "citing_paper_id": "259937070",
      "cited_paper_id": 12128172
    },
    {
      "context_text": ", forward warping [17], transformer [18], context [19], depth [20], patch-based [21], [22] and deformable convolution [23], [24].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only various methods and techniques. No dataset names are present in the context.",
      "processing_time": 45.71920299530029,
      "citing_paper_id": "259937070",
      "cited_paper_id": 253553798
    },
    {
      "context_text": "Kernel-based methods [15], [30] incorporate both motion estimation and frame reconstruction.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and techniques. There are no clear identifiers for datasets in the provided context.",
      "processing_time": 45.85926103591919,
      "citing_paper_id": "259937070",
      "cited_paper_id": 13697803
    },
    {
      "context_text": "Besides, Transformer [23] has also been used in VFI tasks, Zhang et al. [24] design a hybrid CNN and Transformer, and propose to utilize inter-frame attention to extract both motion and appearance information.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models and methods. The context focuses on the use of Transformer and CNN architectures for video frame interpolation (VFI) tasks.",
      "processing_time": 46.19087243080139,
      "citing_paper_id": "259937070",
      "cited_paper_id": 13756489
    },
    {
      "context_text": "Some approaches design complex high-order motion models, such as cubic [26], [27] and quadratic [28], to address non-linear motions.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only methods or models. The cited papers' titles do not provide additional information about datasets.",
      "processing_time": 45.89431405067444,
      "citing_paper_id": "259937070",
      "cited_paper_id": 17839778
    },
    {
      "context_text": "Some approaches design complex high-order motion models, such as cubic [26], [27] and quadratic [28], to address non-linear motions.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only methods or models. The cited papers' titles do not provide additional information about datasets.",
      "processing_time": 45.89431405067444,
      "citing_paper_id": "259937070",
      "cited_paper_id": 202782364
    },
    {
      "context_text": "Some approaches design complex high-order motion models, such as cubic [26], [27] and quadratic [28], to address non-linear motions.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only methods or models. The cited papers' titles do not provide additional information about datasets.",
      "processing_time": 45.89431405067444,
      "citing_paper_id": "259937070",
      "cited_paper_id": 220713296
    },
    {
      "context_text": "These methods are mainly based on RNN [33], [34], [35] or GAN [36], [37].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only models and methods. There are no verifiable resources that meet the criteria.",
      "processing_time": 45.807323932647705,
      "citing_paper_id": "259937070",
      "cited_paper_id": 115151433
    },
    {
      "context_text": "3) TrainingDetails: WeimplementtheproposedSEVFI-Net in Pytorch [59] and train three models separately on DSEC, MVSEC, and SEID datasets.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "DSEC",
        "MVSEC",
        "SEID"
      ],
      "dataset_descriptions": {
        "DSEC": "Used to train SEVFI-Net for event-based stereo depth estimation, focusing on dynamic scenes and high-speed events.",
        "MVSEC": "Used to train SEVFI-Net for event-based stereo depth estimation, emphasizing multi-view scenarios and complex environments.",
        "SEID": "Used to train SEVFI-Net for event-based stereo depth estimation, targeting specific challenges in event camera data."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions specific datasets used for training models, which are relevant to the research topic of event-based stereo depth estimation.",
      "processing_time": 49.4973783493042,
      "citing_paper_id": "259937070",
      "cited_paper_id": 202786778
    },
    {
      "context_text": "We also incorporate the edge-aware disparity smoothness loss Lds used in [54] to promote the local smoothness of disparities by computing the cost using the gradients of both disparities and frames, which is represented as:",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (edge-aware disparity smoothness loss).",
      "processing_time": 45.28998637199402,
      "citing_paper_id": "259937070",
      "cited_paper_id": 206596513
    },
    {
      "context_text": "The metrics of Peak Signal to Noise Ratio (PSNR) and Structural SIMilarity (SSIM) [58] are used for quantitative evaluation.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions PSNR and SSIM but does not refer to any specific dataset. These are metrics, not datasets.",
      "processing_time": 45.667654037475586,
      "citing_paper_id": "259937070",
      "cited_paper_id": 207761262
    },
    {
      "context_text": "We compare the results with other state-of-the-art VFI methods, such as DAIN [20], EMA-VFI [24], FLAVR [29], RIFE [13], RRIN [16], TTVFI [25], CBMNet [38] and Time Lens [7]. in our result.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only various video frame interpolation methods. No verifiable resources are identified.",
      "processing_time": 45.661394357681274,
      "citing_paper_id": "259937070",
      "cited_paper_id": 211731854
    },
    {
      "context_text": "We compare the results with other state-of-the-art VFI methods, such as DAIN [20], EMA-VFI [24], FLAVR [29], RIFE [13], RRIN [16], TTVFI [25], CBMNet [38] and Time Lens [7]. in our result.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only various video frame interpolation methods. No verifiable resources are identified.",
      "processing_time": 45.661394357681274,
      "citing_paper_id": "259937070",
      "cited_paper_id": 250644220
    },
    {
      "context_text": "To relieve the burden, existing VFI approaches commonly rely on inter-frame motion prediction from neighboring frames [16], and thus can be roughly categorized into warping-based and kernel-based.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and approaches. There are no verifiable resources that meet the criteria for inclusion.",
      "processing_time": 45.901304721832275,
      "citing_paper_id": "259937070",
      "cited_paper_id": 211731854
    },
    {
      "context_text": ", Time Lens [7] and four stateof-the-art F-VFI methods, DAIN [20], RIFE [13], RRIN [16],",
      "catation_intent": "none",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only methods and models. No dataset names are present in the text.",
      "processing_time": 45.70696783065796,
      "citing_paper_id": "259937070",
      "cited_paper_id": 211731854
    },
    {
      "context_text": ", Time Lens [7] and four stateof-the-art F-VFI methods, DAIN [20], RIFE [13], RRIN [16],",
      "catation_intent": "none",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only methods and models. No dataset names are present in the text.",
      "processing_time": 45.70696783065796,
      "citing_paper_id": "259937070",
      "cited_paper_id": 244499996
    },
    {
      "context_text": "There are two public stereo event datasets, DSEC [5] and MVSEC [6].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "DSEC",
        "MVSEC"
      ],
      "dataset_descriptions": {
        "DSEC": "Used to evaluate event-based stereo algorithms in driving scenarios, focusing on dynamic environments and high-speed motion.",
        "MVSEC": "Applied to test event-based stereo vision systems, emphasizing real-world conditions and varying lighting."
      },
      "confidence_score": 1.0,
      "reasoning": "The citation mentions two specific datasets, DSEC and MVSEC, which are relevant to the topic of event-based stereo depth estimation.",
      "processing_time": 47.97712826728821,
      "citing_paper_id": "259937070",
      "cited_paper_id": 232170230
    },
    {
      "context_text": "The dataset features frames at 60 FPS, which [6] AND DSEC [5] is higher than the previous stereo datasets as shown in Table II.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "DSEC"
      ],
      "dataset_descriptions": {
        "DSEC": "Used to evaluate event-based stereo depth estimation methods, featuring frames at 60 FPS, which is higher than previous stereo datasets, enhancing temporal resolution."
      },
      "confidence_score": 1.0,
      "reasoning": "DSEC is identified as a dataset in the context, and it is compared with previous stereo datasets, indicating its relevance to event-based stereo depth estimation.",
      "processing_time": 47.64732003211975,
      "citing_paper_id": "259937070",
      "cited_paper_id": 232170230
    },
    {
      "context_text": "However, this setup suffers from the cross-modality parallax issue, especially in real-world scenes with complex non-linear motions and varying depths [5], [6].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.2,
      "reasoning": "The citation does not mention any specific datasets by name, only referring to issues in real-world scenes. The cited paper title suggests a relevant dataset, but it is not explicitly mentioned in the citation context.",
      "processing_time": 46.211785078048706,
      "citing_paper_id": "259937070",
      "cited_paper_id": 232170230
    },
    {
      "context_text": "We use DSEC [5], MVSEC [6], and our SEID datasets for training and evaluation.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "DSEC",
        "MVSEC",
        "SEID"
      ],
      "dataset_descriptions": {
        "DSEC": "Used for training and evaluating event-based stereo depth estimation models in driving scenarios, providing synchronized event and intensity data.",
        "MVSEC": "Used for training and evaluating event-based stereo depth estimation models, offering diverse indoor and outdoor sequences with ground truth.",
        "SEID": "Self-created dataset used for training and evaluating event-based stereo depth estimation models, likely tailored to specific research needs."
      },
      "confidence_score": 0.9,
      "reasoning": "The context mentions three datasets by name, two of which are clearly identified as datasets for event-based stereo depth estimation. The third is a self-created dataset.",
      "processing_time": 49.48115611076355,
      "citing_paper_id": "259937070",
      "cited_paper_id": 232170230
    },
    {
      "context_text": "Additionally,duetothelimiteddiversityofscenescapturedby current stereo datasets such as the Stereo Event Camera Dataset for Driving Scenarios (DSEC) [5] and the Multi-Vehicle Stereo Event Camera Dataset (MVSEC) [6], they are unable to effectively evaluate the performance of methods in various…",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Stereo Event Camera Dataset for Driving Scenarios (DSEC)",
        "Multi-Vehicle Stereo Event Camera Dataset (MVSEC)"
      ],
      "dataset_descriptions": {
        "Stereo Event Camera Dataset for Driving Scenarios (DSEC)": "Used to evaluate the performance of event-based stereo depth estimation methods in driving scenarios, highlighting limitations in scene diversity.",
        "Multi-Vehicle Stereo Event Camera Dataset (MVSEC)": "Used to assess the effectiveness of event-based stereo depth estimation methods across multiple vehicles, noting limitations in scene diversity."
      },
      "confidence_score": 0.9,
      "reasoning": "The citation mentions two specific datasets, DSEC and MVSEC, which are used to highlight the limited diversity of scenes captured in current stereo datasets.",
      "processing_time": 49.72460746765137,
      "citing_paper_id": "259937070",
      "cited_paper_id": 232170230
    },
    {
      "context_text": "…on synthetic data, restricted by per-pixel spatial alignment and ideal imaging without intense motion and sudden brightness change, which is difﬁcult to fulﬁll in real-world applications since the events and frames are usually captured separately by an event camera and an intensity camera [5], [7].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions the challenges of using synthetic data for event-based stereo depth estimation, implying the need for real-world datasets. However, no specific dataset is named in the context.",
      "processing_time": 46.045226097106934,
      "citing_paper_id": "259937070",
      "cited_paper_id": 232170230
    },
    {
      "context_text": "Although Although existing works [5], [7] directly apply global homography and stereo rectiﬁcation, it is only valid for scenes with a large depth [5] or within a plane [7].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and approaches. The cited paper title suggests a relevant dataset, but it is not mentioned in the citation context.",
      "processing_time": 45.998162031173706,
      "citing_paper_id": "259937070",
      "cited_paper_id": 232170230
    },
    {
      "context_text": "[5], MVSEC [6], AND O UR SEID D ATASETS TABLE V C OMPARISONS ON M ODEL P ARAMETERS AND R UNTIMES continuous multi-frame reconstruction and exhibiting excellent ability in reconstructing dynamic scenes, with the motion patterns closely resembling the ground truth.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "MVSEC",
        "DSEC"
      ],
      "dataset_descriptions": {
        "MVSEC": "Used for evaluating event-based stereo depth estimation, focusing on continuous multi-frame reconstruction and dynamic scene reconstruction.",
        "DSEC": "Used for evaluating event-based stereo depth estimation in driving scenarios, focusing on dynamic scene reconstruction and motion patterns."
      },
      "confidence_score": 1.0,
      "reasoning": "The citation mentions 'MVSEC' and 'DSEC', both of which are known datasets in the field of event-based vision. 'MVSEC' is explicitly mentioned, and 'DSEC' is referenced in the cited paper title.",
      "processing_time": 49.01455044746399,
      "citing_paper_id": "259937070",
      "cited_paper_id": 232170230
    },
    {
      "context_text": "[5] and MVSEC [6] are concentrated within a small range, indicatingthatthescenestheycapturearesimilarandhomogeneous.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "MVSEC",
        "HES"
      ],
      "dataset_descriptions": {
        "MVSEC": "Used to evaluate event-based stereo depth estimation methods, focusing on the homogeneity of captured scenes and their similarity to driving scenarios.",
        "HES": "Used to evaluate event-based stereo depth estimation methods, focusing on the homogeneity of captured scenes and their similarity to driving scenarios."
      },
      "confidence_score": 0.8,
      "reasoning": "The context mentions two datasets, MVSEC and DSEC, which are relevant to event-based stereo depth estimation. The citation discusses the homogeneity of scenes captured by these datasets.",
      "processing_time": 48.946455240249634,
      "citing_paper_id": "259937070",
      "cited_paper_id": 232170230
    },
    {
      "context_text": "Recently, event cameras have been adopted for high-quality VFI [7], [8], [9], [10], [11], [12].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to papers that discuss the use of event cameras for video frame interpolation.",
      "processing_time": 45.91171479225159,
      "citing_paper_id": "259937070",
      "cited_paper_id": 235719472
    },
    {
      "context_text": "…Event-based Video Frame Interpolation (E-VFI) approaches, most of which rely on simulation datasets and require per-pixel spatial alignment between events and frames [7], [8], [9], [10], [11], [12], leading to artifacts and distortions with the stereo event-intensity camera setup as shown in Fig.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.3,
      "reasoning": "The citation mentions 'simulation datasets' but does not provide specific names. The context suggests these datasets are used for training and evaluating E-VFI approaches, but no specific dataset names are given.",
      "processing_time": 46.44016623497009,
      "citing_paper_id": "259937070",
      "cited_paper_id": 235719472
    },
    {
      "context_text": "Comparison of continuous frame reconstruction results by RIFE [13], Time Lens [7] and the proposed SEVFI-Net on SEID.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "SEID"
      ],
      "dataset_descriptions": {
        "SEID": "Used to compare continuous frame reconstruction results by RIFE, Time Lens, and SEVFI-Net, focusing on video frame interpolation performance."
      },
      "confidence_score": 0.9,
      "reasoning": "The context mentions 'SEID' which appears to be a dataset name. It is used to compare the performance of different methods for video frame interpolation.",
      "processing_time": 47.63679313659668,
      "citing_paper_id": "259937070",
      "cited_paper_id": 244499996
    },
    {
      "context_text": ", RIFE [13] and Time Lens [7] to generate intermediate frames and then input them along with the event stream into two existing stereo matching algorithms, i.",
      "catation_intent": "none",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and tools. There are no clear identifiers for datasets in the provided context.",
      "processing_time": 45.87960910797119,
      "citing_paper_id": "259937070",
      "cited_paper_id": 244499996
    },
    {
      "context_text": "Methods EPE (px)↓ > 1px (%)↓ > 2px (%)↓ > 3px (%)↓ DSEC HSM [3] + RIFE [13] 12.",
      "catation_intent": "findings",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and performance metrics. The context is focused on comparing methods for event-based stereo depth estimation.",
      "processing_time": 46.031320095062256,
      "citing_paper_id": "259937070",
      "cited_paper_id": 244499996
    },
    {
      "context_text": "Methods EPE (px)↓ > 1px (%)↓ > 2px (%)↓ > 3px (%)↓ DSEC HSM [3] + RIFE [13] 12.",
      "catation_intent": "findings",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and performance metrics. The context is focused on comparing methods for event-based stereo depth estimation.",
      "processing_time": 46.031320095062256,
      "citing_paper_id": "259937070",
      "cited_paper_id": 253651036
    },
    {
      "context_text": "(b) Comparison of video frame interpolation results of RIFE [13], Time Lens [7], and our proposed SEVFI-Net.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only comparisons between methods. No verifiable resources are identified.",
      "processing_time": 45.682177782058716,
      "citing_paper_id": "259937070",
      "cited_paper_id": 244499996
    },
    {
      "context_text": "In terms of runtime on single frame interpolation settings, ours is lower than DAIN [20], TTVFI [25] and CBMNet [39].",
      "catation_intent": "findings",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets. It only compares the runtime performance of the current method with other methods.",
      "processing_time": 45.82007145881653,
      "citing_paper_id": "259937070",
      "cited_paper_id": 250644220
    },
    {
      "context_text": "In terms of runtime on single frame interpolation settings, ours is lower than DAIN [20], TTVFI [25] and CBMNet [39].",
      "catation_intent": "findings",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets. It only compares the runtime performance of the current method with other methods.",
      "processing_time": 45.82007145881653,
      "citing_paper_id": "259937070",
      "cited_paper_id": 255749379
    },
    {
      "context_text": "In addition, Liu et al. [25] focus on regions with motion consistency differences and propose a trajectory-aware Transformer.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method or model. The context focuses on the methodological approach of using a trajectory-aware Transformer.",
      "processing_time": 46.07230806350708,
      "citing_paper_id": "259937070",
      "cited_paper_id": 250644220
    },
    {
      "context_text": "Typically, only around 10% of the depth values are valid, while the remaining 90% are empty [55].",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific dataset names, only a general observation about depth values. No verifiable resource is identified.",
      "processing_time": 45.86218285560608,
      "citing_paper_id": "259937070",
      "cited_paper_id": 251903532
    },
    {
      "context_text": "Warping-based methods [17]–[21] utilize optical flow that perceives motion information between consecutive frames and captures dense correspondences.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and approaches. No dataset names are present in the citation span.",
      "processing_time": 45.82476854324341,
      "citing_paper_id": "259937070",
      "cited_paper_id": 253553798
    },
    {
      "context_text": ", HSM [3] and SSIE [1], to compare their performance with our method.",
      "catation_intent": [],
      "resource_type": [],
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.3,
      "reasoning": "JSON decode error: Extra data: line 9 column 1 (char 272), response: ```json\n{\n    \"reasoning\": \"The citation does not mention any specific datasets, only methods (HSM a",
      "processing_time": 48.450981855392456,
      "citing_paper_id": "259937070",
      "cited_paper_id": 253651036
    },
    {
      "context_text": "Therefore, we ﬁrst choose two representative VFI methods, i.e. , RIFE [13] and Time Lens [7] to generate intermediate frames, and then input them along with the event stream into two existing stereo matching methods, i.e. , HSM [3] and SSIE [1], to compare their performance with our method.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and models. The context focuses on comparing performance of different methods.",
      "processing_time": 45.86236333847046,
      "citing_paper_id": "259937070",
      "cited_paper_id": 253651036
    },
    {
      "context_text": "For stereo matching task, we compare SEVFI-Net with two existing cross-modal methods, i.e. , HSM [3] and SSIE [1].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only comparisons between methods. No dataset names are present in the citation span.",
      "processing_time": 45.87128233909607,
      "citing_paper_id": "259937070",
      "cited_paper_id": 253651036
    },
    {
      "context_text": "Due to the fact that event and intensity cameras perceive the same light field, the edge information extracted from events and intensity images can be correlated to calculate the sparse disparity map [3], [4].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for correlating edge information from events and intensity images.",
      "processing_time": 45.81228160858154,
      "citing_paper_id": "259937070",
      "cited_paper_id": 253651036
    },
    {
      "context_text": "Recently, event cameras have been adopted for high-quality VFI [7]–[12].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only the adoption of event cameras for video frame interpolation. No verifiable resources are identified.",
      "processing_time": 45.976890087127686,
      "citing_paper_id": "259937070",
      "cited_paper_id": 254612876
    },
    {
      "context_text": "[12] analyze the drawbacks of existing methods and introduce a novel method named EVA(2) for E-VFI via cross-modal alignment and aggregation.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method called EVA(2).",
      "processing_time": 45.75945591926575,
      "citing_paper_id": "259937070",
      "cited_paper_id": 254612876
    },
    {
      "context_text": "To tackle this problem, E-VFI methods are proposed by predicting inter-frame motion information with events [7]–[9], [11], [12], achieving better interpolation performance than FVFI methods.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and approaches. The context focuses on comparing E-VFI methods with FVFI methods using event data, but no named datasets are referenced.",
      "processing_time": 46.45883083343506,
      "citing_paper_id": "259937070",
      "cited_paper_id": 254612876
    },
    {
      "context_text": "As a result, it can significantly degenerate the performance of existing Eventbased Video Frame Interpolation (E-VFI) approaches, most of which rely on simulation datasets and require per-pixel spatial alignment between events and frames [7]–[12], leading to",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.3,
      "reasoning": "The context mentions 'simulation datasets' but does not provide specific names. It is unclear if these are general simulation datasets or specific ones.",
      "processing_time": 46.060916900634766,
      "citing_paper_id": "259937070",
      "cited_paper_id": 254612876
    },
    {
      "context_text": "Sun et al. [39] propose a framework for solving general E-VFI and event-based image deblurring.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method or framework for event-based frame interpolation and deblurring.",
      "processing_time": 45.888874530792236,
      "citing_paper_id": "259937070",
      "cited_paper_id": 255749379
    },
    {
      "context_text": "Compared to F-VFI methods, existing E-VFI methods can estimate a more precise inter-frame motion model with the help of events, thus achieving better frame interpolation reconstruction results [7], [8], [9], [38], [39].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only references to methods and their performance improvements.",
      "processing_time": 45.756288051605225,
      "citing_paper_id": "259937070",
      "cited_paper_id": 255749379
    },
    {
      "context_text": "Quantitative comparisons: Quantitative comparisons are given in Tables IV and V. 1 We ﬁrst notice that E-VFI methods, i.e. , CBMNet [38], REFID [39] and Time Lens [7], despite their utilization of events to estimate motion information and the integration of synthesis-based and ﬂow-based approaches,…",
      "catation_intent": "findings",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and models. The context focuses on comparing different event-based methods for frame interpolation and motion estimation.",
      "processing_time": 46.14028191566467,
      "citing_paper_id": "259937070",
      "cited_paper_id": 255749379
    }
  ],
  "filtering_stats": {
    "original_papers_count": 34,
    "filtered_papers_count": 33,
    "filtered_percentage": "97.1%"
  },
  "extraction_stats": {
    "unique_contexts_processed": 888,
    "total_citation_instances": 1082,
    "successful_extractions": 108,
    "failed_extractions": 974,
    "total_processing_time": 50.96749186515808
  }
}