{
  "results": [
    {
      "context_text": "This metric is defined as follows [20]:",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a metric definition. No datasets are referenced or used in the context provided.",
      "processing_time": 30.003865957260132,
      "citing_paper_id": "37664826",
      "cited_paper_id": 458430
    },
    {
      "context_text": "Humenberger analyzed in his thesis [20] some frame-based algorithms concerning the Mde/s, e. g., the Mobile Ranger, a SAD based stereo matching algorithm implemented on an FPGA, achieves 996.24Mde/s even at 30 fps.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions 'Mde/s' which is likely a metric, and 'Mobile Ranger', which appears to be a method or tool. No specific dataset is mentioned.",
      "processing_time": 19.05906081199646,
      "citing_paper_id": "37664826",
      "cited_paper_id": 458430
    },
    {
      "context_text": "Humenberger analyzed in his thesis [20] some frame-based algorithms concerning the Mde/s, e.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.2,
      "reasoning": "The context mentions 'Mde/s' which could be a typo or abbreviation for a dataset, but there is insufficient information to confirm it as a specific, verifiable dataset. The cited paper title does not help in disambiguating.",
      "processing_time": 31.26325297355652,
      "citing_paper_id": "37664826",
      "cited_paper_id": 458430
    },
    {
      "context_text": "A census transform based algorithm introduced by Humenberger [20] is evaluated by Jung [24] on different platforms.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only algorithms and platforms. There are no clear identifiers for datasets in the provided context.",
      "processing_time": 16.737956285476685,
      "citing_paper_id": "37664826",
      "cited_paper_id": 458430
    },
    {
      "context_text": "More information about the reference stereo algorithm and the ground truth evaluation system can be found in work of Humenberger et al. [20] and Kogler et al. [21].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only references to algorithms and evaluation systems.",
      "processing_time": 16.959990739822388,
      "citing_paper_id": "37664826",
      "cited_paper_id": 458430
    },
    {
      "context_text": "Recently, in the work of Firouzi and Conradt [16] an event-driven stereo matching approach was introduced, using a dynamic cooperative neural network, which processes each incoming event without any buffering.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for event-driven stereo matching.",
      "processing_time": 15.03976559638977,
      "citing_paper_id": "37664826",
      "cited_paper_id": 4833834
    },
    {
      "context_text": "A more detailed description of the event-driven stereo matching approach and its implementation can be found in [4], [19].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to other papers for more detailed information about the approach and implementation.",
      "processing_time": 17.696290731430054,
      "citing_paper_id": "37664826",
      "cited_paper_id": 11395394
    },
    {
      "context_text": "A more detailed description of the event-driven stereo matching approach and its implementation can be found in [4], [19].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to other papers for more detailed information about the approach and implementation.",
      "processing_time": 17.696290731430054,
      "citing_paper_id": "37664826",
      "cited_paper_id": null
    },
    {
      "context_text": "This frameless, asynchronous, timecontinuous, logarithmic photoreceptor offers three essential advantages [4]:",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a hardware architecture for a stereo vision algorithm.",
      "processing_time": 15.480834484100342,
      "citing_paper_id": "37664826",
      "cited_paper_id": 11395394
    },
    {
      "context_text": "Stereo matching using neuromorphic sensors got started with the development of the silicon retina sensors, and in 1989 Mahowald and Delbrück [8] presented a stereo matching approach using a cooperative algorithm, applied on event-based data using static and dynamic image features.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions 'event-based data' but does not specify a named dataset. The context focuses on the method and findings rather than a specific dataset.",
      "processing_time": 17.475483417510986,
      "citing_paper_id": "37664826",
      "cited_paper_id": 15077875
    },
    {
      "context_text": "Stereo matching using neuromorphic sensors got started with the development of the silicon retina sensors, and in 1989 Mahowald and Delbrück [8] presented a stereo matching approach using a cooperative algorithm, applied on event-based data using static and dynamic image features.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions 'event-based data' but does not specify a dataset name. The context is about the methodology and findings rather than a specific dataset.",
      "processing_time": 26.374850749969482,
      "citing_paper_id": "37664826",
      "cited_paper_id": 15077875
    },
    {
      "context_text": "In many computer vision applications, the usage of sparse event information can be an efficient method for enhancing the performance [1], [2], [3], even on embedded computing platforms.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only the general concept of using sparse event information in computer vision applications.",
      "processing_time": 16.05340266227722,
      "citing_paper_id": "37664826",
      "cited_paper_id": 16560320
    },
    {
      "context_text": "In many computer vision applications, the usage of sparse event information can be an efficient method for enhancing the performance [1], [2], [3], even on embedded computing platforms.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only the general concept of using sparse event information in computer vision applications.",
      "processing_time": 16.05340266227722,
      "citing_paper_id": "37664826",
      "cited_paper_id": 207020771
    },
    {
      "context_text": "Similarly, in order to extend the limited application of a single event camera, stereo event studies [7], [8], [9], [10], [11], [12], [13], [14], [15], [16] aimed to estimate the depth.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to stereo event studies. No clear, verifiable datasets are identified.",
      "processing_time": 27.688233137130737,
      "citing_paper_id": "253651036",
      "cited_paper_id": 1408596
    },
    {
      "context_text": "Similarly, in order to extend the limited application of a single event camera, stereo event studies [7], [8], [9], [10], [11], [12], [13], [14], [15], [16] aimed to estimate the depth.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to stereo event studies. No clear, verifiable datasets are identified.",
      "processing_time": 27.688233137130737,
      "citing_paper_id": "253651036",
      "cited_paper_id": 4412139
    },
    {
      "context_text": "Similarly, in order to extend the limited application of a single event camera, stereo event studies [7], [8], [9], [10], [11], [12], [13], [14], [15], [16] aimed to estimate the depth.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to stereo event studies. No clear, verifiable datasets are identified.",
      "processing_time": 27.688233137130737,
      "citing_paper_id": "253651036",
      "cited_paper_id": 12047627
    },
    {
      "context_text": "Similarly, in order to extend the limited application of a single event camera, stereo event studies [7], [8], [9], [10], [11], [12], [13], [14], [15], [16] aimed to estimate the depth.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to stereo event studies. No clear, verifiable datasets are identified.",
      "processing_time": 27.688233137130737,
      "citing_paper_id": "253651036",
      "cited_paper_id": 46937991
    },
    {
      "context_text": "Similarly, in order to extend the limited application of a single event camera, stereo event studies [7], [8], [9], [10], [11], [12], [13], [14], [15], [16] aimed to estimate the depth.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to stereo event studies. No clear, verifiable datasets are identified.",
      "processing_time": 27.688233137130737,
      "citing_paper_id": "253651036",
      "cited_paper_id": 49554392
    },
    {
      "context_text": "Similarly, in order to extend the limited application of a single event camera, stereo event studies [7], [8], [9], [10], [11], [12], [13], [14], [15], [16] aimed to estimate the depth.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to stereo event studies. No clear, verifiable datasets are identified.",
      "processing_time": 27.688233137130737,
      "citing_paper_id": "253651036",
      "cited_paper_id": 49877954
    },
    {
      "context_text": "Similarly, in order to extend the limited application of a single event camera, stereo event studies [7], [8], [9], [10], [11], [12], [13], [14], [15], [16] aimed to estimate the depth.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to stereo event studies. No clear, verifiable datasets are identified.",
      "processing_time": 27.688233137130737,
      "citing_paper_id": "253651036",
      "cited_paper_id": 197431184
    },
    {
      "context_text": "Similarly, in order to extend the limited application of a single event camera, stereo event studies [7], [8], [9], [10], [11], [12], [13], [14], [15], [16] aimed to estimate the depth.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to stereo event studies. No clear, verifiable datasets are identified.",
      "processing_time": 27.688233137130737,
      "citing_paper_id": "253651036",
      "cited_paper_id": 250918780
    },
    {
      "context_text": "This condition is a more relaxed than [7] which assumes that the depth of the event is the same regardless of the event time.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a condition related to depth estimation in event-based stereo systems.",
      "processing_time": 17.189539670944214,
      "citing_paper_id": "253651036",
      "cited_paper_id": 4412139
    },
    {
      "context_text": "The authors of [7] estimated the disparity after aligning events with optical ﬂow in consideration of camera motion.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for estimating disparity using event-based stereo. No verifiable resources are identified.",
      "processing_time": 26.049283742904663,
      "citing_paper_id": "253651036",
      "cited_paper_id": 4412139
    },
    {
      "context_text": "Whilethemethod[7]utilizedthegroundtruthcamerapose andassumedthatthedepthofalleventsinashorttimewindow onlydependsonthepixelcoordinate(notthetemporalcoor-dinate),weestimatethecameraposefromtheinitialmatching methodandassumethateventshavethesamedepthvalueonly atthereferencetime.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and assumptions about camera pose and event depth. No verifiable resource names are present.",
      "processing_time": 28.182029247283936,
      "citing_paper_id": "253651036",
      "cited_paper_id": 4412139
    },
    {
      "context_text": "In [8], semi-dense 3-",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.0,
      "reasoning": "The citation does not provide enough information to identify a specific dataset. The context mentions 'semi-dense 3D reconstruction' but does not name a dataset.",
      "processing_time": 17.634151458740234,
      "citing_paper_id": "253651036",
      "cited_paper_id": 49877954
    },
    {
      "context_text": "In [22], binary event edge images are obtained from the high-pass filter of [24] with fixed parameters.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods or tools. The context focuses on the process of obtaining binary event edge images using a high-pass filter.",
      "processing_time": 17.444733142852783,
      "citing_paper_id": "253651036",
      "cited_paper_id": 53182986
    },
    {
      "context_text": "SHEF reconstructs edge images using the high-pass filter of [24] and non-maximal suppression,",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for reconstructing edge images. No verifiable resources are identified.",
      "processing_time": 28.399048328399658,
      "citing_paper_id": "253651036",
      "cited_paper_id": 53182986
    },
    {
      "context_text": "In E2VID methods, especially E2VID-τ , there exist improperly reconstructed local regions for some data sequences, as already reported in [31].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (E2VID) and its issues. No verifiable resources are identified.",
      "processing_time": 16.433451414108276,
      "citing_paper_id": "253651036",
      "cited_paper_id": 115151433
    },
    {
      "context_text": "We compared the proposed method with the initial stereo matching method described in Section III-A, E2VID [25] based method and the implemented version of SHEF [22].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions E2VID and SHEF, but does not specify them as datasets. They are likely methods or tools. No clear dataset names are provided.",
      "processing_time": 17.926043272018433,
      "citing_paper_id": "253651036",
      "cited_paper_id": 189998802
    },
    {
      "context_text": "We compared the proposed method with the initial stereo matching method described in Section III-A, E2VID [25] based method and the implemented version of SHEF [22].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions E2VID and SHEF, but does not specify them as datasets. They are likely methods or tools. No clear dataset names are provided.",
      "processing_time": 17.926043272018433,
      "citing_paper_id": "253651036",
      "cited_paper_id": 238582843
    },
    {
      "context_text": "NCC cost based E2VIDN [25] methods showed comparable disparity RMSE to the pro-",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (E2VIDN) and a metric (RMSE). No verifiable resources are identified.",
      "processing_time": 18.174973726272583,
      "citing_paper_id": "253651036",
      "cited_paper_id": 189998802
    },
    {
      "context_text": "The event data should be reconstructed into frame images [25], [26], [27], [28], or frame images should be converted into the event camera-like images.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only methods or processes involving event data and frame images. No clear, verifiable dataset names are present.",
      "processing_time": 17.126410961151123,
      "citing_paper_id": "253651036",
      "cited_paper_id": 189998802
    },
    {
      "context_text": "The event data should be reconstructed into frame images [25], [26], [27], [28], or frame images should be converted into the event camera-like images.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only methods or processes involving event data and frame images. No clear, verifiable dataset names are present.",
      "processing_time": 17.126410961151123,
      "citing_paper_id": "253651036",
      "cited_paper_id": 232110319
    },
    {
      "context_text": "methods (HSM-Init, E2VID-τ [25] and SHEF [22]) for inliers and disparity RMSE and MAE.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods and metrics. No dataset names are present in the text.",
      "processing_time": 16.866601943969727,
      "citing_paper_id": "253651036",
      "cited_paper_id": 189998802
    },
    {
      "context_text": "We evaluate [25] into the two event grouping manners.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.0,
      "reasoning": "The citation does not mention any specific dataset names. The context is too vague to infer the use of a particular dataset.",
      "processing_time": 25.66267991065979,
      "citing_paper_id": "253651036",
      "cited_paper_id": 189998802
    },
    {
      "context_text": "The proposed method can estimate disparity as densely as the ground truth and perform better than E2VID [25] and SHEF [22].",
      "catation_intent": "findings",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only comparisons to methods (E2VID and SHEF). These are not datasets but rather methods or tools.",
      "processing_time": 18.728120803833008,
      "citing_paper_id": "253651036",
      "cited_paper_id": 189998802
    },
    {
      "context_text": "The proposed method can estimate disparity as densely as the ground truth and perform better than E2VID [25] and SHEF [22].",
      "catation_intent": "findings",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only comparisons to methods (E2VID and SHEF). These are not datasets but rather methods or tools.",
      "processing_time": 18.728120803833008,
      "citing_paper_id": "253651036",
      "cited_paper_id": 238582843
    },
    {
      "context_text": "The reconstruction performance of [25] affected by the number of events.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a reference to the performance of a method described in another paper.",
      "processing_time": 15.769065618515015,
      "citing_paper_id": "253651036",
      "cited_paper_id": 189998802
    },
    {
      "context_text": "For several years, various applications for event camera have been studied [1], [2], [3], [4], [5].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general applications of event cameras. No verifiable resources are identified.",
      "processing_time": 16.850812196731567,
      "citing_paper_id": "253651036",
      "cited_paper_id": 198908810
    },
    {
      "context_text": "For several years, various applications for event camera have been studied [1], [2], [3], [4], [5].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general applications of event cameras. No verifiable resources are identified.",
      "processing_time": 16.850812196731567,
      "citing_paper_id": "253651036",
      "cited_paper_id": 250408092
    },
    {
      "context_text": "Studies [17], [18], [19], [20] use two types of vision data, but cannot operate as stereo because frames and events are acquired from a monocular camera.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a general reference to 'vision data' which is too generic.",
      "processing_time": 26.879984140396118,
      "citing_paper_id": "253651036",
      "cited_paper_id": 231951439
    },
    {
      "context_text": "In order to extend the usability of event cameras in general scenarios, several attempts [17], [18], [19], [20], [21] have recently been made to combine the advantages of frame and event, which can also enable the various applications of frame cameras.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general attempts to combine frame and event data. No verifiable resources are identified.",
      "processing_time": 17.098904609680176,
      "citing_paper_id": "253651036",
      "cited_paper_id": 231951439
    },
    {
      "context_text": "In order to extend the usability of event cameras in general scenarios, several attempts [17], [18], [19], [20], [21] have recently been made to combine the advantages of frame and event, which can also enable the various applications of frame cameras.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general attempts to combine frame and event data. No verifiable resources are identified.",
      "processing_time": 17.098904609680176,
      "citing_paper_id": "253651036",
      "cited_paper_id": 233346127
    },
    {
      "context_text": "To accomplish the same goal as [17] in a monocular camera, the authors of [18] estimated depth map in high frame rate from the monocular event camera which also provides frame images.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for estimating depth maps from monocular event cameras.",
      "processing_time": 15.148408651351929,
      "citing_paper_id": "253651036",
      "cited_paper_id": 231951439
    },
    {
      "context_text": "In[22],binaryeventedgeimagesareobtainedfromthe high-passﬁlterof[24]withﬁxedparameters.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.0,
      "reasoning": "The citation does not mention any specific datasets, only methods or tools. The context is too limited to identify a dataset.",
      "processing_time": 15.990518569946289,
      "citing_paper_id": "253651036",
      "cited_paper_id": 238582843
    },
    {
      "context_text": "In [22], [23], the authors conveyed a study to perceive 3D scene by ﬁrstly conﬁguring an event and a frame camera as stereo.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method involving stereo cameras. No clear, verifiable dataset names are present.",
      "processing_time": 15.125283002853394,
      "citing_paper_id": "253651036",
      "cited_paper_id": 238582843
    },
    {
      "context_text": "Thus,[22]showslowaccuracyondisparity estimationasinTableI.",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific dataset names. It only refers to accuracy on disparity estimation, which is a method or finding rather than a dataset.",
      "processing_time": 18.14717698097229,
      "citing_paper_id": "253651036",
      "cited_paper_id": 238582843
    },
    {
      "context_text": "SHEF [22] cannot properly estimate disparity on unclear edge regions where it is difﬁcult to generalize a high-pass ﬁlter to build binary edge maps.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (SHEF) and its limitations in estimating disparity in certain regions.",
      "processing_time": 17.37505292892456,
      "citing_paper_id": "253651036",
      "cited_paper_id": 238582843
    },
    {
      "context_text": "The proposed method outperformed other stereo event frame methods (HSM-Init, E2VID-τ [25] and SHEF [22]) for inliers and disparity RMSE and MAE.",
      "catation_intent": "findings",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and performance metrics.",
      "processing_time": 14.066050291061401,
      "citing_paper_id": "253651036",
      "cited_paper_id": 238582843
    },
    {
      "context_text": "WeimplementedSHEF[22]without computationtimeoptimization.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (SHEF) which is not a dataset.",
      "processing_time": 28.780412673950195,
      "citing_paper_id": "253651036",
      "cited_paper_id": 238582843
    },
    {
      "context_text": "The difference can be defined in different forms, such as absolute gray-scale value, mutual information,(22) and census transform.(23) The penalties depend on the difference to the neighborhood disparities.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods or concepts like 'absolute gray-scale value', 'mutual information', and 'census transform'.",
      "processing_time": 29.24451231956482,
      "citing_paper_id": "65040501",
      "cited_paper_id": 18161107
    },
    {
      "context_text": "The bioinspired vision sensor or called event-based camera mimics retinas to asynchronously generate the response to relative light intensity variations rather than the actual image intensity.(3) Event-based cameras are datadriven sensors that have advantages of low redundancy, high temporal resolution (in the order of microseconds), low latency, and high dynamic range (130 dB compared with 60 dB of standard cameras).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific, verifiable datasets. It describes the functionality and benefits of event-based cameras but does not reference any dataset.",
      "processing_time": 17.572957515716553,
      "citing_paper_id": "65040501",
      "cited_paper_id": 21317717
    },
    {
      "context_text": "In this work, DAVIS is used, which is an extension of the dynamic vision sensor (DVS)(7) with higher resolution (240 180) and an additional frame-based intensity readout (not used in this work).(8) Each event is presented with a quadruplet eðt; x; y; pÞ; t is the time stamp, ðx; yÞ is the image coordinates, and p means polarity (ON/OFF) which indicates the luminance increase (ON) or decrease (OFF).",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "DAVIS"
      ],
      "dataset_descriptions": {
        "DAVIS": "Used to capture event-based data for stereo depth estimation, focusing on high-resolution events with time stamps, image coordinates, and polarity information."
      },
      "confidence_score": 1.0,
      "reasoning": "DAVIS is mentioned as a specific dataset used in the research, with details about its structure and usage in the context of event-based stereo depth estimation.",
      "processing_time": 33.80829381942749,
      "citing_paper_id": "65040501",
      "cited_paper_id": 24007071
    },
    {
      "context_text": "Recently, Eibensteiner et al.15 implemented an event-based stereo matching algorithm in hardware on a field-programmable gate array.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only an implementation of an algorithm. No verifiable resources are identified.",
      "processing_time": 16.781399250030518,
      "citing_paper_id": "65040501",
      "cited_paper_id": 37664826
    },
    {
      "context_text": "Recently, Eibensteiner et al.(15) implemented an event-based stereo matching algorithm in hardware on a field-programmable gate array.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only an implementation of an algorithm. No verifiable resources are identified.",
      "processing_time": 15.707947969436646,
      "citing_paper_id": "65040501",
      "cited_paper_id": 37664826
    },
    {
      "context_text": "Restrictions apply. the method of characteristics [64].",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method. There are no verifiable resources or datasets mentioned.",
      "processing_time": 15.100735902786255,
      "citing_paper_id": "269525186",
      "cited_paper_id": 236574
    },
    {
      "context_text": "The PDEs have advection terms and others that resemble those of the inviscid Burgers’ equation [64] since the ﬂow is transporting itself.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a reference to partial differential equations. No verifiable resources are identified.",
      "processing_time": 14.868794441223145,
      "citing_paper_id": "269525186",
      "cited_paper_id": 236574
    },
    {
      "context_text": "One could use other clustering algorithms, such as DBSCAN [83], to treat such interpolation effects as outliers.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (DBSCAN).",
      "processing_time": 14.65108346939087,
      "citing_paper_id": "269525186",
      "cited_paper_id": 355163
    },
    {
      "context_text": "…evaluations, we provide exhaustive comparisons across the existing methods to date: a model-based method where the pose information is given (EMVS) [23], a supervised-learning method [61] trained on real data (outdoor_day2, denoted “SL (R)”) or in simulation (“SL (S)”), and two…",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "outdoor_day2"
      ],
      "dataset_descriptions": {
        "outdoor_day2": "Used to train a supervised-learning method for event-based stereo depth estimation, focusing on real-world outdoor scenarios."
      },
      "confidence_score": 0.8,
      "reasoning": "The citation mentions 'outdoor_day2' as a dataset used for training a supervised-learning method. However, the primary focus is on comparing methods rather than the dataset itself.",
      "processing_time": 23.151720762252808,
      "citing_paper_id": "269525186",
      "cited_paper_id": 1082643
    },
    {
      "context_text": "…[8] is a powerful framework that allows us to tackle multiple motion estimation problems (rotational motion [9], [10], [11], [12], homographic motion [7], [13], [14], feature ﬂow estimation [15], [16], [17], [18], motion segmentation [19], [20], [21], [22], and also reconstruction [7], [23], [24]).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various motion estimation problems and methods. The cited paper titles do not provide additional context about datasets.",
      "processing_time": 17.823312997817993,
      "citing_paper_id": "269525186",
      "cited_paper_id": 1082643
    },
    {
      "context_text": "…[8] is a powerful framework that allows us to tackle multiple motion estimation problems (rotational motion [9], [10], [11], [12], homographic motion [7], [13], [14], feature ﬂow estimation [15], [16], [17], [18], motion segmentation [19], [20], [21], [22], and also reconstruction [7], [23], [24]).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various motion estimation problems and methods. The cited paper titles do not provide additional context about datasets.",
      "processing_time": 17.823312997817993,
      "citing_paper_id": "269525186",
      "cited_paper_id": 3845250
    },
    {
      "context_text": "…[8] is a powerful framework that allows us to tackle multiple motion estimation problems (rotational motion [9], [10], [11], [12], homographic motion [7], [13], [14], feature ﬂow estimation [15], [16], [17], [18], motion segmentation [19], [20], [21], [22], and also reconstruction [7], [23], [24]).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various motion estimation problems and methods. The cited paper titles do not provide additional context about datasets.",
      "processing_time": 17.823312997817993,
      "citing_paper_id": "269525186",
      "cited_paper_id": 13360027
    },
    {
      "context_text": "…[8] is a powerful framework that allows us to tackle multiple motion estimation problems (rotational motion [9], [10], [11], [12], homographic motion [7], [13], [14], feature ﬂow estimation [15], [16], [17], [18], motion segmentation [19], [20], [21], [22], and also reconstruction [7], [23], [24]).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various motion estimation problems and methods. The cited paper titles do not provide additional context about datasets.",
      "processing_time": 17.823312997817993,
      "citing_paper_id": "269525186",
      "cited_paper_id": 229211559
    },
    {
      "context_text": "…[8] is a powerful framework that allows us to tackle multiple motion estimation problems (rotational motion [9], [10], [11], [12], homographic motion [7], [13], [14], feature ﬂow estimation [15], [16], [17], [18], motion segmentation [19], [20], [21], [22], and also reconstruction [7], [23], [24]).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various motion estimation problems and methods. The cited paper titles do not provide additional context about datasets.",
      "processing_time": 17.823312997817993,
      "citing_paper_id": "269525186",
      "cited_paper_id": 254591426
    },
    {
      "context_text": "…[8] is a powerful framework that allows us to tackle multiple motion estimation problems (rotational motion [9], [10], [11], [12], homographic motion [7], [13], [14], feature ﬂow estimation [15], [16], [17], [18], motion segmentation [19], [20], [21], [22], and also reconstruction [7], [23], [24]).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various motion estimation problems and methods. The cited paper titles do not provide additional context about datasets.",
      "processing_time": 17.823312997817993,
      "citing_paper_id": "269525186",
      "cited_paper_id": null
    },
    {
      "context_text": "…[8] is a powerful framework that allows us to tackle multiple motion estimation problems (rotational motion [9], [10], [11], [12], homographic motion [7], [13], [14], feature ﬂow estimation [15], [16], [17], [18], motion segmentation [19], [20], [21], [22], and also reconstruction [7], [23], [24]).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various motion estimation problems and methods. The cited paper titles do not provide additional context about datasets.",
      "processing_time": 17.823312997817993,
      "citing_paper_id": "269525186",
      "cited_paper_id": null
    },
    {
      "context_text": "The problem is difﬁcult, and often one settles for estimating depth alone, with or without knowledge of the camera motion [23], [61], [62].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only the difficulty of the problem and references to other works.",
      "processing_time": 15.68362808227539,
      "citing_paper_id": "269525186",
      "cited_paper_id": 1082643
    },
    {
      "context_text": "In terms of architectures, the three most common ones are U-Net [34], [49], FireNet [28], and RAFT [44], [50].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span mentions architectures (U-Net, FireNet, RAFT) but does not reference any specific datasets. The cited papers are related to optical flow estimation using event-based cameras, which is adjacent to stereo depth estimation but do not explicitly mention datasets.",
      "processing_time": 21.465513467788696,
      "citing_paper_id": "269525186",
      "cited_paper_id": 3396150
    },
    {
      "context_text": "In terms of architectures, the three most common ones are U-Net [34], [49], FireNet [28], and RAFT [44], [50].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span mentions architectures (U-Net, FireNet, RAFT) but does not reference any specific datasets. The cited papers are related to optical flow estimation using event-based cameras, which is adjacent to stereo depth estimation but do not explicitly mention datasets.",
      "processing_time": 21.465513467788696,
      "citing_paper_id": "269525186",
      "cited_paper_id": 239049376
    },
    {
      "context_text": "In terms of architectures, the three most common ones are U-Net [34], [49], FireNet [28], and RAFT [44], [50].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span mentions architectures (U-Net, FireNet, RAFT) but does not reference any specific datasets. The cited papers are related to optical flow estimation using event-based cameras, which is adjacent to stereo depth estimation but do not explicitly mention datasets.",
      "processing_time": 21.465513467788696,
      "citing_paper_id": "269525186",
      "cited_paper_id": 257637142
    },
    {
      "context_text": "In terms of architectures, the three most common ones are U-Net [34], [49], FireNet [28], and RAFT [44], [50].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span mentions architectures (U-Net, FireNet, RAFT) but does not reference any specific datasets. The cited papers are related to optical flow estimation using event-based cameras, which is adjacent to stereo depth estimation but do not explicitly mention datasets.",
      "processing_time": 21.465513467788696,
      "citing_paper_id": "269525186",
      "cited_paper_id": null
    },
    {
      "context_text": "The dataset was extended in [34] to provide ground truth (GT) optical ﬂow, computed as the motion ﬁeld [6] given the camera velocity and the depth of the scene.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.3,
      "reasoning": "The context mentions extending a dataset to provide ground truth optical flow, which is relevant to event-based stereo depth estimation. However, the dataset name is not explicitly mentioned.",
      "processing_time": 17.53015446662903,
      "citing_paper_id": "269525186",
      "cited_paper_id": 3396150
    },
    {
      "context_text": "Some learning-based works [27], [28], [34] also have a multi-scale component, inherited from the use of a U-Net architecture.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only references to other works and a U-Net architecture. No verifiable resources are identified.",
      "processing_time": 28.718234539031982,
      "citing_paper_id": "269525186",
      "cited_paper_id": 3396150
    },
    {
      "context_text": "Zhu et al. [27] extended EV-FlowNet [34] to the unsupervised setting using a motion-compensation loss inspired by the average timestamp images in [19].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and models. The context focuses on extending a method (EV-FlowNet) and using a loss function inspired by another method.",
      "processing_time": 19.56695866584778,
      "citing_paper_id": "269525186",
      "cited_paper_id": 3396150
    },
    {
      "context_text": "Zhu et al. [27] extended EV-FlowNet [34] to the unsupervised setting using a motion-compensation loss inspired by the average timestamp images in [19].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and models. The context focuses on extending a method (EV-FlowNet) and using a loss function inspired by another method.",
      "processing_time": 19.56695866584778,
      "citing_paper_id": "269525186",
      "cited_paper_id": 3845250
    },
    {
      "context_text": "Zhu et al. [27] extended EV-FlowNet [34] to the unsupervised setting using a motion-compensation loss inspired by the average timestamp images in [19].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and models. The context focuses on extending a method (EV-FlowNet) and using a loss function inspired by another method.",
      "processing_time": 19.56695866584778,
      "citing_paper_id": "269525186",
      "cited_paper_id": 56475917
    },
    {
      "context_text": "[34] change and its polarity p k ∈ { +1 , − 1 } .",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (EV-FlowNet) and a concept (polarity). No verifiable datasets are referenced.",
      "processing_time": 18.87155246734619,
      "citing_paper_id": "269525186",
      "cited_paper_id": 3396150
    },
    {
      "context_text": "EV-FlowNet [34] pioneered these approaches.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions EV-FlowNet but does not refer to it as a dataset. It is described as a method or model for self-supervised optical flow estimation.",
      "processing_time": 18.867850065231323,
      "citing_paper_id": "269525186",
      "cited_paper_id": 3396150
    },
    {
      "context_text": "Semi-supervised methods use the grayscale images from a colocated camera (e.g., DAVIS [56]) as a supervisory signal: images are warped using the ﬂow predicted by the ANN and their photometric consistency is used as loss function [34], [45], [46].",
      "catation_intent": "reusable resource",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions 'DAVIS' as a sensor providing grayscale images used for supervision in semi-supervised methods. However, it does not specify a dataset name, only a sensor type.",
      "processing_time": 30.59854817390442,
      "citing_paper_id": "269525186",
      "cited_paper_id": 3396150
    },
    {
      "context_text": "Semi-supervised methods use the grayscale images from a colocated camera (e.g., DAVIS [56]) as a supervisory signal: images are warped using the ﬂow predicted by the ANN and their photometric consistency is used as loss function [34], [45], [46].",
      "catation_intent": "reusable resource",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions 'DAVIS' as a sensor providing grayscale images used for supervision in semi-supervised methods. However, it does not specify a dataset name, only a sensor type.",
      "processing_time": 30.59854817390442,
      "citing_paper_id": "269525186",
      "cited_paper_id": 19091270
    },
    {
      "context_text": "Semi-supervised methods use the grayscale images from a colocated camera (e.g., DAVIS [56]) as a supervisory signal: images are warped using the ﬂow predicted by the ANN and their photometric consistency is used as loss function [34], [45], [46].",
      "catation_intent": "reusable resource",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions 'DAVIS' as a sensor providing grayscale images used for supervision in semi-supervised methods. However, it does not specify a dataset name, only a sensor type.",
      "processing_time": 30.59854817390442,
      "citing_paper_id": "269525186",
      "cited_paper_id": null
    },
    {
      "context_text": "Semi-supervised methods use the grayscale images from a colocated camera (e.g., DAVIS [56]) as a supervisory signal: images are warped using the ﬂow predicted by the ANN and their photometric consistency is used as loss function [34], [45], [46].",
      "catation_intent": "reusable resource",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions 'DAVIS' as a sensor providing grayscale images used for supervision in semi-supervised methods. However, it does not specify a dataset name, only a sensor type.",
      "processing_time": 30.59854817390442,
      "citing_paper_id": "269525186",
      "cited_paper_id": null
    },
    {
      "context_text": "Datasets: First, we evaluate our method on sequences from the MVSEC dataset [4], [34], which is the de facto standard dataset used by prior works to benchmark optical ﬂow.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "MVSEC"
      ],
      "dataset_descriptions": {
        "MVSEC": "Used to evaluate the method on optical flow estimation for event-based cameras, providing sequences for benchmarking against prior works."
      },
      "confidence_score": 1.0,
      "reasoning": "The MVSEC dataset is explicitly mentioned and used for evaluating the method on optical flow estimation for event-based cameras.",
      "processing_time": 22.11620783805847,
      "citing_paper_id": "269525186",
      "cited_paper_id": 3396150
    },
    {
      "context_text": "Datasets: First, we evaluate our method on sequences from the MVSEC dataset [4], [34], which is the de facto standard dataset used by prior works to benchmark optical ﬂow.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "MVSEC"
      ],
      "dataset_descriptions": {
        "MVSEC": "Used to evaluate the method on optical flow estimation for event-based cameras, providing sequences for benchmarking against prior works."
      },
      "confidence_score": 1.0,
      "reasoning": "The MVSEC dataset is explicitly mentioned and used for evaluating the method on optical flow estimation for event-based cameras.",
      "processing_time": 22.11620783805847,
      "citing_paper_id": "269525186",
      "cited_paper_id": null
    },
    {
      "context_text": "Current state-of-the-art approaches are ANNs [27], [30], [34], [44], [45], [46], largely inspired by frame-based optical ﬂow architectures [47], [48].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only methods and models. The cited papers' titles suggest a focus on event-based cameras and optical flow estimation, but do not explicitly name datasets.",
      "processing_time": 19.378214597702026,
      "citing_paper_id": "269525186",
      "cited_paper_id": 3396150
    },
    {
      "context_text": "Current state-of-the-art approaches are ANNs [27], [30], [34], [44], [45], [46], largely inspired by frame-based optical ﬂow architectures [47], [48].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only methods and models. The cited papers' titles suggest a focus on event-based cameras and optical flow estimation, but do not explicitly name datasets.",
      "processing_time": 19.378214597702026,
      "citing_paper_id": "269525186",
      "cited_paper_id": 56475917
    },
    {
      "context_text": "Current state-of-the-art approaches are ANNs [27], [30], [34], [44], [45], [46], largely inspired by frame-based optical ﬂow architectures [47], [48].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only methods and models. The cited papers' titles suggest a focus on event-based cameras and optical flow estimation, but do not explicitly name datasets.",
      "processing_time": 19.378214597702026,
      "citing_paper_id": "269525186",
      "cited_paper_id": 239049376
    },
    {
      "context_text": "Current state-of-the-art approaches are ANNs [27], [30], [34], [44], [45], [46], largely inspired by frame-based optical ﬂow architectures [47], [48].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only methods and models. The cited papers' titles suggest a focus on event-based cameras and optical flow estimation, but do not explicitly name datasets.",
      "processing_time": 19.378214597702026,
      "citing_paper_id": "269525186",
      "cited_paper_id": null
    },
    {
      "context_text": "Current state-of-the-art approaches are ANNs [27], [30], [34], [44], [45], [46], largely inspired by frame-based optical ﬂow architectures [47], [48].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only methods and models. The cited papers' titles suggest a focus on event-based cameras and optical flow estimation, but do not explicitly name datasets.",
      "processing_time": 19.378214597702026,
      "citing_paper_id": "269525186",
      "cited_paper_id": null
    },
    {
      "context_text": "Current state-of-the-art approaches are ANNs [27], [30], [34], [44], [45], [46], largely inspired by frame-based optical ﬂow architectures [47], [48].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only methods and models. The cited papers' titles suggest a focus on event-based cameras and optical flow estimation, but do not explicitly name datasets.",
      "processing_time": 19.378214597702026,
      "citing_paper_id": "269525186",
      "cited_paper_id": null
    },
    {
      "context_text": "The results of our experimental evaluation are surprising: the above design choices are key to our simple, model-based tile-based method achieving the best accuracy among all state-of-the-art methods, including supervised-learning ones, on the de facto benchmark of MVSEC indoor sequences [34].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "MVSEC indoor sequences"
      ],
      "dataset_descriptions": {
        "MVSEC indoor sequences": "Used to evaluate the accuracy of the proposed tile-based method against state-of-the-art methods, including supervised-learning approaches, in event-based stereo depth estimation."
      },
      "confidence_score": 1.0,
      "reasoning": "The citation mentions 'MVSEC indoor sequences' as a benchmark, which is a specific dataset used for evaluating event-based stereo depth estimation methods.",
      "processing_time": 33.96380043029785,
      "citing_paper_id": "269525186",
      "cited_paper_id": 3396150
    },
    {
      "context_text": "The ECD dataset [63] is a lower resolution, standard dataset to assess camera ego-motion [9], [16], [25], [69], [70], [71], [72].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "ECD dataset"
      ],
      "dataset_descriptions": {
        "ECD dataset": "Used to assess camera ego-motion in lower resolution settings, focusing on robustness in visual SLAM and related tasks."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions the ECD dataset, which is used to assess camera ego-motion. This is a specific dataset with a clear identifier and is relevant to the topic of event-based stereo depth estimation.",
      "processing_time": 23.65659809112549,
      "citing_paper_id": "269525186",
      "cited_paper_id": 3738244
    },
    {
      "context_text": "The ECD dataset [63] is a lower resolution, standard dataset to assess camera ego-motion [9], [16], [25], [69], [70], [71], [72].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "ECD dataset"
      ],
      "dataset_descriptions": {
        "ECD dataset": "Used to assess camera ego-motion in lower resolution settings, focusing on robustness in visual SLAM and related tasks."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions the ECD dataset, which is used to assess camera ego-motion. This is a specific dataset with a clear identifier and is relevant to the topic of event-based stereo depth estimation.",
      "processing_time": 23.65659809112549,
      "citing_paper_id": "269525186",
      "cited_paper_id": 9729856
    },
    {
      "context_text": "The ECD dataset [63] is a lower resolution, standard dataset to assess camera ego-motion [9], [16], [25], [69], [70], [71], [72].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "ECD dataset"
      ],
      "dataset_descriptions": {
        "ECD dataset": "Used to assess camera ego-motion in lower resolution settings, focusing on robustness in visual SLAM and related tasks."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions the ECD dataset, which is used to assess camera ego-motion. This is a specific dataset with a clear identifier and is relevant to the topic of event-based stereo depth estimation.",
      "processing_time": 23.65659809112549,
      "citing_paper_id": "269525186",
      "cited_paper_id": 9865213
    },
    {
      "context_text": "The ECD dataset [63] is a lower resolution, standard dataset to assess camera ego-motion [9], [16], [25], [69], [70], [71], [72].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "ECD dataset"
      ],
      "dataset_descriptions": {
        "ECD dataset": "Used to assess camera ego-motion in lower resolution settings, focusing on robustness in visual SLAM and related tasks."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions the ECD dataset, which is used to assess camera ego-motion. This is a specific dataset with a clear identifier and is relevant to the topic of event-based stereo depth estimation.",
      "processing_time": 23.65659809112549,
      "citing_paper_id": "269525186",
      "cited_paper_id": 250408092
    },
    {
      "context_text": "The ECD dataset [63] is a lower resolution, standard dataset to assess camera ego-motion [9], [16], [25], [69], [70], [71], [72].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "ECD dataset"
      ],
      "dataset_descriptions": {
        "ECD dataset": "Used to assess camera ego-motion in lower resolution settings, focusing on robustness in visual SLAM and related tasks."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions the ECD dataset, which is used to assess camera ego-motion. This is a specific dataset with a clear identifier and is relevant to the topic of event-based stereo depth estimation.",
      "processing_time": 23.65659809112549,
      "citing_paper_id": "269525186",
      "cited_paper_id": null
    },
    {
      "context_text": "Depth and Ego-motion estimation for the slider_depth sequence (real data) from the ECD dataset [63].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "ECD dataset"
      ],
      "dataset_descriptions": {
        "ECD dataset": "Used for depth and ego-motion estimation in real-world scenarios, specifically for the slider_depth sequence. The dataset supports research into event-based methods for visual odometry and SLAM."
      },
      "confidence_score": 1.0,
      "reasoning": "The citation mentions the 'ECD dataset' which is a specific dataset used for depth and ego-motion estimation. The dataset is relevant to the topic of event-based stereo depth estimation.",
      "processing_time": 24.280404329299927,
      "citing_paper_id": "269525186",
      "cited_paper_id": 9865213
    },
    {
      "context_text": "The advantages of the time-aware warp (8) over (4) to produce better IWEs (higher FWL) are most noticeable on sequences like slider_depth [63] and DSEC (see Fig.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "slider_depth",
        "DSEC"
      ],
      "dataset_descriptions": {
        "slider_depth": "Used to evaluate the performance of time-aware warp methods, focusing on improving intensity-weighted events (IWEs) and frame-wise loss (FWL) in event-based stereo depth estimation.",
        "DSEC": "Used to evaluate the performance of time-aware warp methods, focusing on improving intensity-weighted events (IWEs) and frame-wise loss (FWL) in event-based stereo depth estimation."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions 'slider_depth' and 'DSEC', which are specific datasets used in event-based vision research. These datasets are used to evaluate the performance of time-aware warp methods.",
      "processing_time": 32.13479566574097,
      "citing_paper_id": "269525186",
      "cited_paper_id": 9865213
    },
    {
      "context_text": "4 illustrates this with an occlusion example taken from the slider_depth sequence [63].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "slider_depth sequence"
      ],
      "dataset_descriptions": {
        "slider_depth sequence": "Used to demonstrate occlusion handling in event-based stereo depth estimation, focusing on the accuracy of depth maps under partial occlusions."
      },
      "confidence_score": 0.6,
      "reasoning": "The citation mentions 'slider_depth sequence' which appears to be part of a larger dataset or collection of sequences. However, without more context, it is unclear if this is a specific, verifiable dataset or just a sequence within a broader dataset.",
      "processing_time": 25.206601858139038,
      "citing_paper_id": "269525186",
      "cited_paper_id": 9865213
    },
    {
      "context_text": "To assess the effect of the proposed time-aware warp (8), we conducted experiments on MVSEC, DSEC and ECD [63] datasets.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "MVSEC",
        "DSEC",
        "ECD"
      ],
      "dataset_descriptions": {
        "MVSEC": "Used to evaluate the time-aware warp method, focusing on event-based data for pose estimation, visual odometry, and SLAM.",
        "DSEC": "Used to evaluate the time-aware warp method, focusing on event-based data for pose estimation, visual odometry, and SLAM.",
        "ECD": "Used to evaluate the time-aware warp method, focusing on event-based data for pose estimation, visual odometry, and SLAM."
      },
      "confidence_score": 1.0,
      "reasoning": "The citation mentions three specific datasets: MVSEC, DSEC, and ECD. These are used to conduct experiments assessing the effect of a time-aware warp method.",
      "processing_time": 32.974507331848145,
      "citing_paper_id": "269525186",
      "cited_paper_id": 9865213
    },
    {
      "context_text": "2) Results on ECD: Depth and ego-motion estimation results on the slider_depth sequence from the ECD dataset [63] are shown on Fig.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "ECD dataset"
      ],
      "dataset_descriptions": {
        "ECD dataset": "Used for depth and ego-motion estimation, specifically evaluating performance on the slider_depth sequence. The dataset supports research in event-based stereo depth estimation."
      },
      "confidence_score": 1.0,
      "reasoning": "The citation mentions the 'ECD dataset' which is a specific dataset used for depth and ego-motion estimation. The cited paper title confirms it is a dataset.",
      "processing_time": 23.080265998840332,
      "citing_paper_id": "269525186",
      "cited_paper_id": 9865213
    },
    {
      "context_text": "At occlusions (dartboard in slider_depth [63] and garage door in DSEC [5]), upwind and Burgers’ produce sharper IWEs.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "DSEC"
      ],
      "dataset_descriptions": {
        "DSEC": "Used to evaluate stereo depth estimation methods, specifically focusing on the garage door scene to assess the sharpness of intensity-weighted events."
      },
      "confidence_score": 0.9,
      "reasoning": "The citation mentions 'DSEC' which is likely a dataset given the context of event-based data. The other reference 'slider_depth' does not appear to be a dataset.",
      "processing_time": 23.515924215316772,
      "citing_paper_id": "269525186",
      "cited_paper_id": 9865213
    },
    {
      "context_text": "17 shows the results on a synthetic sequence from [63].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.3,
      "reasoning": "The citation mentions a synthetic sequence, which suggests the use of a dataset or simulator. However, the specific name of the dataset is not provided in the citation context.",
      "processing_time": 18.820093631744385,
      "citing_paper_id": "269525186",
      "cited_paper_id": 9865213
    },
    {
      "context_text": "Depth and Ego-motion estimation for the simulation_3planes sequence from the ECD dataset [63].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "ECD dataset"
      ],
      "dataset_descriptions": {
        "ECD dataset": "Used for depth and ego-motion estimation in the simulation_3planes sequence, focusing on event-based data for pose estimation, visual odometry, and SLAM."
      },
      "confidence_score": 1.0,
      "reasoning": "The citation mentions the 'ECD dataset', which is a specific dataset used for event-based data. The cited paper title confirms it is a dataset for pose estimation, visual odometry, and SLAM.",
      "processing_time": 24.7691068649292,
      "citing_paper_id": "269525186",
      "cited_paper_id": 9865213
    },
    {
      "context_text": "For depth accuracy evaluation, we use standard metrics following previous work on monocular depth estimation [57], [74].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only standard metrics for depth accuracy evaluation. The cited papers do not introduce new datasets but focus on methods.",
      "processing_time": 18.560256481170654,
      "citing_paper_id": "269525186",
      "cited_paper_id": 11977588
    },
    {
      "context_text": "For depth accuracy evaluation, we use standard metrics following previous work on monocular depth estimation [57], [74].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only standard metrics for depth accuracy evaluation. The cited papers do not introduce new datasets but focus on methods.",
      "processing_time": 18.560256481170654,
      "citing_paper_id": "269525186",
      "cited_paper_id": 226298400
    },
    {
      "context_text": "Prior work has proposed adaptations of frame-based approaches (block matching [36], Lucas-Kanade [37]), ﬁlter-banks [38], [39], spatio-temporal plane-ﬁtting [40], [41], time surface matching [42], variational optimization on voxelized events [43], and feature-based contrast maximization [7], [15].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various methods and approaches. The cited paper titles do not provide additional context to identify datasets.",
      "processing_time": 18.043326139450073,
      "citing_paper_id": "269525186",
      "cited_paper_id": 13360027
    },
    {
      "context_text": "Prior work has proposed adaptations of frame-based approaches (block matching [36], Lucas-Kanade [37]), ﬁlter-banks [38], [39], spatio-temporal plane-ﬁtting [40], [41], time surface matching [42], variational optimization on voxelized events [43], and feature-based contrast maximization [7], [15].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various methods and approaches. The cited paper titles do not provide additional context to identify datasets.",
      "processing_time": 18.043326139450073,
      "citing_paper_id": "269525186",
      "cited_paper_id": 16638035
    },
    {
      "context_text": "Prior work has proposed adaptations of frame-based approaches (block matching [36], Lucas-Kanade [37]), ﬁlter-banks [38], [39], spatio-temporal plane-ﬁtting [40], [41], time surface matching [42], variational optimization on voxelized events [43], and feature-based contrast maximization [7], [15].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various methods and approaches. The cited paper titles do not provide additional context to identify datasets.",
      "processing_time": 18.043326139450073,
      "citing_paper_id": "269525186",
      "cited_paper_id": 52283776
    },
    {
      "context_text": "Prior work has proposed adaptations of frame-based approaches (block matching [36], Lucas-Kanade [37]), ﬁlter-banks [38], [39], spatio-temporal plane-ﬁtting [40], [41], time surface matching [42], variational optimization on voxelized events [43], and feature-based contrast maximization [7], [15].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various methods and approaches. The cited paper titles do not provide additional context to identify datasets.",
      "processing_time": 18.043326139450073,
      "citing_paper_id": "269525186",
      "cited_paper_id": null
    },
    {
      "context_text": "Prior work has proposed adaptations of frame-based approaches (block matching [36], Lucas-Kanade [37]), ﬁlter-banks [38], [39], spatio-temporal plane-ﬁtting [40], [41], time surface matching [42], variational optimization on voxelized events [43], and feature-based contrast maximization [7], [15].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various methods and approaches. The cited paper titles do not provide additional context to identify datasets.",
      "processing_time": 18.043326139450073,
      "citing_paper_id": "269525186",
      "cited_paper_id": null
    },
    {
      "context_text": "Prior work has proposed adaptations of frame-based approaches (block matching [36], Lucas-Kanade [37]), ﬁlter-banks [38], [39], spatio-temporal plane-ﬁtting [40], [41], time surface matching [42], variational optimization on voxelized events [43], and feature-based contrast maximization [7], [15].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various methods and approaches. The cited paper titles do not provide additional context to identify datasets.",
      "processing_time": 18.043326139450073,
      "citing_paper_id": "269525186",
      "cited_paper_id": null
    },
    {
      "context_text": "To mitigate isolated patches with very large depth values we apply median ﬁlters [35] and a Charbonnier loss [67] for regularization.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods (median filters and Charbonnier loss).",
      "processing_time": 15.599066734313965,
      "citing_paper_id": "269525186",
      "cited_paper_id": 16219282
    },
    {
      "context_text": "Because of the above, we believe that the proposed design choices deserve to be called “secrets” [35].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, models, or methods. It refers to design choices and principles in optical flow estimation, which is related but does not provide a verifiable resource.",
      "processing_time": 19.11538529396057,
      "citing_paper_id": "269525186",
      "cited_paper_id": 16219282
    },
    {
      "context_text": "The event camera has 346 × 260 pixel resolution [56].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific dataset, only a technical specification of an event camera.",
      "processing_time": 15.267067193984985,
      "citing_paper_id": "269525186",
      "cited_paper_id": 19091270
    },
    {
      "context_text": "Each sequence provides events, frames, calibration information, and IMU data from a DAVIS240 C camera ( 240 × 180 pixels [73]), as well as ground truth camera poses from a motion capture system (at 200 Hz).",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context describes a dataset that includes event sequences, frames, calibration information, and IMU data from a DAVIS240 C camera, along with ground truth camera poses. However, no specific dataset name is provided.",
      "processing_time": 20.661051511764526,
      "citing_paper_id": "269525186",
      "cited_paper_id": 24007071
    },
    {
      "context_text": "…the scene complexity, the large pixel displacement and the high dynamic range. been tackled by changing the objective function, from contrast to the energy of an average timestamp image [27], [28], but this loss is not straightforward to interpret [8], [29], and is not without its problems [30].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The provided context does not mention any specific datasets, only discusses methodological approaches and challenges in event-based stereo depth estimation.",
      "processing_time": 15.581423044204712,
      "citing_paper_id": "269525186",
      "cited_paper_id": 56475917
    },
    {
      "context_text": "…the scene complexity, the large pixel displacement and the high dynamic range. been tackled by changing the objective function, from contrast to the energy of an average timestamp image [27], [28], but this loss is not straightforward to interpret [8], [29], and is not without its problems [30].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The provided context does not mention any specific datasets, only discusses methodological approaches and challenges in event-based stereo depth estimation.",
      "processing_time": 15.581423044204712,
      "citing_paper_id": "269525186",
      "cited_paper_id": null
    },
    {
      "context_text": "Zhu et al. [27] report that the contrast objective (variance) overﬁts to the events.",
      "catation_intent": "findings",
      "resource_type": "finding",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method or finding related to event-based learning.",
      "processing_time": 14.757048606872559,
      "citing_paper_id": "269525186",
      "cited_paper_id": 56475917
    },
    {
      "context_text": "Remark: Warping to two reference times (min and max) was proposed in [27], but with important differences: (i) it was done for the average timestamp loss, hence it did not consider the effect on contrast or focus functions [8], and (ii) it had a completely different motivation: to lessen a…",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and motivations. The context is focused on comparing methodologies rather than using a particular dataset.",
      "processing_time": 15.213137865066528,
      "citing_paper_id": "269525186",
      "cited_paper_id": 56475917
    },
    {
      "context_text": "Closest to our work are [27], [57] because they estimate a depth-parameterized motion ﬁeld that best ﬁts the event data.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and approaches. The context focuses on the estimation of a depth-parameterized motion field using event data, which is a methodological discussion rather than a reference to a dataset.",
      "processing_time": 20.087000608444214,
      "citing_paper_id": "269525186",
      "cited_paper_id": 56475917
    },
    {
      "context_text": "Closest to our work are [27], [57] because they estimate a depth-parameterized motion ﬁeld that best ﬁts the event data.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and approaches. The context focuses on the estimation of a depth-parameterized motion field using event data, which is a methodological discussion rather than a reference to a dataset.",
      "processing_time": 20.087000608444214,
      "citing_paper_id": "269525186",
      "cited_paper_id": 226298400
    },
    {
      "context_text": "Preliminary work on applying CM to estimate optical ﬂow has reported event collapse [25], [26], producing ﬂows at undesired optima that warp events to few pixels or lines [27].",
      "catation_intent": "findings",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only methods and findings related to event collapse in contrast maximization frameworks.",
      "processing_time": 15.56582760810852,
      "citing_paper_id": "269525186",
      "cited_paper_id": 56475917
    },
    {
      "context_text": "Preliminary work on applying CM to estimate optical ﬂow has reported event collapse [25], [26], producing ﬂows at undesired optima that warp events to few pixels or lines [27].",
      "catation_intent": "findings",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only methods and findings related to event collapse in contrast maximization frameworks.",
      "processing_time": 15.56582760810852,
      "citing_paper_id": "269525186",
      "cited_paper_id": 250408092
    },
    {
      "context_text": "Their loss function consists of an event alignment error using the ﬂow predicted by the ANN [27], [28], [30], [57], [58], [59].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only methods and models. The cited papers' titles suggest a focus on unsupervised learning with event-based sensors, but do not specify datasets.",
      "processing_time": 19.471266269683838,
      "citing_paper_id": "269525186",
      "cited_paper_id": 56475917
    },
    {
      "context_text": "Their loss function consists of an event alignment error using the ﬂow predicted by the ANN [27], [28], [30], [57], [58], [59].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only methods and models. The cited papers' titles suggest a focus on unsupervised learning with event-based sensors, but do not specify datasets.",
      "processing_time": 19.471266269683838,
      "citing_paper_id": "269525186",
      "cited_paper_id": 226298400
    },
    {
      "context_text": "The loss functions are based on the energy of an average timestamp image [27] or on the photometric consistency of edge-maps warped by the predicted ﬂow [57].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only loss functions and methods. The cited papers' titles suggest a focus on unsupervised learning with event-based sensors, but do not explicitly name datasets.",
      "processing_time": 19.801881074905396,
      "citing_paper_id": "269525186",
      "cited_paper_id": 56475917
    },
    {
      "context_text": "The loss functions are based on the energy of an average timestamp image [27] or on the photometric consistency of edge-maps warped by the predicted ﬂow [57].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only loss functions and methods. The cited papers' titles suggest a focus on unsupervised learning with event-based sensors, but do not explicitly name datasets.",
      "processing_time": 19.801881074905396,
      "citing_paper_id": "269525186",
      "cited_paper_id": 226298400
    },
    {
      "context_text": "For dense optical ﬂow motion, the warp used is [27], [28] x where θ = { v ( x ) } x ∈ Ω is a ﬂow ﬁeld on the image plane Ω at a set time, e.g., t ref .",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and concepts related to optical flow and image warping.",
      "processing_time": 15.51758074760437,
      "citing_paper_id": "269525186",
      "cited_paper_id": 56475917
    },
    {
      "context_text": "Noticethat[27]wastrainedontheout-door_day2 sequence, which is a similar driving sequence to the testone,whiletheothermethodsweretrainedondronedata [81].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.3,
      "reasoning": "The context mentions 'out-door_day2 sequence' and 'drone data', both of which could be datasets. However, 'out-door_day2 sequence' lacks a clear identifier, and 'drone data' is too generic. The cited paper title suggests 'UZH-FPV Drone Racing Dataset' is a plausible dataset, but it is not mentioned in the context.",
      "processing_time": 23.205305337905884,
      "citing_paper_id": "269525186",
      "cited_paper_id": 119096559
    },
    {
      "context_text": "As discussed in (Section IV-D1 and Table IV), ECN [57] might have overﬁt to this outdoor sequence that reports a very small error (0.7m/s).",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.2,
      "reasoning": "The citation does not mention any specific dataset names, only a general reference to an 'outdoor sequence'. No clear, verifiable dataset name is provided.",
      "processing_time": 16.83689045906067,
      "citing_paper_id": "269525186",
      "cited_paper_id": 226298400
    },
    {
      "context_text": "…exhaustive comparisons across the existing methods to date: a model-based method where the pose information is given (EMVS) [23], a supervised-learning method [61] trained on real data (outdoor_day2, denoted “SL (R)”) or in simulation (“SL (S)”), and two unsupervised-learning methods [27], [57].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "outdoor_day2"
      ],
      "dataset_descriptions": {
        "outdoor_day2": "Used to train a supervised-learning method for event-based stereo depth estimation, focusing on real-world outdoor scenarios."
      },
      "confidence_score": 0.9,
      "reasoning": "The citation mentions 'outdoor_day2' as a dataset used for training a supervised-learning method. No other specific datasets are mentioned.",
      "processing_time": 22.408650636672974,
      "citing_paper_id": "269525186",
      "cited_paper_id": 226298400
    },
    {
      "context_text": "The proposed methods achieve overall better accuracy on the indoor sequences and competitive results on the outdoor sequence compared with ECN [57], the closest work to ours.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a comparison with another method (ECN). There are no clear identifiers for datasets in the provided context.",
      "processing_time": 18.196926593780518,
      "citing_paper_id": "269525186",
      "cited_paper_id": 226298400
    },
    {
      "context_text": "Following the convention [57], we report the metrics for indoor as the average of the three indoor sequences.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a convention for reporting metrics. No verifiable resources are identified.",
      "processing_time": 14.893763542175293,
      "citing_paper_id": "269525186",
      "cited_paper_id": 226298400
    },
    {
      "context_text": "Firstrow: corridor sequencefrom[21].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.2,
      "reasoning": "The citation does not provide enough information to identify a specific dataset. The reference to 'corridor sequence' is too generic and lacks a clear identifier.",
      "processing_time": 16.83719778060913,
      "citing_paper_id": "269525186",
      "cited_paper_id": 229211559
    },
    {
      "context_text": "The sequences in EMSGC [21] are recorded with a hand-held DAVIS346 camera ( 346 × 260 pixels).",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "EMSGC"
      ],
      "dataset_descriptions": {
        "EMSGC": "Used to record sequences with a DAVIS346 camera for event-based motion segmentation, focusing on spatio-temporal graph cuts methodology."
      },
      "confidence_score": 1.0,
      "reasoning": "The citation mentions the EMSGC dataset, which is used for recording sequences with a DAVIS346 camera. This dataset is relevant to event-based stereo depth estimation.",
      "processing_time": 24.01267147064209,
      "citing_paper_id": "269525186",
      "cited_paper_id": 229211559
    },
    {
      "context_text": "Finally, we also test sequences from two motion segmentation datasets [20], [21].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.3,
      "reasoning": "The citation mentions 'motion segmentation datasets' but does not provide specific names. The titles of the cited papers do not clarify the dataset names.",
      "processing_time": 17.89461612701416,
      "citing_paper_id": "269525186",
      "cited_paper_id": 229211559
    },
    {
      "context_text": "To this end, we show results on three sequences from [20], [21] in Fig.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.0,
      "reasoning": "The citation does not provide specific dataset names, only references to sequences from other papers. No clear, verifiable datasets are mentioned.",
      "processing_time": 16.11905813217163,
      "citing_paper_id": "269525186",
      "cited_paper_id": 229211559
    },
    {
      "context_text": "The proposed meth-ods are compared with an unsupervised-learning method [59] (Section II) and a supervised-learning method E-RAFT [44].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods. The context focuses on comparing the proposed methods with other learning methods.",
      "processing_time": 17.117321491241455,
      "citing_paper_id": "269525186",
      "cited_paper_id": 239049376
    },
    {
      "context_text": "Notice that both DNN methods [44], [59] train and evaluate on the DSEC dataset, which is dominantly forward driving motion.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "DSEC"
      ],
      "dataset_descriptions": {
        "DSEC": "Used to train and evaluate DNN methods for event-based stereo depth estimation, focusing on forward driving motion scenarios."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions the DSEC dataset, which is used for training and evaluating DNN methods focused on event-based stereo depth estimation.",
      "processing_time": 22.16976523399353,
      "citing_paper_id": "269525186",
      "cited_paper_id": 239049376
    },
    {
      "context_text": "Since GT is missing at IMOs and points outside the LiDAR’s FOV, the supervised method [44] may provide inaccurate predictions around IMOs and road points closetothecamera,whereasourmethodproducessharpedges.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a supervised method and a comparison to the authors' method. No clear, verifiable dataset names are present.",
      "processing_time": 17.877583503723145,
      "citing_paper_id": "269525186",
      "cited_paper_id": 239049376
    },
    {
      "context_text": "Wealso evaluate on a recent dataset that provides ground truth ﬂow: DSEC [44].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "DSEC"
      ],
      "dataset_descriptions": {
        "DSEC": "Used to evaluate dense optical flow from event cameras, providing ground truth flow for validation in event-based stereo depth estimation."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions 'DSEC' as a dataset providing ground truth flow, which is relevant to event-based stereo depth estimation.",
      "processing_time": 21.811506271362305,
      "citing_paper_id": "269525186",
      "cited_paper_id": 239049376
    },
    {
      "context_text": "Supervised methods train ANNs in simulation and/or real-data [44], [49], [50], [51], [52], [53], [54].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general references to supervised methods and ANNs. No multi-word proper nouns, acronyms, or hyphenated names with digits are present.",
      "processing_time": 20.096057653427124,
      "citing_paper_id": "269525186",
      "cited_paper_id": 239049376
    },
    {
      "context_text": "Supervised methods train ANNs in simulation and/or real-data [44], [49], [50], [51], [52], [53], [54].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general references to supervised methods and ANNs. No multi-word proper nouns, acronyms, or hyphenated names with digits are present.",
      "processing_time": 20.096057653427124,
      "citing_paper_id": "269525186",
      "cited_paper_id": 257637142
    },
    {
      "context_text": "Supervised methods train ANNs in simulation and/or real-data [44], [49], [50], [51], [52], [53], [54].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general references to supervised methods and ANNs. No multi-word proper nouns, acronyms, or hyphenated names with digits are present.",
      "processing_time": 20.096057653427124,
      "citing_paper_id": "269525186",
      "cited_paper_id": null
    },
    {
      "context_text": "Supervised methods train ANNs in simulation and/or real-data [44], [49], [50], [51], [52], [53], [54].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general references to supervised methods and ANNs. No multi-word proper nouns, acronyms, or hyphenated names with digits are present.",
      "processing_time": 20.096057653427124,
      "citing_paper_id": "269525186",
      "cited_paper_id": null
    },
    {
      "context_text": "This is in part because the warp (4) can describe very complex ﬂow ﬁelds, which can push the events to accumulate in few pixels (i.e., event collapse [25], [26]).",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a phenomenon called 'event collapse' which is not a dataset.",
      "processing_time": 15.702246904373169,
      "citing_paper_id": "269525186",
      "cited_paper_id": 250408092
    },
    {
      "context_text": "The latter may be due to event collapse [25], but given recent advances on overcoming this issue [31], we show it is possible to succeed.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a methodological issue (event collapse) and a reference to overcoming it. No verifiable resources are identified.",
      "processing_time": 18.97659683227539,
      "citing_paper_id": "269525186",
      "cited_paper_id": 250408092
    },
    {
      "context_text": "Due to the regularizer in [24], the very ﬁne structure (e.g., the poster contents) might not be crisp.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method or technique involving a regularizer and optical flow.",
      "processing_time": 16.343600034713745,
      "citing_paper_id": "269525186",
      "cited_paper_id": 254591426
    },
    {
      "context_text": "These two quantities are entangled, and it is possible to use computed optical ﬂow to recover brightness, i.e., reconstruct intensity images [24].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method or approach involving optical flow and image reconstruction.",
      "processing_time": 16.33992052078247,
      "citing_paper_id": "269525186",
      "cited_paper_id": 254591426
    },
    {
      "context_text": "While this is not a problem in simulation, it incurs a performance gap when trained models are used to predict ﬂow on real data, due to often a large domain gap between training and test data [52], [55].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.3,
      "reasoning": "The citation mentions a performance gap between simulation and real data, suggesting the use of a dataset to highlight this issue. However, no specific dataset name is provided in the citation context.",
      "processing_time": 19.341479539871216,
      "citing_paper_id": "269525186",
      "cited_paper_id": 257505349
    },
    {
      "context_text": "As a result, these learning-based methods may overﬁt to the driving data (i.e., tend to predict forward motion) and fail to produce good results in other motions and datasets [55] (e.g., see E-RAFT rows on the MVSEC indoor seqs. in Table I).",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "MVSEC indoor seqs."
      ],
      "dataset_descriptions": {
        "MVSEC indoor seqs.": "Used to evaluate the generalizability of event-based optical flow methods, specifically testing performance on indoor sequences beyond driving data."
      },
      "confidence_score": 0.85,
      "reasoning": "The citation mentions 'MVSEC indoor seqs.' which is likely a dataset used for evaluating event-based optical flow estimation methods. The context suggests that the dataset is used to test the generalizability of the methods beyond driving data.",
      "processing_time": 26.55829405784607,
      "citing_paper_id": "269525186",
      "cited_paper_id": 257505349
    },
    {
      "context_text": "We develop two explicit methods to solve the PDEs, one with upwind differences and one with a conservative scheme adapted to Burgers’ terms [65].",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods for solving PDEs.",
      "processing_time": 14.223865747451782,
      "citing_paper_id": "269525186",
      "cited_paper_id": 267945091
    },
    {
      "context_text": "…Maximization (CM) [7], [8] is a powerful framework that allows us to tackle multiple motion estimation problems (rotational motion [9], [10], [11], [12], homographic motion [7], [13], [14], feature ﬂow estimation [15], [16], [17], [18], motion segmentation [19], [20], [21], [22], and also…",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various motion estimation problems and methods. There are no clear identifiers for datasets.",
      "processing_time": 17.83860158920288,
      "citing_paper_id": "269525186",
      "cited_paper_id": 268379520
    },
    {
      "context_text": "The main difference between our ﬂow (Section III-C) and concurrent proposals [53], [59], [82] is the motion hypothesis and its underlying assumptions: (7) assumes that the ﬂow is constant along its streamlines within short time intervals, which produces linear motion trajectories (Fig.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only discusses methodological differences.",
      "processing_time": 14.842848777770996,
      "citing_paper_id": "269525186",
      "cited_paper_id": null
    },
    {
      "context_text": "…work, Contrast Maximization (CM) [7], [8] is a powerful framework that allows us to tackle multiple motion estimation problems (rotational motion [9], [10], [11], [12], homographic motion [7], [13], [14], feature ﬂow estimation [15], [16], [17], [18], motion segmentation [19], [20], [21], [22],…",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various motion estimation problems and methods. There are no clear identifiers for datasets or other verifiable resources.",
      "processing_time": 18.116694450378418,
      "citing_paper_id": "269525186",
      "cited_paper_id": null
    },
    {
      "context_text": "This version of RAFT [48] was introduced along with the DSEC ﬂow benchmark and showed it can estimate pixel correspondences for large displacements.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions 'DSEC flow benchmark' which appears to be a specific dataset or benchmark. However, since it is primarily used for score comparison and not as a downloadable dataset, it is excluded.",
      "processing_time": 19.818978309631348,
      "citing_paper_id": "269525186",
      "cited_paper_id": null
    },
    {
      "context_text": "Among prior work, Contrast Maximization (CM) [7], [8] is a powerful framework that allows us to tackle multiple motion estimation problems (rotational motion [9], [10], [11], [12], homographic motion [7], [13], [14], feature ﬂow estimation [15], [16], [17], [18], motion segmentation [19], [20],…",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods and frameworks. There are no clear identifiers for datasets within the text.",
      "processing_time": 17.822721004486084,
      "citing_paper_id": "269525186",
      "cited_paper_id": null
    },
    {
      "context_text": "Among prior work, Contrast Maximization (CM) [7], [8] is a powerful framework that allows us to tackle multiple motion estimation problems (rotational motion [9], [10], [11], [12], homographic motion [7], [13], [14], feature ﬂow estimation [15], [16], [17], [18], motion segmentation [19], [20],…",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods and frameworks. There are no clear identifiers for datasets within the text.",
      "processing_time": 17.822721004486084,
      "citing_paper_id": "269525186",
      "cited_paper_id": null
    },
    {
      "context_text": "The CM framework [7] assumes events E .",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a framework and events. There are no clear identifiers for datasets.",
      "processing_time": 17.050941467285156,
      "citing_paper_id": "269525186",
      "cited_paper_id": null
    },
    {
      "context_text": "Each scale of our method has the same computational complexity as CM [7], O ( N e + N p ) because the multi-reference warps yield a constant scaling factor.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (CM) which is not a dataset.",
      "processing_time": 15.085415601730347,
      "citing_paper_id": "269525186",
      "cited_paper_id": null
    },
    {
      "context_text": "1) Objective Functions Based on the IWE Gradient: Among the contrast functions proposed in [7], [8], we use two functions based on the gradient of the IWE with q = 1 (the L 1 -norm) and q = 2 (the squared L 2 -norm).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only objective functions and their mathematical properties.",
      "processing_time": 15.678833484649658,
      "citing_paper_id": "269525186",
      "cited_paper_id": null
    },
    {
      "context_text": "1) Objective Functions Based on the IWE Gradient: Among the contrast functions proposed in [7], [8], we use two functions based on the gradient of the IWE with q = 1 (the L 1 -norm) and q = 2 (the squared L 2 -norm).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only objective functions and their mathematical properties.",
      "processing_time": 15.678833484649658,
      "citing_paper_id": "269525186",
      "cited_paper_id": null
    },
    {
      "context_text": "We compare the gradient-based functions ( L 1 and L 2 ), image variance [7], average timestamp [27], and normalized average timestamp [30].",
      "catation_intent": "none",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods and metrics. There are no clear identifiers for datasets.",
      "processing_time": 17.022267818450928,
      "citing_paper_id": "269525186",
      "cited_paper_id": null
    },
    {
      "context_text": "This poses the challenge of rethinking visual processing [2], [3]: motion patterns (i.e., optical ﬂow ) are no longer obtained by analyzing the intensities of images captured at regular intervals, but by analyzing the stream of per-pixel brightness changes produced by the event camera.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general change in visual processing methods.",
      "processing_time": 14.811028718948364,
      "citing_paper_id": "269525186",
      "cited_paper_id": null
    },
    {
      "context_text": "The ﬂow is obtained as the solution to the problem where, λ > 0 is the regularizer weight, and we use the total variation (TV) [66] as regularizer.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (total variation) used as a regularizer.",
      "processing_time": 16.734180688858032,
      "citing_paper_id": "269525186",
      "cited_paper_id": null
    },
    {
      "context_text": "…but with important differences: (i) it was done for the average timestamp loss, hence it did not consider the effect on contrast or focus functions [8], and (ii) it had a completely different motivation: to lessen a back-propagation scaling problem, so that the gradients of the loss would not…",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only discusses methodological differences and motivations.",
      "processing_time": 15.662880659103394,
      "citing_paper_id": "269525186",
      "cited_paper_id": null
    },
    {
      "context_text": "Stemming from an accurate and spatially-dependent contrast loss (the gradient magnitude [8]), we model the problem using a tile of patches (in ﬂow or depth parameters) and propose solutions to several problems: overﬁtting, occlusions, and convergence.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only discusses methodological aspects.",
      "processing_time": 14.030076265335083,
      "citing_paper_id": "269525186",
      "cited_paper_id": null
    },
    {
      "context_text": "Event alignment is measured by the strength of the edges of the IWE, which is directly related to image contrast [8].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method or concept related to image contrast.",
      "processing_time": 15.056755542755127,
      "citing_paper_id": "269525186",
      "cited_paper_id": null
    },
    {
      "context_text": "Ground truth (GT) in de facto standard datasets [4], [5] is obtained by the motion ﬁeld [6] given additional depth sensors and camera motion.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.3,
      "reasoning": "The citation mentions 'de facto standard datasets' but does not specify any particular dataset names. It refers to the general concept of ground truth in datasets used for depth estimation.",
      "processing_time": 19.097434759140015,
      "citing_paper_id": "269525186",
      "cited_paper_id": null
    },
    {
      "context_text": "It provides events, grayscale frames, IMU data, camera poses, and scene depth from a LiDAR [4].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions various types of data but does not specify a named dataset. It describes a collection of data types rather than a specific, verifiable dataset.",
      "processing_time": 18.0667462348938,
      "citing_paper_id": "269525186",
      "cited_paper_id": null
    },
    {
      "context_text": "State-of-the-art approaches are usually trained and evaluated in common datasets such as KITTI [9], Make3D [3] and NYUv2 [20].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "KITTI",
        "Make3D",
        "NYUv2"
      ],
      "dataset_descriptions": {
        "KITTI": "Used for training and evaluating stereo depth estimation algorithms, providing real-world driving scenarios with synchronized and calibrated stereo images.",
        "Make3D": "Used for training and evaluating single-image depth estimation, providing 3D scene structure from a single still image.",
        "NYUv2": "Used for training and evaluating indoor depth estimation, providing RGB-D images and corresponding depth maps for indoor scenes."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions three datasets: KITTI, Make3D, and NYUv2. These are specific datasets used for training and evaluation in the field of computer vision and robotics.",
      "processing_time": 33.688923358917236,
      "citing_paper_id": "223957202",
      "cited_paper_id": 545361
    },
    {
      "context_text": "State-of-the-art approaches are usually trained and evaluated in common datasets such as KITTI [9], Make3D [3] and NYUv2 [20].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "KITTI",
        "Make3D",
        "NYUv2"
      ],
      "dataset_descriptions": {
        "KITTI": "Used for training and evaluating stereo depth estimation algorithms, providing real-world driving scenarios with synchronized and calibrated stereo images.",
        "Make3D": "Used for training and evaluating single-image depth estimation, providing 3D scene structure from a single still image.",
        "NYUv2": "Used for training and evaluating indoor depth estimation, providing RGB-D images and corresponding depth maps for indoor scenes."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions three datasets: KITTI, Make3D, and NYUv2. These are specific datasets used for training and evaluation in the field of computer vision and robotics.",
      "processing_time": 33.688923358917236,
      "citing_paper_id": "223957202",
      "cited_paper_id": 9455111
    },
    {
      "context_text": "State-of-the-art approaches are usually trained and evaluated in common datasets such as KITTI [9], Make3D [3] and NYUv2 [20].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "KITTI",
        "Make3D",
        "NYUv2"
      ],
      "dataset_descriptions": {
        "KITTI": "Used for training and evaluating stereo depth estimation algorithms, providing real-world driving scenarios with synchronized and calibrated stereo images.",
        "Make3D": "Used for training and evaluating single-image depth estimation, providing 3D scene structure from a single still image.",
        "NYUv2": "Used for training and evaluating indoor depth estimation, providing RGB-D images and corresponding depth maps for indoor scenes."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions three datasets: KITTI, Make3D, and NYUv2. These are specific datasets used for training and evaluation in the field of computer vision and robotics.",
      "processing_time": 33.688923358917236,
      "citing_paper_id": "223957202",
      "cited_paper_id": 253064894
    },
    {
      "context_text": "Early works on event-based depth estimation used multi-view stereo [22] and later Simultaneous Localization and Mapping (SLAM) [25, 29, 37, 15] to build a representation of the environment (i.e.: map) and therefore derivate metric depth.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only general methods and approaches. No verifiable resource names are present.",
      "processing_time": 17.47584295272827,
      "citing_paper_id": "223957202",
      "cited_paper_id": 3738244
    },
    {
      "context_text": "Early works on event-based depth estimation used multi-view stereo [22] and later Simultaneous Localization and Mapping (SLAM) [25, 29, 37, 15] to build a representation of the environment (i.e.: map) and therefore derivate metric depth.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only general methods and approaches. No verifiable resource names are present.",
      "processing_time": 17.47584295272827,
      "citing_paper_id": "223957202",
      "cited_paper_id": 16588072
    },
    {
      "context_text": "Early works on event-based depth estimation used multi-view stereo [22] and later Simultaneous Localization and Mapping (SLAM) [25, 29, 37, 15] to build a representation of the environment (i.e.: map) and therefore derivate metric depth.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only general methods and approaches. No verifiable resource names are present.",
      "processing_time": 17.47584295272827,
      "citing_paper_id": "223957202",
      "cited_paper_id": 20619009
    },
    {
      "context_text": "Early works on event-based depth estimation used multi-view stereo [22] and later Simultaneous Localization and Mapping (SLAM) [25, 29, 37, 15] to build a representation of the environment (i.e.: map) and therefore derivate metric depth.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only general methods and approaches. No verifiable resource names are present.",
      "processing_time": 17.47584295272827,
      "citing_paper_id": "223957202",
      "cited_paper_id": 26324573
    },
    {
      "context_text": "Recently, these sensors have received great interest in various computer vision fields, ranging from computational photography [27, 26, 30, 31]1 to visual odometry [29, 25, 24, 37, 40, 14] and depth prediction [15, 25, 22, 36, 38, 33, 40].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general applications of event-based sensors. No dataset names are provided.",
      "processing_time": 17.49424910545349,
      "citing_paper_id": "223957202",
      "cited_paper_id": 3738244
    },
    {
      "context_text": "Recently, these sensors have received great interest in various computer vision fields, ranging from computational photography [27, 26, 30, 31]1 to visual odometry [29, 25, 24, 37, 40, 14] and depth prediction [15, 25, 22, 36, 38, 33, 40].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general applications of event-based sensors. No dataset names are provided.",
      "processing_time": 17.49424910545349,
      "citing_paper_id": "223957202",
      "cited_paper_id": 4412139
    },
    {
      "context_text": "Recently, these sensors have received great interest in various computer vision fields, ranging from computational photography [27, 26, 30, 31]1 to visual odometry [29, 25, 24, 37, 40, 14] and depth prediction [15, 25, 22, 36, 38, 33, 40].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general applications of event-based sensors. No dataset names are provided.",
      "processing_time": 17.49424910545349,
      "citing_paper_id": "223957202",
      "cited_paper_id": 16588072
    },
    {
      "context_text": "Recently, these sensors have received great interest in various computer vision fields, ranging from computational photography [27, 26, 30, 31]1 to visual odometry [29, 25, 24, 37, 40, 14] and depth prediction [15, 25, 22, 36, 38, 33, 40].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general applications of event-based sensors. No dataset names are provided.",
      "processing_time": 17.49424910545349,
      "citing_paper_id": "223957202",
      "cited_paper_id": 20619009
    },
    {
      "context_text": "Recently, these sensors have received great interest in various computer vision fields, ranging from computational photography [27, 26, 30, 31]1 to visual odometry [29, 25, 24, 37, 40, 14] and depth prediction [15, 25, 22, 36, 38, 33, 40].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general applications of event-based sensors. No dataset names are provided.",
      "processing_time": 17.49424910545349,
      "citing_paper_id": "223957202",
      "cited_paper_id": 26324573
    },
    {
      "context_text": "Recently, these sensors have received great interest in various computer vision fields, ranging from computational photography [27, 26, 30, 31]1 to visual odometry [29, 25, 24, 37, 40, 14] and depth prediction [15, 25, 22, 36, 38, 33, 40].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general applications of event-based sensors. No dataset names are provided.",
      "processing_time": 17.49424910545349,
      "citing_paper_id": "223957202",
      "cited_paper_id": 49877954
    },
    {
      "context_text": "Recently, these sensors have received great interest in various computer vision fields, ranging from computational photography [27, 26, 30, 31]1 to visual odometry [29, 25, 24, 37, 40, 14] and depth prediction [15, 25, 22, 36, 38, 33, 40].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general applications of event-based sensors. No dataset names are provided.",
      "processing_time": 17.49424910545349,
      "citing_paper_id": "223957202",
      "cited_paper_id": 56475917
    },
    {
      "context_text": "Recently, these sensors have received great interest in various computer vision fields, ranging from computational photography [27, 26, 30, 31]1 to visual odometry [29, 25, 24, 37, 40, 14] and depth prediction [15, 25, 22, 36, 38, 33, 40].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general applications of event-based sensors. No dataset names are provided.",
      "processing_time": 17.49424910545349,
      "citing_paper_id": "223957202",
      "cited_paper_id": 189998802
    },
    {
      "context_text": "Early works on event-based depth estimation used multiview stereo [22] and later Simultaneous Localization and Mapping (SLAM) [25, 29, 37, 15] to build a representation of the environment (i.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only methods and approaches such as multiview stereo and SLAM. The cited papers also do not provide specific dataset names.",
      "processing_time": 19.640814781188965,
      "citing_paper_id": "223957202",
      "cited_paper_id": 3738244
    },
    {
      "context_text": "Early works on event-based depth estimation used multiview stereo [22] and later Simultaneous Localization and Mapping (SLAM) [25, 29, 37, 15] to build a representation of the environment (i.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only methods and approaches such as multiview stereo and SLAM. The cited papers also do not provide specific dataset names.",
      "processing_time": 19.640814781188965,
      "citing_paper_id": "223957202",
      "cited_paper_id": 16588072
    },
    {
      "context_text": "Early works on event-based depth estimation used multiview stereo [22] and later Simultaneous Localization and Mapping (SLAM) [25, 29, 37, 15] to build a representation of the environment (i.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only methods and approaches such as multiview stereo and SLAM. The cited papers also do not provide specific dataset names.",
      "processing_time": 19.640814781188965,
      "citing_paper_id": "223957202",
      "cited_paper_id": 20619009
    },
    {
      "context_text": "Early works on event-based depth estimation used multiview stereo [22] and later Simultaneous Localization and Mapping (SLAM) [25, 29, 37, 15] to build a representation of the environment (i.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only methods and approaches such as multiview stereo and SLAM. The cited papers also do not provide specific dataset names.",
      "processing_time": 19.640814781188965,
      "citing_paper_id": "223957202",
      "cited_paper_id": 26324573
    },
    {
      "context_text": "Depth prediction using event cameras has experienced a surge in popularity in recent years [29, 22, 36, 25, 24, 15, 40, 36, 38, 33], due to its potential in robotics and the automotive industry.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only a general trend in depth prediction using event cameras. No specific dataset names are provided.",
      "processing_time": 17.760346174240112,
      "citing_paper_id": "223957202",
      "cited_paper_id": 3738244
    },
    {
      "context_text": "Depth prediction using event cameras has experienced a surge in popularity in recent years [29, 22, 36, 25, 24, 15, 40, 36, 38, 33], due to its potential in robotics and the automotive industry.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only a general trend in depth prediction using event cameras. No specific dataset names are provided.",
      "processing_time": 17.760346174240112,
      "citing_paper_id": "223957202",
      "cited_paper_id": 4412139
    },
    {
      "context_text": "Depth prediction using event cameras has experienced a surge in popularity in recent years [29, 22, 36, 25, 24, 15, 40, 36, 38, 33], due to its potential in robotics and the automotive industry.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only a general trend in depth prediction using event cameras. No specific dataset names are provided.",
      "processing_time": 17.760346174240112,
      "citing_paper_id": "223957202",
      "cited_paper_id": 16588072
    },
    {
      "context_text": "Depth prediction using event cameras has experienced a surge in popularity in recent years [29, 22, 36, 25, 24, 15, 40, 36, 38, 33], due to its potential in robotics and the automotive industry.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only a general trend in depth prediction using event cameras. No specific dataset names are provided.",
      "processing_time": 17.760346174240112,
      "citing_paper_id": "223957202",
      "cited_paper_id": 26324573
    },
    {
      "context_text": "Depth prediction using event cameras has experienced a surge in popularity in recent years [29, 22, 36, 25, 24, 15, 40, 36, 38, 33], due to its potential in robotics and the automotive industry.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only a general trend in depth prediction using event cameras. No specific dataset names are provided.",
      "processing_time": 17.760346174240112,
      "citing_paper_id": "223957202",
      "cited_paper_id": 49877954
    },
    {
      "context_text": "Depth prediction using event cameras has experienced a surge in popularity in recent years [29, 22, 36, 25, 24, 15, 40, 36, 38, 33], due to its potential in robotics and the automotive industry.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only a general trend in depth prediction using event cameras. No specific dataset names are provided.",
      "processing_time": 17.760346174240112,
      "citing_paper_id": "223957202",
      "cited_paper_id": 56475917
    },
    {
      "context_text": "However, most existing works are limited: they can reliably only predict sparse or semi-dense depth maps [29, 22, 36, 25, 24, 15, 40, 36, 38] or rely on a stereo setup to generate dense depth predictions [33].",
      "catation_intent": "limitation",
      "resource_type": "limitation",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only limitations of existing works. No dataset names are provided in the context.",
      "processing_time": 17.481693029403687,
      "citing_paper_id": "223957202",
      "cited_paper_id": 3738244
    },
    {
      "context_text": "However, most existing works are limited: they can reliably only predict sparse or semi-dense depth maps [29, 22, 36, 25, 24, 15, 40, 36, 38] or rely on a stereo setup to generate dense depth predictions [33].",
      "catation_intent": "limitation",
      "resource_type": "limitation",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only limitations of existing works. No dataset names are provided in the context.",
      "processing_time": 17.481693029403687,
      "citing_paper_id": "223957202",
      "cited_paper_id": 4412139
    },
    {
      "context_text": "However, most existing works are limited: they can reliably only predict sparse or semi-dense depth maps [29, 22, 36, 25, 24, 15, 40, 36, 38] or rely on a stereo setup to generate dense depth predictions [33].",
      "catation_intent": "limitation",
      "resource_type": "limitation",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only limitations of existing works. No dataset names are provided in the context.",
      "processing_time": 17.481693029403687,
      "citing_paper_id": "223957202",
      "cited_paper_id": 16588072
    },
    {
      "context_text": "However, most existing works are limited: they can reliably only predict sparse or semi-dense depth maps [29, 22, 36, 25, 24, 15, 40, 36, 38] or rely on a stereo setup to generate dense depth predictions [33].",
      "catation_intent": "limitation",
      "resource_type": "limitation",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only limitations of existing works. No dataset names are provided in the context.",
      "processing_time": 17.481693029403687,
      "citing_paper_id": "223957202",
      "cited_paper_id": 26324573
    },
    {
      "context_text": "However, most existing works are limited: they can reliably only predict sparse or semi-dense depth maps [29, 22, 36, 25, 24, 15, 40, 36, 38] or rely on a stereo setup to generate dense depth predictions [33].",
      "catation_intent": "limitation",
      "resource_type": "limitation",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only limitations of existing works. No dataset names are provided in the context.",
      "processing_time": 17.481693029403687,
      "citing_paper_id": "223957202",
      "cited_paper_id": 49877954
    },
    {
      "context_text": "However, most existing works are limited: they can reliably only predict sparse or semi-dense depth maps [29, 22, 36, 25, 24, 15, 40, 36, 38] or rely on a stereo setup to generate dense depth predictions [33].",
      "catation_intent": "limitation",
      "resource_type": "limitation",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only limitations of existing works. No dataset names are provided in the context.",
      "processing_time": 17.481693029403687,
      "citing_paper_id": "223957202",
      "cited_paper_id": 56475917
    },
    {
      "context_text": "[24] sparse yes yes no [37] sparse yes yes no [15] semi-dense yes no no [25] semi-dense yes yes no [36] semi-dense no yes no [38] semi-dense no yes no",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only categories of data density and processing types. No verifiable resource names are present.",
      "processing_time": 18.283827304840088,
      "citing_paper_id": "223957202",
      "cited_paper_id": 4412139
    },
    {
      "context_text": "[24] sparse yes yes no [37] sparse yes yes no [15] semi-dense yes no no [25] semi-dense yes yes no [36] semi-dense no yes no [38] semi-dense no yes no",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only categories of data density and processing types. No verifiable resource names are present.",
      "processing_time": 18.283827304840088,
      "citing_paper_id": "223957202",
      "cited_paper_id": 16588072
    },
    {
      "context_text": "[24] sparse yes yes no [37] sparse yes yes no [15] semi-dense yes no no [25] semi-dense yes yes no [36] semi-dense no yes no [38] semi-dense no yes no",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only categories of data density and processing types. No verifiable resource names are present.",
      "processing_time": 18.283827304840088,
      "citing_paper_id": "223957202",
      "cited_paper_id": 20619009
    },
    {
      "context_text": "[24] sparse yes yes no [37] sparse yes yes no [15] semi-dense yes no no [25] semi-dense yes yes no [36] semi-dense no yes no [38] semi-dense no yes no",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only categories of data density and processing types. No verifiable resource names are present.",
      "processing_time": 18.283827304840088,
      "citing_paper_id": "223957202",
      "cited_paper_id": 26324573
    },
    {
      "context_text": "[24] sparse yes yes no [37] sparse yes yes no [15] semi-dense yes no no [25] semi-dense yes yes no [36] semi-dense no yes no [38] semi-dense no yes no",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only categories of data density and processing types. No verifiable resource names are present.",
      "processing_time": 18.283827304840088,
      "citing_paper_id": "223957202",
      "cited_paper_id": 49877954
    },
    {
      "context_text": "Purely vision-based methods have investigated the use of stereo event cameras for depth estimation [36, 38] in which they rely on maximizing a temporal (as opposed to",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions 'stereo event cameras for depth estimation' but does not specify any dataset names. The cited papers do not provide additional dataset information.",
      "processing_time": 18.548113346099854,
      "citing_paper_id": "223957202",
      "cited_paper_id": 4412139
    },
    {
      "context_text": "Purely vision-based methods have investigated the use of stereo event cameras for depth estimation [36, 38] in which they rely on maximizing a temporal (as opposed to",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions 'stereo event cameras for depth estimation' but does not specify any dataset names. The cited papers do not provide additional dataset information.",
      "processing_time": 18.548113346099854,
      "citing_paper_id": "223957202",
      "cited_paper_id": 49877954
    },
    {
      "context_text": "We implement an event camera sensor in CARLA [4] based on the previous event simulator ESIM [23].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions CARLA and ESIM, but neither are datasets. CARLA is a driving simulator, and ESIM is an event simulator. No datasets are explicitly mentioned or used.",
      "processing_time": 20.086606979370117,
      "citing_paper_id": "223957202",
      "cited_paper_id": 5550767
    },
    {
      "context_text": "We release DENSE, a dataset recorded in CARLA, which comprises events, intensity frames, semantic labels, and depth maps.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "DENSE"
      ],
      "dataset_descriptions": {
        "DENSE": "Used to provide event-based stereo depth estimation data, including events, intensity frames, semantic labels, and depth maps, recorded in the CARLA simulator."
      },
      "confidence_score": 1.0,
      "reasoning": "DENSE is identified as a dataset containing events, intensity frames, semantic labels, and depth maps, recorded in CARLA. CARLA itself is a simulator, not a dataset.",
      "processing_time": 24.189817428588867,
      "citing_paper_id": "223957202",
      "cited_paper_id": 5550767
    },
    {
      "context_text": "• The implementation of an event camera plugin in the CARLA [4] simulator.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions the CARLA simulator but does not refer to a specific dataset. The CARLA simulator is a tool or environment, not a dataset.",
      "processing_time": 19.238126516342163,
      "citing_paper_id": "223957202",
      "cited_paper_id": 5550767
    },
    {
      "context_text": "CARLA Towns 01 to 05 are the scenes for training, Town 06 and 07 for validation, and the test sequence is acquired using Town 10.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "CARLA Towns 01 to 05",
        "CARLA Town 06",
        "CARLA Town 07",
        "CARLA Town 10"
      ],
      "dataset_descriptions": {
        "CARLA Towns 01 to 05": "Used for training models in an urban driving simulation, focusing on various driving scenarios and conditions.",
        "CARLA Town 06": "Used for validating models trained on CARLA Towns 01 to 05, assessing performance on unseen but similar urban environments.",
        "CARLA Town 07": "Used for validating models trained on CARLA Towns 01 to 05, assessing performance on unseen but similar urban environments.",
        "CARLA Town 10": "Used for testing the final model, evaluating its performance in a completely new urban environment."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions specific towns within the CARLA simulator used for training, validation, and testing. CARLA is a well-known open-source driving simulator, and the towns are specific components of this simulator.",
      "processing_time": 44.95083570480347,
      "citing_paper_id": "223957202",
      "cited_paper_id": 5550767
    },
    {
      "context_text": "Fig 5 shows the qualitative results on the DENSE test sequence corresponding with Town10 in CARLA.",
      "catation_intent": "reusable resource",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions 'DENSE test sequence' and 'Town10 in CARLA', which are specific components of a dataset or simulation environment. However, 'DENSE' and 'CARLA' are not datasets but rather a test sequence and a simulator, respectively.",
      "processing_time": 21.52340006828308,
      "citing_paper_id": "223957202",
      "cited_paper_id": 5550767
    },
    {
      "context_text": "We train with a batch size of 20 and a learning rate of 10 − 4 and use the Adam [16] optimizer.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions the Adam optimizer, which is a method, not a dataset. No datasets are mentioned in the context.",
      "processing_time": 17.725037097930908,
      "citing_paper_id": "223957202",
      "cited_paper_id": 6628106
    },
    {
      "context_text": "We train with a batch size of 20 and a learning rate of 10−4 and use the Adam [16] optimizer.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions the Adam optimizer, which is a method, not a dataset. No datasets are mentioned in the citation context.",
      "processing_time": 18.007266998291016,
      "citing_paper_id": "223957202",
      "cited_paper_id": 6628106
    },
    {
      "context_text": "The results show metric numbers within the range of state of the art image-based methods in popular datasets like KITTI.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "KITTI"
      ],
      "dataset_descriptions": {
        "KITTI": "Used to compare the performance of event-based stereo depth estimation methods against state-of-the-art image-based methods, focusing on accuracy and robustness in real-world scenarios."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions 'KITTI' as a popular dataset, and the cited paper title confirms it is a dataset used in vision and robotics.",
      "processing_time": 23.531070709228516,
      "citing_paper_id": "223957202",
      "cited_paper_id": 9455111
    },
    {
      "context_text": "MegaDepth is trained with the MD dataset from images available on the Internet and this achieves superior generalizability than MonoDepth which is trained with KITTI [9] and reported the values from Zhu et al [40].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "KITTI"
      ],
      "dataset_descriptions": {
        "KITTI": "Used to train MonoDepth for stereo depth estimation, comparing its performance against MegaDepth trained on the MD dataset. Focuses on evaluating generalizability and accuracy in depth estimation tasks."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions the KITTI dataset, which is a well-known dataset in the field of computer vision and robotics, particularly for stereo depth estimation.",
      "processing_time": 24.306573390960693,
      "citing_paper_id": "223957202",
      "cited_paper_id": 9455111
    },
    {
      "context_text": "Another work proposed multi-scale features with Markov Random Field (MRF) [1].",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method involving multi-scale features with MRF.",
      "processing_time": 23.674896717071533,
      "citing_paper_id": "223957202",
      "cited_paper_id": 10748875
    },
    {
      "context_text": "Event cameras, such as the Dynamic Vision Sensor (DVS) [19] or the ATIS [21], are bio-inspired vision sensors with radically different working principles compared to conventional cameras.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only hardware sensors. No dataset names are present in the citation span.",
      "processing_time": 25.02707839012146,
      "citing_paper_id": "223957202",
      "cited_paper_id": 21891196
    },
    {
      "context_text": "In this section, we present qualitative and quantitative results and compare them with previous methods [40] on the MVSEC dataset.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "MVSEC"
      ],
      "dataset_descriptions": {
        "MVSEC": "Used to evaluate and compare the performance of the proposed method against previous approaches in event-based stereo depth estimation."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions the MVSEC dataset, which is a specific dataset used for event-based vision research, including depth estimation.",
      "processing_time": 30.579118013381958,
      "citing_paper_id": "223957202",
      "cited_paper_id": 56475917
    },
    {
      "context_text": "The event stream is grouped into nonoverlapping windows of events and converted to tensor-like voxel grids [40].",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific dataset names, only a method for converting event streams into tensor-like voxel grids.",
      "processing_time": 24.749680519104004,
      "citing_paper_id": "223957202",
      "cited_paper_id": 56475917
    },
    {
      "context_text": "Recently, several learning-based approaches have emerged that have led to significant improvements in depth estimation [40, 33].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only references to learning-based approaches. No clear identifiers for datasets are present.",
      "processing_time": 25.17110323905945,
      "citing_paper_id": "223957202",
      "cited_paper_id": 56475917
    },
    {
      "context_text": "[40] semi-dense yes yes yes [33] dense no yes yes Ours dense yes yes yes",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not contain any specific dataset names or verifiable resources. It appears to be a table or list comparing different methods, including the authors' own method.",
      "processing_time": 27.121594667434692,
      "citing_paper_id": "223957202",
      "cited_paper_id": 56475917
    },
    {
      "context_text": "One way to encode these events is by representing them as a spatio-temporal voxel grid [40, 8] with Figure 2: Our network architecture, image adapted from [27].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only methods and representations. The cited papers' titles do not provide additional dataset information.",
      "processing_time": 26.397547721862793,
      "citing_paper_id": "223957202",
      "cited_paper_id": 56475917
    },
    {
      "context_text": "One way to encode these events is by representing them as a spatio-temporal voxel grid [40, 8] with Figure 2: Our network architecture, image adapted from [27].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only methods and representations. The cited papers' titles do not provide additional dataset information.",
      "processing_time": 26.397547721862793,
      "citing_paper_id": "223957202",
      "cited_paper_id": 189998802
    },
    {
      "context_text": "Among these, [40] presents a feedforward neural network that jointly predicts relative camera pose and per-pixel disparities.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for predicting camera pose and disparities.",
      "processing_time": 24.57764434814453,
      "citing_paper_id": "223957202",
      "cited_paper_id": 56475917
    },
    {
      "context_text": "(a) E2VID[27] (b) MegaDepth on E2VID (c) Events (d) Ours on events",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions 'E2VID' and 'MegaDepth', but these are likely methods or models rather than datasets. No specific datasets are mentioned.",
      "processing_time": 26.70795750617981,
      "citing_paper_id": "223957202",
      "cited_paper_id": 189998802
    },
    {
      "context_text": "7 depicts the case of predicting depth from a reconstructed frame using E2VID [27], which has HDR and does not suffer from motion blur.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions E2VID, which is a method for processing event camera data, not a dataset. No specific dataset is mentioned or used in the context.",
      "processing_time": 27.105539083480835,
      "citing_paper_id": "223957202",
      "cited_paper_id": 189998802
    },
    {
      "context_text": "MegaDepth is further applied to frames reconstructed from events using E2VID [27].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions 'E2VID' which is a method for reconstructing frames from events using an event camera. No specific dataset is mentioned.",
      "processing_time": 26.36351776123047,
      "citing_paper_id": "223957202",
      "cited_paper_id": 189998802
    },
    {
      "context_text": "MegaDepth refers to MegaDepth [18] using E2VID [27] reconstructed frames and Ours refers to our method trained using S∗ → (S + R).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions 'MegaDepth' and 'E2VID', but neither are used as datasets. MegaDepth is a method for depth estimation, and E2VID is a method for reconstructing frames from event cameras. No specific datasets are mentioned.",
      "processing_time": 30.776773929595947,
      "citing_paper_id": "223957202",
      "cited_paper_id": 189998802
    },
    {
      "context_text": "MegaDepth + refers to MegaDepth [18] using E2VID [27] reconstructed frames and Ours # refers to our method trained using S ∗ → ( S + R ) .",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "MegaDepth"
      ],
      "dataset_descriptions": {
        "MegaDepth": "Used to evaluate stereo depth estimation methods, specifically comparing performance with event camera reconstructed frames. The dataset provides dense depth maps for training and evaluation."
      },
      "confidence_score": 0.9,
      "reasoning": "The context mentions 'MegaDepth' and 'E2VID', but only MegaDepth is referred to as a dataset. E2VID is likely a method or tool.",
      "processing_time": 33.68497347831726,
      "citing_paper_id": "223957202",
      "cited_paper_id": 189998802
    },
    {
      "context_text": "However, the number of events between the two cameras may also differ, and so we apply a similarity loss on the census transforms [21] of the images.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific dataset names, only a method (census transforms). No verifiable datasets are referenced.",
      "processing_time": 26.369680643081665,
      "citing_paper_id": "56475917",
      "cited_paper_id": 703552
    },
    {
      "context_text": "2, and a robust similarity loss between the census transforms [21, 17] of the deblurred event images.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and techniques. There are no clear identifiers for datasets in the provided context.",
      "processing_time": 26.683756589889526,
      "citing_paper_id": "56475917",
      "cited_paper_id": 703552
    },
    {
      "context_text": "[10], there has been a strong interest in the development of algorithms that leverage the benefits provided by these cameras.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, models, or methods. It only refers to a general interest in developing algorithms for event-based cameras.",
      "processing_time": 27.8287193775177,
      "citing_paper_id": "56475917",
      "cited_paper_id": 2497402
    },
    {
      "context_text": "Recently, there have been several works, such as [4, 5, 13, 26, 24], that have shown that optical ﬂow, and other types of motion information, can be estimated from a spatiotemporal volume of events, by propagating the events along the optical ﬂow direction, and attempting to minimize the motion…",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general concepts and methods. No dataset names are present in the text.",
      "processing_time": 26.66176152229309,
      "citing_paper_id": "56475917",
      "cited_paper_id": 3328976
    },
    {
      "context_text": "[13], who use a loss which minimizes the sum of squares of the average timestamp at each pixel.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for loss calculation.",
      "processing_time": 24.389171600341797,
      "citing_paper_id": "56475917",
      "cited_paper_id": 3845250
    },
    {
      "context_text": "[13] for a neural network, by generating a single fully differentiable loss function that allows our networks to learn optical flow and structure from motion in an unsupervised manner.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for learning optical flow and structure from motion.",
      "processing_time": 25.772039651870728,
      "citing_paper_id": "56475917",
      "cited_paper_id": 3845250
    },
    {
      "context_text": "As the translation predicted by SFMLearner is only up to a scale factor, we present errors in terms of angular error between both the predicted translation and rotations.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only discusses the method's performance metric.",
      "processing_time": 24.681612730026245,
      "citing_paper_id": "56475917",
      "cited_paper_id": 11977588
    },
    {
      "context_text": "As there is currently no public code to the extent of our knowledge for unsupervised deep SFM methods with a stereo loss, we compare our ego-motion results against SFMLearner by Zhou et al. [22], which learns egomotion and depth from monocular grayscale images, while ac-knowledging that our loss has access to an additional stereo image at training time.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (SFMLearner) for comparison. The context focuses on the methodology and results rather than a dataset.",
      "processing_time": 28.73232388496399,
      "citing_paper_id": "56475917",
      "cited_paper_id": 11977588
    },
    {
      "context_text": "We train the SFMLearner models on the VI-Sensor images from the outdoor day2 sequence, once again cropping out the hood of the car.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.3,
      "reasoning": "The context mentions using VI-Sensor images from the outdoor day2 sequence, which appears to be a specific dataset used for training models. However, the name 'VI-Sensor' is not a clear, verifiable dataset name.",
      "processing_time": 30.04804825782776,
      "citing_paper_id": "56475917",
      "cited_paper_id": 11977588
    },
    {
      "context_text": "[23] show that a network can learn a camera’s egomotion and depth using camera reprojection and a photoconsistency loss.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for learning depth and ego-motion.",
      "processing_time": 24.944142818450928,
      "citing_paper_id": "56475917",
      "cited_paper_id": 11977588
    },
    {
      "context_text": "As there is currently no public code to the extent of our knowledge for unsupervised deep SFM methods with a stereo loss, we compare our ego-motion results against SFMLearner [23], and ECN [20], which learn egomotion and depth from monocular images and events.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models and methods. The context focuses on comparing ego-motion results against other methods.",
      "processing_time": 27.07361149787903,
      "citing_paper_id": "56475917",
      "cited_paper_id": 11977588
    },
    {
      "context_text": "1, we can see that our method outperforms EV-FlowNet in almost all experiments, and nears the performance of UnFlow on the short 1 frame sequences.",
      "catation_intent": "findings",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only comparisons between methods. No verifiable resources are identified.",
      "processing_time": 26.129873514175415,
      "citing_paper_id": "56475917",
      "cited_paper_id": 19160323
    },
    {
      "context_text": "1, where we compare our results against EV-FlowNet [24] and the image method UnFlow [12].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only comparisons with other methods.",
      "processing_time": 25.08896827697754,
      "citing_paper_id": "56475917",
      "cited_paper_id": 19160323
    },
    {
      "context_text": "[12] extend this work by applying a bidirectional census loss to improve the quality of the flow.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (bidirectional census loss) applied to improve optical flow quality.",
      "processing_time": 26.85653805732727,
      "citing_paper_id": "56475917",
      "cited_paper_id": 19160323
    },
    {
      "context_text": "[25] use an EM based feature tracking method to perform visual-inertial odometry, while Rebecq et al.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for visual-inertial odometry.",
      "processing_time": 25.92345905303955,
      "citing_paper_id": "56475917",
      "cited_paper_id": 20619009
    },
    {
      "context_text": "[9] demonstrate that a Kalman filter can reconstruct the pose of the camera and a local map.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (Kalman filter) and a general application (reconstructing camera pose and local map).",
      "processing_time": 28.04484272003174,
      "citing_paper_id": "56475917",
      "cited_paper_id": 26324573
    },
    {
      "context_text": "it is possible to apply a simple geometrical model under the assumption that the camera setup is symmetric and that the camera revolution is restricted to a planar motion [12], [15].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a geometrical model and assumptions about camera setup. No verifiable resources are referenced.",
      "processing_time": 27.784015655517578,
      "citing_paper_id": "21539113",
      "cited_paper_id": 377732
    },
    {
      "context_text": "As a standard stereo method, we used [27] again as well as the fast Census-based stereo matching method [30].",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods. The context focuses on stereo matching algorithms, which are methods, not datasets.",
      "processing_time": 27.536815881729126,
      "citing_paper_id": "21539113",
      "cited_paper_id": 458430
    },
    {
      "context_text": "V ISION-BASED systems [1], [2] enabling 3-D panoramic vision provide enhanced capabilities such as autonomous navigation, localization [3], [4], mapping [5], exploration, or environmental sensing (e.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general capabilities of vision-based systems. No verifiable resources are identified.",
      "processing_time": 27.042959928512573,
      "citing_paper_id": "21539113",
      "cited_paper_id": 2804295
    },
    {
      "context_text": "V ISION-BASED systems [1], [2] enabling 3-D panoramic vision provide enhanced capabilities such as autonomous navigation, localization [3], [4], mapping [5], exploration, or environmental sensing (e.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general capabilities of vision-based systems. No verifiable resources are identified.",
      "processing_time": 27.042959928512573,
      "citing_paper_id": "21539113",
      "cited_paper_id": 9213824
    },
    {
      "context_text": "V ISION-BASED systems [1], [2] enabling 3-D panoramic vision provide enhanced capabilities such as autonomous navigation, localization [3], [4], mapping [5], exploration, or environmental sensing (e.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general capabilities of vision-based systems. No verifiable resources are identified.",
      "processing_time": 27.042959928512573,
      "citing_paper_id": "21539113",
      "cited_paper_id": 15125781
    },
    {
      "context_text": "To determine the parameters R and φ, we follow the approach from [26].",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for calibrating a rotating sensor-matrix camera.",
      "processing_time": 26.61291193962097,
      "citing_paper_id": "21539113",
      "cited_paper_id": 10054975
    },
    {
      "context_text": ", situation awareness) [6], [7], which are useful in robotic and wide-area surveillance applications.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general applications. No dataset names are present in the text.",
      "processing_time": 26.608132123947144,
      "citing_paper_id": "21539113",
      "cited_paper_id": 23486523
    },
    {
      "context_text": ", situation awareness) [6], [7], which are useful in robotic and wide-area surveillance applications.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general applications. No dataset names are present in the text.",
      "processing_time": 26.608132123947144,
      "citing_paper_id": "21539113",
      "cited_paper_id": 40864267
    },
    {
      "context_text": "The resulting sensor data are a stream of asynchronous events, encoded in the form of an addressevent representation [25], yielding a sparse visual code.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific, verifiable datasets. It describes a method of encoding sensor data into an address-event representation, which is not a dataset.",
      "processing_time": 28.67734169960022,
      "citing_paper_id": "21539113",
      "cited_paper_id": 27144412
    },
    {
      "context_text": "Hence, to allow for extrinsic calibration between our camera modalities we use a pattern of AprilTags [15].",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions AprilTags but does not refer to a dataset. It is used as a method for extrinsic calibration between camera modalities.",
      "processing_time": 28.012346744537354,
      "citing_paper_id": "237142365",
      "cited_paper_id": 277804
    },
    {
      "context_text": "To allow for accurate intrinsic parameter estimation, in particular radial distortion of the lenses, we validate for each calibration sequence that the detections of AprilTags are spread over the full image dimension of each camera.",
      "catation_intent": "none",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific, verifiable datasets. It only discusses the use of AprilTags for calibration, which is a method or tool, not a dataset.",
      "processing_time": 29.168527603149414,
      "citing_paper_id": "237142365",
      "cited_paper_id": 277804
    },
    {
      "context_text": "To enable detection of AprilTags in the event stream, we transform the events into a frame-like structure.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a method for transforming events into a frame-like structure for detecting AprilTags.",
      "processing_time": 27.301954984664917,
      "citing_paper_id": "237142365",
      "cited_paper_id": 277804
    },
    {
      "context_text": "The exposure time is set to a low value of 3.2 milliseconds to minimize motion blur but still allow detection of AprilTags.",
      "catation_intent": "none",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a technical detail about exposure time and the use of AprilTags. The cited paper title confirms that AprilTags is a method, not a dataset.",
      "processing_time": 29.863720893859863,
      "citing_paper_id": "237142365",
      "cited_paper_id": 277804
    },
    {
      "context_text": "Using a time surface representation, we noticed a higher detection recall of AprilTags and hence a slightly more robust calibration.",
      "catation_intent": "findings",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a method (AprilTag) and its performance in a certain representation. No verifiable dataset is referenced.",
      "processing_time": 28.19831657409668,
      "citing_paper_id": "237142365",
      "cited_paper_id": 277804
    },
    {
      "context_text": "[12] as well as the recent dataset DSEC by Gehrig et al.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "DSEC"
      ],
      "dataset_descriptions": {
        "DSEC": "Used to evaluate event-based stereo depth estimation methods, providing synchronized event and intensity data for 3D perception tasks."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions 'DSEC' which is a specific dataset name. The cited paper title confirms it is a dataset for 3D perception, relevant to event-based stereo depth estimation.",
      "processing_time": 33.22834777832031,
      "citing_paper_id": "237142365",
      "cited_paper_id": 3416874
    },
    {
      "context_text": "Similar to the TUM-VI dataset [3], we assume that the IMU measurements are subject to white noise with standard deviation σw and an additive bias value.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "TUM-VI"
      ],
      "dataset_descriptions": {
        "TUM-VI": "Used to evaluate visual-inertial odometry systems, focusing on IMU measurements with white noise and additive bias, providing a benchmark for system performance."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions the TUM-VI dataset, which is a specific dataset used for evaluating visual-inertial odometry systems. The dataset is referenced for its characteristics relevant to IMU measurements.",
      "processing_time": 35.23751497268677,
      "citing_paper_id": "237142365",
      "cited_paper_id": 4884375
    },
    {
      "context_text": "Contrary to most existing datasets, we provide the calibration of IMU biases, as well as axis scaling and misalignment similar to [3].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific dataset names, only a general reference to 'existing datasets'. The cited paper title suggests a benchmark, which is excluded unless it refers to a specific, downloadable dataset.",
      "processing_time": 29.62075638771057,
      "citing_paper_id": "237142365",
      "cited_paper_id": 4884375
    },
    {
      "context_text": "Fourth, we provide calibrated IMU noise parameters which are required for accurate probabilistic modeling in state estimation algorithms [3].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific dataset, only IMU noise parameters which are not considered a dataset.",
      "processing_time": 25.850313901901245,
      "citing_paper_id": "237142365",
      "cited_paper_id": 4884375
    },
    {
      "context_text": "We provide evaluation for Basalt [17] and VINS-Fusion [18, 19, 20].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only methods or systems (Basalt, VINS-Fusion). The cited papers' titles do not provide additional information about datasets.",
      "processing_time": 29.72822642326355,
      "citing_paper_id": "237142365",
      "cited_paper_id": 7334757
    },
    {
      "context_text": "We provide evaluation for Basalt [17] and VINS-Fusion [18, 19, 20].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only methods or systems (Basalt, VINS-Fusion). The cited papers' titles do not provide additional information about datasets.",
      "processing_time": 29.72822642326355,
      "citing_paper_id": "237142365",
      "cited_paper_id": 57825739
    },
    {
      "context_text": "We provide evaluation for Basalt [17] and VINS-Fusion [18, 19, 20].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only methods or systems (Basalt, VINS-Fusion). The cited papers' titles do not provide additional information about datasets.",
      "processing_time": 29.72822642326355,
      "citing_paper_id": "237142365",
      "cited_paper_id": 119105448
    },
    {
      "context_text": "Basalt and VINS-Fusion perform well for non-challenging sequences as expected.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods (Basalt and VINS-Fusion). The context is about performance evaluation of these methods, not the use of datasets.",
      "processing_time": 29.836297035217285,
      "citing_paper_id": "237142365",
      "cited_paper_id": 7334757
    },
    {
      "context_text": "Basalt and VINS-Fusion perform well for non-challenging sequences as expected.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods (Basalt and VINS-Fusion). The context is about performance evaluation of these methods, not the use of datasets.",
      "processing_time": 29.836297035217285,
      "citing_paper_id": "237142365",
      "cited_paper_id": 57825739
    },
    {
      "context_text": "Basalt and VINS-Fusion perform well for non-challenging sequences as expected.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods (Basalt and VINS-Fusion). The context is about performance evaluation of these methods, not the use of datasets.",
      "processing_time": 29.836297035217285,
      "citing_paper_id": "237142365",
      "cited_paper_id": 119105448
    },
    {
      "context_text": "[6] captures full 6 DOF motions and precise ground truth by a MoCap system indoors.",
      "catation_intent": "reusable resource",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific dataset name, only a method or system for capturing motion. The MoCap system is described as a tool, not a dataset.",
      "processing_time": 29.1297767162323,
      "citing_paper_id": "237142365",
      "cited_paper_id": 9865213
    },
    {
      "context_text": "[5] provide a dataset which focuses on evaluation of visual navigation tasks.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "A Dataset for Visual Navigation with Neuromorphic Methods"
      ],
      "dataset_descriptions": {
        "A Dataset for Visual Navigation with Neuromorphic Methods": "Used to evaluate visual navigation tasks, focusing on neuromorphic methods and their application in dynamic environments."
      },
      "confidence_score": 0.8,
      "reasoning": "The context mentions a dataset focused on visual navigation tasks, which is relevant to event-based stereo depth estimation.",
      "processing_time": 32.9974684715271,
      "citing_paper_id": "237142365",
      "cited_paper_id": 17272393
    },
    {
      "context_text": "To perform this joint calibration, we modified the calibration tools from Basalt [17].",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method or tool called Basalt. There are no verifiable resources that meet the criteria for inclusion.",
      "processing_time": 29.241902828216553,
      "citing_paper_id": "237142365",
      "cited_paper_id": 119105448
    },
    {
      "context_text": "However, our evaluation also shows that Basalt and VINSFusion result in large drift for most of the challenging sequences, e.g. with fast motion in running-easy, runninghard, skate-hard, slide, and bike-hard or with low light, e.g. in floor2-dark and bike-dark.",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only challenging sequences which are not clearly identified as datasets.",
      "processing_time": 26.227848529815674,
      "citing_paper_id": "237142365",
      "cited_paper_id": 119105448
    },
    {
      "context_text": "Their main benefits are very high dynamic range (up to 140dB compared to 60dB of traditional cameras), high temporal resolution and low latency (in the order of microseconds), low power consumption and strongly reduced motion blur [1, 2].",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only characteristics of event cameras. No dataset names are present in the citation span.",
      "processing_time": 28.37217354774475,
      "citing_paper_id": "237142365",
      "cited_paper_id": 219687018
    },
    {
      "context_text": "Inspired by [16], we use time surfaces which take the latest as well as the next future event at time ttarget into account.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method or concept (time surfaces) used in the research.",
      "processing_time": 26.939337491989136,
      "citing_paper_id": "237142365",
      "cited_paper_id": 220870707
    },
    {
      "context_text": "[11] present the dataset ViViD, which contains sequences for visual navigation in poor illumination conditions.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "ViViD"
      ],
      "dataset_descriptions": {
        "ViViD": "Used to evaluate visual navigation algorithms under poor illumination conditions, focusing on the robustness of event-based stereo depth estimation systems."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions a specific dataset, ViViD, which is used for visual navigation in poor illumination conditions. This is relevant to event-based stereo depth estimation.",
      "processing_time": 33.18744683265686,
      "citing_paper_id": "237142365",
      "cited_paper_id": 221081555
    },
    {
      "context_text": "Stereo matching methods, either consider global objective optimization such as belief propagation [37], and graph-cuts [22]; or local correspondence such as the adaptive support-weight approach [42], and cost-volume filtering [16].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context discusses various stereo matching methods but does not mention any specific datasets. The cited papers are methods or algorithms, not datasets.",
      "processing_time": 28.977361917495728,
      "citing_paper_id": "244306440",
      "cited_paper_id": 544261
    },
    {
      "context_text": "Stereo matching methods, either consider global objective optimization such as belief propagation [37], and graph-cuts [22]; or local correspondence such as the adaptive support-weight approach [42], and cost-volume filtering [16].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context discusses various stereo matching methods but does not mention any specific datasets. The cited papers are methods or algorithms, not datasets.",
      "processing_time": 28.977361917495728,
      "citing_paper_id": "244306440",
      "cited_paper_id": 1680724
    },
    {
      "context_text": "Stereo matching methods, either consider global objective optimization such as belief propagation [37], and graph-cuts [22]; or local correspondence such as the adaptive support-weight approach [42], and cost-volume filtering [16].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context discusses various stereo matching methods but does not mention any specific datasets. The cited papers are methods or algorithms, not datasets.",
      "processing_time": 28.977361917495728,
      "citing_paper_id": "244306440",
      "cited_paper_id": 2457778
    },
    {
      "context_text": "Stereo matching methods, either consider global objective optimization such as belief propagation [37], and graph-cuts [22]; or local correspondence such as the adaptive support-weight approach [42], and cost-volume filtering [16].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context discusses various stereo matching methods but does not mention any specific datasets. The cited papers are methods or algorithms, not datasets.",
      "processing_time": 28.977361917495728,
      "citing_paper_id": "244306440",
      "cited_paper_id": 7495827
    },
    {
      "context_text": "Learning-based approaches perform stereo matching, cost aggregation, disparity computation and refinements in an end-to-end fashion without handcrafted parameters by 3D convolutions [25, 20, 4].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only general methods and approaches. No dataset names are present in the text.",
      "processing_time": 28.598078727722168,
      "citing_paper_id": "244306440",
      "cited_paper_id": 2658860
    },
    {
      "context_text": "Learning-based approaches perform stereo matching, cost aggregation, disparity computation and refinements in an end-to-end fashion without handcrafted parameters by 3D convolutions [25, 20, 4].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only general methods and approaches. No dataset names are present in the text.",
      "processing_time": 28.598078727722168,
      "citing_paper_id": "244306440",
      "cited_paper_id": 102352684
    },
    {
      "context_text": "We utilize soft argmin disparity estimation [20], to regress the per-pixel disparity from the final cost volume.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for disparity estimation.",
      "processing_time": 26.204586505889893,
      "citing_paper_id": "244306440",
      "cited_paper_id": 2658860
    },
    {
      "context_text": "Recent methods estimate depth using learning-based frameworks without relying on hand-crafted parameters, and can also estimate metric depth based on the prior knowledge of the network [4, 5, 20, 41], thanks to the modern GPUs, creative architectures, and public-available large scale datasets.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.3,
      "reasoning": "The context mentions 'public-available large scale datasets' but does not specify any particular dataset names. The cited papers do not provide additional specific dataset names either.",
      "processing_time": 29.56563639640808,
      "citing_paper_id": "244306440",
      "cited_paper_id": 2658860
    },
    {
      "context_text": "Recent methods estimate depth using learning-based frameworks without relying on hand-crafted parameters, and can also estimate metric depth based on the prior knowledge of the network [4, 5, 20, 41], thanks to the modern GPUs, creative architectures, and public-available large scale datasets.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.3,
      "reasoning": "The context mentions 'public-available large scale datasets' but does not specify any particular dataset names. The cited papers do not provide additional specific dataset names either.",
      "processing_time": 29.56563639640808,
      "citing_paper_id": "244306440",
      "cited_paper_id": 4252896
    },
    {
      "context_text": "Recent methods estimate depth using learning-based frameworks without relying on hand-crafted parameters, and can also estimate metric depth based on the prior knowledge of the network [4, 5, 20, 41], thanks to the modern GPUs, creative architectures, and public-available large scale datasets.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.3,
      "reasoning": "The context mentions 'public-available large scale datasets' but does not specify any particular dataset names. The cited papers do not provide additional specific dataset names either.",
      "processing_time": 29.56563639640808,
      "citing_paper_id": "244306440",
      "cited_paper_id": 102352684
    },
    {
      "context_text": "The event-intensity combination is studied previously in feature detection and tracking for visual SLAM [39], event to image reconstruction by high-pass filtering [34], super-resolution from a sequence of event stacks and intensity image [27, 17] and image enhancement by unifying the low dynamic range image with an event map [14].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets by name. It discusses various methods and approaches but does not refer to any named, verifiable datasets.",
      "processing_time": 29.3209490776062,
      "citing_paper_id": "244306440",
      "cited_paper_id": 3738244
    },
    {
      "context_text": "The event-intensity combination is studied previously in feature detection and tracking for visual SLAM [39], event to image reconstruction by high-pass filtering [34], super-resolution from a sequence of event stacks and intensity image [27, 17] and image enhancement by unifying the low dynamic range image with an event map [14].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets by name. It discusses various methods and approaches but does not refer to any named, verifiable datasets.",
      "processing_time": 29.3209490776062,
      "citing_paper_id": "244306440",
      "cited_paper_id": 208547669
    },
    {
      "context_text": "Our feature extractor follows the Res-Net architecture [15], and we utilize feature pyramids [5] to create our cost volumes through feature correlation [7] instead of concatenation.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and architectures.",
      "processing_time": 25.99511170387268,
      "citing_paper_id": "244306440",
      "cited_paper_id": 4252896
    },
    {
      "context_text": "Although further accuracy can be reached by utilizing more 3D convolutions in a pyramid design [5], recent papers have focused on increasing the accuracy while consuming less memory.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method or approach. The context focuses on the design and efficiency of a stereo matching network.",
      "processing_time": 29.198535680770874,
      "citing_paper_id": "244306440",
      "cited_paper_id": 4252896
    },
    {
      "context_text": "Semidense depth by incorporating the camera velocity for event synchronization was proposed in [48].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for semidense depth estimation using event synchronization.",
      "processing_time": 27.220262050628662,
      "citing_paper_id": "244306440",
      "cited_paper_id": 4412139
    },
    {
      "context_text": "Later methods improve the accuracy by incorporating orientation sensitive filters [3], cooperative regularization [29, 11], and spiking neural networks [28, 6, 1].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only methods and approaches. The cited papers' titles suggest a focus on methods rather than datasets.",
      "processing_time": 29.308310985565186,
      "citing_paper_id": "244306440",
      "cited_paper_id": 4833834
    },
    {
      "context_text": "Later methods improve the accuracy by incorporating orientation sensitive filters [3], cooperative regularization [29, 11], and spiking neural networks [28, 6, 1].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only methods and approaches. The cited papers' titles suggest a focus on methods rather than datasets.",
      "processing_time": 29.308310985565186,
      "citing_paper_id": "244306440",
      "cited_paper_id": 12047627
    },
    {
      "context_text": "Later methods improve the accuracy by incorporating orientation sensitive filters [3], cooperative regularization [29, 11], and spiking neural networks [28, 6, 1].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only methods and approaches. The cited papers' titles suggest a focus on methods rather than datasets.",
      "processing_time": 29.308310985565186,
      "citing_paper_id": "244306440",
      "cited_paper_id": 205698386
    },
    {
      "context_text": "Thus, we utilize deformable convolutions as they aim to go further than the fixed geometric structures of ordinary convolutional networks, and learn dense spatial transformations with additional offsets [19].",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (deformable convolutions).",
      "processing_time": 27.417046785354614,
      "citing_paper_id": "244306440",
      "cited_paper_id": 7428689
    },
    {
      "context_text": "The less computationally expensive methods use deformable convolutions [19] as guided or adaptive aggregating layers [43, 41].",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and techniques. There are no verifiable resources that meet the criteria for inclusion.",
      "processing_time": 28.944945335388184,
      "citing_paper_id": "244306440",
      "cited_paper_id": 7428689
    },
    {
      "context_text": "Early attempts utilized the low latency and power consumption of event cameras to perform fast and efficient stereo matching [21, 32]; where the matched events followed a triangulation stage in 3D to estimate the depth.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and approaches. The context focuses on the use of event cameras and stereo matching techniques.",
      "processing_time": 28.56661605834961,
      "citing_paper_id": "244306440",
      "cited_paper_id": 11177597
    },
    {
      "context_text": "Reaching a faster and fully asynchronous design using spiking neural networks [13], remains as our future direction.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a future direction involving spiking neural networks.",
      "processing_time": 27.405543088912964,
      "citing_paper_id": "244306440",
      "cited_paper_id": 11763153
    },
    {
      "context_text": "Depth estimation without explicit event matching was introduced in [46].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for depth estimation using stereo event cameras.",
      "processing_time": 26.879281282424927,
      "citing_paper_id": "244306440",
      "cited_paper_id": 49877954
    },
    {
      "context_text": "Our last stage utilizes stereo depth refinement modules [4] to upsample lower scales to higher intermediate scales and then to our final output scale in our framework.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method or module for stereo depth refinement.",
      "processing_time": 26.45931386947632,
      "citing_paper_id": "244306440",
      "cited_paper_id": 102352684
    },
    {
      "context_text": "Where E is the epoch number in which we stop using the image reconstruction loss and only continue training with the EPE, and λ1 and λ2 are the weighting factors.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only training parameters and loss functions.",
      "processing_time": 26.894832849502563,
      "citing_paper_id": "244306440",
      "cited_paper_id": 102496818
    },
    {
      "context_text": "However, as our final intention is to estimate depth we do not actually need our network to reconstruct images, therefore, we stop utilizing the image reconstruction loss mid way and only use the end-point-error (EPE) as or main loss.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only discusses loss functions and network training aspects.",
      "processing_time": 27.38906764984131,
      "citing_paper_id": "244306440",
      "cited_paper_id": 102496818
    },
    {
      "context_text": "LEPE(dv, d̂v) = 1\nV V∑ v=0 |dv − d̂v| (1)\nImage reconstruction loss.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only a loss function and image reconstruction loss. No verifiable resources are identified.",
      "processing_time": 28.909735441207886,
      "citing_paper_id": "244306440",
      "cited_paper_id": 102496818
    },
    {
      "context_text": "We use the L1 loss also known as the EPE [10], i.e., the mean disparity error in pixels between the ground-truth (GT) disparity dv and estimated disparity of our model d̂v for pixel v among the V valid pixels of the depth.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific dataset names, only a metric (EPE) which is excluded according to the rules.",
      "processing_time": 28.785477876663208,
      "citing_paper_id": "244306440",
      "cited_paper_id": 102496818
    },
    {
      "context_text": "We use the L1 loss also known as the EPE [10], i.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a loss function (L1 loss/EPE).",
      "processing_time": 27.865243196487427,
      "citing_paper_id": "244306440",
      "cited_paper_id": 102496818
    },
    {
      "context_text": "Further refinement modules [10] can be leveraged to refine the estimated depth using prior learned knowledge or from new incoming images.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods or approaches. There are no clear identifiers for datasets in the provided context.",
      "processing_time": 29.142879247665405,
      "citing_paper_id": "244306440",
      "cited_paper_id": 102496818
    },
    {
      "context_text": "L= { LEPE + λ1LLPIPS + λ2LL1 , epoch < E LEPE , epoch ≥ E\n(2)",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only loss functions and training epochs. No verifiable resources are identified.",
      "processing_time": 28.778830528259277,
      "citing_paper_id": "244306440",
      "cited_paper_id": 102496818
    },
    {
      "context_text": "For LPIPS, we use the AlexNet variant [23] following [27, 36, 17].",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models and methods. The context is about using the AlexNet variant for LPIPS, which is not a dataset.",
      "processing_time": 29.94534206390381,
      "citing_paper_id": "244306440",
      "cited_paper_id": 195908774
    },
    {
      "context_text": "For LPIPS, we use the AlexNet variant [23] following [27, 36, 17].",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models and methods. The context is about using the AlexNet variant for LPIPS, which is not a dataset.",
      "processing_time": 29.94534206390381,
      "citing_paper_id": "244306440",
      "cited_paper_id": 208547669
    },
    {
      "context_text": "Our sequential design for unifying the event stacks and intensity images, the recycling network, is adopted from e2sri [27, 17].",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (recycling network) adopted from another paper.",
      "processing_time": 27.361572265625,
      "citing_paper_id": "244306440",
      "cited_paper_id": 208547669
    },
    {
      "context_text": "nature of the problem, occlusions, imperfect imaging settings, blurred or low dynamic range images, repetitive patterns, and texture-less regions [24].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general challenges in stereo depth estimation.",
      "processing_time": 27.593701124191284,
      "citing_paper_id": "244306440",
      "cited_paper_id": 219303641
    },
    {
      "context_text": "Estimating the depth from two or more views is one of the long-persisting topics that is tackled in many ways [24].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general topic of stereo-based depth estimation.",
      "processing_time": 27.147979259490967,
      "citing_paper_id": "244306440",
      "cited_paper_id": 219303641
    },
    {
      "context_text": "In spite of the significant progress, poor lighting conditions and complex materials properties are issues that have been less studied [24].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only general issues in the field.",
      "processing_time": 27.357593774795532,
      "citing_paper_id": "244306440",
      "cited_paper_id": 219303641
    },
    {
      "context_text": "Although we utilized ESIM and CARLA separately, a recent plugin [18] includes event simulation in CARLA.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only tools and methods. The context is about integrating event simulation into the CARLA simulator, which is not a dataset.",
      "processing_time": 29.58083176612854,
      "citing_paper_id": "244306440",
      "cited_paper_id": 223957202
    },
    {
      "context_text": "As discussed in [1], local event information is inherently ambiguous.",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general statement about local event information ambiguity.",
      "processing_time": 27.13687014579773,
      "citing_paper_id": "226298400",
      "cited_paper_id": 1151030
    },
    {
      "context_text": "The different methods often adapt principles known from frame-based vision, namely correlation [4], [16], gradient [3] and local frequency estimation [19].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and principles from frame-based vision. No verifiable resources are identified.",
      "processing_time": 29.2217378616333,
      "citing_paper_id": "226298400",
      "cited_paper_id": 2070927
    },
    {
      "context_text": "We compare our results against two state-of-the-art optical flow methods for eventbased cameras: EV − FlowNet [27] and a recent stereo method [28] (in the tables - Zhu18).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions comparing results against state-of-the-art methods but does not specify any datasets. The cited papers are methods, not datasets.",
      "processing_time": 29.457528829574585,
      "citing_paper_id": "226298400",
      "cited_paper_id": 3396150
    },
    {
      "context_text": "We compare our results against two state-of-the-art optical flow methods for eventbased cameras: EV − FlowNet [27] and a recent stereo method [28] (in the tables - Zhu18).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions comparing results against state-of-the-art methods but does not specify any datasets. The cited papers are methods, not datasets.",
      "processing_time": 29.457528829574585,
      "citing_paper_id": "226298400",
      "cited_paper_id": 56475917
    },
    {
      "context_text": "We compare our results against two state-of-the-art optical ﬂow methods for event-based cameras: EV − FlowNet [27] and a recent stereo method [28] (in the tables - Zhu 18).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and comparisons. No verifiable datasets are referenced.",
      "processing_time": 28.861023426055908,
      "citing_paper_id": "226298400",
      "cited_paper_id": 3396150
    },
    {
      "context_text": "To compare against [27] and [28], we account for the difference in the frame rates (for example, EV-FlowNet uses the frame rate of the DAVIS classical frames) by scaling our optical ﬂow.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.3,
      "reasoning": "The context mentions 'DAVIS classical frames' which could imply a dataset, but it is not clear if it is a specific, verifiable dataset. No other datasets are mentioned.",
      "processing_time": 30.37288188934326,
      "citing_paper_id": "226298400",
      "cited_paper_id": 3396150
    },
    {
      "context_text": "[27] released the MVSEC dataset [25] and proposed self-supervised learning algorithm to estimate optical flow.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "MVSEC"
      ],
      "dataset_descriptions": {
        "MVSEC": "Used to evaluate self-supervised learning algorithms for optical flow estimation in event-based cameras, focusing on the performance and accuracy of the proposed method."
      },
      "confidence_score": 1.0,
      "reasoning": "The citation mentions the MVSEC dataset, which is a specific, verifiable dataset used in the context of event-based cameras and optical flow estimation.",
      "processing_time": 33.866018772125244,
      "citing_paper_id": "226298400",
      "cited_paper_id": 3396150
    },
    {
      "context_text": "To compare against [27] and [28], we account for the difference in the frame rates (for example, EV-FlowNet uses the frame rate of the DAVIS classical frames) by scaling our optical flow.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.3,
      "reasoning": "The context mentions 'DAVIS classical frames' which is likely part of a dataset or a specific data source used for event-based cameras. However, it does not mention a specific dataset name. The context is focused on methodological details rather than a specific dataset.",
      "processing_time": 32.30588150024414,
      "citing_paper_id": "226298400",
      "cited_paper_id": 3396150
    },
    {
      "context_text": "To compare against [27] and [28], we account for the difference in the frame rates (for example, EV-FlowNet uses the frame rate of the DAVIS classical frames) by scaling our optical flow.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.3,
      "reasoning": "The context mentions 'DAVIS classical frames' which is likely part of a dataset or a specific data source used for event-based cameras. However, it does not mention a specific dataset name. The context is focused on methodological details rather than a specific dataset.",
      "processing_time": 32.30588150024414,
      "citing_paper_id": "226298400",
      "cited_paper_id": 56475917
    },
    {
      "context_text": "Similar to KITTI and EV-FlowNet, we report the percentage of outliers - values with error more than 3 pixels or 5% of the ﬂow vector magnitude.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "KITTI"
      ],
      "dataset_descriptions": {
        "KITTI": "Used to report the percentage of outliers in depth estimation, focusing on error metrics for event-based stereo depth estimation."
      },
      "confidence_score": 0.7,
      "reasoning": "The citation mentions KITTI and EV-FlowNet, but does not specify their use as datasets. KITTI is known as a dataset, but it is not explicitly stated here. EV-FlowNet is a method, not a dataset.",
      "processing_time": 37.11853647232056,
      "citing_paper_id": "226298400",
      "cited_paper_id": 3396150
    },
    {
      "context_text": "Unlike [27], which uses grayscale information as a supervision signal, our proposed framework uses only events and thus can work in challenging lighting conditions.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (grayscale information as a supervision signal) used in a different approach.",
      "processing_time": 29.3091037273407,
      "citing_paper_id": "226298400",
      "cited_paper_id": 3396150
    },
    {
      "context_text": "Even though various normalization strategies have been proposed to train neural networks [12], [20], [15], it is important to point out, scaling the data does not guarantee good conditioning.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only normalization strategies for training neural networks.",
      "processing_time": 28.837888479232788,
      "citing_paper_id": "226298400",
      "cited_paper_id": 4076251
    },
    {
      "context_text": "The majority of event-based depth estimation methods use two or more event cameras [26], [24], [28].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and approaches. No dataset names are present in the citation span.",
      "processing_time": 29.064791440963745,
      "citing_paper_id": "226298400",
      "cited_paper_id": 4412139
    },
    {
      "context_text": "The majority of event-based depth estimation methods use two or more event cameras [26], [24], [28].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and approaches. No dataset names are present in the citation span.",
      "processing_time": 29.064791440963745,
      "citing_paper_id": "226298400",
      "cited_paper_id": 56475917
    },
    {
      "context_text": "are reported in degrees and radians respectively [28], AEE is in m/s.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only units of measurement. No dataset names are present in the citation context.",
      "processing_time": 29.066771030426025,
      "citing_paper_id": "226298400",
      "cited_paper_id": 56475917
    },
    {
      "context_text": "More complicated event binning schemes have been used in prior work [28], but we find our slice images faster to",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method or approach. The context is too limited to identify a verifiable dataset.",
      "processing_time": 29.76214623451233,
      "citing_paper_id": "226298400",
      "cited_paper_id": 56475917
    },
    {
      "context_text": "We provide comparison to the work in [28], although it uses a stereo setup and reports results only on the outdoor day 1 sequence.",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific dataset names, only a general reference to a stereo setup and an outdoor day 1 sequence. No clear, verifiable dataset name is provided.",
      "processing_time": 30.788597345352173,
      "citing_paper_id": "226298400",
      "cited_paper_id": 56475917
    },
    {
      "context_text": "To be consistent with [28], we report our trajectory estimation relative pose and relative rotation errors as RPE = arccos( tpred ·tgt ‖tpred‖2·‖tgt‖2 ) and RRE = ‖logm(R T predRgt)‖2.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only error metrics and methods. The context is focused on reporting evaluation metrics.",
      "processing_time": 29.525413513183594,
      "citing_paper_id": "226298400",
      "cited_paper_id": 56475917
    },
    {
      "context_text": "Unlike [28], we train SfMlearner on the event images, and not on the classical frames to allow for evaluation on the night sequences.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific dataset names, only a method (SfMlearner) and a type of data (event images).",
      "processing_time": 30.205972909927368,
      "citing_paper_id": "226298400",
      "cited_paper_id": 56475917
    },
    {
      "context_text": "Our design facilitates training because residual learning is conducted throughout the network for each level of features, while in comparison, the original ResNet [7] does that only in design blocks.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a comparison to the ResNet architecture. No verifiable resources are identified.",
      "processing_time": 30.325455904006958,
      "citing_paper_id": "226298400",
      "cited_paper_id": 206594692
    },
    {
      "context_text": "In this work we use eDVS4337 circuit as a light, compact and low power solution that is used in several robotic applications and anomaly detection [15,21–24].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a hardware component (eDVS4337 circuit) used in robotic applications and anomaly detection.",
      "processing_time": 31.093130111694336,
      "citing_paper_id": "4833834",
      "cited_paper_id": 3453940
    },
    {
      "context_text": "[18] have shown that by adding additional cameras to the stereo setup (Trinocular vision vs.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method involving trinocular vision. No clear, verifiable dataset names are present.",
      "processing_time": 30.54425048828125,
      "citing_paper_id": "4833834",
      "cited_paper_id": 10712214
    },
    {
      "context_text": "1c) as a stimulus to analyze the detection rate in an area-based, an event image-based, and a time-based algorithm [16].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only algorithms and approaches. No clear, verifiable dataset names are present.",
      "processing_time": 30.653234481811523,
      "citing_paper_id": "4833834",
      "cited_paper_id": 11177597
    },
    {
      "context_text": "Similar to the analysis performed in [16], and in order to analytically evaluate the detection rate, we have created the disparity histogram using both algorithms for the events generated within a time period of 5 s (Figs.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific, verifiable datasets. It only refers to creating a disparity histogram using algorithms, which is a methodological step rather than a dataset.",
      "processing_time": 31.287150144577026,
      "citing_paper_id": "4833834",
      "cited_paper_id": 11177597
    },
    {
      "context_text": "A range of detected disparity values within −1 and +1 of the ground truth value is considered as correct [16].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific dataset names, only a general reference to ground truth values. No multi-word proper nouns, acronyms, or hyphenated names with digits are present.",
      "processing_time": 32.36199450492859,
      "citing_paper_id": "4833834",
      "cited_paper_id": 11177597
    },
    {
      "context_text": "This behavior has been observed in previous works [16].",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general observation from previous works.",
      "processing_time": 30.181452989578247,
      "citing_paper_id": "4833834",
      "cited_paper_id": 11177597
    },
    {
      "context_text": "The main line of research in event-based stereo matching using DVS is focused on temporal matching [16,17].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only research approaches and methods. The cited papers' titles also do not indicate the use of specific datasets.",
      "processing_time": 32.2398521900177,
      "citing_paper_id": "4833834",
      "cited_paper_id": 11177597
    },
    {
      "context_text": "The main line of research in event-based stereo matching using DVS is focused on temporal matching [16,17].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only research approaches and methods. The cited papers' titles also do not indicate the use of specific datasets.",
      "processing_time": 32.2398521900177,
      "citing_paper_id": "4833834",
      "cited_paper_id": 17693733
    },
    {
      "context_text": "disparity with respect to the ground truth and is used as a performance criteria in the previous works [16].",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific dataset names, only a general reference to 'ground truth'. No multi-word proper nouns, acronyms, or hyphenated names with digits are present.",
      "processing_time": 33.430330991744995,
      "citing_paper_id": "4833834",
      "cited_paper_id": 11177597
    },
    {
      "context_text": "These results show the advantages of the cooperative approach compared to the purely time-based event matching, in which the best average detection rate for a simple stimulus does not exceed 30 % [16].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a comparison of methods. No dataset names are present in the citation span.",
      "processing_time": 31.887459754943848,
      "citing_paper_id": "4833834",
      "cited_paper_id": 11177597
    },
    {
      "context_text": "[16] proposed a purely event-driven matching using temporal correlation and polarity correlation of the events.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and approaches. The context is focused on describing a method for event-based stereo matching.",
      "processing_time": 32.34247088432312,
      "citing_paper_id": "4833834",
      "cited_paper_id": 11177597
    },
    {
      "context_text": "In 1989 Mahowald and Delbruck developed a micro circuit which was the first hardware implementation of Marr’s cooperative network [6].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a hardware implementation of a theoretical model.",
      "processing_time": 30.943121910095215,
      "citing_paper_id": "4833834",
      "cited_paper_id": 15077875
    },
    {
      "context_text": "have not quantitatively analyzed the performance of the algorithm in [17], we have replicated this algorithm.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only an algorithm from another paper.",
      "processing_time": 31.142374753952026,
      "citing_paper_id": "4833834",
      "cited_paper_id": 17693733
    },
    {
      "context_text": "showed the coherency of the detected disparity in depth [17].",
      "catation_intent": "findings",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific dataset names or resources. It only refers to the coherency of detected disparity in depth, which is a methodological outcome.",
      "processing_time": 33.06727957725525,
      "citing_paper_id": "4833834",
      "cited_paper_id": 17693733
    },
    {
      "context_text": "To show the performance of the algorithm for temporally-overlapping stimuli, two simultaneously moving pens are used but the accuracy of the algorithm is not analytically reported [17].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method or experiment setup involving two moving pens.",
      "processing_time": 31.364240884780884,
      "citing_paper_id": "4833834",
      "cited_paper_id": 17693733
    },
    {
      "context_text": "Bottom extracted disparity maps and disparity histogram using algorithm in [17]",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific dataset names, only an algorithm. No verifiable datasets are referenced.",
      "processing_time": 31.974472284317017,
      "citing_paper_id": "4833834",
      "cited_paper_id": 17693733
    },
    {
      "context_text": "Bottom extracted disparitymaps and disparity histogram using algorithm in [17]",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific dataset names, only an algorithm. No verifiable datasets are referenced.",
      "processing_time": 32.08456802368164,
      "citing_paper_id": "4833834",
      "cited_paper_id": 17693733
    },
    {
      "context_text": "Time (sec) D et ec tio n ra te Cooperative Network Epipolrar and Ordering Constraint [17] Average detection rate within 10sec",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method or approach. The context is too limited to identify a verifiable dataset.",
      "processing_time": 33.05150127410889,
      "citing_paper_id": "4833834",
      "cited_paper_id": 17693733
    },
    {
      "context_text": "[17] combined epipolar constraint with temporal matching and ordering constraints to eliminate mismatched events",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and approaches. The context focuses on combining epipolar constraint with temporal matching and ordering constraints.",
      "processing_time": 33.50918221473694,
      "citing_paper_id": "4833834",
      "cited_paper_id": 17693733
    },
    {
      "context_text": "Due to intrinsic jitter delay and latency in a pixel’s response which varies pixel by pixel [17], temporal coincidence alone is not reliable enough for event matching especially when the stimuli generate temporally-overlapping stream of events (i.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only discusses challenges in event-based stereo matching.",
      "processing_time": 31.847981929779053,
      "citing_paper_id": "4833834",
      "cited_paper_id": 17693733
    },
    {
      "context_text": "Although the algorithm proposed in [17] is able to extract the disparity maps associated with the depths, the performance of this algorithm drops when the disparity is increased or equivalently stimuli come closer (compare Figs.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only an algorithm's performance in extracting disparity maps.",
      "processing_time": 32.414363861083984,
      "citing_paper_id": "4833834",
      "cited_paper_id": 17693733
    },
    {
      "context_text": "In spite ofmany unanswered questions about neurophysiologicalmechanisms of the disparity detection, nowadays it is widely accepted thatmammalian brains utilize a competitive process over disparity sensitive populations of neurons, to encode and detect horizontal disparity [25].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a reference to neurophysiological mechanisms and models. No verifiable resources are identified.",
      "processing_time": 33.50020503997803,
      "citing_paper_id": "4833834",
      "cited_paper_id": 38166972
    },
    {
      "context_text": "The first attempt to answer this question was performed by David Marr who proposed a laminar network of sharply tuned disparity detector neurons, called cooperative network, to algorithmically model basic principles of the disparity detection mechanism of the brain [5].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a theoretical model proposed by David Marr.",
      "processing_time": 32.70793318748474,
      "citing_paper_id": "4833834",
      "cited_paper_id": 267799236
    },
    {
      "context_text": "The main idea of our algorithm is borrowed from David Marr’s cooperative computing approach [5].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a conceptual approach from David Marr’s work.",
      "processing_time": 32.917625427246094,
      "citing_paper_id": "4833834",
      "cited_paper_id": 267799236
    },
    {
      "context_text": "…event descriptors, e.g., using ﬁxed accumulation techniques to produce an event image by counting the number of positive and negative events [4], [15], [16], using known camera motions [4], discretizing event sequence [17], [18], [44], and combining event voxels and intensity images [19], [20].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only methods and techniques for processing event data. The cited papers' titles also do not indicate the use of specific datasets.",
      "processing_time": 34.91950702667236,
      "citing_paper_id": "250374739",
      "cited_paper_id": 185541
    },
    {
      "context_text": "…event descriptors, e.g., using ﬁxed accumulation techniques to produce an event image by counting the number of positive and negative events [4], [15], [16], using known camera motions [4], discretizing event sequence [17], [18], [44], and combining event voxels and intensity images [19], [20].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only methods and techniques for processing event data. The cited papers' titles also do not indicate the use of specific datasets.",
      "processing_time": 34.91950702667236,
      "citing_paper_id": "250374739",
      "cited_paper_id": 11177597
    },
    {
      "context_text": "…event descriptors, e.g., using ﬁxed accumulation techniques to produce an event image by counting the number of positive and negative events [4], [15], [16], using known camera motions [4], discretizing event sequence [17], [18], [44], and combining event voxels and intensity images [19], [20].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only methods and techniques for processing event data. The cited papers' titles also do not indicate the use of specific datasets.",
      "processing_time": 34.91950702667236,
      "citing_paper_id": "250374739",
      "cited_paper_id": 231951439
    },
    {
      "context_text": "Some global methods are based on the semi-global matching [3], the belief propagation on a Markov Random Field [5], [33], [34], and the cooperative stereo algorithm [35]–[38] to improve stereo matching.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only methods and algorithms. The cited papers' titles do not provide additional dataset information.",
      "processing_time": 34.17580604553223,
      "citing_paper_id": "250374739",
      "cited_paper_id": 767650
    },
    {
      "context_text": "Some global methods are based on the semi-global matching [3], the belief propagation on a Markov Random Field [5], [33], [34], and the cooperative stereo algorithm [35]–[38] to improve stereo matching.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only methods and algorithms. The cited papers' titles do not provide additional dataset information.",
      "processing_time": 34.17580604553223,
      "citing_paper_id": "250374739",
      "cited_paper_id": 49877954
    },
    {
      "context_text": "Some global methods are based on the semi-global matching [3], the belief propagation on a Markov Random Field [5], [33], [34], and the cooperative stereo algorithm [35]–[38] to improve stereo matching.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only methods and algorithms. The cited papers' titles do not provide additional dataset information.",
      "processing_time": 34.17580604553223,
      "citing_paper_id": "250374739",
      "cited_paper_id": 65040501
    },
    {
      "context_text": "Specifically, for non-learning-based methods, we compared with FCVF* [3], [63], SGM* [3], [64], TSES [4], and CopNet [65].",
      "catation_intent": "none",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods and models. No dataset names are present in the text.",
      "processing_time": 33.8371946811676,
      "citing_paper_id": "250374739",
      "cited_paper_id": 1680724
    },
    {
      "context_text": "For a rectiﬁed stereo image pair, right image pixels can be translated using disparity values along each epipolar line to construct a warped left intensity image [56].",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for disparity estimation in stereo images.",
      "processing_time": 32.90198850631714,
      "citing_paper_id": "250374739",
      "cited_paper_id": 1695579
    },
    {
      "context_text": "The proposed method has been evaluated using the publicly available multi-vehicle stereo event camera (MVSEC) dataset [22] and DSEC dataset [60].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "MVSEC",
        "DSEC"
      ],
      "dataset_descriptions": {
        "MVSEC": "Used to evaluate the proposed method for event-based stereo depth estimation, focusing on 3D perception tasks with event camera data.",
        "DSEC": "Used to evaluate the proposed method for event-based stereo depth estimation, providing additional validation with diverse event camera data."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions two specific datasets, MVSEC and DSEC, which are used for evaluating the proposed method in the context of event-based stereo depth estimation.",
      "processing_time": 47.2553653717041,
      "citing_paper_id": "250374739",
      "cited_paper_id": 3416874
    },
    {
      "context_text": "We utilize the publicly available Multi Vehicle Stereo Event Camera (MVSEC) dataset [22] and DSEC dataset [60] to train, validate, and test the efﬁcacy of the proposed unsupervised event stereo method.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Multi Vehicle Stereo Event Camera (MVSEC) dataset",
        "DSEC dataset"
      ],
      "dataset_descriptions": {
        "Multi Vehicle Stereo Event Camera (MVSEC) dataset": "Used to train, validate, and test the proposed unsupervised event stereo method, focusing on 3D perception tasks using event camera data.",
        "DSEC dataset": "Used to train, validate, and test the proposed unsupervised event stereo method, focusing on 3D perception tasks using dynamic and event-based camera data."
      },
      "confidence_score": 1.0,
      "reasoning": "The context explicitly mentions two datasets, MVSEC and DSEC, which are used for training, validation, and testing an unsupervised event stereo method.",
      "processing_time": 52.79144048690796,
      "citing_paper_id": "250374739",
      "cited_paper_id": 3416874
    },
    {
      "context_text": "First, recent studies [4], [8], [15]–[20] have utilized different embedding mechanisms to aggregate event sequences for extracting meaningful event descriptors in the stereo event matching.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions 'event sequences' and 'stereo event matching', which are relevant to event-based stereo depth estimation. However, no specific dataset names are mentioned.",
      "processing_time": 35.83017897605896,
      "citing_paper_id": "250374739",
      "cited_paper_id": 11177597
    },
    {
      "context_text": "First, recent studies [4], [8], [15]–[20] have utilized different embedding mechanisms to aggregate event sequences for extracting meaningful event descriptors in the stereo event matching.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions 'event sequences' and 'stereo event matching', which are relevant to event-based stereo depth estimation. However, no specific dataset names are mentioned.",
      "processing_time": 35.83017897605896,
      "citing_paper_id": "250374739",
      "cited_paper_id": 262638843
    },
    {
      "context_text": "…on event frames [23], [24] or time surfaces [25], by exploiting simultaneity and temporal correlations of the events across sensors [15], [25], [26], by comparing local context descriptors [27], [28] of the spatial distribution of events, etc.) Since these methods match events by…",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods and approaches. The context is focused on describing various techniques for event-based stereo matching.",
      "processing_time": 35.094382524490356,
      "citing_paper_id": "250374739",
      "cited_paper_id": 11177597
    },
    {
      "context_text": "The predicted disparity maps are then used for the unsupervised loss calculation that utilizes the multi-scale disparity maps, as the multi-scale losses are proven to be beneﬁcial for unsupervised training (see the ablation study section) [12].",
      "catation_intent": "findings",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific dataset names. It refers to a method or finding about the benefits of multi-scale losses in unsupervised training.",
      "processing_time": 35.477569580078125,
      "citing_paper_id": "250374739",
      "cited_paper_id": 11977588
    },
    {
      "context_text": "Note that the disparity smoothness loss is divided by j − 1 to allow gradients to be derived from larger spatial regions directly, as shown effective in previous studies [12], [59].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only references to previous studies. No dataset names are provided in the context.",
      "processing_time": 34.01999545097351,
      "citing_paper_id": "250374739",
      "cited_paper_id": 11977588
    },
    {
      "context_text": "Then, following [10]–[12], [14] the local appearance matching loss L la consists of a mean absolute difference term and a structural similarity index (SSIM) loss term, which can be expressed as ), (14) where K denotes the number of pixels.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only loss functions and their components. No verifiable resources are identified.",
      "processing_time": 34.57175302505493,
      "citing_paper_id": "250374739",
      "cited_paper_id": 11977588
    },
    {
      "context_text": "Then, following [10]–[12], [14] the local appearance matching loss L la consists of a mean absolute difference term and a structural similarity index (SSIM) loss term, which can be expressed as ), (14) where K denotes the number of pixels.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only loss functions and their components. No verifiable resources are identified.",
      "processing_time": 34.57175302505493,
      "citing_paper_id": "250374739",
      "cited_paper_id": 14517241
    },
    {
      "context_text": "This is because the multi-scale loss acts as a coarse-to-reﬁne approach that allows gradients to be derived from larger spatial regions [12].",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a methodological approach. The context is too limited to infer the use of any particular dataset.",
      "processing_time": 34.71700739860535,
      "citing_paper_id": "250374739",
      "cited_paper_id": 11977588
    },
    {
      "context_text": "…on event frames [23], [24] or time surfaces [25], by exploiting simultaneity and temporal correlations of the events across sensors [15], [25], [26], by comparing local context descriptors [27], [28] of the spatial distribution of events, etc.) Since these methods match events by comparing…",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods and approaches. There are no clear identifiers for datasets in the text.",
      "processing_time": 35.07897353172302,
      "citing_paper_id": "250374739",
      "cited_paper_id": 12880482
    },
    {
      "context_text": "Speciﬁcally, to get the low-level image features, we use the ﬁrst ﬁve layers of VGG16 [58] as the pre-trained features extractor, which outputs two features Fig.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a pre-trained model (VGG16) used for feature extraction. No verifiable datasets are referenced.",
      "processing_time": 36.265183448791504,
      "citing_paper_id": "250374739",
      "cited_paper_id": 14124313
    },
    {
      "context_text": "…distribution of events, etc.) Since these methods match events by comparing their neighborhoods, they often impose additional constraints (e.g., epipolar constraint [29], ordering, uniqueness, edge orientation, and polarity, etc.) to reduce ambiguous and false event correspondence [30]–[32].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods and constraints used in event-based stereo depth estimation.",
      "processing_time": 34.55615758895874,
      "citing_paper_id": "250374739",
      "cited_paper_id": 14354416
    },
    {
      "context_text": "The existing frame-based methods consider the local appearance matching losses (e.g., mean absolute difference and structural similarity index measure (SSIM) [10]–[13], [21]) that seek to minimize the reconstruction loss between the warped and ground-truth intensity images.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and metrics. The context focuses on frame-based methods and their performance metrics.",
      "processing_time": 35.45240783691406,
      "citing_paper_id": "250374739",
      "cited_paper_id": 14517241
    },
    {
      "context_text": "Recent frame-based unsupervised stereo matching methods [10]–[14] have demonstrated that the unsupervised learning-based stereo methods can be trained by transforming the depth estimation problem as a warping-based reconstruction problem, i.e., by utilizing the predicted disparity maps to warp the…",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and approaches. The context is focused on describing the methodology of unsupervised stereo matching.",
      "processing_time": 36.04462504386902,
      "citing_paper_id": "250374739",
      "cited_paper_id": 14517241
    },
    {
      "context_text": "Stereo frame-based unsupervised methods generally consider the depth estimation problem as a view synthesis problem and proceed to solve it via minimizing the photo-metric warping loss (i.e., appearance matching loss) [10]–[14], [21], [45]–[48].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and approaches. The context focuses on stereo frame-based unsupervised methods and their approach to depth estimation.",
      "processing_time": 36.92043399810791,
      "citing_paper_id": "250374739",
      "cited_paper_id": 14517241
    },
    {
      "context_text": "…try to solve event correspondence problem (e.g., using traditional stereo metrics such as normalized cross-correlation on event frames [23], [24] or time surfaces [25], by exploiting simultaneity and temporal correlations of the events across sensors [15], [25], [26], by comparing local…",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods and approaches. There are no clear identifiers for datasets.",
      "processing_time": 35.05490279197693,
      "citing_paper_id": "250374739",
      "cited_paper_id": 24236495
    },
    {
      "context_text": "where w(·) denotes the disparity-based warping [49].",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for disparity-based warping.",
      "processing_time": 34.22698760032654,
      "citing_paper_id": "250374739",
      "cited_paper_id": 29151213
    },
    {
      "context_text": "Most of the methods extended the warping mechanism with bilinear sampling methods, proposed in [49], to derive a fully differentiable training loss.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and techniques.",
      "processing_time": 33.40623140335083,
      "citing_paper_id": "250374739",
      "cited_paper_id": 29151213
    },
    {
      "context_text": "Several methods extended the local appearance matching loss along with the left-right consistency loss [11], [51], [52], segmentation cue loss [46], maximum depth heuristic loss [51], and occlusion-aware mechanisms [21], [53], [54].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only various losses and mechanisms used in stereo depth estimation methods.",
      "processing_time": 35.04283142089844,
      "citing_paper_id": "250374739",
      "cited_paper_id": 31613165
    },
    {
      "context_text": "Several methods extended the local appearance matching loss along with the left-right consistency loss [11], [51], [52], segmentation cue loss [46], maximum depth heuristic loss [51], and occlusion-aware mechanisms [21], [53], [54].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only various losses and mechanisms used in stereo depth estimation methods.",
      "processing_time": 35.04283142089844,
      "citing_paper_id": "250374739",
      "cited_paper_id": 195440681
    },
    {
      "context_text": "Several methods extended the local appearance matching loss along with the left-right consistency loss [11], [51], [52], segmentation cue loss [46], maximum depth heuristic loss [51], and occlusion-aware mechanisms [21], [53], [54].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only various losses and mechanisms used in stereo depth estimation methods.",
      "processing_time": 35.04283142089844,
      "citing_paper_id": "250374739",
      "cited_paper_id": 221761595
    },
    {
      "context_text": "Jason et al. [50] extended the local appearance matching loss with a spatial smoothness loss for unsupervised monocular depth estimation, which has been later adopted by most of the unsupervised stereo methods [46], [51].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and approaches. The context focuses on the extension of a loss function for depth estimation.",
      "processing_time": 35.42243576049805,
      "citing_paper_id": "250374739",
      "cited_paper_id": 31613165
    },
    {
      "context_text": "Hand-crafted embedding methods for stereo depth estimation generally convert sparse 3D event points into a rasterized or voxelized representation (e.g., event images [43]) to make use of the off-the-self frame-based stereo methods.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific, verifiable datasets. It only refers to a general method of converting sparse 3D event points into representations like event images, which is not a specific dataset.",
      "processing_time": 38.8967981338501,
      "citing_paper_id": "250374739",
      "cited_paper_id": 46938951
    },
    {
      "context_text": "To generate the event images, the FCVF* and SGM* use the temporal event aggregation for feature accumulation [3].",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods (FCVF* and SGM*) and a general process (temporal event aggregation).",
      "processing_time": 36.71671199798584,
      "citing_paper_id": "250374739",
      "cited_paper_id": 49877954
    },
    {
      "context_text": "The FCVF* and SGM* methods are frame-based methods that are re-implemented for event images in [3] whereas CopNet uses a co-operative stereo algorithm to generate sparse disparity maps.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and algorithms. The context focuses on describing the methods used for event-based stereo depth estimation.",
      "processing_time": 36.711400747299194,
      "citing_paper_id": "250374739",
      "cited_paper_id": 49877954
    },
    {
      "context_text": "Early pioneering studies demonstrated the usability of event cameras for the stereo depth estimation and introduced several non-learning-based event stereo matching algorithms [3]–[7].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only references to studies and algorithms. No clear, verifiable datasets are identified.",
      "processing_time": 36.008079051971436,
      "citing_paper_id": "250374739",
      "cited_paper_id": 49877954
    },
    {
      "context_text": "Speciﬁcally, for non-learning-based meth-ods, we compared with FCVF* [3], [63], SGM* [3], [64], TSES [4], and CopNet [65].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods and models. No dataset names are present in the text.",
      "processing_time": 35.75200319290161,
      "citing_paper_id": "250374739",
      "cited_paper_id": 49877954
    },
    {
      "context_text": "Event representation is an active ﬁeld of research and there exists a signiﬁcant number of proposals for event representation for various tasks such as object recognition, classiﬁcation, image reconstruction, visual odometry etc [1], [39]–[42].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general tasks and fields of research. No clear identifiers for datasets are present.",
      "processing_time": 36.452985763549805,
      "citing_paper_id": "250374739",
      "cited_paper_id": 52291653
    },
    {
      "context_text": "…by exploiting simultaneity and temporal correlations of the events across sensors [15], [25], [26], by comparing local context descriptors [27], [28] of the spatial distribution of events, etc.) Since these methods match events by comparing their neighborhoods, they often impose additional…",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods and approaches. There are no clear identifiers for datasets in the text.",
      "processing_time": 36.44925260543823,
      "citing_paper_id": "250374739",
      "cited_paper_id": 157060825
    },
    {
      "context_text": "Following the existing unsupervised methods for frame-based [11]–[14], we adopted a warping-based unsupervised loss as the baseline unsupervised loss.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only unsupervised methods and losses. No verifiable resources are identified.",
      "processing_time": 35.991971254348755,
      "citing_paper_id": "250374739",
      "cited_paper_id": 221761595
    },
    {
      "context_text": "Furthermore, we have performed an additional comparison experiment between a state-of-the-art intensity frame-based unsupervised method, PASMNet [11] and our event-based unsupervised method on challenging scenarios from the DSEC dataset [60].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "DSEC"
      ],
      "dataset_descriptions": {
        "DSEC": "Used to compare performance between a state-of-the-art intensity frame-based unsupervised method and an event-based unsupervised method in challenging stereo depth estimation scenarios."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions the DSEC dataset, which is used for comparing performance between an intensity frame-based method and an event-based method in challenging scenarios.",
      "processing_time": 44.5366895198822,
      "citing_paper_id": "250374739",
      "cited_paper_id": 221761595
    },
    {
      "context_text": "Table V also shows the quantitative comparison between the PASMNet [11] and our event-based method on the DSEC dataset.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "DSEC"
      ],
      "dataset_descriptions": {
        "DSEC": "Used to quantitatively compare the performance of the PASMNet and the event-based method, focusing on stereo depth estimation accuracy using event camera data."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions the DSEC dataset, which is a specific dataset used for evaluating stereo depth estimation methods, particularly those involving event cameras.",
      "processing_time": 43.98206281661987,
      "citing_paper_id": "250374739",
      "cited_paper_id": 221761595
    },
    {
      "context_text": "Thus, compared with the intensity-based method, our event-based method produce [11]) and our proposed event-based method in challenging lighting conditions (Zurich in DSEC dataset [60]).",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "DSEC"
      ],
      "dataset_descriptions": {
        "DSEC": "Used to evaluate the performance of the proposed event-based stereo depth estimation method under challenging lighting conditions, focusing on accuracy and robustness."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions the DSEC dataset, which is a specific dataset used for evaluating event-based stereo depth estimation methods.",
      "processing_time": 42.922210454940796,
      "citing_paper_id": "250374739",
      "cited_paper_id": 221761595
    },
    {
      "context_text": "…losses only consider local intensity-level differences (e.g., local window or per-pixel difference) that can yield incorrect disparity values [57], and ii) reducing the feature-level epipolar correlation difference between a warped image and a ground-truth image can implicitly supervise the…",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only discusses methodological aspects of stereo depth estimation.",
      "processing_time": 35.192086696624756,
      "citing_paper_id": "250374739",
      "cited_paper_id": 233210697
    },
    {
      "context_text": "However, pixel-level correlations may contain incorrect values owing to the lighting mismatch between stereo cameras [57], and hence, it is intuitive to use low-level feature correlation information since low-level feature maps contain the distinguishable structure and texture features extracted…",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only discusses the challenges and intuition behind using low-level feature correlation information in stereo depth estimation.",
      "processing_time": 36.849674701690674,
      "citing_paper_id": "250374739",
      "cited_paper_id": 233210697
    },
    {
      "context_text": "Tulyakov et al. [8] proposed a temporal-relationship-based learning mechanism for event embedding, namely continuous fully-connected (CFC) layer-based embedding, that utilizes an event queue to store the most recent events and calculates the temporal relationships among events based on their…",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for event embedding. The context focuses on the technique rather than a particular dataset.",
      "processing_time": 36.84499216079712,
      "citing_paper_id": "250374739",
      "cited_paper_id": 262638843
    },
    {
      "context_text": "Per-pixel temporal relationships (e.g., based on event timestamps) for event embedding have been shown to provide meaningful feature information for disparity regression [8].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for embedding event sequences for stereo depth estimation.",
      "processing_time": 34.76881003379822,
      "citing_paper_id": "250374739",
      "cited_paper_id": 262638843
    },
    {
      "context_text": "Following [8], the event queue Q stores the N most recent events in each pixel location by the time of their arrival, as shown in Fig.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for handling event sequences in stereo depth estimation.",
      "processing_time": 34.76505947113037,
      "citing_paper_id": "250374739",
      "cited_paper_id": 262638843
    },
    {
      "context_text": "A novel epipolar feature correlation difference loss, along with local appearance matching losses, supervises the event stereo network during the training. for event-based monocular depth estimation [17], [44], they generally predict less accurate depth maps than stereo matching approaches [8].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets by name. It discusses methods and approaches but does not reference any particular dataset.",
      "processing_time": 36.40809488296509,
      "citing_paper_id": "250374739",
      "cited_paper_id": 262638843
    },
    {
      "context_text": "However, the hand-crafted methods are generally prone to motion blur and perform comparatively worse than a learning-based embedding method [8].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a comparison between hand-crafted methods and learning-based embedding methods.",
      "processing_time": 36.15856647491455,
      "citing_paper_id": "250374739",
      "cited_paper_id": 262638843
    },
    {
      "context_text": "More prominently, Tulyakov et al. [8] utilize event occurrence times to extract the temporal relationships of irregularly sampled asynchronous events at each pixel location.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for handling asynchronous events in stereo depth estimation.",
      "processing_time": 34.959948778152466,
      "citing_paper_id": "250374739",
      "cited_paper_id": 262638843
    },
    {
      "context_text": "To overcome this, Tulyakov et al. [8] and Ahmed et al. [9] have recently proposed deep learning-based stereo methods to produce dense disparity maps from stereo event sequences.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and approaches. The context focuses on deep learning-based stereo methods for producing dense disparity maps from stereo event sequences.",
      "processing_time": 38.424036264419556,
      "citing_paper_id": "250374739",
      "cited_paper_id": 262638843
    },
    {
      "context_text": "In addition, Tulyakov et al. [8] introduced a temporal convolution-based embedding that performs temporal aggregation using the timestamp and polarity of each event.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for temporal aggregation in event-based stereo depth estimation.",
      "processing_time": 35.15657615661621,
      "citing_paper_id": "250374739",
      "cited_paper_id": 262638843
    },
    {
      "context_text": "Recent methods [8], [9] focus on devising end-to-end learning models that include a learning-based event embedding module and a CNN-based stereo matching module.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and models. The context focuses on describing the components of recent methods in event-based stereo depth estimation.",
      "processing_time": 38.05635690689087,
      "citing_paper_id": "250374739",
      "cited_paper_id": 262638843
    },
    {
      "context_text": "These figures have been improved by about one order of magnitude with respect to previous DVS designs [7]-[9].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only improvements in DVS designs. No verifiable resources are identified.",
      "processing_time": 35.5481858253479,
      "citing_paper_id": "1686141",
      "cited_paper_id": 2497402
    },
    {
      "context_text": "Of special interest for very high speed processing applications are the so-called “Dynamic Vision Sensors” (DVS), where each pixel autonomously computes the normalized time derivative of the sensed light ( ) and provides an output event with its (x, y) coordinate when this amount exceeds a preset contrast [7]-[9].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only describes the functionality of Dynamic Vision Sensors (DVS).",
      "processing_time": 35.32092213630676,
      "citing_paper_id": "1686141",
      "cited_paper_id": 2497402
    },
    {
      "context_text": "In this paper, we present a new DVS sensor with better contrast sensitivity than previously reported ones [7]-[9] and demonstrate its application in event-based computation of 3D using the output from two DVS sensors connected to a convolution network hardware.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a new DVS sensor and its application in event-based computation of 3D. No verifiable datasets are referenced.",
      "processing_time": 37.81598091125488,
      "citing_paper_id": "1686141",
      "cited_paper_id": 2497402
    },
    {
      "context_text": "1(c) shows the original photo transduction stage [7], [9] with Av = nn, where nn is the subthreshold slope factor of NMOS transistor Mn1.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only technical details about a photo transduction stage.",
      "processing_time": 35.320068359375,
      "citing_paper_id": "1686141",
      "cited_paper_id": 2497402
    },
    {
      "context_text": "1(a) shows the basic block diagram of a typical DVS pixel [7]-[8], [20].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only references to DVS pixels and their block diagrams. No verifiable resources are identified.",
      "processing_time": 37.26527523994446,
      "citing_paper_id": "1686141",
      "cited_paper_id": 2497402
    },
    {
      "context_text": "1(a) shows the basic block diagram of a typical DVS pixel [7]-[8], [20].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only references to DVS pixels and their block diagrams. No verifiable resources are identified.",
      "processing_time": 37.26527523994446,
      "citing_paper_id": "1686141",
      "cited_paper_id": 6885978
    },
    {
      "context_text": "1(a) shows the basic block diagram of a typical DVS pixel [7]-[8], [20].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only references to DVS pixels and their block diagrams. No verifiable resources are identified.",
      "processing_time": 37.26527523994446,
      "citing_paper_id": "1686141",
      "cited_paper_id": 13820553
    },
    {
      "context_text": "A FPN down to 0.9% has been measured (25-50% lower with than previous designs)\nAv 12≈\nAvC1 C2⁄ 60≈\nInj\nInj AIIph Isne\nVGj VSj– nnUT\n--------------------- ≈=\nVlog VGj VSj–( ) j 1=\nN\n∑ VDC NnnUT Iphlog+= =\nVDC NnnUT AI Isn⁄( )log= Av nnN≈\nunder a illumination of 60 lux, while measured latency was 3μs, kept equal to the best previously reported values.",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The provided context does not mention any specific datasets, models, or methods. It appears to be discussing performance metrics and technical specifications of a design.",
      "processing_time": 37.616811990737915,
      "citing_paper_id": "1686141",
      "cited_paper_id": 6885978
    },
    {
      "context_text": "Current mirror Mp1-Mp2 amplifies photo current to AIIph [20] feeding a column of N diode-connected transistors Mnj, j = 1,... N.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any datasets, only hardware components and circuits. There are no specific, verifiable datasets in the context.",
      "processing_time": 36.11892294883728,
      "citing_paper_id": "1686141",
      "cited_paper_id": 6885978
    },
    {
      "context_text": "This transistor column performs a transimpedance amplification from input current AIIph to output voltage Vlog.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any datasets, only a technical description of a transistor column. No verifiable resources are identified.",
      "processing_time": 35.648157596588135,
      "citing_paper_id": "1686141",
      "cited_paper_id": 6885978
    },
    {
      "context_text": "Current mirror Mp1-Mp2 amplifies photo current to AIIph [20] feeding a column of N diode-connected transistors Mnj, j = 1,.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only hardware components and circuits. There are no verifiable resources that meet the criteria.",
      "processing_time": 37.045092821121216,
      "citing_paper_id": "1686141",
      "cited_paper_id": 6885978
    },
    {
      "context_text": "Frame-based stereovision processes are also incompatible with precise timings usually used in the early visual areas of the brain [10].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a reference to neural processes in the retina. No verifiable resources are identified.",
      "processing_time": 37.24628782272339,
      "citing_paper_id": "1686141",
      "cited_paper_id": 8372163
    },
    {
      "context_text": "However, in Leñero’s [8] scheme the pre-amplifier required to match NMOS and PMOS transistors and had high power consumption.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any datasets, only discusses a hardware scheme. No specific, verifiable datasets are referenced.",
      "processing_time": 35.641496419906616,
      "citing_paper_id": "1686141",
      "cited_paper_id": 13820553
    },
    {
      "context_text": "1: (a) Pixel block diagram, (b) data driven asynchronous event generation, (c) Delbrück’s original photo current transduction circuit, (d) Leñero’s transduction with mismatch sensitive pre-amplification, (e) proposed transduction with mismatch insensitive pre-amplification.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only technical details of event-driven sensors. No verifiable resources are identified.",
      "processing_time": 37.03352618217468,
      "citing_paper_id": "1686141",
      "cited_paper_id": 13820553
    },
    {
      "context_text": "1(d) shows Leñero’s [8] photo transduction and pre-amplification stage, where helped to reduce capacitive spread to C1/C2 = 5 while improving overall voltage gain to about with smaller area.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a hardware component of an event-driven dynamic vision sensor.",
      "processing_time": 36.0949330329895,
      "citing_paper_id": "1686141",
      "cited_paper_id": 13820553
    },
    {
      "context_text": "Maximum output event rate was kept high at 20Meps thanks to Boahen’s burst-mode row parallel AER read out scheme [21].",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for improving event rate in neuromorphic systems.",
      "processing_time": 35.280258893966675,
      "citing_paper_id": "1686141",
      "cited_paper_id": 15184309
    },
    {
      "context_text": "Frame-based stereo is also “token-based” meaning that direct matching of pixels is rarely performed, instead several features are used such as orientation [12], optical flow [13] but mainly corners-based descriptors of local luminance [14].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods and features used in stereo estimation.",
      "processing_time": 35.874560594558716,
      "citing_paper_id": "1686141",
      "cited_paper_id": 18419821
    },
    {
      "context_text": "Since a lower MDE indicates less depth estimation error, based on the comparison of MDE, ASNet performs better than StereoSpike [14], TSES [15], and CopNet [16] in most situations.",
      "catation_intent": "findings",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only comparisons between methods. No verifiable resources are identified.",
      "processing_time": 36.31706929206848,
      "citing_paper_id": "265257632",
      "cited_paper_id": 4412139
    },
    {
      "context_text": "…blocks to reduce the number of network parameters and computational complexity, achieving lightweight deep neural networks [4]; and using cost aggregation-based disparity estimation methods to output gradually improving depth estimates at different time intervals for real-time depth estimation [5].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and approaches for stereo depth estimation.",
      "processing_time": 35.618025064468384,
      "citing_paper_id": "265257632",
      "cited_paper_id": 53082511
    },
    {
      "context_text": "Spiking neural networks have been the main focus for event-based stereo depth estimation [9].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a focus on spiking neural networks for event-based stereo depth estimation.",
      "processing_time": 37.214163064956665,
      "citing_paper_id": "265257632",
      "cited_paper_id": 205698386
    },
    {
      "context_text": "…and lightweight blocks to reduce the number of network parameters and computational complexity, achieving lightweight deep neural networks [4]; and using cost aggregation-based disparity estimation methods to output gradually improving depth estimates at different time intervals for…",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and approaches for stereo matching and lightweight deep networks.",
      "processing_time": 36.31367874145508,
      "citing_paper_id": "265257632",
      "cited_paper_id": 237267197
    },
    {
      "context_text": "Following well-performing stereo depth estimation network MobileStereoNet [4], we design our networks using some of their sub-networks.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (MobileStereoNet) which is not included as per instructions.",
      "processing_time": 37.408060789108276,
      "citing_paper_id": "265257632",
      "cited_paper_id": 237267197
    },
    {
      "context_text": "The datasets can be used not only for stereo but also for scene ﬂow, SLAM, and other event-based applications.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.3,
      "reasoning": "The context mentions 'datasets' but does not specify any particular dataset names. The cited papers do not provide additional specific dataset names either.",
      "processing_time": 37.19709014892578,
      "citing_paper_id": "22158024",
      "cited_paper_id": 1686141
    },
    {
      "context_text": "The datasets can be used not only for stereo but also for scene ﬂow, SLAM, and other event-based applications.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.3,
      "reasoning": "The context mentions 'datasets' but does not specify any particular dataset names. The cited papers do not provide additional specific dataset names either.",
      "processing_time": 37.19709014892578,
      "citing_paper_id": "22158024",
      "cited_paper_id": 6258804
    },
    {
      "context_text": "Recently, some datasets for event-based simultaneous localization and mapping (SLAM) (Kogler et al., 2013; Serrano-Gotarredona et al., 2013) have become available, but none of those are created for event based stereo matching and the above previous works do not release their test datasets.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.3,
      "reasoning": "The context mentions datasets for event-based SLAM but specifies that none are created for event-based stereo matching. No specific dataset names are provided.",
      "processing_time": 38.12147355079651,
      "citing_paper_id": "22158024",
      "cited_paper_id": 1686141
    },
    {
      "context_text": "Recently, some datasets for event-based simultaneous localization and mapping (SLAM) (Kogler et al., 2013; Serrano-Gotarredona et al., 2013) have become available, but none of those are created for event based stereo matching and the above previous works do not release their test datasets.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.3,
      "reasoning": "The context mentions datasets for event-based SLAM but specifies that none are created for event-based stereo matching. No specific dataset names are provided.",
      "processing_time": 38.12147355079651,
      "citing_paper_id": "22158024",
      "cited_paper_id": 6258804
    },
    {
      "context_text": "Recently, some datasets for eventbased simultaneous localization and mapping (SLAM) (Kogler et al., 2013; Serrano-Gotarredona et al., 2013) have become available, but none of those are created for event based stereo matching and the above previous works do not release their test datasets.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions datasets for event-based SLAM but specifies that none are for event-based stereo matching. No specific dataset names are provided.",
      "processing_time": 37.1942777633667,
      "citing_paper_id": "22158024",
      "cited_paper_id": 1686141
    },
    {
      "context_text": "The datasets used in previous works (Rogister et al., 2012; Camuñasmesa et al., 2014; Firouzi and Conradt, 2016) both assume the cameras are static.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.3,
      "reasoning": "The context mentions datasets used in previous works but does not specify the names of the datasets. The cited papers' titles suggest the datasets are related to event-based stereo matching, but no specific dataset names are provided.",
      "processing_time": 40.776609659194946,
      "citing_paper_id": "22158024",
      "cited_paper_id": 4833834
    },
    {
      "context_text": "The datasets used in previous works (Rogister et al., 2012; Camuñasmesa et al., 2014; Firouzi and Conradt, 2016) both assume the cameras are static.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.3,
      "reasoning": "The context mentions datasets used in previous works but does not specify the names of the datasets. The cited papers' titles suggest the datasets are related to event-based stereo matching, but no specific dataset names are provided.",
      "processing_time": 40.776609659194946,
      "citing_paper_id": "22158024",
      "cited_paper_id": 17693733
    },
    {
      "context_text": "Inthis paper we have replicated Rogister’s and Firouzi’s algorithms.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only algorithms. No dataset names are present in the citation span.",
      "processing_time": 36.706458568573,
      "citing_paper_id": "22158024",
      "cited_paper_id": 4833834
    },
    {
      "context_text": "Inthis paper we have replicated Rogister’s and Firouzi’s algorithms.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only algorithms. No dataset names are present in the citation span.",
      "processing_time": 36.706458568573,
      "citing_paper_id": "22158024",
      "cited_paper_id": 17693733
    },
    {
      "context_text": "Firouzi and Conradt (2016) came up with the dynamic cooperative neural network to make use of the stream of the events.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (dynamic cooperative neural network).",
      "processing_time": 35.23215413093567,
      "citing_paper_id": "22158024",
      "cited_paper_id": 4833834
    },
    {
      "context_text": "Cop-net is used to denote Firouzi’s cooperative network approach, and EMP is used to denote our Event-based Message Passing approach.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods and approaches. No verifiable resources are identified.",
      "processing_time": 36.70541286468506,
      "citing_paper_id": "22158024",
      "cited_paper_id": 4833834
    },
    {
      "context_text": "There are some state-of-art event-based stereo matching algorithms like Rogister’s ( Rogister et al., 2012), Camuñasmesa (Camuñasmesa et al., 2014), and Firouzi’s (Firouzi and Conradt, 2016).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only algorithms and methods. No verifiable resources are identified.",
      "processing_time": 36.70244383811951,
      "citing_paper_id": "22158024",
      "cited_paper_id": 4833834
    },
    {
      "context_text": "There are some state-of-art event-based stereo matching algorithms like Rogister’s ( Rogister et al., 2012), Camuñasmesa (Camuñasmesa et al., 2014), and Firouzi’s (Firouzi and Conradt, 2016).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only algorithms and methods. No verifiable resources are identified.",
      "processing_time": 36.70244383811951,
      "citing_paper_id": "22158024",
      "cited_paper_id": 17693733
    },
    {
      "context_text": "A schematic of the MRF connectivity and messages are shown in Figure 4C .",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (Belief Propagation) and a figure. No verifiable dataset names are present.",
      "processing_time": 38.301121950149536,
      "citing_paper_id": "22158024",
      "cited_paper_id": 7495827
    },
    {
      "context_text": "Finding labels that minimize the cost corresponds to a maximum a posteriori (MAP) estimation problem in an appropriately deﬁned MRF (Sun et al., 2003 Felzenszwalb and Huttenlocher, 2004).",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and algorithms. The context is about the MAP estimation problem in MRFs, which is a methodological discussion.",
      "processing_time": 38.94321632385254,
      "citing_paper_id": "22158024",
      "cited_paper_id": 7495827
    },
    {
      "context_text": "Finding labels that minimize the cost corresponds to a maximum a posteriori (MAP) estimation problem in an appropriately deﬁned MRF (Sun et al., 2003 Felzenszwalb and Huttenlocher, 2004).",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and algorithms. The context is about the MAP estimation problem in MRFs, which is a methodological discussion.",
      "processing_time": 38.94321632385254,
      "citing_paper_id": "22158024",
      "cited_paper_id": 8702465
    },
    {
      "context_text": "BP is a message passing algorithm for performing inference on graphical models, such as Bayesian networks and Markov random ﬁelds (MRF).",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (Belief Propagation) used for stereo matching.",
      "processing_time": 36.95737433433533,
      "citing_paper_id": "22158024",
      "cited_paper_id": 7495827
    },
    {
      "context_text": "Our goal is to ﬁnd proper label for each pixel to minimize the cost, which corresponds to a maximum a posteriori estimation problem in an appropriately deﬁned Markov Random Field(MAP-MRF).",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only a methodological approach to stereo matching using belief propagation.",
      "processing_time": 36.678964376449585,
      "citing_paper_id": "22158024",
      "cited_paper_id": 7495827
    },
    {
      "context_text": "However, as shown in Figure 1 , the classical BP does not work for event accumulated frames, so we have to construct a modiﬁed MRF to manage the event-driven input and formulate a dynamic updating mechanism to deal with the temporal correlation of the event stream.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a method (Belief Propagation) and a modified approach for event-driven input. No verifiable datasets are referenced.",
      "processing_time": 39.31009268760681,
      "citing_paper_id": "22158024",
      "cited_paper_id": 7495827
    },
    {
      "context_text": "The max-product BP algorithm can be used to solve the MAP-MRF problem eﬃciently (Felzenszwalb and Huttenlocher, 2004).",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only algorithms and methods. The context is about using the max-product BP algorithm for solving the MAP-MRF problem, which is not directly related to datasets.",
      "processing_time": 40.16061472892761,
      "citing_paper_id": "22158024",
      "cited_paper_id": 7495827
    },
    {
      "context_text": "The max-product BP algorithm can be used to solve the MAP-MRF problem eﬃciently (Felzenszwalb and Huttenlocher, 2004).",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only algorithms and methods. The context is about using the max-product BP algorithm for solving the MAP-MRF problem, which is not directly related to datasets.",
      "processing_time": 40.16061472892761,
      "citing_paper_id": "22158024",
      "cited_paper_id": 8702465
    },
    {
      "context_text": "Finally, the disparity output stage estimates the disparity of each event and generates a semi-dense disparity map with the updated MRF.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for estimating disparity using an MRF.",
      "processing_time": 36.249759912490845,
      "citing_paper_id": "22158024",
      "cited_paper_id": 7495827
    },
    {
      "context_text": "Inspired by Cook et al. (2011) who was using message passing algorithm to jointly estimate ego-motion intensity and optical ﬂow, we explore message passing for stereo depth estimation.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (message passing algorithm) and its application. No verifiable resources are identified.",
      "processing_time": 38.07401442527771,
      "citing_paper_id": "22158024",
      "cited_paper_id": 8385399
    },
    {
      "context_text": "For the event-driven message passing framework, we follow the idea from Felzenszwalb and Huttenlocher (2004), which deﬁnes stereo matching as a labeling problem.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for stereo matching. The context is focused on the methodology rather than the data used.",
      "processing_time": 38.06734013557434,
      "citing_paper_id": "22158024",
      "cited_paper_id": 8702465
    },
    {
      "context_text": "One possible method for estimating stereo is to construct frames by accumulating events over a period of time, and then use the BPmethod (Felzenszwalb and Huttenlocher, 2004) to process in a frame-based manner.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for stereo estimation using event accumulation and belief propagation.",
      "processing_time": 36.92926049232483,
      "citing_paper_id": "22158024",
      "cited_paper_id": 8702465
    },
    {
      "context_text": "We the min convolution algorithm from Felzenszwalb and Huttenlocher (2004) to reduce the complexity of message updating to be linear rather than quadratic in the number of labels.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions an algorithm but does not refer to any specific dataset. The citation is about a method to improve computational efficiency.",
      "processing_time": 37.48806095123291,
      "citing_paper_id": "22158024",
      "cited_paper_id": 8702465
    },
    {
      "context_text": "The max-product BP algorithm can be used to solve the MAP-MRF problem efficiently (Felzenszwalb and Huttenlocher, 2004).",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions the max-product BP algorithm but does not refer to any specific dataset. The context is about the algorithm's application to solve the MAP-MRF problem.",
      "processing_time": 39.08444595336914,
      "citing_paper_id": "22158024",
      "cited_paper_id": 8702465
    },
    {
      "context_text": "One possible method for estimating stereo is to construct frames by accumulating events over a period of time, and then use the BP method (Felzenszwalb and Huttenlocher, 2004) to process in a frame-based manner.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for stereo estimation using event accumulation and belief propagation.",
      "processing_time": 36.912625789642334,
      "citing_paper_id": "22158024",
      "cited_paper_id": 8702465
    },
    {
      "context_text": "BP as a global cost optimization method is used by some state-of-art frame-based stereo methods on the Middlebury and KITTI benchmarks.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Middlebury",
        "KITTI"
      ],
      "dataset_descriptions": {
        "Middlebury": "Used to evaluate frame-based stereo methods, focusing on global cost optimization techniques for depth estimation.",
        "KITTI": "Used to evaluate frame-based stereo methods, focusing on global cost optimization techniques for depth estimation in autonomous vehicle scenarios."
      },
      "confidence_score": 0.9,
      "reasoning": "The citation mentions 'Middlebury' and 'KITTI' as benchmarks, which are commonly used datasets in stereo vision and autonomous driving research.",
      "processing_time": 47.87148666381836,
      "citing_paper_id": "22158024",
      "cited_paper_id": 12986049
    },
    {
      "context_text": "Traditional frame-based stereo vision systems continue to steadily mature, in part thanks to publicly available datasets, such as the Middlebury (Scharstein and Szeliski, 2002) and KITTI (Menze and Geiger, 2015) benchmarks.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Middlebury",
        "KITTI"
      ],
      "dataset_descriptions": {
        "Middlebury": "Used to evaluate stereo vision algorithms, providing ground truth disparity maps for benchmarking performance in depth estimation.",
        "KITTI": "Utilized for evaluating stereo vision and scene flow algorithms in automotive settings, offering real-world driving scenarios with ground truth data."
      },
      "confidence_score": 1.0,
      "reasoning": "The citation mentions two well-known datasets, Middlebury and KITTI, which are used for stereo vision and scene flow estimation. These datasets are relevant to the topic of event-based stereo depth estimation.",
      "processing_time": 51.04285025596619,
      "citing_paper_id": "22158024",
      "cited_paper_id": 12986049
    },
    {
      "context_text": "In the experiments, ST is used to denote Rogister’s method which enforces Space (epipolar) and Time constraints for stereo matching.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (Rogister’s method) used for stereo matching. No verifiable resources are identified.",
      "processing_time": 38.63823366165161,
      "citing_paper_id": "22158024",
      "cited_paper_id": 17693733
    },
    {
      "context_text": "Rogister et al. used one moving pen and two simultaneously moving pens as stimulus and showed the detected disparity (Rogister et al., 2012), but the accuracy of the algorithm is not quantitatively analyzed.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method or experimental setup. The context focuses on the use of moving pens as stimuli, which is not a verifiable dataset.",
      "processing_time": 39.74030876159668,
      "citing_paper_id": "22158024",
      "cited_paper_id": 17693733
    },
    {
      "context_text": "However, matching using temporal and polarity criterion alone is prone to errors because the latency of events varies (jitter) (Rogister et al., 2012).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only discusses a methodological issue in event-based stereo matching.",
      "processing_time": 35.963462352752686,
      "citing_paper_id": "22158024",
      "cited_paper_id": 17693733
    },
    {
      "context_text": "Event-based vision sensors loosely mimic biological retinas, asynchronously generating events in response to relative light intensity changes rather than absolute image intensity (Posch et al., 2011).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only describes the functionality of event-based vision sensors.",
      "processing_time": 35.49816846847534,
      "citing_paper_id": "22158024",
      "cited_paper_id": 21317717
    },
    {
      "context_text": "Many researchers have explored event-based matching criterions for event-based cameras such as ATIS (Posch et al., 2011) and DVS (Lichtsteiner et al., 2008).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions event-based cameras ATIS and DVS but does not refer to them as datasets. They are hardware devices, not datasets.",
      "processing_time": 38.43443202972412,
      "citing_paper_id": "22158024",
      "cited_paper_id": 21317717
    },
    {
      "context_text": "For the event-based stereo setup, we rely on two DAVIS240C (Brandli et al., 2014) sensors.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions hardware sensors but does not refer to any specific dataset. The context is about the setup and not about data collection or analysis.",
      "processing_time": 37.63302135467529,
      "citing_paper_id": "22158024",
      "cited_paper_id": 24007071
    },
    {
      "context_text": "Calibration is performed by using the frame-capture capability of the DAVIS240C to simultaneously record frames from both sensors, which can then be used with OpenCV to calibrate.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions the DAVIS240C sensor but does not refer to it as a dataset. It is used for calibration, which is a methodological step.",
      "processing_time": 38.61474108695984,
      "citing_paper_id": "22158024",
      "cited_paper_id": 24007071
    },
    {
      "context_text": "Data from the DAVIS240C, ZED, and Vicon are simultaneously recorded using the Robot Operating System (ROS) 3 .",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions specific hardware (DAVIS240C, ZED, Vicon) used for recording data, but does not specify a named dataset. The context is about the recording process, not a reusable dataset.",
      "processing_time": 40.665284872055054,
      "citing_paper_id": "22158024",
      "cited_paper_id": 24007071
    },
    {
      "context_text": "Events are read out from each DAVIS240C sensor independently over two separate USB cables, but their timestamps are synchronized using the standard timestamp synchronization feature of the sensors (which relies on the audio cable seen in the ﬁgure).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context describes the setup and synchronization of DAVIS240C sensors but does not mention any specific dataset. The citation is about the technical details of the sensors rather than a dataset.",
      "processing_time": 39.55347013473511,
      "citing_paper_id": "22158024",
      "cited_paper_id": 24007071
    },
    {
      "context_text": "It consists of the ZED frame-based stereo sensor mounted below two event-based DAVIS240C sensors, all of which are held together with a 3D printed plastic mounting.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context describes a setup of sensors but does not mention any specific datasets. No dataset names are present in the citation span.",
      "processing_time": 37.616737842559814,
      "citing_paper_id": "22158024",
      "cited_paper_id": 24007071
    },
    {
      "context_text": "[14,13], which took advantage of the high temporal resolution of the events to remove motion blur from an event image using an estimate of the motion in the scene, such as optical flow or camera pose.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only methods and approaches. The titles of the cited papers also do not indicate the use of specific datasets.",
      "processing_time": 39.02289867401123,
      "citing_paper_id": "4412139",
      "cited_paper_id": 1082643
    },
    {
      "context_text": "[14,13], which took advantage of the high temporal resolution of the events to remove motion blur from an event image using an estimate of the motion in the scene, such as optical flow or camera pose.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only methods and approaches. The titles of the cited papers also do not indicate the use of specific datasets.",
      "processing_time": 39.02289867401123,
      "citing_paper_id": "4412139",
      "cited_paper_id": 30723444
    },
    {
      "context_text": "[13] use the pose of a single camera from multiple views to generate a disparity space volume, in which the correct depth is similarly deblurred.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific dataset names, only a method for generating a disparity space volume using a single camera's pose from multiple views.",
      "processing_time": 37.83542466163635,
      "citing_paper_id": "4412139",
      "cited_paper_id": 1082643
    },
    {
      "context_text": "[22] use a novel local event context descriptor based on the distances between events in a window, which they extend in [23] to produce a dense disparity estimate.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and approaches. The context is focused on describing a method for event-driven stereo matching.",
      "processing_time": 38.40250325202942,
      "citing_paper_id": "4412139",
      "cited_paper_id": 1408596
    },
    {
      "context_text": "We used the camera velocities provided in the dataset from [21], which were generated by linear interpolation of the lidar odometry poses provided from MVSEC, and are provided in addition to ground truth optical flow for the sequences in the dataset.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "MVSEC"
      ],
      "dataset_descriptions": {
        "MVSEC": "Used to provide camera velocities and ground truth optical flow for sequences, supporting self-supervised optical flow estimation for event-based cameras."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions using camera velocities from a dataset, which includes ground truth optical flow for sequences. The dataset is associated with MVSEC, which is a known event-based vision dataset.",
      "processing_time": 45.255173444747925,
      "citing_paper_id": "4412139",
      "cited_paper_id": 3396150
    },
    {
      "context_text": "We evaluate our results on the Multi Vehicle Stereo Event Camera dataset(1) [20], and show significant improvements in disparity error over state of the art event based stereo methods, which rely on additional, more computationally expensive, smoothness regularizations.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Multi Vehicle Stereo Event Camera dataset"
      ],
      "dataset_descriptions": {
        "Multi Vehicle Stereo Event Camera dataset": "Used to evaluate event-based stereo depth estimation methods, focusing on reducing disparity error without computationally expensive smoothness regularizations."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions a specific dataset used for evaluating results in event-based stereo depth estimation, which is directly relevant to the research topic.",
      "processing_time": 44.6854202747345,
      "citing_paper_id": "4412139",
      "cited_paper_id": 3416874
    },
    {
      "context_text": "We evaluated our algorithm on the Multi Vehicle Stereo Event Camera (MVSEC) dataset [20].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Multi Vehicle Stereo Event Camera (MVSEC)"
      ],
      "dataset_descriptions": {
        "Multi Vehicle Stereo Event Camera (MVSEC)": "Used to evaluate the algorithm for event-based stereo depth estimation, focusing on 3D perception tasks with event camera data."
      },
      "confidence_score": 1.0,
      "reasoning": "The citation clearly mentions the use of the MVSEC dataset for evaluating the algorithm, which is directly relevant to event-based stereo depth estimation.",
      "processing_time": 45.76923847198486,
      "citing_paper_id": "4412139",
      "cited_paper_id": 3416874
    },
    {
      "context_text": "[9] use the synchronized images to perform object detection and tracking.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.0,
      "reasoning": "The context does not mention any specific dataset names, only a general reference to 'synchronized images'. No clear, verifiable resource is identified.",
      "processing_time": 37.81581687927246,
      "citing_paper_id": "4412139",
      "cited_paper_id": 3845250
    },
    {
      "context_text": "While both sets of results look visually reasonable, we can see that our method suffers less from foreground fattening [6] (e.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method or finding. There are no clear identifiers for datasets in the given context.",
      "processing_time": 38.37797451019287,
      "citing_paper_id": "4412139",
      "cited_paper_id": 5535646
    },
    {
      "context_text": "This is similar to the tri-state logic error function presented in [7], except we explicitly do not reward pixels that are both 0 in the intersection term, as we only want to capture associations between events, and not between pixels without events.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method or approach. The context is focused on a specific aspect of the method, not on data.",
      "processing_time": 38.17282199859619,
      "citing_paper_id": "4412139",
      "cited_paper_id": 11177597
    },
    {
      "context_text": "However, the works in [19], [5] and [14] show that motion blur can be removed from an event image if the optical flow for each pixel is known.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only a methodological approach involving optical flow and event images.",
      "processing_time": 37.393178939819336,
      "citing_paper_id": "4412139",
      "cited_paper_id": 13360027
    },
    {
      "context_text": "However, the works in [19], [5] and [14] show that motion blur can be removed from an event image if the optical flow for each pixel is known.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only a methodological approach involving optical flow and event images.",
      "processing_time": 37.393178939819336,
      "citing_paper_id": "4412139",
      "cited_paper_id": 30723444
    },
    {
      "context_text": "[14] use a state estimator with pose and sparse depths to generate ‘motion compensated’ event images, on which they perform feature tracking.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for generating 'motion compensated' event images.",
      "processing_time": 36.381683111190796,
      "citing_paper_id": "4412139",
      "cited_paper_id": 30723444
    },
    {
      "context_text": "[2] use local spatial information in the form of local Gabor filters as features, while in [3], they track clusters of events to aid in tracking with occlusion.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and approaches. The context focuses on the use of local spatial information and event clustering for visual tracking.",
      "processing_time": 39.336108922958374,
      "citing_paper_id": "4412139",
      "cited_paper_id": 51614974
    },
    {
      "context_text": "DDD17 [35] and DDD20 [36] are datasets comprising many hours of driving data from a monochrome DAVIS346 camera featuring various vehicle control data.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "DDD17",
        "DDD20"
      ],
      "dataset_descriptions": {
        "DDD17": "Used to collect driving data from a monochrome DAVIS346 camera, featuring various vehicle control data for event-based stereo depth estimation.",
        "DDD20": "Used to collect driving data from a monochrome DAVIS346 camera, featuring various vehicle control data for event-based stereo depth estimation."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions two specific datasets, DDD17 and DDD20, which are used for driving data collected from a monochrome DAVIS346 camera. These datasets are clearly identified and described.",
      "processing_time": 55.662184715270996,
      "citing_paper_id": "232170230",
      "cited_paper_id": 396580
    },
    {
      "context_text": "Brisbane-Event-GEHRIG ADD [32] 0.1 - - 2D Bounding Box 1MP Detection [33] 4 - yes 0.9 - - 2D Bounding Box N-CARS [34] 0.01 - - Binary Class DDD17 [35] 0.1 - no 0.1 - - Vehicle Control, GPS DDD20 [36] 0.1 - no 0.1 - - Vehicle Control, GPS DET [37] 1.0 - - Lane Extraction Brisbane-Event-VPR [38] or not applicable respectively.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "DDD17"
      ],
      "dataset_descriptions": {
        "DDD17": "Used for end-to-end driving tasks, specifically vehicle control and GPS, with event-based data from the DAVIS sensor."
      },
      "confidence_score": 0.8,
      "reasoning": "The citation mentions several datasets, including DDD17, which is confirmed by the cited paper title. Other names like Brisbane-Event-GEHRIG, N-CARS, and DET are also mentioned but lack specific context or disambiguation.",
      "processing_time": 48.84475755691528,
      "citing_paper_id": "232170230",
      "cited_paper_id": 396580
    },
    {
      "context_text": "We apply a high-quality demosaicing algorithm [39] before mapping the debayered image to 8 bits.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific dataset, only a demosaicing algorithm. The cited paper title confirms it is a method, not a dataset.",
      "processing_time": 38.972442626953125,
      "citing_paper_id": "232170230",
      "cited_paper_id": 3013314
    },
    {
      "context_text": "To the best of our knowledge, MVSEC [2] is the only dataset with a stereo event camera in driving scenarios.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "MVSEC"
      ],
      "dataset_descriptions": {
        "MVSEC": "Used to evaluate event-based stereo depth estimation methods in driving scenarios, providing a unique dataset with stereo event cameras."
      },
      "confidence_score": 1.0,
      "reasoning": "The citation mentions MVSEC as the only dataset with a stereo event camera in driving scenarios, which is directly relevant to the research topic of event-based stereo depth estimation.",
      "processing_time": 44.401201486587524,
      "citing_paper_id": "232170230",
      "cited_paper_id": 3416874
    },
    {
      "context_text": "Different from MVSEC, however, we provide event camera data with 3 times higher resolution from two Prophesee Gen 3.1 sensors with a baseline of 60 cm similar to KITTI 360 [5].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "MVSEC",
        "KITTI 360"
      ],
      "dataset_descriptions": {
        "MVSEC": "Used to compare with the new dataset, focusing on event camera data for 3D perception tasks, highlighting differences in resolution and sensor type.",
        "KITTI 360": "Referenced for baseline comparison, providing context for the sensor setup and resolution improvements in the new dataset."
      },
      "confidence_score": 1.0,
      "reasoning": "The citation mentions 'MVSEC' and 'KITTI 360', both of which are known datasets in the field of event-based vision and stereo depth estimation.",
      "processing_time": 53.63083076477051,
      "citing_paper_id": "232170230",
      "cited_paper_id": 3416874
    },
    {
      "context_text": "Overall, the evaluation suggests that stereo methods trained on MVSEC do not generalize to this dataset and must be retrained to reach acceptable performance.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "MVSEC"
      ],
      "dataset_descriptions": {
        "MVSEC": "Used to train stereo methods for 3D perception, focusing on the generalization of these methods to event-based cameras. The dataset highlights the need for retraining to achieve acceptable performance."
      },
      "confidence_score": 1.0,
      "reasoning": "The citation mentions 'MVSEC' as a dataset used for training stereo methods, which is relevant to event-based stereo depth estimation.",
      "processing_time": 46.04689025878906,
      "citing_paper_id": "232170230",
      "cited_paper_id": 3416874
    },
    {
      "context_text": "The combination of signiﬁcantly higher camera resolution and larger baseline therefore addresses one of the main limitations of the driving sequences of the MVSEC dataset.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "MVSEC"
      ],
      "dataset_descriptions": {
        "MVSEC": "Used to highlight limitations in camera resolution and baseline for 3D perception tasks, specifically addressing these issues in the current research."
      },
      "confidence_score": 1.0,
      "reasoning": "The citation mentions the MVSEC dataset, which is a specific dataset used for 3D perception with event cameras. The context indicates that the dataset has limitations in camera resolution and baseline, which are addressed in the current research.",
      "processing_time": 46.93839430809021,
      "citing_paper_id": "232170230",
      "cited_paper_id": 3416874
    },
    {
      "context_text": "A signiﬁcant drawback of MVSEC for stereo algorithms is the small resolution of the DAVIS346 cameras ( 0 .",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "MVSEC"
      ],
      "dataset_descriptions": {
        "MVSEC": "Used to highlight limitations in event-based stereo algorithms, specifically the low resolution of the DAVIS346 cameras, impacting 3D perception accuracy."
      },
      "confidence_score": 0.8,
      "reasoning": "The citation mentions MVSEC, which is a dataset for event-based stereo vision, but it focuses on a limitation of the dataset rather than its usage.",
      "processing_time": 45.55600595474243,
      "citing_paper_id": "232170230",
      "cited_paper_id": 3416874
    },
    {
      "context_text": "MVSEC and this work are the only dataset with a stereo camera setup.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "MVSEC"
      ],
      "dataset_descriptions": {
        "MVSEC": "Used to provide a stereo camera setup for 3D perception, specifically for event-based stereo depth estimation, enhancing the evaluation of algorithms in dynamic environments."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions MVSEC as a dataset with a stereo camera setup, which is relevant to event-based stereo depth estimation.",
      "processing_time": 44.38510799407959,
      "citing_paper_id": "232170230",
      "cited_paper_id": 3416874
    },
    {
      "context_text": "What our dataset has in common with MVSEC is the LIDAR sensor.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "MVSEC"
      ],
      "dataset_descriptions": {
        "MVSEC": "Used for 3D perception tasks, specifically comparing LIDAR sensor data with event camera data to enhance depth estimation."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions 'MVSEC' which is a specific dataset name. The cited paper title confirms it is a dataset used for 3D perception, making it highly relevant.",
      "processing_time": 45.313931941986084,
      "citing_paper_id": "232170230",
      "cited_paper_id": 3416874
    },
    {
      "context_text": "Closest to our work is the MVSEC dataset [2].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "MVSEC"
      ],
      "dataset_descriptions": {
        "MVSEC": "Used to evaluate event-based stereo depth estimation methods, providing event camera data for 3D perception tasks."
      },
      "confidence_score": 1.0,
      "reasoning": "The citation mentions the MVSEC dataset, which is a specific, verifiable dataset used for 3D perception with event cameras.",
      "processing_time": 43.26503252983093,
      "citing_paper_id": "232170230",
      "cited_paper_id": 3416874
    },
    {
      "context_text": "Since then, a number of large scale datasets for semantic segmentation have been released [21], [22], [23], [24].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.0,
      "reasoning": "The context mentions 'large scale datasets for semantic segmentation' but does not specify any dataset names. The cited paper titles do not provide additional specific dataset names.",
      "processing_time": 38.50686287879944,
      "citing_paper_id": "232170230",
      "cited_paper_id": 3943983
    },
    {
      "context_text": "Since then, a number of large scale datasets for semantic segmentation have been released [21], [22], [23], [24].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.0,
      "reasoning": "The context mentions 'large scale datasets for semantic segmentation' but does not specify any dataset names. The cited paper titles do not provide additional specific dataset names.",
      "processing_time": 38.50686287879944,
      "citing_paper_id": "232170230",
      "cited_paper_id": 215754584
    },
    {
      "context_text": "Still, the majority of event-based stereo methods only estimate sparse or semi-dense depth [8], [9], [10], [11].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to other papers. No verifiable resources are identified.",
      "processing_time": 37.90434122085571,
      "citing_paper_id": "232170230",
      "cited_paper_id": 4412139
    },
    {
      "context_text": "Since then, a number of large scale datasets for semantic segmentation have been released [21]–[24].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.3,
      "reasoning": "The citation mentions 'large scale datasets for semantic segmentation' but does not specify any particular dataset names. The cited paper title suggests a specific dataset, but it is not explicitly mentioned in the citation context.",
      "processing_time": 41.232635259628296,
      "citing_paper_id": "232170230",
      "cited_paper_id": 5753855
    },
    {
      "context_text": "Following [3], [47], we evaluate methods on this dataset with the N pixel disparity error metric (DN ).",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.3,
      "reasoning": "The context mentions 'this dataset' but does not provide a specific name. The cited paper title suggests a high-resolution stereo dataset, but without a specific name, it cannot be included.",
      "processing_time": 40.206871032714844,
      "citing_paper_id": "232170230",
      "cited_paper_id": 14915763
    },
    {
      "context_text": "The fusion of frames and events has proven to be beneficial in several prior works [13]–[17].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only references to prior works that have shown benefits of fusing frames and events.",
      "processing_time": 38.48497653007507,
      "citing_paper_id": "232170230",
      "cited_paper_id": 50775406
    },
    {
      "context_text": "our baseline implementation uses the voxel grid representation [49] that contains events within a duration of 50 milliseconds.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific dataset names, only a method or representation. The context focuses on the voxel grid representation for event data, which is not a dataset.",
      "processing_time": 39.26787304878235,
      "citing_paper_id": "232170230",
      "cited_paper_id": 56475917
    },
    {
      "context_text": "The fusion of frames and events has proven to be beneficial in several prior works [13], [14], [15], [16], [17].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to prior works. There are no clear identifiers for datasets, models, or methods.",
      "processing_time": 39.260735511779785,
      "citing_paper_id": "232170230",
      "cited_paper_id": 59619729
    },
    {
      "context_text": "Oxford RobotCar [27] and KAIST Urban [28] target localization and mapping, while DDAD [29] and DrivingStereo [6] are specialized for monocular or stereo depth estimation respectively.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Oxford RobotCar",
        "KAIST Urban",
        "DDAD",
        "DrivingStereo"
      ],
      "dataset_descriptions": {
        "Oxford RobotCar": "Used for localization and mapping in diverse urban environments, providing multi-level sensor data for robust navigation systems.",
        "KAIST Urban": "Used for localization and mapping in urban settings, offering a rich dataset of sensor data for autonomous vehicle development.",
        "DDAD": "Specialized for monocular depth estimation, providing high-quality images and depth maps for training and evaluating depth estimation algorithms.",
        "DrivingStereo": "Specialized for stereo depth estimation, offering synchronized stereo image pairs and ground truth depth maps for accurate depth perception."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions specific datasets used for localization, mapping, and depth estimation. These datasets are clearly identified and their usage is described.",
      "processing_time": 61.86893916130066,
      "citing_paper_id": "232170230",
      "cited_paper_id": 145990403
    },
    {
      "context_text": "To address this issue we implement a filtering method similar to those employed by the KITTI depth [46] and DrivingStereo dataset [6]: Accumulate a local Lidar pointcloud for each view Generate a disparity map from the rectified image pair using SGM [42] which is not affected by moving objects.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "KITTI depth",
        "DrivingStereo"
      ],
      "dataset_descriptions": {
        "KITTI depth": "Used to implement a filtering method for generating disparity maps from rectified image pairs, focusing on sparsity invariant techniques.",
        "DrivingStereo": "Used to implement a filtering method for generating disparity maps from rectified image pairs, focusing on sparsity invariant techniques."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions two specific datasets, KITTI depth and DrivingStereo, which are used for implementing a filtering method for generating disparity maps.",
      "processing_time": 51.67424035072327,
      "citing_paper_id": "232170230",
      "cited_paper_id": 206429195
    },
    {
      "context_text": "Instead of capturing real-world data, Synthia [30] and [31] provide perfect ground truth in synthetic environments.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "SYNTHIA"
      ],
      "dataset_descriptions": {
        "SYNTHIA": "Used to provide synthetic environments with perfect ground truth for training and evaluation, enhancing the robustness of event-based stereo depth estimation algorithms."
      },
      "confidence_score": 0.9,
      "reasoning": "Synthia is identified as a dataset providing synthetic images for semantic segmentation, which can be relevant for event-based stereo depth estimation.",
      "processing_time": 44.962284326553345,
      "citing_paper_id": "232170230",
      "cited_paper_id": 206594095
    },
    {
      "context_text": "The Waymo open dataset [25] and nuScenes [26] instead focus mainly on object detection or tracking.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Waymo open dataset",
        "nuScenes"
      ],
      "dataset_descriptions": {
        "Waymo open dataset": "Used for object detection and tracking in autonomous driving, not directly for event-based stereo depth estimation.",
        "nuScenes": "Used for object detection and tracking in autonomous driving, not directly for event-based stereo depth estimation."
      },
      "confidence_score": 0.7,
      "reasoning": "The context mentions the Waymo open dataset and nuScenes, which are both specific datasets used in autonomous driving research. However, the context indicates they are not directly relevant to event-based stereo depth estimation.",
      "processing_time": 52.69906163215637,
      "citing_paper_id": "232170230",
      "cited_paper_id": 209140225
    },
    {
      "context_text": "This is the case for example for the older DAVIS cameras [18] but not for state-ofthe-art event cameras [19].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only types of cameras. No verifiable resources are identified.",
      "processing_time": 37.47656512260437,
      "citing_paper_id": "232170230",
      "cited_paper_id": 215799961
    },
    {
      "context_text": "In order to assess the difficulty of this dataset, the stereo method presented in [48] is evaluated on the test set.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.2,
      "reasoning": "The citation mentions evaluating a stereo method on a 'test set', but does not specify the name of the dataset. The context does not provide enough information to identify a specific, verifiable dataset.",
      "processing_time": 40.517611265182495,
      "citing_paper_id": "232170230",
      "cited_paper_id": 262638843
    },
    {
      "context_text": "WE EVALUATE THE METHOD PRESENTED IN [48] ON A TOTAL OF 12 SEQUENCES IN 3 DIFFERENT AREAS WITHIN SWITZERLAND.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.3,
      "reasoning": "The context mentions evaluating a method on sequences but does not specify a named dataset. The sequences are described generically without a specific identifier.",
      "processing_time": 39.08427333831787,
      "citing_paper_id": "232170230",
      "cited_paper_id": 262638843
    },
    {
      "context_text": "Inspired by convolutional RNNs [4, 29, 55] and continuous temporal dynamics of biological neuron models [18, 30], we propose a novel event processing method combining merits of both sides.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only references to methods and models. No verifiable resources are identified.",
      "processing_time": 38.444908618927,
      "citing_paper_id": "250602271",
      "cited_paper_id": 485828
    },
    {
      "context_text": "Inspired by convolutional RNNs [4, 29, 55] and continuous temporal dynamics of biological neuron models [18, 30], we propose a novel event processing method combining merits of both sides.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only references to methods and models. No verifiable resources are identified.",
      "processing_time": 38.444908618927,
      "citing_paper_id": "250602271",
      "cited_paper_id": 1753085
    },
    {
      "context_text": "However, fully-connected RNNs are not efficient for information extraction of images.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, models, or methods. It only discusses the inefficiency of fully-connected RNNs for image information extraction.",
      "processing_time": 39.75935173034668,
      "citing_paper_id": "250602271",
      "cited_paper_id": 485828
    },
    {
      "context_text": "An alternative encoding method for events is to use recurrent neural networks (RNNs), given their inherent ability to encode temporal sequences.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (RNNs) for encoding temporal sequences of events.",
      "processing_time": 38.25487995147705,
      "citing_paper_id": "250602271",
      "cited_paper_id": 485828
    },
    {
      "context_text": "Different embodiments of RCNN include convolutional long short-term memory (ConvLSTM) [55] and convolutional gated recurrent units (ConvGRU) [4], where additional gating variables were used for memory modulation.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models and methods. There are no verifiable resources that meet the criteria for inclusion.",
      "processing_time": 38.249993562698364,
      "citing_paper_id": "250602271",
      "cited_paper_id": 485828
    },
    {
      "context_text": "A natural thought is to incorporate RNNs into convolutional operations.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only a methodological idea about incorporating RNNs into convolutional operations.",
      "processing_time": 38.68157196044922,
      "citing_paper_id": "250602271",
      "cited_paper_id": 485828
    },
    {
      "context_text": "Different from traditional RNNs constructed with artificial neurons, spiking neural networks (SNNs) [30] uses spiking neuron models inspired by biology with inherent self-recurrence.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (spiking neural networks).",
      "processing_time": 37.6740837097168,
      "citing_paper_id": "250602271",
      "cited_paper_id": 485828
    },
    {
      "context_text": "Different from traditional RNNs constructed with artificial neurons, spiking neural networks (SNNs) [30] uses spiking neuron models inspired by biology with inherent self-recurrence.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (spiking neural networks).",
      "processing_time": 37.6740837097168,
      "citing_paper_id": "250602271",
      "cited_paper_id": 1753085
    },
    {
      "context_text": "There has been an increasing number of applications of SNNs in deep learning [5, 12, 21, 22, 28, 38, 45, 50, 54, 60], and the network’s asynchronous nature makes it an ideal solution for event-based tasks [7, 8, 23, 27, 41, 57].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general applications of SNNs and their suitability for event-based tasks. No verifiable resources are identified.",
      "processing_time": 40.141520738601685,
      "citing_paper_id": "250602271",
      "cited_paper_id": 1234009
    },
    {
      "context_text": "There has been an increasing number of applications of SNNs in deep learning [5, 12, 21, 22, 28, 38, 45, 50, 54, 60], and the network’s asynchronous nature makes it an ideal solution for event-based tasks [7, 8, 23, 27, 41, 57].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general applications of SNNs and their suitability for event-based tasks. No verifiable resources are identified.",
      "processing_time": 40.141520738601685,
      "citing_paper_id": "250602271",
      "cited_paper_id": 203593170
    },
    {
      "context_text": "There has been an increasing number of applications of SNNs in deep learning [5, 12, 21, 22, 28, 38, 45, 50, 54, 60], and the network’s asynchronous nature makes it an ideal solution for event-based tasks [7, 8, 23, 27, 41, 57].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general applications of SNNs and their suitability for event-based tasks. No verifiable resources are identified.",
      "processing_time": 40.141520738601685,
      "citing_paper_id": "250602271",
      "cited_paper_id": 211258776
    },
    {
      "context_text": "There has been an increasing number of applications of SNNs in deep learning [5, 12, 21, 22, 28, 38, 45, 50, 54, 60], and the network’s asynchronous nature makes it an ideal solution for event-based tasks [7, 8, 23, 27, 41, 57].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general applications of SNNs and their suitability for event-based tasks. No verifiable resources are identified.",
      "processing_time": 40.141520738601685,
      "citing_paper_id": "250602271",
      "cited_paper_id": 213704910
    },
    {
      "context_text": "There has been an increasing number of applications of SNNs in deep learning [5, 12, 21, 22, 28, 38, 45, 50, 54, 60], and the network’s asynchronous nature makes it an ideal solution for event-based tasks [7, 8, 23, 27, 41, 57].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general applications of SNNs and their suitability for event-based tasks. No verifiable resources are identified.",
      "processing_time": 40.141520738601685,
      "citing_paper_id": "250602271",
      "cited_paper_id": 226976144
    },
    {
      "context_text": "There has been an increasing number of applications of SNNs in deep learning [5, 12, 21, 22, 28, 38, 45, 50, 54, 60], and the network’s asynchronous nature makes it an ideal solution for event-based tasks [7, 8, 23, 27, 41, 57].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general applications of SNNs and their suitability for event-based tasks. No verifiable resources are identified.",
      "processing_time": 40.141520738601685,
      "citing_paper_id": "250602271",
      "cited_paper_id": 247675601
    },
    {
      "context_text": "[39] applied a modified version of LSTM [19] to event-based recognition, but the model was not specifically designed to preserve spatial information.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (LSTM) applied to event-based recognition. No datasets are referenced for use in the research.",
      "processing_time": 39.36950993537903,
      "citing_paper_id": "250602271",
      "cited_paper_id": 1561703
    },
    {
      "context_text": "To our best knowledge, our models are the first to perform streaming experiments for dense disparity estimation on the MVSEC dataset.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "MVSEC"
      ],
      "dataset_descriptions": {
        "MVSEC": "Used for streaming experiments in dense disparity estimation, focusing on 3D perception with event cameras."
      },
      "confidence_score": 1.0,
      "reasoning": "The citation mentions the MVSEC dataset, which is a specific dataset used for 3D perception with event cameras. The context indicates it is used for dense disparity estimation in streaming experiments.",
      "processing_time": 45.21266555786133,
      "citing_paper_id": "250602271",
      "cited_paper_id": 3416874
    },
    {
      "context_text": "We demonstrated the superiority of our model over existing state-of-the-art works on both the MVSEC and the DSEC dataset.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "MVSEC",
        "DSEC"
      ],
      "dataset_descriptions": {
        "MVSEC": "Used to evaluate the model's performance in 3D perception tasks, focusing on stereo event camera data.",
        "DSEC": "Used to assess the model's effectiveness in depth estimation, specifically with dynamic and static scenes captured by event cameras."
      },
      "confidence_score": 1.0,
      "reasoning": "The citation mentions two specific datasets, MVSEC and DSEC, which are used to evaluate the performance of the model.",
      "processing_time": 48.712135553359985,
      "citing_paper_id": "250602271",
      "cited_paper_id": 3416874
    },
    {
      "context_text": "We demonstrated their advantages over other methods on the event-based stereo matching task on the MVSEC dataset.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "MVSEC"
      ],
      "dataset_descriptions": {
        "MVSEC": "Used to evaluate event-based stereo matching methods, focusing on 3D perception tasks with event camera data."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions the use of the MVSEC dataset for evaluating event-based stereo matching methods. The dataset is clearly identified and relevant to the research topic.",
      "processing_time": 44.27641201019287,
      "citing_paper_id": "250602271",
      "cited_paper_id": 3416874
    },
    {
      "context_text": "We split and preprocess the Indoor Flying dataset from the MVSEC using the same setting as [2, 51, 62].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Indoor Flying dataset",
        "MVSEC"
      ],
      "dataset_descriptions": {
        "Indoor Flying dataset": "Used for preprocessing and splitting in event-based stereo depth estimation, focusing on indoor flying scenarios with event cameras.",
        "MVSEC": "Used for preprocessing and splitting in event-based stereo depth estimation, providing a multivehicle stereo event camera dataset for 3D perception."
      },
      "confidence_score": 0.9,
      "reasoning": "The context mentions the 'Indoor Flying dataset' from the 'MVSEC'. The cited papers confirm that MVSEC is a dataset, and the context indicates it is used for preprocessing and splitting.",
      "processing_time": 53.86129093170166,
      "citing_paper_id": "250602271",
      "cited_paper_id": 3416874
    },
    {
      "context_text": "We split and preprocess the Indoor Flying dataset from the MVSEC using the same setting as [2, 51, 62].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Indoor Flying dataset",
        "MVSEC"
      ],
      "dataset_descriptions": {
        "Indoor Flying dataset": "Used for preprocessing and splitting in event-based stereo depth estimation, focusing on indoor flying scenarios with event cameras.",
        "MVSEC": "Used for preprocessing and splitting in event-based stereo depth estimation, providing a multivehicle stereo event camera dataset for 3D perception."
      },
      "confidence_score": 0.9,
      "reasoning": "The context mentions the 'Indoor Flying dataset' from the 'MVSEC'. The cited papers confirm that MVSEC is a dataset, and the context indicates it is used for preprocessing and splitting.",
      "processing_time": 53.86129093170166,
      "citing_paper_id": "250602271",
      "cited_paper_id": 4412139
    },
    {
      "context_text": "We conduct our experiments on the MVSEC dataset [63], which contains depth information collected by a LIDAR sensor and event streams obtained from two event cameras with corresponding 20 Hz intensity images at 346×260 resolution.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "MVSEC"
      ],
      "dataset_descriptions": {
        "MVSEC": "Used to conduct experiments on event-based stereo depth estimation, leveraging depth information from LIDAR and event streams from two event cameras with 20 Hz intensity images."
      },
      "confidence_score": 1.0,
      "reasoning": "The context clearly mentions the MVSEC dataset, which is used for experiments involving depth information and event streams from event cameras.",
      "processing_time": 45.59205865859985,
      "citing_paper_id": "250602271",
      "cited_paper_id": 3416874
    },
    {
      "context_text": "We demonstrate the advantage of CTC and DTC over other event encoding methods on a set of criteria of event-based stereo matching, on the Multi Vehicle Stereo Event Camera (MVSEC) [63] dataset.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Multi Vehicle Stereo Event Camera (MVSEC)"
      ],
      "dataset_descriptions": {
        "Multi Vehicle Stereo Event Camera (MVSEC)": "Used to evaluate event-based stereo matching methods, focusing on criteria such as accuracy and efficiency in 3D perception tasks."
      },
      "confidence_score": 1.0,
      "reasoning": "The MVSEC dataset is explicitly mentioned and is used for evaluating event-based stereo matching methods.",
      "processing_time": 45.1958909034729,
      "citing_paper_id": "250602271",
      "cited_paper_id": 3416874
    },
    {
      "context_text": "It also decreases data storage space for potential hardware applications, similar approaches were taken in [62].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general approach to reducing data storage space.",
      "processing_time": 37.79810047149658,
      "citing_paper_id": "250602271",
      "cited_paper_id": 4412139
    },
    {
      "context_text": "TSES [62] utilized the velocity of the camera to approximate optical flow and build time synchronized event disparity volumes.",
      "catation_intent": "reusable resource",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.3,
      "reasoning": "The citation mentions 'TSES' which appears to be a dataset or method used for building time synchronized event disparity volumes. However, without more context, it is unclear if TSES is a dataset or a method.",
      "processing_time": 41.43238043785095,
      "citing_paper_id": "250602271",
      "cited_paper_id": 4412139
    },
    {
      "context_text": "The so-called handcrafted methods [25, 31, 35, 36, 40, 53, 58, 64] directly convert events to event frames based on the four dimensional information of each event.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods for converting events to event frames. No verifiable resources are identified.",
      "processing_time": 39.16650032997131,
      "citing_paper_id": "250602271",
      "cited_paper_id": 13373696
    },
    {
      "context_text": "The so-called handcrafted methods [25, 31, 35, 36, 40, 53, 58, 64] directly convert events to event frames based on the four dimensional information of each event.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods for converting events to event frames. No verifiable resources are identified.",
      "processing_time": 39.16650032997131,
      "citing_paper_id": "250602271",
      "cited_paper_id": 52814827
    },
    {
      "context_text": "[25, 31] stored histograms of events of different polarities in different channels to avoid information loss due to polarity cancellation.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific dataset names, only a method for storing event histograms. No clear, verifiable dataset is identified.",
      "processing_time": 38.38121485710144,
      "citing_paper_id": "250602271",
      "cited_paper_id": 13373696
    },
    {
      "context_text": "An extension of our principles to SNNs implemented in neuromorphic hardware [10, 11, 14, 24, 33, 43] could further lead to super fast event-based stereo system [3].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to neuromorphic hardware and spiking neural networks. No verifiable datasets are identified.",
      "processing_time": 40.092737674713135,
      "citing_paper_id": "250602271",
      "cited_paper_id": 25268038
    },
    {
      "context_text": "An extension of our principles to SNNs implemented in neuromorphic hardware [10, 11, 14, 24, 33, 43] could further lead to super fast event-based stereo system [3].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to neuromorphic hardware and spiking neural networks. No verifiable datasets are identified.",
      "processing_time": 40.092737674713135,
      "citing_paper_id": "250602271",
      "cited_paper_id": 119297695
    },
    {
      "context_text": "An extension of our principles to SNNs implemented in neuromorphic hardware [10, 11, 14, 24, 33, 43] could further lead to super fast event-based stereo system [3].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to neuromorphic hardware and spiking neural networks. No verifiable datasets are identified.",
      "processing_time": 40.092737674713135,
      "citing_paper_id": "250602271",
      "cited_paper_id": 235078812
    },
    {
      "context_text": "[65] created voxel grids by interpolation based on timestamps of events.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific dataset names, only a method for creating voxel grids from event timestamps.",
      "processing_time": 37.387091636657715,
      "citing_paper_id": "250602271",
      "cited_paper_id": 56475917
    },
    {
      "context_text": "Most of the works in stereo matching based on deep learning are established on image datasets [20, 32, 52, 56, 59].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions 'image datasets' but does not specify any particular dataset names. The cited paper titles do not provide additional dataset names either.",
      "processing_time": 39.47923302650452,
      "citing_paper_id": "250602271",
      "cited_paper_id": 119304432
    },
    {
      "context_text": "Most of the works in stereo matching based on deep learning are established on image datasets [20, 32, 52, 56, 59].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions 'image datasets' but does not specify any particular dataset names. The cited paper titles do not provide additional dataset names either.",
      "processing_time": 39.47923302650452,
      "citing_paper_id": "250602271",
      "cited_paper_id": 216036364
    },
    {
      "context_text": "The model can be viewed as a non-spiking form of the leaky integrate and fire (LIF) neuron with conductance synapse [44], without specifically defining its synaptic dynamics.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a model or method. There are no verifiable resources that meet the criteria for inclusion.",
      "processing_time": 39.67991638183594,
      "citing_paper_id": "250602271",
      "cited_paper_id": 196016124
    },
    {
      "context_text": "[9] used time-surface with linear time decay to construct event images.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for constructing event images.",
      "processing_time": 36.62386465072632,
      "citing_paper_id": "250602271",
      "cited_paper_id": 211126617
    },
    {
      "context_text": "[34, 47] applied specially designed asynchronous convolution for sparse events data.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific dataset names, only a method for processing sparse events data.",
      "processing_time": 37.758089542388916,
      "citing_paper_id": "250602271",
      "cited_paper_id": 214605597
    },
    {
      "context_text": "LTC further enhances its ability by integrating f into the time constant of the system:\ndx(t)\ndt = −\n[ 1\nτ + f(x(t), I(t), t, θ)\n] x(t)+\nf(x(t), I(t), t, θ)A\n(3)\nwhere the system time constant becomes an inputdependent term τ1+τf(x(t),I(t),t,θ) and A is a scale parameter.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The provided context does not mention any specific datasets, only a mathematical model and equations. There are no verifiable resources or datasets mentioned.",
      "processing_time": 39.45466113090515,
      "citing_paper_id": "250602271",
      "cited_paper_id": 222319014
    },
    {
      "context_text": "The liquid time-constant network (LTC) [18,26], an expansion of the continuous time RNN [13], circumvents this problem by using continuous valued activation functions for its neuron, whose dynamics is modulated by an inputdependent system time constant.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models and methods. The context is focused on describing the LTC network and its relation to continuous time RNNs.",
      "processing_time": 40.610079288482666,
      "citing_paper_id": "250602271",
      "cited_paper_id": 222319014
    },
    {
      "context_text": "We term both convolution LTC (convLTC) and convolution LTC without reversal potential (convLTCOR) as continuous time convolution (CTC).",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only model architectures. There are no verifiable resources or datasets mentioned.",
      "processing_time": 38.972662687301636,
      "citing_paper_id": "250602271",
      "cited_paper_id": 222319014
    },
    {
      "context_text": "Note that for CTC, we use simulation results from the convLTCOR model.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions 'simulation results' but does not specify a dataset. The term 'convLTCOR model' is a method, not a dataset.",
      "processing_time": 40.400232791900635,
      "citing_paper_id": "250602271",
      "cited_paper_id": 222319014
    },
    {
      "context_text": "The dynamics of the convLTCOR model is mainly characterized by its membrane time constant τm, abstracted from this intuition, we develop the discrete time convolution model (DTC), formulated as:\nxtcij = σ(τcx t−1 cij + Icij(t)) (9)\nwhere Icij(t) is defined the same as in Eq.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The provided context does not mention any specific datasets, only a model formulation. There are no clear identifiers for datasets or other verifiable resources.",
      "processing_time": 39.81900668144226,
      "citing_paper_id": "250602271",
      "cited_paper_id": 222319014
    },
    {
      "context_text": "The LTC network [18] is an expansion of continuoustime RNN (CT-RNN) [13], which can be described by an ordinary differential equation (ODE):\ndx(t) dt = −x(t) τ + f(x(t), I(t), t, θ) (2)\nwhere τ characterizes the speed and the coupling sensitivity of the dynamical system, x(t) is the hidden state, I(t) is the input, t represents time and f is a neural network parameterized by θ.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models and equations. There are no verifiable resources that meet the criteria.",
      "processing_time": 39.27693796157837,
      "citing_paper_id": "250602271",
      "cited_paper_id": 222319014
    },
    {
      "context_text": "In the fully connected structure, the synaptic input of an LTC neuron contains inputs from all the other neurons.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general description of a neural network structure.",
      "processing_time": 37.33519101142883,
      "citing_paper_id": "250602271",
      "cited_paper_id": 222319014
    },
    {
      "context_text": "The LTC network was only applied for low dimensional temporal sequence modeling.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (LTC network) and its application. There are no verifiable resources or datasets mentioned.",
      "processing_time": 40.58322715759277,
      "citing_paper_id": "250602271",
      "cited_paper_id": 222319014
    },
    {
      "context_text": "Empirically we found that the training of the convLTC model was unstable, during which gradients sometimes tended to vanish.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (convLTC model).",
      "processing_time": 38.56997776031494,
      "citing_paper_id": "250602271",
      "cited_paper_id": 222319014
    },
    {
      "context_text": "The resulting convolution LTC neuron and its simplified version can be formulated as:\ndxcij(t)\ndt =−\n[ 1\nτm,c +\nIcij(t)\nCm,c\n] xcij(t)\n+ Icij(t)\nCm,c Erev,c + Eleak,c τm,c\n(6)\ndxcij(t)\ndt = Eleak,c − xcij(t) τm,c + Icij(t) Cm,c (7) Icij(t) = ∑ h ∑ k wchkP t h+i,k+j (8)\nwith Wg(t) in the previous section specified by Icij(t), which represents the convolution input on channel c at location i, j from the event frame pre-processed by SBT, h and k are spatial coordinates on the input plane.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only mathematical formulations and neural circuit policies. No verifiable resources are identified.",
      "processing_time": 39.425954818725586,
      "citing_paper_id": "250602271",
      "cited_paper_id": 222319014
    },
    {
      "context_text": "However, the LTC was only applied for low dimensional temporal sequence modeling and it lacks the ability to encode high dimensional spatial features.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (LTC) and its limitations. There are no verifiable resources or datasets mentioned.",
      "processing_time": 39.795050621032715,
      "citing_paper_id": "250602271",
      "cited_paper_id": 222319014
    },
    {
      "context_text": "The output of the LTC neuron is normalized by a parametrical sigmoid function σ(xcij) = 1/(1 + exp(γc(μc − xcij))), where γc and μc are trainable parameters that scale and shift xcij .",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only a mathematical function used in a neural network model.",
      "processing_time": 38.737927198410034,
      "citing_paper_id": "250602271",
      "cited_paper_id": 222319014
    },
    {
      "context_text": "In the original work, the LTC neuron was evolved at a frequency six times higher than the input sampling rate, leading to a six times slower output rate for an equal temporal span of the input.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only discusses the evolution of the LTC neuron in relation to input sampling rates.",
      "processing_time": 39.40977430343628,
      "citing_paper_id": "250602271",
      "cited_paper_id": 222319014
    },
    {
      "context_text": "We develop continuous time convolution (CTC), an expansion of LTC, for encoding high dimensional spatial-temporal data.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (CTC) for processing high-dimensional spatial-temporal data.",
      "processing_time": 39.409440755844116,
      "citing_paper_id": "250602271",
      "cited_paper_id": 222319014
    },
    {
      "context_text": "Inspired by recent studies [2,6,42,48], we further develop a dual-path structure for feature embedding fused by SPADE with multi-scale dilated convolution.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and models. The context focuses on the development of a dual-path structure for feature embedding.",
      "processing_time": 40.35507798194885,
      "citing_paper_id": "250602271",
      "cited_paper_id": 231759393
    },
    {
      "context_text": "The camera intrinsics and extrinsics are estimated using a grid of AprilTags [35] that is moved in front of the sensor rig and calibrated using Kalibr.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions AprilTags but does not refer to it as a dataset. It is used as a method for estimating camera intrinsics and extrinsics.",
      "processing_time": 40.552778482437134,
      "citing_paper_id": "3416874",
      "cited_paper_id": 277804
    },
    {
      "context_text": "[5] provide a large dataset of a DAVIS 346B mounted behind the windshield of a car, with 12 hours of driving, intended for end to end learning of various driving related tasks.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "DDD17"
      ],
      "dataset_descriptions": {
        "DDD17": "Used to provide a large dataset of a DAVIS 346B sensor mounted in a car, capturing 12 hours of driving data for end-to-end learning of driving-related tasks."
      },
      "confidence_score": 1.0,
      "reasoning": "The citation mentions a specific dataset with a clear name and purpose, which is relevant to the topic of event-based stereo depth estimation.",
      "processing_time": 47.75978684425354,
      "citing_paper_id": "3416874",
      "cited_paper_id": 396580
    },
    {
      "context_text": "In [15], the authors propose a novel context descriptor to perform matching, and the authors in [16] use a stereo event camera undergoing pure rotation to perform depth estimation and panoramic stitching.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and approaches. The context is about stereo depth estimation using event cameras, but no dataset names are provided.",
      "processing_time": 41.01695680618286,
      "citing_paper_id": "3416874",
      "cited_paper_id": 1408596
    },
    {
      "context_text": "Later works in [8], [9] and [10] have adapted cooperative methods for stereo depth to event based cameras, due to their applicability to asynchronous, point based measurements.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only cooperative methods adapted for stereo depth estimation with event-based cameras.",
      "processing_time": 39.38590431213379,
      "citing_paper_id": "3416874",
      "cited_paper_id": 7151414
    },
    {
      "context_text": "The authors in [17] and [18] proposed novel methods to perform feature tracking in the event space, which they extended in [19] and [20] to perform visual and visual inertial odometry, respectively.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods for feature tracking and odometry.",
      "processing_time": 37.88115572929382,
      "citing_paper_id": "3416874",
      "cited_paper_id": 7884141
    },
    {
      "context_text": "Early works in [6], [7] present stereo depth estimation results with a number of spatial and temporal costs.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only general methods and results. No clear, verifiable resource names are provided.",
      "processing_time": 39.983463764190674,
      "citing_paper_id": "3416874",
      "cited_paper_id": 11177597
    },
    {
      "context_text": "The transformation that takes a point from the lidar frame to the left DAVIS frame was initially calibrated using the Camera and Range Calibration Toolbox [33].",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions a toolbox, which is a method or tool, not a dataset. No specific dataset is mentioned.",
      "processing_time": 38.6970329284668,
      "citing_paper_id": "3416874",
      "cited_paper_id": 12339854
    },
    {
      "context_text": "lidar are calibrated using the Camera and Range Calibration Toolbox4 [33], and fine tuned manually, and the hand eye calibration between the mocap model pose in the motion capture",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only tools and methods for calibration. There are no clear identifiers for datasets in the provided context.",
      "processing_time": 40.52195882797241,
      "citing_paper_id": "3416874",
      "cited_paper_id": 12339854
    },
    {
      "context_text": "The camera intrinsics, stereo extrinsics, and camera-IMU extrinsics are calibrated using the Kalibr toolbox3 [30], [31], [32], the extrinsics between the left DAVIS camera and Velodyne lidar are calibrated using the Camera and Range Calibration Toolbox4 [33], and fine tuned manually, and the hand eye calibration between the mocap model pose in the motion capture world frame and the left DAVIS camera pose is performed using CamOdoCal5 [34].",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only tools and methods for calibration. No verifiable datasets are referenced.",
      "processing_time": 39.73869705200195,
      "citing_paper_id": "3416874",
      "cited_paper_id": 12339854
    },
    {
      "context_text": "The camera intrinsics, stereo extrinsics, and camera-IMU extrinsics are calibrated using the Kalibr toolbox3 [30], [31], [32], the extrinsics between the left DAVIS camera and Velodyne",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only tools and methods for calibration. No verifiable datasets are referenced.",
      "processing_time": 39.733938694000244,
      "citing_paper_id": "3416874",
      "cited_paper_id": 15778738
    },
    {
      "context_text": "The camera intrinsics, stereo extrinsics, and camera-IMU extrinsics are calibrated using the Kalibr toolbox3 [30], [31], [32], the extrinsics between the left DAVIS camera and Velodyne",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only tools and methods for calibration. No verifiable datasets are referenced.",
      "processing_time": 39.733938694000244,
      "citing_paper_id": "3416874",
      "cited_paper_id": 120110206
    },
    {
      "context_text": "Similarly, [11] and [12] apply a set of temporal, epipolar, ordering and polarity constraints to determine matches, while [13] compare this with matching based on the output of a bank of orientation filters.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and approaches. The context focuses on comparing different constraints and filters for stereo matching.",
      "processing_time": 40.70916223526001,
      "citing_paper_id": "3416874",
      "cited_paper_id": 17693733
    },
    {
      "context_text": "For each sequence with lidar measurements, we run the Lidar Odometry and Mapping (LOAM) algorithm [29] to generate dense 3D local maps, which are projected into each DAVIS camera to generate dense depth images at 20 Hz, and to provide 3D pose for the handheld sequences.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only the use of the LOAM algorithm for generating dense 3D local maps and depth images. No verifiable dataset names are provided.",
      "processing_time": 42.130236864089966,
      "citing_paper_id": "3416874",
      "cited_paper_id": 18612391
    },
    {
      "context_text": "The authors in [21] use events to estimate angular velocity of a camera, while [22] and [23] perform visual odometry by building an up to a scale map.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and approaches. The context focuses on the use of event cameras for various tasks, but no dataset names are provided.",
      "processing_time": 41.76631307601929,
      "citing_paper_id": "3416874",
      "cited_paper_id": 26324573
    },
    {
      "context_text": "• Images and IMU measurements from the VI Sensor.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions 'Images and IMU measurements from the VI Sensor' but does not specify a named dataset. The VI Sensor is likely a device or sensor setup rather than a dataset.",
      "processing_time": 42.30558633804321,
      "citing_paper_id": "3416874",
      "cited_paper_id": 109416659
    },
    {
      "context_text": "The timestamps for the VI Sensor messages in the dataset are then modified to compensate for this offset.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.3,
      "reasoning": "The context mentions a dataset but does not specify a name. The reference to 'the dataset' is too generic and lacks a specific identifier.",
      "processing_time": 40.8154399394989,
      "citing_paper_id": "3416874",
      "cited_paper_id": 109416659
    },
    {
      "context_text": "In comparison, this dataset provides event streams from two synchronized and calibrated Dynamic Vision and Active Pixel Sensors (DAVISm346b), with long indoor and outdoor sequences in a variety of illuminations and speeds, along with accurate depth images and pose at up to 100Hz, generated from a lidar system rigidly mounted on top of the cameras, as in Fig 1, along with motion capture and GPS.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "DAVISm346b"
      ],
      "dataset_descriptions": {
        "DAVISm346b": "Used to provide event streams from synchronized and calibrated sensors, including long indoor and outdoor sequences with various illuminations and speeds, depth images, and pose data at up to 100Hz."
      },
      "confidence_score": 0.9,
      "reasoning": "The context describes a specific dataset with detailed characteristics, including event streams, depth images, and pose data from synchronized sensors.",
      "processing_time": 50.24514055252075,
      "citing_paper_id": "3416874",
      "cited_paper_id": 109416659
    },
    {
      "context_text": "In addition, we have mounted a VI Sensor [27], originally",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific dataset, only a sensor system. No verifiable dataset names are present.",
      "processing_time": 39.541959285736084,
      "citing_paper_id": "3416874",
      "cited_paper_id": 109416659
    },
    {
      "context_text": "In addition, we calibrate the temporal offset between the DAVIS stereo pair and the VI Sensor by finding the temporal offset that maximizes the cross correlation between the magnitude of the gyroscope angular velocities from the IMUs of the left DAVIS and the VI Sensor.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only sensors and their synchronization. No verifiable dataset names are present.",
      "processing_time": 39.53532862663269,
      "citing_paper_id": "3416874",
      "cited_paper_id": 109416659
    },
    {
      "context_text": "In this section, we describe the various steps performed to calibrate the intrinsic parameters of each DAVIS and VI-Sensor camera, as well as the extrinsic transformations between each of the cameras, IMUs and the lidar.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only hardware components and calibration processes.",
      "processing_time": 38.64911651611328,
      "citing_paper_id": "3416874",
      "cited_paper_id": 109416659
    },
    {
      "context_text": "For high speed sequences, the DAVIS stereo rig and VI Sensor are mounted on the handlebar of a motorcycle (Fig 2d), along with the GPS device.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions specific hardware setups but does not refer to any named datasets. The context is about the setup for collecting data, not the use of a specific dataset.",
      "processing_time": 42.100754737854004,
      "citing_paper_id": "3416874",
      "cited_paper_id": 109416659
    },
    {
      "context_text": "In addition, we have mounted a VI Sensor [27], originally developed by Skybotix for comparison with frame based methods.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific, verifiable datasets. It only refers to a VI Sensor, which is a device, not a dataset.",
      "processing_time": 40.792842388153076,
      "citing_paper_id": "3416874",
      "cited_paper_id": 109416659
    },
    {
      "context_text": "In addition, [24] and [25] also fuse events with measurements from an IMU to perform visual inertial odometry.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods or approaches. The context focuses on the fusion of events with IMU measurements for visual inertial odometry.",
      "processing_time": 42.092522859573364,
      "citing_paper_id": "3416874",
      "cited_paper_id": 204780933
    },
    {
      "context_text": "This process is iterated until convergence to find the velocity best describing the initial inlier set found by fast MC-RANSAC, ¯ ϖ ← ¯ ϖ + δ ϖ ∗ .",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (MC-RANSAC) used for processing data. No verifiable datasets are referenced.",
      "processing_time": 41.39426136016846,
      "citing_paper_id": "260164484",
      "cited_paper_id": 837271
    },
    {
      "context_text": "The fast version of MC-RANSAC (Sec.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only a method (MC-RANSAC). There are no clear identifiers for datasets or other verifiable resources.",
      "processing_time": 41.92800688743591,
      "citing_paper_id": "260164484",
      "cited_paper_id": 837271
    },
    {
      "context_text": "This iterative process uses the velocities found during MC-RANSAC as an initial condition.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (MC-RANSAC).",
      "processing_time": 39.91069316864014,
      "citing_paper_id": "260164484",
      "cited_paper_id": 837271
    },
    {
      "context_text": "The inlier tracklets from MC-RANSAC are used to define the trajectory optimization problem (Fig.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (MC-RANSAC) used for trajectory optimization. No verifiable datasets are referenced.",
      "processing_time": 41.713356733322144,
      "citing_paper_id": "260164484",
      "cited_paper_id": 837271
    },
    {
      "context_text": "It also uses Motion-Compensated RANSAC (MC-RANSAC) [10] to consider the unique measurement times during outlier rejection and independently provide better tracklets and initial conditions for estimation.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (MC-RANSAC).",
      "processing_time": 40.119051456451416,
      "citing_paper_id": "260164484",
      "cited_paper_id": 837271
    },
    {
      "context_text": "The largest inlier set found is used as the initial segmentation for the full iterative MC-RANSAC.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (MC-RANSAC) and its application. No verifiable resources are identified.",
      "processing_time": 41.50074648857117,
      "citing_paper_id": "260164484",
      "cited_paper_id": 837271
    },
    {
      "context_text": "In contrast to these existing works, it maintains temporal resolution with either frame-based or event-based feature detection and tracking and with a RANSAC formulation [10] that separates outlier rejection from estimation.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (RANSAC) and general concepts. No verifiable resources are identified.",
      "processing_time": 41.494648456573486,
      "citing_paper_id": "260164484",
      "cited_paper_id": 837271
    },
    {
      "context_text": "Both versions of MC-RANSAC are compared to traditional RANSAC in [10].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a comparison between different versions of RANSAC algorithms.",
      "processing_time": 40.7554566860199,
      "citing_paper_id": "260164484",
      "cited_paper_id": 837271
    },
    {
      "context_text": "This process is repeated a user-specified number of times and then the largest inlier set is refined using the full iterative MC-RANSAC (Sec.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only a method (MC-RANSAC). There are no verifiable resources or datasets mentioned.",
      "processing_time": 41.89887881278992,
      "citing_paper_id": "260164484",
      "cited_paper_id": 837271
    },
    {
      "context_text": "This assumption is incorrect for asynchronous event tracklets and this paper instead uses MC-RANSAC [10], which makes no assumptions about common state times and uses a constant-velocity model in SE(3).",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (MC-RANSAC) used for processing asynchronous event tracklets.",
      "processing_time": 41.482367277145386,
      "citing_paper_id": "260164484",
      "cited_paper_id": 837271
    },
    {
      "context_text": "This assumption is incorrect for asynchronous event tracklets and this paper instead uses MC-RANSAC [10], which makes no assumptions about common state times and uses a constant-velocity model in SE (3) .",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (MC-RANSAC) used for processing asynchronous event tracklets.",
      "processing_time": 41.6852080821991,
      "citing_paper_id": "260164484",
      "cited_paper_id": 837271
    },
    {
      "context_text": "This is more accurate outlier rejection than using fast MC-RANSAC alone.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (RANSAC) for improving outlier rejection in 3D visual sensors.",
      "processing_time": 41.47709035873413,
      "citing_paper_id": "260164484",
      "cited_paper_id": 837271
    },
    {
      "context_text": "Outlier rejection was done with 10000 iterations of fast MC-RANSAC followed with one call of iterative MC-RANSAC, both using an inlier threshold of 5%.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for outlier rejection using RANSAC variants.",
      "processing_time": 41.19673752784729,
      "citing_paper_id": "260164484",
      "cited_paper_id": 837271
    },
    {
      "context_text": "MC-RANSAC segments these tracklets into inliers and outliers by finding the most tracklets that can be explained by a single velocity.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (MC-RANSAC) used for processing tracklets.",
      "processing_time": 41.19243836402893,
      "citing_paper_id": "260164484",
      "cited_paper_id": 837271
    },
    {
      "context_text": "2) Iterative MC-RANSAC: A more accurate iterative MC-RANSAC approach minimizes the reprojection error of each tracklet in image space with the cost function, where R j,k is the covariance matrix for the measurements of the j th tracklet and M fast is the number of inliers found by the fast MC-RANSAC.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (MC-RANSAC) and its application. No verifiable resources are identified.",
      "processing_time": 42.781115770339966,
      "citing_paper_id": "260164484",
      "cited_paper_id": 837271
    },
    {
      "context_text": "Traditional VO pipeline uses Random Sample Consensus (RANSAC) [33] to remove tracklet outliers before estimation.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (RANSAC) used in visual odometry.",
      "processing_time": 41.87084341049194,
      "citing_paper_id": "260164484",
      "cited_paper_id": 1143169
    },
    {
      "context_text": "V ISUAL Odometry (VO) is a technique to estimate egomotion in robotics [1], [2], [3], [4], [5].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only the technique of Visual Odometry. No dataset names are present in the context.",
      "processing_time": 43.0180287361145,
      "citing_paper_id": "260164484",
      "cited_paper_id": 3299195
    },
    {
      "context_text": "V ISUAL Odometry (VO) is a technique to estimate egomotion in robotics [1], [2], [3], [4], [5].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only the technique of Visual Odometry. No dataset names are present in the context.",
      "processing_time": 43.0180287361145,
      "citing_paper_id": "260164484",
      "cited_paper_id": 14925984
    },
    {
      "context_text": "Many pipelines do this by grouping similar feature times to a common time [7], [8], [9].",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only a general method for handling feature times in pipelines. The cited papers' titles do not provide additional context to identify specific datasets.",
      "processing_time": 44.47431492805481,
      "citing_paper_id": "260164484",
      "cited_paper_id": 3738244
    },
    {
      "context_text": "Many pipelines do this by grouping similar feature times to a common time [7], [8], [9].",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only a general method for handling feature times in pipelines. The cited papers' titles do not provide additional context to identify specific datasets.",
      "processing_time": 44.47431492805481,
      "citing_paper_id": "260164484",
      "cited_paper_id": 220870707
    },
    {
      "context_text": "Ultimate-SLAM [7] extends [17] to use the IMU to generate motion-compensated event frames and reformulates the cost function for camera egomotion estimation.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and approaches. The context focuses on extending a method to incorporate IMU data for motion compensation and egomotion estimation.",
      "processing_time": 43.998043060302734,
      "citing_paper_id": "260164484",
      "cited_paper_id": 3738244
    },
    {
      "context_text": "[18] extract features using an asynchronous feature detector (Arc* [19]) from a stereo pair of event cameras and adopt an estimation pipeline similar to [7].",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and tools. The context focuses on the use of an asynchronous feature detector and event cameras, which are not datasets.",
      "processing_time": 44.46663689613342,
      "citing_paper_id": "260164484",
      "cited_paper_id": 3738244
    },
    {
      "context_text": "[18] extract features using an asynchronous feature detector (Arc* [19]) from a stereo pair of event cameras and adopt an estimation pipeline similar to [7].",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and tools. The context focuses on the use of an asynchronous feature detector and event cameras, which are not datasets.",
      "processing_time": 44.46663689613342,
      "citing_paper_id": "260164484",
      "cited_paper_id": 49864158
    },
    {
      "context_text": "Mueggler et al. [27] use a continuous-time pose estimation framework that uses IMU measurements and represents the trajectory as cumulative cubic B-splines.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for continuous-time pose estimation using IMU measurements and cumulative cubic B-splines.",
      "processing_time": 43.368873596191406,
      "citing_paper_id": "260164484",
      "cited_paper_id": 9729856
    },
    {
      "context_text": "Features are detected and tracked in the event frames and each feature is assigned an event time from the SAE.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a method for detecting and tracking features in event frames. No verifiable resource names are present.",
      "processing_time": 43.57787799835205,
      "citing_paper_id": "260164484",
      "cited_paper_id": 12475678
    },
    {
      "context_text": "1) Event Clustering: The stereo event stream is rectified and clustered to construct new SAEs and new binary event frames (Fig.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for processing stereo event streams.",
      "processing_time": 42.384045124053955,
      "citing_paper_id": "260164484",
      "cited_paper_id": 12475678
    },
    {
      "context_text": "The system takes an asynchronous stereo event stream and clusters the events into event frames and SAEs.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a method for processing event streams.",
      "processing_time": 42.38125562667847,
      "citing_paper_id": "260164484",
      "cited_paper_id": 12475678
    },
    {
      "context_text": "The timestamp of each tracklet state is assigned from the nearest event in the associated SAE.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a method or process involving tracklets and events. No clear, verifiable dataset names are present.",
      "processing_time": 44.208019971847534,
      "citing_paper_id": "260164484",
      "cited_paper_id": 12475678
    },
    {
      "context_text": "The temporal resolution of event cameras is maintained by assigning (possibly unique) times to each feature from the corresponding event in a Surface of Active Events (SAE) [31].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method or concept (Surface of Active Events).",
      "processing_time": 42.37180495262146,
      "citing_paper_id": "260164484",
      "cited_paper_id": 12475678
    },
    {
      "context_text": "The SAE records the most recent event timestamp of each pixel location and is used to maintain the asynchronous nature of the event camera.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific, verifiable datasets. It describes a method (SAE) used in event-based cameras.",
      "processing_time": 43.55822443962097,
      "citing_paper_id": "260164484",
      "cited_paper_id": 12475678
    },
    {
      "context_text": "[23] present a feature-based stereo VO pipeline using events, which group events into frames and then adopts a similar estimation framework to a traditional frame-based stereo VO pipeline [3].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method or approach. The context is about a feature-based stereo visual odometry pipeline using events, which is a method rather than a dataset.",
      "processing_time": 45.46073031425476,
      "citing_paper_id": "260164484",
      "cited_paper_id": 14925984
    },
    {
      "context_text": "The sliding window width is five and LIBVISO2 is run on both full-and half-resolution images.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (LIBVISO2) and a parameter setting. No verifiable resources are identified.",
      "processing_time": 45.12402057647705,
      "citing_paper_id": "260164484",
      "cited_paper_id": 16284071
    },
    {
      "context_text": "A sliding-window version of the system is implemented in MATLAB using LIBVISO2 [36] for feature detection and tracking.",
      "catation_intent": "none",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only tools and methods. The cited paper title does not provide additional information about datasets.",
      "processing_time": 43.76222586631775,
      "citing_paper_id": "260164484",
      "cited_paper_id": 16284071
    },
    {
      "context_text": "[21] estimate SE(3) motion using only eventbased cameras.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for estimating SE(3) motion using event-based cameras.",
      "processing_time": 43.329241037368774,
      "citing_paper_id": "260164484",
      "cited_paper_id": 26324573
    },
    {
      "context_text": "It is compared quantitatively to other estimation techniques in [24, 35].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a comparison to other estimation techniques. No verifiable resources are identified.",
      "processing_time": 44.1825852394104,
      "citing_paper_id": "260164484",
      "cited_paper_id": 27987704
    },
    {
      "context_text": "This interpolation can also be used to define the estimation states at a subset of the measurement times [24, 35].",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for interpolation. No dataset names are present in the citation context.",
      "processing_time": 44.56880712509155,
      "citing_paper_id": "260164484",
      "cited_paper_id": 27987704
    },
    {
      "context_text": "The system’s computational performance should be improved by implementing it in a more efficient language, such as C++, and using keytimes to reduce the number of the estimation states [24, 35].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only suggestions for improving computational performance.",
      "processing_time": 42.949116945266724,
      "citing_paper_id": "260164484",
      "cited_paper_id": 27987704
    },
    {
      "context_text": "…γ k , can be defined as a continuous-time function with respect to the global trajectory state, , where T k ( τ ) is the pose at time t k ≤ τ ≤ t k +1 , J ( · ) − 1 is the inverse left Jacobian function, ln( · ) is the inverse exponential map, and ( · ) ∨ is the inverse lifting operator [34].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only mathematical functions and operations relevant to state estimation in robotics.",
      "processing_time": 43.933839082717896,
      "citing_paper_id": "260164484",
      "cited_paper_id": 65172180
    },
    {
      "context_text": "…result equal to zero gives the perturbation that minimizes the linearization, where H j,k is the Jacobian of error function in (7), where the partial derivative of the sensor model is evaluated at the nominal value, T k,k ′ is the adjoint of SE (3) and J k,k ′ is the left Jacobian of SE (3) [34].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only mathematical concepts and equations. There are no clear identifiers for datasets or other verifiable resources.",
      "processing_time": 45.25649547576904,
      "citing_paper_id": "260164484",
      "cited_paper_id": 65172180
    },
    {
      "context_text": "…δ x = { δ ξ , δ ϖ , δ ζ } , linearizing (10) as where where P j,k and P k are matrices to pick the specific components of the total perturbation, G j,k is the Jacobian of (13), and E k is the Jacobian of the prior error function in (11), where and ( · ) ⋏ is the R 6 × 1 to R 6 × 6 operator [34].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, models, or methods. It appears to be a mathematical derivation or explanation.",
      "processing_time": 43.732200622558594,
      "citing_paper_id": "260164484",
      "cited_paper_id": 65172180
    },
    {
      "context_text": "Substituting (3) into (2) approximates the error term as, where [34].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, models, or methods. It appears to be referencing a mathematical derivation or formula.",
      "processing_time": 43.73021340370178,
      "citing_paper_id": "260164484",
      "cited_paper_id": 65172180
    },
    {
      "context_text": "This constant velocity over a time, ∆ t , gives the relative SE (3) transformation, where exp( · ) is the matrix exponential and ( · ) ∧ is the lifting operator that converts R 6 × 1 to R 4 × 4 [34].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only mathematical concepts and operators. There are no verifiable resources or datasets mentioned.",
      "processing_time": 44.85073280334473,
      "citing_paper_id": "260164484",
      "cited_paper_id": 65172180
    },
    {
      "context_text": "The trajectory is initialized with an Ackermann motion model [29], and globally optimized with a B-spline-based continuous-time estimation framework.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and models. There are no verifiable resources that meet the criteria for inclusion.",
      "processing_time": 45.07827401161194,
      "citing_paper_id": "260164484",
      "cited_paper_id": 195496021
    },
    {
      "context_text": "It is evaluated on the publicly available Multi Vehicle Stereo Event Camera (MVSEC) dataset [11], where it obtains a more accurate and smoother trajectory estimate than the state-of-theart Event-based Stereo Visual Odometry (ESVO) [9].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Multi Vehicle Stereo Event Camera (MVSEC)"
      ],
      "dataset_descriptions": {
        "Multi Vehicle Stereo Event Camera (MVSEC)": "Used to evaluate the accuracy and smoothness of trajectory estimates in event-based stereo depth estimation, comparing performance against state-of-the-art methods."
      },
      "confidence_score": 1.0,
      "reasoning": "The citation mentions the MVSEC dataset, which is used for evaluating the accuracy and smoothness of trajectory estimates in event-based stereo depth estimation.",
      "processing_time": 54.418986797332764,
      "citing_paper_id": "260164484",
      "cited_paper_id": 220870707
    },
    {
      "context_text": "The presented pipeline is evaluated on the MVSEC dataset [11] and compared against the publicly available ESVO [9], a discrete event-based stereo VO pipeline.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "MVSEC"
      ],
      "dataset_descriptions": {
        "MVSEC": "Used to evaluate the presented pipeline for event-based stereo depth estimation, comparing performance against the ESVO pipeline."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions the MVSEC dataset, which is a specific dataset used for evaluating the presented pipeline. The dataset is clearly identified and used for evaluation purposes.",
      "processing_time": 51.41636610031128,
      "citing_paper_id": "260164484",
      "cited_paper_id": 220870707
    },
    {
      "context_text": "ESVO [9] uses parallel tracking and mapping to estimate the egomotion trajectory and a semidense 3D scene reconstruction.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (ESVO) and its capabilities. No verifiable resources are identified.",
      "processing_time": 45.40080118179321,
      "citing_paper_id": "260164484",
      "cited_paper_id": 220870707
    },
    {
      "context_text": "IMU Dynamic Vision Sensor Odometry using Lines (IDOL) [20] uses an alternative VIO paradigm.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method or framework called IDOL. There are no clear identifiers for datasets in the provided context.",
      "processing_time": 45.887696981430054,
      "citing_paper_id": "260164484",
      "cited_paper_id": 221112528
    },
    {
      "context_text": "It could also be directly replaced with event-based methods, e.g., Arc* [19] or HASTE [32].",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods (Arc* and HASTE).",
      "processing_time": 44.128705978393555,
      "citing_paper_id": "260164484",
      "cited_paper_id": 221670108
    },
    {
      "context_text": "Hadviger et al. [23] present a feature-based stereo VO pipeline using events, which group events into frames and then adopts a similar estimation framework to a traditional frame-based stereo VO pipeline [3].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method or pipeline. The context focuses on the methodological approach rather than the use of a particular dataset.",
      "processing_time": 46.43313479423523,
      "citing_paper_id": "260164484",
      "cited_paper_id": 235794981
    },
    {
      "context_text": "In [4], a 64×64 DVS was evaluated for person fall detection.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.2,
      "reasoning": "The citation mentions a DVS (Dynamic Vision Sensor) used for person fall detection, but does not specify a named dataset. The context is too generic to identify a specific, verifiable dataset.",
      "processing_time": 47.03866720199585,
      "citing_paper_id": "7224209",
      "cited_paper_id": 18577595
    },
    {
      "context_text": "[1] and demonstrated on a 2×128×128 pixel vision sensor [9].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific, verifiable datasets. It only refers to a vision sensor, which is an instrument, not a dataset.",
      "processing_time": 45.8703191280365,
      "citing_paper_id": "7224209",
      "cited_paper_id": 22794412
    },
    {
      "context_text": "[1] and demonstrated on a 2×128×128 pixel vision sensor [9].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific, verifiable datasets. It only refers to a vision sensor, which is an instrument, not a dataset.",
      "processing_time": 45.8703191280365,
      "citing_paper_id": "7224209",
      "cited_paper_id": 43823997
    },
    {
      "context_text": "Some works [27, 28] also target the problem of instantaneous stereo (depth maps produced using events over very short time intervals), but they use two non-simultaneous event cameras.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a methodological approach involving non-simultaneous event cameras.",
      "processing_time": 45.20431900024414,
      "citing_paper_id": "49877954",
      "cited_paper_id": 185541
    },
    {
      "context_text": "These methods exploit a constrained hardware setup (two rotating event cameras with known motion) to either (i) recover intensity images on which conventional stereo is applied [27] or (ii) match events across cameras using temporal metrics and then use triangulation [28].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and approaches. The context focuses on the use of event cameras and stereo matching techniques.",
      "processing_time": 46.14086437225342,
      "citing_paper_id": "49877954",
      "cited_paper_id": 185541
    },
    {
      "context_text": "We report themean depth error, themedian depth error and the relative error (defined as the mean depth error divided by the depth range of the scene [13]).",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only evaluation metrics. No dataset names are present in the citation context.",
      "processing_time": 44.8022780418396,
      "citing_paper_id": "49877954",
      "cited_paper_id": 1082643
    },
    {
      "context_text": "6 Event polarity is not used, as [13] shows that it is not needed for 3D reconstruction.",
      "catation_intent": "findings",
      "resource_type": "finding",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method or finding from another paper.",
      "processing_time": 44.49334144592285,
      "citing_paper_id": "49877954",
      "cited_paper_id": 1082643
    },
    {
      "context_text": "No post-processing (such as regularization through median filtering [13]) was performed.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for post-processing. The context is too limited to infer any dataset usage.",
      "processing_time": 46.40682363510132,
      "citing_paper_id": "49877954",
      "cited_paper_id": 1082643
    },
    {
      "context_text": "Our method is inspired by multi-view stereo pipelines for conventional cameras, such as DTAM [30], which aim at maximizing the photometric consistency through a number of narrow-baseline video frames.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions DTAM but does not indicate it is a dataset. It is described as a method or pipeline for dense tracking and mapping.",
      "processing_time": 47.00857901573181,
      "citing_paper_id": "49877954",
      "cited_paper_id": 1336659
    },
    {
      "context_text": "We propose to apply patch-match to compare a pair of spike-history maps, in place of the photometric warping error as used in DTAM [30].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (DTAM) and a technique (patch-match).",
      "processing_time": 46.74066400527954,
      "citing_paper_id": "49877954",
      "cited_paper_id": 1336659
    },
    {
      "context_text": "To evaluate our method, we use sequences from publicly available simulators [36] and datasets [34], and we also collect our own sequences using a stereo event-camera rig (Fig.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Multivehicle Stereo Event Camera Dataset"
      ],
      "dataset_descriptions": {
        "Multivehicle Stereo Event Camera Dataset": "Used to evaluate the proposed method for event-based stereo depth estimation, providing sequences for 3D perception tasks."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions the use of sequences from publicly available datasets, and the cited paper title confirms the use of a specific dataset.",
      "processing_time": 54.96813201904297,
      "citing_paper_id": "49877954",
      "cited_paper_id": 3416874
    },
    {
      "context_text": "3(a)), a number of stereo observations from a real stereo event-camera sequence [34] have been created (Figs.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Multivehicle Stereo Event Camera Dataset"
      ],
      "dataset_descriptions": {
        "Multivehicle Stereo Event Camera Dataset": "Used to create stereo observations from a real stereo event-camera sequence, focusing on 3D perception and event-based stereo depth estimation."
      },
      "confidence_score": 0.9,
      "reasoning": "The citation mentions a 'real stereo event-camera sequence' which is likely part of the Multivehicle Stereo Event Camera Dataset. The dataset is used for creating stereo observations, relevant to event-based stereo depth estimation.",
      "processing_time": 58.56971573829651,
      "citing_paper_id": "49877954",
      "cited_paper_id": 3416874
    },
    {
      "context_text": "The evaluation is performed on six sequences, including a synthetic sequence from the simulator [36], three sequences collected by ourselves (hand-held) and two sequences from [34] (with a stereo event camera mounted on a drone).",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Multivehicle Stereo Event Camera Dataset",
        "event-camera dataset"
      ],
      "dataset_descriptions": {
        "Multivehicle Stereo Event Camera Dataset": "Used to evaluate stereo depth estimation methods, focusing on sequences captured with a stereo event camera mounted on a drone.",
        "event-camera dataset": "Used to evaluate stereo depth estimation methods, focusing on synthetic sequences generated by a simulator."
      },
      "confidence_score": 0.9,
      "reasoning": "The context mentions specific sequences used for evaluation, including a synthetic sequence from a simulator and sequences from a cited paper. These are likely datasets or data sources.",
      "processing_time": 61.365222454071045,
      "citing_paper_id": "49877954",
      "cited_paper_id": 3416874
    },
    {
      "context_text": "The evaluation is performed on six sequences, including a synthetic sequence from the simulator [36], three sequences collected by ourselves (hand-held) and two sequences from [34] (with a stereo event camera mounted on a drone).",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Multivehicle Stereo Event Camera Dataset",
        "event-camera dataset"
      ],
      "dataset_descriptions": {
        "Multivehicle Stereo Event Camera Dataset": "Used to evaluate stereo depth estimation methods, focusing on sequences captured with a stereo event camera mounted on a drone.",
        "event-camera dataset": "Used to evaluate stereo depth estimation methods, focusing on synthetic sequences generated by a simulator."
      },
      "confidence_score": 0.9,
      "reasoning": "The context mentions specific sequences used for evaluation, including a synthetic sequence from a simulator and sequences from a cited paper. These are likely datasets or data sources.",
      "processing_time": 61.365222454071045,
      "citing_paper_id": "49877954",
      "cited_paper_id": 9865213
    },
    {
      "context_text": "The other two sequences are collected in a large indoor environment using a drone [34], with pose information also from a motion-capture system.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.3,
      "reasoning": "The context mentions a 'large indoor environment' and 'motion-capture system', but does not specify a named dataset. The cited paper title suggests a relevant dataset, but the context itself does not confirm its use.",
      "processing_time": 49.988314390182495,
      "citing_paper_id": "49877954",
      "cited_paper_id": 3416874
    },
    {
      "context_text": "Hence, event cameras open the door to tackle challenging scenarios that are inaccessible to standard cameras, such as high-speed and/or HDR tracking [2–8], control [9,10] and Simultaneous Localization and Mapping (SLAM) [11–16].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only general applications of event cameras. No dataset names are present in the text.",
      "processing_time": 47.443620920181274,
      "citing_paper_id": "49877954",
      "cited_paper_id": 3738244
    },
    {
      "context_text": "Hence, event cameras open the door to tackle challenging scenarios that are inaccessible to standard cameras, such as high-speed and/or HDR tracking [2–8], control [9,10] and Simultaneous Localization and Mapping (SLAM) [11–16].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only general applications of event cameras. No dataset names are present in the text.",
      "processing_time": 47.443620920181274,
      "citing_paper_id": "49877954",
      "cited_paper_id": 22700300
    },
    {
      "context_text": "Hence, event cameras open the door to tackle challenging scenarios that are inaccessible to standard cameras, such as high-speed and/or HDR tracking [2–8], control [9,10] and Simultaneous Localization and Mapping (SLAM) [11–16].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only general applications of event cameras. No dataset names are present in the text.",
      "processing_time": 47.443620920181274,
      "citing_paper_id": "49877954",
      "cited_paper_id": 202687061
    },
    {
      "context_text": "To evaluate our method, we use sequences from publicly available simulators [36] and datasets [34], and we also collect our own sequences using a stereo eventcamera rig (Fig.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "event-camera dataset"
      ],
      "dataset_descriptions": {
        "event-camera dataset": "Used to evaluate the proposed method for event-based stereo depth estimation, focusing on pose estimation, visual odometry, and SLAM."
      },
      "confidence_score": 0.9,
      "reasoning": "The context mentions 'datasets' and 'simulators', but only the dataset is relevant to the extraction criteria. The cited paper title confirms the dataset is specific and verifiable.",
      "processing_time": 56.16424083709717,
      "citing_paper_id": "49877954",
      "cited_paper_id": 9865213
    },
    {
      "context_text": "Dataset simulation 3planes [36] Indoor flying1 [34] Indoor flying3 [34]",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "3planes",
        "Indoor flying1",
        "Indoor flying3"
      ],
      "dataset_descriptions": {
        "3planes": "Used to simulate event-based data for indoor flying scenarios, focusing on pose estimation and visual odometry.",
        "Indoor flying1": "Used to evaluate event-based algorithms in indoor flying environments, specifically for visual odometry and SLAM.",
        "Indoor flying3": "Used to test event-based stereo depth estimation in complex indoor flying scenarios, emphasizing robustness and accuracy."
      },
      "confidence_score": 0.9,
      "reasoning": "The citation mentions specific datasets used for indoor flying scenarios, which are relevant to event-based stereo depth estimation.",
      "processing_time": 64.13262033462524,
      "citing_paper_id": "49877954",
      "cited_paper_id": 9865213
    },
    {
      "context_text": "Speciﬁcally, to create two distinctive maps, we advocate the use of Time-Surface inspired by [32] for event-based pattern recognition.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions 'Time-Surface' but does not refer to it as a dataset. It is likely a method or concept used for event-based pattern recognition.",
      "processing_time": 48.63143181800842,
      "citing_paper_id": "49877954",
      "cited_paper_id": 13373696
    },
    {
      "context_text": "However, as shown in [31], stereo temporal consistency does not strictly hold at the pixel level because of signal latency and jitter eﬀects.",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only discusses the limitations of stereo temporal consistency due to signal latency and jitter effects.",
      "processing_time": 46.7079713344574,
      "citing_paper_id": "49877954",
      "cited_paper_id": 14354416
    },
    {
      "context_text": "The stereo rig consists of two Dynamic and Active Pixel Vision Sensors (DAVIS) [37] of 240 × 180 pixel resolution, which are calibrated intrinsically and extrinsically 3 using Kalibr [38].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions hardware components (DAVIS sensors) and a calibration tool (Kalibr), but does not refer to any specific datasets.",
      "processing_time": 47.66192269325256,
      "citing_paper_id": "49877954",
      "cited_paper_id": 15778738
    },
    {
      "context_text": "The stereo rig consists of two Dynamic and Active Pixel Vision Sensors (DAVIS) [37] of 240 × 180 pixel resolution, which are calibrated intrinsically and extrinsically 3 using Kalibr [38].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions hardware components (DAVIS sensors) and a calibration tool (Kalibr), but does not refer to any specific datasets.",
      "processing_time": 47.66192269325256,
      "citing_paper_id": "49877954",
      "cited_paper_id": 24007071
    },
    {
      "context_text": "The stereo rig consists of two Dynamic and Active Pixel Vision Sensors (DAVIS) [37] of 240 × 180 pixel resolution, which are calibrated intrinsically and extrinsically(3) using Kalibr [38].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions the use of DAVIS sensors and Kalibr for calibration, but does not refer to any specific dataset. The context is about the setup and calibration process, not the use of a dataset.",
      "processing_time": 50.7993426322937,
      "citing_paper_id": "49877954",
      "cited_paper_id": 15778738
    },
    {
      "context_text": "…and extrinsic parameters) as well as the pose of the left event camera T sr at each observation are known (e.g., from a tracking algorithm such as [12,14]), the points x 1 and x 2 are given by x 1 ( ρ ) = π ( T sr π − 1 ( x , ρ )) and x 2 ( ρ ) = π ( T E T sr π − 1 ( x , ρ )), respectively.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method or algorithm for tracking and reconstruction with an event camera.",
      "processing_time": 46.34903335571289,
      "citing_paper_id": "49877954",
      "cited_paper_id": 26324573
    },
    {
      "context_text": "Some preliminary works addressed this issue by combining event cameras with additional sensors, such as standard cameras [8, 17, 18] or depth sensors [17, 19], to simplify the estimation task at hand.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only references to methods and approaches using additional sensors.",
      "processing_time": 45.790451765060425,
      "citing_paper_id": "49877954",
      "cited_paper_id": 202687061
    },
    {
      "context_text": "Temporal coherence (e.g., simultaneity) of events across both left and right cameras is used to ﬁnd matching events, and then standard triangulation [23] recovers depth.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for depth recovery using event-based stereo vision.",
      "processing_time": 46.68860578536987,
      "citing_paper_id": "49877954",
      "cited_paper_id": 261497446
    },
    {
      "context_text": "(2011), and Belbachir et al. (2012). These works apply matching methodes based on events accumulation to build frames so standard binocular vision techniques can be applied. We are intentionally getting away from these approaches, as building frames induces usually a lost in temporal precision and is not allowing us to exploit the event-based representation at its fullest potential. In Kogler et al. (2011), the authors also claimed the benefit of using accurate time information provided by the sensor instead of simply accumulating events to build local/global frames. They developed an event matching algorithm that is pretty similar to the one in Rogister et al. (2011). At the time, the community started to focus more on how to infer depth from the temporal information rather than just the spatial information obtained via events accumulation.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and approaches. The focus is on the evolution of techniques in event-based stereo depth estimation.",
      "processing_time": 48.41880512237549,
      "citing_paper_id": "49554392",
      "cited_paper_id": 7224209
    },
    {
      "context_text": "(2011), and Belbachir et al. (2012). These works apply matching methodes based on events accumulation to build frames so standard binocular vision techniques can be applied.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and techniques. There are no clear identifiers for datasets in the provided context.",
      "processing_time": 48.01475429534912,
      "citing_paper_id": "49554392",
      "cited_paper_id": 7224209
    },
    {
      "context_text": "(2011), and Belbachir et al. (2012). These works apply matching methodes based on events accumulation to build frames so standard binocular vision techniques can be applied. We are intentionally getting away from these approaches, as building frames induces usually a lost in temporal precision and is not allowing us to exploit the event-based representation at its fullest potential. In Kogler et al. (2011), the authors also claimed the benefit of using accurate time information provided by the sensor instead of simply accumulating events to build local/global frames.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and approaches. The focus is on the methodology and the limitations of existing approaches.",
      "processing_time": 48.22040820121765,
      "citing_paper_id": "49554392",
      "cited_paper_id": 7224209
    },
    {
      "context_text": "(2011), and Belbachir et al. (2012). These works apply matching methodes based on events accumulation to build frames so standard binocular vision techniques can be applied. We are intentionally getting away from these approaches, as building frames induces usually a lost in temporal precision and is not allowing us to exploit the event-based representation at its fullest potential. In Kogler et al. (2011), the authors also claimed the benefit of using accurate time information provided by the sensor instead of simply accumulating events to build local/global frames. They developed an event matching algorithm that is pretty similar to the one in Rogister et al. (2011). At the time, the community started to focus more on how to infer depth from the temporal information rather than just the spatial information obtained via events accumulation. Piatkowska et al. (2013) implemented an eventbased form of a “cooperative” computation of depth coupled with a winner-take-all mechanism to match events temporaly close and spatially constrained by the epipolar geometry satisfied by the event-based vision sensors.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only methods and approaches. The focus is on the evolution of techniques in event-based stereo depth estimation.",
      "processing_time": 48.86388063430786,
      "citing_paper_id": "49554392",
      "cited_paper_id": 7224209
    },
    {
      "context_text": "(2011), and Belbachir et al. (2012). These works apply matching methodes based on events accumulation to build frames so standard binocular vision techniques can be applied. We are intentionally getting away from these approaches, as building frames induces usually a lost in temporal precision and is not allowing us to exploit the event-based representation at its fullest potential. In Kogler et al. (2011), the authors also claimed the benefit of using accurate time information provided by the sensor instead of simply accumulating events to build local/global frames. They developed an event matching algorithm that is pretty similar to the one in Rogister et al. (2011). At the time, the community started to focus more on how to infer depth from the temporal information rather than just the spatial information obtained via events accumulation. Piatkowska et al. (2013) implemented an eventbased form of a “cooperative” computation of depth coupled with a winner-take-all mechanism to match events temporaly close and spatially constrained by the epipolar geometry satisfied by the event-based vision sensors. This cooperative technique has been actually initiated by the early work of Marr and Poggio (1976) on frame-based cameras.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific, verifiable datasets. It discusses various methods and approaches in event-based stereo depth estimation but does not reference any named datasets.",
      "processing_time": 49.443145751953125,
      "citing_paper_id": "49554392",
      "cited_paper_id": 7224209
    },
    {
      "context_text": "This is the case for all the existing event-based vision sensors as the ones listed summarized in Delbruck (2016) since they are derived to some extend from the DVS.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a reference to event-based vision sensors. No verifiable resources are identified.",
      "processing_time": 47.80356478691101,
      "citing_paper_id": "49554392",
      "cited_paper_id": 12093070
    },
    {
      "context_text": "The use of time allowed the reformulation of the epipolar constraint as a time coincidence phenomenon as shown in Benosman et al. (2011).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method or concept from the cited paper.",
      "processing_time": 46.03861379623413,
      "citing_paper_id": "49554392",
      "cited_paper_id": 14354416
    },
    {
      "context_text": "It combines for the ﬁrst time: precise timing (Rogister et al., 2011), local motion consistency (Benosman et al., 2012) and light consistency in the temporal domain (Posch et al., 2011).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods and techniques. The cited papers' titles do not provide additional context to identify datasets.",
      "processing_time": 48.57614493370056,
      "citing_paper_id": "49554392",
      "cited_paper_id": 17407641
    },
    {
      "context_text": "It combines for the ﬁrst time: precise timing (Rogister et al., 2011), local motion consistency (Benosman et al., 2012) and light consistency in the temporal domain (Posch et al., 2011).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods and techniques. The cited papers' titles do not provide additional context to identify datasets.",
      "processing_time": 48.57614493370056,
      "citing_paper_id": "49554392",
      "cited_paper_id": 21317717
    },
    {
      "context_text": "The Asynchronous Time-based Image Sensor (Posch et al., 2011) used in this paper is an asynchronous camera that contains an array of independently operating pixels that combine an asynchronous level-crossing detector and a separate exposure measurement circuit.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context describes a sensor used in the research but does not mention any specific dataset. The focus is on the hardware and its capabilities.",
      "processing_time": 48.3840069770813,
      "citing_paper_id": "49554392",
      "cited_paper_id": 21317717
    },
    {
      "context_text": "Newer generations of event-based sensors have either integrated a synchronous frame mode (Berner et al., 2013) or have implemented a level crossing sampling mechanism to captur and encode luminance in an asynchronous way (Posch et al., 2011).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only sensor technologies and their capabilities.",
      "processing_time": 46.48639750480652,
      "citing_paper_id": "49554392",
      "cited_paper_id": 21317717
    },
    {
      "context_text": "The Asynchronous Time-based Image Sensor (ATIS) used in this work is a time-domain encoding image sensor with QVGA resolution (Posch et al., 2008, 2011).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions ATIS, which is a sensor, not a dataset. No specific dataset is referenced.",
      "processing_time": 47.36363458633423,
      "citing_paper_id": "49554392",
      "cited_paper_id": 21317717
    },
    {
      "context_text": "This list includes sensors that combine the event-based sampling with a traditional synchronous frame acquisition mechanism (Berner et al., 2013; Brandli et al., 2014) to acquire also gray levels.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only sensors and their capabilities. There are no clear identifiers for datasets.",
      "processing_time": 47.780640840530396,
      "citing_paper_id": "49554392",
      "cited_paper_id": 24007071
    },
    {
      "context_text": "This last consideration is the main motivation splitting this work from many others that can be found in literature in event-based stereo vision as Schraml et al. (2007), Kogler et al. (2009), Schraml et al. (2010), Dominguez-Morales et al. (2011), and Belbachir et al. (2012).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to other works in the field of event-based stereo vision.",
      "processing_time": 47.9703266620636,
      "citing_paper_id": "49554392",
      "cited_paper_id": 24236495
    },
    {
      "context_text": "This last consideration is the main motivation splitting this work from many others that can be found in literature in event-based stereo vision as Schraml et al. (2007), Kogler et al. (2009), Schraml et al. (2010), Dominguez-Morales et al. (2011), and Belbachir et al. (2012).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to other works in the field of event-based stereo vision.",
      "processing_time": 47.9703266620636,
      "citing_paper_id": "49554392",
      "cited_paper_id": 27059477
    },
    {
      "context_text": "This last consideration is the main motivation splitting this work from many others that can be found in literature in event-based stereo vision as Schraml et al. (2007), Kogler et al. (2009), Schraml et al. (2010), Dominguez-Morales et al. (2011), and Belbachir et al. (2012).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to other works in the field of event-based stereo vision.",
      "processing_time": 47.9703266620636,
      "citing_paper_id": "49554392",
      "cited_paper_id": null
    },
    {
      "context_text": "Results show however that higher amount of correct matches can be reached if more constraints are added on the matching (Camuñas-Mesa et al., 2014).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method or approach. The context is about adding constraints to improve matching accuracy in event-driven stereo vision.",
      "processing_time": 48.55138421058655,
      "citing_paper_id": "49554392",
      "cited_paper_id": 25365120
    },
    {
      "context_text": "The observation that visual motion appears smooth and continuous if viewed above a certain frame-rate is, however, more related to characteristics of the human eye and visual system than to the quality of the acquisition and encoding of the visual information as a serie of images (Akolkar et al., 2015).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only discusses characteristics of visual systems and neuromorphic event-driven systems.",
      "processing_time": 47.096447467803955,
      "citing_paper_id": "49554392",
      "cited_paper_id": 30459761
    },
    {
      "context_text": "The observation that visual motion appears smooth and continuous if viewed above a certain frame-rate is, however, more related to characteristics of the human eye and visual system than to the quality of the acquisition and encoding of the visual information as a serie of images (Akolkar et al., 2015). As soon as changes or motions are involved, which is the case for most machine vision applications, the universally accepted paradigm of visual frame acquisition becomes fundamentally flawed. If a camera observes a dynamic scene, no matter where the frame-rate is set to, it will always be wrong. Because there is no relation whatsoever between dynamics present in a scene and the chosen frame-rate, over-sampling and/or under-sampling occur, and moreover both usually happen at the same time. When acquiring a natural scene with a fast moving object, e.g., a ball thrown in front of a static background with a standard video camera, motion blur and displacement of the moving object between adjacent frames will result from under-sampling the object, while repeatedly sampling and acquiring static background over and over again will lead to large amounts of redundant, previously known data. As a result, the scene is under- and over-sampled at the same time! Interestingly, this far-from-optimal strategy of acquiring dynamic visual information has been accepted by the machine vision community for decades, likely due to the lack of convincing alternatives. Recently, research in the field of Neuromorphic Engineering has resulted in a new type of vision sensors that acquire visual information in a very different way. These sensors are based on pixels that can optimize their own sampling depending on the visual information they individually receive. If scenes change quickly, the pixel samples at a high rate; if nothing changes, the pixel stops acquiring redundant data and goes idle until the scene changes significantly again in the sensors’ field of view. These sensors introduce another paradigm of visual information acquisition: the pixels, instead of being driven by a fixed frequency that makes them work synchronously as in a classic frame-based sensor, are independent both in the samples acquisition times and the exposure durations. The data acquired that way is globally a time-continuous stream of visual information. In order to do so, each pixel defines the timing of its own sampling points in response to its visual input by reacting to changes of the amount of incident light. As a consequence, the sampling process is no longer governed by a fixed external signal defined in the time domain but by the signal to be sampled itself, or more precisely by the variations of the signal in the amplitude domain. Mahowald (1992) introduced the early form of the neuromorphic vision sensor that lead to several variations of what are presently known as the event-based vision sensors: (Lichtsteiner et al., 2006; Serrano-Gotarredona and LinaresBarranco, 2013) are encoding temporal contrasts asynchronously in the form of pulses called events. Newer generations of event-based sensors have either integrated a synchronous frame mode (Berner et al., 2013) or have implemented a level crossing sampling mechanism to captur and encode luminance in an asynchronous way (Posch et al., 2011). The Asynchronous Time-based Image Sensor (Posch et al., 2011) used in this paper is an asynchronous camera that contains an array of independently operating pixels that combine an asynchronous level-crossing detector and a separate exposure measurement circuit. Each exposure measurement by an individual pixel is triggered by a level-crossing event. Hence each pixel independently samples its illuminance upon detection of a change of a certain magnitude in this same luminance, thus establishing its instantaneous gray level after it has changed. The result of the exposure measurement (i.e., the new gray level) is asynchronously output off the sensor together with the pixel‘s coordinates in the sensor array. As a result, image information is not acquired frame-wise but continuously, and conditionally, only from parts of the scene where there is new visual information. Or in other words, only information that is relevant–because it has changed– is acquired, transmitted, stored and eventually processed by machine vision algorithms. Pixel acquisition and readout times of microseconds to milliseconds are achieved, resulting in temporal resolutions equivalent to conventional sensors running at tens to hundreds of thousands frames per second. The implications of this approach for machine vision can hardly be overstated. Now, for the first time, the strict temporal resolution vs. data rate tradeoff that limits all frame-based vision acquisition can be overcome. Visual data acquisition simultaneously becomes fast and sparse. Obviously the advantages of acquiring dynamic vision data this way, i.e., ultra-high-speed operation combined with reduced power consumption, transmission bandwidth and memory requirements, do not end at the acquisition stage. All subsequent processing strongly benefits from the fact that the sensors encode visual dynamics into highly resolved spatiotemporal patterns of “events,” representing the relevant features of motion such as moving object contours and trajectories virtually in continuous time. The event-based formulation of stereovision has already produced striking results in stereovision. The use of time allowed the reformulation of the epipolar constraint as a time coincidence phenomenon as shown in Benosman et al. (2011). Epipolar lines defining the relation established by two vision sensors appear as stuctures of co- occurent events.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only discusses the limitations of traditional frame-based cameras and introduces neuromorphic event-driven sensors.",
      "processing_time": 48.81663513183594,
      "citing_paper_id": "49554392",
      "cited_paper_id": 30459761
    },
    {
      "context_text": "The observation that visual motion appears smooth and continuous if viewed above a certain frame-rate is, however, more related to characteristics of the human eye and visual system than to the quality of the acquisition and encoding of the visual information as a serie of images (Akolkar et al., 2015). As soon as changes or motions are involved, which is the case for most machine vision applications, the universally accepted paradigm of visual frame acquisition becomes fundamentally flawed. If a camera observes a dynamic scene, no matter where the frame-rate is set to, it will always be wrong. Because there is no relation whatsoever between dynamics present in a scene and the chosen frame-rate, over-sampling and/or under-sampling occur, and moreover both usually happen at the same time. When acquiring a natural scene with a fast moving object, e.g., a ball thrown in front of a static background with a standard video camera, motion blur and displacement of the moving object between adjacent frames will result from under-sampling the object, while repeatedly sampling and acquiring static background over and over again will lead to large amounts of redundant, previously known data. As a result, the scene is under- and over-sampled at the same time! Interestingly, this far-from-optimal strategy of acquiring dynamic visual information has been accepted by the machine vision community for decades, likely due to the lack of convincing alternatives. Recently, research in the field of Neuromorphic Engineering has resulted in a new type of vision sensors that acquire visual information in a very different way. These sensors are based on pixels that can optimize their own sampling depending on the visual information they individually receive. If scenes change quickly, the pixel samples at a high rate; if nothing changes, the pixel stops acquiring redundant data and goes idle until the scene changes significantly again in the sensors’ field of view. These sensors introduce another paradigm of visual information acquisition: the pixels, instead of being driven by a fixed frequency that makes them work synchronously as in a classic frame-based sensor, are independent both in the samples acquisition times and the exposure durations. The data acquired that way is globally a time-continuous stream of visual information. In order to do so, each pixel defines the timing of its own sampling points in response to its visual input by reacting to changes of the amount of incident light. As a consequence, the sampling process is no longer governed by a fixed external signal defined in the time domain but by the signal to be sampled itself, or more precisely by the variations of the signal in the amplitude domain. Mahowald (1992) introduced the early form of the neuromorphic vision sensor that lead to several variations of what are presently known as the event-based vision sensors: (Lichtsteiner et al., 2006; Serrano-Gotarredona and LinaresBarranco, 2013) are encoding temporal contrasts asynchronously in the form of pulses called events. Newer generations of event-based sensors have either integrated a synchronous frame mode (Berner et al., 2013) or have implemented a level crossing sampling mechanism to captur and encode luminance in an asynchronous way (Posch et al., 2011). The Asynchronous Time-based Image Sensor (Posch et al., 2011) used in this paper is an asynchronous camera that contains an array of independently operating pixels that combine an asynchronous level-crossing detector and a separate exposure measurement circuit. Each exposure measurement by an individual pixel is triggered by a level-crossing event. Hence each pixel independently samples its illuminance upon detection of a change of a certain magnitude in this same luminance, thus establishing its instantaneous gray level after it has changed. The result of the exposure measurement (i.e., the new gray level) is asynchronously output off the sensor together with the pixel‘s coordinates in the sensor array. As a result, image information is not acquired frame-wise but continuously, and conditionally, only from parts of the scene where there is new visual information. Or in other words, only information that is relevant–because it has changed– is acquired, transmitted, stored and eventually processed by machine vision algorithms. Pixel acquisition and readout times of microseconds to milliseconds are achieved, resulting in temporal resolutions equivalent to conventional sensors running at tens to hundreds of thousands frames per second. The implications of this approach for machine vision can hardly be overstated. Now, for the first time, the strict temporal resolution vs. data rate tradeoff that limits all frame-based vision acquisition can be overcome. Visual data acquisition simultaneously becomes fast and sparse. Obviously the advantages of acquiring dynamic vision data this way, i.e., ultra-high-speed operation combined with reduced power consumption, transmission bandwidth and memory requirements, do not end at the acquisition stage. All subsequent processing strongly benefits from the fact that the sensors encode visual dynamics into highly resolved spatiotemporal patterns of “events,” representing the relevant features of motion such as moving object contours and trajectories virtually in continuous time. The event-based formulation of stereovision has already produced striking results in stereovision. The use of time allowed the reformulation of the epipolar constraint as a time coincidence phenomenon as shown in Benosman et al. (2011). Epipolar lines defining the relation established by two vision sensors appear as stuctures of co- occurent events. This methodology can be naturally extended to solve the problem of 3D matching and reconstructions from events as introduced in Rogister et al. (2011) and Carneiro et al. (2013). Event-based stereovision techniques based on changes events assume no luminance in the",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only discusses the limitations of traditional frame-based cameras and introduces neuromorphic event-driven sensors.",
      "processing_time": 48.80628728866577,
      "citing_paper_id": "49554392",
      "cited_paper_id": 30459761
    },
    {
      "context_text": "The observation that visual motion appears smooth and continuous if viewed above a certain frame-rate is, however, more related to characteristics of the human eye and visual system than to the quality of the acquisition and encoding of the visual information as a serie of images (Akolkar et al., 2015). As soon as changes or motions are involved, which is the case for most machine vision applications, the universally accepted paradigm of visual frame acquisition becomes fundamentally flawed. If a camera observes a dynamic scene, no matter where the frame-rate is set to, it will always be wrong. Because there is no relation whatsoever between dynamics present in a scene and the chosen frame-rate, over-sampling and/or under-sampling occur, and moreover both usually happen at the same time. When acquiring a natural scene with a fast moving object, e.g., a ball thrown in front of a static background with a standard video camera, motion blur and displacement of the moving object between adjacent frames will result from under-sampling the object, while repeatedly sampling and acquiring static background over and over again will lead to large amounts of redundant, previously known data. As a result, the scene is under- and over-sampled at the same time! Interestingly, this far-from-optimal strategy of acquiring dynamic visual information has been accepted by the machine vision community for decades, likely due to the lack of convincing alternatives. Recently, research in the field of Neuromorphic Engineering has resulted in a new type of vision sensors that acquire visual information in a very different way. These sensors are based on pixels that can optimize their own sampling depending on the visual information they individually receive. If scenes change quickly, the pixel samples at a high rate; if nothing changes, the pixel stops acquiring redundant data and goes idle until the scene changes significantly again in the sensors’ field of view. These sensors introduce another paradigm of visual information acquisition: the pixels, instead of being driven by a fixed frequency that makes them work synchronously as in a classic frame-based sensor, are independent both in the samples acquisition times and the exposure durations. The data acquired that way is globally a time-continuous stream of visual information. In order to do so, each pixel defines the timing of its own sampling points in response to its visual input by reacting to changes of the amount of incident light. As a consequence, the sampling process is no longer governed by a fixed external signal defined in the time domain but by the signal to be sampled itself, or more precisely by the variations of the signal in the amplitude domain. Mahowald (1992) introduced the early form of the neuromorphic vision sensor that lead to several variations of what are presently known as the event-based vision sensors: (Lichtsteiner et al., 2006; Serrano-Gotarredona and LinaresBarranco, 2013) are encoding temporal contrasts asynchronously in the form of pulses called events. Newer generations of event-based sensors have either integrated a synchronous frame mode (Berner et al., 2013) or have implemented a level crossing sampling mechanism to captur and encode luminance in an asynchronous way (Posch et al., 2011). The Asynchronous Time-based Image Sensor (Posch et al., 2011) used in this paper is an asynchronous camera that contains an array of independently operating pixels that combine an asynchronous level-crossing detector and a separate exposure measurement circuit. Each exposure measurement by an individual pixel is triggered by a level-crossing event. Hence each pixel independently samples its illuminance upon detection of a change of a certain magnitude in this same luminance, thus establishing its instantaneous gray level after it has changed. The result of the exposure measurement (i.e., the new gray level) is asynchronously output off the sensor together with the pixel‘s coordinates in the sensor array. As a result, image information is not acquired frame-wise but continuously, and conditionally, only from parts of the scene where there is new visual information. Or in other words, only information that is relevant–because it has changed– is acquired, transmitted, stored and eventually processed by machine vision algorithms. Pixel acquisition and readout times of microseconds to milliseconds are achieved, resulting in temporal resolutions equivalent to conventional sensors running at tens to hundreds of thousands frames per second. The implications of this approach for machine vision can hardly be overstated. Now, for the first time, the strict temporal resolution vs. data rate tradeoff that limits all frame-based vision acquisition can be overcome. Visual data acquisition simultaneously becomes fast and sparse. Obviously the advantages of acquiring dynamic vision data this way, i.e., ultra-high-speed operation combined with reduced power consumption, transmission bandwidth and memory requirements, do not end at the acquisition stage. All subsequent processing strongly benefits from the fact that the sensors encode visual dynamics into highly resolved spatiotemporal patterns of “events,” representing the relevant features of motion such as moving object contours and trajectories virtually in continuous time. The event-based formulation of stereovision has already produced striking results in stereovision. The use of time allowed the reformulation of the epipolar constraint as a time coincidence phenomenon as shown in Benosman et al. (2011). Epipolar lines defining the relation established by two vision sensors appear as stuctures of co- occurent events. This methodology can be naturally extended to solve the problem of 3D matching and reconstructions from events as introduced in Rogister et al. (2011) and Carneiro et al.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only discusses the limitations of traditional frame-based cameras and introduces neuromorphic event-driven sensors.",
      "processing_time": 48.8020715713501,
      "citing_paper_id": "49554392",
      "cited_paper_id": 30459761
    },
    {
      "context_text": "The observation that visual motion appears smooth and continuous if viewed above a certain frame-rate is, however, more related to characteristics of the human eye and visual system than to the quality of the acquisition and encoding of the visual information as a serie of images (Akolkar et al., 2015). As soon as changes or motions are involved, which is the case for most machine vision applications, the universally accepted paradigm of visual frame acquisition becomes fundamentally flawed. If a camera observes a dynamic scene, no matter where the frame-rate is set to, it will always be wrong. Because there is no relation whatsoever between dynamics present in a scene and the chosen frame-rate, over-sampling and/or under-sampling occur, and moreover both usually happen at the same time. When acquiring a natural scene with a fast moving object, e.g., a ball thrown in front of a static background with a standard video camera, motion blur and displacement of the moving object between adjacent frames will result from under-sampling the object, while repeatedly sampling and acquiring static background over and over again will lead to large amounts of redundant, previously known data. As a result, the scene is under- and over-sampled at the same time! Interestingly, this far-from-optimal strategy of acquiring dynamic visual information has been accepted by the machine vision community for decades, likely due to the lack of convincing alternatives. Recently, research in the field of Neuromorphic Engineering has resulted in a new type of vision sensors that acquire visual information in a very different way. These sensors are based on pixels that can optimize their own sampling depending on the visual information they individually receive. If scenes change quickly, the pixel samples at a high rate; if nothing changes, the pixel stops acquiring redundant data and goes idle until the scene changes significantly again in the sensors’ field of view. These sensors introduce another paradigm of visual information acquisition: the pixels, instead of being driven by a fixed frequency that makes them work synchronously as in a classic frame-based sensor, are independent both in the samples acquisition times and the exposure durations. The data acquired that way is globally a time-continuous stream of visual information. In order to do so, each pixel defines the timing of its own sampling points in response to its visual input by reacting to changes of the amount of incident light. As a consequence, the sampling process is no longer governed by a fixed external signal defined in the time domain but by the signal to be sampled itself, or more precisely by the variations of the signal in the amplitude domain. Mahowald (1992) introduced the early form of the neuromorphic vision sensor that lead to several variations of what are presently known as the event-based vision sensors: (Lichtsteiner et al.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only discusses the limitations of traditional frame-based cameras and introduces neuromorphic event-driven sensors.",
      "processing_time": 48.80435752868652,
      "citing_paper_id": "49554392",
      "cited_paper_id": 30459761
    },
    {
      "context_text": "Since the seminal work of Marr and Poggio (1977) and Julesz (1963) and other pioneers, stereovision has increasingly been studied.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only historical references to stereovision research.",
      "processing_time": 47.32733464241028,
      "citing_paper_id": "49554392",
      "cited_paper_id": 60545300
    },
    {
      "context_text": "Since the seminal work of Marr and Poggio (1977) and Julesz (1963) and other pioneers, stereovision has increasingly been studied.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only historical references to stereovision research.",
      "processing_time": 47.32733464241028,
      "citing_paper_id": "49554392",
      "cited_paper_id": null
    },
    {
      "context_text": "Their work is discussed in more detail in Chapter 3.3 (Lichtsteiner et al., 2008; Liu and Delbrück, 2010; Chen et al., 2011; Posch et al., 2011).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to works that discuss event-based vision sensors. No clear, verifiable datasets are identified.",
      "processing_time": 49.63089466094971,
      "citing_paper_id": "167210006",
      "cited_paper_id": 761676
    },
    {
      "context_text": "Their work is discussed in more detail in Chapter 3.3 (Lichtsteiner et al., 2008; Liu and Delbrück, 2010; Chen et al., 2011; Posch et al., 2011).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to works that discuss event-based vision sensors. No clear, verifiable datasets are identified.",
      "processing_time": 49.63089466094971,
      "citing_paper_id": "167210006",
      "cited_paper_id": 6119048
    },
    {
      "context_text": "Their work is discussed in more detail in Chapter 3.3 (Lichtsteiner et al., 2008; Liu and Delbrück, 2010; Chen et al., 2011; Posch et al., 2011).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to works that discuss event-based vision sensors. No clear, verifiable datasets are identified.",
      "processing_time": 49.63089466094971,
      "citing_paper_id": "167210006",
      "cited_paper_id": 15431819
    },
    {
      "context_text": "In Marr and Poggio (1976), the authors laid the foundation for research in this ﬁeld at an early stage.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only foundational research work.",
      "processing_time": 46.43862771987915,
      "citing_paper_id": "167210006",
      "cited_paper_id": 767650
    },
    {
      "context_text": "The research of Marr and Poggio (1976, 1977, 1979) and Marr (1982) forms the beginning of these algorithms.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only foundational research works. The context is about the theoretical beginnings of stereo vision algorithms.",
      "processing_time": 48.78249502182007,
      "citing_paper_id": "167210006",
      "cited_paper_id": 767650
    },
    {
      "context_text": "The research of Marr and Poggio (1976, 1977, 1979) and Marr (1982) forms the beginning of these algorithms.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only foundational research works. The context is about the theoretical beginnings of stereo vision algorithms.",
      "processing_time": 48.78249502182007,
      "citing_paper_id": "167210006",
      "cited_paper_id": 84293972
    },
    {
      "context_text": "Additionally to global and local methods there are also iterative algorithms (Scharstein et al., 2002; Szeliski, 2010) including the biologically motivated approach of Marr and Poggio (1976).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to methods and algorithms. No verifiable resources are identified.",
      "processing_time": 48.327057123184204,
      "citing_paper_id": "167210006",
      "cited_paper_id": 767650
    },
    {
      "context_text": "Additionally to global and local methods there are also iterative algorithms (Scharstein et al., 2002; Szeliski, 2010) including the biologically motivated approach of Marr and Poggio (1976).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to methods and algorithms. No verifiable resources are identified.",
      "processing_time": 48.327057123184204,
      "citing_paper_id": "167210006",
      "cited_paper_id": 27350524
    },
    {
      "context_text": "Examples for event-based stereo vision applications applying networks with spiking neurons are Dikov et al. (2017), Osswald et al. (2017), Rebecq et al. (2017), and Haessig et al. (2019).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to papers that apply networks with spiking neurons for event-based stereo vision.",
      "processing_time": 49.22435975074768,
      "citing_paper_id": "167210006",
      "cited_paper_id": 1082643
    },
    {
      "context_text": "In Rebecq et al. (2017), Rebecq presents a method for Event-based Multi-View Stereo (EMVS).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions a method (EMVS) rather than a specific dataset. No datasets are explicitly named or described in the citation context.",
      "processing_time": 48.3162145614624,
      "citing_paper_id": "167210006",
      "cited_paper_id": 1082643
    },
    {
      "context_text": "It is easily possible to extend this technique, since on the one hand, events from diﬀerent senders can be combined, and on the other hand, forwarding to multiple recipients is feasible (Lazzaro and Wawrzynek, 1995).",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method or protocol extension. No dataset names are present in the citation context.",
      "processing_time": 48.77133536338806,
      "citing_paper_id": "167210006",
      "cited_paper_id": 1843701
    },
    {
      "context_text": "Neuromorphic hardware as SpiNNaker (Furber et al., 2006, 2014), ThrueNorth (Merolla et al., 2014), Spikey (Pfeil et al., 2013), and Loihi (Davies et al., 2018) model the massively parallel structure of the brain.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only neuromorphic hardware platforms. No verifiable resources are identified.",
      "processing_time": 48.495089292526245,
      "citing_paper_id": "167210006",
      "cited_paper_id": 3005339
    },
    {
      "context_text": "Neuromorphic hardware as SpiNNaker (Furber et al., 2006, 2014), ThrueNorth (Merolla et al., 2014), Spikey (Pfeil et al., 2013), and Loihi (Davies et al., 2018) model the massively parallel structure of the brain.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only neuromorphic hardware platforms. No verifiable resources are identified.",
      "processing_time": 48.495089292526245,
      "citing_paper_id": "167210006",
      "cited_paper_id": 12429722
    },
    {
      "context_text": "In Cumming and Parker (1997), theories are investigated to what extent the signals of cortical neurons are related to conscious binocular depth perception.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only theoretical investigations into neuronal responses.",
      "processing_time": 47.04002070426941,
      "citing_paper_id": "167210006",
      "cited_paper_id": 4417488
    },
    {
      "context_text": "A quite general review about the broad range of 3D-reconstruction techniques, is provided in Butime et al. (2006).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a review of 3D reconstruction techniques.",
      "processing_time": 47.526217460632324,
      "citing_paper_id": "167210006",
      "cited_paper_id": 4871280
    },
    {
      "context_text": "Algorithms are based on SNNs and EBS only develop their potential when they are applied on neuromorphic hardware (Khan et al., 2008).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only algorithms and hardware. There are no verifiable resources that meet the criteria.",
      "processing_time": 48.75555777549744,
      "citing_paper_id": "167210006",
      "cited_paper_id": 5203122
    },
    {
      "context_text": "The sensor is characterized by a large contrast range, but suﬀers greatly from temporal redundancies and a temporal resolution which, due to global integration, is limited by the frame rate (Lichtsteiner et al., 2008).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only discusses the characteristics of a vision sensor.",
      "processing_time": 46.56032133102417,
      "citing_paper_id": "167210006",
      "cited_paper_id": 6119048
    },
    {
      "context_text": "The design decisions are based on the three main objectives; high contrast range, low error rate, and low latency (Lichtsteiner et al., 2008).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only design objectives for a vision sensor.",
      "processing_time": 47.272544145584106,
      "citing_paper_id": "167210006",
      "cited_paper_id": 6119048
    },
    {
      "context_text": "Therefore, good results can only be achieved with uniform illumination of the scene (Lichtsteiner et al., 2008).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a requirement for uniform illumination in scenes.",
      "processing_time": 46.812501192092896,
      "citing_paper_id": "167210006",
      "cited_paper_id": 6119048
    },
    {
      "context_text": "The basic idea is based on the addressing of pixels or neurons, with their x-and y-value , within their array (Lichtsteiner et al., 2008).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method or technology. The context is about addressing pixels or neurons within an array, which is not a dataset.",
      "processing_time": 50.82735013961792,
      "citing_paper_id": "167210006",
      "cited_paper_id": 6119048
    },
    {
      "context_text": "This sensor has adaptable photoreceptors and a network capable of spatial smoothing (Lichtsteiner et al., 2008).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a sensor and its capabilities. No verifiable resources are identified.",
      "processing_time": 48.45885634422302,
      "citing_paper_id": "167210006",
      "cited_paper_id": 6119048
    },
    {
      "context_text": "This sensor was used exclusively for test and demonstration purposes prooﬁng biological theses (Lichtsteiner et al., 2008).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific dataset, only a sensor used for testing and demonstrating biological theses. No clear dataset name or usage is provided.",
      "processing_time": 50.18056774139404,
      "citing_paper_id": "167210006",
      "cited_paper_id": 6119048
    },
    {
      "context_text": "The large contrast range of these cameras is based on the logarithmic compression of the photoreceptor circuits and the local, event-based quantization (Lichtsteiner et al., 2008).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only describes the technical aspects of event-based cameras.",
      "processing_time": 47.87030267715454,
      "citing_paper_id": "167210006",
      "cited_paper_id": 6119048
    },
    {
      "context_text": "With the development of neuromorphic visual sensors (Lichtsteiner et al., 2008), a new physical constraint is now also applicable in artiﬁcial vision: time (Kogler et al., 2011a; Rogister et al., 2012; Dikov et al., 2017).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only references to papers discussing neuromorphic visual sensors and their capabilities.",
      "processing_time": 47.86093521118164,
      "citing_paper_id": "167210006",
      "cited_paper_id": 6119048
    },
    {
      "context_text": "(Lichtsteiner et al., 2008).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a sensor technology. No dataset names are present in the context.",
      "processing_time": 47.861164808273315,
      "citing_paper_id": "167210006",
      "cited_paper_id": 6119048
    },
    {
      "context_text": "It comprises the three main layers shown in Figure 2 and both intermediate layers of horizontal and amacrine cells (Boahen, 2005; Zaghloul and Boahen, 2006).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only layers and cells in a silicon retina model. No verifiable resources are identified.",
      "processing_time": 49.29994511604309,
      "citing_paper_id": "167210006",
      "cited_paper_id": 6580214
    },
    {
      "context_text": "It comprises the three main layers shown in Figure 2 and both intermediate layers of horizontal and amacrine cells (Boahen, 2005; Zaghloul and Boahen, 2006).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only layers and cells in a silicon retina model. No verifiable resources are identified.",
      "processing_time": 49.29994511604309,
      "citing_paper_id": "167210006",
      "cited_paper_id": 125802815
    },
    {
      "context_text": "On this basis six algorithms (Kolmogorov and Zabih, 2002; Pons et al., 2005; Goesele et al., 2006; Vogiatzis et al., 2007; Furukawa, 2008) for reconstruction of dense objects with calibrated cameras are calibrated cameras are evaluated.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only algorithms and methods for reconstruction. There are no clear identifiers for datasets.",
      "processing_time": 49.155571937561035,
      "citing_paper_id": "167210006",
      "cited_paper_id": 7863489
    },
    {
      "context_text": "On this basis six algorithms (Kolmogorov and Zabih, 2002; Pons et al., 2005; Goesele et al., 2006; Vogiatzis et al., 2007; Furukawa, 2008) for reconstruction of dense objects with calibrated cameras are calibrated cameras are evaluated.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only algorithms and methods for reconstruction. There are no clear identifiers for datasets.",
      "processing_time": 49.155571937561035,
      "citing_paper_id": "167210006",
      "cited_paper_id": 10715823
    },
    {
      "context_text": "This was fundamentally diﬀerent with the color dynamic and active-pixel vision sensor (C-DAVIS) from Li et al. (2015).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions a sensor (C-DAVIS) but does not refer to a specific dataset. The context does not indicate the use of a dataset for training, evaluation, or any other purpose.",
      "processing_time": 51.99678158760071,
      "citing_paper_id": "167210006",
      "cited_paper_id": 10523713
    },
    {
      "context_text": "C-DAVIS, a neuromorphic sensor capable of color recognition, has been available since 2015, but color perception is only implemented in the synchronous and not in the event-based part of the camera (Li et al., 2015).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a neuromorphic sensor. No dataset names are present in the citation span.",
      "processing_time": 49.53769040107727,
      "citing_paper_id": "167210006",
      "cited_paper_id": 10523713
    },
    {
      "context_text": "This process ensures that the EM circuit is also asynchronous and the corresponding gray-value is updated for each event (Posch et al., 2011, 2014; Brandli, 2015).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only references to methods and hardware design.",
      "processing_time": 47.646554946899414,
      "citing_paper_id": "167210006",
      "cited_paper_id": 10523713
    },
    {
      "context_text": "Furthermore, event-based vision is changing technologies and algorithms in ﬁelds such as health-care, security, surveillance, entertainment and industrial automation (Brandli, 2015).",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only general fields where event-based vision is applied.",
      "processing_time": 47.837326526641846,
      "citing_paper_id": "167210006",
      "cited_paper_id": 10523713
    },
    {
      "context_text": "To avoid unwanted oscillations there is a subdivision into sub-circuits (Brandli, 2015), as shown in Figure 6 .",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method or design approach. No verifiable resources are identified.",
      "processing_time": 48.979700803756714,
      "citing_paper_id": "167210006",
      "cited_paper_id": 10523713
    },
    {
      "context_text": "The fastest approach is Pons et al. (2005) and the slowest one is Goesele et al. (2006).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only references to methods or approaches.",
      "processing_time": 46.75844120979309,
      "citing_paper_id": "167210006",
      "cited_paper_id": 10715823
    },
    {
      "context_text": "The approach of Mallik et al. (2005) goes in a similar direction.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, models, or methods. It only references a similar approach by Mallik et al. (2005).",
      "processing_time": 50.93160438537598,
      "citing_paper_id": "167210006",
      "cited_paper_id": 10736854
    },
    {
      "context_text": "The sensors of Ruedi et al. (2003) as well as Mallik et al. (2005) are, regarding their technical implementation, far superior to the cameras of Mahowald and Mead (1991) and Boahen (2000).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to technical implementations of sensors and cameras. No verifiable resources are identified.",
      "processing_time": 49.9586706161499,
      "citing_paper_id": "167210006",
      "cited_paper_id": 10736854
    },
    {
      "context_text": "The sensors of Ruedi et al. (2003) as well as Mallik et al. (2005) are, regarding their technical implementation, far superior to the cameras of Mahowald and Mead (1991) and Boahen (2000).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to technical implementations of sensors and cameras. No verifiable resources are identified.",
      "processing_time": 49.9586706161499,
      "citing_paper_id": "167210006",
      "cited_paper_id": 14825371
    },
    {
      "context_text": "The sensors of Ruedi et al. (2003) as well as Mallik et al. (2005) are, regarding their technical implementation, far superior to the cameras of Mahowald and Mead (1991) and Boahen (2000).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to technical implementations of sensors and cameras. No verifiable resources are identified.",
      "processing_time": 49.9586706161499,
      "citing_paper_id": "167210006",
      "cited_paper_id": 248269324
    },
    {
      "context_text": "The sensors of Ruedi et al. (2003) as well as Mallik et al. (2005) are, regarding their technical implementation, far superior to the cameras of Mahowald and Mead (1991) and Boahen (2000).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to technical implementations of sensors and cameras. No verifiable resources are identified.",
      "processing_time": 49.9586706161499,
      "citing_paper_id": "167210006",
      "cited_paper_id": null
    },
    {
      "context_text": "EMVS, which is based on the work of Space-Sweep Approach (Collins, 1996), estimates semi-dense 3D-structures with only one event-based camera.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (Space-Sweep Approach) and a type of camera (event-based camera).",
      "processing_time": 50.12873840332031,
      "citing_paper_id": "167210006",
      "cited_paper_id": 11008141
    },
    {
      "context_text": "In the case of Dikov et al. (2017) and Kaiser et al. (2018) this is SpiNNaker.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a hardware platform called SpiNNaker. No datasets are referenced or used in the context provided.",
      "processing_time": 50.12053632736206,
      "citing_paper_id": "167210006",
      "cited_paper_id": 12429722
    },
    {
      "context_text": "The authors implement a technique for topographic map formation on SpiNNaker.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a technique implemented on a hardware platform.",
      "processing_time": 48.01941704750061,
      "citing_paper_id": "167210006",
      "cited_paper_id": 12429722
    },
    {
      "context_text": "It allows spikes to be transferred from neurons of one chip to neurons of a second chip (Boahen, 1998, 2000).",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for transferring spikes between neuromorphic chips.",
      "processing_time": 48.391075134277344,
      "citing_paper_id": "167210006",
      "cited_paper_id": 12952441
    },
    {
      "context_text": "For a comprehensive introduction to SNNs see Maass (1997), Vreeken (2003), and Grüning and Bohte (2014).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only references to introductory materials on spiking neural networks.",
      "processing_time": 48.65299654006958,
      "citing_paper_id": "167210006",
      "cited_paper_id": 14100503
    },
    {
      "context_text": "The scientific team around Ruedi developed one of the first sensors with a stronger focus on applicability (Ruedi et al., 2003).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a sensor developed by Ruedi et al. The context is about the development of a sensor, not the use of a dataset.",
      "processing_time": 51.94411301612854,
      "citing_paper_id": "167210006",
      "cited_paper_id": 14825371
    },
    {
      "context_text": "The scientiﬁc team around Ruedi developed one of the ﬁrst sensors with a stronger focus on applicability (Ruedi et al., 2003).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a sensor developed by Ruedi et al. The context focuses on the development of a sensor, not on the use of a dataset.",
      "processing_time": 52.53167223930359,
      "citing_paper_id": "167210006",
      "cited_paper_id": 14825371
    },
    {
      "context_text": "The scientiﬁc team around Ruedi developed one of the ﬁrst sensors with a stronger focus on applicability (Ruedi et al., 2003).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a sensor developed by Ruedi et al. The context focuses on the development of a sensor, not on the use of a dataset.",
      "processing_time": 52.53167223930359,
      "citing_paper_id": "167210006",
      "cited_paper_id": 248269324
    },
    {
      "context_text": "However, for long there have been no applicable event-based sensors implementing color vision (Delbrück et al., 2010; Posch et al., 2014).",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only the lack of color vision in event-based sensors.",
      "processing_time": 48.647398948669434,
      "citing_paper_id": "167210006",
      "cited_paper_id": 15431819
    },
    {
      "context_text": "In Delbrück et al. (2010) criteria for the classiﬁcation of biologically inspired sensors are introduced.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only criteria for classifying biologically inspired sensors.",
      "processing_time": 48.636688232421875,
      "citing_paper_id": "167210006",
      "cited_paper_id": 15431819
    },
    {
      "context_text": "In Delbrück et al. (2010), the utopian features of a perfect camera are identiﬁed as an inﬁnitely high resolution, an inﬁnitely wide contrast range and an inﬁnite number of frames per second.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only theoretical aspects of a perfect camera. No verifiable resources are identified.",
      "processing_time": 49.91590166091919,
      "citing_paper_id": "167210006",
      "cited_paper_id": 15431819
    },
    {
      "context_text": "The data processing of such a perfect sensor would of course be enormously computationally demanding, but nature also has a solution for this; retinas take over a large part of the processing and thus only transmit relevant information to the brain (Delbrück et al., 2010).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific dataset, only a general concept of event-based vision sensors. No dataset names are present in the citation span.",
      "processing_time": 28.12197780609131,
      "citing_paper_id": "167210006",
      "cited_paper_id": 15431819
    },
    {
      "context_text": "Julesz showed by means of a simple experiment how our brain can reliably solve the correspondence problem (Julesz, 1960, 1964).",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a historical experiment. No verifiable resources are identified.",
      "processing_time": 49.21084427833557,
      "citing_paper_id": "167210006",
      "cited_paper_id": 15641530
    },
    {
      "context_text": "The artiﬁcial photoreceptor (P) is modeled based on the cone and consists of two components, a time-continuous light sensor and an adaptive circuit (Mahowald and Mead, 1991; Mahowald, 1992; Douglas et al., 1995; Posch et al., 2014).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only models and methods. No verifiable resources are identified.",
      "processing_time": 49.45759582519531,
      "citing_paper_id": "167210006",
      "cited_paper_id": 16585751
    },
    {
      "context_text": "The artiﬁcial photoreceptor (P) is modeled based on the cone and consists of two components, a time-continuous light sensor and an adaptive circuit (Mahowald and Mead, 1991; Mahowald, 1992; Douglas et al., 1995; Posch et al., 2014).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only models and methods. No verifiable resources are identified.",
      "processing_time": 49.45759582519531,
      "citing_paper_id": "167210006",
      "cited_paper_id": null
    },
    {
      "context_text": "The obvious advantage of being able to use decades of research, is impaired by the existing disadvantages of frame-based cameras, such as redundancy, high latency etc. (Berner et al., 2013; Posch et al., 2014; Cohen et al., 2017).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to papers discussing frame-based cameras and their limitations.",
      "processing_time": 48.6074001789093,
      "citing_paper_id": "167210006",
      "cited_paper_id": 17870509
    },
    {
      "context_text": "DAVIS, introduced in Berner et al. (2013), is a hybrid of DVS and APS.",
      "catation_intent": "reusable resource",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.2,
      "reasoning": "DAVIS is mentioned as a hybrid of DVS and APS, which are relevant to event-based sensors. However, it is not explicitly referred to as a dataset.",
      "processing_time": 51.04455208778381,
      "citing_paper_id": "167210006",
      "cited_paper_id": 17870509
    },
    {
      "context_text": "Energy eﬃciency, scalability, and real-time interfacing with the environment caused by high parallelism are advantages of this technology (Furber et al., 2014; Davies et al., 2018).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only general advantages of a technology. No dataset names are present in the citation span.",
      "processing_time": 50.34003305435181,
      "citing_paper_id": "167210006",
      "cited_paper_id": 25268038
    },
    {
      "context_text": "In Barnard and Fischler (1982), Dhond and Aggarwal (1989), .",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only references to papers. No verifiable resources are identified.",
      "processing_time": 49.19023156166077,
      "citing_paper_id": "167210006",
      "cited_paper_id": 25699143
    },
    {
      "context_text": "A further diﬀerentiation results in local and global methods (Szeliski, 2010).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a differentiation between local and global methods in computer vision.",
      "processing_time": 49.43771576881409,
      "citing_paper_id": "167210006",
      "cited_paper_id": 27350524
    },
    {
      "context_text": "A further differentiation results in local and global methods (Szeliski, 2010).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a differentiation between local and global methods in computer vision.",
      "processing_time": 49.43421387672424,
      "citing_paper_id": "167210006",
      "cited_paper_id": 27350524
    },
    {
      "context_text": "Newer approaches from this area extract very reliable characteristics and use them as seeds to determine further correspondences (Szeliski, 2010).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only general approaches and methods.",
      "processing_time": 48.13898468017578,
      "citing_paper_id": "167210006",
      "cited_paper_id": 27350524
    },
    {
      "context_text": "Additionally its high temporal resolution is noteworthy (Posch et al., 2007).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific dataset, only a characteristic of a sensor. No dataset names are present in the citation span.",
      "processing_time": 49.6585578918457,
      "citing_paper_id": "167210006",
      "cited_paper_id": 27818987
    },
    {
      "context_text": "In Akolkar et al. (2015), the advantages of event-based sensors for pattern recognition are discussed in detail.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only discusses the advantages of event-based sensors for pattern recognition.",
      "processing_time": 48.877403020858765,
      "citing_paper_id": "167210006",
      "cited_paper_id": 30459761
    },
    {
      "context_text": "The generated pixel data at one time is called frame and thus the frequency of the read-out is called frame rate (Mahowald, 1992; Akolkar et al., 2015).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only discusses the concept of frame rate in the context of neuromorphic event-driven systems.",
      "processing_time": 50.032833099365234,
      "citing_paper_id": "167210006",
      "cited_paper_id": 30459761
    },
    {
      "context_text": "A discussion of the advantages of combining EBS and SNN is done in Akolkar et al. (2015).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a discussion of combining EBS and SNN. No verifiable resources are identified.",
      "processing_time": 50.029340505599976,
      "citing_paper_id": "167210006",
      "cited_paper_id": 30459761
    },
    {
      "context_text": "In the case of a negative change in illumination, if it gets darker, this eﬀect reverses (Rodieck, 1998).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, models, or methods. It only references a book discussing visual perception.",
      "processing_time": 49.15925168991089,
      "citing_paper_id": "167210006",
      "cited_paper_id": 30956636
    },
    {
      "context_text": "Cones for color recognition and sharp vision, as well as rods for vision under bad lighting conditions (Rodieck, 1998).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, models, or methods. It only refers to biological aspects of vision.",
      "processing_time": 49.150102615356445,
      "citing_paper_id": "167210006",
      "cited_paper_id": 30956636
    },
    {
      "context_text": "In the case of a negative change in illumination, if it gets darker, this effect reverses (Rodieck, 1998).",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, models, or methods. It only references a change in illumination effect, which is a general observation.",
      "processing_time": 50.47827076911926,
      "citing_paper_id": "167210006",
      "cited_paper_id": 30956636
    },
    {
      "context_text": "Additionally, a comparison of monocular and binocular stimuli states that binocular have shorter latencies than monocular responses (Adachi-Usami and Lehmann, 1983).",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a comparison of monocular and binocular stimuli. No verifiable resource is identified.",
      "processing_time": 50.01726031303406,
      "citing_paper_id": "167210006",
      "cited_paper_id": 33172686
    },
    {
      "context_text": "Although there are already some implementations of networks on neuromorphic hardware (Dikov et al., 2017; Andreopoulos et al., 2018; Kaiser et al., 2018), research in this area is not that far yet.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only references to existing implementations of networks on neuromorphic hardware.",
      "processing_time": 49.13800668716431,
      "citing_paper_id": "167210006",
      "cited_paper_id": 46937991
    },
    {
      "context_text": "The application of DAVIS or ATIS in contrast to DVS has already signiﬁcantly improved the outcome of several approaches like (Reverter Valeiras et al., 2016; Piatkowska et al., 2017; Andreopoulos et al., 2018; Ieng et al., 2018).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to methods and systems (DAVIS, ATIS, DVS). No clear, verifiable datasets are identified.",
      "processing_time": 52.70447278022766,
      "citing_paper_id": "167210006",
      "cited_paper_id": 46937991
    },
    {
      "context_text": "The application of DAVIS or ATIS in contrast to DVS has already signiﬁcantly improved the outcome of several approaches like (Reverter Valeiras et al., 2016; Piatkowska et al., 2017; Andreopoulos et al., 2018; Ieng et al., 2018).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to methods and systems (DAVIS, ATIS, DVS). No clear, verifiable datasets are identified.",
      "processing_time": 52.70447278022766,
      "citing_paper_id": "167210006",
      "cited_paper_id": 49554392
    },
    {
      "context_text": "Although ﬁrst approaches like (Dikov et al., 2017; Andreopoulos et al., 2018) exist, implying that there is potential.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only references to prior work. There is no indication of a reusable resource being used.",
      "processing_time": 50.8008017539978,
      "citing_paper_id": "167210006",
      "cited_paper_id": 46937991
    },
    {
      "context_text": "Another example is shown in Bogdan et al. (2018).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a reference to another paper. There is no indication of dataset usage or description.",
      "processing_time": 49.99188780784607,
      "citing_paper_id": "167210006",
      "cited_paper_id": 49554229
    },
    {
      "context_text": "According to the authors of Ieng et al. (2018), disparities are thus not reliably detected in uncontrolled lighting conditions and unstructured scenes.",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only discusses challenges in disparity detection under certain conditions.",
      "processing_time": 48.8330020904541,
      "citing_paper_id": "167210006",
      "cited_paper_id": 49554392
    },
    {
      "context_text": "Therefore, Ieng et al. (2018) presents an entirely time-based method that exploits the unique characteristics of neuromorphic sensors, such as high temporal resolution, even more.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method that exploits the characteristics of neuromorphic sensors.",
      "processing_time": 49.122787952423096,
      "citing_paper_id": "167210006",
      "cited_paper_id": 49554392
    },
    {
      "context_text": "The approach of section 4.2.3 introduced in Ieng et al. (2018) was published in summer 2018 and is therefore quite new.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method or approach. There are no clear identifiers for datasets in the provided context.",
      "processing_time": 51.15191650390625,
      "citing_paper_id": "167210006",
      "cited_paper_id": 49554392
    },
    {
      "context_text": "Also noteworthy are the results of Martel et al. (2018) and Haessig et al. (2019).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to other research works. No dataset names are provided in the context.",
      "processing_time": 50.26316523551941,
      "citing_paper_id": "167210006",
      "cited_paper_id": 53086261
    },
    {
      "context_text": "Quite diﬀerent solutions to recover depth from event-based data are shown in Martel et al. (2018) and Haessig et al. (2019).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to papers discussing methods for depth recovery from event-based data.",
      "processing_time": 14.500454187393188,
      "citing_paper_id": "167210006",
      "cited_paper_id": 53086261
    },
    {
      "context_text": "Complementing the event-based stereo setup, two mirrors and a mirror-galvanometer driven laser are used in Martel et al. (2018).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only hardware components used in an experimental setup.",
      "processing_time": 49.35506057739258,
      "citing_paper_id": "167210006",
      "cited_paper_id": 53086261
    },
    {
      "context_text": "Pioneers for silicon retinas are Mahowald and Mead who had already introduced their Silicon VLSI Retina in 1991 (Mahowald and Mead, 1991; Mahowald, 1994).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only the introduction of a silicon retina by Mahowald and Mead. No verifiable resources are identified.",
      "processing_time": 52.09038853645325,
      "citing_paper_id": "167210006",
      "cited_paper_id": 60778343
    },
    {
      "context_text": "Pioneers for silicon retinas are Mahowald and Mead who had already introduced their Silicon VLSI Retina in 1991 (Mahowald and Mead, 1991; Mahowald, 1994).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only the introduction of a silicon retina by Mahowald and Mead. No verifiable resources are identified.",
      "processing_time": 52.09038853645325,
      "citing_paper_id": "167210006",
      "cited_paper_id": null
    },
    {
      "context_text": "The component B additionally converts these signals into ON-and OFF-values (Mahowald, 1994; Posch et al., 2014).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only components and methods. No verifiable resources are identified.",
      "processing_time": 49.34257888793945,
      "citing_paper_id": "167210006",
      "cited_paper_id": 60778343
    },
    {
      "context_text": "There is a greater probability of incorrect correspondences since contrasting changes in lighting are less common for neighboring pixels (Marr and Poggio, 1977; Marr, 1982).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only references to theoretical works on vision processing.",
      "processing_time": 49.57865786552429,
      "citing_paper_id": "167210006",
      "cited_paper_id": 84293972
    },
    {
      "context_text": "So if a disparity of neighboring neurons is consistent, it is more likely to be correct and the corresponding signal is thus ampliﬁed (Marr and Poggio, 1979; Marr, 1982).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only theoretical concepts and models. No verifiable resources are identified.",
      "processing_time": 50.41575360298157,
      "citing_paper_id": "167210006",
      "cited_paper_id": 84293972
    },
    {
      "context_text": "So if a disparity of neighboring neurons is consistent, it is more likely to be correct and the corresponding signal is thus ampliﬁed (Marr and Poggio, 1979; Marr, 1982).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only theoretical concepts and models. No verifiable resources are identified.",
      "processing_time": 50.41575360298157,
      "citing_paper_id": "167210006",
      "cited_paper_id": 120306234
    },
    {
      "context_text": "The surfaces of objects are generally perceived as smooth (Marr, 1982).",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general statement about perception. No verifiable resources are identified.",
      "processing_time": 50.58940315246582,
      "citing_paper_id": "167210006",
      "cited_paper_id": 84293972
    },
    {
      "context_text": "Therefore C 1 inhibits the communication between DSNs in vertical and horizontal direction (Marr and Poggio, 1979; Marr, 1982).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only theoretical models and concepts. No verifiable resources are identified.",
      "processing_time": 50.40831160545349,
      "citing_paper_id": "167210006",
      "cited_paper_id": 84293972
    },
    {
      "context_text": "Therefore C 1 inhibits the communication between DSNs in vertical and horizontal direction (Marr and Poggio, 1979; Marr, 1982).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only theoretical models and concepts. No verifiable resources are identified.",
      "processing_time": 50.40831160545349,
      "citing_paper_id": "167210006",
      "cited_paper_id": 120306234
    },
    {
      "context_text": "Although 3D-wiring has been regarded as the more eﬃcient technology for more than 20 years (Milenkovic and Milutinovic, 1998), there are still only a few immature approaches (Kurino et al., 2000; Culurciello and Andreou, 2005).",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, models, or methods. It only references prior research works and their findings.",
      "processing_time": 49.942890882492065,
      "citing_paper_id": "167210006",
      "cited_paper_id": 110164654
    },
    {
      "context_text": "The approaches presented so far (see chapter 4.1 and 4.2) are all based on the biological theories of binocular vision investigated by Marr and Poggio (1979).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only references to biological theories of binocular vision.",
      "processing_time": 49.94322419166565,
      "citing_paper_id": "167210006",
      "cited_paper_id": 120306234
    },
    {
      "context_text": "On the other hand it concerns accommodation (20–300 cm) caused by the change in shape of the eye lens when objects at diﬀerent distances are focused (Ganong, 1972; Cutting, 1997).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific, verifiable datasets. It discusses accommodation in the eye, which is a physiological process, and does not reference any dataset.",
      "processing_time": 52.813271045684814,
      "citing_paper_id": "167210006",
      "cited_paper_id": 267858765
    },
    {
      "context_text": "On the other hand it concerns accommodation (20–300 cm) caused by the change in shape of the eye lens when objects at diﬀerent distances are focused (Ganong, 1972; Cutting, 1997).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific, verifiable datasets. It discusses accommodation in the eye, which is a physiological process, and does not reference any dataset.",
      "processing_time": 52.813271045684814,
      "citing_paper_id": "167210006",
      "cited_paper_id": null
    },
    {
      "context_text": "The near objects move, in the perspective of the observer faster than the more distant ones (Ganong, 1972; Cutting, 1997; Goldstein, 2015).",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, models, or methods. It only references general observations about visual perception.",
      "processing_time": 50.913490772247314,
      "citing_paper_id": "167210006",
      "cited_paper_id": 267858765
    },
    {
      "context_text": "The near objects move, in the perspective of the observer faster than the more distant ones (Ganong, 1972; Cutting, 1997; Goldstein, 2015).",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, models, or methods. It only references general observations about visual perception.",
      "processing_time": 50.913490772247314,
      "citing_paper_id": "167210006",
      "cited_paper_id": null
    },
    {
      "context_text": "The developers focused on reducing the pixel size to 9 µ m and lower the energy consumption (Yaﬀe et al., 2017).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only hardware development details.",
      "processing_time": 48.20181727409363,
      "citing_paper_id": "167210006",
      "cited_paper_id": null
    },
    {
      "context_text": "Apart from a priori knowledge, the techniques for depth perception can be roughly divided into oculomotor and visual stimuli (Ganong, 1972; Goldstein, 2015).",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, models, or methods. It only provides a general classification of depth perception techniques.",
      "processing_time": 50.563265562057495,
      "citing_paper_id": "167210006",
      "cited_paper_id": null
    },
    {
      "context_text": "The merging of the two separately recorded signals of both eyes, is named fusion and it is necessary to not permanently see double (Ganong, 1972; Goldstein, 2015).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, models, or methods. It only describes a concept related to binocular vision.",
      "processing_time": 36.19916224479675,
      "citing_paper_id": "167210006",
      "cited_paper_id": null
    },
    {
      "context_text": "This coherence is the basis of the correspondence problem in binocular vision shown graphically in Figure 1 (Julesz, 1960; Ganong, 1972; Goldstein, 2015).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to graphical representations and theoretical foundations of binocular vision.",
      "processing_time": 50.89525771141052,
      "citing_paper_id": "167210006",
      "cited_paper_id": null
    },
    {
      "context_text": "If the illumination increases significantly, the depicted process drives the ganglion cells creating action potentials that reach the visual center of the brain via the optic nerve (Ganong, 1972; Goldstein, 2015).",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, models, or methods. It describes a biological process related to vision.",
      "processing_time": 51.07993125915527,
      "citing_paper_id": "167210006",
      "cited_paper_id": null
    },
    {
      "context_text": "Apart from a priori knowledge , the techniques for depth perception can be roughly divided into oculomotor and visual stimuli (Ganong, 1972; Goldstein, 2015).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, models, or methods. It only references general techniques for depth perception.",
      "processing_time": 50.18751668930054,
      "citing_paper_id": "167210006",
      "cited_paper_id": null
    },
    {
      "context_text": "If the illumination increases signiﬁcantly, the depicted process drives the ganglion cells creating action potentials that reach the visual center of the brain via the optic nerve (Ganong, 1972; Goldstein, 2015).",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, models, or methods. It describes a biological process related to vision.",
      "processing_time": 51.06966304779053,
      "citing_paper_id": "167210006",
      "cited_paper_id": null
    },
    {
      "context_text": "On the other hand it concerns accommodation (20–300 cm) caused by the change in shape of the eye lens when objects at different distances are focused (Ganong, 1972; Cutting, 1997).",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, models, or methods. It only refers to accommodation in the context of eye lens shape changes.",
      "processing_time": 51.06579828262329,
      "citing_paper_id": "167210006",
      "cited_paper_id": null
    },
    {
      "context_text": "[3] deals with a topic unrelated to stereo vision, but they also have to correlate events with different timing.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, and the context is about a topic unrelated to stereo vision.",
      "processing_time": 49.72576856613159,
      "citing_paper_id": "7083033",
      "cited_paper_id": null
    },
    {
      "context_text": "In [16], the Marr–Poggio cooperative stereo algorithm [17] is implemented in a chip.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (Marr–Poggio cooperative stereo algorithm) and its implementation in hardware.",
      "processing_time": 52.322691440582275,
      "citing_paper_id": "17693733",
      "cited_paper_id": 767650
    },
    {
      "context_text": "Stereo and motion estimation have been combined using partial derivatives [12] as well as Markov random fields [13].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and approaches for combining stereo and motion estimation.",
      "processing_time": 50.69007658958435,
      "citing_paper_id": "17693733",
      "cited_paper_id": 2610586
    },
    {
      "context_text": "Biological systems are data-driven, and they encode visual data asynchronously as sparse spiking outputs rather than frames of pixel values [2].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general characteristic of biological systems. No dataset names are present in the citation span.",
      "processing_time": 51.05496859550476,
      "citing_paper_id": "17693733",
      "cited_paper_id": 9066827
    },
    {
      "context_text": "Other techniques rely on heuristics to provide matches from a modelbased reasoning [8], or constraints on the temporal derivative of disparity [9].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only techniques and methods. The context is too generic to infer the use of a specific dataset.",
      "processing_time": 52.31254696846008,
      "citing_paper_id": "17693733",
      "cited_paper_id": 10889427
    },
    {
      "context_text": "They rely heavily on the use of local spatiotemporal representation of luminances to encode orientation [6], or to define an extension of cornerbased descriptors to temporal volumes of images [7].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods or approaches. There are no clear identifiers for datasets in the given context.",
      "processing_time": 51.71708703041077,
      "citing_paper_id": "17693733",
      "cited_paper_id": 14463776
    },
    {
      "context_text": "To begin with, we employ the soft spatial-channel attention proposed in [16] to select important motion feature pixels at a fine granularity.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for selecting important motion feature pixels.",
      "processing_time": 49.70343017578125,
      "citing_paper_id": "274611240",
      "cited_paper_id": 3458516
    },
    {
      "context_text": "Some of these meth-ods [4,10,14,34] used 3D convolutions to perform cost volume construction and aggregation, but the high computational and memory costs of 3D convolutions make it hardly apply to high-resolution images and large-scale scenes.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and their limitations.",
      "processing_time": 12.248977899551392,
      "citing_paper_id": "274611240",
      "cited_paper_id": 4252896
    },
    {
      "context_text": "Most of the learning-based methods [4,5,10,15,18,31,33,34] for stereo matching work with RGB images produced by frame-based cameras.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only a general reference to RGB images from frame-based cameras used in stereo matching.",
      "processing_time": 51.70268392562866,
      "citing_paper_id": "274611240",
      "cited_paper_id": 4252896
    },
    {
      "context_text": "…f R , f L ∈ R H × W × D , we compute a 3D correlation volume by restricting computation of the inner product to feature vectors that share the same first index: We construct a 4-layer pyramid { C k } k ∈ [1 , 4] by pooling the last dimension of the correlation volume with repeated average pooling.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a method for computing a 3D correlation volume and constructing a pyramid. No verifiable datasets are referenced.",
      "processing_time": 53.07685613632202,
      "citing_paper_id": "274611240",
      "cited_paper_id": 4252896
    },
    {
      "context_text": "According to the linearized event generation model (EGM) [8,11], assuming a constant illumination condition, the change in brightness ∆L ( u k , t k ) during a short time interval ∆t can be approximated using the information from the images as follows: where ⊤ is the spatial derivatives of the…",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The provided context does not mention any specific datasets, only a model (EGM) and a theoretical approximation method. There are no clear identifiers for datasets.",
      "processing_time": 53.2262601852417,
      "citing_paper_id": "274611240",
      "cited_paper_id": 118684904
    },
    {
      "context_text": "Specifically, according to the linearized event generation model [8,11], the edge pattern present in the event data is dependent on the motion of the moving edges.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a theoretical model about event data. No verifiable resources are identified.",
      "processing_time": 51.01977300643921,
      "citing_paper_id": "274611240",
      "cited_paper_id": 118684904
    },
    {
      "context_text": "Therefore, similar to motion features in optical flow estimation [13,19,28], the motion features in stereo matching can be regarded as an implicit representation of the per-pixel horizontal motion be-tween a rectified stereo pair.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only discusses the concept of motion features in stereo matching.",
      "processing_time": 51.020087003707886,
      "citing_paper_id": "274611240",
      "cited_paper_id": 233033368
    },
    {
      "context_text": "Taking inspiration from optical flow networks GMA [13] and TMA [19], we recognize that the motion features in RAFT structure can be augmented with additional information.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and models. The context focuses on the inspiration from optical flow networks and does not indicate the use of any datasets.",
      "processing_time": 53.62061381340027,
      "citing_paper_id": "274611240",
      "cited_paper_id": 233033368
    },
    {
      "context_text": "We also do not use split two due to the significant difference between training and testing events, as mentioned in [1, 7, 29].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general reference to splits in a dataset. No multi-word proper nouns, acronyms, or hyphenated names with digits are present.",
      "processing_time": 53.61738419532776,
      "citing_paper_id": "274611240",
      "cited_paper_id": 253513043
    },
    {
      "context_text": "The state-of-the-art event stereo network [7] proposed a differentiable event selection network to extract relevant event tensors and fused event features and image features with neighbor cross similarity feature module (NCSF).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method or model. The focus is on the technique used for event stereo depth estimation.",
      "processing_time": 52.71576189994812,
      "citing_paper_id": "274611240",
      "cited_paper_id": 253513043
    },
    {
      "context_text": "For comparison, we borrow the results of event-based method [29] and event-image fusion methods [6, 7, 22] from the original papers, respectively.",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only references to other methods and their results.",
      "processing_time": 50.63781714439392,
      "citing_paper_id": "274611240",
      "cited_paper_id": 253513043
    },
    {
      "context_text": "Then we extract the relevant events following the event selection method proposed in [7].",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for event selection. No dataset names are provided in the context.",
      "processing_time": 51.95606708526611,
      "citing_paper_id": "274611240",
      "cited_paper_id": 253513043
    },
    {
      "context_text": "A few attempts [6, 7, 22] have been made for fusing event and image data to improve stereo matching.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only general attempts at fusing event and image data for stereo matching.",
      "processing_time": 51.669615745544434,
      "citing_paper_id": "274611240",
      "cited_paper_id": 253513043
    },
    {
      "context_text": "Note that the results of DDES [29], EIS [22], and SCSNet [7] are borrowed from the original papers, respectively.",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to other papers' results. No dataset names are provided.",
      "processing_time": 51.94891095161438,
      "citing_paper_id": "274611240",
      "cited_paper_id": 253513043
    },
    {
      "context_text": "Table 1 presents a comparison of our proposed method with previous event-only methods [1, 9, 29, 35] and event-image fusion methods [6,7,22] on the MVSEC dataset.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "MVSEC"
      ],
      "dataset_descriptions": {
        "MVSEC": "Used to compare the proposed method with previous event-only and event-image fusion methods, focusing on performance metrics in event-based stereo depth estimation."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions the MVSEC dataset, which is a specific dataset used for evaluating event-based stereo depth estimation methods.",
      "processing_time": 58.41714286804199,
      "citing_paper_id": "274611240",
      "cited_paper_id": 253513043
    },
    {
      "context_text": "For comparison, we select the same frames used in [7,22,29].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for selecting frames. No verifiable resource names are provided.",
      "processing_time": 52.24710130691528,
      "citing_paper_id": "274611240",
      "cited_paper_id": 253513043
    },
    {
      "context_text": "Our proposed network demonstrates superior performance on the overall splits average than previous methods and the state-of-the-art event-image fusion approach [7].",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a comparison of performance against previous methods and a state-of-the-art approach.",
      "processing_time": 52.51409983634949,
      "citing_paper_id": "274611240",
      "cited_paper_id": 253513043
    },
    {
      "context_text": "To fuse features from both event and image modalities, previous work [7] proposes the neighbor cross similarity feature (NCSF) module to extract representations that capture the correlation between the two modalities.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for fusing features from event and image modalities.",
      "processing_time": 51.64694166183472,
      "citing_paper_id": "274611240",
      "cited_paper_id": 253513043
    },
    {
      "context_text": "Left and right: Recovered inverse depth maps at reference perspectives. hand-crafted features from event data, such as event corners [20]–[23], which are typically adapted from the original Harris [24] and FAST [25] methods.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions 'event corners' and cites papers that discuss corner detection methods, including Harris and FAST. However, no specific dataset names are mentioned.",
      "processing_time": 53.301275968551636,
      "citing_paper_id": "269614135",
      "cited_paper_id": 1388140
    },
    {
      "context_text": "Left and right: Recovered inverse depth maps at reference perspectives. hand-crafted features from event data, such as event corners [20]–[23], which are typically adapted from the original Harris [24] and FAST [25] methods.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions 'event corners' and cites papers that discuss corner detection methods, including Harris and FAST. However, no specific dataset names are mentioned.",
      "processing_time": 53.301275968551636,
      "citing_paper_id": "269614135",
      "cited_paper_id": 1694378
    },
    {
      "context_text": "Left and right: Recovered inverse depth maps at reference perspectives. hand-crafted features from event data, such as event corners [20]–[23], which are typically adapted from the original Harris [24] and FAST [25] methods.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions 'event corners' and cites papers that discuss corner detection methods, including Harris and FAST. However, no specific dataset names are mentioned.",
      "processing_time": 53.301275968551636,
      "citing_paper_id": "269614135",
      "cited_paper_id": 14149386
    },
    {
      "context_text": "…resolution and up to 160 dB dynamic range [1], event cameras are qualified to deal with challenging scenarios that are inaccessible to standard cameras, such as high-speed and/or high-dynamic-range (HDR) tracking [2]–[8], control [9, 10] and Simultaneous Localization and Mapping (SLAM) [11]–[17].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only applications and capabilities of event cameras. No dataset names are present in the text.",
      "processing_time": 52.681721687316895,
      "citing_paper_id": "269614135",
      "cited_paper_id": 9642065
    },
    {
      "context_text": "…resolution and up to 160 dB dynamic range [1], event cameras are qualified to deal with challenging scenarios that are inaccessible to standard cameras, such as high-speed and/or high-dynamic-range (HDR) tracking [2]–[8], control [9, 10] and Simultaneous Localization and Mapping (SLAM) [11]–[17].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only applications and capabilities of event cameras. No dataset names are present in the text.",
      "processing_time": 52.681721687316895,
      "citing_paper_id": "269614135",
      "cited_paper_id": 11454240
    },
    {
      "context_text": "…resolution and up to 160 dB dynamic range [1], event cameras are qualified to deal with challenging scenarios that are inaccessible to standard cameras, such as high-speed and/or high-dynamic-range (HDR) tracking [2]–[8], control [9, 10] and Simultaneous Localization and Mapping (SLAM) [11]–[17].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only applications and capabilities of event cameras. No dataset names are present in the text.",
      "processing_time": 52.681721687316895,
      "citing_paper_id": "269614135",
      "cited_paper_id": 22700300
    },
    {
      "context_text": "…is widely witnessed in event-based VIO pipelines [15, 33], which typically build features from motion-compensated event sets [4] or event images [32], and furthermore, fuse with inertial measurements by means of either the Kalman filter [34] or the keyframe-based nonlinear optimization [35].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only methods and approaches used in event-based VIO pipelines.",
      "processing_time": 51.92106628417969,
      "citing_paper_id": "269614135",
      "cited_paper_id": 12751695
    },
    {
      "context_text": "…is widely witnessed in event-based VIO pipelines [15, 33], which typically build features from motion-compensated event sets [4] or event images [32], and furthermore, fuse with inertial measurements by means of either the Kalman filter [34] or the keyframe-based nonlinear optimization [35].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only methods and approaches used in event-based VIO pipelines.",
      "processing_time": 51.92106628417969,
      "citing_paper_id": "269614135",
      "cited_paper_id": 13984777
    },
    {
      "context_text": "This strategy is widely witnessed in event-based VIO pipelines [15, 33], which typically build features from motion-compensated event sets [4] or event images [32], and furthermore, fuse with inertial measurements by means of either the Kalman filter [34] or the keyframe-based nonlinear…",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only methods and approaches used in event-based VIO pipelines.",
      "processing_time": 51.9212851524353,
      "citing_paper_id": "269614135",
      "cited_paper_id": 13360027
    },
    {
      "context_text": "This strategy is widely witnessed in event-based VIO pipelines [15, 33], which typically build features from motion-compensated event sets [4] or event images [32], and furthermore, fuse with inertial measurements by means of either the Kalman filter [34] or the keyframe-based nonlinear…",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only methods and approaches used in event-based VIO pipelines.",
      "processing_time": 51.9212851524353,
      "citing_paper_id": "269614135",
      "cited_paper_id": 30723444
    },
    {
      "context_text": "We propose a novel method inspired by [14, 37].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only references to methods or findings.",
      "processing_time": 50.07455015182495,
      "citing_paper_id": "269614135",
      "cited_paper_id": 52283776
    },
    {
      "context_text": "Different from [37] which directly uses the number of events as the metric to control the time length for event accumulation, we apply the contrast of event image [32].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods or approaches. The context focuses on the difference in methodology between the current work and the cited work.",
      "processing_time": 53.56540250778198,
      "citing_paper_id": "269614135",
      "cited_paper_id": 52283776
    },
    {
      "context_text": "Additionally, strategies for tracking event corners are presented by [22, 26].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods or strategies for tracking event corners using event cameras.",
      "processing_time": 50.95035266876221,
      "citing_paper_id": "269614135",
      "cited_paper_id": 52988231
    },
    {
      "context_text": "The contrast of image can be quantified by a variety of dispersion metrics, and we simply use the variance loss because of its advantageous accuracy and computation complexity over other alternatives [38].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a discussion about loss functions and their properties.",
      "processing_time": 51.43873190879822,
      "citing_paper_id": "269614135",
      "cited_paper_id": 119309624
    },
    {
      "context_text": "Although the success of these feature-based solutions ( e . g ., [31]) has been witnessed to some extent, event features, however, are not as theoretically robust as their standard-vision counterparts.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a comparison between event features and standard vision features.",
      "processing_time": 51.9007363319397,
      "citing_paper_id": "269614135",
      "cited_paper_id": 235794981
    },
    {
      "context_text": "Endowed with microsecond temporal resolution and up to 160 dB dynamic range [1], event cameras are qualified to deal with challenging scenarios that are inaccessible to standard cameras, such as high-speed and/or high-dynamic-range (HDR) tracking [2]–[8], control [9, 10] and Simultaneous…",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only discusses the capabilities of event cameras.",
      "processing_time": 51.432223320007324,
      "citing_paper_id": "269614135",
      "cited_paper_id": null
    },
    {
      "context_text": "Next, we show how the cooperative approach can be adapted for DVS data and we present our algorithm in more details.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only algorithms and approaches. The cited papers' titles suggest a focus on methods rather than datasets.",
      "processing_time": 53.54800534248352,
      "citing_paper_id": "6079544",
      "cited_paper_id": 11177597
    },
    {
      "context_text": "Next, we show how the cooperative approach can be adapted for DVS data and we present our algorithm in more details.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only algorithms and approaches. The cited papers' titles suggest a focus on methods rather than datasets.",
      "processing_time": 53.54800534248352,
      "citing_paper_id": "6079544",
      "cited_paper_id": 17693733
    },
    {
      "context_text": "Secondly, relative contrast changes are mainly caused by moving objects, which eliminates the need for a time-consuming segmentation of moving (foreground) and static background objects as a preprocessing step, e.g. tracking of people [22].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method or approach for tracking multiple persons using a Dynamic Vision Sensor.",
      "processing_time": 52.82964563369751,
      "citing_paper_id": "185541",
      "cited_paper_id": 310741
    },
    {
      "context_text": "Stereo matching refers generally to the search for corresponding primitive descriptions in views from different perspectives [19] in order to reconstruct 3D information by triangulation.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general concept of stereo matching. No verifiable resources are identified.",
      "processing_time": 52.82774209976196,
      "citing_paper_id": "185541",
      "cited_paper_id": 767650
    },
    {
      "context_text": "Since the early work on event-driven vision in [17], several prototypes of dynamic vision sensors [16, 5, 3] were developed in the last decade and numerous stereo matching approaches based on events were published.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general developments in event-driven vision and stereo matching approaches.",
      "processing_time": 12.84079384803772,
      "citing_paper_id": "185541",
      "cited_paper_id": 1776032
    },
    {
      "context_text": "Aside from this, in Lee et al. [14] a stereo system was used for gesture control.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a stereo system for gesture control. No verifiable resources are identified.",
      "processing_time": 52.185935497283936,
      "citing_paper_id": "185541",
      "cited_paper_id": 2507261
    },
    {
      "context_text": "…efficiency, high temporal resolution and compressed sensing [3] to efficiently solve computer vision tasks: e.g. real-time gesture control interface [14], event-driven formulation of epipolar constraint [4], stereo matching and 3D reconstruction [25] [6] and high-speed object classification [2].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and applications. The context focuses on the use of event-driven processing and stereo silicon retinas for various computer vision tasks.",
      "processing_time": 54.9816575050354,
      "citing_paper_id": "185541",
      "cited_paper_id": 2507261
    },
    {
      "context_text": "Typical examples are highly condensed and possibly enriched descriptions of distinctive scene points used for e.g. solving visual correspondences, object matching and image analysis [1] [20] [32].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only general concepts and applications. The cited papers do not provide additional context to identify specific datasets.",
      "processing_time": 53.52204775810242,
      "citing_paper_id": "185541",
      "cited_paper_id": 3346458
    },
    {
      "context_text": "Typical examples are highly condensed and possibly enriched descriptions of distinctive scene points used for e.g. solving visual correspondences, object matching and image analysis [1] [20] [32].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only general concepts and applications. The cited papers do not provide additional context to identify specific datasets.",
      "processing_time": 53.52204775810242,
      "citing_paper_id": "185541",
      "cited_paper_id": 16560320
    },
    {
      "context_text": "Typical examples are highly condensed and possibly enriched descriptions of distinctive scene points used for e.g. solving visual correspondences, object matching and image analysis [1] [20] [32].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only general concepts and applications. The cited papers do not provide additional context to identify specific datasets.",
      "processing_time": 53.52204775810242,
      "citing_paper_id": "185541",
      "cited_paper_id": 207020771
    },
    {
      "context_text": "However, in non-simultaneous stereo vision like in the case of concentric panoramas [15] data collection spans a two dimensional (x-t) space, where 2D spatial information is not explicitly available.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific, verifiable datasets. It discusses the nature of data collection in non-simultaneous stereo vision but does not refer to any named dataset.",
      "processing_time": 55.098185539245605,
      "citing_paper_id": "185541",
      "cited_paper_id": 5612031
    },
    {
      "context_text": "Events from a panoramic view showing the events polarity ; ON-events (red) and OFF-events (blue). symmetric setup, the ratio of the vertical scaling factor between the left and right view s 2:1 = 1 [15].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific, verifiable datasets. It describes a visual representation of events from a panoramic view, which is not a dataset.",
      "processing_time": 53.23754024505615,
      "citing_paper_id": "185541",
      "cited_paper_id": 5612031
    },
    {
      "context_text": "Panoramic vision in 3D [13] has the advantage of providing a full 360◦ view; hence features and objects can be observed continuously supporting navigation [29] and localization tasks [11], e.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only general concepts and applications of panoramic vision in 3D.",
      "processing_time": 52.172818422317505,
      "citing_paper_id": "185541",
      "cited_paper_id": 7397735
    },
    {
      "context_text": "An area-based approach using an adapted cost measure for event data was presented by Schraml et al. [27] and demonstrated in a real-time tracking application.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method and its application.",
      "processing_time": 50.90877819061279,
      "citing_paper_id": "185541",
      "cited_paper_id": 24236495
    },
    {
      "context_text": "This method is a top performer in the Middlebury database [26].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Middlebury database"
      ],
      "dataset_descriptions": {
        "Middlebury database": "Used to evaluate the performance of the stereo correspondence algorithm, focusing on dense two-frame stereo matching accuracy and robustness."
      },
      "confidence_score": 1.0,
      "reasoning": "The citation mentions the 'Middlebury database' which is a well-known dataset in the field of stereo vision. It is used to evaluate the performance of the method discussed.",
      "processing_time": 60.094796895980835,
      "citing_paper_id": "185541",
      "cited_paper_id": 195859047
    },
    {
      "context_text": "Disparity thereby results from computing the minimal cost path through the cost map of all pairwise costs between two corresponding event map lines [26].",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for computing disparity in stereo vision.",
      "processing_time": 51.38338112831116,
      "citing_paper_id": "185541",
      "cited_paper_id": 195859047
    },
    {
      "context_text": "Consecutively, the disparity map is refined by replacing disparities that are very different to its surroundings with a locally mean value, following the approach from [8].",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for refining disparity maps.",
      "processing_time": 51.8519561290741,
      "citing_paper_id": "185541",
      "cited_paper_id": null
    },
    {
      "context_text": "In order to find the optimal method several disparity computation strategies were evaluated: a winner-takes-all (WTA) strategy, the semi-global matching method [8] and a dynamicprogramming (DP) approach.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods for disparity computation.",
      "processing_time": 52.421191930770874,
      "citing_paper_id": "185541",
      "cited_paper_id": null
    },
    {
      "context_text": "Laserbased systems [31] are capable of providing 360 • panoramas in 3D.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a capability of laser-based systems.",
      "processing_time": 51.560951709747314,
      "citing_paper_id": "185541",
      "cited_paper_id": null
    },
    {
      "context_text": "Some works [37, 11, 38] adopt heuristic cooperative regularization from [26] by deﬁning a spatio-temporal inhibitory and excitatory neighborhood for each event, while others [56, 57] use belief propagation and semi-global matching on sparse MRF models, were nodes are active only during a ﬁxed…",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and approaches. The context focuses on algorithmic techniques rather than data sources.",
      "processing_time": 54.12993884086609,
      "citing_paper_id": "262638843",
      "cited_paper_id": 767650
    },
    {
      "context_text": "Finally, one can use asynchronous networks, where every neuron has an internal state that is updated by events, such as Spiked Neural Networks (SNN) [6] or specially designed convolutional neural networks [5, 35].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models and methods. No verifiable resources are identified.",
      "processing_time": 52.93141150474548,
      "citing_paper_id": "262638843",
      "cited_paper_id": 1234009
    },
    {
      "context_text": "In [4] descriptors are computed as a bank of orientation-sensitive spatial filter responses, in [68] as a vector of distances to closest events in several spatial directions, in [69] as a histogram of orientations of vectors pointing to the closest events in a spatial window.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only methods for computing descriptors in event-driven stereo vision. No clear, verifiable datasets are identified.",
      "processing_time": 15.001560926437378,
      "citing_paper_id": "262638843",
      "cited_paper_id": 1408596
    },
    {
      "context_text": "In [4] descriptors are computed as a bank of orientation-sensitive spatial filter responses, in [68] as a vector of distances to closest events in several spatial directions, in [69] as a histogram of orientations of vectors pointing to the closest events in a spatial window.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only methods for computing descriptors in event-driven stereo vision. No clear, verifiable datasets are identified.",
      "processing_time": 15.001560926437378,
      "citing_paper_id": "262638843",
      "cited_paper_id": 12047627
    },
    {
      "context_text": "Therefore, later methods relied on hand-crafted descriptors [4, 68, 69] and similarity measures [44, 67, 38].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only references to methods and descriptors. No verifiable resources are identified.",
      "processing_time": 54.11737585067749,
      "citing_paper_id": "262638843",
      "cited_paper_id": 1408596
    },
    {
      "context_text": "Therefore, later methods relied on hand-crafted descriptors [4, 68, 69] and similarity measures [44, 67, 38].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only references to methods and descriptors. No verifiable resources are identified.",
      "processing_time": 54.11737585067749,
      "citing_paper_id": "262638843",
      "cited_paper_id": 12047627
    },
    {
      "context_text": "First successes of deep learning in stereo matching were achieved by replacing individual algorithmic elements in legacy methods (often [28, 16]) with neural networks, e.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to legacy methods and neural networks in stereo matching.",
      "processing_time": 53.83547639846802,
      "citing_paper_id": "262638843",
      "cited_paper_id": 1440939
    },
    {
      "context_text": "First successes of deep learning in stereo matching were achieved by replacing individual algorithmic elements in legacy methods (often [28, 16]) with neural networks, e.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to legacy methods and neural networks in stereo matching.",
      "processing_time": 53.83547639846802,
      "citing_paper_id": "262638843",
      "cited_paper_id": null
    },
    {
      "context_text": "An application to even-based recognition with RNNs and long-term memory is [33].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific dataset, only an application of RNNs and long-term memory for event-based recognition.",
      "processing_time": 53.676422119140625,
      "citing_paper_id": "262638843",
      "cited_paper_id": 1561703
    },
    {
      "context_text": "Our proposed Dense Deep Event Stereo (DDES) method performs better than other single viewpoint methods, such as TSES [67], CopNet [38], SGM* [16, 64] and FCVF* [17, 64] and even performs on-par with the Semi-Dense 3D method [64] that fuses depth from several viewpoints using known camera motion.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods and models. There are no verifiable resources that meet the criteria for inclusion.",
      "processing_time": 55.04652810096741,
      "citing_paper_id": "262638843",
      "cited_paper_id": 1680724
    },
    {
      "context_text": "Our proposed Dense Deep Event Stereo (DDES) method performs better than other single viewpoint methods, such as TSES [67], CopNet [38], SGM* [16, 64] and FCVF* [17, 64] and even performs on-par with the Semi-Dense 3D method [64] that fuses depth from several viewpoints using known camera motion.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods and models. There are no verifiable resources that meet the criteria for inclusion.",
      "processing_time": 55.04652810096741,
      "citing_paper_id": "262638843",
      "cited_paper_id": null
    },
    {
      "context_text": "Results for TSES [67] and CopNet [38] are from [67] and results for SemiDense 3D [64], SGM* [16, 64] and FCVF* [17, 64] are from [64].",
      "catation_intent": "findings",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods and models. There are no clear identifiers for datasets within the text.",
      "processing_time": 54.92016315460205,
      "citing_paper_id": "262638843",
      "cited_paper_id": 1680724
    },
    {
      "context_text": "Results for TSES [67] and CopNet [38] are from [67] and results for SemiDense 3D [64], SGM* [16, 64] and FCVF* [17, 64] are from [64].",
      "catation_intent": "findings",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods and models. There are no clear identifiers for datasets within the text.",
      "processing_time": 54.92016315460205,
      "citing_paper_id": "262638843",
      "cited_paper_id": null
    },
    {
      "context_text": "Next, we compare the proposed stereo method to the state-of-the-art event-based methods [67, 64, 38], and to two traditional methods [16, 17] which were adopted to work on event images in [64].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only comparisons to other methods. No verifiable resources are identified.",
      "processing_time": 53.46085596084595,
      "citing_paper_id": "262638843",
      "cited_paper_id": 1680724
    },
    {
      "context_text": "Next, we compare the proposed stereo method to the state-of-the-art event-based methods [67, 64, 38], and to two traditional methods [16, 17] which were adopted to work on event images in [64].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only comparisons to other methods. No verifiable resources are identified.",
      "processing_time": 53.46085596084595,
      "citing_paper_id": "262638843",
      "cited_paper_id": null
    },
    {
      "context_text": "5 TSES [67] 36 44 36 CopNet [38] 61 100 64 SGM* [16, 64] 93 – 119 FCVF* [17, 64] 99 – 103",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only model names and performance metrics. No verifiable resources are identified.",
      "processing_time": 54.09650635719299,
      "citing_paper_id": "262638843",
      "cited_paper_id": 1680724
    },
    {
      "context_text": "5 TSES [67] 36 44 36 CopNet [38] 61 100 64 SGM* [16, 64] 93 – 119 FCVF* [17, 64] 99 – 103",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only model names and performance metrics. No verifiable resources are identified.",
      "processing_time": 54.09650635719299,
      "citing_paper_id": "262638843",
      "cited_paper_id": null
    },
    {
      "context_text": "Results on the Indoor Flying dataset using sparse ground truth, following the protocol from [66] described in §3.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Indoor Flying dataset"
      ],
      "dataset_descriptions": {
        "Indoor Flying dataset": "Used to evaluate event-based stereo depth estimation methods, focusing on performance with sparse ground truth and following a specific evaluation protocol."
      },
      "confidence_score": 0.9,
      "reasoning": "The citation mentions the 'Indoor Flying dataset' which is a specific dataset used for evaluation. The context indicates it is used with sparse ground truth and follows a specific protocol.",
      "processing_time": 62.33466076850891,
      "citing_paper_id": "262638843",
      "cited_paper_id": 3396150
    },
    {
      "context_text": "To preserve time information, in addition to positive and negative event counts, [66] stores the timestamp of the last positive and negative events at every location, while [60] saves the average timestamps of the updates.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods for handling event data in cameras.",
      "processing_time": 52.74097442626953,
      "citing_paper_id": "262638843",
      "cited_paper_id": 3396150
    },
    {
      "context_text": "Existing methods [34, 25, 30, 53, 66, 60] use handcrafted transformations to convert event sequences to frame-based representations, that we call event images.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods for converting event sequences to frame-based representations.",
      "processing_time": 53.809794902801514,
      "citing_paper_id": "262638843",
      "cited_paper_id": 3396150
    },
    {
      "context_text": "Since similar methods [34, 25, 30, 53, 66, 60] worked well in many applications, we use this solution as our baseline.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only references to methods and their applications. No dataset names are present in the citation span.",
      "processing_time": 55.47721242904663,
      "citing_paper_id": "262638843",
      "cited_paper_id": 3396150
    },
    {
      "context_text": "Since similar methods [34, 25, 30, 53, 66, 60] worked well in many applications, we use this solution as our baseline.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only references to methods and their applications. No dataset names are present in the citation span.",
      "processing_time": 55.47721242904663,
      "citing_paper_id": "262638843",
      "cited_paper_id": 10775932
    },
    {
      "context_text": "We use the Multi Vehicle Stereo Event Camera Dataset (MVSEC) [65] which is available online [31].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Multi Vehicle Stereo Event Camera Dataset (MVSEC)"
      ],
      "dataset_descriptions": {
        "Multi Vehicle Stereo Event Camera Dataset (MVSEC)": "Used to evaluate stereo depth estimation methods using event cameras, focusing on 3D perception in dynamic environments."
      },
      "confidence_score": 1.0,
      "reasoning": "The context explicitly mentions the 'Multi Vehicle Stereo Event Camera Dataset (MVSEC)', which is a specific dataset used for 3D perception with event cameras. The dataset is clearly identified and relevant to the research topic.",
      "processing_time": 64.56878280639648,
      "citing_paper_id": "262638843",
      "cited_paper_id": 3416874
    },
    {
      "context_text": "We use the Multi Vehicle Stereo Event Camera Dataset (MVSEC) [65] which is available online [31].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Multi Vehicle Stereo Event Camera Dataset (MVSEC)"
      ],
      "dataset_descriptions": {
        "Multi Vehicle Stereo Event Camera Dataset (MVSEC)": "Used to evaluate stereo depth estimation methods using event cameras, focusing on 3D perception in dynamic environments."
      },
      "confidence_score": 1.0,
      "reasoning": "The context explicitly mentions the 'Multi Vehicle Stereo Event Camera Dataset (MVSEC)', which is a specific dataset used for 3D perception with event cameras. The dataset is clearly identified and relevant to the research topic.",
      "processing_time": 64.56878280639648,
      "citing_paper_id": "262638843",
      "cited_paper_id": null
    },
    {
      "context_text": "The regularization module, implemented as an hourglass network with short-cut connections between the contracting and the expanding parts and 2d [27, 10, 36, 23] or 3d [20, 52, 63, 19, 7] convolutions, enforces stereo matching constraints and computes disparities or a distribution over disparities.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods and architectures. No dataset names are present in the text.",
      "processing_time": 54.38203048706055,
      "citing_paper_id": "262638843",
      "cited_paper_id": 4252896
    },
    {
      "context_text": "An embedding module computes image descriptors for left and right images, a matching module performs a correlation [10, 27, 36, 19, 23, 59, 50], computes matching signatures [52] or simply concatenates [20, 7, 63] left and shifted right descriptors for every disparity.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets by name. It describes methods and modules used in stereo matching but does not reference any particular dataset.",
      "processing_time": 55.6463086605072,
      "citing_paper_id": "262638843",
      "cited_paper_id": 4252896
    },
    {
      "context_text": "An embedding module computes image descriptors for left and right images, a matching module performs a correlation [10, 27, 36, 19, 23, 59, 50], computes matching signatures [52] or simply concatenates [20, 7, 63] left and shifted right descriptors for every disparity.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets by name. It describes methods and modules used in stereo matching but does not reference any particular dataset.",
      "processing_time": 55.6463086605072,
      "citing_paper_id": "262638843",
      "cited_paper_id": 31613165
    },
    {
      "context_text": "An embedding module computes image descriptors for left and right images, a matching module performs a correlation [10, 27, 36, 19, 23, 59, 50], computes matching signatures [52] or simply concatenates [20, 7, 63] left and shifted right descriptors for every disparity.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets by name. It describes methods and modules used in stereo matching but does not reference any particular dataset.",
      "processing_time": 55.6463086605072,
      "citing_paper_id": "262638843",
      "cited_paper_id": 31762881
    },
    {
      "context_text": "An embedding module computes image descriptors for left and right images, a matching module performs a correlation [10, 27, 36, 19, 23, 59, 50], computes matching signatures [52] or simply concatenates [20, 7, 63] left and shifted right descriptors for every disparity.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets by name. It describes methods and modules used in stereo matching but does not reference any particular dataset.",
      "processing_time": 55.6463086605072,
      "citing_paper_id": "262638843",
      "cited_paper_id": 51891703
    },
    {
      "context_text": "An embedding module computes image descriptors for left and right images, a matching module performs a correlation [10, 27, 36, 19, 23, 59, 50], computes matching signatures [52] or simply concatenates [20, 7, 63] left and shifted right descriptors for every disparity.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets by name. It describes methods and modules used in stereo matching but does not reference any particular dataset.",
      "processing_time": 55.6463086605072,
      "citing_paper_id": "262638843",
      "cited_paper_id": 206594275
    },
    {
      "context_text": "Current works solve the stereo matching by training a neural network end-to-end, which combines embedding, matching, regularization, and sometimes reﬁnement modules in a single model [10, 27, 20, 63, 36, 19, 23, 7, 52, 59, 50].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to papers that describe methods or models.",
      "processing_time": 54.07175588607788,
      "citing_paper_id": "262638843",
      "cited_paper_id": 4252896
    },
    {
      "context_text": "in robotics [29], medical imaging [32], remote sensing [47], or computational photography [55, 2].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general application areas. No dataset names are present in the context.",
      "processing_time": 34.83772397041321,
      "citing_paper_id": "262638843",
      "cited_paper_id": 4694685
    },
    {
      "context_text": "in robotics [29], medical imaging [32], remote sensing [47], or computational photography [55, 2].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general application areas. No dataset names are present in the context.",
      "processing_time": 34.83772397041321,
      "citing_paper_id": "262638843",
      "cited_paper_id": 12986049
    },
    {
      "context_text": "Finally, a variety of works is dedicated to implement and perform stereo matching on neuromorphic chips and ﬁeld-programmable gate arrays (FPGA) [1, 9].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only hardware implementations of stereo matching. No dataset names are present in the citation context.",
      "processing_time": 55.45800828933716,
      "citing_paper_id": "262638843",
      "cited_paper_id": 5575601
    },
    {
      "context_text": "Finally, a variety of works is dedicated to implement and perform stereo matching on neuromorphic chips and ﬁeld-programmable gate arrays (FPGA) [1, 9].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only hardware implementations of stereo matching. No dataset names are present in the citation context.",
      "processing_time": 55.45800828933716,
      "citing_paper_id": "262638843",
      "cited_paper_id": 34855834
    },
    {
      "context_text": "Finally, a variety of works is dedicated to implement and perform stereo matching on neuromorphic chips and fieldprogrammable gate arrays (FPGA) [1, 9].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only hardware implementations of stereo matching algorithms.",
      "processing_time": 53.78406858444214,
      "citing_paper_id": "262638843",
      "cited_paper_id": 5575601
    },
    {
      "context_text": "Finally, a variety of works is dedicated to implement and perform stereo matching on neuromorphic chips and fieldprogrammable gate arrays (FPGA) [1, 9].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only hardware implementations of stereo matching algorithms.",
      "processing_time": 53.78406858444214,
      "citing_paper_id": "262638843",
      "cited_paper_id": 34855834
    },
    {
      "context_text": "For example, [34] saves the polarity of the last event, [30] sums event polarities in every location during a predetermined time interval, and [25] counts the number of positive and negative events to avoid information loss due to polarity cancellation.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods for processing event-based data. No dataset names are present.",
      "processing_time": 55.148188829422,
      "citing_paper_id": "262638843",
      "cited_paper_id": 10775932
    },
    {
      "context_text": "Existing methods [34, 25, 30, 53, 66, 60] use hand-crafted transformations to convert event sequences to frame-based representations, that we call event images .",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods for converting event sequences to frame-based representations.",
      "processing_time": 55.145108461380005,
      "citing_paper_id": "262638843",
      "cited_paper_id": 10775932
    },
    {
      "context_text": "Most methods use fixed accumulation intervals [22, 42, 4, 44], while [69] sets accumulation time equal to the average of the inverse of the event rate, and [67] warps event positions as if they were all triggered at the same time using depth hypothesis and known camera motion.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods and approaches. No dataset names are present in the text.",
      "processing_time": 55.74361777305603,
      "citing_paper_id": "262638843",
      "cited_paper_id": 12047627
    },
    {
      "context_text": "Nobel prize wining experiments [18] showed that the retina is most sensitive to temporal brightness gradients, and is blind to static scenes in absence of eye movements [41].",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only experimental findings about the retina's sensitivity to temporal brightness gradients.",
      "processing_time": 55.62045216560364,
      "citing_paper_id": "262638843",
      "cited_paper_id": 14801990
    },
    {
      "context_text": "Most methods use ﬁxed accumulation intervals [22, 42, 4, 44], while [69] sets accumulation time equal to the average of the inverse of the event rate, and [67] warps event positions as if they were all triggered at the same time using depth hypothesis and known camera motion.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and approaches. The context focuses on different techniques for handling event data in stereo matching.",
      "processing_time": 56.35130572319031,
      "citing_paper_id": "262638843",
      "cited_paper_id": 17693733
    },
    {
      "context_text": "Early methods [22, 42] compared events using only their timestamps which led to matching ambiguities, due to the noise, variable cameras sensitivity, and imperfect camera synchronization.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and challenges in event-based stereo matching.",
      "processing_time": 55.4385290145874,
      "citing_paper_id": "262638843",
      "cited_paper_id": 17693733
    },
    {
      "context_text": "Some works [37, 11, 38] adopt heuristic cooperative regularization from [26] by defining a spatio-temporal inhibitory and excitatory neighborhood for each event, while others [56, 57] use belief propagation and semi-global matching on sparse MRF models, were nodes are active only during a fixed interval after receiving an event.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions methods and approaches but does not explicitly reference any specific datasets. The cited papers' titles suggest they are about methods rather than datasets.",
      "processing_time": 56.94755005836487,
      "citing_paper_id": "262638843",
      "cited_paper_id": 22158024
    },
    {
      "context_text": "Some works [37, 11, 38] adopt heuristic cooperative regularization from [26] by defining a spatio-temporal inhibitory and excitatory neighborhood for each event, while others [56, 57] use belief propagation and semi-global matching on sparse MRF models, were nodes are active only during a fixed interval after receiving an event.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions methods and approaches but does not explicitly reference any specific datasets. The cited papers' titles suggest they are about methods rather than datasets.",
      "processing_time": 56.94755005836487,
      "citing_paper_id": "262638843",
      "cited_paper_id": 65040501
    },
    {
      "context_text": "Current works solve the stereo matching by training a neural network end-to-end, which combines embedding, matching, regularization, and sometimes refinement modules in a single model [10, 27, 20, 63, 36, 19, 23, 7, 52, 59, 50].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only methods and models. The cited papers' titles do not provide additional dataset information.",
      "processing_time": 56.604628562927246,
      "citing_paper_id": "262638843",
      "cited_paper_id": 31613165
    },
    {
      "context_text": "Current works solve the stereo matching by training a neural network end-to-end, which combines embedding, matching, regularization, and sometimes refinement modules in a single model [10, 27, 20, 63, 36, 19, 23, 7, 52, 59, 50].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only methods and models. The cited papers' titles do not provide additional dataset information.",
      "processing_time": 56.604628562927246,
      "citing_paper_id": "262638843",
      "cited_paper_id": 31762881
    },
    {
      "context_text": "Current works solve the stereo matching by training a neural network end-to-end, which combines embedding, matching, regularization, and sometimes refinement modules in a single model [10, 27, 20, 63, 36, 19, 23, 7, 52, 59, 50].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only methods and models. The cited papers' titles do not provide additional dataset information.",
      "processing_time": 56.604628562927246,
      "citing_paper_id": "262638843",
      "cited_paper_id": 51891703
    },
    {
      "context_text": "Current works solve the stereo matching by training a neural network end-to-end, which combines embedding, matching, regularization, and sometimes refinement modules in a single model [10, 27, 20, 63, 36, 19, 23, 7, 52, 59, 50].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only methods and models. The cited papers' titles do not provide additional dataset information.",
      "processing_time": 56.604628562927246,
      "citing_paper_id": "262638843",
      "cited_paper_id": 206594275
    },
    {
      "context_text": "The regularization module, implemented as an hourglass network with shortcut connections between the contracting and the expanding parts and 2d [27, 10, 36, 23] or 3d [20, 52, 63, 19, 7] convolutions, enforces stereo matching constraints and computes disparities or a distribution over disparities.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets by name. It describes a method for stereo matching using a regularization module and convolutional networks.",
      "processing_time": 56.60205078125,
      "citing_paper_id": "262638843",
      "cited_paper_id": 31613165
    },
    {
      "context_text": "The regularization module, implemented as an hourglass network with shortcut connections between the contracting and the expanding parts and 2d [27, 10, 36, 23] or 3d [20, 52, 63, 19, 7] convolutions, enforces stereo matching constraints and computes disparities or a distribution over disparities.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets by name. It describes a method for stereo matching using a regularization module and convolutional networks.",
      "processing_time": 56.60205078125,
      "citing_paper_id": "262638843",
      "cited_paper_id": 31762881
    },
    {
      "context_text": "The regularization module, implemented as an hourglass network with shortcut connections between the contracting and the expanding parts and 2d [27, 10, 36, 23] or 3d [20, 52, 63, 19, 7] convolutions, enforces stereo matching constraints and computes disparities or a distribution over disparities.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets by name. It describes a method for stereo matching using a regularization module and convolutional networks.",
      "processing_time": 56.60205078125,
      "citing_paper_id": "262638843",
      "cited_paper_id": 206594275
    },
    {
      "context_text": "Best results are obtained with fully-supervised training on large synthetic datasets with ground truth [27] and an L(1) or cross-entropy [52] loss, while some methods use weakly supervised settings [63, 39, 59], relying on geometric constraints of the task.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.0,
      "reasoning": "The context mentions 'large synthetic datasets' but does not provide a specific name. No other datasets are explicitly named.",
      "processing_time": 56.337597608566284,
      "citing_paper_id": "262638843",
      "cited_paper_id": 31613165
    },
    {
      "context_text": "Finally, some methods [36, 23, 19] have a refinement module, that improves the initial low-resolution disparity relying on leftright warping error.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and a refinement module. No verifiable resources are identified.",
      "processing_time": 56.177138328552246,
      "citing_paper_id": "262638843",
      "cited_paper_id": 31762881
    },
    {
      "context_text": "similarity metric [62, 24, 51, 61, 8], smoothness penalty [45, 21], matching confidence [46] and disparity post-processing [13].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to methods or aspects of stereo depth estimation.",
      "processing_time": 56.0493848323822,
      "citing_paper_id": "262638843",
      "cited_paper_id": 32514247
    },
    {
      "context_text": "Most recent methods [59, 50] further",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to other papers. There is no information about the usage of datasets or their characteristics.",
      "processing_time": 57.197344064712524,
      "citing_paper_id": "262638843",
      "cited_paper_id": 51891703
    },
    {
      "context_text": "Best results are obtained with fully-supervised training on large synthetic datasets with ground truth [27] and an L or cross-entropy [52] loss, while some methods use weakly supervised settings [63, 39, 59], relying on geometric constraints of the task.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.3,
      "reasoning": "The context mentions 'large synthetic datasets' but does not provide a specific name. The cited paper titles do not clarify a specific dataset name either.",
      "processing_time": 57.3483681678772,
      "citing_paper_id": "262638843",
      "cited_paper_id": 51891703
    },
    {
      "context_text": "Best results are obtained with fully-supervised training on large synthetic datasets with ground truth [27] and an L or cross-entropy [52] loss, while some methods use weakly supervised settings [63, 39, 59], relying on geometric constraints of the task.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.3,
      "reasoning": "The context mentions 'large synthetic datasets' but does not provide a specific name. The cited paper titles do not clarify a specific dataset name either.",
      "processing_time": 57.3483681678772,
      "citing_paper_id": "262638843",
      "cited_paper_id": 206594275
    },
    {
      "context_text": "…preserve spatial information, but are not applicable for the same reason as CNNs. Finally, one can use asynchronous networks, where every neuron has an internal state that is updated by events, such as Spiked Neural Networks (SNN) [6] or specially designed convolutional neural networks [5, 35].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and models. The context focuses on the use of asynchronous networks and SNNs for event-based processing.",
      "processing_time": 57.80018472671509,
      "citing_paper_id": "262638843",
      "cited_paper_id": 195346586
    },
    {
      "context_text": "Several studies have applied events timing together with additional constraints to compute depth from stereo visual information (Marr and Poggio, 1976; Mahowald and Delbrück, 1989; Tsang and Shi, 2004; Kogler et al., 2009; Domínguez-Morales et al., 2012; Carneiro et al., 2013; Serrano-Gotarredona…",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to various studies and methods. No clear, verifiable datasets are identified.",
      "processing_time": 14.867557764053345,
      "citing_paper_id": "12047627",
      "cited_paper_id": 767650
    },
    {
      "context_text": "In the second one, we selected a small cluster of pixels which responded to that LED with a ﬁr-ing rate above a certain threshold, and we calculated the average coordinate, obtaining sub-pixel accuracy.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific, verifiable datasets. It describes a method for selecting a cluster of pixels and calculating their average coordinate.",
      "processing_time": 57.065046072006226,
      "citing_paper_id": "12047627",
      "cited_paper_id": 1150626
    },
    {
      "context_text": "In order to obtain the projection matrices of a system, many different techniques have been proposed, and they can be classiﬁed into the following two categories (Zhang, 2000): • Photogrammetric calibration: using a calibration object with known geometry in 3D space.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for camera calibration. There are no verifiable resources that meet the criteria.",
      "processing_time": 56.91353464126587,
      "citing_paper_id": "12047627",
      "cited_paper_id": 1150626
    },
    {
      "context_text": ", 2011; Serrano-Gotarredona and Linares-Barranco, 2013) or auditory systems (Lazzaro et al., 1993; Cauwenberghs et al., 1998; Chan et al., 2007).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to papers discussing auditory systems and silicon cochleae. No verifiable datasets are identified.",
      "processing_time": 57.561131954193115,
      "citing_paper_id": "12047627",
      "cited_paper_id": 1310843
    },
    {
      "context_text": "…very popular, and different sensing chips have been reported for vision (Lichtsteiner et al., 2008; Leñero-Bardallo et al., 2010, 2011; Posch et al., 2011; Serrano-Gotarredona and Linares-Barranco, 2013) or auditory systems (Lazzaro et al., 1993; Cauwenberghs et al., 1998; Chan et al., 2007).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various sensing chips and systems. No verifiable resources are identified.",
      "processing_time": 56.57002568244934,
      "citing_paper_id": "12047627",
      "cited_paper_id": 1310843
    },
    {
      "context_text": "…senses relevant information (like a change in the relative light) it asynchronously sends an event out, which can be processed by event-based processors (Venier et al., 1997; Choi et al., 2005; Silver et al., 2007; Khan et al., 2008; Camuñas-Mesa et al., 2011, 2012; Zamarreño-Ramos et al., 2013).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to methods and concepts in event-based processing.",
      "processing_time": 56.14668583869934,
      "citing_paper_id": "12047627",
      "cited_paper_id": 2311346
    },
    {
      "context_text": "In particular, it must add the kernel values to the pixels belonging to the appropriate neighborhood around the address of the input event, as done in previous event-driven convolution processors (Serrano-Gotarredona et al., 1999, 2006, 2008, 2009; Camuñas-Mesa et al., 2011, 2012).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only references to previous work on event-driven convolution processors.",
      "processing_time": 56.14427351951599,
      "citing_paper_id": "12047627",
      "cited_paper_id": 3641670
    },
    {
      "context_text": "In particular, it must add the kernel values to the pixels belonging to the appropriate neighborhood around the address of the input event, as done in previous event-driven convolution processors (Serrano-Gotarredona et al., 1999, 2006, 2008, 2009; Camuñas-Mesa et al., 2011, 2012).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only references to previous work on event-driven convolution processors.",
      "processing_time": 56.14427351951599,
      "citing_paper_id": "12047627",
      "cited_paper_id": 8287877
    },
    {
      "context_text": "One of the most useful advantages of event-driven DVS based vision sensing and processing is the high temporal resolution down to fractions of micro seconds (Lichtsteiner et al., 2008; Posch et al., 2011; Serrano-Gotarredona and Linares-Barranco, 2013).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only discusses the advantages of event-driven DVS sensors.",
      "processing_time": 55.686734437942505,
      "citing_paper_id": "12047627",
      "cited_paper_id": 6119048
    },
    {
      "context_text": "…used in our experimental setup generate output events when they detect a change in luminance in a pixel, indicating in the polarity of the event if that change means increasing or decreasing luminance (Lichtsteiner et al., 2008; Posch et al., 2011; Serrano-Gotarredona and Linares-Barranco, 2013).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation describes the functionality of event-based sensors but does not mention any specific datasets.",
      "processing_time": 55.23381996154785,
      "citing_paper_id": "12047627",
      "cited_paper_id": 6119048
    },
    {
      "context_text": "These properties (high speed and low energy) are making AER sensors very popular, and different sensing chips have been reported for vision (Lichtsteiner et al., 2008; Leñero-Bardallo et al., 2010, 2011; Posch et al., 2011; Serrano-Gotarredona and Linares-Barranco, 2013) or auditory systems…",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only references to various AER sensor chips and their properties. No verifiable dataset names are present.",
      "processing_time": 57.54425597190857,
      "citing_paper_id": "12047627",
      "cited_paper_id": 6119048
    },
    {
      "context_text": "This system can be solved using the 36 pairs of points mentioned before (Benosman et al., 2011).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific dataset, only a method or system described in the cited paper.",
      "processing_time": 55.799124002456665,
      "citing_paper_id": "12047627",
      "cited_paper_id": 14354416
    },
    {
      "context_text": "This ﬁrst algorithm (Carneiro et al., 2013) consists of applying the following restrictions (1–4) to the events generated by the silicon retinas.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only an algorithm and its application to events from silicon retinas.",
      "processing_time": 56.5488805770874,
      "citing_paper_id": "12047627",
      "cited_paper_id": 16160208
    },
    {
      "context_text": "…have applied events timing together with additional constraints to compute depth from stereo visual information (Marr and Poggio, 1976; Mahowald and Delbrück, 1989; Tsang and Shi, 2004; Kogler et al., 2009; Domínguez-Morales et al., 2012; Carneiro et al., 2013; Serrano-Gotarredona et al., 2013).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to methods and research works. No dataset names are present in the text.",
      "processing_time": 56.88850164413452,
      "citing_paper_id": "12047627",
      "cited_paper_id": 16160208
    },
    {
      "context_text": "As is described in detail in (Hartley and Zisserman, 2003), when a 3D point in space M is projected onto pixel m 1 in retina 1, the corresponding pixel m 2 lies on an epipolar line in retina 2 (Carneiro et al., 2013).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only theoretical concepts and methods.",
      "processing_time": 55.21960115432739,
      "citing_paper_id": "12047627",
      "cited_paper_id": 16160208
    },
    {
      "context_text": "Reconstructing the original 3D point requires matching each pair of events produced by point M at time t (Carneiro et al., 2013).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for 3D reconstruction using event-based data. No clear, verifiable dataset names are present.",
      "processing_time": 58.460957050323486,
      "citing_paper_id": "12047627",
      "cited_paper_id": 16160208
    },
    {
      "context_text": "This calibration object usually consists of two or three planes orthogonal to each other (Faugeras, 1993).",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific, verifiable datasets. It refers to a method or object used in calibration, which is not a dataset.",
      "processing_time": 57.95931124687195,
      "citing_paper_id": "12047627",
      "cited_paper_id": 61523068
    },
    {
      "context_text": "The work from Litzenberger etal [5] describes a vehicle counting system using the same silicon retina sensor described in the work from Lichtsteiner etal [6], which is developed at the AIT(2)/ETH(3) and also used for the described stereo vision system in this paper.",
      "catation_intent": "reusable resource",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions a silicon retina sensor but does not specify a dataset. The sensor is used for a vehicle counting system and a stereo vision system, but no dataset names are provided.",
      "processing_time": 59.399468421936035,
      "citing_paper_id": "27059477",
      "cited_paper_id": 3130866
    },
    {
      "context_text": "The work from Litzenberger etal [5] describes a vehicle counting system using the same silicon retina sensor described in the work from Lichtsteiner etal [6], which is developed at the AIT(2)/ETH(3) and also used for the described stereo vision system in this paper.",
      "catation_intent": "reusable resource",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions a silicon retina sensor but does not specify a dataset. The sensor is used for a vehicle counting system and a stereo vision system, but no dataset names are provided.",
      "processing_time": 59.399468421936035,
      "citing_paper_id": "27059477",
      "cited_paper_id": 15357188
    },
    {
      "context_text": "Hess [4] used a global disparity filter in his work to find a main disparity of the received events.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (global disparity filter) used in the context of event-based stereo matching.",
      "processing_time": 57.95449471473694,
      "citing_paper_id": "27059477",
      "cited_paper_id": 7083033
    },
    {
      "context_text": "Fukushima etal [2] describe in their work an early implementation of an artificial retina.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific dataset, only an early implementation of an artificial retina. No verifiable dataset is referenced.",
      "processing_time": 13.234808921813965,
      "citing_paper_id": "27059477",
      "cited_paper_id": 62613141
    },
    {
      "context_text": "This communication protocol is an asynchronous digital multiplexing protocol, previously proposed by (Mahowald, 1992) and (Sivilotti, 1991) in order to model the transmission of neural information in biological systems.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only references to communication protocols and neural information transmission.",
      "processing_time": 56.525392293930054,
      "citing_paper_id": "30913835",
      "cited_paper_id": 1776032
    },
    {
      "context_text": "Several AER designs have been proposed in the literature (Mahowald, 1992)(Mortara, 1998), and the one we used in this paper was developed by (Boahen, 2000).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions AER designs but does not refer to any specific dataset. The cited papers are about neuromorphic systems and do not mention datasets.",
      "processing_time": 58.44098925590515,
      "citing_paper_id": "30913835",
      "cited_paper_id": 1776032
    },
    {
      "context_text": "Several AER designs have been proposed in the literature (Mahowald, 1992)(Mortara, 1998), and the one we used in this paper was developed by (Boahen, 2000).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions AER designs but does not refer to any specific dataset. The cited papers are about neuromorphic systems and do not mention datasets.",
      "processing_time": 58.44098925590515,
      "citing_paper_id": "30913835",
      "cited_paper_id": 15184309
    },
    {
      "context_text": "Moreover it has been proven analytically (Mahowald, 1992), that used with a system that has a sparse activation profile (as transient imager in our case) the address-event communication framework is able to preserve timing information orders of magnitude better than sequential scan.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method or system. The context focuses on the analytical proof of the address-event communication framework's ability to preserve timing information.",
      "processing_time": 59.17273163795471,
      "citing_paper_id": "30913835",
      "cited_paper_id": 1776032
    },
    {
      "context_text": "The pixel address-event data are read-out through non-greedy Boahen-type arbiter (Boahen, 2000).",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific dataset, only a method for reading out pixel address-event data. No verifiable dataset is referenced.",
      "processing_time": 57.125991106033325,
      "citing_paper_id": "30913835",
      "cited_paper_id": 15184309
    },
    {
      "context_text": "The computation of internal camera parameters (including focal length, principal point, skew coefficient and radial lens distortion), external parameter (rotation matrix and translation vector) and epipolar line alignment was performed using the Camera Calibration Toolbox (Bouguet, 2005) that employs a nonlinear gradient descent technique.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a method (Camera Calibration Toolbox) used for calibration. No datasets are referenced.",
      "processing_time": 57.73235583305359,
      "citing_paper_id": "30913835",
      "cited_paper_id": null
    },
    {
      "context_text": "The computation of internal camera parameters (including focal length, principal point, skew coefficient and radial lens distortion), external parameter (rotation matrix and translation vector) and epipolar line alignment was performed using the Camera Calibration Toolbox® (Bouguet, 2005) that employs a nonlinear gradient descent technique.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions the Camera Calibration Toolbox, which is a method or tool, not a dataset. No datasets are explicitly mentioned or used in the described context.",
      "processing_time": 58.622008323669434,
      "citing_paper_id": "30913835",
      "cited_paper_id": null
    },
    {
      "context_text": "…(including focal length, principal point, skew coefficient and radial lens distortion), external parameter (rotation matrix and translation vector) and epipolar line alignment was performed using the Camera Calibration Toolbox® (Bouguet, 2005) that employs a nonlinear gradient descent technique.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a tool (Camera Calibration Toolbox) used for calibration. No verifiable datasets are referenced.",
      "processing_time": 58.261459827423096,
      "citing_paper_id": "30913835",
      "cited_paper_id": null
    },
    {
      "context_text": "The realm of loop-closure detection predominantly features two methodologies: mid-level features [20] and the bag-of-words model [21].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methodologies. The context is about loop-closure detection methods, not datasets.",
      "processing_time": 57.116493701934814,
      "citing_paper_id": "259075396",
      "cited_paper_id": 710586
    },
    {
      "context_text": "With the ability to adeptly handle swift motions, scenes with significant HDR, and challenging lighting, event cameras emerge as superior alternatives, increasing both SLAM and VO’s robustness and precision in environments that would confound traditional cameras.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only discusses the advantages of event cameras over traditional cameras in certain environments.",
      "processing_time": 57.494139194488525,
      "citing_paper_id": "259075396",
      "cited_paper_id": 8524627
    },
    {
      "context_text": "Towards a traditional frame reconstruction from events, [27] proposes a",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for real-time 3D reconstruction and tracking using an event camera.",
      "processing_time": 57.263046979904175,
      "citing_paper_id": "259075396",
      "cited_paper_id": 26324573
    },
    {
      "context_text": "Method B/A D/I EPLa LCb More Information Kim [27] B D LIR × 3 EKFs + Image reconst.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.0,
      "reasoning": "The citation does not mention any specific datasets, only methods and approaches. The context is too sparse to identify any verifiable resources.",
      "processing_time": 58.25059151649475,
      "citing_paper_id": "259075396",
      "cited_paper_id": 26324573
    },
    {
      "context_text": "Stereo DAVIS sequences evaluation from [29] with δPh align = [0, 0] ⊤ since both sensors (event-based and frame-based) share the same photoreceptor pixel array.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Stereo DAVIS sequences"
      ],
      "dataset_descriptions": {
        "Stereo DAVIS sequences": "Used to evaluate stereo depth estimation with event-based and frame-based sensors sharing the same photoreceptor pixel array, focusing on alignment and reconstruction accuracy."
      },
      "confidence_score": 0.9,
      "reasoning": "The citation mentions 'Stereo DAVIS sequences' which appears to be a specific dataset used for evaluation in the context of stereo event cameras. No other datasets are mentioned.",
      "processing_time": 64.98071241378784,
      "citing_paper_id": "259075396",
      "cited_paper_id": 49877954
    },
    {
      "context_text": "Some popular graph-based SLAM methods include ORB-SLAM [15], Basalt [16], and VINSFusion [17].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions methods and tools (ORB-SLAM, Basalt, VINSFusion) but does not reference any specific datasets. No verifiable resources are identified.",
      "processing_time": 59.66862773895264,
      "citing_paper_id": "259075396",
      "cited_paper_id": 119105448
    },
    {
      "context_text": "Some popular graph-based SLAM methods include ORB-SLAM [15], Basalt [16], and VINSFusion [17].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions methods and tools (ORB-SLAM, Basalt, VINSFusion) but does not reference any specific datasets. No verifiable resources are identified.",
      "processing_time": 59.66862773895264,
      "citing_paper_id": "259075396",
      "cited_paper_id": 220713377
    },
    {
      "context_text": "Method VECtor sequences [8] TUM-VIE sequences [9] Mean Mean corridors corridors units units school school mocap mocap mocap mocap mocap VECtor TUM-VIE dolly walk dolly scooter dolly scooter 1d-trans 3d-trans 6dof desk desk2 large-scale small-scale ORB-SLAM3 (SVIO) [15] 0.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "VECtor sequences",
        "TUM-VIE sequences"
      ],
      "dataset_descriptions": {
        "VECtor sequences": "Used to evaluate multi-sensor SLAM systems, focusing on event-centric data for accurate localization and mapping.",
        "TUM-VIE sequences": "Used to evaluate multi-sensor SLAM systems, providing diverse sequences for testing robustness in various environments."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions 'VECtor sequences' and 'TUM-VIE sequences', which are specific datasets used for evaluating multi-sensor SLAM systems. These datasets are relevant to event-based stereo depth estimation.",
      "processing_time": 67.8047981262207,
      "citing_paper_id": "259075396",
      "cited_paper_id": 220713377
    },
    {
      "context_text": "Method VECtor sequences [8] TUM-VIE sequences [9] Mean Mean corridors corridors units units school school mocap mocap mocap mocap mocap VECtor TUM-VIE dolly walk dolly scooter dolly scooter 1d-trans 3d-trans 6dof desk desk2 large-scale small-scale ORB-SLAM3 (SVIO) [15] 0.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "VECtor sequences",
        "TUM-VIE sequences"
      ],
      "dataset_descriptions": {
        "VECtor sequences": "Used to evaluate multi-sensor SLAM systems, focusing on event-centric data for accurate localization and mapping.",
        "TUM-VIE sequences": "Used to evaluate multi-sensor SLAM systems, providing diverse sequences for testing robustness in various environments."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions 'VECtor sequences' and 'TUM-VIE sequences', which are specific datasets used for evaluating multi-sensor SLAM systems. These datasets are relevant to event-based stereo depth estimation.",
      "processing_time": 67.8047981262207,
      "citing_paper_id": "259075396",
      "cited_paper_id": 250127779
    },
    {
      "context_text": "470 ESVO (Stereo Events) [29] × × × × 13.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "ESVO (Stereo Events)"
      ],
      "dataset_descriptions": {
        "ESVO (Stereo Events)": "Used to evaluate event-based stereo visual odometry, focusing on stereo depth estimation using event cameras. The dataset provides synchronized stereo events for testing and validation."
      },
      "confidence_score": 0.9,
      "reasoning": "The citation mentions 'ESVO (Stereo Events)', which appears to be a dataset or a specific resource used for event-based stereo visual odometry. The context suggests it is used for evaluation or experimentation.",
      "processing_time": 66.4423279762268,
      "citing_paper_id": "259075396",
      "cited_paper_id": 220870707
    },
    {
      "context_text": "Rebecq [21] B D EI × Monocular (PTAM) Zhou [29] B D TS × Stereo (PTAM) Kueng [30] A I × × Event-aided Tracking Rosinol [26] A I MEF × Mono + IMU (front-end) Hidalgo-Carrió [4] A D EGM × Monocular Odometry Proposed A I E3CT ✓ Stereo (PTAM) + DLc",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods and approaches. There are no clear identifiers for datasets.",
      "processing_time": 58.40610957145691,
      "citing_paper_id": "259075396",
      "cited_paper_id": 220870707
    },
    {
      "context_text": "Events-Frames hybridization .",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only a concept related to event-based stereo visual odometry.",
      "processing_time": 12.946009635925293,
      "citing_paper_id": "259075396",
      "cited_paper_id": 220870707
    },
    {
      "context_text": "This allows event cameras to operate at high speed, in very low-light conditions, and be more resistant to motion blur [5].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a characteristic of event cameras. No verifiable resources are identified.",
      "processing_time": 57.84390354156494,
      "citing_paper_id": "259075396",
      "cited_paper_id": 231682212
    },
    {
      "context_text": "Experiments on mocap-desk2 TUM-VIE dataset that show the capability of the proposed events-frames fusion method to maintain and track features in bight and dimmed scenes where grayscale-only frames may fail.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "TUM-VIE"
      ],
      "dataset_descriptions": {
        "TUM-VIE": "Used to evaluate the proposed events-frames fusion method, demonstrating its ability to maintain and track features in bright and dimmed scenes where grayscale-only frames may fail."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions the TUM-VIE dataset, which is a specific dataset used for evaluating event-based vision algorithms. The dataset is used to demonstrate the effectiveness of the proposed method in maintaining and tracking features under varying lighting conditions.",
      "processing_time": 66.29926419258118,
      "citing_paper_id": "259075396",
      "cited_paper_id": 244476938
    },
    {
      "context_text": "Red lines denote the temporal matching for keypoints of two consecutive keyframes (left).",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only a visual representation of temporal matching for keypoints. No verifiable resources are identified.",
      "processing_time": 58.677505016326904,
      "citing_paper_id": "259075396",
      "cited_paper_id": 244476938
    },
    {
      "context_text": "Section IV comprehensively evaluates the algorithm on the most recent VECtor [9] and TUM-VIE [10] benchmarks, along with defining the limitations.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "VECtor"
      ],
      "dataset_descriptions": {
        "VECtor": "Used to evaluate the algorithm on event-based stereo depth estimation, focusing on multi-sensor SLAM performance and limitations."
      },
      "confidence_score": 0.85,
      "reasoning": "The citation mentions VECtor and TUM-VIE as benchmarks, but only VECtor is a dataset according to the cited paper title. TUM-VIE is excluded as it is likely a benchmark or challenge.",
      "processing_time": 65.75388145446777,
      "citing_paper_id": "259075396",
      "cited_paper_id": 250127779
    },
    {
      "context_text": "This alignment term is observed to be constant for the same sensor rig with non-varying intrinsic and extrinsic parameters. δP value can be estimated using an offline optimization process only once on a selected number of frames (the more the accurate) with high confidence feature matches, and this value is given in Section IV for both VECtor and TUM-VIE sequences.",
      "catation_intent": [],
      "resource_type": [],
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.3,
      "reasoning": "JSON decode error: Extra data: line 12 column 1 (char 749), response: ```json\n{\n    \"reasoning\": \"The context mentions 'VECtor' and 'TUM-VIE sequences', which are specifi",
      "processing_time": 73.94811248779297,
      "citing_paper_id": "259075396",
      "cited_paper_id": 250127779
    },
    {
      "context_text": "comprehensively evaluates the algorithm on the most recent VECtor [8] and TUM-VIE [9] benchmarks, along with defining the limitations.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "VECtor",
        "TUM-VIE"
      ],
      "dataset_descriptions": {
        "VECtor": "Used to evaluate the algorithm's performance in multi-sensor SLAM, focusing on event-centric data and providing a versatile benchmark for testing.",
        "TUM-VIE": "Used to assess the algorithm's capabilities in event-based stereo depth estimation, offering a benchmark with real-world scenarios and challenging conditions."
      },
      "confidence_score": 1.0,
      "reasoning": "The citation mentions VECtor and TUM-VIE as benchmarks, which are specific datasets used for evaluating algorithms in event-based stereo depth estimation.",
      "processing_time": 67.60984969139099,
      "citing_paper_id": "259075396",
      "cited_paper_id": 250127779
    },
    {
      "context_text": "In subsection IV-A, we compare DH-PTAM with other RGB image-based and event-based/-aided methods on the HDR large-scale sequences of the publicly available dataset VECtor [9] due to its high-quality ground truth values and sensors calibration parameters.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "VECtor"
      ],
      "dataset_descriptions": {
        "VECtor": "Used to compare DH-PTAM with other RGB image-based and event-based/-aided methods, leveraging its high-quality ground truth values and sensor calibration parameters."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions the use of the VECtor dataset for comparing methods, which is relevant to the topic of event-based stereo depth estimation.",
      "processing_time": 64.8111674785614,
      "citing_paper_id": "259075396",
      "cited_paper_id": 250127779
    },
    {
      "context_text": "In subsection IV-A, we compare DHPTAM with other RGB image-based and event-based/-aided methods on the HDR large-scale sequences of the publicly available dataset VECtor [8] due to its high-quality ground truth values and sensors calibration parameters.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "VECtor"
      ],
      "dataset_descriptions": {
        "VECtor": "Used to compare DHPTAM with other RGB image-based and event-based/-aided methods, leveraging its high-quality ground truth values and sensor calibration parameters."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions the use of the VECtor dataset for comparing methods, which is relevant to the research topic of event-based stereo depth estimation.",
      "processing_time": 64.88864016532898,
      "citing_paper_id": "259075396",
      "cited_paper_id": 250127779
    },
    {
      "context_text": "Conversely, direct methods [7] attempt to process all available sensor data, such as individual pixel intensity changes in images (events) or all RGB frame pixels, without any intermediate filtering or feature extraction in the front-end, relying on the back-end to handle the entire data.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only describes a methodological approach.",
      "processing_time": 57.164791107177734,
      "citing_paper_id": "259075396",
      "cited_paper_id": 269614135
    },
    {
      "context_text": " I t w t t w t t w t t ( ) ( ) ( ) ( ) (5) exc",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.0,
      "reasoning": "The provided context does not contain any specific, verifiable dataset names or clear usage descriptions.",
      "processing_time": 58.319077491760254,
      "citing_paper_id": "205698386",
      "cited_paper_id": 541711
    },
    {
      "context_text": "Early attempts utilized the hand-crafted method to determine corresponding events [15,5,41,42,24,27,38,25,26,8].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods. The cited papers' titles suggest they are about event-based stereo vision, but do not explicitly name datasets.",
      "processing_time": 61.007829427719116,
      "citing_paper_id": "253513043",
      "cited_paper_id": 1082643
    },
    {
      "context_text": "Early attempts utilized the hand-crafted method to determine corresponding events [15,5,41,42,24,27,38,25,26,8].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods. The cited papers' titles suggest they are about event-based stereo vision, but do not explicitly name datasets.",
      "processing_time": 61.007829427719116,
      "citing_paper_id": "253513043",
      "cited_paper_id": 4412139
    },
    {
      "context_text": "Early attempts utilized the hand-crafted method to determine corresponding events [15,5,41,42,24,27,38,25,26,8].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods. The cited papers' titles suggest they are about event-based stereo vision, but do not explicitly name datasets.",
      "processing_time": 61.007829427719116,
      "citing_paper_id": "253513043",
      "cited_paper_id": 6079544
    },
    {
      "context_text": "Early attempts utilized the hand-crafted method to determine corresponding events [15,5,41,42,24,27,38,25,26,8].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods. The cited papers' titles suggest they are about event-based stereo vision, but do not explicitly name datasets.",
      "processing_time": 61.007829427719116,
      "citing_paper_id": "253513043",
      "cited_paper_id": 7151414
    },
    {
      "context_text": "Early attempts utilized the hand-crafted method to determine corresponding events [15,5,41,42,24,27,38,25,26,8].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods. The cited papers' titles suggest they are about event-based stereo vision, but do not explicitly name datasets.",
      "processing_time": 61.007829427719116,
      "citing_paper_id": "253513043",
      "cited_paper_id": 11177597
    },
    {
      "context_text": "Early attempts utilized the hand-crafted method to determine corresponding events [15,5,41,42,24,27,38,25,26,8].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods. The cited papers' titles suggest they are about event-based stereo vision, but do not explicitly name datasets.",
      "processing_time": 61.007829427719116,
      "citing_paper_id": "253513043",
      "cited_paper_id": 157060825
    },
    {
      "context_text": "Most of recent stereo matching algorithms are learning-based and perform well on various large-scale public benchmarks [6,32,34,13,12,7].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.3,
      "reasoning": "The citation span mentions 'large-scale public benchmarks' but does not specify any particular dataset names. The cited papers do not provide additional context to identify specific datasets.",
      "processing_time": 60.77979326248169,
      "citing_paper_id": "253513043",
      "cited_paper_id": 2658860
    },
    {
      "context_text": "Most of recent stereo matching algorithms are learning-based and perform well on various large-scale public benchmarks [6,32,34,13,12,7].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.3,
      "reasoning": "The citation span mentions 'large-scale public benchmarks' but does not specify any particular dataset names. The cited papers do not provide additional context to identify specific datasets.",
      "processing_time": 60.77979326248169,
      "citing_paper_id": "253513043",
      "cited_paper_id": 4252896
    },
    {
      "context_text": "Most of recent stereo matching algorithms are learning-based and perform well on various large-scale public benchmarks [6,32,34,13,12,7].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.3,
      "reasoning": "The citation span mentions 'large-scale public benchmarks' but does not specify any particular dataset names. The cited papers do not provide additional context to identify specific datasets.",
      "processing_time": 60.77979326248169,
      "citing_paper_id": "253513043",
      "cited_paper_id": 216036364
    },
    {
      "context_text": "Most of recent stereo matching algorithms are learning-based and perform well on various large-scale public benchmarks [6,32,34,13,12,7].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.3,
      "reasoning": "The citation span mentions 'large-scale public benchmarks' but does not specify any particular dataset names. The cited papers do not provide additional context to identify specific datasets.",
      "processing_time": 60.77979326248169,
      "citing_paper_id": "253513043",
      "cited_paper_id": 225072923
    },
    {
      "context_text": "1 presents a comparison of the proposed method with previous single modality methods (only-events [29,2] or only-images [12,6]) and events-images fusion methods [22].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only comparisons between different methods. No dataset names are present in the text.",
      "processing_time": 59.41390776634216,
      "citing_paper_id": "253513043",
      "cited_paper_id": 4252896
    },
    {
      "context_text": "1 presents a comparison of the proposed method with previous single modality methods (only-events [29,2] or only-images [12,6]) and events-images fusion methods [22].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only comparisons between different methods. No dataset names are present in the text.",
      "processing_time": 59.41390776634216,
      "citing_paper_id": "253513043",
      "cited_paper_id": 235306612
    },
    {
      "context_text": "1 presents a comparison of the proposed method with previous single modality methods (only-events [29,2] or only-images [12,6]) and events-images fusion methods [22].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only comparisons between different methods. No dataset names are present in the text.",
      "processing_time": 59.41390776634216,
      "citing_paper_id": "253513043",
      "cited_paper_id": 262638843
    },
    {
      "context_text": "Therefore, we train the frame-based model [12,6] using intensity images (APS) from the MVSEC dataset and select the models with the best performance in the validation until convergence.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "MVSEC"
      ],
      "dataset_descriptions": {
        "MVSEC": "Used to train and validate a frame-based model for stereo depth estimation, focusing on intensity images (APS) to achieve optimal performance."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions the MVSEC dataset, which is a specific dataset used for training and validating a frame-based model for stereo depth estimation.",
      "processing_time": 64.47859358787537,
      "citing_paper_id": "253513043",
      "cited_paper_id": 4252896
    },
    {
      "context_text": "The most successful methods of early studies using conventional RGB images have adopted end-to-end deep learning networks [32,37,18,35,34,12,6,33,30].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to methods and papers. No dataset names are present in the context.",
      "processing_time": 59.772725343704224,
      "citing_paper_id": "253513043",
      "cited_paper_id": 4252896
    },
    {
      "context_text": "The most successful methods of early studies using conventional RGB images have adopted end-to-end deep learning networks [32,37,18,35,34,12,6,33,30].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to methods and papers. No dataset names are present in the context.",
      "processing_time": 59.772725343704224,
      "citing_paper_id": "253513043",
      "cited_paper_id": 51891703
    },
    {
      "context_text": "The most successful methods of early studies using conventional RGB images have adopted end-to-end deep learning networks [32,37,18,35,34,12,6,33,30].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to methods and papers. No dataset names are present in the context.",
      "processing_time": 59.772725343704224,
      "citing_paper_id": "253513043",
      "cited_paper_id": 208512761
    },
    {
      "context_text": "The most successful methods of early studies using conventional RGB images have adopted end-to-end deep learning networks [32,37,18,35,34,12,6,33,30].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to methods and papers. No dataset names are present in the context.",
      "processing_time": 59.772725343704224,
      "citing_paper_id": "253513043",
      "cited_paper_id": 216036364
    },
    {
      "context_text": "They modified the stacked hourglass architecture proposed in [6].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a modification to a neural network architecture.",
      "processing_time": 58.64602279663086,
      "citing_paper_id": "253513043",
      "cited_paper_id": 4252896
    },
    {
      "context_text": "In addition to the disparity estimation loss, we use the learned perceptual similarity loss (LPIPS) [36] and the L 1 loss as image reconstruction losses to train the differentiable event selection network more robustly.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only loss functions and a network training approach.",
      "processing_time": 58.49387812614441,
      "citing_paper_id": "253513043",
      "cited_paper_id": 4766599
    },
    {
      "context_text": "As demonstrated in [36], the combination of these two losses encourage the sharp structural details.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method or finding related to loss functions.",
      "processing_time": 58.885658740997314,
      "citing_paper_id": "253513043",
      "cited_paper_id": 4766599
    },
    {
      "context_text": "We use the conv1-conv5 layers from [36].",
      "catation_intent": "none",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific dataset, only the use of conv1-conv5 layers from another paper.",
      "processing_time": 34.58318090438843,
      "citing_paper_id": "253513043",
      "cited_paper_id": 4766599
    },
    {
      "context_text": "We use the Adam [14] optimizer with β 1 = 0 .",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions the Adam optimizer, which is a method, not a dataset. No datasets are mentioned in the citation context.",
      "processing_time": 15.243675947189331,
      "citing_paper_id": "253513043",
      "cited_paper_id": 6628106
    },
    {
      "context_text": "They outperform traditional methods by a large margin on public benchmarks ( e . g ., Scene Flow [20] and KITTI [21]).",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Scene Flow",
        "KITTI"
      ],
      "dataset_descriptions": {
        "Scene Flow": "Used to evaluate disparity, optical flow, and scene flow estimation methods, providing synthetic data with ground truth annotations for training and testing.",
        "KITTI": "Used to evaluate stereo depth estimation methods, providing real-world driving scenarios with ground truth depth maps and stereo image pairs."
      },
      "confidence_score": 1.0,
      "reasoning": "The citation mentions 'Scene Flow' and 'KITTI' as public benchmarks where the methods outperform traditional approaches. These are known datasets in the field of computer vision and robotics, particularly relevant for stereo depth estimation.",
      "processing_time": 28.123833179473877,
      "citing_paper_id": "253513043",
      "cited_paper_id": 12986049
    },
    {
      "context_text": "They outperform traditional methods by a large margin on public benchmarks ( e . g ., Scene Flow [20] and KITTI [21]).",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Scene Flow",
        "KITTI"
      ],
      "dataset_descriptions": {
        "Scene Flow": "Used to evaluate disparity, optical flow, and scene flow estimation methods, providing synthetic data with ground truth annotations for training and testing.",
        "KITTI": "Used to evaluate stereo depth estimation methods, providing real-world driving scenarios with ground truth depth maps and stereo image pairs."
      },
      "confidence_score": 1.0,
      "reasoning": "The citation mentions 'Scene Flow' and 'KITTI' as public benchmarks where the methods outperform traditional approaches. These are known datasets in the field of computer vision and robotics, particularly relevant for stereo depth estimation.",
      "processing_time": 28.123833179473877,
      "citing_paper_id": "253513043",
      "cited_paper_id": 206594275
    },
    {
      "context_text": "However, they followed the existing event representation [31,9] used in the image reconstruction, and embedding for event-image disparity network has been less studied.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only references to existing event representation methods. No clear, verifiable datasets are identified.",
      "processing_time": 13.918589353561401,
      "citing_paper_id": "253513043",
      "cited_paper_id": 53725328
    },
    {
      "context_text": "Number-based stacking methods [22,31,9] accumulate a certain number of events predefined by the user and put them into the network in the form of images.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods for processing event-based data.",
      "processing_time": 13.106959342956543,
      "citing_paper_id": "253513043",
      "cited_paper_id": 53725328
    },
    {
      "context_text": "For LPIPS, we use variants of AlexNet [16].",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific dataset, only a method (AlexNet).",
      "processing_time": 11.770780324935913,
      "citing_paper_id": "253513043",
      "cited_paper_id": 195908774
    },
    {
      "context_text": "Estimating depth from a stereo image pair has been an important problem in the field of computer vision [17,28].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general problem in computer vision.",
      "processing_time": 12.466757774353027,
      "citing_paper_id": "253513043",
      "cited_paper_id": 219303641
    },
    {
      "context_text": "We also do not evaluate the split 2 quantitatively due to the difference in dynamic characteristics in the training and testing events, as mentioned in [29,2].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific dataset names. It only refers to 'split 2' which is too generic and lacks a clear identifier.",
      "processing_time": 16.35759711265564,
      "citing_paper_id": "253513043",
      "cited_paper_id": 235306612
    },
    {
      "context_text": "We also do not evaluate the split 2 quantitatively due to the difference in dynamic characteristics in the training and testing events, as mentioned in [29,2].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific dataset names. It only refers to 'split 2' which is too generic and lacks a clear identifier.",
      "processing_time": 16.35759711265564,
      "citing_paper_id": "253513043",
      "cited_paper_id": 262638843
    },
    {
      "context_text": "A new embedding of a 4D queue including both temporal and spatial information of event data for deep learning was proposed in [29], and the later study [2] improved the accuracy using various techniques.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only methods and improvements in accuracy. No dataset names are provided.",
      "processing_time": 24.6616530418396,
      "citing_paper_id": "253513043",
      "cited_paper_id": 235306612
    },
    {
      "context_text": "A new embedding of a 4D queue including both temporal and spatial information of event data for deep learning was proposed in [29], and the later study [2] improved the accuracy using various techniques.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only methods and improvements in accuracy. No dataset names are provided.",
      "processing_time": 24.6616530418396,
      "citing_paper_id": "253513043",
      "cited_paper_id": 262638843
    },
    {
      "context_text": "For a fair comparison, we use the mean depth error , mean disparity error and one-pixel-accuracy used in [22,29,2] as the metrics.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only evaluation metrics. The cited papers' titles suggest they are relevant to event-based stereo depth estimation but do not provide specific dataset names.",
      "processing_time": 16.891900300979614,
      "citing_paper_id": "253513043",
      "cited_paper_id": 235306612
    },
    {
      "context_text": "For a fair comparison, we use the mean depth error , mean disparity error and one-pixel-accuracy used in [22,29,2] as the metrics.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only evaluation metrics. The cited papers' titles suggest they are relevant to event-based stereo depth estimation but do not provide specific dataset names.",
      "processing_time": 16.891900300979614,
      "citing_paper_id": "253513043",
      "cited_paper_id": 262638843
    },
    {
      "context_text": "Following [22,29,2], we also use the Indoor Flying dataset from MVSEC, which is captured from a drone flying in a room with various objects, and partition them into three splits.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Indoor Flying dataset from MVSEC"
      ],
      "dataset_descriptions": {
        "Indoor Flying dataset from MVSEC": "Used for event-based stereo depth estimation, capturing drone flights in a room with various objects, partitioned into three splits for training and evaluation."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions the 'Indoor Flying dataset from MVSEC', which is a specific dataset used for event-based stereo depth estimation. The dataset is described as being captured from a drone flying in a room with various objects and is partitioned into three splits.",
      "processing_time": 36.38577198982239,
      "citing_paper_id": "253513043",
      "cited_paper_id": 235306612
    },
    {
      "context_text": "Following [22,29,2], we also use the Indoor Flying dataset from MVSEC, which is captured from a drone flying in a room with various objects, and partition them into three splits.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Indoor Flying dataset from MVSEC"
      ],
      "dataset_descriptions": {
        "Indoor Flying dataset from MVSEC": "Used for event-based stereo depth estimation, capturing drone flights in a room with various objects, partitioned into three splits for training and evaluation."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions the 'Indoor Flying dataset from MVSEC', which is a specific dataset used for event-based stereo depth estimation. The dataset is described as being captured from a drone flying in a room with various objects and is partitioned into three splits.",
      "processing_time": 36.38577198982239,
      "citing_paper_id": "253513043",
      "cited_paper_id": 262638843
    },
    {
      "context_text": "Recently some stereo methods accumulate all events between two frames [29,2] or specify the number of recent events to stack [22].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only methods and approaches. No dataset names are present in the citation span.",
      "processing_time": 13.058548927307129,
      "citing_paper_id": "253513043",
      "cited_paper_id": 235306612
    },
    {
      "context_text": "Recently some stereo methods accumulate all events between two frames [29,2] or specify the number of recent events to stack [22].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only methods and approaches. No dataset names are present in the citation span.",
      "processing_time": 13.058548927307129,
      "citing_paper_id": "253513043",
      "cited_paper_id": 262638843
    },
    {
      "context_text": "The time-based stacking method [29,40,2] generally discretizes the event between two frames of an image along the time axis.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only a method for handling events in stereo depth estimation.",
      "processing_time": 11.76726770401001,
      "citing_paper_id": "253513043",
      "cited_paper_id": 235306612
    },
    {
      "context_text": "The time-based stacking method [29,40,2] generally discretizes the event between two frames of an image along the time axis.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only a method for handling events in stereo depth estimation.",
      "processing_time": 11.76726770401001,
      "citing_paper_id": "253513043",
      "cited_paper_id": 262638843
    },
    {
      "context_text": "Analytically, our method shows better performance on the overall sequence average than both the existing DSEC event-only baseline [29] and the state-of-the-arts [22] that use image and event fusion.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets by name. It only refers to methods and baselines.",
      "processing_time": 14.663108587265015,
      "citing_paper_id": "253513043",
      "cited_paper_id": 262638843
    },
    {
      "context_text": "For comparison, we borrow the results of event-based approach [29] and event-image fusion method [22] from the original papers, respectively.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only references to methods and approaches from other papers.",
      "processing_time": 23.538688898086548,
      "citing_paper_id": "253513043",
      "cited_paper_id": 262638843
    },
    {
      "context_text": "For comparison, we select frames similar to the ones used in [29] and [22].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a comparison of frames used in other studies. No clear, verifiable dataset names are present.",
      "processing_time": 13.892548561096191,
      "citing_paper_id": "253513043",
      "cited_paper_id": 262638843
    },
    {
      "context_text": "From [1], for noise Z with d µ ( z ) ∝ exp( − ν ( z ))d z and any twice differentiable ν , the Jacobian matrix of y ε at M can be obtained as follows: Being able to compute the perturbed maximizer and its Jacobian allows optimizing functions that depend on M through y ε .",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only mathematical concepts and methods.",
      "processing_time": 12.220136404037476,
      "citing_paper_id": "253513043",
      "cited_paper_id": null
    },
    {
      "context_text": "We extend existing methods for event-based processing by using the cooperative approach [1], which enables spatiotemporal and asynchronous stereo matching.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for cooperative computation of stereo disparity.",
      "processing_time": 12.441866874694824,
      "citing_paper_id": "121601380",
      "cited_paper_id": 767650
    },
    {
      "context_text": "Derived from the cooperative stereo method [1], the possible feature locations in the disparity space are modelled as nodes in the network.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for stereo disparity computation.",
      "processing_time": 23.183389902114868,
      "citing_paper_id": "121601380",
      "cited_paper_id": 767650
    },
    {
      "context_text": "They developed a stereo vision chip based on the cooperative algorithm by Marr and Poggio [1] and obtained promising results for one-dimensional image matching.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method and results.",
      "processing_time": 11.132022619247437,
      "citing_paper_id": "121601380",
      "cited_paper_id": 767650
    },
    {
      "context_text": "In this work, we revise the possibility of modelling event-driven stereo matching by using a bio-inspired approach: the cooperative network [1].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method or approach. The context focuses on a bio-inspired approach to event-driven stereo matching.",
      "processing_time": 14.163309335708618,
      "citing_paper_id": "121601380",
      "cited_paper_id": 767650
    },
    {
      "context_text": "The cooperative approach [1] applies stereo fusion to the primitive descriptions (features, edges) of the left and right view.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for stereo disparity computation.",
      "processing_time": 12.469988107681274,
      "citing_paper_id": "121601380",
      "cited_paper_id": 767650
    },
    {
      "context_text": "An example of a bio-inspired vision device is the dynamic vision sensor (DVS) [2], which mimics some aspects of retina’s functionality.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a bio-inspired vision device called DVS. No dataset names are present in the citation context.",
      "processing_time": 14.615084648132324,
      "citing_paper_id": "121601380",
      "cited_paper_id": 2070927
    },
    {
      "context_text": "The algorithm was implemented in Matlab and positively veriﬁed within proof of concept [14] on both synthetic and event datasets.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.3,
      "reasoning": "The context mentions 'synthetic and event datasets' but does not provide specific names. The term 'event datasets' is too generic without a specific identifier.",
      "processing_time": 24.94397258758545,
      "citing_paper_id": "121601380",
      "cited_paper_id": 6079544
    },
    {
      "context_text": "We have shown the proof of concept within preliminary results of the algorithm in [14].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a proof of concept for an algorithm.",
      "processing_time": 11.269890785217285,
      "citing_paper_id": "121601380",
      "cited_paper_id": 6079544
    },
    {
      "context_text": "Furthermore, the 3D reconstruction method has been proposed [10] based on geometrical and time constraints for matching events from more than two sensors.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for 3D reconstruction using event-based sensors.",
      "processing_time": 13.008537530899048,
      "citing_paper_id": "121601380",
      "cited_paper_id": 10712214
    },
    {
      "context_text": "The applicability of this model to the characteristics of DVS has already been proven by Mahowald and Delbruck [6], and we extend the algorithm to two-dimensional image matching, as proposed in [9].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only references to previous work and extensions of algorithms.",
      "processing_time": 10.993514776229858,
      "citing_paper_id": "121601380",
      "cited_paper_id": 267878248
    },
    {
      "context_text": "[42] ++ + + + +++ [43] + ++ +++ + +++ [44] ++ +++ [51] + ++ + ++ [35] + + +++ ++ [49] + +++ [48] 1 + + +++ [47] 2 ++ [40] + + ++ + + [53] + ++ + +++ + [45] + ++ + +++ [55] + + ++ ++ [41] + ++ ++ ++ + [52] +++ +++ + +++ [28] ++ + ++ +++ + [57] +++ +++ + [50] ++ ++ + ++",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.0,
      "reasoning": "The citation span does not contain any specific dataset names or verifiable resources. The context is too generic and lacks specific identifiers.",
      "processing_time": 12.399461269378662,
      "citing_paper_id": "246656358",
      "cited_paper_id": 185541
    },
    {
      "context_text": "Neuromorphic computing consists of a variety of brain-inspired computers, devices, and models that contrast the pervasive von Neumann computer architecture [3].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general overview of neuromorphic computing. No verifiable resources are identified.",
      "processing_time": 12.416517972946167,
      "citing_paper_id": "246656358",
      "cited_paper_id": 605892
    },
    {
      "context_text": "Moreover, neuromorphic computing has reached some significant milestones, which have been well summarized in [3].",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a summary of milestones in neuromorphic computing.",
      "processing_time": 12.757189512252808,
      "citing_paper_id": "246656358",
      "cited_paper_id": 605892
    },
    {
      "context_text": "An intermediate complexity choice is the Izhikevich model [31].",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific dataset, only a model. The context is about the Izhikevich model, which is a method, not a dataset.",
      "processing_time": 26.666454076766968,
      "citing_paper_id": "246656358",
      "cited_paper_id": 814743
    },
    {
      "context_text": "It eliminates the dependency on absolute lighting level and instead the receptors respond to changes in the incident light (also known as temporal contrast) [13,34].",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a biological concept related to neural computations in the retina.",
      "processing_time": 12.411320924758911,
      "citing_paper_id": "246656358",
      "cited_paper_id": 1226657
    },
    {
      "context_text": "The latency between triggered events has dropped to less than 0.5 µ s [5,6].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only technical details about event-based sensors.",
      "processing_time": 23.91419219970703,
      "citing_paper_id": "246656358",
      "cited_paper_id": 2792722
    },
    {
      "context_text": "The latency between triggered events has dropped to less than 0.5 µ s [5,6].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only technical details about event-based sensors.",
      "processing_time": 23.91419219970703,
      "citing_paper_id": "246656358",
      "cited_paper_id": 197634653
    },
    {
      "context_text": "The depth prediction is done with a fully convolutional neural network, based on the UNet architecture [59].",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions a method (UNet architecture) rather than a dataset. No specific dataset is referenced in the citation context.",
      "processing_time": 13.403092622756958,
      "citing_paper_id": "246656358",
      "cited_paper_id": 3719281
    },
    {
      "context_text": "Later, time surfaces (hierarchy of time surfaces (HOTS) and histograms of averaged time surfaces (HATS)) were presented, which used the spatiotemporal resolution provided by the event-based cameras to generate 3D maps of event data [26,27].",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions 'event data' but does not specify a dataset name. The cited paper title suggests a method (HATS) rather than a dataset.",
      "processing_time": 14.617148876190186,
      "citing_paper_id": "246656358",
      "cited_paper_id": 3993392
    },
    {
      "context_text": "The authors of [52] showed that motion blur can be solved by having a range of disparities to synchronize the position of events.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for solving motion blur using event-based stereo. No verifiable resources are identified.",
      "processing_time": 13.399805307388306,
      "citing_paper_id": "246656358",
      "cited_paper_id": 4412139
    },
    {
      "context_text": "Going further, a spatiotemporal adaptive cooperative approach was used to calculate disparities for each incoming event in [44,45].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only methods and approaches. The cited papers' titles do not provide additional dataset information.",
      "processing_time": 23.88007950782776,
      "citing_paper_id": "246656358",
      "cited_paper_id": 6079544
    },
    {
      "context_text": "Going further, a spatiotemporal adaptive cooperative approach was used to calculate disparities for each incoming event in [44,45].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only methods and approaches. The cited papers' titles do not provide additional dataset information.",
      "processing_time": 23.88007950782776,
      "citing_paper_id": "246656358",
      "cited_paper_id": 7151414
    },
    {
      "context_text": "[42] [63] Embedded Blackfin BF537 DSP Stereo 3.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only hardware components. There are no multi-word proper nouns, acronyms, or hyphenated names with digits that could indicate a dataset.",
      "processing_time": 16.487664222717285,
      "citing_paper_id": "246656358",
      "cited_paper_id": 15357188
    },
    {
      "context_text": "An event-based camera combined with TrueNorth has successfully demonstrated the ability of a fully event-based stereo system [41].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific dataset. It refers to a hardware demonstration using TrueNorth, which is a neurosynaptic chip, not a dataset.",
      "processing_time": 15.450581550598145,
      "citing_paper_id": "246656358",
      "cited_paper_id": 15373627
    },
    {
      "context_text": "In [41], the program that is running on TrueNorth was written in the Corelet programming language that was speciﬁcally created by IBM for this hardware platform.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a programming language and hardware platform.",
      "processing_time": 32.54580855369568,
      "citing_paper_id": "246656358",
      "cited_paper_id": 15373627
    },
    {
      "context_text": "At the time of publication for this paper, a few neuromorphic processors have already been manufactured and well-utilized: TrueNorth [36], Loihi [37], SpiNNaker [38], and some others still in development [39].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions neuromorphic processors but does not refer to any specific datasets. The names mentioned are models or hardware, not datasets.",
      "processing_time": 14.39002776145935,
      "citing_paper_id": "246656358",
      "cited_paper_id": 15373627
    },
    {
      "context_text": "At the time of publication for this paper, a few neuromorphic processors have already been manufactured and well-utilized: TrueNorth [36], Loihi [37], SpiNNaker [38], and some others still in development [39].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions neuromorphic processors but does not refer to any specific datasets. The names mentioned are models or hardware, not datasets.",
      "processing_time": 14.39002776145935,
      "citing_paper_id": "246656358",
      "cited_paper_id": 25268038
    },
    {
      "context_text": "This neuromorphic chip consists of systems of equations that deﬁne the behaviour of TrueNorth’s neurons.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a neuromorphic chip called TrueNorth. The title confirms that TrueNorth is a hardware design, not a dataset.",
      "processing_time": 14.573619842529297,
      "citing_paper_id": "246656358",
      "cited_paper_id": 15373627
    },
    {
      "context_text": "The second method utilizing a TrueNorth processor realized a fully event-based system that achieved high computational efﬁciency and low power consumption.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method involving a TrueNorth processor. No verifiable datasets are referenced.",
      "processing_time": 24.573650360107422,
      "citing_paper_id": "246656358",
      "cited_paper_id": 15373627
    },
    {
      "context_text": "It used an event-based camera, and its output data was connected to nine TrueNorth chips.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific dataset, only hardware components. The citation is about using an event-based camera and TrueNorth chips, which are not datasets.",
      "processing_time": 14.597692966461182,
      "citing_paper_id": "246656358",
      "cited_paper_id": 15373627
    },
    {
      "context_text": "Table 3 presents seventeen methods that use standard CPUs, GPUs, or embedded platforms, only two methods that use FPGAs and one method that utilizes a TrueNorth processor.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only hardware platforms used in various methods.",
      "processing_time": 11.379766464233398,
      "citing_paper_id": "246656358",
      "cited_paper_id": 15373627
    },
    {
      "context_text": "Some solutions exist that can remove or mitigate motion blur or substitute it with motion flow using deep neural networks [2].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for removing motion blur using deep neural networks.",
      "processing_time": 12.136192083358765,
      "citing_paper_id": "246656358",
      "cited_paper_id": 18123440
    },
    {
      "context_text": "This is one of the ﬁrst models that describes the behaviour of biological neurons [30].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific dataset, only a model describing biological neurons. No dataset names are present in the citation span.",
      "processing_time": 26.019163846969604,
      "citing_paper_id": "246656358",
      "cited_paper_id": 20873334
    },
    {
      "context_text": "The non-linear dynamic range is fairly the same in all the presented research papers with a maximum value up to 143 dB [7].",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a technical specification of a sensor.",
      "processing_time": 10.7391939163208,
      "citing_paper_id": "246656358",
      "cited_paper_id": 21317717
    },
    {
      "context_text": "In another work, a belief propagation neural network, which is deﬁned in [54], was used and a mean depth error of 61.14–92%, with a maximum depth of 5 m was reported [55].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific dataset names, only a method and performance metrics.",
      "processing_time": 11.68762755393982,
      "citing_paper_id": "246656358",
      "cited_paper_id": 22158024
    },
    {
      "context_text": "Monocular depth estimation meth-ods [55,57] have shown good performance in outdoor scenes up to thirty meters.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods for monocular depth estimation. No dataset names are present in the citation span.",
      "processing_time": 14.864825963973999,
      "citing_paper_id": "246656358",
      "cited_paper_id": 22158024
    },
    {
      "context_text": "The study in [47] was the ﬁrst (to the best of authors’ knowledge) to provide a 3D reconstruction of a scene based on 6-DOF camera tracking.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method or approach for 3D reconstruction and 6-DOF tracking using an event camera.",
      "processing_time": 14.35164761543274,
      "citing_paper_id": "246656358",
      "cited_paper_id": 26324573
    },
    {
      "context_text": "Nevertheless, [47] trail-blazed monocular event-based depth perception and 3D reconstruction for the next generation of efforts in this area.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method or approach. The context focuses on the pioneering work in monocular event-based depth perception and 3D reconstruction.",
      "processing_time": 14.860221862792969,
      "citing_paper_id": "246656358",
      "cited_paper_id": 26324573
    },
    {
      "context_text": "One of the ﬁrst to solve the correspondence problem with event-based vision sensors was [42].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific dataset, only a method or system for solving the correspondence problem with event-based vision sensors.",
      "processing_time": 13.81204080581665,
      "citing_paper_id": "246656358",
      "cited_paper_id": 30913835
    },
    {
      "context_text": "The researchers in [42] used an embedded device with a DSP, which utilized a simple algo-rithm for stereo processing— camera calibration and rectiﬁcation, stereo correspondence calculation and reconstruction (disparity map).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific, verifiable datasets. It describes an embedded system and its components but does not reference any dataset used for evaluation or training.",
      "processing_time": 15.12900185585022,
      "citing_paper_id": "246656358",
      "cited_paper_id": 30913835
    },
    {
      "context_text": "The correspondence problem with a neuromorphic computing platform, SpiNNaker, was ﬁrst solved by [40].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method or system (SpiNNaker) used to solve the correspondence problem.",
      "processing_time": 13.357478380203247,
      "citing_paper_id": "246656358",
      "cited_paper_id": 34855834
    },
    {
      "context_text": "However, this noise can be filtered out using background filtering techniques [32].",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a technique for filtering noise in event-based systems.",
      "processing_time": 11.202821493148804,
      "citing_paper_id": "246656358",
      "cited_paper_id": 203162947
    },
    {
      "context_text": "The results from the MVSEC dataset are compared with frame-based state-of-the-art methods [61,62], and with another monocular event-based method [58].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "MVSEC"
      ],
      "dataset_descriptions": {
        "MVSEC": "Used to compare results of event-based stereo depth estimation with frame-based and monocular event-based methods, focusing on performance metrics and consistency."
      },
      "confidence_score": 1.0,
      "reasoning": "The citation mentions the MVSEC dataset, which is a specific, verifiable dataset used for comparing results with other methods.",
      "processing_time": 19.18094778060913,
      "citing_paper_id": "246656358",
      "cited_paper_id": 206596513
    },
    {
      "context_text": "The main differences between SNNs and ANNs are well summarized in [29]:",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a comparison between SNNs and ANNs.",
      "processing_time": 12.353701114654541,
      "citing_paper_id": "246656358",
      "cited_paper_id": 208098355
    },
    {
      "context_text": "Hence, it can be extensively used in safety-critical applications like those related to the automotive industry [33].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general application area. No verifiable resources are identified.",
      "processing_time": 13.334480047225952,
      "citing_paper_id": "246656358",
      "cited_paper_id": 220314130
    },
    {
      "context_text": "5 [24] 320 × 262 1000 >100 13 × 13 70 [8] 132 × 1024 10 × 10 0.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not contain any specific, verifiable dataset names or other resources. It appears to be a series of numbers and dimensions, likely referring to experimental parameters or configurations.",
      "processing_time": 16.108357191085815,
      "citing_paper_id": "246656358",
      "cited_paper_id": null
    },
    {
      "context_text": "Shi and Tomasi (Shi & Tomasi, 1994) give more details about features in their work and describe which features are good for tracking.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only features for tracking. The context is about describing features rather than using a dataset.",
      "processing_time": 13.3449068069458,
      "citing_paper_id": "44623261",
      "cited_paper_id": 778478
    },
    {
      "context_text": "7 shows a frame built for a feature-based image processing algorithm.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a frame for an image processing algorithm.",
      "processing_time": 11.88614296913147,
      "citing_paper_id": "44623261",
      "cited_paper_id": 1150626
    },
    {
      "context_text": "One year later, Mahowald and Mead (Mahowald & Mead, 1989) implemented the ﬁrst retina sensor based on silicon and established the name Silicon Retina .",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only the development of a silicon retina sensor. No verifiable resources are identified.",
      "processing_time": 12.94746994972229,
      "citing_paper_id": "44623261",
      "cited_paper_id": 1776032
    },
    {
      "context_text": "One year later, Mahowald and Mead (Mahowald & Mead, 1989) implemented the ﬁrst retina sensor based on silicon and established the name Silicon Retina .",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only the development of a silicon retina sensor. No verifiable resources are identified.",
      "processing_time": 12.94746994972229,
      "citing_paper_id": "44623261",
      "cited_paper_id": 8280907
    },
    {
      "context_text": "The silicon retina uses the so-called Address-Event-Representation (AER) as output format which was proposed by Sivilotti (Sivilotti, 1991) and Mahowald (Mahowald, 1992) in order to model the transmission of neural information within biological systems.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only the Address-Event-Representation (AER) format and its origins. No verifiable datasets are referenced.",
      "processing_time": 15.10929799079895,
      "citing_paper_id": "44623261",
      "cited_paper_id": 1776032
    },
    {
      "context_text": "The silicon retina uses the so-called Address-Event-Representation (AER) as output format which was proposed by Sivilotti (Sivilotti, 1991) and Mahowald (Mahowald, 1992) in order to model the transmission of neural information within biological systems.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only the Address-Event-Representation (AER) format and its origins. No verifiable datasets are referenced.",
      "processing_time": 15.10929799079895,
      "citing_paper_id": "44623261",
      "cited_paper_id": 8280907
    },
    {
      "context_text": "In 1988 Mead and Mahowald (Mead & Mahowald, 1988) developed a silicon model of the early steps in human visual processing.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a silicon model of early visual processing. The context is about the development of a model, not the use of a dataset.",
      "processing_time": 15.896048784255981,
      "citing_paper_id": "44623261",
      "cited_paper_id": 1776032
    },
    {
      "context_text": "In 1988 Mead and Mahowald (Mead & Mahowald, 1988) developed a silicon model of the early steps in human visual processing.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a silicon model of early visual processing. The context is about the development of a model, not the use of a dataset.",
      "processing_time": 15.896048784255981,
      "citing_paper_id": "44623261",
      "cited_paper_id": 8280907
    },
    {
      "context_text": "2 Address-event data representation The silicon retina uses the so-called Address-Event-Representation (AER) as output format which was proposed by Sivilotti (Sivilotti, 1991) and Mahowald (Mahowald, 1992) in order to model the transmission of neural information within biological systems.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a data representation method (AER).",
      "processing_time": 24.357237577438354,
      "citing_paper_id": "44623261",
      "cited_paper_id": 1776032
    },
    {
      "context_text": "Fawcett (Fawcett, 2004), (Fawcett, 2006) and Provost and Fawcett (Provost & Fawcett, 2001) give an introduction and practical considerations for ROC analysis in their work.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only references to works discussing ROC analysis.",
      "processing_time": 11.334458351135254,
      "citing_paper_id": "44623261",
      "cited_paper_id": 5415722
    },
    {
      "context_text": "The optical transient sensor (H¨aﬂinger & Bergh, 2002), (Lichtsteiner et al., 2004) used for the stereo matching algorithms described in this work, is a sensor developed at the AIT 1 and ETH 2 and is described in the work of Lichtsteiner et al. (Lichtsteiner et al., 2006).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only sensors and algorithms. No verifiable datasets are referenced.",
      "processing_time": 12.075777292251587,
      "citing_paper_id": "44623261",
      "cited_paper_id": 15357188
    },
    {
      "context_text": "ASICs or even more FPGAs, can be used to put such customized architectures into practice and thus to exploit the immanent parallelism of stereo-vision algorithms (Porter & Bergmann, 1997).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only hardware and implementation frameworks for stereo vision algorithms.",
      "processing_time": 11.872338771820068,
      "citing_paper_id": "44623261",
      "cited_paper_id": 62185095
    },
    {
      "context_text": "Some algorithms using block-based techniques are shown in ( (Banks et al., 1997), (Banks et al.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only algorithms and techniques. There are no clear identifiers for datasets in the provided context.",
      "processing_time": 14.515117406845093,
      "citing_paper_id": "44623261",
      "cited_paper_id": 62592856
    },
    {
      "context_text": "Some algorithms using block-based techniques are shown in ( (Banks et al., 1997), (Banks et al., 1999), (Zabih & Woodﬁll, 1994)).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only references to algorithms and techniques. No verifiable resources are identified.",
      "processing_time": 12.931692361831665,
      "citing_paper_id": "44623261",
      "cited_paper_id": 62592856
    },
    {
      "context_text": "The research of this sensor type goes back to Fukushima et al. (Fukushima et al., 1970) who made a ﬁrst implementation of an artiﬁcial retina in 1970.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a historical reference to an artificial retina implementation.",
      "processing_time": 12.687671899795532,
      "citing_paper_id": "44623261",
      "cited_paper_id": 62613141
    },
    {
      "context_text": "(Fukushima et al., 1970) who made a first implementation of an artificial retina in 1970.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific dataset, only an early implementation of an artificial retina. No verifiable resource is identified.",
      "processing_time": 12.684877634048462,
      "citing_paper_id": "44623261",
      "cited_paper_id": 62613141
    },
    {
      "context_text": "The images are treated with some morphological operations (Gonzales & Woods, 2002) for the enhancement of features, which are required by the next step of the center matching algorithm.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for image enhancement.",
      "processing_time": 11.174329280853271,
      "citing_paper_id": "44623261",
      "cited_paper_id": null
    },
    {
      "context_text": "MVSEC [13] is considered as the ﬁrst modern cross-modal dataset given its rich sensor setup of a pair of DAVIS m346B sensors (346 × 260, events and APS frames, six-axis built-in IMU, about 10cm baseline), a VI-Sensor that includes a stereo camera (about 10cm baseline) and an in-built nine-axis IMU,…",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "MVSEC"
      ],
      "dataset_descriptions": {
        "MVSEC": "Used to evaluate event-based stereo depth estimation methods, leveraging its rich sensor setup including event cameras and stereo cameras with IMUs."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions MVSEC as a dataset with a rich sensor setup, including event cameras and stereo cameras, which is directly relevant to event-based stereo depth estimation.",
      "processing_time": 19.698161363601685,
      "citing_paper_id": "250127779",
      "cited_paper_id": 3416874
    },
    {
      "context_text": "3) Validation on state-of-the-art algorithms: We validated our data sequences by applying ORB-SLAM3 [27] with regular stereo setting, ORB-SLAM3 with RGB-D setting, VINS-mono [28] with monocular-inertial setting, VINS-fusion with stereo-inertial setting, and LIO-SAM [29] with LiDAR-inertial setting.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only algorithms and methods used for validation. No verifiable datasets are referenced.",
      "processing_time": 14.318220138549805,
      "citing_paper_id": "250127779",
      "cited_paper_id": 7334757
    },
    {
      "context_text": "Popular examples are given by the TUM-RGBD [5], KITTI [6], and EuRoC [7] datasets.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "TUM-RGBD",
        "KITTI",
        "EuRoC"
      ],
      "dataset_descriptions": {
        "TUM-RGBD": "Used for evaluating event-based stereo depth estimation algorithms, providing synchronized RGB-D data for indoor scenes.",
        "KITTI": "Used for benchmarking event-based stereo depth estimation, offering real-world driving scenarios with stereo camera data.",
        "EuRoC": "Used for testing event-based stereo depth estimation in challenging environments, featuring indoor and outdoor flight sequences."
      },
      "confidence_score": 1.0,
      "reasoning": "The citation mentions three datasets by name, all of which are commonly used in robotics and computer vision research, particularly for stereo depth estimation.",
      "processing_time": 28.312198877334595,
      "citing_paper_id": "250127779",
      "cited_paper_id": 9455111
    },
    {
      "context_text": "D-eDVS [11] 128×128 6 6 4 6 6 Hand-held MoCap 6 evbench [12] 240×180 6 6 4 6 6 Mobile Robot Odometer 6 MVSEC [13] 346×260 4 4 6 16 6/9 Diverse MoCap + Cartographer 4 UZH-FPV [3] 346×260 6 4 6 6 6 Drone Total Station w/ spline 4 ViViD [14] 240×180 6 6 4 16 9 Hand-held MoCap + LOAM 4 ViViD++ [15] 640×480 6 6 6 64 9 Driving LOAM w/ GPS 4 DSEC [16] 640×480 4 4 6 16 6 Driving RTK GPS 4 AGRI-EBV [17] 240×180 6 4 4 16 6 Mobile Robot LiDAR SLAM 4 TUM-VIE [18] 1280×720 4 4 6 6 6 Diverse MoCap (partial) 4",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "D-eDVS",
        "evbench",
        "MVSEC",
        "UZH-FPV",
        "ViViD",
        "ViViD++",
        "DSEC",
        "AGRI-EBV",
        "TUM-VIE"
      ],
      "dataset_descriptions": {
        "D-eDVS": "Used for hand-held motion capture with a 128x128 resolution sensor, focusing on event-based stereo depth estimation.",
        "evbench": "Used for mobile robot odometry with a 240x180 resolution sensor, evaluating event-based stereo depth estimation in robotic navigation.",
        "MVSEC": "Used for diverse motion capture and cartographer applications with a 346x260 resolution sensor, assessing event-based stereo depth estimation in various environments.",
        "UZH-FPV": "Used for drone total station with spline tracking using a 346x260 resolution sensor, testing event-based stereo depth estimation in aerial applications.",
        "ViViD": "Used for hand-held motion capture with LOAM using a 240x180 resolution sensor, evaluating event-based stereo depth estimation in handheld scenarios.",
        "ViViD++": "Used for driving with LOAM and GPS using a 640x480 resolution sensor, assessing event-based stereo depth estimation in driving scenarios.",
        "DSEC": "Used for driving with RTK GPS using a 640x480 resolution sensor, testing event-based stereo depth estimation in high-precision driving applications.",
        "AGRI-EBV": "Used for mobile robot LiDAR SLAM with a 240x180 resolution, focusing on event-based stereo depth estimation in agricultural robotics.",
        "TUM-VIE": "Used for diverse motion capture (partial) with a 1280x720 resolution sensor, evaluating event-based stereo depth estimation in a variety of settings."
      },
      "confidence_score": 1.0,
      "reasoning": "The citation span lists several datasets used for event-based stereo depth estimation, including D-eDVS, evbench, MVSEC, UZH-FPV, ViViD, ViViD++, DSEC, AGRI-EBV, and TUM-VIE. These datasets are described with specific characteristics such as resolution, number of cameras, and application scenarios.",
      "processing_time": 70.46542429924011,
      "citing_paper_id": "250127779",
      "cited_paper_id": 11191105
    },
    {
      "context_text": "[11] in 2014, providing both event (128× 128, events only) and RGB-D streams with relatively low resolution.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.3,
      "reasoning": "The citation mentions 'event (128× 128, events only) and RGB-D streams' which suggests a dataset or data stream, but does not provide a specific name. The context is too generic and lacks a clear identifier.",
      "processing_time": 17.980918884277344,
      "citing_paper_id": "250127779",
      "cited_paper_id": 11191105
    },
    {
      "context_text": "For further details on the internal operation principle of event cameras, the reader is kindly referred to prior art [1], [2].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only references to prior art about event cameras.",
      "processing_time": 11.45203971862793,
      "citing_paper_id": "250127779",
      "cited_paper_id": 118684904
    },
    {
      "context_text": "…850nm infrared strobes to locate the passive spherical markers, we have adopted the common practice of putting an infrared ﬁlter (PHTODE IR690, cutoff frequency of 400-690nm) in front of the lenses [18], [21], thus blocking most of the ﬂashing and reducing the background noise in the event stream.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only hardware components and methods. The context is about using an infrared filter to reduce background noise in the event stream.",
      "processing_time": 15.346958875656128,
      "citing_paper_id": "250127779",
      "cited_paper_id": 195800546
    },
    {
      "context_text": "Similar to the MVSEC sequences, DSEC [16] uses two stereo cameras but omits the LiDAR and the IMU.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "DSEC"
      ],
      "dataset_descriptions": {
        "DSEC": "Used to provide stereo event camera data for driving scenarios, focusing on depth estimation without LiDAR and IMU data."
      },
      "confidence_score": 1.0,
      "reasoning": "DSEC is mentioned as a dataset used for driving scenarios with stereo event cameras, similar to MVSEC but without LiDAR and IMU.",
      "processing_time": 19.24687623977661,
      "citing_paper_id": "250127779",
      "cited_paper_id": 232170230
    },
    {
      "context_text": "Zujevs et al. propose agricultural robotics-oriented datasets [17] that are recorded by a mobile robot in different types of agricultural environments during autumn season.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions datasets recorded by a mobile robot in agricultural environments, which aligns with the topic of event-based stereo depth estimation. However, the specific dataset names are not provided.",
      "processing_time": 15.616648197174072,
      "citing_paper_id": "250127779",
      "cited_paper_id": 239039476
    },
    {
      "context_text": "It traditionally involves stereo matching [42], optimized stereo matching using graph-cuts [22], and cost-volume filtering [16].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only methods and techniques. The cited papers' titles do not provide additional information about datasets.",
      "processing_time": 14.472388982772827,
      "citing_paper_id": "249980412",
      "cited_paper_id": 1680724
    },
    {
      "context_text": "It traditionally involves stereo matching [42], optimized stereo matching using graph-cuts [22], and cost-volume filtering [16].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only methods and techniques. The cited papers' titles do not provide additional information about datasets.",
      "processing_time": 14.472388982772827,
      "citing_paper_id": "249980412",
      "cited_paper_id": 7495827
    },
    {
      "context_text": "Recently, learning-based approaches improved accuracy drastically [4, 19, 26] which was also further improved by utilizing more 3D convolutions [5], deformable convolutions [18] and adaptive aggregations [45,46].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only methods and improvements in learning-based approaches for stereo depth estimation.",
      "processing_time": 13.296051025390625,
      "citing_paper_id": "249980412",
      "cited_paper_id": 2658860
    },
    {
      "context_text": "Recently, learning-based approaches improved accuracy drastically [4, 19, 26] which was also further improved by utilizing more 3D convolutions [5], deformable convolutions [18] and adaptive aggregations [45,46].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only methods and improvements in learning-based approaches for stereo depth estimation.",
      "processing_time": 13.296051025390625,
      "citing_paper_id": "249980412",
      "cited_paper_id": 7428689
    },
    {
      "context_text": "Recently, learning-based approaches improved accuracy drastically [4, 19, 26] which was also further improved by utilizing more 3D convolutions [5], deformable convolutions [18] and adaptive aggregations [45,46].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only methods and improvements in learning-based approaches for stereo depth estimation.",
      "processing_time": 13.296051025390625,
      "citing_paper_id": "249980412",
      "cited_paper_id": 102352684
    },
    {
      "context_text": "Smooth L1 loss is widely used in image-based stereo matching because of its robustness in disparity discontinuities and low sensitivity to outliers or noise in comparison to L2 loss [5,37,45,46].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only discusses the use of Smooth L1 loss in stereo matching. No dataset names are present in the context.",
      "processing_time": 15.611526012420654,
      "citing_paper_id": "249980412",
      "cited_paper_id": 4252896
    },
    {
      "context_text": "Smooth L1 loss is widely used in image-based stereo matching because of its robustness in disparity discontinuities and low sensitivity to outliers or noise in comparison to L2 loss [5,37,45,46].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only discusses the use of Smooth L1 loss in stereo matching. No dataset names are present in the context.",
      "processing_time": 15.611526012420654,
      "citing_paper_id": "249980412",
      "cited_paper_id": 119304432
    },
    {
      "context_text": "Smooth L1 loss is widely used in image-based stereo matching because of its robustness in disparity discontinuities and low sensitivity to outliers or noise in comparison to L2 loss [5,37,45,46].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only discusses the use of Smooth L1 loss in stereo matching. No dataset names are present in the context.",
      "processing_time": 15.611526012420654,
      "citing_paper_id": "249980412",
      "cited_paper_id": 233204703
    },
    {
      "context_text": "Recently, learning-based approaches improved accuracy drastically [4, 19, 26] which was also further improved by utilizing more 3D convolutions [5], deformable convolutions [18] and adaptive aggregations [45, 46].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only methods and improvements in stereo matching techniques.",
      "processing_time": 12.051031112670898,
      "citing_paper_id": "249980412",
      "cited_paper_id": 4252896
    },
    {
      "context_text": "Recently, learning-based approaches improved accuracy drastically [4, 19, 26] which was also further improved by utilizing more 3D convolutions [5], deformable convolutions [18] and adaptive aggregations [45, 46].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only methods and improvements in stereo matching techniques.",
      "processing_time": 12.051031112670898,
      "citing_paper_id": "249980412",
      "cited_paper_id": 119304432
    },
    {
      "context_text": "We use feature pyramid networks [5] to recover details from each layer in a coarse to fine manner in multiple resolutions.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (feature pyramid networks).",
      "processing_time": 11.820745468139648,
      "citing_paper_id": "249980412",
      "cited_paper_id": 4252896
    },
    {
      "context_text": "Instead, we choose to align the future information to current ‘softly’ [38], computing the discrepancy between past-only and past and future events by the probability distance through the relative entropy (the well-known and widely used KL divergence for knowledge distillation [15, 38] and recent successful application in the event literature [7].",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and concepts such as KL divergence and knowledge distillation.",
      "processing_time": 12.894033432006836,
      "citing_paper_id": "249980412",
      "cited_paper_id": 7200347
    },
    {
      "context_text": "Unlike traditional convolu-tions, deformable convolutions learn dense spatial transformations with additional offsets by learning to expand (deform) to a ‘larger receptive ﬁeld’ instead of ﬁxed off-sets [18].",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (deformable convolutions).",
      "processing_time": 12.268330335617065,
      "citing_paper_id": "249980412",
      "cited_paper_id": 7428689
    },
    {
      "context_text": "Such problems were addressed by utilizing orientation-sensitive filters [3], and cooperative regularization [10, 33].",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods or approaches. The context is about addressing problems using orientation-sensitive filters and cooperative regularization.",
      "processing_time": 14.780400514602661,
      "citing_paper_id": "249980412",
      "cited_paper_id": 12047627
    },
    {
      "context_text": "Feature correlation [9] is used instead of concatenating features as the inner product convolves data from the left pair with data from the correct pair instead of convolving data with filters, resulting in a light-weighted network.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for feature correlation. The context is about a technical detail in network architecture.",
      "processing_time": 14.496965408325195,
      "citing_paper_id": "249980412",
      "cited_paper_id": 12552176
    },
    {
      "context_text": "However, imperfections such as real-world noise and different event threshold values among stereo pairs make the problem non-trivial [21, 34].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only discusses challenges in event-based stereo depth estimation.",
      "processing_time": 11.596312284469604,
      "citing_paper_id": "249980412",
      "cited_paper_id": 17693733
    },
    {
      "context_text": "Spiking neural networks were also the main study direction to address event-based stereo depth [1,8,31].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only the use of spiking neural networks for event-based stereo depth estimation.",
      "processing_time": 13.981686115264893,
      "citing_paper_id": "249980412",
      "cited_paper_id": 34855834
    },
    {
      "context_text": "Spiking neural networks were also the main study direction to address event-based stereo depth [1,8,31].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only the use of spiking neural networks for event-based stereo depth estimation.",
      "processing_time": 13.981686115264893,
      "citing_paper_id": "249980412",
      "cited_paper_id": 205698386
    },
    {
      "context_text": "Other proposals include utilizing camera velocity for event synchronization [49], or estimating depth without explicit event matching [47].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods or approaches. The context is focused on techniques for event synchronization and depth estimation.",
      "processing_time": 14.772985219955444,
      "citing_paper_id": "249980412",
      "cited_paper_id": 49877954
    },
    {
      "context_text": "To estimate accuracy depth at edges, we hierarchically upsample the predicted low-resolution disparity to higher intermediate scales by re-ﬁnement [4].",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for upsampling disparity maps.",
      "processing_time": 12.274768114089966,
      "citing_paper_id": "249980412",
      "cited_paper_id": 102352684
    },
    {
      "context_text": "The pipeline of EI-Stereo [27] is arguably popular in literature [45, 46] but component determines the quality; we have a concentration network and (b) knowledge transfer from future.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (EI-Stereo) and its components. No verifiable datasets are referenced.",
      "processing_time": 15.588864803314209,
      "citing_paper_id": "249980412",
      "cited_paper_id": 119304432
    },
    {
      "context_text": "The objective functions [11] were also studied as ‘focus loss’ as they resemble the loss functions in shape-from-focus applications.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only loss functions which are not considered datasets.",
      "processing_time": 11.599020719528198,
      "citing_paper_id": "249980412",
      "cited_paper_id": 119309624
    },
    {
      "context_text": "This line of research has gathered interest within the event camera community and has advanced in many aspects [23] including novel algorithms and attempts to generalize to real-world situations.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general advancements in the field. No dataset names are present in the text.",
      "processing_time": 14.763975381851196,
      "citing_paper_id": "249980412",
      "cited_paper_id": 219303641
    },
    {
      "context_text": "More details can be found in [23,27,43].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.0,
      "reasoning": "The citation span does not provide any specific information about datasets or their usage. The cited papers are likely referenced for background or related work.",
      "processing_time": 13.971436023712158,
      "citing_paper_id": "249980412",
      "cited_paper_id": 219303641
    },
    {
      "context_text": "( i.e ., shiny) objects, and low-texture areas [23,27].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general challenges in stereo depth estimation.",
      "processing_time": 12.86933445930481,
      "citing_paper_id": "249980412",
      "cited_paper_id": 219303641
    },
    {
      "context_text": "A simple yet very fast visual odometry using graph-based optimizations was presented [25] for motion averaging and was verified by the motion of a high-precision robot arm.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for visual odometry. There are no clear identifiers for datasets in the provided context.",
      "processing_time": 13.966738224029541,
      "citing_paper_id": "249980412",
      "cited_paper_id": 232170519
    },
    {
      "context_text": "In an early work of estimating stereo depth in a complementary setting by utilizing both events and intensity images [27], they unify the event stacks and intensity images in a ‘recycling network’, a recurrent module that iterates over events and images to reconstruct a blur-free imagelike tensor that has high dynamic range properties of events.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific dataset names, only a method for combining events and intensity images for depth estimation.",
      "processing_time": 13.257606506347656,
      "citing_paper_id": "249980412",
      "cited_paper_id": 244306440
    },
    {
      "context_text": "5, we qualitatively compare the output depth predictions of our method to the state-of-the-art event-intensity stereo depth estimation method [27].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a comparison to another method. No dataset names are provided in the context.",
      "processing_time": 14.729702711105347,
      "citing_paper_id": "249980412",
      "cited_paper_id": 244306440
    },
    {
      "context_text": "These modules are commonly used in stereo depth estimation networks, thus we follow the stereo matching design from [27], which in turn is also inspired from previous arts.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only references to stereo matching designs and methods.",
      "processing_time": 11.808039665222168,
      "citing_paper_id": "249980412",
      "cited_paper_id": 244306440
    },
    {
      "context_text": "When we utilize event and intensity images using our fusion scheme, ours (Ours on E+I) clearly outperforms the state-of-the-art method [27] by large margins.",
      "catation_intent": "findings",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a comparison of methods. No dataset names are provided in the context.",
      "processing_time": 13.496403694152832,
      "citing_paper_id": "249980412",
      "cited_paper_id": 244306440
    },
    {
      "context_text": "As shown in the table, our method that only utilizes events (Ours on E) has much lower errors in comparison to the DSEC event-only baseline [43] in all metrics and to the state of the art [27] in MAE (the main metric), 1PE and 2PE but only worse in RMSE.",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions 'DSEC event-only baseline' which appears to be a dataset or benchmark used for comparison. However, since it is primarily used for score comparison and not as a downloadable dataset, it is excluded.",
      "processing_time": 17.68826675415039,
      "citing_paper_id": "249980412",
      "cited_paper_id": 244306440
    },
    {
      "context_text": "Deep learning solutions considered combining a novel sequence embedding [43], or fusing depth and intensity images to cover the best from both worlds [27] to create highly detailed depth estimates.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and approaches. The context focuses on deep learning solutions and combining different types of images for depth estimation.",
      "processing_time": 16.14441752433777,
      "citing_paper_id": "249980412",
      "cited_paper_id": 244306440
    },
    {
      "context_text": "Following well-performing stereo depth estimation networks [27], we design our model using some of their subnetworks.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only references to other models or methods. There is no indication of a dataset being used.",
      "processing_time": 15.302315950393677,
      "citing_paper_id": "249980412",
      "cited_paper_id": 244306440
    },
    {
      "context_text": ", shiny) objects, and low-texture areas [23, 27].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general challenges in stereo depth estimation.",
      "processing_time": 12.638120889663696,
      "citing_paper_id": "249980412",
      "cited_paper_id": 244306440
    },
    {
      "context_text": "2 FPS, while [27] only performs at 10 FPS.",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only performance metrics. No verifiable resources are identified.",
      "processing_time": 12.282354354858398,
      "citing_paper_id": "249980412",
      "cited_paper_id": 244306440
    },
    {
      "context_text": "Note that we do not use their initial modules such as the event representation in [43] or the parts for merging event and intensity images in [27] and rather focus on the stereo matching modules.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only modules and methods from other papers.",
      "processing_time": 11.392090082168579,
      "citing_paper_id": "249980412",
      "cited_paper_id": 244306440
    },
    {
      "context_text": "Dataset Platform Terrain Event Cameras LiDAR CIS Cameras Semantic Labels The Event Camera Slider Urban Inivation DVS 240C N/A DVS APS Pixel N/A Dataset [19] Hand Held Indoor 240x180 240x180 Grayscale MVSEC [29] Car + Motorcycle Urban Inivation DVS 346 Velodyne VLP-16 Vi-Sensor N/A Quadrotor Indoor Flight 346x260 752x480 Grayscale KITTI 360 [15] Car Urban N/A Velodyne HDL-64E YES 37 Classes DSEC [9] Car Urban and Suburban Prophesee Gen 3 Velodyne VLP-16 FLIR Backfly S 11 Classes 640x480 1440x1080 RGB VECtor [8] Helmet + Cart Indoor Prophesee Gen 3 Ouster OS0-128 FLIR Grasshopper3 N/A 640x480 1224 × 1024 Grayscale TUM-VIE [14] Helmet + handheld Indoor and Outdoor Prophesee Gen 4 N/A IDS Camera uEye N/A 1280x720 1224 × 1024 Grayscale Car Forest and Urban OVC 3b M3ED Quadroped Forest and Urban Prophesee Gen 4 Ouster OS1-64U 1280x800 11 Classes UAV Forest and Urban 1280x720 RGB + Grayscale 3D Instances",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "DVS APS Pixel",
        "MVSEC",
        "KITTI 360",
        "DSEC",
        "VECtor",
        "TUM-VIE",
        "OVC 3b M3ED"
      ],
      "dataset_descriptions": {
        "DVS APS Pixel": "Used for indoor hand-held experiments with grayscale images, focusing on event-based stereo depth estimation.",
        "MVSEC": "Used for car and motorcycle urban scenarios with grayscale images, focusing on event-based visual odometry and SLAM.",
        "KITTI 360": "Used for urban driving scenarios with semantic labels, focusing on event-based stereo depth estimation and scene understanding.",
        "DSEC": "Used for urban and suburban driving scenarios with 11 classes, focusing on event-based stereo depth estimation.",
        "VECtor": "Used for indoor and outdoor helmet and cart scenarios, focusing on multi-sensor SLAM and event-based depth estimation.",
        "TUM-VIE": "Used for indoor and outdoor helmet and handheld scenarios with grayscale images, focusing on event-based visual-inertial odometry.",
        "OVC 3b M3ED": "Used for forest and urban quadroped scenarios with 11 classes, focusing on event-based stereo depth estimation and 3D instance segmentation."
      },
      "confidence_score": 1.0,
      "reasoning": "The context lists several datasets used for event-based stereo depth estimation, including specific details about their content and usage.",
      "processing_time": 54.815261125564575,
      "citing_paper_id": "259380779",
      "cited_paper_id": 9865213
    },
    {
      "context_text": "Dataset Platform Terrain Event Cameras LiDAR CIS Cameras Semantic Labels The Event Camera Slider Urban Inivation DVS 240C N/A DVS APS Pixel N/A Dataset [19] Hand Held Indoor 240x180 240x180 Grayscale MVSEC [29] Car + Motorcycle Urban Inivation DVS 346 Velodyne VLP-16 Vi-Sensor N/A Quadrotor Indoor Flight 346x260 752x480 Grayscale KITTI 360 [15] Car Urban N/A Velodyne HDL-64E YES 37 Classes DSEC [9] Car Urban and Suburban Prophesee Gen 3 Velodyne VLP-16 FLIR Backfly S 11 Classes 640x480 1440x1080 RGB VECtor [8] Helmet + Cart Indoor Prophesee Gen 3 Ouster OS0-128 FLIR Grasshopper3 N/A 640x480 1224 × 1024 Grayscale TUM-VIE [14] Helmet + handheld Indoor and Outdoor Prophesee Gen 4 N/A IDS Camera uEye N/A 1280x720 1224 × 1024 Grayscale Car Forest and Urban OVC 3b M3ED Quadroped Forest and Urban Prophesee Gen 4 Ouster OS1-64U 1280x800 11 Classes UAV Forest and Urban 1280x720 RGB + Grayscale 3D Instances",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "DVS APS Pixel",
        "MVSEC",
        "KITTI 360",
        "DSEC",
        "VECtor",
        "TUM-VIE",
        "OVC 3b M3ED"
      ],
      "dataset_descriptions": {
        "DVS APS Pixel": "Used for indoor hand-held experiments with grayscale images, focusing on event-based stereo depth estimation.",
        "MVSEC": "Used for car and motorcycle urban scenarios with grayscale images, focusing on event-based visual odometry and SLAM.",
        "KITTI 360": "Used for urban driving scenarios with semantic labels, focusing on event-based stereo depth estimation and scene understanding.",
        "DSEC": "Used for urban and suburban driving scenarios with 11 classes, focusing on event-based stereo depth estimation.",
        "VECtor": "Used for indoor and outdoor helmet and cart scenarios, focusing on multi-sensor SLAM and event-based depth estimation.",
        "TUM-VIE": "Used for indoor and outdoor helmet and handheld scenarios with grayscale images, focusing on event-based visual-inertial odometry.",
        "OVC 3b M3ED": "Used for forest and urban quadroped scenarios with 11 classes, focusing on event-based stereo depth estimation and 3D instance segmentation."
      },
      "confidence_score": 1.0,
      "reasoning": "The context lists several datasets used for event-based stereo depth estimation, including specific details about their content and usage.",
      "processing_time": 54.815261125564575,
      "citing_paper_id": "259380779",
      "cited_paper_id": 232170230
    },
    {
      "context_text": "Dataset Platform Terrain Event Cameras LiDAR CIS Cameras Semantic Labels The Event Camera Slider Urban Inivation DVS 240C N/A DVS APS Pixel N/A Dataset [19] Hand Held Indoor 240x180 240x180 Grayscale MVSEC [29] Car + Motorcycle Urban Inivation DVS 346 Velodyne VLP-16 Vi-Sensor N/A Quadrotor Indoor Flight 346x260 752x480 Grayscale KITTI 360 [15] Car Urban N/A Velodyne HDL-64E YES 37 Classes DSEC [9] Car Urban and Suburban Prophesee Gen 3 Velodyne VLP-16 FLIR Backfly S 11 Classes 640x480 1440x1080 RGB VECtor [8] Helmet + Cart Indoor Prophesee Gen 3 Ouster OS0-128 FLIR Grasshopper3 N/A 640x480 1224 × 1024 Grayscale TUM-VIE [14] Helmet + handheld Indoor and Outdoor Prophesee Gen 4 N/A IDS Camera uEye N/A 1280x720 1224 × 1024 Grayscale Car Forest and Urban OVC 3b M3ED Quadroped Forest and Urban Prophesee Gen 4 Ouster OS1-64U 1280x800 11 Classes UAV Forest and Urban 1280x720 RGB + Grayscale 3D Instances",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "DVS APS Pixel",
        "MVSEC",
        "KITTI 360",
        "DSEC",
        "VECtor",
        "TUM-VIE",
        "OVC 3b M3ED"
      ],
      "dataset_descriptions": {
        "DVS APS Pixel": "Used for indoor hand-held experiments with grayscale images, focusing on event-based stereo depth estimation.",
        "MVSEC": "Used for car and motorcycle urban scenarios with grayscale images, focusing on event-based visual odometry and SLAM.",
        "KITTI 360": "Used for urban driving scenarios with semantic labels, focusing on event-based stereo depth estimation and scene understanding.",
        "DSEC": "Used for urban and suburban driving scenarios with 11 classes, focusing on event-based stereo depth estimation.",
        "VECtor": "Used for indoor and outdoor helmet and cart scenarios, focusing on multi-sensor SLAM and event-based depth estimation.",
        "TUM-VIE": "Used for indoor and outdoor helmet and handheld scenarios with grayscale images, focusing on event-based visual-inertial odometry.",
        "OVC 3b M3ED": "Used for forest and urban quadroped scenarios with 11 classes, focusing on event-based stereo depth estimation and 3D instance segmentation."
      },
      "confidence_score": 1.0,
      "reasoning": "The context lists several datasets used for event-based stereo depth estimation, including specific details about their content and usage.",
      "processing_time": 54.815261125564575,
      "citing_paper_id": "259380779",
      "cited_paper_id": 250127779
    },
    {
      "context_text": "The Open Vision Computer 3b (OVC) [22] provides high-quality hardware-synchronized global shutter stereo images in grayscale, a single RGB image stream, and high-quality IMU measurements.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions the OVC as a hardware system providing various types of sensor data, but does not refer to a specific dataset.",
      "processing_time": 15.293720722198486,
      "citing_paper_id": "259380779",
      "cited_paper_id": 52308074
    },
    {
      "context_text": "Additionally, the visual sensors are soft mounted relative to the lidar itself and thus the transformation between the lidar and the cameras may shift slightly during sequences (the OVC and the event cameras are rigidly attached).",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only hardware components and their mounting. There are no clear identifiers for datasets.",
      "processing_time": 13.491225719451904,
      "citing_paper_id": "259380779",
      "cited_paper_id": 52308074
    },
    {
      "context_text": "The OVC orchestrates the system and provides synchronization signals to the other sensors.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a system called OVC which is not a dataset.",
      "processing_time": 12.871652126312256,
      "citing_paper_id": "259380779",
      "cited_paper_id": 52308074
    },
    {
      "context_text": "The event cameras are placed at an equivalent baseline and a similar field of view to the OVC imagers to provide relevant comparisons for VIO applications.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only hardware configurations. No dataset names are present in the citation span.",
      "processing_time": 13.255415201187134,
      "citing_paper_id": "259380779",
      "cited_paper_id": 52308074
    },
    {
      "context_text": "The OVC3b was designed to go on smaller robotic platforms that have size constraints with a baseline of 12 cm.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a hardware platform called OVC3b. There are no verifiable resources that meet the criteria for inclusion.",
      "processing_time": 15.546109676361084,
      "citing_paper_id": "259380779",
      "cited_paper_id": 52308074
    },
    {
      "context_text": "Event cameras are particularly well suited for these operational scenarios, since they can react and respond with low latencies and high dynamic range [7].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general property of event cameras. No verifiable resources are identified.",
      "processing_time": 13.675450325012207,
      "citing_paper_id": "259380779",
      "cited_paper_id": 118684904
    },
    {
      "context_text": "Event cameras are naturally suited for optical flow estimation because motion is naturally encoded within the event stream [7].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general property of event cameras. No dataset names are present in the citation span.",
      "processing_time": 15.542938470840454,
      "citing_paper_id": "259380779",
      "cited_paper_id": 118684904
    },
    {
      "context_text": "Moreover, we targeted scenarios where traditional CIS imagers fail, such as undercanopy in forests or a fast transition from brightness to darkness [7].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only general scenarios where traditional CIS imagers fail. No verifiable resources are identified.",
      "processing_time": 13.92989468574524,
      "citing_paper_id": "259380779",
      "cited_paper_id": 118684904
    },
    {
      "context_text": "Furthermore, low-latency segmentation of independently moving objects (IMOs) remains a challenging task for event cameras [5].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a challenge for event cameras. No verifiable resources are identified.",
      "processing_time": 13.653903484344482,
      "citing_paper_id": "259380779",
      "cited_paper_id": 213193728
    },
    {
      "context_text": "E-RAFT demonstrated state-of-the-art performance on the DSEC [9] dataset.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "DSEC"
      ],
      "dataset_descriptions": {
        "DSEC": "Used to evaluate E-RAFT's performance in stereo depth estimation for driving scenarios, providing event camera data for benchmarking."
      },
      "confidence_score": 1.0,
      "reasoning": "DSEC is explicitly mentioned as a dataset used to evaluate E-RAFT's performance in driving scenarios.",
      "processing_time": 18.785094022750854,
      "citing_paper_id": "259380779",
      "cited_paper_id": 232170230
    },
    {
      "context_text": "Recently, DSEC [9] introduced a KITTI-like dataset providing ground-truth pose and depth with corresponding events.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "DSEC"
      ],
      "dataset_descriptions": {
        "DSEC": "Used to provide ground-truth pose and depth with corresponding events for driving scenarios, enabling evaluation of event-based stereo depth estimation methods."
      },
      "confidence_score": 1.0,
      "reasoning": "DSEC is identified as a dataset providing ground-truth pose and depth with corresponding events, similar to KITTI, for driving scenarios.",
      "processing_time": 19.919384002685547,
      "citing_paper_id": "259380779",
      "cited_paper_id": 232170230
    },
    {
      "context_text": "We use the flow evaluation metrics described in DSEC [9] NPE: the percentage of groundtruth pixels with optical flow magnitude error > N.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "DSEC"
      ],
      "dataset_descriptions": {
        "DSEC": "Used to obtain flow evaluation metrics for driving scenarios, specifically the percentage of groundtruth pixels with optical flow magnitude error > N."
      },
      "confidence_score": 1.0,
      "reasoning": "DSEC is mentioned as a source for flow evaluation metrics, which is relevant to event-based stereo depth estimation.",
      "processing_time": 18.478858470916748,
      "citing_paper_id": "259380779",
      "cited_paper_id": 232170230
    },
    {
      "context_text": "Vision-based perception is particularly difficult due to the high dynamic range required [16].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general challenge related to vision-based perception.",
      "processing_time": 12.606866836547852,
      "citing_paper_id": "259380779",
      "cited_paper_id": 246576350
    },
    {
      "context_text": "FasterLIO [1] provides poses and velocity-corrected LiDAR frames for every sweep of the LiDAR.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (FasterLIO) for providing poses and velocity-corrected LiDAR frames.",
      "processing_time": 16.295445680618286,
      "citing_paper_id": "259380779",
      "cited_paper_id": 247231064
    },
    {
      "context_text": "Forest Fast autonomous flights in forests have recently attracted more attention in the robotics community, due to the challenges of these cluttered environments [17, 28].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only general research interest in autonomous flights in forests.",
      "processing_time": 12.848084211349487,
      "citing_paper_id": "259380779",
      "cited_paper_id": 248527447
    },
    {
      "context_text": "TUM-VIE [14] and VECtor [8] each provide head-mounted ego-centric motion as well as smooth pole-mounted trajectories inside a fixed indoor environment.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "TUM-VIE",
        "VECtor"
      ],
      "dataset_descriptions": {
        "TUM-VIE": "Used to provide head-mounted ego-centric motion data in a fixed indoor environment, supporting research in event-based stereo depth estimation.",
        "VECtor": "Used to provide smooth pole-mounted trajectories in a fixed indoor environment, supporting research in multi-sensor SLAM and event-based stereo depth estimation."
      },
      "confidence_score": 1.0,
      "reasoning": "The citation mentions two datasets, TUM-VIE and VECtor, which are used to provide head-mounted ego-centric motion and smooth pole-mounted trajectories in a fixed indoor environment.",
      "processing_time": 28.090455293655396,
      "citing_paper_id": "259380779",
      "cited_paper_id": 250127779
    },
    {
      "context_text": "These scenes are particularly challenging for algorithms that exploit the sparsity of events to reduce computation [23], as the event count can average 100 MegaEvents per second (MEPS), as shown by Fig.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only discusses the challenges of high event rates for certain algorithms.",
      "processing_time": 13.472694158554077,
      "citing_paper_id": "259380779",
      "cited_paper_id": null
    },
    {
      "context_text": "We compare our method to “FCVF” [18], “Libelas” method [19], and EDS method [7] to demonstrate the performance of our method.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only methods and comparisons. No verifiable resources are identified.",
      "processing_time": 14.702654361724854,
      "citing_paper_id": "1408596",
      "cited_paper_id": 185541
    },
    {
      "context_text": "We compare our method to “FCVF” [18], “Libelas” method [19], and EDS method [7] to demonstrate the performance of our method.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only methods and comparisons. No verifiable resources are identified.",
      "processing_time": 14.702654361724854,
      "citing_paper_id": "1408596",
      "cited_paper_id": 1680724
    },
    {
      "context_text": "We compare our method to “FCVF” [18], “Libelas” method [19], and EDS method [7] to demonstrate the performance of our method.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only methods and comparisons. No verifiable resources are identified.",
      "processing_time": 14.702654361724854,
      "citing_paper_id": "1408596",
      "cited_paper_id": 5535646
    },
    {
      "context_text": "In fact, the method [7] ﬁnds the matching events by only using the spatial distribution of events in horizontal direction, thus mismatched between events will occur in this method.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for event-driven stereo matching. No verifiable resources are identified.",
      "processing_time": 14.418987274169922,
      "citing_paper_id": "1408596",
      "cited_paper_id": 185541
    },
    {
      "context_text": "The test dataset is generated with structure light sensors and masked the corresponding reference (left) event map from DVS. EDS[7] FCVF [18] We adopt various metrics from Middlebury [20] to quantitatively compare our method to related methods [7, 18, 19], as shown in Table 1.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Middlebury"
      ],
      "dataset_descriptions": {
        "Middlebury": "Used to quantitatively compare the proposed method with related methods using various metrics, focusing on performance evaluation in stereo depth estimation."
      },
      "confidence_score": 0.9,
      "reasoning": "The context mentions 'Middlebury' as a source of metrics for quantitative comparison. No other specific datasets are mentioned.",
      "processing_time": 19.786656141281128,
      "citing_paper_id": "1408596",
      "cited_paper_id": 185541
    },
    {
      "context_text": "The test dataset is generated with structure light sensors and masked the corresponding reference (left) event map from DVS. EDS[7] FCVF [18] We adopt various metrics from Middlebury [20] to quantitatively compare our method to related methods [7, 18, 19], as shown in Table 1.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Middlebury"
      ],
      "dataset_descriptions": {
        "Middlebury": "Used to quantitatively compare the proposed method with related methods using various metrics, focusing on performance evaluation in stereo depth estimation."
      },
      "confidence_score": 0.9,
      "reasoning": "The context mentions 'Middlebury' as a source of metrics for quantitative comparison. No other specific datasets are mentioned.",
      "processing_time": 19.786656141281128,
      "citing_paper_id": "1408596",
      "cited_paper_id": 1680724
    },
    {
      "context_text": "The test dataset is generated with structure light sensors and masked the corresponding reference (left) event map from DVS. EDS[7] FCVF [18] We adopt various metrics from Middlebury [20] to quantitatively compare our method to related methods [7, 18, 19], as shown in Table 1.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Middlebury"
      ],
      "dataset_descriptions": {
        "Middlebury": "Used to quantitatively compare the proposed method with related methods using various metrics, focusing on performance evaluation in stereo depth estimation."
      },
      "confidence_score": 0.9,
      "reasoning": "The context mentions 'Middlebury' as a source of metrics for quantitative comparison. No other specific datasets are mentioned.",
      "processing_time": 19.786656141281128,
      "citing_paper_id": "1408596",
      "cited_paper_id": 5535646
    },
    {
      "context_text": "Recently, Schraml et al. [7] proposed to solve the problem of ﬁnding corresponding events in the time domain by deﬁning a novel cost measure that is based on event distributions using inter-event distances, but failed in processing DVS data with similar event distributions in the horizontal…",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a method or approach by Schraml et al. The term 'DVS data' is mentioned but it is not a specific dataset name.",
      "processing_time": 17.519068241119385,
      "citing_paper_id": "1408596",
      "cited_paper_id": 185541
    },
    {
      "context_text": "The energy function (5) can be efﬁciently solved with the highly efﬁcient approximate inference algorithm [17], which achieves 100+ times faster than using gradient descent methods.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only an inference algorithm. No dataset names are present in the citation context.",
      "processing_time": 14.966420888900757,
      "citing_paper_id": "1408596",
      "cited_paper_id": 5574079
    },
    {
      "context_text": "Our method also relates to a large body of classic dense stereo matching methods such as dynamic programming based methods [8, 9], belief propagation based methods [10, 11, 12], graph cuts based methods [13] and so on.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only various methods for stereo matching. No verifiable resources are identified.",
      "processing_time": 15.5138680934906,
      "citing_paper_id": "1408596",
      "cited_paper_id": 5880703
    },
    {
      "context_text": "Our method also relates to a large body of classic dense stereo matching methods such as dynamic programming based methods [8, 9], belief propagation based methods [10, 11, 12], graph cuts based methods [13] and so on.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only various methods for stereo matching. No verifiable resources are identified.",
      "processing_time": 15.5138680934906,
      "citing_paper_id": "1408596",
      "cited_paper_id": 6788573
    },
    {
      "context_text": "Our method also relates to a large body of classic dense stereo matching methods such as dynamic programming based methods [8, 9], belief propagation based methods [10, 11, 12], graph cuts based methods [13] and so on.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only various methods for stereo matching. No verifiable resources are identified.",
      "processing_time": 15.5138680934906,
      "citing_paper_id": "1408596",
      "cited_paper_id": 7495827
    },
    {
      "context_text": "In fact, the method [14] represents the local context of an “on” pixel with these spatial distances from “off” pixels to the “on” event.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for representing the local context of an 'on' pixel using spatial distances.",
      "processing_time": 14.998011589050293,
      "citing_paper_id": "1408596",
      "cited_paper_id": 8027136
    },
    {
      "context_text": "Hausdorff et al. [14] proposed a method to compare binary images by calculating distance from each pixel to nearest “on” pixels.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for comparing binary images.",
      "processing_time": 12.231154680252075,
      "citing_paper_id": "1408596",
      "cited_paper_id": 8027136
    },
    {
      "context_text": "A more comprehensive survey on stereo matching can be found in [2].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a reference to a survey on stereo matching.",
      "processing_time": 14.216975927352905,
      "citing_paper_id": "1408596",
      "cited_paper_id": 195859047
    },
    {
      "context_text": "In this report, we primarily focus on the event embedding method and design a ConvLSTM [3]-based event feature extractor.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (ConvLSTM) which is not a dataset.",
      "processing_time": 21.556951999664307,
      "citing_paper_id": "263339606",
      "cited_paper_id": 6352419
    },
    {
      "context_text": "Before being fed into the ConvLSTM, the event voxel is split into 3 separate bins.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for processing event data. No verifiable resources are identified.",
      "processing_time": 22.436026096343994,
      "citing_paper_id": "263339606",
      "cited_paper_id": 6352419
    },
    {
      "context_text": "By using ConvLSTM for event embedding, we are able to achieve a lower MAE value (0.59) than directly using event voxel grid as the stereo network input.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (ConvLSTM) and a metric (MAE).",
      "processing_time": 22.780141830444336,
      "citing_paper_id": "263339606",
      "cited_paper_id": 6352419
    },
    {
      "context_text": "We then design a ConvLSTM-based embedding method to extract spatio-temporal event features from the event voxel in a sequential manner, which can be helpful for the followed stereo matching network.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a method for extracting spatio-temporal event features using ConvLSTM.",
      "processing_time": 23.07994556427002,
      "citing_paper_id": "263339606",
      "cited_paper_id": 6352419
    },
    {
      "context_text": "The ConvLSTM then processes each bin while keeping the spatial dimension consistent.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a method (ConvLSTM).",
      "processing_time": 21.396499395370483,
      "citing_paper_id": "263339606",
      "cited_paper_id": 6352419
    },
    {
      "context_text": "In this report, to efﬁciently gather the “valuable events” from the stacked voxel, we design a ConvLSTM-based Figure 2: Qualitative comparison of the predicted and GT disparity map.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a method (ConvLSTM) and a qualitative comparison of disparity maps. No verifiable datasets are referenced.",
      "processing_time": 25.140642404556274,
      "citing_paper_id": "263339606",
      "cited_paper_id": 6352419
    },
    {
      "context_text": "The ConvLSTM module is composed of two convolutional layers where the channel dimension of both hidden state and cell state is set to 32.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a method (ConvLSTM).",
      "processing_time": 22.117785215377808,
      "citing_paper_id": "263339606",
      "cited_paper_id": 6352419
    },
    {
      "context_text": "1, we visualize the diagram of proposed ConvLSTM-based event feature extractor.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (ConvLSTM-based event feature extractor).",
      "processing_time": 22.922102451324463,
      "citing_paper_id": "263339606",
      "cited_paper_id": 6352419
    },
    {
      "context_text": "The ConvLSTM module extracts not only spatial but also temporal information while processing the stacked event stream.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a method (ConvLSTM) and its capabilities. No verifiable resources are identified.",
      "processing_time": 14.67151165008545,
      "citing_paper_id": "263339606",
      "cited_paper_id": 6352419
    },
    {
      "context_text": "An event voxel grid is divided into three bins and they are fed to ConvLSTM in sequential manner.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a method (ConvLSTM) and a general description of how event data is processed. No verifiable dataset names are present.",
      "processing_time": 26.05451273918152,
      "citing_paper_id": "263339606",
      "cited_paper_id": 6352419
    },
    {
      "context_text": "We de-sign the ConvLSTM-based event feature extractor that considers both spatial and temporal nature of event stream data in a more efﬁcient way than the voxel-based or queue-based method.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a method (ConvLSTM) for processing event stream data. No verifiable datasets are referenced.",
      "processing_time": 24.68871021270752,
      "citing_paper_id": "263339606",
      "cited_paper_id": 6352419
    },
    {
      "context_text": "While using these networks as the backbone structure in conjunction with the event data embedding module, we empirically found that the performance of PSMNet is comparable to using AANet while being sig-niﬁcantly faster to train in our system.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models (PSMNet, AANet). The context focuses on comparing the performance and training speed of these models.",
      "processing_time": 25.813085794448853,
      "citing_paper_id": "263339606",
      "cited_paper_id": 216036364
    },
    {
      "context_text": "As aforementioned, we also test using AANet and it achieves almost similar performance to PSMNet.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only models (AANet and PSMNet). No verifiable resources are identified.",
      "processing_time": 24.392844438552856,
      "citing_paper_id": "263339606",
      "cited_paper_id": 216036364
    },
    {
      "context_text": "For the stereo matching module, we test two networks that perform well on conventional stereo images: PSMNet [1] and AANet [5].",
      "catation_intent": "reusable resource",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions networks (PSMNet and AANet) but does not refer to any specific datasets. The context is about testing networks on conventional stereo images, which implies the use of some stereo image datasets, but no specific datasets are named.",
      "processing_time": 28.160139083862305,
      "citing_paper_id": "263339606",
      "cited_paper_id": 216036364
    },
    {
      "context_text": "For the stereo matching module, we test two networks that perform well on conventional stereo images: PSM-Net [1] and AANet [5].",
      "catation_intent": "reusable resource",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions networks (PSM-Net and AANet) but does not refer to any specific datasets. The context is about testing networks on conventional stereo images, which implies the use of some stereo image datasets, but no specific datasets are named.",
      "processing_time": 28.272486209869385,
      "citing_paper_id": "263339606",
      "cited_paper_id": 216036364
    },
    {
      "context_text": "Prior studies introduced event data representations using hand-crafted methods (Sironi et al. 2018; Zhu et al. 2018b, 2019) to produce a dense event feature map from the sparse event data.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only methods for processing event data. No dataset names are provided.",
      "processing_time": 23.252974033355713,
      "citing_paper_id": "235306612",
      "cited_paper_id": 3396150
    },
    {
      "context_text": "Prior studies introduced event data representations using hand-crafted methods (Sironi et al. 2018; Zhu et al. 2018b, 2019) to produce a dense event feature map from the sparse event data.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only methods for processing event data. No dataset names are provided.",
      "processing_time": 23.252974033355713,
      "citing_paper_id": "235306612",
      "cited_paper_id": 56475917
    },
    {
      "context_text": "We use the Indoor Flying dataset from the MVSEC and divide them into three split, following (Tulyakov et al. 2019; Zhu, Chen, and Daniilidis 2018).",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Indoor Flying dataset",
        "MVSEC"
      ],
      "dataset_descriptions": {
        "Indoor Flying dataset": "Used to train and evaluate event-based stereo depth estimation methods, focusing on indoor flying scenarios with event cameras.",
        "MVSEC": "A collection of event-based datasets, including the Indoor Flying dataset, used for training and evaluating event-based vision algorithms."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions the 'Indoor Flying dataset' from the 'MVSEC'. This dataset is used for event-based stereo depth estimation, which is directly relevant to the research topic.",
      "processing_time": 35.70245385169983,
      "citing_paper_id": "235306612",
      "cited_paper_id": 3396150
    },
    {
      "context_text": "We use the Indoor Flying dataset from the MVSEC and divide them into three split, following (Tulyakov et al. 2019; Zhu, Chen, and Daniilidis 2018).",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Indoor Flying dataset",
        "MVSEC"
      ],
      "dataset_descriptions": {
        "Indoor Flying dataset": "Used to train and evaluate event-based stereo depth estimation methods, focusing on indoor flying scenarios with event cameras.",
        "MVSEC": "A collection of event-based datasets, including the Indoor Flying dataset, used for training and evaluating event-based vision algorithms."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions the 'Indoor Flying dataset' from the 'MVSEC'. This dataset is used for event-based stereo depth estimation, which is directly relevant to the research topic.",
      "processing_time": 35.70245385169983,
      "citing_paper_id": "235306612",
      "cited_paper_id": 262638843
    },
    {
      "context_text": "We evaluate our proposed method on the Multi Vehicle Stereo Event Camera Dataset (MVSEC) (Zhu et al. 2018a).",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Multi Vehicle Stereo Event Camera Dataset (MVSEC)"
      ],
      "dataset_descriptions": {
        "Multi Vehicle Stereo Event Camera Dataset (MVSEC)": "Used to evaluate the proposed method for event-based stereo depth estimation, focusing on performance metrics and robustness in dynamic environments."
      },
      "confidence_score": 1.0,
      "reasoning": "The citation clearly mentions the use of the MVSEC dataset for evaluating the proposed method in the context of event-based stereo depth estimation.",
      "processing_time": 30.61263084411621,
      "citing_paper_id": "235306612",
      "cited_paper_id": 3396150
    },
    {
      "context_text": "The proposed deep event stereo network has been evaluated using the public open dataset: the multi vehicle stereo event camera (MVSEC) dataset (Zhu et al. 2018a).",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "multi vehicle stereo event camera (MVSEC)"
      ],
      "dataset_descriptions": {
        "multi vehicle stereo event camera (MVSEC)": "Used to evaluate the deep event stereo network, focusing on stereo depth estimation performance with event-based cameras."
      },
      "confidence_score": 1.0,
      "reasoning": "The citation clearly mentions the use of the MVSEC dataset for evaluating the proposed deep event stereo network.",
      "processing_time": 29.31930708885193,
      "citing_paper_id": "235306612",
      "cited_paper_id": 3396150
    },
    {
      "context_text": "The MVSEC consists of precise depth information recorded form Lidar sensors along with event streams from two event cameras and corresponding intensity images with a resolution of 346 (cid:2) 260 pixels.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "MVSEC"
      ],
      "dataset_descriptions": {
        "MVSEC": "Used to provide precise depth information from Lidar sensors, event streams from event cameras, and intensity images for event-based stereo depth estimation."
      },
      "confidence_score": 1.0,
      "reasoning": "MVSEC is identified as a dataset containing depth information, event streams, and intensity images, which is relevant to event-based stereo depth estimation.",
      "processing_time": 29.52485179901123,
      "citing_paper_id": "235306612",
      "cited_paper_id": 3396150
    },
    {
      "context_text": "Speciﬁcally, we compare our method with the Semi-Dense 3D (Zhou et al. 2018), FCVF* (Hosni et al. 2012; Zhou et al. 2018), SGM* (Hirschmuller 2007; Zhou et al. 2018), TSES (Zhu, Chen, and Daniilidis 2018), CopNet (Piatkowska et al. 2017) and DDES (Tulyakov et al. 2019).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods and models. No dataset names are present in the text.",
      "processing_time": 23.952726364135742,
      "citing_paper_id": "235306612",
      "cited_paper_id": 7151414
    },
    {
      "context_text": "Speciﬁcally, we compare our method with the Semi-Dense 3D (Zhou et al. 2018), FCVF* (Hosni et al. 2012; Zhou et al. 2018), SGM* (Hirschmuller 2007; Zhou et al. 2018), TSES (Zhu, Chen, and Daniilidis 2018), CopNet (Piatkowska et al. 2017) and DDES (Tulyakov et al. 2019).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods and models. No dataset names are present in the text.",
      "processing_time": 23.952726364135742,
      "citing_paper_id": "235306612",
      "cited_paper_id": 49877954
    },
    {
      "context_text": "Speciﬁcally, we compare our method with the Semi-Dense 3D (Zhou et al. 2018), FCVF* (Hosni et al. 2012; Zhou et al. 2018), SGM* (Hirschmuller 2007; Zhou et al. 2018), TSES (Zhu, Chen, and Daniilidis 2018), CopNet (Piatkowska et al. 2017) and DDES (Tulyakov et al. 2019).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods and models. No dataset names are present in the text.",
      "processing_time": 23.952726364135742,
      "citing_paper_id": "235306612",
      "cited_paper_id": 262638843
    },
    {
      "context_text": "Speciﬁcally, we compare our method with the Semi-Dense 3D (Zhou et al. 2018), FCVF* (Hosni et al. 2012; Zhou et al. 2018), SGM* (Hirschmuller 2007; Zhou et al. 2018), TSES (Zhu, Chen, and Daniilidis 2018), CopNet (Piatkowska et al. 2017) and DDES (Tulyakov et al. 2019).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods and models. No dataset names are present in the text.",
      "processing_time": 23.952726364135742,
      "citing_paper_id": "235306612",
      "cited_paper_id": null
    },
    {
      "context_text": "The CopNet and TSES relies only on spatial information of the events and fails to preserve the temporal information of the events, which is essential for ﬁnding event correspondence in time and leads to poor disparity estimation.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and their limitations. No dataset names are present in the citation span.",
      "processing_time": 24.163042068481445,
      "citing_paper_id": "235306612",
      "cited_paper_id": 7151414
    },
    {
      "context_text": "We then perform a gating mechanism (Dauphin et al. 2017) for normalizing and learning semantic correspondences.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (gating mechanism) from the cited paper.",
      "processing_time": 23.240155458450317,
      "citing_paper_id": "235306612",
      "cited_paper_id": 16119010
    },
    {
      "context_text": "Early pioneering studies have reported that event cameras can be used for depth estimation (Zhou et al. 2018; Tulyakov et al. 2019; Zhu, Chen, and Daniilidis 2018; Xie, Chen, and Orchard 2017; Dikov et al. 2017; Schraml and Belbachir 2010; Schraml, Sch¨on, and Milosevic 2007).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to studies and methods. No dataset names are present in the text.",
      "processing_time": 24.373368501663208,
      "citing_paper_id": "235306612",
      "cited_paper_id": 34855834
    },
    {
      "context_text": "Early pioneering studies have reported that event cameras can be used for depth estimation (Zhou et al. 2018; Tulyakov et al. 2019; Zhu, Chen, and Daniilidis 2018; Xie, Chen, and Orchard 2017; Dikov et al. 2017; Schraml and Belbachir 2010; Schraml, Sch¨on, and Milosevic 2007).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to studies and methods. No dataset names are present in the text.",
      "processing_time": 24.373368501663208,
      "citing_paper_id": "235306612",
      "cited_paper_id": 49877954
    },
    {
      "context_text": "Early pioneering studies have reported that event cameras can be used for depth estimation (Zhou et al. 2018; Tulyakov et al. 2019; Zhu, Chen, and Daniilidis 2018; Xie, Chen, and Orchard 2017; Dikov et al. 2017; Schraml and Belbachir 2010; Schraml, Sch¨on, and Milosevic 2007).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to studies and methods. No dataset names are present in the text.",
      "processing_time": 24.373368501663208,
      "citing_paper_id": "235306612",
      "cited_paper_id": 262638843
    },
    {
      "context_text": "(Dikov et al. 2017; Piatkowska, Belbachir, and Gelautz 2013; Firouzi and Conradt 2016) extended the cooperative stereo algorithm using iterative non-linear operations to extract disparities.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only algorithms and methods. The context focuses on extending a cooperative stereo algorithm, which is not a dataset.",
      "processing_time": 25.55950927734375,
      "citing_paper_id": "235306612",
      "cited_paper_id": 34855834
    },
    {
      "context_text": "Note that, for the stereo matching sub-network, we applied the existing model used for the event stereo matching framework (Tulyakov, Ivanov, and Fleuret 2018).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific dataset, only a model used for event stereo matching. No dataset names are provided in the context.",
      "processing_time": 23.944550275802612,
      "citing_paper_id": "235306612",
      "cited_paper_id": 46938951
    },
    {
      "context_text": "…and respective ground truth intensity image be I gt , then the reconstruction loss l R can be expressed as l R = l 1 ( I gt ; I recon ) + SSIM ( I gt ; I recon ) : For training the stereo matching sub-network, we adopt sub-pixel cross entropy loss, following (Tulyakov, Ivanov, and Fleuret 2018).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only loss functions and training methods. The cited paper title does not provide additional context about datasets.",
      "processing_time": 24.153671979904175,
      "citing_paper_id": "235306612",
      "cited_paper_id": 46938951
    },
    {
      "context_text": "(Zhou et al. 2018; Zhu, Chen, and Daniilidis 2018) explicitly used camera motion information to improve depth estimation.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only the use of camera motion information for depth estimation.",
      "processing_time": 22.55886197090149,
      "citing_paper_id": "235306612",
      "cited_paper_id": 49877954
    },
    {
      "context_text": "To generate the event images, the FCVF* and SGM* use temporal event aggregation for feature accumulation, as used in Semi-Dense 3D (Zhou et al. 2018).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions 'Semi-Dense 3D' which is a method, not a dataset. No specific dataset is mentioned or used in the context.",
      "processing_time": 25.08768582344055,
      "citing_paper_id": "235306612",
      "cited_paper_id": 49877954
    },
    {
      "context_text": "To generate the event images, the FCVF* and SGM* use temporal event aggregation for feature accumulation, as used in Semi-Dense 3D (Zhou et al. 2018).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions 'Semi-Dense 3D' which is a method, not a dataset. No specific dataset is mentioned or used in the context.",
      "processing_time": 25.08768582344055,
      "citing_paper_id": "235306612",
      "cited_paper_id": null
    },
    {
      "context_text": "Although the Semi-Dense 3D (Zhou et al. 2018) uses additional information such as known camera motion and works with continuous depth values, it fails to map the disparity in a dense manner due to relying on only temporal coherence among events via the forward-projection approach without any event…",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (Semi-Dense 3D) and its limitations. No verifiable datasets are referenced.",
      "processing_time": 26.27023482322693,
      "citing_paper_id": "235306612",
      "cited_paper_id": 49877954
    },
    {
      "context_text": "Note that the results for TSES (Zhu, Chen, and Daniilidis 2018), Semi-Dense 3D (Zhou et al. 2018), and DDES (Tulyakov et al. 2019) are borrowed from the original papers.",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to other papers' results. No clear, verifiable datasets are identified.",
      "processing_time": 25.09498953819275,
      "citing_paper_id": "235306612",
      "cited_paper_id": 49877954
    },
    {
      "context_text": "Note that the results for TSES (Zhu, Chen, and Daniilidis 2018), Semi-Dense 3D (Zhou et al. 2018), and DDES (Tulyakov et al. 2019) are borrowed from the original papers.",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to other papers' results. No clear, verifiable datasets are identified.",
      "processing_time": 25.09498953819275,
      "citing_paper_id": "235306612",
      "cited_paper_id": 262638843
    },
    {
      "context_text": "Note that the FCVF* and SGM* meth-ods are the frame-based methods but works on event images, implemented in (Zhou et al. 2018).",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only methods (FCVF* and SGM*) that work on event images. No clear, verifiable dataset names are provided.",
      "processing_time": 16.393226623535156,
      "citing_paper_id": "235306612",
      "cited_paper_id": 49877954
    },
    {
      "context_text": "Note that the FCVF* and SGM* meth-ods are the frame-based methods but works on event images, implemented in (Zhou et al. 2018).",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only methods (FCVF* and SGM*) that work on event images. No clear, verifiable dataset names are provided.",
      "processing_time": 16.393226623535156,
      "citing_paper_id": "235306612",
      "cited_paper_id": null
    },
    {
      "context_text": "Inspired by the recent studies (Kalia, Navab, and Salcudean 2019; Rebecq et al. 2019; Scheerlinck et al. 2020; VidalMata et al. 2019; Wang et al. 2019; Watkins et al. 2018), we explicitly reconstruct intensity images from the input event streams and use them as a guidance for event features.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only methods and approaches. The cited papers' titles also do not indicate the use of specific datasets.",
      "processing_time": 25.543707609176636,
      "citing_paper_id": "235306612",
      "cited_paper_id": 53725328
    },
    {
      "context_text": "Inspired by the recent studies (Kalia, Navab, and Salcudean 2019; Rebecq et al. 2019; Scheerlinck et al. 2020; VidalMata et al. 2019; Wang et al. 2019; Watkins et al. 2018), we explicitly reconstruct intensity images from the input event streams and use them as a guidance for event features.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only methods and approaches. The cited papers' titles also do not indicate the use of specific datasets.",
      "processing_time": 25.543707609176636,
      "citing_paper_id": "235306612",
      "cited_paper_id": 189998802
    },
    {
      "context_text": "The proposed aggregation sub-network is based on the spatially-adaptive denormalization (SPADE) method (Park et al. 2019) that modulates the existing feature using the conditional feature with learned scale and shift parameters.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (SPADE) used in the research.",
      "processing_time": 23.422496795654297,
      "citing_paper_id": "235306612",
      "cited_paper_id": 81981856
    },
    {
      "context_text": "The sub-network uses a stacked dilated SPatially-Adaptive DEnor-malization (stacked dilated SPADE) mechanism (Schus-ter et al. 2019; Park et al. 2019) that modulates the event features using the reconstructed image features.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and mechanisms. The context is focused on the technical details of the model architecture.",
      "processing_time": 25.075662851333618,
      "citing_paper_id": "235306612",
      "cited_paper_id": 81981856
    },
    {
      "context_text": "(Zou et al. 2017) produced disparity at every location based on interpolation while (Tulyakov et al. 2019; Gehrig et al. 2019; Chen et al. 2020) focused on learning-based event representations instead of hand-crafted or rule-based event accumulation for deep learning-based stereo matching.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only methods and approaches. The cited papers' titles do not provide additional dataset information.",
      "processing_time": 25.066527843475342,
      "citing_paper_id": "235306612",
      "cited_paper_id": 119120139
    },
    {
      "context_text": "(Zou et al. 2017) produced disparity at every location based on interpolation while (Tulyakov et al. 2019; Gehrig et al. 2019; Chen et al. 2020) focused on learning-based event representations instead of hand-crafted or rule-based event accumulation for deep learning-based stereo matching.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only methods and approaches. The cited papers' titles do not provide additional dataset information.",
      "processing_time": 25.066527843475342,
      "citing_paper_id": "235306612",
      "cited_paper_id": 262638843
    },
    {
      "context_text": "As shown in Figure 2, the image reconstruction sub-network consists of two separate branches, namely, a regular branch and an attention branch, in the encoder segment (Uddin and Jung 2020).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method or architecture. The context focuses on the structure of a neural network.",
      "processing_time": 25.082523584365845,
      "citing_paper_id": "235306612",
      "cited_paper_id": 219549150
    },
    {
      "context_text": "We do not use the split two due to the difference in dynamic characteristics in the training and testing events, as mentioned in (Tulyakov et al. 2019).",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific dataset names, only a general reference to splits in event data. No clear, verifiable dataset name is provided.",
      "processing_time": 24.85383439064026,
      "citing_paper_id": "235306612",
      "cited_paper_id": 262638843
    },
    {
      "context_text": "The event embedding sub-network contains a kernel network with continuous fully connected layers, following (Tulyakov et al. 2019), for left/right event-to-feature embedding.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for event-to-feature embedding. No dataset names are present in the citation context.",
      "processing_time": 25.336095571517944,
      "citing_paper_id": "235306612",
      "cited_paper_id": 262638843
    },
    {
      "context_text": "After obtaining the left and right aggregated features from the reconstructed image features and embedded event features, the aggregated features are fed to a stereo matching sub-network, following (Tulyakov et al. 2019).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method or model. The context focuses on the process of aggregating features and feeding them into a stereo matching sub-network.",
      "processing_time": 26.251526832580566,
      "citing_paper_id": "235306612",
      "cited_paper_id": 262638843
    },
    {
      "context_text": "We use the same evaluation protocol as (Tulyakov et al. 2019; Zhu, Chen, and Daniilidis 2018) and compute mean depth error , median depth error and mean disparity error for the sparse disparity ground truth and additional one-pixel accuracy (1 PA) for the dense disparity ground truth.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only evaluation metrics and protocols. No dataset names are provided in the context.",
      "processing_time": 24.847604513168335,
      "citing_paper_id": "235306612",
      "cited_paper_id": 262638843
    },
    {
      "context_text": "We construct our proposed architecture by adopting the stereo framework of (Tulyakov et al. 2019) as the base-line with several major architectural modiﬁcations.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a baseline stereo framework from a cited paper.",
      "processing_time": 23.59753394126892,
      "citing_paper_id": "235306612",
      "cited_paper_id": 262638843
    },
    {
      "context_text": "The DDES (Tulyakov et al. 2019) works well for the dense disparity estimation due to the learning-based event accumulation with a kernel network and explicit stereo matching framework.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions DDES, which is a method for dense disparity estimation using event-based data. No specific dataset is mentioned.",
      "processing_time": 25.062509536743164,
      "citing_paper_id": "235306612",
      "cited_paper_id": 262638843
    },
    {
      "context_text": "Note that the baseline method is DDES (Tulyakov et al. 2019). racy.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a baseline method (DDES).",
      "processing_time": 23.575570106506348,
      "citing_paper_id": "235306612",
      "cited_paper_id": 262638843
    },
    {
      "context_text": "(Xie, Zhang, and Wang 2018) used a traditional semi-global matching approach (Hirschmuller 2007).",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (semi-global matching).",
      "processing_time": 22.70365023612976,
      "citing_paper_id": "235306612",
      "cited_paper_id": null
    },
    {
      "context_text": "However, the Semi-Dense 3D relies only on temporal information of the events and fails to preserve spatial information in events which leads to poor results in the FCVF* and SGM*.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only discusses limitations of a method.",
      "processing_time": 22.52668261528015,
      "citing_paper_id": "235306612",
      "cited_paper_id": null
    },
    {
      "context_text": "In the work of Tang [6] an example of feature-based matching is shown where extracted feature points are connected to chains and further used for the matching step.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for feature-based matching in stereo images.",
      "processing_time": 23.400158882141113,
      "citing_paper_id": "11177597",
      "cited_paper_id": 1016708
    },
    {
      "context_text": "In 1988, Mead and Mahowald [9] developed an electronic silicon model which reproduced the basic steps of human visual processing.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a silicon model of early visual processing.",
      "processing_time": 23.000064611434937,
      "citing_paper_id": "11177597",
      "cited_paper_id": 8280907
    },
    {
      "context_text": "[7] have evaluated several area-based costs functions for grayscale images produced with a silicon retina stereo system (aggregation of events over time), including Normalized Cross-Correlation (NCC), Normalized Sum of Absolute Differences (NSAD), Sum of Squared Differences (SSD) and Census-Transform.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and evaluation of cost functions for grayscale images from a silicon retina stereo system.",
      "processing_time": 25.74649930000305,
      "citing_paper_id": "11177597",
      "cited_paper_id": 30913835
    },
    {
      "context_text": "[4].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not provide any specific dataset names or details about the usage of datasets. The title suggests a methodological focus rather than a specific dataset.",
      "processing_time": 25.744498014450073,
      "citing_paper_id": "11177597",
      "cited_paper_id": 62592856
    },
    {
      "context_text": "The ability to process the stereo correspondence problem in real time, which solves the input data without storing it for later processing, is crucial in machine-vision systems such as robots and autonomous vehicles [1].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only discusses the importance of real-time processing in stereo matching for machine vision systems.",
      "processing_time": 25.735791206359863,
      "citing_paper_id": "254531210",
      "cited_paper_id": 458430
    },
    {
      "context_text": "The disparity maps were computed using frame-based approaches [1], [9].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only frame-based approaches for computing disparity maps. No verifiable resources are identified.",
      "processing_time": 25.970380306243896,
      "citing_paper_id": "254531210",
      "cited_paper_id": 458430
    },
    {
      "context_text": "The disparity maps were computed using frame-based approaches [1], [9].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only frame-based approaches for computing disparity maps. No verifiable resources are identified.",
      "processing_time": 25.970380306243896,
      "citing_paper_id": "254531210",
      "cited_paper_id": 1680724
    },
    {
      "context_text": "In [14], a time-correlated kernel was used for event-driven operations, but this method requires recalculating the synaptic weights for each event input, which leads to excessive computation.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method involving time-correlated kernels and neuromorphic silicon retinas.",
      "processing_time": 24.613102674484253,
      "citing_paper_id": "254531210",
      "cited_paper_id": 4833834
    },
    {
      "context_text": "In frame-based stereo systems, most stereo correspondence algorithms ﬁnd a matching point by comparing the similarity between the visual features of the left and right frame images and calculate the distance along the camera geometry through the disparity between the two matching points [2].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general description of stereo vision algorithms. No verifiable resources are identified.",
      "processing_time": 25.737945795059204,
      "citing_paper_id": "254531210",
      "cited_paper_id": 8688550
    },
    {
      "context_text": "A similar idea was adopted for event-based stereo systems in previous studies [3], [4], [5], [6].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only references to previous studies. The cited paper titles do not provide additional information about datasets.",
      "processing_time": 26.46444320678711,
      "citing_paper_id": "254531210",
      "cited_paper_id": 11395394
    },
    {
      "context_text": "A similar idea was adopted for event-based stereo systems in previous studies [3], [4], [5], [6].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only references to previous studies. The cited paper titles do not provide additional information about datasets.",
      "processing_time": 26.46444320678711,
      "citing_paper_id": "254531210",
      "cited_paper_id": 30913835
    },
    {
      "context_text": "The methods reported in [5] and [6] do not generate frame images to compute the matching cost but compare each event with the collected events of another DVS pair within a speciﬁed spatiotemporal window.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and algorithms. The context focuses on the methodology of comparing events from DVS pairs without generating frame images.",
      "processing_time": 15.97440505027771,
      "citing_paper_id": "254531210",
      "cited_paper_id": 11395394
    },
    {
      "context_text": "Thus, stereo systems utilizing DVSs have recently gained popularity and have been reported in the literature [3], [4], [5], [6], [7], [8].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to literature. No dataset names are present in the context.",
      "processing_time": 25.723949909210205,
      "citing_paper_id": "254531210",
      "cited_paper_id": 11395394
    },
    {
      "context_text": "Thus, stereo systems utilizing DVSs have recently gained popularity and have been reported in the literature [3], [4], [5], [6], [7], [8].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to literature. No dataset names are present in the context.",
      "processing_time": 25.723949909210205,
      "citing_paper_id": "254531210",
      "cited_paper_id": 30913835
    },
    {
      "context_text": "Thus, stereo systems utilizing DVSs have recently gained popularity and have been reported in the literature [3], [4], [5], [6], [7], [8].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to literature. No dataset names are present in the context.",
      "processing_time": 25.723949909210205,
      "citing_paper_id": "254531210",
      "cited_paper_id": 205698386
    },
    {
      "context_text": "Compared with other ASIC approaches [3], [17], our FPGA-based implementation achieved considerable accuracy and power performance with lesser hardware.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a comparison with other ASIC approaches. The context is about hardware performance, not dataset usage.",
      "processing_time": 26.458176612854004,
      "citing_paper_id": "254531210",
      "cited_paper_id": 30913835
    },
    {
      "context_text": "In [3], grayscale images were generated by collecting spike events over 20 ms.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific dataset by name, only a method for generating grayscale images from spike events.",
      "processing_time": 25.300236701965332,
      "citing_paper_id": "254531210",
      "cited_paper_id": 30913835
    },
    {
      "context_text": "Since the disparity data generated from SCN is frameless, the latency was estimated using the amount of SCN spikes and rate of change of edges as proposed in [7].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific dataset by name. It discusses a method for estimating latency in a spiking neural network model for event-based stereo vision, which is not a dataset.",
      "processing_time": 27.50049924850464,
      "citing_paper_id": "254531210",
      "cited_paper_id": 205698386
    },
    {
      "context_text": "In most previous studies, a processor was used for SCN design [7], [8], [11], [17].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to previous studies using processors for SCN design. No verifiable resources are identified.",
      "processing_time": 26.452494144439697,
      "citing_paper_id": "254531210",
      "cited_paper_id": 205698386
    },
    {
      "context_text": "In [7], another method was proposed for constructing the network, which separates the SCN into two layers as follows : When a coincidence neuron is activated, it excites the disparity-layer neurons according to the continuity constraint.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for constructing a network. No verifiable resources are identified.",
      "processing_time": 25.48739266395569,
      "citing_paper_id": "254531210",
      "cited_paper_id": 205698386
    },
    {
      "context_text": "In [7] and [8], a cooperative stereo algorithm employing a spiking-neuron model was presented.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method or model. The context focuses on a cooperative stereo algorithm using a spiking-neuron model.",
      "processing_time": 26.925389528274536,
      "citing_paper_id": "254531210",
      "cited_paper_id": 205698386
    },
    {
      "context_text": "Neu-romorphic computing systems utilizing ﬁeld-programmable analog array or ﬁeld-programmable gate array (FPGA) devices have been successfully demonstrated the advantages of asynchronous computations [26], [27], [28].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only hardware and computational methods. There are no clear identifiers for datasets in the text.",
      "processing_time": 26.330727338790894,
      "citing_paper_id": "254531210",
      "cited_paper_id": 232152677
    },
    {
      "context_text": "To avoid rendering entire frames, foveation has also been used in AR/VR headsets [11]",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a technique (foveation) used in AR/VR headsets.",
      "processing_time": 26.20834231376648,
      "citing_paper_id": "239050401",
      "cited_paper_id": 4326416
    },
    {
      "context_text": "non-idealities: encompassing parasitic photocurrents and junction leakages, these effects bias event generation to one specific polarity [13].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only non-idealities in sensors. No dataset names are present in the citation context.",
      "processing_time": 25.70942187309265,
      "citing_paper_id": "239050401",
      "cited_paper_id": 41474355
    },
    {
      "context_text": "In [3], they combined a laser light source with a stereo setup consisting of two DAVIS240 cameras [7].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions a stereo setup with DAVIS240 cameras, which are event-based sensors. However, no specific dataset is mentioned.",
      "processing_time": 25.953378200531006,
      "citing_paper_id": "239050401",
      "cited_paper_id": 53086261
    },
    {
      "context_text": "MC3D then converted temporal information of events at each pixel into disparity.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (MC3D) and its functionality. No verifiable resources are identified.",
      "processing_time": 26.56684923171997,
      "citing_paper_id": "239050401",
      "cited_paper_id": 206767633
    },
    {
      "context_text": "The SL system MC3D [2] comprised of a laser point projector and a DVS.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions a system (MC3D) but does not refer to a specific dataset. The context is about the components of the system, not a dataset.",
      "processing_time": 27.14474081993103,
      "citing_paper_id": "239050401",
      "cited_paper_id": 206767633
    },
    {
      "context_text": "Logarithmic depth is computed similar to [26] as:",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for computing logarithmic depth.",
      "processing_time": 23.573993921279907,
      "citing_paper_id": "239050401",
      "cited_paper_id": 231951439
    },
    {
      "context_text": "Previous approaches estimated dense depth from monocular events [25] or combination of events and frames [26].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only previous methods and approaches.",
      "processing_time": 23.169368743896484,
      "citing_paper_id": "239050401",
      "cited_paper_id": 231951439
    },
    {
      "context_text": "Network Architecture The general network architecture is inspired from U-Net [25, 26], which was used previously used for monocular depth estimation with events and combining events with images.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a network architecture inspired by U-Net. No verifiable datasets are referenced.",
      "processing_time": 26.43478035926819,
      "citing_paper_id": "239050401",
      "cited_paper_id": 231951439
    },
    {
      "context_text": "Similar to [26] our architecture uses skip connections and residual block.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only architectural components. No dataset names are present in the citation span.",
      "processing_time": 25.94310975074768,
      "citing_paper_id": "239050401",
      "cited_paper_id": 231951439
    },
    {
      "context_text": "On the MVSEC Dataset In order to be able to compare our results with the other approaches in the literature, we also train and evaluate ALED on the MVSEC dataset [43].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "MVSEC"
      ],
      "dataset_descriptions": {
        "MVSEC": "Used to train and evaluate ALED for event-based stereo depth estimation, providing a benchmark for comparison with other approaches in the literature."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions the MVSEC dataset, which is a specific, verifiable dataset used for training and evaluation in the context of event-based stereo depth estimation.",
      "processing_time": 30.740906953811646,
      "citing_paper_id": "257232560",
      "cited_paper_id": 3416874
    },
    {
      "context_text": "For an easier learning, the LiDAR projection and ground truth images are normalized between 0 and 1 based on the maximum LiDAR range (200m in the case of our synthetic SLED dataset, 100m in the case of the MVSEC dataset [43]).",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "MVSEC dataset"
      ],
      "dataset_descriptions": {
        "MVSEC dataset": "Used to normalize ground truth images for event-based stereo depth estimation, focusing on 3D perception with a maximum LiDAR range of 100m."
      },
      "confidence_score": 0.9,
      "reasoning": "The context mentions two datasets: 'synthetic SLED dataset' and 'MVSEC dataset'. The MVSEC dataset is referenced with a citation, indicating it is a verifiable resource.",
      "processing_time": 32.51852202415466,
      "citing_paper_id": "257232560",
      "cited_paper_id": 3416874
    },
    {
      "context_text": "While we can use the MVSEC dataset [43] for low-resolution cameras, its ground truth is constructed by accumulating point clouds from a LiDAR sensor, a solution which introduces errors in case of moving objects.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "MVSEC"
      ],
      "dataset_descriptions": {
        "MVSEC": "Used for low-resolution camera testing, but its ground truth construction using LiDAR introduces errors for moving objects, affecting depth estimation accuracy."
      },
      "confidence_score": 0.9,
      "reasoning": "The MVSEC dataset is mentioned as a resource used for low-resolution cameras, but with limitations regarding ground truth accuracy for moving objects.",
      "processing_time": 30.300512075424194,
      "citing_paper_id": "257232560",
      "cited_paper_id": 3416874
    },
    {
      "context_text": "These methods either use machine learning [4,14,39] or traditional image processing operations [19].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only methods and approaches. The cited papers' titles do not provide additional information about datasets.",
      "processing_time": 26.777976751327515,
      "citing_paper_id": "257232560",
      "cited_paper_id": 3615240
    },
    {
      "context_text": "These methods either use machine learning [4,14,39] or traditional image processing operations [19].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only methods and approaches. The cited papers' titles do not provide additional information about datasets.",
      "processing_time": 26.777976751327515,
      "citing_paper_id": "257232560",
      "cited_paper_id": 52096397
    },
    {
      "context_text": "These methods either use machine learning [4,14,39] or traditional image processing operations [19].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only methods and approaches. The cited papers' titles do not provide additional information about datasets.",
      "processing_time": 26.777976751327515,
      "citing_paper_id": "257232560",
      "cited_paper_id": 206429195
    },
    {
      "context_text": "For these reasons, we use the CARLA simulator [6] (version 0.9.14) to generate a dataset with perfect synchronization and calibration of the sensors, and perfect ground truth depth.",
      "catation_intent": "reusable resource",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions generating a dataset using the CARLA simulator, but does not name the dataset itself. The CARLA simulator is a tool, not a dataset.",
      "processing_time": 26.894017219543457,
      "citing_paper_id": "257232560",
      "cited_paper_id": 5550767
    },
    {
      "context_text": "On the SLED Dataset For training on the SLED dataset, we use the Adam optimizer [18] with a learning rate of 10 − 4 and a batch size of 4, and train for a total of 50 epochs.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "SLED"
      ],
      "dataset_descriptions": {
        "SLED": "Used for training a model with the Adam optimizer, focusing on event-based stereo depth estimation with specific hyperparameters and training duration."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions the SLED dataset, which is used for training a model with specific parameters. The dataset is clearly identified and used in the research.",
      "processing_time": 31.080777406692505,
      "citing_paper_id": "257232560",
      "cited_paper_id": 6628106
    },
    {
      "context_text": "These works include frame interpolation and deblurring [26,27,31], feature tracking [8,20], object detection [3,16,38], or even steering prediction [13].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only various applications and methods. The cited papers' titles do not provide additional dataset information.",
      "processing_time": 15.105671644210815,
      "citing_paper_id": "257232560",
      "cited_paper_id": 7884141
    },
    {
      "context_text": "These works include frame interpolation and deblurring [26,27,31], feature tracking [8,20], object detection [3,16,38], or even steering prediction [13].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only various applications and methods. The cited papers' titles do not provide additional dataset information.",
      "processing_time": 15.105671644210815,
      "citing_paper_id": "257232560",
      "cited_paper_id": 53749928
    },
    {
      "context_text": "These works include frame interpolation and deblurring [26,27,31], feature tracking [8,20], object detection [3,16,38], or even steering prediction [13].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only various applications and methods. The cited papers' titles do not provide additional dataset information.",
      "processing_time": 15.105671644210815,
      "citing_paper_id": "257232560",
      "cited_paper_id": 88486714
    },
    {
      "context_text": "These works include frame interpolation and deblurring [26,27,31], feature tracking [8,20], object detection [3,16,38], or even steering prediction [13].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only various applications and methods. The cited papers' titles do not provide additional dataset information.",
      "processing_time": 15.105671644210815,
      "citing_paper_id": "257232560",
      "cited_paper_id": 218673616
    },
    {
      "context_text": "These works include frame interpolation and deblurring [26,27,31], feature tracking [8,20], object detection [3,16,38], or even steering prediction [13].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only various applications and methods. The cited papers' titles do not provide additional dataset information.",
      "processing_time": 15.105671644210815,
      "citing_paper_id": "257232560",
      "cited_paper_id": 240156806
    },
    {
      "context_text": "Weikersdorfer et al. [41] used an RGB-D camera to obtain dense depths, and used the depth-augmented events to perform SLAM.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions the use of an RGB-D camera to obtain dense depths and perform SLAM, but does not specify a named dataset. The cited paper title confirms the focus on event-based 3D SLAM but does not mention a specific dataset.",
      "processing_time": 29.210761785507202,
      "citing_paper_id": "257232560",
      "cited_paper_id": 11191105
    },
    {
      "context_text": "Convolutions are followed by a PReLU activation function [10], and instance normalization is used in the ResNet encoders as proposed by Pan et al.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and architectures. The context is about the use of PReLU activation functions and instance normalization in ResNet encoders.",
      "processing_time": 27.57571792602539,
      "citing_paper_id": "257232560",
      "cited_paper_id": 13740328
    },
    {
      "context_text": "While [32,33] used traditional model-based approaches, [25] entirely relied on learning-based networks: an attention-based network to construct detailed events representations, then a convolutional network for depth map inference.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only model-based and learning-based approaches. No verifiable resources are identified.",
      "processing_time": 26.541994333267212,
      "citing_paper_id": "257232560",
      "cited_paper_id": 21539113
    },
    {
      "context_text": "While [32,33] used traditional model-based approaches, [25] entirely relied on learning-based networks: an attention-based network to construct detailed events representations, then a convolutional network for depth map inference.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only model-based and learning-based approaches. No verifiable resources are identified.",
      "processing_time": 26.541994333267212,
      "citing_paper_id": "257232560",
      "cited_paper_id": 24236495
    },
    {
      "context_text": "While [32,33] used traditional model-based approaches, [25] entirely relied on learning-based networks: an attention-based network to construct detailed events representations, then a convolutional network for depth map inference.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only model-based and learning-based approaches. No verifiable resources are identified.",
      "processing_time": 26.541994333267212,
      "citing_paper_id": "257232560",
      "cited_paper_id": 249980412
    },
    {
      "context_text": "For a fully calibrated and synced setup, this process would allow for the superimposition of events and RGB images, a task which is only possible at the moment through the use of specific low-resolution frame+events cameras like the DAVIS240C [2].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions a specific camera model, DAVIS240C, which is used for capturing low-resolution frame+events data. However, it does not refer to a dataset but rather a device.",
      "processing_time": 27.81269407272339,
      "citing_paper_id": "257232560",
      "cited_paper_id": 24007071
    },
    {
      "context_text": "For a fully calibrated and synced setup, this process would allow for the superimposition of events and RGB images, a task which is only possible at the moment through the use of speciﬁc low-resolution frame+events cameras like the DAVIS240C [2].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions a specific camera model, DAVIS240C, which is used for capturing low-resolution frame+events data. However, it does not refer to a dataset but rather a device.",
      "processing_time": 27.810859441757202,
      "citing_paper_id": "257232560",
      "cited_paper_id": 24007071
    },
    {
      "context_text": "Most of the investigations focused on the fusion of events and frames, thanks to sensors offering both modalities like the DAVIS camera [2].",
      "catation_intent": "reusable resource",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions the DAVIS camera, which is a sensor providing both event-based and frame-based data, but does not explicitly mention a dataset. The citation is more about the technology used rather than a specific dataset.",
      "processing_time": 28.35359025001526,
      "citing_paper_id": "257232560",
      "cited_paper_id": 24007071
    },
    {
      "context_text": "Convolutions are followed by a PReLU activation function [10], and instance normalization is used in the ResNet encoders as proposed by Pan et al. [28].",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and architectures.",
      "processing_time": 24.264211893081665,
      "citing_paper_id": "257232560",
      "cited_paper_id": 50781790
    },
    {
      "context_text": "While most of these approaches employ a RGB camera as the secondary sensor [7,14,15,42], other authors have proposed using alternative modalities, such as stereo cameras [23] or more recently event cameras [5].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only different types of sensors used in various approaches.",
      "processing_time": 24.76788830757141,
      "citing_paper_id": "257232560",
      "cited_paper_id": 52096397
    },
    {
      "context_text": "Explored issues include calibration [35,36], and very recently, point clouds enhancement with events [21] and LiDAR densiﬁcation [5].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only references to papers discussing calibration and point cloud enhancement. No verifiable datasets are identified.",
      "processing_time": 26.88266611099243,
      "citing_paper_id": "257232560",
      "cited_paper_id": 56718925
    },
    {
      "context_text": "Convolutional encoders (in the form of ResNet Basic Blocks [11]) are then used to compute feature maps at scales 1 / 2 , 1 / 4 , and 1 / 8 , doubling the number of channels every time.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a method (ResNet Basic Blocks).",
      "processing_time": 25.91415762901306,
      "citing_paper_id": "257232560",
      "cited_paper_id": 206594692
    },
    {
      "context_text": "This could be achieved by using sparse convolutional networks [24] for instance, and could be subject to future work.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (sparse convolutional networks).",
      "processing_time": 25.896594524383545,
      "citing_paper_id": "257232560",
      "cited_paper_id": 214605597
    },
    {
      "context_text": "Then, for each following scale, the decoded feature map from the previous scale is upscaled by using convex upsampling [37].",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for upsampling feature maps.",
      "processing_time": 24.988991260528564,
      "citing_paper_id": "257232560",
      "cited_paper_id": 214667893
    },
    {
      "context_text": "We follow the formulation of Perot et al. [29], where the Discretized Event Volume V for input events { e i = ( x i , y i , p i , t i ) } Ni =1 is described as: t ∗ i = ( B − 1) where ( x, y ) is the position of the event, t its timestamp, and p its polarity.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for processing event data. No dataset names are present in the citation span.",
      "processing_time": 26.63833737373352,
      "citing_paper_id": "257232560",
      "cited_paper_id": 221970052
    },
    {
      "context_text": "A solution could be to use an alternative representation, such as TORE Volumes [1], or to use adaptive accumulation times using methods such as the one proposed by Liu and Delbrück [22].",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and representations. TORE Volumes is a method for event cameras, not a dataset.",
      "processing_time": 27.444475650787354,
      "citing_paper_id": "257232560",
      "cited_paper_id": 232170422
    },
    {
      "context_text": "Cutoﬀ Event-based Event-and frame-based LiDAR-and event-based Zhu et al. [44] E2Depth [12] RAMNet [9] EvT Qualitative results are presented in Fig.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only models and methods. There are no clear identifiers for datasets.",
      "processing_time": 26.874738931655884,
      "citing_paper_id": "257232560",
      "cited_paper_id": 248085042
    },
    {
      "context_text": "Compared to the frames+events EvT + method of Sabater et al. [30], we improve the error by {− 2 .",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a comparison to a method from another paper.",
      "processing_time": 47.45448589324951,
      "citing_paper_id": "257232560",
      "cited_paper_id": 248085042
    },
    {
      "context_text": "[44] E2Depth [12] RAMNet [9] EvT [30] Cui et al.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only models and methods. There are no clear identifiers for datasets.",
      "processing_time": 48.34659194946289,
      "citing_paper_id": "257232560",
      "cited_paper_id": 248085042
    },
    {
      "context_text": "Schraml et al. [32,33] and Nam et al. [25] used two event cameras, and estimated depths by creating images of accumulated events for each camera and applying stereo matching.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions the use of event cameras for depth estimation but does not specify any named datasets. The focus is on the methodology using event cameras.",
      "processing_time": 49.58829927444458,
      "citing_paper_id": "257232560",
      "cited_paper_id": 249980412
    },
    {
      "context_text": "Stereo event-frame multimodal datasets [12], [13], [14], [15] improve 3D dynamic scene motion perception using the distance sensors.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Multivehicle Stereo Event Camera Dataset",
        "M3ED"
      ],
      "dataset_descriptions": {
        "Multivehicle Stereo Event Camera Dataset": "Used to enhance 3D dynamic scene motion perception with event cameras, focusing on distance sensor integration and multi-vehicle scenarios.",
        "M3ED": "Utilized for improving 3D perception in dynamic scenes using multi-robot, multi-sensor data, focusing on event-based stereo depth estimation."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions 'Stereo event-frame multimodal datasets' and cites specific papers. Based on the titles, two datasets are clearly identified and relevant to the topic of event-based stereo depth estimation.",
      "processing_time": 57.69958972930908,
      "citing_paper_id": "271892156",
      "cited_paper_id": 3416874
    },
    {
      "context_text": "Stereo event-frame multimodal datasets [12], [13], [14], [15] improve 3D dynamic scene motion perception using the distance sensors.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Multivehicle Stereo Event Camera Dataset",
        "M3ED"
      ],
      "dataset_descriptions": {
        "Multivehicle Stereo Event Camera Dataset": "Used to enhance 3D dynamic scene motion perception with event cameras, focusing on distance sensor integration and multi-vehicle scenarios.",
        "M3ED": "Utilized for improving 3D perception in dynamic scenes using multi-robot, multi-sensor data, focusing on event-based stereo depth estimation."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions 'Stereo event-frame multimodal datasets' and cites specific papers. Based on the titles, two datasets are clearly identified and relevant to the topic of event-based stereo depth estimation.",
      "processing_time": 57.69958972930908,
      "citing_paper_id": "271892156",
      "cited_paper_id": 259380779
    },
    {
      "context_text": "Stereo event-frame multimodal datasets [12], [13], [14], [15] further introduce LiDAR and stereo cameras to improve 3D dynamic scene perception.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Multivehicle Stereo Event Camera Dataset",
        "M3ED"
      ],
      "dataset_descriptions": {
        "Multivehicle Stereo Event Camera Dataset": "Used to enhance 3D dynamic scene perception with event cameras, focusing on stereo vision and LiDAR integration.",
        "M3ED": "Utilized for multi-robot, multi-sensor, and multi-modal data fusion, specifically to improve 3D perception in dynamic environments."
      },
      "confidence_score": 0.9,
      "reasoning": "The context mentions 'Stereo event-frame multimodal datasets' and cites specific papers. Based on the titles, two datasets are identified: 'Multivehicle Stereo Event Camera Dataset' and 'M3ED'. Both are relevant to event-based stereo depth estimation.",
      "processing_time": 57.8403217792511,
      "citing_paper_id": "271892156",
      "cited_paper_id": 3416874
    },
    {
      "context_text": "Stereo event-frame multimodal datasets [12], [13], [14], [15] further introduce LiDAR and stereo cameras to improve 3D dynamic scene perception.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Multivehicle Stereo Event Camera Dataset",
        "M3ED"
      ],
      "dataset_descriptions": {
        "Multivehicle Stereo Event Camera Dataset": "Used to enhance 3D dynamic scene perception with event cameras, focusing on stereo vision and LiDAR integration.",
        "M3ED": "Utilized for multi-robot, multi-sensor, and multi-modal data fusion, specifically to improve 3D perception in dynamic environments."
      },
      "confidence_score": 0.9,
      "reasoning": "The context mentions 'Stereo event-frame multimodal datasets' and cites specific papers. Based on the titles, two datasets are identified: 'Multivehicle Stereo Event Camera Dataset' and 'M3ED'. Both are relevant to event-based stereo depth estimation.",
      "processing_time": 57.8403217792511,
      "citing_paper_id": "271892156",
      "cited_paper_id": 259380779
    },
    {
      "context_text": "For example, the MVSEC [12] and FusionPortableV2 [14] datasets introduced the LiDAR and stereo DAVIS event camera [20], [21], [22] to fuse the multimodal knowledge for 3D perception, such as depth estimation and SLAM.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "MVSEC",
        "FusionPortableV2"
      ],
      "dataset_descriptions": {
        "MVSEC": "Used to introduce LiDAR and stereo DAVIS event camera for 3D perception tasks, specifically depth estimation and SLAM.",
        "FusionPortableV2": "Used to introduce LiDAR and stereo DAVIS event camera for 3D perception tasks, specifically depth estimation and SLAM."
      },
      "confidence_score": 1.0,
      "reasoning": "The citation mentions two specific datasets, MVSEC and FusionPortableV2, which are used for 3D perception tasks including depth estimation and SLAM.",
      "processing_time": 56.77420401573181,
      "citing_paper_id": "271892156",
      "cited_paper_id": 3416874
    },
    {
      "context_text": "For example, N-CARS [5] and ADD [6] datasets took Gen1 event camera with 304 × 240 resolution to classify and detect objects.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "N-CARS",
        "ADD"
      ],
      "dataset_descriptions": {
        "N-CARS": "Used for object classification and detection with Gen1 event cameras, focusing on robust event-based object classification at 304 × 240 resolution.",
        "ADD": "Used for automotive detection with Gen1 event cameras, focusing on large-scale event-based detection at 304 × 240 resolution."
      },
      "confidence_score": 0.9,
      "reasoning": "The context mentions two specific datasets, N-CARS and ADD, which are used for object classification and detection with event cameras. These datasets are directly relevant to the topic of event-based stereo depth estimation.",
      "processing_time": 57.254249811172485,
      "citing_paper_id": "271892156",
      "cited_paper_id": 3993392
    },
    {
      "context_text": "For example, N-CARS [5] and ADD [6] datasets took Gen1 event camera with 304 × 240 resolution to classify and detect objects.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "N-CARS",
        "ADD"
      ],
      "dataset_descriptions": {
        "N-CARS": "Used for object classification and detection with Gen1 event cameras, focusing on robust event-based object classification at 304 × 240 resolution.",
        "ADD": "Used for automotive detection with Gen1 event cameras, focusing on large-scale event-based detection at 304 × 240 resolution."
      },
      "confidence_score": 0.9,
      "reasoning": "The context mentions two specific datasets, N-CARS and ADD, which are used for object classification and detection with event cameras. These datasets are directly relevant to the topic of event-based stereo depth estimation.",
      "processing_time": 57.254249811172485,
      "citing_paper_id": "271892156",
      "cited_paper_id": 210860813
    },
    {
      "context_text": "As shown in Table I, event-only unimodal datasets [5], [6], [7], [8] only focus on relatively simple vision tasks, while being difficult to model ego-motion and 3D dynamic scene.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.2,
      "reasoning": "The context mentions 'event-only unimodal datasets' but does not provide specific names. The cited paper titles do not clarify specific datasets.",
      "processing_time": 49.50813341140747,
      "citing_paper_id": "271892156",
      "cited_paper_id": 3993392
    },
    {
      "context_text": "As shown in Table I, event-only unimodal datasets [5], [6], [7], [8] only focus on relatively simple vision tasks, while being difficult to model ego-motion and 3D dynamic scene.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.2,
      "reasoning": "The context mentions 'event-only unimodal datasets' but does not provide specific names. The cited paper titles do not clarify specific datasets.",
      "processing_time": 49.50813341140747,
      "citing_paper_id": "271892156",
      "cited_paper_id": 210860813
    },
    {
      "context_text": "Event-only unimodal datasets [5], [6], [7], [8] just utilize the event camera to perform relatively simple vision tasks in autonomous driving.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.2,
      "reasoning": "The context mentions 'event-only unimodal datasets' but does not provide specific names. The cited papers do not clarify specific dataset names either.",
      "processing_time": 49.27648115158081,
      "citing_paper_id": "271892156",
      "cited_paper_id": 3993392
    },
    {
      "context_text": "Event-only unimodal datasets [5], [6], [7], [8] just utilize the event camera to perform relatively simple vision tasks in autonomous driving.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.2,
      "reasoning": "The context mentions 'event-only unimodal datasets' but does not provide specific names. The cited papers do not clarify specific dataset names either.",
      "processing_time": 49.27648115158081,
      "citing_paper_id": "271892156",
      "cited_paper_id": 210860813
    },
    {
      "context_text": "C V ] 16 A ug 2024 Event-onlyUnimodal N-CARS [5] 304 × 240 Classification ADD [6] 304 × 240 Detection 1MP Detection [7] 1280 × 720 Detection DET [8] 1280 × camera and event camera in parallel, and spatially align them by warping operation [16].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "N-CARS",
        "ADD",
        "1MP Detection",
        "DET"
      ],
      "dataset_descriptions": {
        "N-CARS": "Used for event-based object classification, focusing on 304x240 resolution events to evaluate unimodal performance.",
        "ADD": "Used for event-based object detection, focusing on 304x240 resolution events to evaluate detection accuracy.",
        "1MP": "Used for event-based object detection, focusing on 1280x720 resolution events to evaluate high-resolution detection performance.",
        "DET": "Used for event-based object detection, focusing on 1280x720 resolution events to evaluate detection accuracy."
      },
      "confidence_score": 0.9,
      "reasoning": "The context mentions several datasets by name, which are likely used for event-based stereo depth estimation or related tasks. The datasets are referenced in the context of detection and classification tasks.",
      "processing_time": 59.28275227546692,
      "citing_paper_id": "271892156",
      "cited_paper_id": 3993392
    },
    {
      "context_text": "C V ] 16 A ug 2024 Event-onlyUnimodal N-CARS [5] 304 × 240 Classification ADD [6] 304 × 240 Detection 1MP Detection [7] 1280 × 720 Detection DET [8] 1280 × camera and event camera in parallel, and spatially align them by warping operation [16].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "N-CARS",
        "ADD",
        "1MP Detection",
        "DET"
      ],
      "dataset_descriptions": {
        "N-CARS": "Used for event-based object classification, focusing on 304x240 resolution events to evaluate unimodal performance.",
        "ADD": "Used for event-based object detection, focusing on 304x240 resolution events to evaluate detection accuracy.",
        "1MP": "Used for event-based object detection, focusing on 1280x720 resolution events to evaluate high-resolution detection performance.",
        "DET": "Used for event-based object detection, focusing on 1280x720 resolution events to evaluate detection accuracy."
      },
      "confidence_score": 0.9,
      "reasoning": "The context mentions several datasets by name, which are likely used for event-based stereo depth estimation or related tasks. The datasets are referenced in the context of detection and classification tasks.",
      "processing_time": 59.28275227546692,
      "citing_paper_id": "271892156",
      "cited_paper_id": 210860813
    },
    {
      "context_text": "Then, considering that event data cannot be directly used for the standard calibration, we reconstruct the event stream into the event frame via E2VID [23], [24].",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific dataset names, only a method (E2VID) for reconstructing event streams into event frames. No datasets are explicitly referenced.",
      "processing_time": 50.20158052444458,
      "citing_paper_id": "271892156",
      "cited_paper_id": 115151433
    },
    {
      "context_text": "Event camera [4] has garnered increasing attention due to its advantages of high dynamic range and high temporal resolution.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only the technology of event cameras. No dataset names are present in the citation span.",
      "processing_time": 49.13301730155945,
      "citing_paper_id": "271892156",
      "cited_paper_id": 118684904
    },
    {
      "context_text": "In contrast, the event camera [4] with the advantages of high dynamic range and high temporal resolution has been applied in assisting frame camera for multimodal fusion, which relies heavily on the pixel-level spatial alignment between various modal data.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only discusses the use of event cameras in multimodal fusion.",
      "processing_time": 48.12978196144104,
      "citing_paper_id": "271892156",
      "cited_paper_id": 118684904
    },
    {
      "context_text": "Event-frame multimodal datasets [9], [10], [11] integrate frame camera and sensors of vehicle control ( e.g. , IMU ) to fuse cross-modal complementary knowledge for motion perception in autonomous driving.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.3,
      "reasoning": "The context mentions 'Event-frame multimodal datasets' but does not provide specific names. It describes the integration of frame cameras and sensors for autonomous driving, which is relevant to event-based stereo depth estimation.",
      "processing_time": 51.48396611213684,
      "citing_paper_id": "271892156",
      "cited_paper_id": 221808419
    },
    {
      "context_text": "Event-frame multimodal datasets [9], [10], [11] introduce IMU to model the ego-motion of vehicles, but still fail for 3D scene understanding.",
      "catation_intent": "limitation",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions 'Event-frame multimodal datasets' but does not specify any particular dataset names. The citation is used to highlight a limitation in existing datasets rather than to reference a specific reusable resource.",
      "processing_time": 51.29850149154663,
      "citing_paper_id": "271892156",
      "cited_paper_id": 221808419
    },
    {
      "context_text": "Brisbane-Event-VPR dataset [11] maximizes the similarity between features extracted from event and frame data for dynamic scene understanding, such as place recognition for autonomous driving in challenging lighting conditions.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Brisbane-Event-VPR"
      ],
      "dataset_descriptions": {
        "Brisbane-Event-VPR": "Used for dynamic scene understanding, specifically for place recognition in autonomous driving under challenging lighting conditions, maximizing feature similarity between event and frame data."
      },
      "confidence_score": 1.0,
      "reasoning": "The Brisbane-Event-VPR dataset is explicitly mentioned and used for dynamic scene understanding, particularly for place recognition in autonomous driving under challenging lighting conditions.",
      "processing_time": 54.15128803253174,
      "citing_paper_id": "271892156",
      "cited_paper_id": 221808419
    },
    {
      "context_text": "The DSEC [13] and M3ED [15] datasets further utilized the Prophesee event camera with a higher resolution for more fine-grained stereo matching in driving scenarios.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "DSEC",
        "M3ED"
      ],
      "dataset_descriptions": {
        "DSEC": "Used for fine-grained stereo matching in driving scenarios with event cameras, enhancing resolution and detail.",
        "M3ED": "Utilized for multi-robot, multi-sensor, multi-environment event data, focusing on high-resolution stereo matching in driving scenarios."
      },
      "confidence_score": 1.0,
      "reasoning": "The citation mentions two datasets, DSEC and M3ED, both used for fine-grained stereo matching in driving scenarios with event cameras.",
      "processing_time": 55.542531967163086,
      "citing_paper_id": "271892156",
      "cited_paper_id": 259380779
    },
    {
      "context_text": "Specifically, we introduce a foundation depth model, namely Metric3D [18], [19], to estimate the reference depth for filtering the projected coarse depth.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a model (Metric3D). The context focuses on the introduction and use of a model for depth estimation, not on a dataset.",
      "processing_time": 51.194652795791626,
      "citing_paper_id": "271892156",
      "cited_paper_id": 269329975
    },
    {
      "context_text": "To generate ground truth depth and optical flow, we use SLAM algorithm [17] to fuse LiDAR point clouds, and filter them with reference depth estimated by a foundation model [18], [19].",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions using a SLAM algorithm and a foundation model, but does not specify any named datasets. The cited papers do not provide additional dataset names.",
      "processing_time": 49.807146072387695,
      "citing_paper_id": "271892156",
      "cited_paper_id": 269329975
    },
    {
      "context_text": "5, we take the low-light enhancement method [2] to fuse the event and frame data for the visibility of the nighttime frame, and then estimate the reference depth from the enhanced image using Metric3D [18], [19].",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions 'Metric3D' but does not refer to it as a dataset. It is described as a method for estimating reference depth from an enhanced image.",
      "processing_time": 50.44696569442749,
      "citing_paper_id": "271892156",
      "cited_paper_id": 269329975
    },
    {
      "context_text": "Novel local event context descriptors are proposed to robustly match events in a temporal-spatial window [26], [27].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only methods or approaches for event-driven stereo matching and depth map estimation.",
      "processing_time": 48.56439399719238,
      "citing_paper_id": "245300947",
      "cited_paper_id": 1408596
    },
    {
      "context_text": "Novel local event context descriptors are proposed to robustly match events in a temporal-spatial window [26], [27].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only methods or approaches for event-driven stereo matching and depth map estimation.",
      "processing_time": 48.56439399719238,
      "citing_paper_id": "245300947",
      "cited_paper_id": 157060825
    },
    {
      "context_text": "COPNET [48], SEMI-DENSE 3D [7], SGM∗ [19][7], FCV F∗ [49][7]",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods and models. No dataset names are present in the context.",
      "processing_time": 48.56158375740051,
      "citing_paper_id": "245300947",
      "cited_paper_id": 1680724
    },
    {
      "context_text": "Following [8], we furthermore compare against SGM∗ and FCVF∗, two traditional methods [19][49] adjusted",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods. There are no clear identifiers for datasets in the provided context.",
      "processing_time": 48.96390724182129,
      "citing_paper_id": "245300947",
      "cited_paper_id": 1680724
    },
    {
      "context_text": "As can be observed, our proposed method HDES outperforms TSES [30], CopNet [48], Semi-Dense 3D [7], SGM∗ [19][7] and FCV F∗ [49][7].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only comparisons between methods. No dataset names are present in the text.",
      "processing_time": 48.73261904716492,
      "citing_paper_id": "245300947",
      "cited_paper_id": 1680724
    },
    {
      "context_text": "Our architecture is a fully convolutional neural network which is based on the Unet architecture [41].",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions the Unet architecture but does not refer to any specific dataset. The context is about the method used, not a dataset.",
      "processing_time": 49.350467920303345,
      "citing_paper_id": "245300947",
      "cited_paper_id": 3719281
    },
    {
      "context_text": "As can be observed, our proposed method HDES outperforms TSES [30], CopNet [48], Semi-Dense 3D [7], SGM ∗ [19][7] and FCVF ∗ [49][7].",
      "catation_intent": "findings",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only comparisons between methods. No dataset names are present in the text.",
      "processing_time": 48.73074960708618,
      "citing_paper_id": "245300947",
      "cited_paper_id": 7151414
    },
    {
      "context_text": "R ESULTS FOR HDES ( OUR PROPOSED METHOD ) TSES [30], C OP N ET [48], S EMI -",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets by name. It refers to methods and possibly results, but no clear dataset identifiers are present.",
      "processing_time": 49.34645628929138,
      "citing_paper_id": "245300947",
      "cited_paper_id": 7151414
    },
    {
      "context_text": "Our ﬁnal experiments compare our hybrid method HDES against the purely event-based methods DDES [8]—a state-of-the-art learning-based method—and Semi-Dense 3D [7], TSES [30], and CopNet [48], which are more traditional methods.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and models. The context focuses on comparing different methods for event-based stereo depth estimation.",
      "processing_time": 49.59343695640564,
      "citing_paper_id": "245300947",
      "cited_paper_id": 7151414
    },
    {
      "context_text": "Rogister et al. [24] ﬁnd matches under the assumption that correlated events are likely to appear within a small time interval and on the same epipolar line.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for finding matches in event-based stereo matching.",
      "processing_time": 48.16092014312744,
      "citing_paper_id": "245300947",
      "cited_paper_id": 17693733
    },
    {
      "context_text": "Original work [21], [22] accumulates events to generate event-images such that a frame-based stereo matching al-gorithm can be applied.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for generating event-images from events.",
      "processing_time": 47.87687802314758,
      "citing_paper_id": "245300947",
      "cited_paper_id": 27059477
    },
    {
      "context_text": "Such methods have been developed to solve many traditional vision problems such as optical flow estimation [31], [32], intensity frame reconstruction [33], [5], action recognition [34], and object tracking [35].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only various computer vision problems. No dataset names are present in the text.",
      "processing_time": 48.953224420547485,
      "citing_paper_id": "245300947",
      "cited_paper_id": 59222403
    },
    {
      "context_text": "Such methods have been developed to solve many traditional vision problems such as optical flow estimation [31], [32], intensity frame reconstruction [33], [5], action recognition [34], and object tracking [35].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only various computer vision problems. No dataset names are present in the text.",
      "processing_time": 48.953224420547485,
      "citing_paper_id": "245300947",
      "cited_paper_id": 226292034
    },
    {
      "context_text": "Event cameras hold several important beneﬁts over standard cameras such as low latency ( ∼ 1 µ s), absence of motion blur, high dynamic range (140 dB vs 60 dB for traditional cameras [1]), and low power consumption.",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only technical advantages of event cameras over traditional cameras.",
      "processing_time": 47.30778121948242,
      "citing_paper_id": "245300947",
      "cited_paper_id": 119309624
    },
    {
      "context_text": "Piatkowska et al. [28] and Firouzi et al. [29] model event-driven stereo matching by a cooperative network for reliable 3D depth perception.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and approaches for event-driven stereo matching.",
      "processing_time": 47.86733388900757,
      "citing_paper_id": "245300947",
      "cited_paper_id": 121601380
    },
    {
      "context_text": "Sharp intensity images are changed into blurry intensity images following [46].",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for transforming sharp intensity images into blurry ones.",
      "processing_time": 47.30262541770935,
      "citing_paper_id": "245300947",
      "cited_paper_id": 131774014
    },
    {
      "context_text": "Such methods have been developed to solve many traditional vision problems such as optical ﬂow estimation [31], [32], intensity frame reconstruction [33], [5], action recognition [34], and object tracking [35].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various vision problems and methods. No dataset names are present in the context.",
      "processing_time": 49.124764919281006,
      "citing_paper_id": "245300947",
      "cited_paper_id": 145916256
    },
    {
      "context_text": "Such methods have been developed to solve many traditional vision problems such as optical ﬂow estimation [31], [32], intensity frame reconstruction [33], [5], action recognition [34], and object tracking [35].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various vision problems and methods. No dataset names are present in the context.",
      "processing_time": 49.124764919281006,
      "citing_paper_id": "245300947",
      "cited_paper_id": 189998802
    },
    {
      "context_text": "Such methods have been developed to solve many traditional vision problems such as optical ﬂow estimation [31], [32], intensity frame reconstruction [33], [5], action recognition [34], and object tracking [35].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various vision problems and methods. No dataset names are present in the context.",
      "processing_time": 49.124764919281006,
      "citing_paper_id": "245300947",
      "cited_paper_id": 226291858
    },
    {
      "context_text": "…on event camera based computer vision, the reader is kindly referred to Gallego et al.’s survey [2] offering a good summary of several important contributions on problems such as object tracking [3], pattern recognition [4], intensity image reconstruction [5] and stereo depth estimation [6], [7].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only references to various applications of event cameras, including stereo depth estimation.",
      "processing_time": 48.72528672218323,
      "citing_paper_id": "245300947",
      "cited_paper_id": 189998802
    },
    {
      "context_text": "DeepPruner [16] improves on the complexity and runtime induced by building the cost volume and the 3D convolutions.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (DeepPruner).",
      "processing_time": 47.8565788269043,
      "citing_paper_id": "245300947",
      "cited_paper_id": 202565789
    },
    {
      "context_text": "Events and frames are utilized as a bi-modal input to recover sharp, high quality images in [36] and [33].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only the use of events and frames as inputs for image recovery and deblurring. No clear, verifiable dataset names are provided.",
      "processing_time": 50.2251455783844,
      "citing_paper_id": "245300947",
      "cited_paper_id": 220545977
    },
    {
      "context_text": "Events and frames are utilized as a bi-modal input to recover sharp, high quality images in [36] and [33].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only the use of events and frames as inputs for image recovery and deblurring. No clear, verifiable dataset names are provided.",
      "processing_time": 50.2251455783844,
      "citing_paper_id": "245300947",
      "cited_paper_id": 226291858
    },
    {
      "context_text": "’s survey [2] offering a good summary of several important contributions on problems such as object tracking [3], pattern recognition [4], intensity image reconstruction [5] and stereo depth estimation [6], [7].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general research areas and problems. There are no clear identifiers for datasets or other verifiable resources.",
      "processing_time": 49.94888091087341,
      "citing_paper_id": "245300947",
      "cited_paper_id": 221846159
    },
    {
      "context_text": "We first demonstrate the effectiveness of the stereo matching module by comparing with traditional stereo matching methods “Fast Cost-Volume Filtering” (FCVF) [6], “Libelas” method [5], and event driven stereo matching method of EDS [17].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods and models. There are no clear identifiers for datasets within the text.",
      "processing_time": 49.12805771827698,
      "citing_paper_id": "157060825",
      "cited_paper_id": 5535646
    },
    {
      "context_text": "EDS[17] FCVF [6] Libelas[5] Ours Avgerr 7.93 2.14 10.07 1.23\nA50 2.29 0.34 0.28 0.25 A90 21.15 4.39 49.0 2.05\nBad 2.0 49.1% 14.39% 22.35% 10.0% Bad 4.0 37.6% 9.78% 21.44% 5.8%\nTable 1: Errors for each of the evaluated methods.",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only performance metrics for different methods. No dataset names are provided.",
      "processing_time": 48.501569986343384,
      "citing_paper_id": "157060825",
      "cited_paper_id": 5535646
    },
    {
      "context_text": "Particularly, from the errors on metrics of “A50” and “A90”, we can see that while the errors of EDS [17], FCVF [6] and Libelas [5] increase rapidly, our method remains high accuracy almost all the time.",
      "catation_intent": "findings",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and their performance metrics.",
      "processing_time": 47.27482461929321,
      "citing_paper_id": "157060825",
      "cited_paper_id": 5535646
    },
    {
      "context_text": "EDS[17] FCVF [6] Libelas[5] Ours Avgerr 7.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not contain any specific dataset names or verifiable resources. It only lists abbreviations and methods without clear identifiers.",
      "processing_time": 48.888036489486694,
      "citing_paper_id": "157060825",
      "cited_paper_id": 5535646
    },
    {
      "context_text": "For fairly comparing the stereo matching performance, we run the FCVF [6] and Libelas [5] on the RGB images so that these two methods can achieve the best accuracy, and only the depth of pixels with events are selected for comparison.",
      "catation_intent": "reusable resource",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets by name. It only refers to methods (FCVF, Libelas) and a general type of data (RGB images).",
      "processing_time": 50.595683574676514,
      "citing_paper_id": "157060825",
      "cited_paper_id": 5535646
    },
    {
      "context_text": "[3] proposed a high performance event-driven matching approach based on time-correlation.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method or approach. There are no clear identifiers for datasets in the context.",
      "processing_time": 49.11554574966431,
      "citing_paper_id": "157060825",
      "cited_paper_id": 11395394
    },
    {
      "context_text": "The work [2] constructs the covariance matrix through a brute force searching, which is time consuming.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for constructing a covariance matrix.",
      "processing_time": 46.741321325302124,
      "citing_paper_id": "157060825",
      "cited_paper_id": 12475678
    },
    {
      "context_text": "According to the work [2], the velocity of an event is proportional to the slope of the fitted plane formed by using the timestamps of its neighboring events.",
      "catation_intent": "findings",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method or finding related to event-based visual flow.",
      "processing_time": 47.61427402496338,
      "citing_paper_id": "157060825",
      "cited_paper_id": 12475678
    },
    {
      "context_text": "In theory, the velocity of events can be obtained by calculating the gradient of timestamp image, as [2] did.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific dataset, only a method for obtaining the velocity of events using timestamps.",
      "processing_time": 48.07377219200134,
      "citing_paper_id": "157060825",
      "cited_paper_id": 12475678
    },
    {
      "context_text": "We apply the Integral Images [14] based method presented in Section 3 to efficiently solve the equation above.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific dataset. It refers to a method for solving stereo correspondence using integral images.",
      "processing_time": 48.67944836616516,
      "citing_paper_id": "157060825",
      "cited_paper_id": 14196634
    },
    {
      "context_text": "To achieve real-time performance, we employ the Integral Images [14] to accelerate the computation of normals of events buffering in duration [t −Mt, t +Mt].",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific dataset, only a method (Integral Images) used for accelerating computations. No dataset names are present in the citation context.",
      "processing_time": 49.303263664245605,
      "citing_paper_id": "157060825",
      "cited_paper_id": 14196634
    },
    {
      "context_text": "In this paper, we proposed a real-time event stream enhancement method with Integral Images [18] whose complexity is linear to the number of events.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a method for enhancing event streams using Integral Images. The cited paper title confirms that Integral Images is a method, not a dataset.",
      "processing_time": 50.74872589111328,
      "citing_paper_id": "157060825",
      "cited_paper_id": 14196634
    },
    {
      "context_text": "[16] proposed an area-based stereo matching approach and demonstrated in a real-time tracking application.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for real-time tracking using stereo vision.",
      "processing_time": 48.23611855506897,
      "citing_paper_id": "157060825",
      "cited_paper_id": 24236495
    },
    {
      "context_text": "Some stereo matching approaches [13, 15, 16, 19] for DVS data are published in the last decade.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to stereo matching approaches for DVS data.",
      "processing_time": 48.66784906387329,
      "citing_paper_id": "157060825",
      "cited_paper_id": 24236495
    },
    {
      "context_text": "This includes, for instance, advances in panoramic 3D vision [25, 12], retrieving information from moving handheld cameras for Simultaneous Localisation and Mapping (SLAM) [29] or camera pose estimation [13, 10].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only general advancements in 3D vision, SLAM, and camera pose estimation. No verifiable resources are identified.",
      "processing_time": 50.74025559425354,
      "citing_paper_id": "7151414",
      "cited_paper_id": 185541
    },
    {
      "context_text": "This includes, for instance, advances in panoramic 3D vision [25, 12], retrieving information from moving handheld cameras for Simultaneous Localisation and Mapping (SLAM) [29] or camera pose estimation [13, 10].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only general advancements in 3D vision, SLAM, and camera pose estimation. No verifiable resources are identified.",
      "processing_time": 50.74025559425354,
      "citing_paper_id": "7151414",
      "cited_paper_id": null
    },
    {
      "context_text": "Cooperative stereo [20] is one of the first algorithms describing the process of stereoscopic vision and its formulation by a computational model.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only an algorithm for stereoscopic vision.",
      "processing_time": 47.560123443603516,
      "citing_paper_id": "7151414",
      "cited_paper_id": 767650
    },
    {
      "context_text": "The third algorithm CoopSt [21] is the cooperative stereo using time-based single event matching.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only an algorithm. No verifiable resources are identified.",
      "processing_time": 48.35071778297424,
      "citing_paper_id": "7151414",
      "cited_paper_id": 6079544
    },
    {
      "context_text": "In most of the event-based matching algorithms [11, 21] matching candidates are weighted by their similarity to the reference event e as defined in Eq.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only algorithms and methods. The cited papers' titles suggest a focus on event-based stereo vision but do not explicitly name datasets.",
      "processing_time": 50.609429597854614,
      "citing_paper_id": "7151414",
      "cited_paper_id": 6079544
    },
    {
      "context_text": "In most of the event-based matching algorithms [11, 21] matching candidates are weighted by their similarity to the reference event e as defined in Eq.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only algorithms and methods. The cited papers' titles suggest a focus on event-based stereo vision but do not explicitly name datasets.",
      "processing_time": 50.609429597854614,
      "citing_paper_id": "7151414",
      "cited_paper_id": 7083033
    },
    {
      "context_text": "In this paper, we are building on the work presented in [21].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a reference to another paper's work.",
      "processing_time": 47.08987617492676,
      "citing_paper_id": "7151414",
      "cited_paper_id": 6079544
    },
    {
      "context_text": "The main contribution of this paper is the development of an improved cooperative stereo matching algorithm which builds upon the original method presented in [21].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method or algorithm. The context focuses on the development of an improved cooperative stereo matching algorithm.",
      "processing_time": 49.402575969696045,
      "citing_paper_id": "7151414",
      "cited_paper_id": 6079544
    },
    {
      "context_text": "Furthermore, in [21] noise events were assumed to be filtered out by a density threshold in the cooperative network, however, this method was found not to be reliable.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for filtering noise events in a cooperative network.",
      "processing_time": 48.34072208404541,
      "citing_paper_id": "7151414",
      "cited_paper_id": 6079544
    },
    {
      "context_text": "T-Corr [17] SAD+2SF [16] CoopSt [21] proposed",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods or approaches. The cited paper title suggests a focus on stereo vision but does not specify a dataset.",
      "processing_time": 50.32182955741882,
      "citing_paper_id": "7151414",
      "cited_paper_id": 6079544
    },
    {
      "context_text": "The first row presents results of the algorithm from [21], and the second row the results of the proposed improved algorithm.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only algorithms. There are no clear identifiers for datasets in the provided context.",
      "processing_time": 49.192034006118774,
      "citing_paper_id": "7151414",
      "cited_paper_id": 6079544
    },
    {
      "context_text": "Following that, a software version of cooperative stereo for global disparity estimation was implemented by Hess in his semester thesis [11].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method or software implementation.",
      "processing_time": 46.690566062927246,
      "citing_paper_id": "7151414",
      "cited_paper_id": 7083033
    },
    {
      "context_text": "Events can be converted to images by aggregation over a specific time period, as presented in [26][17], or reconstructed to gray-scale images by more sophisticated algorithms, as shown in [6] or [1].",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only methods for converting events to images. No verifiable resources are identified.",
      "processing_time": 48.95846390724182,
      "citing_paper_id": "7151414",
      "cited_paper_id": 10280488
    },
    {
      "context_text": "Events can be converted to images by aggregation over a specific time period, as presented in [26][17], or reconstructed to gray-scale images by more sophisticated algorithms, as shown in [6] or [1].",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only methods for converting events to images. No verifiable resources are identified.",
      "processing_time": 48.95846390724182,
      "citing_paper_id": "7151414",
      "cited_paper_id": 24236495
    },
    {
      "context_text": "[5] apply Bayesian filtering to the initial matches projected into 3D space.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method applied to initial matches in 3D space.",
      "processing_time": 48.51276159286499,
      "citing_paper_id": "7151414",
      "cited_paper_id": 10712214
    },
    {
      "context_text": "Recently, bio-inspired Dynamic Vision Sensors (DVS) [18, 22] have gained a lot of recognition, mainly due to their advantages over clocked (frame-based) cameras such as high-temporal resolution, wide dynamic range and low power consumption.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only discusses the advantages of Dynamic Vision Sensors (DVS).",
      "processing_time": 47.88990616798401,
      "citing_paper_id": "7151414",
      "cited_paper_id": 15357188
    },
    {
      "context_text": "Assuming a fixed DVS pose, the amount of delivered data is significantly reduced because only changes in the scene are detected.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a general description of DVS (Dynamic Vision Sensor) behavior.",
      "processing_time": 48.94575834274292,
      "citing_paper_id": "7151414",
      "cited_paper_id": 15357188
    },
    {
      "context_text": "Among the most recent methods on event-based 3D reconstruction - even though not directly connected to the work presented in this paper - we can find panorama stereo vision from rotating, stereo line-DVS [26], multiview stereo from a single sensor [23] or simultaneous 3D reconstruction and 6-DoF tracking [13].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods and systems. No verifiable resources are identified.",
      "processing_time": 48.74360466003418,
      "citing_paper_id": "7151414",
      "cited_paper_id": 15357188
    },
    {
      "context_text": "Among the most recent methods on event-based 3D reconstruction - even though not directly connected to the work presented in this paper - we can find panorama stereo vision from rotating, stereo line-DVS [26], multiview stereo from a single sensor [23] or simultaneous 3D reconstruction and 6-DoF tracking [13].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods and systems. No verifiable resources are identified.",
      "processing_time": 48.74360466003418,
      "citing_paper_id": "7151414",
      "cited_paper_id": 24236495
    },
    {
      "context_text": "In this paper we are tackling the task of 3D stereo reconstruction from a static stereo DVS setup (Figure 1a).",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only a task and a setup. No verifiable resources are identified.",
      "processing_time": 48.97475075721741,
      "citing_paper_id": "7151414",
      "cited_paper_id": 15357188
    },
    {
      "context_text": "[28] and FPGA by Eibensteiner et al.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only references to a paper and FPGA implementation.",
      "processing_time": 47.898494482040405,
      "citing_paper_id": "7151414",
      "cited_paper_id": 22213242
    },
    {
      "context_text": "[26] evaluated window-based stereo matching with different matching cost variants, achieving the best performance with Normalized Sum of Absolute Differences (NSAD).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (window-based stereo matching) and a metric (NSAD).",
      "processing_time": 49.61095452308655,
      "citing_paper_id": "7151414",
      "cited_paper_id": 24236495
    },
    {
      "context_text": "For the calibration and rectification of the silicon retina stereo sensor, the calibration toolbox of Bouguet [3] is used, as described in more detail in [8].",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions a calibration toolbox but does not refer to a specific dataset. The toolbox is a method or tool, not a dataset.",
      "processing_time": 49.60859251022339,
      "citing_paper_id": "7151414",
      "cited_paper_id": 54145653
    },
    {
      "context_text": "The error rates for T-Corr and SAD+2SF were taken from the literature [14] and [16], respectively.",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only error rates from literature.",
      "processing_time": 46.84554886817932,
      "citing_paper_id": "7151414",
      "cited_paper_id": null
    },
    {
      "context_text": "We trained and tested our network on the Multi Vehicle Stereo Event Camera (MVSEC) dataset (Zhu et al. 2018b).",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Multi Vehicle Stereo Event Camera (MVSEC)"
      ],
      "dataset_descriptions": {
        "Multi Vehicle Stereo Event Camera (MVSEC)": "Used to train and test a network for event-based stereo depth estimation, focusing on 3D perception tasks using event camera data."
      },
      "confidence_score": 1.0,
      "reasoning": "The MVSEC dataset is explicitly mentioned and used for training and testing a network for event-based stereo depth estimation.",
      "processing_time": 53.312753677368164,
      "citing_paper_id": "238198645",
      "cited_paper_id": 3416874
    },
    {
      "context_text": "Implemented on dedicated chips such as Intel Loihi [5], IBM TrueNorth [2], Brainchip Akida [40] or Tianjic [34], these models could become a new paradigm for ultra-low power computation in the coming years.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only hardware platforms. No verifiable resources are identified.",
      "processing_time": 48.72553372383118,
      "citing_paper_id": "238198645",
      "cited_paper_id": 3608458
    },
    {
      "context_text": "Implemented on dedicated chips such as Intel Loihi [5], IBM TrueNorth [2], Brainchip Akida [40] or Tianjic [34], these models could become a new paradigm for ultra-low power computation in the coming years.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only hardware platforms. No verifiable resources are identified.",
      "processing_time": 48.72553372383118,
      "citing_paper_id": "238198645",
      "cited_paper_id": 207935891
    },
    {
      "context_text": "As a result, it is essentially implementable on dedicated neuromorphic hardware, such as Intel Loihi [7], IBM TrueNorth [8].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only hardware platforms. There are no verifiable resources that meet the criteria for inclusion.",
      "processing_time": 48.95299673080444,
      "citing_paper_id": "238198645",
      "cited_paper_id": 3608458
    },
    {
      "context_text": "Implemented on dedicated chips such as Intel Loihi [7], IBM TrueNorth [8], Brainchip Akida [9] or",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only hardware platforms. There are no verifiable resources that meet the criteria for inclusion.",
      "processing_time": 49.7746741771698,
      "citing_paper_id": "238198645",
      "cited_paper_id": 3608458
    },
    {
      "context_text": "As a result, it is essentially implementable on dedicated neuromorphic hardware, such as Intel Loihi [5], IBM TrueNorth [2].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions hardware platforms but does not refer to any specific datasets, models, or methods.",
      "processing_time": 47.27771759033203,
      "citing_paper_id": "238198645",
      "cited_paper_id": 3608458
    },
    {
      "context_text": "The model in [7] was the ﬁrst successful multi-scale architecture designed for depth estimation from RGB images, and was consequently followed by advances based on similar approaches [24] [23] [14].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only models and methods for depth estimation from RGB images.",
      "processing_time": 48.93603754043579,
      "citing_paper_id": "238198645",
      "cited_paper_id": 4572038
    },
    {
      "context_text": "The model in [7] was the ﬁrst successful multi-scale architecture designed for depth estimation from RGB images, and was consequently followed by advances based on similar approaches [24] [23] [14].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only models and methods for depth estimation from RGB images.",
      "processing_time": 48.93603754043579,
      "citing_paper_id": "238198645",
      "cited_paper_id": 102496818
    },
    {
      "context_text": "Depth is an important feature of the surrounding space whose estimation finds its place in various tasks across many different fields [1].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general statement about depth estimation. No verifiable resources are identified.",
      "processing_time": 48.48984098434448,
      "citing_paper_id": "238198645",
      "cited_paper_id": 4694685
    },
    {
      "context_text": "Such neurons are inexpensive to simulate and can be deployed in large models, whereas more complex ones such as Hodgkin-Huxley [16], Izhikevich [18] or even SRM [13] are still too computationally expensive to be trained on modern hardware.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only neuron models. No datasets are referenced or used in the context provided.",
      "processing_time": 49.42054462432861,
      "citing_paper_id": "238198645",
      "cited_paper_id": 6195748
    },
    {
      "context_text": "7 TSES* [38] 36 44 36 CopNet* [31] 61 100 64",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.0,
      "reasoning": "The citation span does not contain any specific dataset names or clear identifiers. It appears to be a table or list of results, possibly comparing different methods or models.",
      "processing_time": 49.87030363082886,
      "citing_paper_id": "238198645",
      "cited_paper_id": 7151414
    },
    {
      "context_text": "StereoSpike also outperforms TSES [38] and CopNet [31], which only predict depth at event positions.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only comparisons to other methods. No clear, verifiable dataset names are present.",
      "processing_time": 48.57961058616638,
      "citing_paper_id": "238198645",
      "cited_paper_id": 7151414
    },
    {
      "context_text": "DTC-SPADE [44] is not represented here as the authors did not use the same colormap as other studies and did not publish their code.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (DTC-SPADE) and a reason why it was not included in the study.",
      "processing_time": 49.97659969329834,
      "citing_paper_id": "238198645",
      "cited_paper_id": 81981856
    },
    {
      "context_text": "The model in [22] used the same input embedding and backbone as [21], but proposed a preliminary network using spatially-adaptive normalization (SPADE) [23] to reconstruct grayscale intensity images jointly with depth maps.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models and methods. The context focuses on the use of SPADE for image synthesis and depth map reconstruction.",
      "processing_time": 49.973106145858765,
      "citing_paper_id": "238198645",
      "cited_paper_id": 81981856
    },
    {
      "context_text": "DTC-SPADE [44] has managed to incorporate elements of the latter for the beneﬁt of accuracy and without exploding algorithmic size.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (DTC-SPADE) and a reference to another paper. No verifiable resources are identified.",
      "processing_time": 50.142712116241455,
      "citing_paper_id": "238198645",
      "cited_paper_id": 81981856
    },
    {
      "context_text": "[44] also used a similar matching backbone as well as SPADE, but differed in its input encoding.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (SPADE) and a general reference to input encoding. No verifiable resources are identified.",
      "processing_time": 50.055256605148315,
      "citing_paper_id": "238198645",
      "cited_paper_id": 81981856
    },
    {
      "context_text": "The model in [1] used the same input embedding and backbone as [39], but proposed a preliminary network using spatially-adaptive normalization (SPADE) [33] to reconstruct grayscale intensity images jointly with depth maps.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models and methods. The context focuses on the use of SPADE for image synthesis and depth map reconstruction.",
      "processing_time": 50.05019807815552,
      "citing_paper_id": "238198645",
      "cited_paper_id": 81981856
    },
    {
      "context_text": "From a visual neuroscience perspective, there is strong evidence that binocular cues are mixed in a similar manner at very early stages of the visual system [41].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a reference to a paper discussing binocular cues in the visual system.",
      "processing_time": 49.20493769645691,
      "citing_paper_id": "238198645",
      "cited_paper_id": 86423050
    },
    {
      "context_text": "Dynamic Vision Sensors (DVS) have recently gathered the interest of scientists and industrial actors, thanks to a growing number of research papers explaining how to process their output [11].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general interest in Dynamic Vision Sensors (DVS). No verifiable resources are identified.",
      "processing_time": 49.595542669296265,
      "citing_paper_id": "238198645",
      "cited_paper_id": 118684904
    },
    {
      "context_text": "We therefore consider that Brainchip’s Akida chip [35] is a good fit.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a hardware solution. There are no verifiable resources that meet the criteria.",
      "processing_time": 49.49757099151611,
      "citing_paper_id": "238198645",
      "cited_paper_id": 207935891
    },
    {
      "context_text": "Implemented on dedicated chips such as Intel Loihi [3], IBM TrueNorth [1], Brainchip Akida [35] or Tianjic [29], these models could become a new paradigm for ultra-low power computation in the coming years.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only hardware platforms. There are no verifiable resources that meet the criteria for inclusion.",
      "processing_time": 49.589335918426514,
      "citing_paper_id": "238198645",
      "cited_paper_id": 207935891
    },
    {
      "context_text": "A notable exception is (Gehrig et al. 2020), but they only regressed 3 variables, while we propose here to regress the values of 260× 346 = 89960 pixels.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a comparison of regression variable counts.",
      "processing_time": 48.0970516204834,
      "citing_paper_id": "238198645",
      "cited_paper_id": 212414806
    },
    {
      "context_text": "We then investigated outdoor scenarios by following the same training, test and validation splits as [19].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific dataset names, only referring to splits used in another paper. No verifiable resource is identified.",
      "processing_time": 48.81460785865784,
      "citing_paper_id": "238198645",
      "cited_paper_id": 223957202
    },
    {
      "context_text": "In outdoor scenarios, our model outperforms E2Depth [19] by a large margin at all cutoff distances, and with 25× fewer parameters.",
      "catation_intent": "findings",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a comparison with another model (E2Depth).",
      "processing_time": 48.088093757629395,
      "citing_paper_id": "238198645",
      "cited_paper_id": 223957202
    },
    {
      "context_text": "divide by the maximum number of spike count) for an easier-to-learn distribution of data and better generalization [19].",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.2,
      "reasoning": "The citation does not mention any specific dataset names, only a method for data distribution. The context is too vague to identify a specific dataset.",
      "processing_time": 49.623403787612915,
      "citing_paper_id": "238198645",
      "cited_paper_id": 223957202
    },
    {
      "context_text": "However, we argue that the task of depth recovery from events has a minor temporal component and can be solved by a fully feedforward model with minimal temporal knowledge; therefore the use of convLSTMs in [19] is suboptimal and unnecessarily costly in terms of computation.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (convLSTMs) and a critique of its use. No verifiable resources are identified.",
      "processing_time": 49.7151095867157,
      "citing_paper_id": "238198645",
      "cited_paper_id": 223957202
    },
    {
      "context_text": "Finally, in [19], dense metric depth was recovered from only one camera, and showed good performances with a recurrent, monocular encoder-decoder architecture on outdoor sequences.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for recovering dense metric depth using a monocular encoder-decoder architecture.",
      "processing_time": 49.13485860824585,
      "citing_paper_id": "238198645",
      "cited_paper_id": 223957202
    },
    {
      "context_text": "As a counterbalance measure, we used data augmentation; as [19].",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific dataset names, only a general reference to data augmentation. No multi-word proper nouns, acronyms, or hyphenated names with digits are present.",
      "processing_time": 49.96255826950073,
      "citing_paper_id": "238198645",
      "cited_paper_id": 223957202
    },
    {
      "context_text": "As in [19], we used a combination of a regression loss with a regularization loss.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a combination of losses used in training. No verifiable resources are identified.",
      "processing_time": 48.77775812149048,
      "citing_paper_id": "238198645",
      "cited_paper_id": 223957202
    },
    {
      "context_text": "Please note that [19] also used simulated data that did not come from MVSEC, but this does not prevent StereoSpike from outperforming this competitor by a large margin with substantially less parameters (cf.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.3,
      "reasoning": "The context mentions 'simulated data' but does not specify a dataset name. MVSEC is mentioned but not as a dataset used in the cited paper.",
      "processing_time": 49.95425200462341,
      "citing_paper_id": "238198645",
      "cited_paper_id": 223957202
    },
    {
      "context_text": "According to [19], the minimization of this term encourages smooth depth changes as well as sharp depth discontinuities in the depth map prediction, hence helping the network to represent objects that stand out of the background of the scene (e.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for improving depth map predictions.",
      "processing_time": 47.67699432373047,
      "citing_paper_id": "238198645",
      "cited_paper_id": 223957202
    },
    {
      "context_text": "This is not due to the fact that [19] takes input from only one camera as the monocular version of StereoSpike still remains consistently superior to this competitor.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a comparison between methods. No dataset names are present in the citation span.",
      "processing_time": 48.94441771507263,
      "citing_paper_id": "238198645",
      "cited_paper_id": 223957202
    },
    {
      "context_text": "While SNNs generally remain less accurate than their analog counterpart (i.e., Analog Neural Networks or ANNs), the gap in accuracy is decreasing, even on challenging problems like ImageNet [9].",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions ImageNet but does not indicate it is used as a dataset in the research. It is referenced to describe the performance of SNNs compared to ANNs.",
      "processing_time": 50.279963970184326,
      "citing_paper_id": "238198645",
      "cited_paper_id": 226976144
    },
    {
      "context_text": "While SNNs generally remain less accurate than their analog counterpart (i.e., Analog Neural Networks or ANNs), the gap in accuracy is decreasing, even on challenging problems like ImageNet [9].",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions ImageNet but does not indicate it is used as a dataset in the research. It is referenced to describe the performance of SNNs compared to ANNs.",
      "processing_time": 50.279963970184326,
      "citing_paper_id": "238198645",
      "cited_paper_id": 235359262
    },
    {
      "context_text": "While SNNs generally remain less accurate than their analog counterpart (i.e., Analog Neural Networks or ANNs), the gap in accuracy is decreasing, even on challenging problems like ImageNet [9].",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions ImageNet but does not indicate it is used as a dataset in the research. It is referenced to describe the performance of SNNs compared to ANNs.",
      "processing_time": 50.279963970184326,
      "citing_paper_id": "238198645",
      "cited_paper_id": 241440878
    },
    {
      "context_text": "So far, SNNs have been used for classification tasks like image recognition [8], object detection [18], or motion segmentation [27].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only general tasks such as image recognition, object detection, and motion segmentation.",
      "processing_time": 49.24026036262512,
      "citing_paper_id": "238198645",
      "cited_paper_id": 226976144
    },
    {
      "context_text": "We use the derivative of the arctan function as our surrogate gradient in this paper, as suggested in [8].",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for using a surrogate gradient.",
      "processing_time": 47.668827295303345,
      "citing_paper_id": "238198645",
      "cited_paper_id": 226976144
    },
    {
      "context_text": ", Analog Neural Networks or ANNs), the gap in accuracy is decreasing, even on challenging problems like ImageNet [8].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific dataset names, only a reference to ImageNet which is excluded as it is part of a benchmark challenge.",
      "processing_time": 49.59764623641968,
      "citing_paper_id": "238198645",
      "cited_paper_id": 226976144
    },
    {
      "context_text": "The bottleneck consisted in 2 SEWResBlocks [9] following each other and with ADD connect function.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (SEWResBlocks) used in the architecture.",
      "processing_time": 49.14295434951782,
      "citing_paper_id": "238198645",
      "cited_paper_id": 235359262
    },
    {
      "context_text": "So far, SNNs have been used for classification tasks like image recognition [9,10], object detection [3,20], or motion segmentation [31].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only general tasks such as image recognition, object detection, and motion segmentation.",
      "processing_time": 49.092079877853394,
      "citing_paper_id": "238198645",
      "cited_paper_id": 235359262
    },
    {
      "context_text": ", Analog Neural Networks or ANNs), the gap in accuracy is decreasing, even on challenging problems like ImageNet [9].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific dataset names, only a reference to ImageNet which is not used in the described research.",
      "processing_time": 49.35606503486633,
      "citing_paper_id": "238198645",
      "cited_paper_id": 235359262
    },
    {
      "context_text": "So far, SNNs have been used for classiﬁcation tasks like image recognition [9,10], object detection [3,20], or motion segmentation [31].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general tasks and applications of SNNs. There are no verifiable resources or datasets mentioned.",
      "processing_time": 49.60602831840515,
      "citing_paper_id": "238198645",
      "cited_paper_id": 241440878
    },
    {
      "context_text": "For example, [50] proposes a method to estimate affine fundamental matrices without explicit correspondences, but it does not solve the full problem of depth estimation.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for estimating affine fundamental matrices. No datasets are referenced for use or evaluation.",
      "processing_time": 49.260493755340576,
      "citing_paper_id": "250918780",
      "cited_paper_id": 227768
    },
    {
      "context_text": "Correspondence-free approaches for depth estimation have been proposed for frame-based cameras [48]–[50].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only methods or approaches for depth estimation. The cited paper titles do not provide additional information about datasets.",
      "processing_time": 49.85344433784485,
      "citing_paper_id": "250918780",
      "cited_paper_id": 227768
    },
    {
      "context_text": "Correspondence-free approaches for depth estimation have been proposed for frame-based cameras [48]–[50].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only methods or approaches for depth estimation. The cited paper titles do not provide additional information about datasets.",
      "processing_time": 49.85344433784485,
      "citing_paper_id": "250918780",
      "cited_paper_id": 11008141
    },
    {
      "context_text": "Inspired by [10], [48], we circumvent the data association task by leveraging the sparsity of events (event cameras naturally highlight edges, which are sparse, in hardware) and by exploiting the continuous set of camera viewpoints at which events are available.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only methods and approaches. The cited papers' titles do not provide additional dataset information.",
      "processing_time": 49.3444619178772,
      "citing_paper_id": "250918780",
      "cited_paper_id": 1082643
    },
    {
      "context_text": "Inspired by [10], [48], we circumvent the data association task by leveraging the sparsity of events (event cameras naturally highlight edges, which are sparse, in hardware) and by exploiting the continuous set of camera viewpoints at which events are available.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only methods and approaches. The cited papers' titles do not provide additional dataset information.",
      "processing_time": 49.3444619178772,
      "citing_paper_id": "250918780",
      "cited_paper_id": 11008141
    },
    {
      "context_text": "The effectiveness of EVO is largely due to its mapping module, Event-based Multi-View Stereo (EMVS) [10], which enables 3D reconstruction without the need to recover image intensity, without having to explicitly solve for data association between events, and without the need of a GPU (it is fast on a standard CPU –e.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific dataset names, only a method (EMVS) and its capabilities. The context focuses on the method's functionality rather than a dataset.",
      "processing_time": 50.01564979553223,
      "citing_paper_id": "250918780",
      "cited_paper_id": 1082643
    },
    {
      "context_text": "The front-view projection of the DSI in Figure 2 corresponds to the confidence map, which is called this way because it is used in AGT to “select the most confident pixels in the depth map” [10] (since voxels with many ray intersections are more likely to capture true 3D points than voxels with few ray intersections).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific dataset names, only a method (AGT) and a concept (DSI). The context focuses on the methodology and the confidence map used in depth estimation.",
      "processing_time": 50.578479290008545,
      "citing_paper_id": "250918780",
      "cited_paper_id": 1082643
    },
    {
      "context_text": "2Mev/s throughput per CPU core [10]) and that it estimates depth without explicit data association.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for event-based multi-view stereo reconstruction.",
      "processing_time": 48.049593687057495,
      "citing_paper_id": "250918780",
      "cited_paper_id": 1082643
    },
    {
      "context_text": ", the 3D structure emerges by event refocusing [10], [18].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method or approach for 3D reconstruction using event cameras.",
      "processing_time": 49.33069086074829,
      "citing_paper_id": "250918780",
      "cited_paper_id": 1082643
    },
    {
      "context_text": ", fusion) of such back-projected rays or “refocused events”, which has not been considered before (since [10] and newer approaches [52] do not consider data fusion, e.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and approaches. The context focuses on the fusion of back-projected rays or 'refocused events', which is a methodological aspect.",
      "processing_time": 50.25492453575134,
      "citing_paper_id": "250918780",
      "cited_paper_id": 1082643
    },
    {
      "context_text": "The method in [10] solves this problem, called EMVS, in two main steps: it builds a Disparity Space Image (DSI) using a space sweeping approach [48] and then detects local maxima of the DSI.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and approaches. The context focuses on describing the steps of the EMVS method.",
      "processing_time": 49.41084337234497,
      "citing_paper_id": "250918780",
      "cited_paper_id": 1082643
    },
    {
      "context_text": "The method in [10] solves this problem, called EMVS, in two main steps: it builds a Disparity Space Image (DSI) using a space sweeping approach [48] and then detects local maxima of the DSI.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and approaches. The context focuses on describing the steps of the EMVS method.",
      "processing_time": 49.41084337234497,
      "citing_paper_id": "250918780",
      "cited_paper_id": 11008141
    },
    {
      "context_text": "In practice, DSIs are discretized over a projective voxel grid with NZ depth planes in [Zmin, Zmax], and the Delta δ in (2) is approximated by bilinear voting [10], [55].",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and approaches. The context is technical and focuses on the discretization and approximation methods used in event-based stereo depth estimation.",
      "processing_time": 49.6571946144104,
      "citing_paper_id": "250918780",
      "cited_paper_id": 1082643
    },
    {
      "context_text": ", 5× 5), then the complexity of [10] is",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not contain any specific dataset names or verifiable resources. It only mentions complexity, which is not a dataset.",
      "processing_time": 48.879070520401,
      "citing_paper_id": "250918780",
      "cited_paper_id": 1082643
    },
    {
      "context_text": "1: Semi-dense depth maps estimated by event-based monocular [10] and stereo methods.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods for estimating depth maps using event-based cameras.",
      "processing_time": 48.785643339157104,
      "citing_paper_id": "250918780",
      "cited_paper_id": 1082643
    },
    {
      "context_text": "So far, results on datasets from three different resolutions have been presented: DAVIS346 (MVSEC), Prophesee Gen3 (DSEC) and Prophesee Gen4 (TUM-VIE).",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "DAVIS346 (MVSEC)",
        "Prophesee Gen3 (DSEC)",
        "Prophesee Gen4 (TUM-VIE)"
      ],
      "dataset_descriptions": {
        "DAVIS346 (MVSEC)": "Used to evaluate event-based stereo depth estimation methods at 346x260 resolution, focusing on dynamic scenes and high temporal resolution.",
        "ProphES": "A 240 × 180 130 dB 3 µs Latency Global Shutter Spatiotemporal Vision Sensor",
        "Prophesee Gen3 (DSEC)": "Used to evaluate event-based stereo depth estimation methods in driving scenarios, providing high-resolution and high-dynamic range data.",
        "Prophesee Gen4 (TUM-VIE)": "Used to evaluate event-based stereo depth estimation methods at higher resolutions, focusing on indoor and outdoor environments."
      },
      "confidence_score": 1.0,
      "reasoning": "The citation mentions three specific datasets used for event-based stereo depth estimation at different resolutions.",
      "processing_time": 57.91036939620972,
      "citing_paper_id": "250918780",
      "cited_paper_id": 1747923
    },
    {
      "context_text": "So far, results on datasets from three different resolutions have been presented: DAVIS346 (MVSEC), Prophesee Gen3 (DSEC) and Prophesee Gen4 (TUM-VIE).",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "DAVIS346 (MVSEC)",
        "Prophesee Gen3 (DSEC)",
        "Prophesee Gen4 (TUM-VIE)"
      ],
      "dataset_descriptions": {
        "DAVIS346 (MVSEC)": "Used to evaluate event-based stereo depth estimation methods at 346x260 resolution, focusing on dynamic scenes and high temporal resolution.",
        "ProphES": "A 240 × 180 130 dB 3 µs Latency Global Shutter Spatiotemporal Vision Sensor",
        "Prophesee Gen3 (DSEC)": "Used to evaluate event-based stereo depth estimation methods in driving scenarios, providing high-resolution and high-dynamic range data.",
        "Prophesee Gen4 (TUM-VIE)": "Used to evaluate event-based stereo depth estimation methods at higher resolutions, focusing on indoor and outdoor environments."
      },
      "confidence_score": 1.0,
      "reasoning": "The citation mentions three specific datasets used for event-based stereo depth estimation at different resolutions.",
      "processing_time": 57.91036939620972,
      "citing_paper_id": "250918780",
      "cited_paper_id": 24007071
    },
    {
      "context_text": "So far, results on datasets from three different resolutions have been presented: DAVIS346 (MVSEC), Prophesee Gen3 (DSEC) and Prophesee Gen4 (TUM-VIE).",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "DAVIS346 (MVSEC)",
        "Prophesee Gen3 (DSEC)",
        "Prophesee Gen4 (TUM-VIE)"
      ],
      "dataset_descriptions": {
        "DAVIS346 (MVSEC)": "Used to evaluate event-based stereo depth estimation methods at 346x260 resolution, focusing on dynamic scenes and high temporal resolution.",
        "ProphES": "A 240 × 180 130 dB 3 µs Latency Global Shutter Spatiotemporal Vision Sensor",
        "Prophesee Gen3 (DSEC)": "Used to evaluate event-based stereo depth estimation methods in driving scenarios, providing high-resolution and high-dynamic range data.",
        "Prophesee Gen4 (TUM-VIE)": "Used to evaluate event-based stereo depth estimation methods at higher resolutions, focusing on indoor and outdoor environments."
      },
      "confidence_score": 1.0,
      "reasoning": "The citation mentions three specific datasets used for event-based stereo depth estimation at different resolutions.",
      "processing_time": 57.91036939620972,
      "citing_paper_id": "250918780",
      "cited_paper_id": 232170230
    },
    {
      "context_text": "We consider typical numbers from a stereo DAVIS346 configuration [74].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a configuration of a stereo DAVIS346 setup. No verifiable resources are identified.",
      "processing_time": 49.557591676712036,
      "citing_paper_id": "250918780",
      "cited_paper_id": 1747923
    },
    {
      "context_text": "We consider typical numbers from a stereo DAVIS346 configuration [74].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a configuration of a stereo DAVIS346 setup. No verifiable resources are identified.",
      "processing_time": 49.557591676712036,
      "citing_paper_id": "250918780",
      "cited_paper_id": 24007071
    },
    {
      "context_text": "For visualization purposes, the first column depicts intensity frames from the DAVIS camera (not used by any method).",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific, verifiable datasets. It only refers to intensity frames from the DAVIS camera, which is not a dataset but a device output.",
      "processing_time": 49.8919837474823,
      "citing_paper_id": "250918780",
      "cited_paper_id": 1747923
    },
    {
      "context_text": "For visualization purposes, the first column depicts intensity frames from the DAVIS camera (not used by any method).",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific, verifiable datasets. It only refers to intensity frames from the DAVIS camera, which is not a dataset but a device output.",
      "processing_time": 49.8919837474823,
      "citing_paper_id": "250918780",
      "cited_paper_id": 24007071
    },
    {
      "context_text": "The DSI has dimensions w × h × NZ , according to the resolution of the event camera DAVIS240 (w = 240, h = 180 pix) and the number of inverse-depth planes used, NZ = 100.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions the DAVIS240 event camera, which is a specific device, but does not refer to a dataset. The dimensions and resolution are described, but no dataset name is provided.",
      "processing_time": 50.44186234474182,
      "citing_paper_id": "250918780",
      "cited_paper_id": 1747923
    },
    {
      "context_text": "The DSI has dimensions w × h × NZ , according to the resolution of the event camera DAVIS240 (w = 240, h = 180 pix) and the number of inverse-depth planes used, NZ = 100.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions the DAVIS240 event camera, which is a specific device, but does not refer to a dataset. The dimensions and resolution are described, but no dataset name is provided.",
      "processing_time": 50.44186234474182,
      "citing_paper_id": "250918780",
      "cited_paper_id": 24007071
    },
    {
      "context_text": "Calibration is today of good quality using the DAVIS frames or image reconstruction [62] if frames are not available.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only hardware and methods. No dataset names are present in the citation context.",
      "processing_time": 49.1269326210022,
      "citing_paper_id": "250918780",
      "cited_paper_id": 1747923
    },
    {
      "context_text": "Calibration is today of good quality using the DAVIS frames or image reconstruction [62] if frames are not available.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only hardware and methods. No dataset names are present in the citation context.",
      "processing_time": 49.1269326210022,
      "citing_paper_id": "250918780",
      "cited_paper_id": 24007071
    },
    {
      "context_text": "The DAVIS comprises a frame-based and an event-based sensor on the same pixel array, thus calibration (intrinsic and extrinsic) is achieved using the intensity frames, and then it is applied to the events.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "DAVIS"
      ],
      "dataset_descriptions": {
        "DAVIS": "Used for calibration of intrinsic and extrinsic parameters in event-based stereo depth estimation, leveraging both frame-based and event-based sensors on the same pixel array."
      },
      "confidence_score": 0.9,
      "reasoning": "The context mentions 'DAVIS' which is a specific sensor system used for capturing both frame-based and event-based data. It is used for calibration purposes in the research.",
      "processing_time": 53.072452545166016,
      "citing_paper_id": "250918780",
      "cited_paper_id": 1747923
    },
    {
      "context_text": "The DAVIS comprises a frame-based and an event-based sensor on the same pixel array, thus calibration (intrinsic and extrinsic) is achieved using the intensity frames, and then it is applied to the events.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "DAVIS"
      ],
      "dataset_descriptions": {
        "DAVIS": "Used for calibration of intrinsic and extrinsic parameters in event-based stereo depth estimation, leveraging both frame-based and event-based sensors on the same pixel array."
      },
      "confidence_score": 0.9,
      "reasoning": "The context mentions 'DAVIS' which is a specific sensor system used for capturing both frame-based and event-based data. It is used for calibration purposes in the research.",
      "processing_time": 53.072452545166016,
      "citing_paper_id": "250918780",
      "cited_paper_id": 24007071
    },
    {
      "context_text": "To illustrate the appearance of the scenes, the first column shows grayscale frames from the DAVIS [67].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "DAVIS"
      ],
      "dataset_descriptions": {
        "DAVIS": "Used to illustrate the appearance of scenes with grayscale frames, providing visual context for event-based stereo depth estimation."
      },
      "confidence_score": 0.8,
      "reasoning": "The context mentions 'DAVIS' which is a known dataset in the field of event-based vision. However, the context does not provide specific details about how the dataset is used beyond showing grayscale frames.",
      "processing_time": 52.73795509338379,
      "citing_paper_id": "250918780",
      "cited_paper_id": 1747923
    },
    {
      "context_text": "To illustrate the appearance of the scenes, the first column shows grayscale frames from the DAVIS [67].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "DAVIS"
      ],
      "dataset_descriptions": {
        "DAVIS": "Used to illustrate the appearance of scenes with grayscale frames, providing visual context for event-based stereo depth estimation."
      },
      "confidence_score": 0.8,
      "reasoning": "The context mentions 'DAVIS' which is a known dataset in the field of event-based vision. However, the context does not provide specific details about how the dataset is used beyond showing grayscale frames.",
      "processing_time": 52.73795509338379,
      "citing_paper_id": "250918780",
      "cited_paper_id": 24007071
    },
    {
      "context_text": "Top: our method produces a semi-dense depth map of the scene (color coded from red (close) to blue (far), overlaid on a grayscale frame of the DAVIS [53]), and a confidence map (Bottom) with the maximum DSI value along each reference view pixel, in negated scale (bright = small; dark = large).\nscore (Section 3.4)), and finally local maxima are extracted to produce a semi-dense depth map.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "DAVIS"
      ],
      "dataset_descriptions": {
        "DAVIS": "Used to generate grayscale frames for overlaying semi-dense depth maps and confidence maps, focusing on event-based stereo depth estimation with high temporal resolution."
      },
      "confidence_score": 0.6,
      "reasoning": "The context mentions 'DAVIS' which is likely a dataset or a specific sensor used in the experiments. However, without more context, it is unclear if 'DAVIS' is a dataset or a device. The cited paper titles do not provide additional clarity.",
      "processing_time": 53.7818877696991,
      "citing_paper_id": "250918780",
      "cited_paper_id": 1747923
    },
    {
      "context_text": "Top: our method produces a semi-dense depth map of the scene (color coded from red (close) to blue (far), overlaid on a grayscale frame of the DAVIS [53]), and a confidence map (Bottom) with the maximum DSI value along each reference view pixel, in negated scale (bright = small; dark = large).\nscore (Section 3.4)), and finally local maxima are extracted to produce a semi-dense depth map.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "DAVIS"
      ],
      "dataset_descriptions": {
        "DAVIS": "Used to generate grayscale frames for overlaying semi-dense depth maps and confidence maps, focusing on event-based stereo depth estimation with high temporal resolution."
      },
      "confidence_score": 0.6,
      "reasoning": "The context mentions 'DAVIS' which is likely a dataset or a specific sensor used in the experiments. However, without more context, it is unclear if 'DAVIS' is a dataset or a device. The cited paper titles do not provide additional clarity.",
      "processing_time": 53.7818877696991,
      "citing_paper_id": "250918780",
      "cited_paper_id": 24007071
    },
    {
      "context_text": "The stereo rigs in [38], [57] consist of two Dynamic and Active Pixel Vision Sensors (DAVIS) [53].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions stereo rigs consisting of DAVIS sensors, which are described in the cited papers. However, no specific dataset is mentioned or used in the context.",
      "processing_time": 49.78657507896423,
      "citing_paper_id": "250918780",
      "cited_paper_id": 1747923
    },
    {
      "context_text": "The stereo rigs in [38], [57] consist of two Dynamic and Active Pixel Vision Sensors (DAVIS) [53].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions stereo rigs consisting of DAVIS sensors, which are described in the cited papers. However, no specific dataset is mentioned or used in the context.",
      "processing_time": 49.78657507896423,
      "citing_paper_id": "250918780",
      "cited_paper_id": 24007071
    },
    {
      "context_text": "In practice, DSIs are discretized over a projective voxel grid with NZ depth planes in [Zmin, Zmax], and the Delta δ in (2) is approximated by bilinear voting [10], [52].",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and approaches. No dataset names are present in the citation span.",
      "processing_time": 48.679778814315796,
      "citing_paper_id": "250918780",
      "cited_paper_id": 3328976
    },
    {
      "context_text": "Interpretation in terms of Contrast Maximization: The proposed stereo fusion method is related to contrast/focus maximization [16], [17].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets. It refers to methods and frameworks, not datasets.",
      "processing_time": 48.677449226379395,
      "citing_paper_id": "250918780",
      "cited_paper_id": 4597042
    },
    {
      "context_text": "Interpretation in terms of Contrast Maximization: The proposed stereo fusion method is related to contrast/focus maximization [16], [17].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets. It refers to methods and frameworks, not datasets.",
      "processing_time": 48.677449226379395,
      "citing_paper_id": "250918780",
      "cited_paper_id": 119309624
    },
    {
      "context_text": "Additionally, EMVS admits an interpretation in terms of event refocusing or event alignment (contrast maximization) [16], which is the state of the art framework to tackle other vision problems [17]–[25].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets. It refers to methods and frameworks, not datasets.",
      "processing_time": 48.76726579666138,
      "citing_paper_id": "250918780",
      "cited_paper_id": 4597042
    },
    {
      "context_text": "Additionally, EMVS admits an interpretation in terms of event refocusing or event alignment (contrast maximization) [16], which is the state of the art framework to tackle other vision problems [17]–[25].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets. It refers to methods and frameworks, not datasets.",
      "processing_time": 48.76726579666138,
      "citing_paper_id": "250918780",
      "cited_paper_id": 119309624
    },
    {
      "context_text": ", they constitute so-called images of warped events (IWEs) [16].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a concept related to event cameras. No verifiable resources are identified.",
      "processing_time": 48.75879406929016,
      "citing_paper_id": "250918780",
      "cited_paper_id": 4597042
    },
    {
      "context_text": "The results are more noticeable in the confidence maps; these are more clear (sharper [16]) in the first columns than in the last ones.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a visual observation about confidence maps. No dataset names are present in the citation context.",
      "processing_time": 49.18482804298401,
      "citing_paper_id": "250918780",
      "cited_paper_id": 4597042
    },
    {
      "context_text": "We also provide precision, recall and F1-score curves [62].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only performance metrics. No dataset names are present in the citation context.",
      "processing_time": 49.09631109237671,
      "citing_paper_id": "250918780",
      "cited_paper_id": 9632354
    },
    {
      "context_text": "The simulator [60], [61] provides synthetic sequences using an ideal event camera model and scenes built using CAD models.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.3,
      "reasoning": "The context mentions a simulator that provides synthetic sequences, which is likely part of a dataset or tool for generating event-based data. However, the name 'simulator' is too generic without a specific identifier.",
      "processing_time": 50.315189361572266,
      "citing_paper_id": "250918780",
      "cited_paper_id": 9865213
    },
    {
      "context_text": "How: Depth estimation with stereo event cameras is predominantly based on exploiting the epipolar constraint and the assumption of temporal coincidence of events across retinas, namely that a moving object produces events of same timestamps on both cameras [39], [42], [43].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only methods and assumptions about stereo event cameras. No verifiable resources are identified.",
      "processing_time": 49.428483724594116,
      "citing_paper_id": "250918780",
      "cited_paper_id": 10712214
    },
    {
      "context_text": "How: Depth estimation with stereo event cameras is predominantly based on exploiting the epipolar constraint and the assumption of temporal coincidence of events across retinas, namely that a moving object produces events of same timestamps on both cameras [39], [42], [43].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only methods and assumptions about stereo event cameras. No verifiable resources are identified.",
      "processing_time": 49.428483724594116,
      "citing_paper_id": "250918780",
      "cited_paper_id": 49554392
    },
    {
      "context_text": "How: Depth estimation with stereo event cameras is predominantly based on exploiting the epipolar constraint and the assumption of temporal coincidence of events across retinas, namely that a moving object produces events of same timestamps on both cameras [39], [42], [43].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only methods and assumptions about stereo event cameras. No verifiable resources are identified.",
      "processing_time": 49.428483724594116,
      "citing_paper_id": "250918780",
      "cited_paper_id": 53086261
    },
    {
      "context_text": "In contrast, our method solves the full depth estimation problem and is not limited to the planar case, following the seminal idea of [48] to sweep space and exploit the sparsity of scene edges.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a methodological approach. The context focuses on the method's capabilities and limitations compared to previous work.",
      "processing_time": 49.67625164985657,
      "citing_paper_id": "250918780",
      "cited_paper_id": 11008141
    },
    {
      "context_text": "In this method temporal consistency is measured across space-time neighborhoods of events by first converting the events into time surfaces (TSs) [47] and then comparing their spatial neighborhoods.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for converting events into time surfaces.",
      "processing_time": 48.65273690223694,
      "citing_paper_id": "250918780",
      "cited_paper_id": 13373696
    },
    {
      "context_text": "The Semi-Global Matching (SGM) method [63] is adapted to event data by feeding time images [47] and masking the produced depth map at recent event locations [38].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and approaches. The context focuses on adapting the SGM method to event data.",
      "processing_time": 49.4191472530365,
      "citing_paper_id": "250918780",
      "cited_paper_id": 13373696
    },
    {
      "context_text": "So, what are sensible ways to compare two DSIs? FMRay density DSIs have very different statistics from natural images, hence standard similarity metrics for image patches may not be the most appropriate ones [54].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only discusses the appropriateness of standard similarity metrics for comparing DSIs.",
      "processing_time": 48.740915298461914,
      "citing_paper_id": "250918780",
      "cited_paper_id": 13713291
    },
    {
      "context_text": "An event-based SLAM system has the potential to efficiently overcome difficult scenarios in these applications [13]–[15].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only references to papers discussing event-based SLAM systems.",
      "processing_time": 48.660507917404175,
      "citing_paper_id": "250918780",
      "cited_paper_id": 16588072
    },
    {
      "context_text": "Our work is inspired by EVO [13], which is the state of the art in event-based monocular VO.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions EVO but does not refer to it as a dataset. It is described as a method or approach, not a reusable data resource.",
      "processing_time": 49.57367014884949,
      "citing_paper_id": "250918780",
      "cited_paper_id": 16588072
    },
    {
      "context_text": "The effectiveness of EVO is largely due to its mapping module, Event-based Multi-View Stereo (EMVS) [10], which enables 3D reconstruction without the need to recover image intensity, without having to explicitly solve for data association between events, and without the need of a GPU (it is fast on a standard CPU –e.g., speed of 1.20 Mev/s/core [10]).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (EMVS) and its capabilities. No verifiable resources are identified.",
      "processing_time": 49.32480239868164,
      "citing_paper_id": "250918780",
      "cited_paper_id": 16588072
    },
    {
      "context_text": "This assumption does not strictly hold [44], [45], and so it is relaxed to account for temporal noise (jitter and delay).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only references to temporal noise in event-based systems.",
      "processing_time": 48.73142337799072,
      "citing_paper_id": "250918780",
      "cited_paper_id": 17407641
    },
    {
      "context_text": "We consider typical numbers from a stereo DAVIS346 configuration [71].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific dataset, only a sensor configuration. No verifiable resource is identified.",
      "processing_time": 48.381795167922974,
      "citing_paper_id": "250918780",
      "cited_paper_id": 19091270
    },
    {
      "context_text": "acquired with two event cameras [50] performing a 1D motion (translation along the X axis, using a linear slider).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific dataset names, only describes the setup involving event cameras. No clear, verifiable dataset is referenced.",
      "processing_time": 49.40091371536255,
      "citing_paper_id": "250918780",
      "cited_paper_id": 24007071
    },
    {
      "context_text": "Top: our method produces a semi-dense depth map of the scene (color coded from red (close) to blue (far), overlaid on a grayscale frame of the DAVIS [50]), and a confidence map (Bottom) with the maximum DSI value along each reference view pixel, in negated scale (bright = small; dark = large).",
      "catation_intent": "reusable resource",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions 'DAVIS' which is likely a specific sensor or dataset, but the title disambiguates it as a sensor. No other specific datasets are mentioned.",
      "processing_time": 49.794801473617554,
      "citing_paper_id": "250918780",
      "cited_paper_id": 24007071
    },
    {
      "context_text": "The stereo rigs in [38], [54] consist of two Dynamic and Active Pixel Vision Sensors (DAVIS) [50].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions hardware components (stereo rigs, DAVIS sensors) but does not refer to any specific dataset. The context is about the setup used in experiments, not a dataset.",
      "processing_time": 49.870898962020874,
      "citing_paper_id": "250918780",
      "cited_paper_id": 24007071
    },
    {
      "context_text": "On the other hand, [49] aims to solve the problem of stereo depth estimation, but for the special case of planar or quasi-planar scenes.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method or approach for stereo depth estimation in planar or quasi-planar scenes.",
      "processing_time": 49.474414110183716,
      "citing_paper_id": "250918780",
      "cited_paper_id": 42595366
    },
    {
      "context_text": "The remaining columns show the output of GTS, SGM, ESVO and our methods.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods and outputs. No verifiable resources are identified.",
      "processing_time": 49.054120779037476,
      "citing_paper_id": "250918780",
      "cited_paper_id": 49554392
    },
    {
      "context_text": "1s 35.42 12.35 6.39 8.45 16.17 29.49 85.34 93.05 96.03 14.46 GTS indep.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not contain any specific, verifiable dataset names or identifiers. It appears to be a list of numerical values, possibly event timestamps or coordinates, which do not qualify as a dataset under the given rules.",
      "processing_time": 50.48581886291504,
      "citing_paper_id": "250918780",
      "cited_paper_id": 49554392
    },
    {
      "context_text": "GTS produces modest results, albeit with many outliers.",
      "catation_intent": "findings",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (GTS) and its performance. No verifiable resources are identified.",
      "processing_time": 49.383145809173584,
      "citing_paper_id": "250918780",
      "cited_paper_id": 49554392
    },
    {
      "context_text": "The Generalized Time-Based Stereovision method (GTS) [43] follows a classical two-step approach: stereo matching plus triangulation.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method called GTS. No verifiable resources are identified.",
      "processing_time": 49.29869771003723,
      "citing_paper_id": "250918780",
      "cited_paper_id": 49554392
    },
    {
      "context_text": "The experi-\n9 Scene GTS [43] SGM [66] ESVO [15] Alg.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.2,
      "reasoning": "The context does not provide enough information to identify specific datasets. The citation appears to reference algorithms or methods rather than datasets.",
      "processing_time": 49.14366412162781,
      "citing_paper_id": "250918780",
      "cited_paper_id": 49554392
    },
    {
      "context_text": "The Generalized Time-Based Stereovision method (GTS) [43] follows a classical two-step approach: stereo\nmatching plus triangulation.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method called GTS. No verifiable resources are identified.",
      "processing_time": 49.05090641975403,
      "citing_paper_id": "250918780",
      "cited_paper_id": 49554392
    },
    {
      "context_text": "1s 35.45 13.61 5.54 7.35 15.03 27.46 85.96 93.51 96.40 11.64 GTS indep.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not contain any specific dataset names or verifiable resources. It appears to be a list of numerical values, possibly event timestamps or coordinates, which do not qualify as a dataset under the given rules.",
      "processing_time": 50.308122873306274,
      "citing_paper_id": "250918780",
      "cited_paper_id": 49554392
    },
    {
      "context_text": "GTS and SGM are also endowed with depth propagation-and-fusion filters, as implemented in [15].",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.0,
      "reasoning": "The citation does not mention any specific datasets, only methods or algorithms. The context is too vague to infer the use of a dataset.",
      "processing_time": 49.21037721633911,
      "citing_paper_id": "250918780",
      "cited_paper_id": 49554392
    },
    {
      "context_text": "Columns 2 to 7 show semi-dense inverse depth maps produced by GTS [43], SGM [66], ESVO [15], our Alg.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only algorithms and methods. The context is focused on comparing different algorithms for producing inverse depth maps.",
      "processing_time": 49.60223197937012,
      "citing_paper_id": "250918780",
      "cited_paper_id": 49554392
    },
    {
      "context_text": "a well-known difficult problem due to the little information carried by each event and their dependency with motion direction (changing “appearance” of events [2], [46]).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only discusses challenges related to event-based stereo depth estimation.",
      "processing_time": 48.95398497581482,
      "citing_paper_id": "250918780",
      "cited_paper_id": 50775406
    },
    {
      "context_text": "Some other works estimate depth by combining an event camera with other devices, such as light projectors [8], [39], [40] or a motorized focal lens [41], which are different from our hardware setup and application.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only hardware setups and methods. No verifiable resources are identified.",
      "processing_time": 49.279305934906006,
      "citing_paper_id": "250918780",
      "cited_paper_id": 53086261
    },
    {
      "context_text": "Some other works estimate depth by combining an event camera with other devices, such as light projectors [8], [39], [40] or a motorized focal lens [41], which are different from our hardware setup and application.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only hardware setups and methods. No verifiable resources are identified.",
      "processing_time": 49.279305934906006,
      "citing_paper_id": "250918780",
      "cited_paper_id": 206767633
    },
    {
      "context_text": "Essentially event simultaneity is exploited to solve the data association problem (establishing event matches), which is a well-known difficult problem due to the little information carried by each event and their dependency with motion direction (changing “appearance” of events [2], [46]).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only discusses the challenges and methods in event-based vision.",
      "processing_time": 49.035157680511475,
      "citing_paper_id": "250918780",
      "cited_paper_id": 118684904
    },
    {
      "context_text": "However, this calls for novel methods to process the unconventional output of event cameras in order to unlock their capabilities [2].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only the need for novel methods to process event camera outputs.",
      "processing_time": 49.27345037460327,
      "citing_paper_id": "250918780",
      "cited_paper_id": 118684904
    },
    {
      "context_text": "Data association is a fundamental problem in event-based vision [2].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general problem in the field of event-based vision.",
      "processing_time": 49.42817759513855,
      "citing_paper_id": "250918780",
      "cited_paper_id": 118684904
    },
    {
      "context_text": "Inspired by human vision, neuromorphic spike-based sensing and processing has been recently investigated for robot vision [1], [2] and retinal implants [3].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only general areas of research. No dataset names are present in the citation span.",
      "processing_time": 49.345235109329224,
      "citing_paper_id": "250918780",
      "cited_paper_id": 118684904
    },
    {
      "context_text": "we found that using Ns ∈ [2, 8] gives satisfactory results.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The provided context does not mention any specific datasets, only a parameter range for satisfactory results. No dataset names are present in the citation span.",
      "processing_time": 49.739253759384155,
      "citing_paper_id": "250918780",
      "cited_paper_id": 118684904
    },
    {
      "context_text": "we found that using Ns ∈ [2, 8] gives satisfactory results.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The provided context does not mention any specific datasets, only a parameter range for satisfactory results. No dataset names are present in the citation span.",
      "processing_time": 49.739253759384155,
      "citing_paper_id": "250918780",
      "cited_paper_id": 206767633
    },
    {
      "context_text": ", the 3D structure emerges by event refocusing [10], [17].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method or approach related to event-based vision.",
      "processing_time": 49.182599782943726,
      "citing_paper_id": "250918780",
      "cited_paper_id": 119309624
    },
    {
      "context_text": "This approach has inspired a lot of literature that focuses on 3D reconstruction over short time intervals (“instantaneous stereo”) [33]–[35].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general approach and literature. No verifiable resources are identified.",
      "processing_time": 49.41641855239868,
      "citing_paper_id": "250918780",
      "cited_paper_id": 205698386
    },
    {
      "context_text": "DSEC [16] Prophesee Gen3 640× 480 60 60.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "DSEC"
      ],
      "dataset_descriptions": {
        "DSEC": "Used to evaluate stereo depth estimation algorithms in driving scenarios, providing event-based data from a Prophesee Gen3 sensor."
      },
      "confidence_score": 1.0,
      "reasoning": "The citation mentions 'DSEC' which is a specific dataset for stereo event camera data in driving scenarios. The context indicates it is used for evaluation.",
      "processing_time": 52.15850639343262,
      "citing_paper_id": "250918780",
      "cited_paper_id": 232170230
    },
    {
      "context_text": "Some datasets, such as MVSEC and DSEC, contain ground truth depth for quantitative assessment of the 3D reconstruction methods.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "MVSEC",
        "DSEC"
      ],
      "dataset_descriptions": {
        "MVSEC": "Used for quantitative assessment of 3D reconstruction methods, providing ground truth depth for evaluating event-based stereo depth estimation algorithms.",
        "DSEC": "Used for quantitative evaluation of stereo depth estimation methods, providing ground truth depth for driving scenarios to assess 3D reconstruction accuracy."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions two specific datasets, MVSEC and DSEC, which are used for quantitative assessment of 3D reconstruction methods in event-based stereo depth estimation.",
      "processing_time": 53.789185523986816,
      "citing_paper_id": "250918780",
      "cited_paper_id": 232170230
    },
    {
      "context_text": "Instead, we show results of our method on driving sequences from the DSEC dataset (Section 4.5), which has a larger baseline of 60 cm.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "DSEC"
      ],
      "dataset_descriptions": {
        "DSEC": "Used to evaluate the proposed method on driving sequences with a 60 cm baseline, focusing on stereo depth estimation using event cameras."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions the DSEC dataset, which is a specific, verifiable dataset used for driving scenarios with event cameras. The dataset is used to evaluate the method described in the paper.",
      "processing_time": 52.61020851135254,
      "citing_paper_id": "250918780",
      "cited_paper_id": 232170230
    },
    {
      "context_text": "Inspired by MVSEC,\nDSEC and TUM-VIE datasets, three sensor resolutions are tested: 320 × 240 pix, 640 × 480 pix and 1280 × 960 pix, respectively.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "DSEC",
        "TUM-VIE"
      ],
      "dataset_descriptions": {
        "DSEC": "Used to test sensor resolutions in event-based stereo depth estimation for driving scenarios, focusing on 320x240, 640x480, and 1280x960 pixel resolutions.",
        "TUM-VIE": "Used to test sensor resolutions in event-based stereo depth estimation, focusing on 320x240, 640x480, and 1280x960 pixel resolutions."
      },
      "confidence_score": 0.9,
      "reasoning": "The context mentions DSEC and TUM-VIE as datasets used for testing sensor resolutions in event-based stereo depth estimation.",
      "processing_time": 55.433146953582764,
      "citing_paper_id": "250918780",
      "cited_paper_id": 232170230
    },
    {
      "context_text": "We evaluate our stereo methods on sequences from five publicly available datasets [16], [38], [57]–[59] and a simulator.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "DSEC"
      ],
      "dataset_descriptions": {
        "DSEC": "Used to evaluate stereo methods in driving scenarios, focusing on event-based camera data to improve depth estimation accuracy."
      },
      "confidence_score": 0.8,
      "reasoning": "The context mentions 'five publicly available datasets' and a simulator, but does not specify the names of the datasets. The cited paper title suggests one potential dataset, DSEC, which is relevant to the topic.",
      "processing_time": 52.648728132247925,
      "citing_paper_id": "250918780",
      "cited_paper_id": 232170230
    },
    {
      "context_text": "Our method works well regardless of the spatial resolution, which is interesting given the increasingly high spatial resolution of event cameras (see the comparisons with ESVO on DSEC and TUM-VIE datasets in the accompanying video).",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "DSEC",
        "TUM-VIE"
      ],
      "dataset_descriptions": {
        "DSEC": "Used to compare the performance of the proposed method with ESVO, focusing on stereo event camera data for driving scenarios.",
        "TUM-VIE": "Used to evaluate the performance of the proposed method against ESVO, emphasizing the robustness across different spatial resolutions."
      },
      "confidence_score": 1.0,
      "reasoning": "The citation mentions two specific datasets, DSEC and TUM-VIE, which are used for comparing the performance of the method with ESVO.",
      "processing_time": 53.699235677719116,
      "citing_paper_id": "250918780",
      "cited_paper_id": 232170230
    },
    {
      "context_text": "Finally, we notice that the ground truth depth of DSEC is sparser than that of MVSEC.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "DSEC",
        "MVSEC"
      ],
      "dataset_descriptions": {
        "DSEC": "Used to compare ground truth depth sparsity with MVSEC, focusing on stereo event camera data for driving scenarios.",
        "MVSEC": "Used to compare ground truth depth sparsity, focusing on stereo event camera data for driving scenarios."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions 'DSEC' and 'MVSEC', which are specific datasets used for comparing ground truth depth sparsity in event camera datasets.",
      "processing_time": 53.416247606277466,
      "citing_paper_id": "250918780",
      "cited_paper_id": 232170230
    },
    {
      "context_text": "Then, we evaluate on higher resolution data: driving dataset DSEC (Section 4.5), 1Mpixel VIO dataset TUMVIE (Section 4.6), and analyze the sensitivity with respect to the camera’s spatial resolution (Section 4.7).",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "DSEC",
        "TUMVIE"
      ],
      "dataset_descriptions": {
        "DSEC": "Used to evaluate the method on high-resolution stereo event camera data for driving scenarios, focusing on performance and accuracy in depth estimation.",
        "TUMVIE": "Used to evaluate the method on high-resolution visual-inertial odometry data, focusing on performance and sensitivity to camera spatial resolution."
      },
      "confidence_score": 1.0,
      "reasoning": "The citation mentions two specific datasets, DSEC and TUMVIE, which are used for evaluating the performance of the method on higher resolution data and analyzing sensitivity to camera spatial resolution.",
      "processing_time": 54.44675135612488,
      "citing_paper_id": "250918780",
      "cited_paper_id": 232170230
    },
    {
      "context_text": "The sequences in the DSEC dataset [16] were recorded with event cameras on a car that drove through Zurich’s surroundings.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "DSEC"
      ],
      "dataset_descriptions": {
        "DSEC": "Used to record sequences with event cameras on a car driving through Zurich’s surroundings, focusing on stereo depth estimation in dynamic environments."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions the DSEC dataset, which is a specific dataset used for stereo event camera data in driving scenarios.",
      "processing_time": 51.9660370349884,
      "citing_paper_id": "250918780",
      "cited_paper_id": 232170230
    },
    {
      "context_text": "We also give results on sequences from the driving dataset DSEC [16].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "DSEC"
      ],
      "dataset_descriptions": {
        "DSEC": "Used to evaluate stereo depth estimation methods in driving scenarios, providing event camera data for algorithm testing and validation."
      },
      "confidence_score": 1.0,
      "reasoning": "The citation mentions the 'driving dataset DSEC', which is a specific dataset used for stereo event camera data in driving scenarios.",
      "processing_time": 51.66677260398865,
      "citing_paper_id": "250918780",
      "cited_paper_id": 232170230
    },
    {
      "context_text": "The outdoor driving MVSEC sequences have very small stereo baseline, which makes them poor for 3D reconstruction [76], hence we test on the recent DSEC dataset.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "DSEC"
      ],
      "dataset_descriptions": {
        "DSEC": "Used to test event-based stereo depth estimation methods in driving scenarios, addressing the limitations of small stereo baselines in other datasets."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions the DSEC dataset, which is used for testing in the context of event-based stereo depth estimation for driving scenarios.",
      "processing_time": 51.94648838043213,
      "citing_paper_id": "250918780",
      "cited_paper_id": 232170230
    },
    {
      "context_text": "The performance of the proposed method is quantitatively characterized using several standard metrics on the datasets with ground truth depth (i.e., MVSEC and DSEC).",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "MVSEC",
        "DSEC"
      ],
      "dataset_descriptions": {
        "MVSEC": "Used to evaluate the proposed method's performance with ground truth depth, focusing on event-based stereo depth estimation.",
        "DSEC": "Used to evaluate the proposed method on driving scenarios, focusing on event camera data for driving scenarios."
      },
      "confidence_score": 1.0,
      "reasoning": "The citation mentions two specific datasets, MVSEC and DSEC, which are used to evaluate the performance of the proposed method with ground truth depth.",
      "processing_time": 53.20325565338135,
      "citing_paper_id": "250918780",
      "cited_paper_id": 232170230
    },
    {
      "context_text": "Throughout the experiments (MVSEC, DSEC, TUM-VIE, time fusion, etc.), we have shown the gains with respect to the monocular method; the main advantages of stereo are: higher accuracy, outlier rejection, and faster convergence (due to the additional parallax).",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "MVSEC",
        "DSEC",
        "TUM-VIE"
      ],
      "dataset_descriptions": {
        "MVSEC": "Used to demonstrate the advantages of stereo over monocular methods in event-based depth estimation, focusing on aspects such as accuracy and convergence speed.",
        "DSEC": "Used to evaluate stereo event camera performance in driving scenarios, highlighting improvements in accuracy and outlier rejection compared to monocular methods.",
        "TUM-VIE": "Used to assess the benefits of stereo in event-based depth estimation, emphasizing faster convergence and higher accuracy in various indoor and outdoor environments."
      },
      "confidence_score": 1.0,
      "reasoning": "The citation mentions multiple datasets, including MVSEC, DSEC, and TUM-VIE, which are used to demonstrate the advantages of stereo over monocular methods in event-based depth estimation.",
      "processing_time": 55.37808394432068,
      "citing_paper_id": "250918780",
      "cited_paper_id": 232170230
    },
    {
      "context_text": "The datasets whose cameras output only events (EVIMO2, DSEC and TUM-VIE), are calibrated by converting events to frames and calibrating the latter (e.g., using [62]).",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "EVIMO2",
        "DSEC",
        "TUM-VIE"
      ],
      "dataset_descriptions": {
        "EVIMO2": "Used for calibrating event-based cameras in event-based stereo depth estimation, focusing on converting events to frames for calibration.",
        "DSEC": "Used for calibrating event-based cameras in driving scenarios, specifically for event-based stereo depth estimation by converting events to frames.",
        "TUM-VIE": "Used for calibrating event-based cameras in various indoor and outdoor environments, focusing on converting events to frames for calibration."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions three specific datasets: EVIMO2, DSEC, and TUM-VIE. These datasets are used for event-based stereo depth estimation, specifically for calibrating cameras that output only events.",
      "processing_time": 55.57483720779419,
      "citing_paper_id": "250918780",
      "cited_paper_id": 232170230
    },
    {
      "context_text": "The datasets whose cameras output only events (EVIMO2, DSEC and TUM-VIE), are calibrated by converting events to frames and calibrating the latter (e.g., using [62]).",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "EVIMO2",
        "DSEC",
        "TUM-VIE"
      ],
      "dataset_descriptions": {
        "EVIMO2": "Used for calibrating event-based cameras in event-based stereo depth estimation, focusing on converting events to frames for calibration.",
        "DSEC": "Used for calibrating event-based cameras in driving scenarios, specifically for event-based stereo depth estimation by converting events to frames.",
        "TUM-VIE": "Used for calibrating event-based cameras in various indoor and outdoor environments, focusing on converting events to frames for calibration."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions three specific datasets: EVIMO2, DSEC, and TUM-VIE. These datasets are used for event-based stereo depth estimation, specifically for calibrating cameras that output only events.",
      "processing_time": 55.57483720779419,
      "citing_paper_id": "250918780",
      "cited_paper_id": null
    },
    {
      "context_text": ", fusion) of such back-projected rays or “refocused events”, which has not been considered before (since [10] and newer approaches [48] do not consider data fusion, e.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.0,
      "reasoning": "The citation does not mention any specific datasets, only methods and approaches. The context is too vague to identify any verifiable resources.",
      "processing_time": 49.755629777908325,
      "citing_paper_id": "250918780",
      "cited_paper_id": 236481116
    },
    {
      "context_text": "This research aims at developing robust multi-camera visual perception systems for the navigation of artificial intelligent systems in challenging environments, like stereo depth perception for SLAM and attention in robots [28], [29].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general research areas and goals. There are no clear identifiers for datasets or other verifiable resources.",
      "processing_time": 50.13880133628845,
      "citing_paper_id": "250918780",
      "cited_paper_id": 237766931
    },
    {
      "context_text": "For a detailed survey on these stereo methods we refer to [36], [37].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only references to other papers for a detailed survey on stereo methods.",
      "processing_time": 49.43814659118652,
      "citing_paper_id": "250918780",
      "cited_paper_id": 246656358
    },
    {
      "context_text": "Recent results on the monocular case [73] suggests that such accuracy and robustness is achievable combining events and frames.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method or approach. The context focuses on the combination of events and frames for achieving accuracy and robustness in monocular cases.",
      "processing_time": 50.59523320198059,
      "citing_paper_id": "250918780",
      "cited_paper_id": 248227281
    },
    {
      "context_text": "1 on the SFM sequence 03 00 from the EVIMO2 dataset [68].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "EVIMO2"
      ],
      "dataset_descriptions": {
        "EVIMO2": "Used to evaluate event-based stereo depth estimation, specifically focusing on motion segmentation, optical flow, and structure from motion in indoor scenes with monocular or stereo algorithms."
      },
      "confidence_score": 1.0,
      "reasoning": "The citation mentions the EVIMO2 dataset, which is a specific, verifiable resource used for evaluating event-based stereo depth estimation.",
      "processing_time": 52.4629111289978,
      "citing_paper_id": "250918780",
      "cited_paper_id": 248572428
    },
    {
      "context_text": "We also test our method on sequences from the EVIMO2 dataset [68], recorded with a trinocular event-camera rig consisting of a Samsung DVS Gen3 [69] and two Prophesee CD Gen3 event cameras [70], all with 640×480 pix.",
      "catation_intent": [],
      "resource_type": [],
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.3,
      "reasoning": "JSON decode error: Expecting ',' delimiter: line 8 column 5 (char 504), response: {\n    \"reasoning\": \"The citation mentions the use of the EVIMO2 dataset for testing their method, wh",
      "processing_time": 52.45747518539429,
      "citing_paper_id": "250918780",
      "cited_paper_id": 248572428
    },
    {
      "context_text": "Such events may be removed during preprocessing [78].",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific dataset, only a method for event flicker removal. No dataset names are present in the citation context.",
      "processing_time": 49.97556686401367,
      "citing_paper_id": "250918780",
      "cited_paper_id": 248834516
    },
    {
      "context_text": "[37,57] were acquired with a handheld stereo or trinocular event camera in indoor environments.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific, verifiable datasets. It only describes the type of data acquisition method used.",
      "processing_time": 49.27737045288086,
      "citing_paper_id": "250918780",
      "cited_paper_id": null
    },
    {
      "context_text": "EVIMO2([57]) Samsung Gen3 640 480 trinocular 75",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "EVIMO2"
      ],
      "dataset_descriptions": {
        "EVIMO2": "Used to provide event-based visual data for stereo depth estimation, focusing on trinocular configurations and sensor fusion."
      },
      "confidence_score": 0.7,
      "reasoning": "The citation mentions 'EVIMO2' which appears to be a dataset or a collection of event-based visual data. However, there is no additional context to confirm its usage or relevance.",
      "processing_time": 52.445008516311646,
      "citing_paper_id": "250918780",
      "cited_paper_id": null
    },
    {
      "context_text": "We also test our method on sequences from the EVIMO2 dataset [71], recorded with a trinocular event-camera rig consisting of a Samsung DVS Gen3 [72] and two Prophesee CD Gen3 event cameras [73], all with 640×480 pix.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "EVIMO2"
      ],
      "dataset_descriptions": {
        "EVIMO2": "Used to test the method on sequences recorded with a trinocular event-camera rig, focusing on evaluating the stereo depth estimation performance with event-based cameras."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions the EVIMO2 dataset, which is a specific dataset used for testing the method described in the research. The dataset is clearly identified and used for evaluating the performance of the method.",
      "processing_time": 52.90627908706665,
      "citing_paper_id": "250918780",
      "cited_paper_id": null
    },
    {
      "context_text": "The EVIMO2 dataset allowed us to establish multi-camera (trinocular) depth estimation and during high-speed motion (which blurs regular frames).",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "EVIMO2"
      ],
      "dataset_descriptions": {
        "EVIMO2": "Used to establish multi-camera (trinocular) depth estimation during high-speed motion, addressing event-based stereo depth estimation and focusing on handling frame blur."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions the EVIMO2 dataset, which is used for multi-camera depth estimation during high-speed motion. This is a specific, verifiable dataset relevant to event-based stereo depth estimation.",
      "processing_time": 52.68133902549744,
      "citing_paper_id": "250918780",
      "cited_paper_id": null
    },
    {
      "context_text": "[59, 57] propose a cost function for the rotating stereo panorama setup in [7] based on temporal event difference.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method or approach for stereo vision using events. The context and titles do not provide specific dataset names.",
      "processing_time": 49.814722299575806,
      "citing_paper_id": "46937991",
      "cited_paper_id": 185541
    },
    {
      "context_text": "[59, 57] propose a cost function for the rotating stereo panorama setup in [7] based on temporal event difference.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method or approach for stereo vision using events. The context and titles do not provide specific dataset names.",
      "processing_time": 49.814722299575806,
      "citing_paper_id": "46937991",
      "cited_paper_id": 8415966
    },
    {
      "context_text": "[59, 57] propose a cost function for the rotating stereo panorama setup in [7] based on temporal event difference.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method or approach for stereo vision using events. The context and titles do not provide specific dataset names.",
      "processing_time": 49.814722299575806,
      "citing_paper_id": "46937991",
      "cited_paper_id": 21539113
    },
    {
      "context_text": "However, most of the recent event-based implementations of the cooperative algorithm do not consider depth gradients [47, 48, 23, 17].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to papers discussing event-based implementations of cooperative algorithms in stereo vision.",
      "processing_time": 49.735480070114136,
      "citing_paper_id": "46937991",
      "cited_paper_id": 6079544
    },
    {
      "context_text": "However, most of the recent event-based implementations of the cooperative algorithm do not consider depth gradients [47, 48, 23, 17].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to papers discussing event-based implementations of cooperative algorithms in stereo vision.",
      "processing_time": 49.735480070114136,
      "citing_paper_id": "46937991",
      "cited_paper_id": 34855834
    },
    {
      "context_text": "However, most of the recent event-based implementations of the cooperative algorithm do not consider depth gradients [47, 48, 23, 17].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to papers discussing event-based implementations of cooperative algorithms in stereo vision.",
      "processing_time": 49.735480070114136,
      "citing_paper_id": "46937991",
      "cited_paper_id": 121601380
    },
    {
      "context_text": "CNNs [35] have been used to learn stereo matching cost [66, 46].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets. It only refers to the use of CNNs for stereo matching cost, which is a methodological approach.",
      "processing_time": 50.19012928009033,
      "citing_paper_id": "46937991",
      "cited_paper_id": 6913648
    },
    {
      "context_text": "CNNs [35] have been used to learn stereo matching cost [66, 46].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets. It only refers to the use of CNNs for stereo matching cost, which is a methodological approach.",
      "processing_time": 50.19012928009033,
      "citing_paper_id": "46937991",
      "cited_paper_id": 14542261
    },
    {
      "context_text": "[7] use a rotating pair of event-based line (vertical) sensors in static scenes and render events from each rotation to an edge map [33], which is subsequently processed using a frame-based panoramic stereo algorithm [36].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and tools. The context describes a process involving event-based sensors and stereo algorithms, but no dataset names are provided.",
      "processing_time": 50.1092095375061,
      "citing_paper_id": "46937991",
      "cited_paper_id": 8415966
    },
    {
      "context_text": "While the successful artificial neural networks may not operate the same way as the brain, both of them utilize highly parallel and hierarchical architectures that gradually abstract input data to more meaningful concepts [8, 51, 16].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only references to papers discussing hierarchical models and neural network architectures.",
      "processing_time": 49.41675305366516,
      "citing_paper_id": "46937991",
      "cited_paper_id": 8920227
    },
    {
      "context_text": "The proposed method and its FPGA implementations [20, 19] are equivalent to the cooperative stereo algorithm [42] with noisy time difference inputs.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only algorithms and hardware implementations.",
      "processing_time": 48.92061519622803,
      "citing_paper_id": "46937991",
      "cited_paper_id": 11395394
    },
    {
      "context_text": "This consists of systems of equations defining the behavior of TrueNorth neurons, encased in modules called corelets [1], and the subsequent composition of the inputs and outputs of these modules.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method or system (TrueNorth neurons and corelets).",
      "processing_time": 49.536328077316284,
      "citing_paper_id": "46937991",
      "cited_paper_id": 11759366
    },
    {
      "context_text": "TrueNorth programs are written in the Corelet Programming Language — a hierarchical, compositional, object-oriented language [1].",
      "catation_intent": "none",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any datasets, only a programming language. There are no specific, verifiable datasets in the context.",
      "processing_time": 49.60955882072449,
      "citing_paper_id": "46937991",
      "cited_paper_id": 11759366
    },
    {
      "context_text": "For autonomous vehicles, drones, and satellites, energy consumption is a challenge [6].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general challenge related to energy consumption in autonomous vehicles, drones, and satellites.",
      "processing_time": 49.52967309951782,
      "citing_paper_id": "46937991",
      "cited_paper_id": 12248226
    },
    {
      "context_text": "Ground truth disparity maps from benchmark frame-based datasets [27, 54, 26, 43] are used to train these Figure 1.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.3,
      "reasoning": "The context mentions 'benchmark frame-based datasets' which are used to train models for disparity maps. However, no specific dataset names are provided in the citation span.",
      "processing_time": 50.24160647392273,
      "citing_paper_id": "46937991",
      "cited_paper_id": 12986049
    },
    {
      "context_text": "Ground truth disparity maps from benchmark frame-based datasets [27, 54, 26, 43] are used to train these Figure 1.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.3,
      "reasoning": "The context mentions 'benchmark frame-based datasets' which are used to train models for disparity maps. However, no specific dataset names are provided in the citation span.",
      "processing_time": 50.24160647392273,
      "citing_paper_id": "46937991",
      "cited_paper_id": 14915763
    },
    {
      "context_text": "Mahowald and Delbrück [41] implemented the Marr and Poggio cooperative stereo algorithm [42], a global approach, in an analog VLSI circuit.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method and its implementation.",
      "processing_time": 48.97291374206543,
      "citing_paper_id": "46937991",
      "cited_paper_id": 15077875
    },
    {
      "context_text": "Mahowald and Delbrück [41] implemented the Marr and Poggio cooperative stereo algorithm [42], a global approach, in an analog VLSI circuit.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method and its implementation.",
      "processing_time": 48.659475326538086,
      "citing_paper_id": "46937991",
      "cited_paper_id": 15077875
    },
    {
      "context_text": "[60, 58] propose DSP implementation of a spatiotemporal similarity method using two live event sensors [37].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and systems. The context is about implementing a spatiotemporal similarity method using event sensors, which does not indicate the use of a specific dataset.",
      "processing_time": 50.82503652572632,
      "citing_paper_id": "46937991",
      "cited_paper_id": 15357188
    },
    {
      "context_text": "[60, 58] propose DSP implementation of a spatiotemporal similarity method using two live event sensors [37].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and systems. The context is about implementing a spatiotemporal similarity method using event sensors, which does not indicate the use of a specific dataset.",
      "processing_time": 50.82503652572632,
      "citing_paper_id": "46937991",
      "cited_paper_id": 24236495
    },
    {
      "context_text": "[60, 58] propose DSP implementation of a spatiotemporal similarity method using two live event sensors [37].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and systems. The context is about implementing a spatiotemporal similarity method using event sensors, which does not indicate the use of a specific dataset.",
      "processing_time": 50.82503652572632,
      "citing_paper_id": "46937991",
      "cited_paper_id": 30913835
    },
    {
      "context_text": "Recently developed event-based cameras such as Dynamic Vision Sensor (DVS) [37, 10] and ATIS [50], inspired by the biological retina, encode pixel illumination changes as events.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only event-based cameras and their capabilities. No verifiable resources are identified.",
      "processing_time": 49.656230211257935,
      "citing_paper_id": "46937991",
      "cited_paper_id": 15357188
    },
    {
      "context_text": "Recently developed event-based cameras such as Dynamic Vision Sensor (DVS) [37, 10] and ATIS [50], inspired by the biological retina, encode pixel illumination changes as events.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only event-based cameras and their capabilities. No verifiable resources are identified.",
      "processing_time": 49.656230211257935,
      "citing_paper_id": "46937991",
      "cited_paper_id": 24007071
    },
    {
      "context_text": "Using a cluster of TrueNorth neurosynaptic processors, we demonstrate their ability to process bilateral event-based inputs streamed live by Dynamic Vision Sensors (DVS), at up to 2,000 disparity maps per second, producing high fidelity disparities which are in turn used to reconstruct, at low power, the depth of events produced from rapidly changing scenes.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only hardware and sensors. No verifiable resources are identified.",
      "processing_time": 49.26829552650452,
      "citing_paper_id": "46937991",
      "cited_paper_id": 15357188
    },
    {
      "context_text": "Using a cluster of TrueNorth neurosynaptic processors, we demonstrate their ability to process bilateral event-based inputs streamed live by Dynamic Vision Sensors (DVS), at up to 2,000 disparity maps per second, producing high fidelity disparities which are in turn used to reconstruct, at low power, the depth of events produced from rapidly changing scenes.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only hardware and sensors. No verifiable resources are identified.",
      "processing_time": 49.26829552650452,
      "citing_paper_id": "46937991",
      "cited_paper_id": 24007071
    },
    {
      "context_text": "The proposed event-based disparity method is implemented using a stereo pair of DAVIS sensors [10] (a version of DVS) and nine TrueNorth NS1e boards [53].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions hardware components (DAVIS sensors and TrueNorth NS1e boards) but does not refer to any specific datasets. The cited papers are about sensors, which are tools, not datasets.",
      "processing_time": 50.33517074584961,
      "citing_paper_id": "46937991",
      "cited_paper_id": 15357188
    },
    {
      "context_text": "The proposed event-based disparity method is implemented using a stereo pair of DAVIS sensors [10] (a version of DVS) and nine TrueNorth NS1e boards [53].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions hardware components (DAVIS sensors and TrueNorth NS1e boards) but does not refer to any specific datasets. The cited papers are about sensors, which are tools, not datasets.",
      "processing_time": 50.33517074584961,
      "citing_paper_id": "46937991",
      "cited_paper_id": 24007071
    },
    {
      "context_text": "A live-feed version of the system running on nine TrueNorth chips is shown to calculate 400 disparity maps per second, and the ability to increase this up to 2,000 disparities per second (subject to certain trade-offs) is demonstrated, for use with high speed event cameras, such as DVS.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only hardware and performance metrics. No verifiable resources are identified.",
      "processing_time": 49.13291788101196,
      "citing_paper_id": "46937991",
      "cited_paper_id": 15357188
    },
    {
      "context_text": "A live-feed version of the system running on nine TrueNorth chips is shown to calculate 400 disparity maps per second, and the ability to increase this up to 2,000 disparities per second (subject to certain trade-offs) is demonstrated, for use with high speed event cameras, such as DVS.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only hardware and performance metrics. No verifiable resources are identified.",
      "processing_time": 49.13291788101196,
      "citing_paper_id": "46937991",
      "cited_paper_id": 24007071
    },
    {
      "context_text": "When the data in a cycle is sparse, as is the case with a DVS sensor, most neurons would not compute for most of the time, resulting in low active power [44].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only discusses the sparsity of data from a DVS sensor. No verifiable resources are identified.",
      "processing_time": 49.536582231521606,
      "citing_paper_id": "46937991",
      "cited_paper_id": 15357188
    },
    {
      "context_text": "When the data in a cycle is sparse, as is the case with a DVS sensor, most neurons would not compute for most of the time, resulting in low active power [44].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only discusses the sparsity of data from a DVS sensor. No verifiable resources are identified.",
      "processing_time": 49.536582231521606,
      "citing_paper_id": "46937991",
      "cited_paper_id": 24007071
    },
    {
      "context_text": "Experiments on real-world sequences demonstrate the ability of the system to take full advantage of the asynchronous and sparse nature of DVS sensors for low power depth reconstruction, in environments where conventional frame-based cameras connected to synchronous processors would be inefficient for rapidly moving objects.",
      "catation_intent": "reusable resource",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only describes experiments using real-world sequences. No clear, verifiable dataset names are provided.",
      "processing_time": 49.490190744400024,
      "citing_paper_id": "46937991",
      "cited_paper_id": 15357188
    },
    {
      "context_text": "Experiments on real-world sequences demonstrate the ability of the system to take full advantage of the asynchronous and sparse nature of DVS sensors for low power depth reconstruction, in environments where conventional frame-based cameras connected to synchronous processors would be inefficient for rapidly moving objects.",
      "catation_intent": "reusable resource",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only describes experiments using real-world sequences. No clear, verifiable dataset names are provided.",
      "processing_time": 49.490190744400024,
      "citing_paper_id": "46937991",
      "cited_paper_id": 24007071
    },
    {
      "context_text": "The system is highly parameterized and can operate with other event based sensors such as ATIS [50] or DVS [37].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions event-based sensors ATIS and DVS, but does not indicate their use as datasets. These are likely hardware devices, not datasets.",
      "processing_time": 49.44774150848389,
      "citing_paper_id": "46937991",
      "cited_paper_id": 15357188
    },
    {
      "context_text": "The system is highly parameterized and can operate with other event based sensors such as ATIS [50] or DVS [37].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions event-based sensors ATIS and DVS, but does not indicate their use as datasets. These are likely hardware devices, not datasets.",
      "processing_time": 49.44774150848389,
      "citing_paper_id": "46937991",
      "cited_paper_id": 24007071
    },
    {
      "context_text": "[52, 14] propose to use event-toevent constraints for calculating matching cost, such as time window, distance to the epipolar line, ordering constraint, and polarity.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and approaches for event-based stereo depth estimation.",
      "processing_time": 48.66825318336487,
      "citing_paper_id": "46937991",
      "cited_paper_id": 16160208
    },
    {
      "context_text": "The main advantages of the proposed method, compared to the related work [17, 49, 45, 52, 57], are simultaneous end-to-end neuromorphic disparity calculation, low power, high throughput, low latency (9-11 ms), and linear scalability to multiple neuromorphic processors for larger input sizes.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods and performance metrics. The cited paper titles do not provide additional context about datasets.",
      "processing_time": 49.123867988586426,
      "citing_paper_id": "46937991",
      "cited_paper_id": 21539113
    },
    {
      "context_text": "The main advantages of the proposed method, compared to the related work [17, 49, 45, 52, 57], are simultaneous end-to-end neuromorphic disparity calculation, low power, high throughput, low latency (9-11 ms), and linear scalability to multiple neuromorphic processors for larger input sizes.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods and performance metrics. The cited paper titles do not provide additional context about datasets.",
      "processing_time": 49.123867988586426,
      "citing_paper_id": "46937991",
      "cited_paper_id": 34855834
    },
    {
      "context_text": "Local methods can be parallelized and find corresponding events using either local features over a spatiotemporal window or event-to-event features [13, 58, 52, 32, 57].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only methods and approaches for event-driven stereo systems. No verifiable resources are identified.",
      "processing_time": 48.932234048843384,
      "citing_paper_id": "46937991",
      "cited_paper_id": 21539113
    },
    {
      "context_text": "Local methods can be parallelized and find corresponding events using either local features over a spatiotemporal window or event-to-event features [13, 58, 52, 32, 57].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only methods and approaches for event-driven stereo systems. No verifiable resources are identified.",
      "processing_time": 48.932234048843384,
      "citing_paper_id": "46937991",
      "cited_paper_id": 24236495
    },
    {
      "context_text": "To benefit from sparse and asynchronous computation, neuromorphic processors have been developed [44, 24, 30, 9, 56].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only references to neuromorphic processors and their development.",
      "processing_time": 48.414011001586914,
      "citing_paper_id": "46937991",
      "cited_paper_id": 22330500
    },
    {
      "context_text": "Our implementation uses a pair of synchronized DAVIS240C cameras [10], connected via Ethernet to a cluster of TrueNorth NS1e boards (Fig.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only hardware components. The citation is for a camera sensor, which is not a dataset.",
      "processing_time": 48.41206073760986,
      "citing_paper_id": "46937991",
      "cited_paper_id": 24007071
    },
    {
      "context_text": "[17] use six SpiNNaker [24] processor boards to implement the cooperative network for 106 × 106 pixels of stereo event data.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions 'stereo event data' but does not specify a dataset name. The context is about hardware implementation and latency, not a specific dataset.",
      "processing_time": 48.35833978652954,
      "citing_paper_id": "46937991",
      "cited_paper_id": 34855834
    },
    {
      "context_text": "The implemented neuromorphic stereo disparity system achieves these advantages, while consuming ∼ 200× less power per pixel per disparity map compared to the stateof-the-art [17].",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a comparison to the state-of-the-art in terms of power consumption.",
      "processing_time": 48.03681945800781,
      "citing_paper_id": "46937991",
      "cited_paper_id": 34855834
    },
    {
      "context_text": "Most global methods [40, 17, 49, 45] are derived from the Marr and Poggio cooperative stereo algorithm [42].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only references to algorithms and methods.",
      "processing_time": 47.215940713882446,
      "citing_paper_id": "46937991",
      "cited_paper_id": 34855834
    },
    {
      "context_text": "With respect to the most relevant state-of-the-art approach [17], our method uses ∼ 200× less power per pixel per disparity map.",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a comparison of power consumption. No verifiable resources are identified.",
      "processing_time": 47.731971740722656,
      "citing_paper_id": "46937991",
      "cited_paper_id": 34855834
    },
    {
      "context_text": "The algorithm converges well when object surfaces are fronto-parallel and candidate matches injected to the network are close together [40, 17].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only discusses the performance of an algorithm under certain conditions.",
      "processing_time": 47.375242710113525,
      "citing_paper_id": "46937991",
      "cited_paper_id": 34855834
    },
    {
      "context_text": "models, followed by sparse-to-dense conversions [18, 5].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models and methods. The context is too limited to infer the use of any datasets.",
      "processing_time": 47.73655605316162,
      "citing_paper_id": "46937991",
      "cited_paper_id": 35157264
    },
    {
      "context_text": "For example, the TrueNorth neuromorphic chip [44] has been used for high throughput Convolutional neural networks (CNNs) [22], character recognition [53], optic flow [11], saliency [3], and gesture recognition [2].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only applications of the TrueNorth neuromorphic chip. No dataset names are present in the citation span.",
      "processing_time": 47.91357898712158,
      "citing_paper_id": "46937991",
      "cited_paper_id": 45560068
    },
    {
      "context_text": "Frame-based stereo disparity methods calculate matching cost using a spatial similarity metric [25, 27, 29] or a cost function learned from a dataset (see reviews [62, 55, 34, 63]).",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.2,
      "reasoning": "The context mentions 'a dataset' but does not provide a specific name. The reference is too generic and lacks a clear identifier.",
      "processing_time": 47.56089472770691,
      "citing_paper_id": "46937991",
      "cited_paper_id": 195859047
    },
    {
      "context_text": "For the new arrival event streams, the existing event-corner features are firstly temporally tracked by the LK optical approach [20] and then spatially matched in left and right event streams.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (LK optical approach) for tracking and matching events in stereo vision.",
      "processing_time": 47.44783329963684,
      "citing_paper_id": "255125395",
      "cited_paper_id": 2121536
    },
    {
      "context_text": "What’s more, as can be seen from the video record of the evaluation using our ESVIO (take the school-scooter 3 in Vector and indoor flying 1 4 in MVSEC [7] as examples).",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "MVSEC"
      ],
      "dataset_descriptions": {
        "MVSEC": "Used to evaluate the ESVIO method for event-based stereo depth estimation, focusing on 3D perception tasks with event cameras."
      },
      "confidence_score": 1.0,
      "reasoning": "The citation mentions 'MVSEC' which is a dataset used for 3D perception with event cameras. The context indicates it is used for evaluating the ESVIO method.",
      "processing_time": 49.505733489990234,
      "citing_paper_id": "255125395",
      "cited_paper_id": 3416874
    },
    {
      "context_text": "In Section IV-B, we compare our methods with other event-based and image-based methods on two publicly available datasets: MVSEC [23] and VECtor [24].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "MVSEC",
        "VECtor"
      ],
      "dataset_descriptions": {
        "MVSEC": "Used to compare event-based and image-based methods for stereo depth estimation, focusing on 3D perception tasks.",
        "VECtor": "Used to evaluate multi-sensor SLAM performance, providing a versatile benchmark for event-centric methods."
      },
      "confidence_score": 1.0,
      "reasoning": "The citation mentions two specific datasets, MVSEC and VECtor, which are relevant to the research topic of event-based stereo depth estimation.",
      "processing_time": 50.14387083053589,
      "citing_paper_id": "255125395",
      "cited_paper_id": 3416874
    },
    {
      "context_text": "In Section IV-B, we compare our methods with other event-based and image-based methods on two publicly available datasets: MVSEC [23] and VECtor [24].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "MVSEC",
        "VECtor"
      ],
      "dataset_descriptions": {
        "MVSEC": "Used to compare event-based and image-based methods for stereo depth estimation, focusing on 3D perception tasks.",
        "VECtor": "Used to evaluate multi-sensor SLAM performance, providing a versatile benchmark for event-centric methods."
      },
      "confidence_score": 1.0,
      "reasoning": "The citation mentions two specific datasets, MVSEC and VECtor, which are relevant to the research topic of event-based stereo depth estimation.",
      "processing_time": 50.14387083053589,
      "citing_paper_id": "255125395",
      "cited_paper_id": 250127779
    },
    {
      "context_text": "In subsection IV-B, we compare our methods with other event-based and image-based methods on two publicly available datasets: MVSEC [23] and VECtor [24].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "MVSEC",
        "VECtor"
      ],
      "dataset_descriptions": {
        "MVSEC": "Used to compare event-based and image-based stereo depth estimation methods, focusing on 3D perception accuracy and robustness.",
        "VECtor": "Used to evaluate multi-sensor SLAM techniques, specifically comparing event-based and image-based methods for depth estimation."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions two specific datasets, MVSEC and VECtor, which are relevant to the research topic of event-based stereo depth estimation.",
      "processing_time": 50.19028854370117,
      "citing_paper_id": "255125395",
      "cited_paper_id": 3416874
    },
    {
      "context_text": "In subsection IV-B, we compare our methods with other event-based and image-based methods on two publicly available datasets: MVSEC [23] and VECtor [24].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "MVSEC",
        "VECtor"
      ],
      "dataset_descriptions": {
        "MVSEC": "Used to compare event-based and image-based stereo depth estimation methods, focusing on 3D perception accuracy and robustness.",
        "VECtor": "Used to evaluate multi-sensor SLAM techniques, specifically comparing event-based and image-based methods for depth estimation."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions two specific datasets, MVSEC and VECtor, which are relevant to the research topic of event-based stereo depth estimation.",
      "processing_time": 50.19028854370117,
      "citing_paper_id": "255125395",
      "cited_paper_id": 250127779
    },
    {
      "context_text": "For the MVSEC [23], we select the sequence captured in the indoor flying room.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "MVSEC"
      ],
      "dataset_descriptions": {
        "MVSEC": "Used to select a specific sequence from the dataset for experiments in the indoor flying room, focusing on 3D perception with event cameras."
      },
      "confidence_score": 1.0,
      "reasoning": "The citation mentions 'MVSEC' which is a specific dataset used for 3D perception with event cameras. The context indicates that a particular sequence from this dataset is selected for experiments.",
      "processing_time": 49.22510838508606,
      "citing_paper_id": "255125395",
      "cited_paper_id": 3416874
    },
    {
      "context_text": "Available: https://github.com/MichaelGrupp /evo [26] C. Campos, R. Elvira, J. J. G. Rodríguez, J. M. Montiel, and J. D. Tars, “ORB-SLAM3: An accurate open-source library for visual, visual– inertial, and multimap SLAM,” IEEE Trans.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (ORB-SLAM3) and a repository (evo). The context is about visual and inertial SLAM, which is related to the topic but does not specify any datasets.",
      "processing_time": 48.295116901397705,
      "citing_paper_id": "255125395",
      "cited_paper_id": 3738244
    },
    {
      "context_text": "While the traditional event-based methods [2], [3], [5] failed in most of the sequences in these two datasets.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.0,
      "reasoning": "The context mentions 'two datasets' but does not provide specific names. The cited paper title does not help disambiguate any dataset names.",
      "processing_time": 47.22056531906128,
      "citing_paper_id": "255125395",
      "cited_paper_id": 3738244
    },
    {
      "context_text": "Index Terms—Visual-Inertial SLAM, sensor fusion, aerial systems: perception and autonomy.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general terms and concepts related to visual-inertial SLAM and sensor fusion.",
      "processing_time": 46.99060869216919,
      "citing_paper_id": "255125395",
      "cited_paper_id": 3738244
    },
    {
      "context_text": "Available: https://github.\ncom/arclab-hku/Event_based_VO-VIO-SLAM/tree/main/ES VIO/supply.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only a GitHub repository. The title suggests a combination of events, images, and IMU data, but no specific dataset names are provided.",
      "processing_time": 47.12236714363098,
      "citing_paper_id": "255125395",
      "cited_paper_id": 3738244
    },
    {
      "context_text": "[3] A. R. Vidal, H. Rebecq, T. Horstschaefer, and D. Scaramuzza, “Ultimate SLAM? combining events, images, and IMU for robust visual SLAM in HDR and high-speed scenarios,” IEEE Robot.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and approaches. The focus is on combining events, images, and IMU for SLAM.",
      "processing_time": 46.667606592178345,
      "citing_paper_id": "255125395",
      "cited_paper_id": 3738244
    },
    {
      "context_text": "[24] L. Gao et al., “VECtor: A versatile event-centric benchmark for multisensor SLAM,” IEEE Robot.",
      "catation_intent": "reusable resource",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method or benchmark called VECtor. No clear dataset names are provided.",
      "processing_time": 46.27825403213501,
      "citing_paper_id": "255125395",
      "cited_paper_id": 3738244
    },
    {
      "context_text": "Due to motion blur, both ORB-SLAM3 and VINS-Fusion fail to extract reliable features in hku_agg_walk sequence, resulting in system failure.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "hku_agg_walk"
      ],
      "dataset_descriptions": {
        "hku_agg_walk": "Used to test the robustness of ORB-SLAM3 and VINS-Fusion in handling motion blur, focusing on feature extraction reliability in high-speed scenarios."
      },
      "confidence_score": 0.6,
      "reasoning": "The context mentions 'hku_agg_walk sequence' which appears to be a specific dataset or sequence used for testing SLAM systems. However, the name does not follow the typical naming conventions for datasets and lacks clear provenance.",
      "processing_time": 49.001389265060425,
      "citing_paper_id": "255125395",
      "cited_paper_id": 3738244
    },
    {
      "context_text": "Most of the existing event-based visual odometers (VO) use monocular event camera [2], [3], [4], while few research on visual odometry based on stereo event cameras [5], [6].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only references to research papers and methods. No verifiable resources are identified.",
      "processing_time": 46.03628349304199,
      "citing_paper_id": "255125395",
      "cited_paper_id": 3738244
    },
    {
      "context_text": "Most of the existing event-based visual odometers (VO) use monocular event camera [2], [3], [4], while few research on visual odometry based on stereo event cameras [5], [6].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only references to research papers and methods. No verifiable resources are identified.",
      "processing_time": 46.03628349304199,
      "citing_paper_id": "255125395",
      "cited_paper_id": 235794981
    },
    {
      "context_text": "As for the MRE evaluation criterion, our ESVIO shows significant improvement compared to other advanced algorithms, e.g. the average MRE of ESVIO is 0.033◦/m while the value of ORB-SLAM3 is 0.12◦/m.",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only performance metrics of algorithms. No dataset names are present in the citation span.",
      "processing_time": 46.03955674171448,
      "citing_paper_id": "255125395",
      "cited_paper_id": 3738244
    },
    {
      "context_text": "In the hku_dark_normal sequence, ORB-SLAM3 cannot extract any feature due to poor light conditions (more qualitative details is in the supplementary material).",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.3,
      "reasoning": "The context mentions a specific sequence 'hku_dark_normal' which appears to be part of a dataset used for evaluating ORB-SLAM3 under poor lighting conditions. However, the name does not follow the typical naming conventions for datasets and lacks clear provenance.",
      "processing_time": 47.01771116256714,
      "citing_paper_id": "255125395",
      "cited_paper_id": 3738244
    },
    {
      "context_text": "[25] M. Grupp, “EVO: Python package for the evaluation of odometry and SLAM,” 2017.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific dataset, only a Python package for evaluating odometry and SLAM. The context is about a method or tool, not a dataset.",
      "processing_time": 46.044382095336914,
      "citing_paper_id": "255125395",
      "cited_paper_id": 3738244
    },
    {
      "context_text": "Available: https://github.com/arclabhku/Event_based_VO-VIO-SLAM.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only a GitHub repository for code. No verifiable datasets are referenced.",
      "processing_time": 45.55670952796936,
      "citing_paper_id": "255125395",
      "cited_paper_id": 3738244
    },
    {
      "context_text": "Although the MPE criterion of ORB-SLAM3 is slightly better than ours in some sequences (e.g. robot-normal, desk-normal, mountain-normal), our ESVIO provides more reliable and accurate results in most of the sequences under harsh situations with HDR or aggressive motion.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a comparison of methods (ORB-SLAM3 and ESVIO) in various sequences. No clear, verifiable dataset names are provided.",
      "processing_time": 46.216771841049194,
      "citing_paper_id": "255125395",
      "cited_paper_id": 3738244
    },
    {
      "context_text": "However, most of the recent research on stereo event cameras has focused on depth estimation and constructing semi-dense or dense maps [16], [17], with less research on VO/SLAM fields.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only general research areas and methods. No verifiable resources are identified.",
      "processing_time": 45.525542974472046,
      "citing_paper_id": "255125395",
      "cited_paper_id": 3738244
    },
    {
      "context_text": "However, most of the recent research on stereo event cameras has focused on depth estimation and constructing semi-dense or dense maps [16], [17], with less research on VO/SLAM fields.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only general research areas and methods. No verifiable resources are identified.",
      "processing_time": 45.525542974472046,
      "citing_paper_id": "255125395",
      "cited_paper_id": 249980412
    },
    {
      "context_text": "Our ESIO has good performance, especially for the sequence hku_agg_walk and hku_dark_normal, our ESIO still can produce reliable and accurate pose estimation even when the state-of-the-art image-based VIO method, ORB-SLAM3, fails.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.3,
      "reasoning": "The context mentions specific sequences (hku_agg_walk and hku_dark_normal) but does not provide enough information to determine if these are datasets or specific scenarios within a larger dataset. No clear dataset names are provided.",
      "processing_time": 46.37019991874695,
      "citing_paper_id": "255125395",
      "cited_paper_id": 3738244
    },
    {
      "context_text": "Observing this complementarity, leveraging both of the advantages of the aforementioned different sensors in combination with an inertial measurement unit (IMU) results in a robust and accurate visual-inertial odometry (VIO) pipeline [3], [4].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and systems. The context focuses on combining different sensors for visual-inertial odometry.",
      "processing_time": 45.71279811859131,
      "citing_paper_id": "255125395",
      "cited_paper_id": 3738244
    },
    {
      "context_text": "Ultimate SLAM [3] furthered the aforementioned research by combining event streams, image frames, and IMU measurements with nonlinear optimization, which leverages the complementary advantages of event cameras and standard cameras.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and approaches. The focus is on the combination of event streams, image frames, and IMU measurements.",
      "processing_time": 45.50125002861023,
      "citing_paper_id": "255125395",
      "cited_paper_id": 3738244
    },
    {
      "context_text": "[11] adopted a continuous-time framework based on cubic spline for smooth trajectory estimation and fused both event streams and IMU together.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for fusing event streams and IMU data.",
      "processing_time": 44.85922908782959,
      "citing_paper_id": "255125395",
      "cited_paper_id": 9729856
    },
    {
      "context_text": "[14] D. Gehrig, H. Rebecq, G. Gallego, and D. Scaramuzza, “EKLT: Asynchronous photometric feature tracking using events and frames,” Int.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (EKLT) for asynchronous photometric feature tracking using events and frames.",
      "processing_time": 45.4421751499176,
      "citing_paper_id": "255125395",
      "cited_paper_id": 50775406
    },
    {
      "context_text": "EKLT-VIO [13] integrated an accurate state-of-the-art event-based feature tracker EKLT [14] with EKF backend to achieve event-based state estimation on Mars-like datasets.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.3,
      "reasoning": "The citation mentions 'Mars-like datasets' but does not provide a specific name. The term 'Mars-like' is too generic and lacks a specific identifier.",
      "processing_time": 45.587456941604614,
      "citing_paper_id": "255125395",
      "cited_paper_id": 50775406
    },
    {
      "context_text": "EVENT cameras are novel bio-inspired sensors [1], which have a high dynamic range (140 dB compared to 60 dB of standard cameras) to handle broad illumination conditions.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only describes properties of event cameras.",
      "processing_time": 44.65015244483948,
      "citing_paper_id": "255125395",
      "cited_paper_id": 118684904
    },
    {
      "context_text": "[10] obtains a discrete number of states based on a spatio-temporal window of event streams, and introduces virtual event frames to achieve nonlinear optimization that refines estimated poses.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for processing event streams. The cited paper title confirms this is about a method, not a dataset.",
      "processing_time": 45.33832573890686,
      "citing_paper_id": "255125395",
      "cited_paper_id": 220713377
    },
    {
      "context_text": "Sequence ORB-SLAM3 [26] Stereo VIO VINS-Fusion [22] Stereo VIO USLAM [10] Mono EIO USLAM [3] Mono EVIO PL-EVIO [4] Mono EVIO Our ESIO Stereo EIO Our ESIO+ Stereo EIO Our ESVIO Stereo EVIO",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods and systems. There are no clear identifiers for datasets within the provided context.",
      "processing_time": 45.18274188041687,
      "citing_paper_id": "255125395",
      "cited_paper_id": 220713377
    },
    {
      "context_text": "[28] M. Gehrig, W. Aarents, D. Gehrig, and D. Scaramuzza, “DSEC: A stereo event camera dataset for driving scenarios,” IEEE Robot.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "DSEC"
      ],
      "dataset_descriptions": {
        "DSEC": "Used to provide stereo event camera data for driving scenarios, focusing on depth estimation and event-based vision algorithms in dynamic environments."
      },
      "confidence_score": 1.0,
      "reasoning": "The citation clearly mentions 'DSEC', which is a specific dataset for stereo event camera data in driving scenarios.",
      "processing_time": 46.31332182884216,
      "citing_paper_id": "255125395",
      "cited_paper_id": 232170230
    },
    {
      "context_text": "1) DSEC Dataset: DSEC [28] is collected by high-resolution stereo event cameras (640 × 480) under driving scenarios, which is challenging for event-based sensors, as forward motions typically produce considerably fewer events at the center.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "DSEC"
      ],
      "dataset_descriptions": {
        "DSEC": "Used to evaluate event-based stereo depth estimation methods in driving scenarios, focusing on the challenges posed by forward motion and sparse event generation at the image center."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions a specific dataset, DSEC, which is used for driving scenarios with stereo event cameras. The dataset is described as challenging for event-based sensors.",
      "processing_time": 46.97143316268921,
      "citing_paper_id": "255125395",
      "cited_paper_id": 232170230
    },
    {
      "context_text": "Since the DSEC dataset does not provide the ground truth 6-DoF poses, we only show the estimated trajectory and the tracking performance of our event-based and image-based features.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "DSEC"
      ],
      "dataset_descriptions": {
        "DSEC": "Used to evaluate the performance of event-based stereo depth estimation methods, focusing on tracking performance in driving scenarios without ground truth 6-DoF poses."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions the DSEC dataset, which is used for evaluating the tracking performance of event-based and image-based features in driving scenarios.",
      "processing_time": 46.78467416763306,
      "citing_paper_id": "255125395",
      "cited_paper_id": 232170230
    },
    {
      "context_text": "[6] adopted stereo feature detection and matching with the geometry method, which adopts reprojection error minimization to achieve pose estimation.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and approaches. There are no clear identifiers for datasets in the provided context.",
      "processing_time": 45.01114892959595,
      "citing_paper_id": "255125395",
      "cited_paper_id": 235794981
    },
    {
      "context_text": "The VECtor [24] dataset consists of a hardwaresynchronized sensor suite that includes stereo event cameras, stereo standard cameras, an RGB-D sensor, a LiDAR, and an IMU.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "VECtor"
      ],
      "dataset_descriptions": {
        "VECtor": "Used to evaluate event-based stereo depth estimation methods, providing synchronized data from stereo event cameras, standard cameras, RGB-D sensors, LiDAR, and IMU."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions the VECtor dataset, which is a multi-sensor dataset including stereo event cameras, suitable for event-based stereo depth estimation.",
      "processing_time": 46.84071683883667,
      "citing_paper_id": "255125395",
      "cited_paper_id": 250127779
    },
    {
      "context_text": "The VECtor [24] dataset consists of a hardware-synchronized sensor suite that includes stereo event cameras, stereo standard cameras, an RGB-D sensor, a LiDAR, and an IMU.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "VECtor"
      ],
      "dataset_descriptions": {
        "VECtor": "The dataset is used for event-based stereo depth estimation, providing synchronized data from stereo event cameras, standard cameras, RGB-D sensors, LiDAR, and IMU."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions the VECtor dataset, which is a multi-sensor dataset including stereo event cameras, suitable for event-based stereo depth estimation.",
      "processing_time": 46.70482563972473,
      "citing_paper_id": "255125395",
      "cited_paper_id": 250127779
    },
    {
      "context_text": "The accuracy is measured with mean position error (MPE, %) and mean rotation error (MRE, ◦/m) aligning the estimated trajectory with ground truth using 6-DOF transformation (in SE3), which is calculated by the tool [25].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific dataset, only tools and metrics. No dataset names are present in the citation span.",
      "processing_time": 44.446436405181885,
      "citing_paper_id": "255125395",
      "cited_paper_id": null
    },
    {
      "context_text": "While the traditional event-based methods [2] [3] [5] failed in most of the sequences in these two datasets.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.0,
      "reasoning": "The context mentions 'two datasets' but does not provide specific names. The reference is too generic and lacks the necessary detail to identify specific datasets.",
      "processing_time": 44.7194390296936,
      "citing_paper_id": "255125395",
      "cited_paper_id": null
    },
    {
      "context_text": "Note that we also evaluate EVO [2] and ESVO [5] in our self-collected datasets, but they failed in all sequences.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions 'self-collected datasets' but does not provide specific names or identifiers. The datasets are used for evaluating EVO and ESVO, which failed in all sequences.",
      "processing_time": 45.01306962966919,
      "citing_paper_id": "255125395",
      "cited_paper_id": null
    },
    {
      "context_text": "EVO [2] proposed a monocular event-based parallel tracking-and-mapping philosophy which applies the image-to-model alignment for tracking and Event-based Multi-View Stereo (EMVS) [8] for mapping.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and approaches. There are no clear identifiers for datasets in the provided context.",
      "processing_time": 44.461480379104614,
      "citing_paper_id": "255125395",
      "cited_paper_id": null
    },
    {
      "context_text": "Besides, we emphasize real-time performance when evaluating our methods, while the computational burden of EVO [2] and ESVO [3] is so large that we had to slow down the rosbag such as × 0 .",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods (EVO and ESVO) and a general reference to rosbags, which are not considered datasets.",
      "processing_time": 44.6823832988739,
      "citing_paper_id": "255125395",
      "cited_paper_id": null
    },
    {
      "context_text": "Most of the existing event-based visual odometers (VO) use monocular event camera [2]–[4], while few research on visual odometry based on stereo event cameras [5] [6].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only references to research papers and general methods.",
      "processing_time": 43.89927935600281,
      "citing_paper_id": "255125395",
      "cited_paper_id": null
    },
    {
      "context_text": "However, the generalization capability of [2] and [5] is slightly poor.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, models, or methods. It only comments on the generalization capability of two unspecified works.",
      "processing_time": 44.417490005493164,
      "citing_paper_id": "255125395",
      "cited_paper_id": null
    },
    {
      "context_text": "Yet, some event-based datasets for stereo vision have been recently released (Andreopoulos et al., 2018; Zhu et al., 2018).",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Multivehicle Stereo Event Camera Dataset"
      ],
      "dataset_descriptions": {
        "Multive": "Used to evaluate event-based stereo depth estimation algorithms, focusing on 3D perception in dynamic environments with high-speed events."
      },
      "confidence_score": 0.9,
      "reasoning": "The context mentions 'event-based datasets for stereo vision' and cites two papers, one of which introduces a specific dataset.",
      "processing_time": 46.096721172332764,
      "citing_paper_id": "226308033",
      "cited_paper_id": 3416874
    },
    {
      "context_text": "Yet, some event-based datasets for stereo vision have been recently released (Andreopoulos et al., 2018; Zhu et al., 2018).",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Multivehicle Stereo Event Camera Dataset"
      ],
      "dataset_descriptions": {
        "Multive": "Used to evaluate event-based stereo depth estimation algorithms, focusing on 3D perception in dynamic environments with high-speed events."
      },
      "confidence_score": 0.9,
      "reasoning": "The context mentions 'event-based datasets for stereo vision' and cites two papers, one of which introduces a specific dataset.",
      "processing_time": 46.096721172332764,
      "citing_paper_id": "226308033",
      "cited_paper_id": 46937991
    },
    {
      "context_text": "This implies solving the stereo-matching problem, i.e., ﬁnding correspondent points in two slightly shifted views (Cumming and Parker, 1997).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a reference to a method or approach in stereo matching.",
      "processing_time": 44.12735986709595,
      "citing_paper_id": "226308033",
      "cited_paper_id": 4417488
    },
    {
      "context_text": "As in biological brains, AMPA synaptic currents can boost the effect of slow NMDA synapses when both synaptic inputs are close in time (González, 2011).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only discusses biological mechanisms and computational neuroscience concepts.",
      "processing_time": 43.95146322250366,
      "citing_paper_id": "226308033",
      "cited_paper_id": 6398909
    },
    {
      "context_text": "However, current machine-vision approaches still lag behind their biological counterpart mainly in terms of bandwidth and power consumption (Tippetts et al., 2016; Steffen et al., 2019).",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only general limitations of machine-vision approaches.",
      "processing_time": 43.76620364189148,
      "citing_paper_id": "226308033",
      "cited_paper_id": 8688550
    },
    {
      "context_text": "The DYNAP chips proposed in Moradi et al. (2018) could be integrated into a system comprising a much higher number of cores [e.g., the IBM TrueNorth chip has 4,096 cores (Merolla et al., 2014), and the Intel Loihi chip has 128 cores (Davies et al., 2018)] without making any changes to the design.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only hardware components and their core counts. No verifiable resources are discussed.",
      "processing_time": 44.0524218082428,
      "citing_paper_id": "226308033",
      "cited_paper_id": 12706847
    },
    {
      "context_text": "Events are sent separately from both retinas to an FPGA using the AER protocol.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a communication protocol (AER) used in neuromorphic systems. No verifiable dataset names are present.",
      "processing_time": 44.4752721786499,
      "citing_paper_id": "226308033",
      "cited_paper_id": 14878668
    },
    {
      "context_text": "Inspired by the biological photoreceptors, the neuromorphic pixels operate independently and send out asynchronous events in response to signiﬁcant changes in illumination using an event-based data protocol Address Event Representation (AER) (Deiss et al., 1998).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a data protocol (AER). No verifiable resources are identified.",
      "processing_time": 44.32236170768738,
      "citing_paper_id": "226308033",
      "cited_paper_id": 14878668
    },
    {
      "context_text": "A pre-processing element (PEL) reduces the input resolution to a 16 × 16 array to redirect the AER events to the destination core on the neuromorphic processor.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a pre-processing element and neuromorphic processor details.",
      "processing_time": 43.92702007293701,
      "citing_paper_id": "226308033",
      "cited_paper_id": 14878668
    },
    {
      "context_text": "Digital peripheral asynchronous input/output logic circuits are used to receive and transmit spikes via an AER communication protocol, analogous to the one used for the event-based input stream.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only describes a communication protocol for neuromorphic systems.",
      "processing_time": 43.921961545944214,
      "citing_paper_id": "226308033",
      "cited_paper_id": 14878668
    },
    {
      "context_text": "Indeed, in this scenario, binocular time series related to diﬀerent stimuli are perfectly synchronized ( Supplementary Figure 8A ) and therefore not distinguishable from the true targets in the temporal domain (Mulansky and Kreuz, 2016).",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for analyzing spike train synchrony. No dataset names are present in the citation context.",
      "processing_time": 44.50866413116455,
      "citing_paper_id": "226308033",
      "cited_paper_id": 14992086
    },
    {
      "context_text": "Each core comprises 256 adaptive exponential integrate-and-fire (AEI&F) silicon neurons that emulate the biophysics of their biological counterpart, and four different dedicated analog circuits that mimic fast and slow excitatory/inhibitory synapse types (Brette and Gerstner, 2005).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a model and hardware implementation details.",
      "processing_time": 43.80279731750488,
      "citing_paper_id": "226308033",
      "cited_paper_id": 16085316
    },
    {
      "context_text": "…ﬁeld of event-based stereo vision has shown successful approaches that interface Spiking Neural Networks (SNNs) with neuromorphic event-based sensors, also referred to as “event cameras,” in order to build real-time event-based visual processing systems (Mahowald, 1994a; Osswald et al., 2017).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general concepts and methods. No verifiable resources are identified.",
      "processing_time": 43.908653020858765,
      "citing_paper_id": "226308033",
      "cited_paper_id": 22922570
    },
    {
      "context_text": "…ﬁeld of event-based stereo vision has shown successful approaches that interface Spiking Neural Networks (SNNs) with neuromorphic event-based sensors, also referred to as “event cameras,” in order to build real-time event-based visual processing systems (Mahowald, 1994a; Osswald et al., 2017).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general concepts and methods. No verifiable resources are identified.",
      "processing_time": 43.908653020858765,
      "citing_paper_id": "226308033",
      "cited_paper_id": 60778343
    },
    {
      "context_text": "(2018) proposed the first fully end-to-end stereo pipeline, implemented on multiple TrueNorth processors (Sawada et al., 2016).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (TrueNorth processors) and a research work. The context is about implementing a stereo pipeline using TrueNorth processors.",
      "processing_time": 44.498586654663086,
      "citing_paper_id": "226308033",
      "cited_paper_id": 33697116
    },
    {
      "context_text": "More recently, Andreopoulos et al. (2018) proposed the first fully end-to-end stereo pipeline, implemented on multiple TrueNorth processors (Sawada et al., 2016).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (TrueNorth processors) and a research work. The context focuses on the implementation of a stereo pipeline using TrueNorth processors.",
      "processing_time": 44.62544918060303,
      "citing_paper_id": "226308033",
      "cited_paper_id": 33697116
    },
    {
      "context_text": "More recently, Andreopoulos et al. (2018) proposed the ﬁrst fully end-to-end stereo pipeline, implemented on multiple TrueNorth processors (Sawada et al., 2016).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method and hardware implementation.",
      "processing_time": 43.374542236328125,
      "citing_paper_id": "226308033",
      "cited_paper_id": 46937991
    },
    {
      "context_text": "For this scenario, we assumed as true matches the stereo correspondences detected with generalized time-based technique (Ieng et al., 2018) with spatial, temporal, and motion consistency used as matching constraints 1 .",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific dataset names, only a method or technique. The context focuses on the use of a generalized time-based technique for stereo correspondences, which is not a dataset.",
      "processing_time": 44.65475153923035,
      "citing_paper_id": "226308033",
      "cited_paper_id": 49554392
    },
    {
      "context_text": "This excitatory-inhibitory balance allows integrating the stimulus spatiotemporal features over time, thereby implementing the matching constraints of cooperative algorithms (Marr and Poggio, 1976; Mahowald, 1994b; Osswald et al., 2017).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only references to methods and theories. No verifiable resources are identified.",
      "processing_time": 43.98561215400696,
      "citing_paper_id": "226308033",
      "cited_paper_id": 60778343
    },
    {
      "context_text": "However, although several biologically-inspired implementations of stereo vision (Mahowald, 1994b; Piatkowska et al., 2013, 2017; Dikov et al., 2017; Osswald et al., 2017; Kaiser et al., 2018) have extensively been explored, only a few solutions fully exploit the advantages of parallel computation,…",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only references to biologically-inspired implementations of stereo vision. No clear, verifiable datasets are identified.",
      "processing_time": 44.031211614608765,
      "citing_paper_id": "226308033",
      "cited_paper_id": 60778343
    },
    {
      "context_text": "Moreover, the need for compelling benchmarks that could show the advantages of spike-based computation in real-world scenarios is currently one of the major challenges for the neuromorphic research ﬁeld (Davies, 2019).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only the need for benchmarks in neuromorphic computing.",
      "processing_time": 43.59918451309204,
      "citing_paper_id": "226308033",
      "cited_paper_id": 203165644
    },
    {
      "context_text": "Brandli et al. [34] propose the combination of a bioinspired, redundancy-suppressing event camera with a pulsed line laser to allow fast terrain reconstruction.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions a method for terrain reconstruction using a specific sensor setup but does not refer to any named, verifiable dataset.",
      "processing_time": 43.874955892562866,
      "citing_paper_id": "247109597",
      "cited_paper_id": 1747923
    },
    {
      "context_text": "Many applications [2], [3] rely on the perception of surroundings and use depth information to reason and react accordingly.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general applications that use depth information. There are no clear identifiers for datasets.",
      "processing_time": 44.1209602355957,
      "citing_paper_id": "247109597",
      "cited_paper_id": 2255738
    },
    {
      "context_text": "To further reduce noise caused by light intensity sensitivity, we use the arithmetic mean ﬁlter algorithm [36] to preserve the polarity of events.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only an algorithm. No dataset names are present in the citation context.",
      "processing_time": 43.75767183303833,
      "citing_paper_id": "247109597",
      "cited_paper_id": 2318812
    },
    {
      "context_text": "Chodosh et al. [22] propose a novel deep recurrent autoencoder based on compressed sensing and alternating directional neural networks (A DNN s) for depth completion.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for depth completion using LiDAR data.",
      "processing_time": 43.924036264419556,
      "citing_paper_id": "247109597",
      "cited_paper_id": 4427514
    },
    {
      "context_text": "The image-based approaches are MonoDepth [45] and MegaDepth [46].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions 'MegaDepth' but does not refer to it as a dataset. It is mentioned as a method or model for single-view depth prediction.",
      "processing_time": 44.14151883125305,
      "citing_paper_id": "247109597",
      "cited_paper_id": 4572038
    },
    {
      "context_text": "Simultaneous localization and mapping (S LAM ) [33], [51] is used for autonomously constructing a descriptive map of passed area and localization by 3-",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (SLAM) which is not a dataset.",
      "processing_time": 43.68461465835571,
      "citing_paper_id": "247109597",
      "cited_paper_id": 13583282
    },
    {
      "context_text": "In addition, we use the dilation operation [37] in computer vision to inﬂate events to enhance the event stream information, which can be described as where E d is the inﬂating event and S is the convolution kernel.",
      "catation_intent": [],
      "resource_type": [],
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.3,
      "reasoning": "JSON decode error: Extra data: line 9 column 1 (char 281), response: ```json\n{\n    \"reasoning\": \"The citation does not mention any specific datasets, only a method (dila",
      "processing_time": 48.37808966636658,
      "citing_paper_id": "247109597",
      "cited_paper_id": 16559374
    },
    {
      "context_text": "The H DL _G RAPH _S LAM is based on normal distributions transform (N DT ) [53] scan matching, which is not constrained by the speciﬁc number of Li DAR beams.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (N DT ).",
      "processing_time": 43.34858179092407,
      "citing_paper_id": "247109597",
      "cited_paper_id": 18365794
    },
    {
      "context_text": "D environment information, which is widely used in intelligent robots [1].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general reference to 'environment information' which is too vague and generic.",
      "processing_time": 43.57410264015198,
      "citing_paper_id": "247109597",
      "cited_paper_id": 20868233
    },
    {
      "context_text": "Schraml et al. [25] convert the stream of event data into image frames at constant time intervals and then use stereo matching methods of traditional cameras for depth estimation.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation describes a method for converting event data into image frames for stereo matching, but does not mention a specific dataset.",
      "processing_time": 43.56620407104492,
      "citing_paper_id": "247109597",
      "cited_paper_id": 21539113
    },
    {
      "context_text": "Kim et al. [24] point out that depth estimation from moving event cameras should be possible.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a concept about depth estimation from moving event cameras.",
      "processing_time": 43.40044617652893,
      "citing_paper_id": "247109597",
      "cited_paper_id": 26324573
    },
    {
      "context_text": "Shin et al. [11] describe a framework for direct visual simultaneous localization and mapping (S LAM ) combining a monocular camera with sparse depth information from Li DAR s.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation describes a method for visual SLAM using a camera and LiDAR, but does not mention any specific datasets.",
      "processing_time": 43.86041283607483,
      "citing_paper_id": "247109597",
      "cited_paper_id": 52285412
    },
    {
      "context_text": "Just as the operation in [38], in this article, we extract the point cloud with the distance in 0–50 m and height in 0–3 m except for the ground.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific, verifiable datasets. It describes a method for extracting point clouds, which is not a dataset but a process.",
      "processing_time": 43.65820074081421,
      "citing_paper_id": "247109597",
      "cited_paper_id": 55701967
    },
    {
      "context_text": "5 [50] and PointPil-lars [38]) are used to evaluate the performance of point cloud data before and after fusion.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and models. The context focuses on evaluating performance using point cloud data, but no named datasets are referenced.",
      "processing_time": 43.96173810958862,
      "citing_paper_id": "247109597",
      "cited_paper_id": 55701967
    },
    {
      "context_text": "13% compared to the work of Hidalgo-Carrió et al. [30] and a maximum improvement of 54.41% compared to the work of Zhu et al. [29] at 10 m.",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only comparisons to previous works. No verifiable resources are identified.",
      "processing_time": 43.30248737335205,
      "citing_paper_id": "247109597",
      "cited_paper_id": 56475917
    },
    {
      "context_text": "The event-based approaches are the work of Zhu et al. [29] and Hidalgo-Carrió et al. [30].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only references to methods or research works.",
      "processing_time": 43.01844143867493,
      "citing_paper_id": "247109597",
      "cited_paper_id": 56475917
    },
    {
      "context_text": "Zhu et al. [29] propose a novel framework for unsupervised learning for event cameras that learns motion information.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for unsupervised learning using event cameras.",
      "processing_time": 43.12605166435242,
      "citing_paper_id": "247109597",
      "cited_paper_id": 56475917
    },
    {
      "context_text": "In recent years, there are some studies [17]–[19] on depth estimation based on Li DAR , event cameras, or fusion with other sensors.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general types of sensors and methods used for depth estimation. No verifiable resource names are provided.",
      "processing_time": 43.721248626708984,
      "citing_paper_id": "247109597",
      "cited_paper_id": 118684904
    },
    {
      "context_text": "In recent years, there are some studies [17]–[19] on depth estimation based on Li DAR , event cameras, or fusion with other sensors.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general types of sensors and methods used for depth estimation. No verifiable resource names are provided.",
      "processing_time": 43.721248626708984,
      "citing_paper_id": "247109597",
      "cited_paper_id": 204576139
    },
    {
      "context_text": "In this article, we use the Delaunay triangulation algorithm [39] to divide the convex hulls into triangles that do not overlap each other and maximize the minimum internal angle.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions the Delaunay triangulation algorithm but does not refer to any specific dataset. The algorithm is used for dividing convex hulls into non-overlapping triangles.",
      "processing_time": 43.89104723930359,
      "citing_paper_id": "247109597",
      "cited_paper_id": 124152930
    },
    {
      "context_text": "In this experiment, we use H DL _G RAPH _S LAM algorithm [52] to evaluate the odometry performance of our approach.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions an algorithm (HDL_GRAPH_SLAM) but does not refer to any specific dataset. The context is focused on evaluating odometry performance using the algorithm.",
      "processing_time": 43.89666676521301,
      "citing_paper_id": "247109597",
      "cited_paper_id": 150310895
    },
    {
      "context_text": "What is more, we apply MegaDepth to the image frames reconstructed from the event using E2VID [47] to predict the depth, which is named MegaDepth + , for a fuller comparison.",
      "catation_intent": "method",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions the use of MegaDepth and E2VID, but neither are datasets. MegaDepth is a method for depth prediction, and E2VID is a method for reconstructing image frames from events. No specific datasets are mentioned.",
      "processing_time": 44.38585186004639,
      "citing_paper_id": "247109597",
      "cited_paper_id": 189998802
    },
    {
      "context_text": "Vehicle detection [48], [49] is an important part of the perception system for intelligent robots, which has become a signiﬁcant technique for many key tasks, including obstacle avoidance, path planning, and so on.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general techniques and applications. No dataset names are present in the text.",
      "processing_time": 43.5828595161438,
      "citing_paper_id": "247109597",
      "cited_paper_id": 214023264
    },
    {
      "context_text": "Recently, Gehrig et al. [35] propose a novel network architecture, which generalizes traditional RNNs to handle asynchronous and irregular data from multiple sensors.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method or architecture. The context is about a novel network architecture for handling asynchronous and irregular data from multiple sensors.",
      "processing_time": 43.921842098236084,
      "citing_paper_id": "247109597",
      "cited_paper_id": 231951439
    },
    {
      "context_text": "The event-image fused approach [35] introduces recurrent asynchronous multimodal (R AM ) networks to handle irregular data from multiple sensors.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for handling data from multiple sensors.",
      "processing_time": 43.34895968437195,
      "citing_paper_id": "247109597",
      "cited_paper_id": 231951439
    },
    {
      "context_text": "In this experiment, we compare our approach with the ip_basic algorithm [23] and the work of Li et al. [15].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only algorithms and methods.",
      "processing_time": 42.868244647979736,
      "citing_paper_id": "247109597",
      "cited_paper_id": 236521582
    },
    {
      "context_text": "With [15] the proposed algorithm, temporal correlations between the pulsed stimulus and the recorded signal can be extracted, which is used as a ﬁltering criterion for the stripe extraction.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for extracting temporal correlations. No dataset names are present in the citation span.",
      "processing_time": 43.65532660484314,
      "citing_paper_id": "247109597",
      "cited_paper_id": 236521582
    },
    {
      "context_text": "Last but not least, compared with the static view [15], the event stream of dynamic view in the intelligent robots has more noisy information and extra environmental information.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a comparison between static and dynamic views in the context of intelligent robots.",
      "processing_time": 43.34571194648743,
      "citing_paper_id": "247109597",
      "cited_paper_id": 236521582
    },
    {
      "context_text": "A recent study [15] proposes a fusion approach to enhancing Li DAR point clouds with the event-based camera.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.2,
      "reasoning": "The citation mentions a fusion approach involving LiDAR point clouds and event-based cameras, but does not specify a dataset. The context is too vague to identify a specific, verifiable dataset.",
      "processing_time": 43.99078607559204,
      "citing_paper_id": "247109597",
      "cited_paper_id": 236521582
    }
  ],
  "filtering_stats": {
    "original_papers_count": 343,
    "filtered_papers_count": 60,
    "filtered_percentage": "17.5%"
  },
  "extraction_stats": {
    "unique_contexts_processed": 1317,
    "total_citation_instances": 1651,
    "successful_extractions": 175,
    "failed_extractions": 1476,
    "total_processing_time": 76.94846320152283
  }
}