{
  "report_info": {
    "title": "EVENT-BASED STEREO DEPTH ESTIMATION - CITING PAPERS 论文引用上下文分析报告",
    "query": "Event-based Stereo Depth Estimation - Citing Papers",
    "generated_at": "2025-09-12 01:29:37",
    "total_papers_queried": 41,
    "papers_with_contexts": 34,
    "papers_without_contexts": 7
  },
  "query_statistics": {
    "total_ids": 41,
    "found_ids": 34,
    "not_found_ids": [
      275031011,
      265479838,
      280693925,
      281243885,
      250511202,
      84182058,
      265491828
    ],
    "total_results": 563,
    "query_time_seconds": 0.829056978225708
  },
  "papers_data": {
    "269720399": {
      "citing_paper_info": {
        "title": "Event-Based Stereo Depth Estimation by Temporal-Spatial Context Learning",
        "abstract": "Event cameras represent a cutting-edge sensor technology, recording asynchronous pixel-level intensity changes with high temporal resolution and a wide dynamic range. These attributes make event-based stereo depth estimation particularly robust for scenarios characterized by rapid changes and challenging lighting conditions. However, previous learning-based approaches for event-based stereo have often overlooked exploiting the temporal context information within the scene, resulting in suboptimal depth estimations. In this letter, we introduce a novel learning-based network for event-based stereo that incorporates two innovative modules: the Event-based Temporal Aggregation Module (E-TAM) and the Temporal-guided Spatial Context Learning Module (T-SCLM). The E-TAM is designed to capture temporal context information among temporal features extracted from the entire event stream, further the T-SCLM exploits the temporal context information to provide guidance for spatial context learning. Subsequently, these merged features are input into the stereo matching network, ultimately yielding the final disparity map. Experimental evaluations conducted on two real-world datasets affirm the superiority of our method when compared to state-of-the-art approaches.",
        "year": 2024,
        "venue": "IEEE Signal Processing Letters",
        "authors": [
          {
            "authorId": "2300922336",
            "name": "Wu Chen"
          },
          {
            "authorId": "2240587447",
            "name": "Yueyi Zhang"
          },
          {
            "authorId": "2125995569",
            "name": "Xiaoyan Sun"
          },
          {
            "authorId": "2257429349",
            "name": "Feng Wu"
          }
        ]
      },
      "citation_summary": {
        "citation_count": 8,
        "unique_cited_count": 6,
        "influential_count": 1,
        "detailed_records_count": 8
      },
      "cited_papers": [
        "56475917",
        "254564733",
        "4252896",
        "262638843",
        "244707609",
        "232170230"
      ],
      "citation_details": [
        {
          "citedcorpusid": 4252896,
          "isinfluential": false,
          "contexts": [
            "Additionally, [2],[14]."
          ],
          "intents": [
            "--"
          ],
          "cited_paper_info": {
            "title": "Pyramid Stereo Matching Network",
            "abstract": "Recent work has shown that depth estimation from a stereo pair of images can be formulated as a supervised learning task to be resolved with convolutional neural networks (CNNs). However, current architectures rely on patch-based Siamese networks, lacking the means to exploit context information for finding correspondence in ill-posed regions. To tackle this problem, we propose PSMNet, a pyramid stereo matching network consisting of two main modules: spatial pyramid pooling and 3D CNN. The spatial pyramid pooling module takes advantage of the capacity of global context information by aggregating context in different scales and locations to form a cost volume. The 3D CNN learns to regularize cost volume using stacked multiple hourglass networks in conjunction with intermediate supervision. The proposed approach was evaluated on several benchmark datasets. Our method ranked first in the KITTI 2012 and 2015 leaderboards before March 18, 2018. The codes of PSMNet are available at: https://github.com/JiaRenChang/PSMNet.",
            "year": 2018,
            "venue": "2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition",
            "authors": [
              {
                "authorId": "2936466",
                "name": "Jia-Ren Chang"
              },
              {
                "authorId": "2143438143",
                "name": "Yonghao Chen"
              }
            ]
          }
        },
        {
          "citedcorpusid": 56475917,
          "isinfluential": false,
          "contexts": [
            "Given an event stream with N events ε = , where each event contains the position ( x k , y k ) , the timestamp t k and the polarity p k , we represent the event stream in the voxel grid format following the approach outlined in [16]."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Unsupervised Event-Based Learning of Optical Flow, Depth, and Egomotion",
            "abstract": "In this work, we propose a novel framework for unsupervised learning for event cameras that learns motion information from only the event stream. In particular, we propose an input representation of the events in the form of a discretized volume that maintains the temporal distribution of the events, which we pass through a neural network to predict the motion of the events. This motion is used to attempt to remove any motion blur in the event image. We then propose a loss function applied to the motion compensated event image that measures the motion blur in this image. We train two networks with this framework, one to predict optical flow, and one to predict egomotion and depths, and evaluate these networks on the Multi Vehicle Stereo Event Camera dataset, along with qualitative results from a variety of different scenes.",
            "year": 2018,
            "venue": "Computer Vision and Pattern Recognition",
            "authors": [
              {
                "authorId": "3385588",
                "name": "A. Z. Zhu"
              },
              {
                "authorId": "36001694",
                "name": "Liangzhe Yuan"
              },
              {
                "authorId": "20728097",
                "name": "Kenneth Chaney"
              },
              {
                "authorId": "1751586",
                "name": "Kostas Daniilidis"
              }
            ]
          }
        },
        {
          "citedcorpusid": 232170230,
          "isinfluential": false,
          "contexts": [
            "Datasets: We conduct evaluations of our proposed method on two stereo event camera datasets, the Multi-Vehicle Stereo Camera Dataset (MVSEC) [23] and the stereo event camera dataset (DSEC) [24]."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "DSEC: A Stereo Event Camera Dataset for Driving Scenarios",
            "abstract": "Once an academic venture, autonomous driving has received unparalleled corporate funding in the last decade. Still, operating conditions of current autonomous cars are mostly restricted to ideal scenarios. This means that driving in challenging illumination conditions such as night, sunrise, and sunset remains an open problem. In these cases, standard cameras are being pushed to their limits in terms of low light and high dynamic range performance. To address these challenges, we propose, DSEC, a new dataset that contains such demanding illumination conditions and provides a rich set of sensory data. DSEC offers data from a wide-baseline stereo setup of two color frame cameras and two high-resolution monochrome event cameras. In addition, we collect lidar data and RTK GPS measurements, both hardware synchronized with all camera data. One of the distinctive features of this dataset is the inclusion of high-resolution event cameras. Event cameras have received increasing attention for their high temporal resolution and high dynamic range performance. However, due to their novelty, event camera datasets in driving scenarios are rare. This work presents the first high resolution, large scale stereo dataset with event cameras. The dataset contains 53 sequences collected by driving in a variety of illumination conditions and provides ground truth disparity for the development and evaluation of event-based stereo algorithms.",
            "year": 2021,
            "venue": "IEEE Robotics and Automation Letters",
            "authors": [
              {
                "authorId": "8329387",
                "name": "Mathias Gehrig"
              },
              {
                "authorId": "2052356146",
                "name": "Willem Aarents"
              },
              {
                "authorId": "51152279",
                "name": "Daniel Gehrig"
              },
              {
                "authorId": "2075371",
                "name": "D. Scaramuzza"
              }
            ]
          }
        },
        {
          "citedcorpusid": 244707609,
          "isinfluential": false,
          "contexts": [
            "Compared to previous methods of converting the overall voxel features into token [18], [19], we design an event-based temporal aggregation module particularly for the voxel representation of events as shown in Fig."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Event-based Video Reconstruction Using Transformer",
            "abstract": "Event cameras, which output events by detecting spatio- temporal brightness changes, bring a novel paradigm to image sensors with high dynamic range and low latency. Previous works have achieved impressive performances on event-based video reconstruction by introducing convolutional neural networks (CNNs). However, intrinsic locality of convolutional operations is not capable of modeling long-range dependency, which is crucial to many vision tasks. In this paper, we present a hybrid CNN- Transformer network for event-based video reconstruction (ET-Net), which merits the fine local information from CNN and global contexts from Transformer In addition, we further propose a Token Pyramid Aggregation strategy to implement multi-scale token integration for relating internal and intersected semantic concepts in the token-space. Experimental results demonstrate that our proposed method achieves superior performance over state-of-the-art methods on multiple real-world event datasets. The code is available at https://github.com/WarranWeng/ET-Net.",
            "year": 2021,
            "venue": "IEEE International Conference on Computer Vision",
            "authors": [
              {
                "authorId": "2107017232",
                "name": "Wenming Weng"
              },
              {
                "authorId": "2145912767",
                "name": "Yueyi Zhang"
              },
              {
                "authorId": "2352456",
                "name": "Zhiwei Xiong"
              }
            ]
          }
        },
        {
          "citedcorpusid": 254564733,
          "isinfluential": false,
          "contexts": [
            "Compared to previous methods of converting the overall voxel features into token [18], [19], we design an event-based temporal aggregation module particularly for the voxel representation of events as shown in Fig."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Recurrent Vision Transformers for Object Detection with Event Cameras",
            "abstract": "We present Recurrent Vision Transformers (RVTs), a novel backbone for object detection with event cameras. Event cameras provide visual information with submillisecond latency at a high-dynamic range and with strong robustness against motion blur. These unique properties offer great potential for low-latency object detection and tracking in time-critical scenarios. Prior work in event-based vision has achieved outstanding detection performance but at the cost of substantial inference time, typically beyond 40 milliseconds. By revisiting the high-level design of recurrent vision backbones, we reduce inference time by a factor of 6 while retaining similar performance. To achieve this, we explore a multi-stage design that utilizes three key concepts in each stage: first, a convolutional prior that can be regarded as a conditional positional embedding. Second, local and dilated global self-attention for spatial feature interaction. Third, recurrent temporal feature aggregation to minimize latency while retaining temporal information. RVTs can be trained from scratch to reach state-of-the-art performance on event-based object detection - achieving an mAP of 47.2% on the Gen1 automotive dataset. At the same time, RVTs offer fast inference (< 12 ms on a T4 GPU) and favorable parameter efficiency (5 × fewer than prior art). Our study brings new insights into effective design choices that can be fruitful for research beyond event-based vision. Code: https://github.com/uzh-rpg/RVT",
            "year": 2022,
            "venue": "Computer Vision and Pattern Recognition",
            "authors": [
              {
                "authorId": "8329387",
                "name": "Mathias Gehrig"
              },
              {
                "authorId": "2075371",
                "name": "D. Scaramuzza"
              }
            ]
          }
        },
        {
          "citedcorpusid": 262638843,
          "isinfluential": true,
          "contexts": [
            "We quantitatively compare the performance of our model with previous state-of-the-art methods, including DDES [9], E-Stereo [26], EITNet [25], Se-cff [10], DTC-SPADE [11], and DTC-PDS [11].",
            "On the MVSEC dataset, we perform dense disparity estimation using the entire ground truth for evaluation, following [9], [11], [25].",
            "For MVSEC, following the previous setting as [9], [11], [17], [25], [26], we divide the Indoor Flying dataset into three subsets and use split 1 and split 3.",
            "DDES [9] was the ﬁrst learning-based method that proposed a module for event sequence embedding, which stores events in a First-in First-out (FIFO) queue to preserve both temporal and spatial information concurrently."
          ],
          "intents": [
            "['result']",
            "['methodology']",
            "['methodology']",
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Learning an Event Sequence Embedding for Dense Event-Based Deep Stereo",
            "abstract": "Today, a frame-based camera is the sensor of choice for machine vision applications. However, these cameras, originally developed for acquisition of static images rather than for sensing of dynamic uncontrolled visual environments, suffer from high power consumption, data rate, latency and low dynamic range. An event-based image sensor addresses these drawbacks by mimicking a biological retina. Instead of measuring the intensity of every pixel in a fixed time-interval, it reports events of significant pixel intensity changes. Every such event is represented by its position, sign of change, and timestamp, accurate to the microsecond. Asynchronous event sequences require special handling, since traditional algorithms work only with synchronous, spatially gridded data. To address this problem we introduce a new module for event sequence embedding, for use in difference applications. The module builds a representation of an event sequence by firstly aggregating information locally across time, using a novel fully-connected layer for an irregularly sampled continuous domain, and then across discrete spatial domain. Based on this module, we design a deep learning-based stereo method for event-based cameras. The proposed method is the first learning-based stereo method for an event-based camera and the only method that produces dense results. We show that large performance increases on the Multi Vehicle Stereo Event Camera Dataset (MVSEC), which became the standard set for benchmarking of event-based stereo methods.",
            "year": 2019,
            "venue": "IEEE International Conference on Computer Vision",
            "authors": [
              {
                "authorId": "1823725",
                "name": "S. Tulyakov"
              },
              {
                "authorId": "2721983",
                "name": "F. Fleuret"
              },
              {
                "authorId": "40519282",
                "name": "Martin Kiefel"
              },
              {
                "authorId": "2871555",
                "name": "Peter Gehler"
              },
              {
                "authorId": "2058954687",
                "name": "Michael Hirsch"
              }
            ]
          }
        },
        {
          "citedcorpusid": null,
          "isinfluential": false,
          "contexts": [
            "We quantitatively compare the performance of our model with previous state-of-the-art methods, including DDES [9], E-Stereo [26], EITNet [25], Se-cff [10], DTC-SPADE [11], and DTC-PDS [11].",
            "On the MVSEC dataset, we perform dense disparity estimation using the entire ground truth for evaluation, following [9], [11], [25].",
            "For MVSEC, following the previous setting as [9], [11], [17], [25], [26], we divide the Indoor Flying dataset into three subsets and use split 1 and split 3."
          ],
          "intents": [
            "['result']",
            "['methodology']",
            "['methodology']"
          ],
          "cited_paper_info": {}
        },
        {
          "citedcorpusid": null,
          "isinfluential": false,
          "contexts": [
            "We quantitatively compare the performance of our model with previous state-of-the-art methods, including DDES [9], E-Stereo [26], EITNet [25], Se-cff [10], DTC-SPADE [11], and DTC-PDS [11].",
            "Given the disparities in data characteristics between the training and testing sets in split 1, coupled with the smaller sample size of the split 1 test set compared to split 3, averaging errors becomes ineffective [26].",
            "For MVSEC, following the previous setting as [9], [11], [17], [25], [26], we divide the Indoor Flying dataset into three subsets and use split 1 and split 3."
          ],
          "intents": [
            "['result']",
            "['background']",
            "['methodology']"
          ],
          "cited_paper_info": {}
        }
      ]
    },
    "272911351": {
      "citing_paper_info": {
        "title": "Event-based Stereo Depth Estimation: A Survey",
        "abstract": "Stereopsis has widespread appeal in computer vision and robotics as it is the predominant way by which we perceive depth to navigate our 3D world. Event cameras are novel bio-inspired sensors that detect per-pixel brightness changes asynchronously, with very high temporal resolution and high dynamic range, enabling machine perception in high-speed motion and broad illumination conditions. The high temporal precision also benefits stereo matching, making disparity (depth) estimation a popular research area for event cameras ever since their inception. Over the last 30 years, the field has evolved rapidly, from low-latency, low-power circuit design to current deep learning (DL) approaches driven by the computer vision community. The bibliography is vast and difficult to navigate for non-experts due its highly interdisciplinary nature. Past surveys have addressed distinct aspects of this topic, in the context of applications, or focusing only on a specific class of techniques, but have overlooked stereo datasets. This survey provides a comprehensive overview, covering both instantaneous stereo and long-term methods suitable for simultaneous localization and mapping (SLAM), along with theoretical and empirical comparisons. It is the first to extensively review DL methods as well as stereo datasets, even providing practical suggestions for creating new benchmarks to advance the field. The main advantages and challenges faced by event-based stereo depth estimation are also discussed. Despite significant progress, challenges remain in achieving optimal performance in not only accuracy but also efficiency, a cornerstone of event-based computing. We identify several gaps and propose future research directions. We hope this survey inspires future research in depth estimation with event cameras and related topics, by serving as an accessible entry point for newcomers, as well as a practical guide for seasoned researchers in the community.",
        "year": 2024,
        "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
        "authors": [
          {
            "authorId": "2155615482",
            "name": "Suman Ghosh"
          },
          {
            "authorId": "144036711",
            "name": "Guillermo Gallego"
          }
        ]
      },
      "citation_summary": {
        "citation_count": 63,
        "unique_cited_count": 57,
        "influential_count": 9,
        "detailed_records_count": 63
      },
      "cited_papers": [
        "269614135",
        "6079544",
        "119309624",
        "254531210",
        "25268038",
        "27059477",
        "250918780",
        "270068050",
        "4833834",
        "271892156",
        "267212137",
        "265479838",
        "157060825",
        "3608458",
        "253651036",
        "7224209",
        "250699235",
        "263339606",
        "255125395",
        "49864158",
        "269137093",
        "46937991",
        "250127779",
        "258213006",
        "262638843",
        "10712214",
        "260293142",
        "238198645",
        "248227281",
        "44969055",
        "252476994",
        "276652376",
        "34855834",
        "4597042",
        "246656358",
        "84182058",
        "73729084",
        "4412139",
        "1082643",
        "16588072",
        "248572428",
        "265257632",
        "250607506",
        "6724907",
        "7083033",
        "167210006",
        "49877954",
        "11177597",
        "17272393",
        "17693733",
        "274611240",
        "257019827",
        "205698386",
        "19091270",
        "12047627",
        "244306440",
        "259075396"
      ],
      "citation_details": [
        {
          "citedcorpusid": 1082643,
          "isinfluential": true,
          "contexts": [
            "The advantages of stereo over monocular (e.g., [29] vs [121]) due to exploiting spatial parallax are consistently clear on both tables: mean errors decrease by 30–45%, and outliers also decrease (by more than half) while the number of recovered points remains.",
            "Images courtesy of [29, 121].",
            "The small parallax motion (except during turning) also makes it difficult to estimate depth with monocular methods like EMVS [121].",
            "The monocular method Event-based Multi-View Stereo ( EMVS ) [121] uses known camera poses to shoot rays from event pixels through 3D space (Fig."
          ],
          "intents": [
            "--",
            "--",
            "--",
            "--"
          ],
          "cited_paper_info": {
            "title": "EMVS: Event-Based Multi-View Stereo—3D Reconstruction with an Event Camera in Real-Time",
            "abstract": "",
            "year": 2018,
            "venue": "International Journal of Computer Vision",
            "authors": [
              {
                "authorId": "3414274",
                "name": "Henri Rebecq"
              },
              {
                "authorId": "144036711",
                "name": "Guillermo Gallego"
              },
              {
                "authorId": "144578041",
                "name": "Elias Mueggler"
              },
              {
                "authorId": "2075371",
                "name": "D. Scaramuzza"
              }
            ]
          }
        },
        {
          "citedcorpusid": 3608458,
          "isinfluential": false,
          "contexts": [
            "…reason for the prevalence of cooperative networks for event-based stereo matching is that they can be realized as Spiking Neural Networks ( SNN s), which can be implemented on highly efficient neuromorphic hardware like SpiNNaker [110], ROLLS [111], Loihi [112], DYNAP [113] and TrueNorth [114]."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Loihi: A Neuromorphic Manycore Processor with On-Chip Learning",
            "abstract": "Loihi is a 60-mm2 chip fabricated in Intels 14-nm process that advances the state-of-the-art modeling of spiking neural networks in silicon. It integrates a wide range of novel features for the field, such as hierarchical connectivity, dendritic compartments, synaptic delays, and, most importantly, programmable synaptic learning rules. Running a spiking convolutional form of the Locally Competitive Algorithm, Loihi can solve LASSO optimization problems with over three orders of magnitude superior energy-delay-product compared to conventional solvers running on a CPU iso-process/voltage/area. This provides an unambiguous example of spike-based computation, outperforming all known conventional solutions.",
            "year": 2018,
            "venue": "IEEE Micro",
            "authors": [
              {
                "authorId": "2087005207",
                "name": "Mike Davies"
              },
              {
                "authorId": "1753812",
                "name": "N. Srinivasa"
              },
              {
                "authorId": "1740851",
                "name": "Tsung-Han Lin"
              },
              {
                "authorId": "3198756",
                "name": "Gautham N. Chinya"
              },
              {
                "authorId": "39098303",
                "name": "Yongqiang Cao"
              },
              {
                "authorId": "2572988",
                "name": "S. H. Choday"
              },
              {
                "authorId": "50280933",
                "name": "G. Dimou"
              },
              {
                "authorId": "144594117",
                "name": "Prasad Joshi"
              },
              {
                "authorId": "39536844",
                "name": "N. Imam"
              },
              {
                "authorId": "2116971410",
                "name": "Shweta Jain"
              },
              {
                "authorId": "2781201",
                "name": "Yuyun Liao"
              },
              {
                "authorId": "3312263",
                "name": "Chit-Kwan Lin"
              },
              {
                "authorId": "145166778",
                "name": "Andrew Lines"
              },
              {
                "authorId": "35804101",
                "name": "Ruokun Liu"
              },
              {
                "authorId": "1710935",
                "name": "D. Mathaikutty"
              },
              {
                "authorId": "2070135234",
                "name": "Steve McCoy"
              },
              {
                "authorId": "2150514037",
                "name": "Arnab Paul"
              },
              {
                "authorId": "2306147",
                "name": "Jonathan Tse"
              },
              {
                "authorId": "35865638",
                "name": "Guruguhanathan Venkataramanan"
              },
              {
                "authorId": "31953633",
                "name": "Y. Weng"
              },
              {
                "authorId": "1665915600",
                "name": "Andreas Wild"
              },
              {
                "authorId": "2108648646",
                "name": "Yoonseok Yang"
              },
              {
                "authorId": "49528487",
                "name": "Hong Wang"
              }
            ]
          }
        },
        {
          "citedcorpusid": 4412139,
          "isinfluential": true,
          "contexts": [
            "Values for CopNet [62] and TSES [57] are taken from [57], whereas the others are from [29].",
            "14: Stereo fusion architecture employed in TSES [57].",
            "The indoor drone sequences being widely used, but the outdoor driving sequences are not suitable for stereo due to the small baseline [57].",
            "Similarly, Time Synchronized Event-based Stereo ( TSES ) [57] uses known and constant camera velocity to warp events during a short time interval.",
            "Image courtesy of [57]. non-spiking event stereo methods.",
            "Grid-like 3D event representations [29, 57, 115] like DSIs have been shown to preserve finer details in depth estimation."
          ],
          "intents": [
            "['background']",
            "['methodology']",
            "['background']",
            "--",
            "['methodology']",
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Realtime Time Synchronized Event-based Stereo",
            "abstract": "In this work, we propose a novel event based stereo method which addresses the problem of motion blur for a moving event camera. Our method uses the velocity of the camera and a range of disparities to synchronize the positions of the events, as if they were captured at a single point in time. We represent these events using a pair of novel time synchronized event disparity volumes, which we show remove motion blur for pixels at the correct disparity in the volume, while further blurring pixels at the wrong disparity. We then apply a novel matching cost over these time synchronized event disparity volumes, which both rewards similarity between the volumes while penalizing blurriness. We show that our method outperforms more expensive, smoothing based event stereo methods, by evaluating on the Multi Vehicle Stereo Event Camera dataset.",
            "year": 2018,
            "venue": "European Conference on Computer Vision",
            "authors": [
              {
                "authorId": "3385588",
                "name": "A. Z. Zhu"
              },
              {
                "authorId": "2116435960",
                "name": "Yibo Chen"
              },
              {
                "authorId": "1751586",
                "name": "Kostas Daniilidis"
              }
            ]
          }
        },
        {
          "citedcorpusid": 4597042,
          "isinfluential": false,
          "contexts": [
            "This has connections with the Contrast Maximization ( CMax ) framework [123, 124]: the depth slices of the DSI can be interpreted as Images of Warped Events ( IWEs ) and recovery of the 3D scene structure amounts to finding the depth map whose IWE has maximum contrast (i.e., best event alignment,…"
          ],
          "intents": [
            "--"
          ],
          "cited_paper_info": {
            "title": "A Unifying Contrast Maximization Framework for Event Cameras, with Applications to Motion, Depth, and Optical Flow Estimation",
            "abstract": "We present a unifying framework to solve several computer vision problems with event cameras: motion, depth and optical flow estimation. The main idea of our framework is to find the point trajectories on the image plane that are best aligned with the event data by maximizing an objective function: the contrast of an image of warped events. Our method implicitly handles data association between the events, and therefore, does not rely on additional appearance information about the scene. In addition to accurately recovering the motion parameters of the problem, our framework produces motion-corrected edge-like images with high dynamic range that can be used for further scene analysis. The proposed method is not only simple, but more importantly, it is, to the best of our knowledge, the first method that can be successfully applied to such a diverse set of important vision tasks with event cameras.",
            "year": 2018,
            "venue": "2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition",
            "authors": [
              {
                "authorId": "144036711",
                "name": "Guillermo Gallego"
              },
              {
                "authorId": "3414274",
                "name": "Henri Rebecq"
              },
              {
                "authorId": "2075371",
                "name": "D. Scaramuzza"
              }
            ]
          }
        },
        {
          "citedcorpusid": 4833834,
          "isinfluential": false,
          "contexts": [
            "On the other hand, Dikov et al. [63] extended their previous cooperative method [69] by implementing it on SpiNNaker digital processor boards.",
            "Firouzi et al. [69] also proposed a cooperative algorithm for events which employed an additional second pattern of inhibitory connections in their dynamic cooperative network to suppress ambiguous matches."
          ],
          "intents": [
            "['methodology']",
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Asynchronous Event-based Cooperative Stereo Matching Using Neuromorphic Silicon Retinas",
            "abstract": "Biologically-inspired event-driven silicon retinas, so called dynamic vision sensors (DVS), allow efficient solutions for various visual perception tasks, e.g. surveillance, tracking, or motion detection. Similar to retinal photoreceptors, any perceived light intensity change in the DVS generates an event at the corresponding pixel. The DVS thereby emits a stream of spatiotemporal events to encode visually perceived objects that in contrast to conventional frame-based cameras, is largely free of redundant background information. The DVS offers multiple additional advantages, but requires the development of radically new asynchronous, event-based information processing algorithms. In this paper we present a fully event-based disparity matching algorithm for reliable 3D depth perception using a dynamic cooperative neural network. The interaction between cooperative cells applies cross-disparity uniqueness-constraints and within-disparity continuity-constraints, to asynchronously extract disparity for each new event, without any need of buffering individual events. We have investigated the algorithm’s performance in several experiments; our results demonstrate smooth disparity maps computed in a purely event-based manner, even in the scenes with temporally-overlapping stimuli.",
            "year": 2016,
            "venue": "Neural Processing Letters",
            "authors": [
              {
                "authorId": "145885214",
                "name": "M. Firouzi"
              },
              {
                "authorId": "3302681",
                "name": "J. Conradt"
              }
            ]
          }
        },
        {
          "citedcorpusid": 6079544,
          "isinfluential": false,
          "contexts": [
            "Later, Piatkowska et al. [82] implemented a cooperative technique for depth computation using a winner-take-all mechanism to match temporally-close and spatially-constrained events."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Asynchronous Stereo Vision for Event-Driven Dynamic Stereo Sensor Using an Adaptive Cooperative Approach",
            "abstract": "",
            "year": 2013,
            "venue": "2013 IEEE International Conference on Computer Vision Workshops",
            "authors": [
              {
                "authorId": "47105977",
                "name": "Ewa Piatkowska"
              },
              {
                "authorId": "1768812",
                "name": "A. Belbachir"
              },
              {
                "authorId": "1990797",
                "name": "M. Gelautz"
              }
            ]
          }
        },
        {
          "citedcorpusid": 6724907,
          "isinfluential": false,
          "contexts": [
            "[129]), the scale invariant depth error (SILog Err), the sum of absolute value of relative differences in depth (AErrR), and δ -accuracy values on the percentage of points whose depth ratio with respect to GT is within some threshold (see [130])."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Are we ready for autonomous driving? The KITTI vision benchmark suite",
            "abstract": "",
            "year": 2012,
            "venue": "2012 IEEE Conference on Computer Vision and Pattern Recognition",
            "authors": [
              {
                "authorId": "47237027",
                "name": "Andreas Geiger"
              },
              {
                "authorId": "37108776",
                "name": "Philip Lenz"
              },
              {
                "authorId": "2422559",
                "name": "R. Urtasun"
              }
            ]
          }
        },
        {
          "citedcorpusid": 7083033,
          "isinfluential": false,
          "contexts": [
            "It was then implemented in software by Delbruck’s student Hess [99]."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Low-level Stereo Matching using Event-based Silicon Retinas",
            "abstract": ". This semester thesis examines the task of computing stereo image disparities using a binocular pair of event-based silicon retinas [1]. Unlike conventional video cameras, this silicon retina is asynchronous and event-based. It responds to reflectance change and thus the events primarily encode object movement. Therefore there is no global frame rate. Two different algorithms were devised to do stereo matching by comparing the time differences of events. The matched events are \"orientation events\" derived from oriented spatio-temporal coincidence of retina events. Additional constraints are used to suppress noise and ensure local and spatial smoothness. The matching is low-level and purely based on events, without any actual object recognition. The first algorithm enforces smoothness by assuming a single frontal object and therefore the same disparity is assigned to all events in a certain period. The second algorithm calculates disparities for every event individually. Smoothness is enforced by restricting the matching search range around the mean disparity of previous events in the neighborhood. Running on a 1.6 GHz Centrino laptop computer, performance ranged from processing 120 keps (kilo events per second) for the global filter to 75 keps for the locally constrained disparity filter.",
            "year": 2006,
            "venue": "",
            "authors": [
              {
                "authorId": "2101638102",
                "name": "Semester Thesis"
              },
              {
                "authorId": "2053785606",
                "name": "Peter Heß"
              },
              {
                "authorId": "1694635",
                "name": "T. Delbrück"
              }
            ]
          }
        },
        {
          "citedcorpusid": 7224209,
          "isinfluential": false,
          "contexts": [
            "Initial works [92, 97, 98, 105, 106] accumulated events into frames for compatibility with standard binocular vision techniques."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "CARE: A dynamic stereo vision sensor system for fall detection",
            "abstract": "This paper presents a recently developed dynamic stereo vision sensor system and its application for fall detection towards safety for elderly at home. The system consists of (1) two optical detector chips with 304×240 event-driven pixels which are only sensitive to relative light intensity changes, (2) an FPGA for interfacing the detectors, early data processing, and stereo matching for depth map reconstruction, (3) a digital signal processor for interpreting the sensor data in real-time for fall recognition, and (4) a wireless communication module for instantly alerting caring institutions. This system was designed for incident detection in private homes of elderly to foster safety and security. The two main advantages of the system, compared to existing wearable systems are from the application's point of view: (a) the stationary installation has a better acceptance for independent living comparing to permanent wearing devices, and (b) the privacy of the system is systematically ensured since the vision detector does not produce real images such as classic video sensors. The system can actually process about 300 kevents per second. It was evaluated using 500 fall cases acquired with a stuntman. More than 90% positive detections were reported. We will show a live demonstration during ISCAS2012 of the sensor system and its capabilities.",
            "year": 2012,
            "venue": "2012 IEEE International Symposium on Circuits and Systems",
            "authors": [
              {
                "authorId": "1768812",
                "name": "A. Belbachir"
              },
              {
                "authorId": "1804754",
                "name": "M. Litzenberger"
              },
              {
                "authorId": "2521747",
                "name": "S. Schraml"
              },
              {
                "authorId": "145863753",
                "name": "M. Hofstätter"
              },
              {
                "authorId": "8446684",
                "name": "M. Bauer"
              },
              {
                "authorId": "1680865",
                "name": "Peter Schön"
              },
              {
                "authorId": "1721721",
                "name": "M. Humenberger"
              },
              {
                "authorId": "1692498",
                "name": "C. Sulzbachner"
              },
              {
                "authorId": "2792132",
                "name": "Tommi Lunden"
              },
              {
                "authorId": "2098023210",
                "name": "M. Merne"
              }
            ]
          }
        },
        {
          "citedcorpusid": 10712214,
          "isinfluential": false,
          "contexts": [
            "Carneiro et al. [81] demonstrated that adding cameras to the stereo setup (e.g., trinocular vision, with more temporal and epipolar constraints), could further disambiguate and produce more reliable matches."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Event-based 3D reconstruction from neuromorphic retinas",
            "abstract": "",
            "year": 2013,
            "venue": "Neural Networks",
            "authors": [
              {
                "authorId": "2057119545",
                "name": "J. Carneiro"
              },
              {
                "authorId": "144975525",
                "name": "S. Ieng"
              },
              {
                "authorId": "153466606",
                "name": "C. Posch"
              },
              {
                "authorId": "1750848",
                "name": "R. Benosman"
              }
            ]
          }
        },
        {
          "citedcorpusid": 11177597,
          "isinfluential": false,
          "contexts": [
            "To emphasize the benefits of accurate time information, [86] proposed a purely event-driven matching procedure using time-based and polarity-based correlation of the events, without aggregation."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Event-Based Stereo Matching Approaches for Frameless Address Event Stereo Data",
            "abstract": "",
            "year": 2011,
            "venue": "International Symposium on Visual Computing",
            "authors": [
              {
                "authorId": "1824241",
                "name": "J. Kogler"
              },
              {
                "authorId": "1721721",
                "name": "M. Humenberger"
              },
              {
                "authorId": "1692498",
                "name": "C. Sulzbachner"
              }
            ]
          }
        },
        {
          "citedcorpusid": 12047627,
          "isinfluential": false,
          "contexts": [
            "Besides timestamp and polarity, the orientation of object edges using Gabor filters has also been used as a supplementary signal for cooperative stereo [78]."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "On the use of orientation filters for 3D reconstruction in event-driven stereo vision",
            "abstract": "The recently developed Dynamic Vision Sensors (DVS) sense visual information asynchronously and code it into trains of events with sub-micro second temporal resolution. This high temporal precision makes the output of these sensors especially suited for dynamic 3D visual reconstruction, by matching corresponding events generated by two different sensors in a stereo setup. This paper explores the use of Gabor filters to extract information about the orientation of the object edges that produce the events, therefore increasing the number of constraints applied to the matching algorithm. This strategy provides more reliably matched pairs of events, improving the final 3D reconstruction.",
            "year": 2014,
            "venue": "Frontiers in Neuroscience",
            "authors": [
              {
                "authorId": "1398500621",
                "name": "L. Camuñas-Mesa"
              },
              {
                "authorId": "1397317865",
                "name": "T. Serrano-Gotarredona"
              },
              {
                "authorId": "144975525",
                "name": "S. Ieng"
              },
              {
                "authorId": "1750848",
                "name": "R. Benosman"
              },
              {
                "authorId": "1397317879",
                "name": "B. Linares-Barranco"
              }
            ]
          }
        },
        {
          "citedcorpusid": 16588072,
          "isinfluential": false,
          "contexts": [
            "This is a challenging VIO dataset (EVO [156], ESVO [41], Ultimate SLAM [157] failed in most sequences)."
          ],
          "intents": [
            "--"
          ],
          "cited_paper_info": {
            "title": "EVO: A Geometric Approach to Event-Based 6-DOF Parallel Tracking and Mapping in Real Time",
            "abstract": "",
            "year": 2017,
            "venue": "IEEE Robotics and Automation Letters",
            "authors": [
              {
                "authorId": "3414274",
                "name": "Henri Rebecq"
              },
              {
                "authorId": "9676873",
                "name": "Timo Horstschaefer"
              },
              {
                "authorId": "144036711",
                "name": "Guillermo Gallego"
              },
              {
                "authorId": "2075371",
                "name": "D. Scaramuzza"
              }
            ]
          }
        },
        {
          "citedcorpusid": 17272393,
          "isinfluential": false,
          "contexts": [
            "It is an evolved version of previous datasets recorded by the same lab [152], [153]."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "A Dataset for Visual Navigation with Neuromorphic Methods",
            "abstract": "Standardized benchmarks in Computer Vision have greatly contributed to the advance of approaches to many problems in the field. If we want to enhance the visibility of event-driven vision and increase its impact, we will need benchmarks that allow comparison among different neuromorphic methods as well as comparison to Computer Vision conventional approaches. We present datasets to evaluate the accuracy of frame-free and frame-based approaches for tasks of visual navigation. Similar to conventional Computer Vision datasets, we provide synthetic and real scenes, with the synthetic data created with graphics packages, and the real data recorded using a mobile robotic platform carrying a dynamic and active pixel vision sensor (DAVIS) and an RGB+Depth sensor. For both datasets the cameras move with a rigid motion in a static scene, and the data includes the images, events, optic flow, 3D camera motion, and the depth of the scene, along with calibration procedures. Finally, we also provide simulated event data generated synthetically from well-known frame-based optical flow datasets.",
            "year": 2016,
            "venue": "Frontiers in Neuroscience",
            "authors": [
              {
                "authorId": "144484799",
                "name": "Francisco Barranco"
              },
              {
                "authorId": "3415312",
                "name": "Cornelia Fermuller"
              },
              {
                "authorId": "1697493",
                "name": "Y. Aloimonos"
              },
              {
                "authorId": "5548576",
                "name": "T. Delbruck"
              }
            ]
          }
        },
        {
          "citedcorpusid": 17693733,
          "isinfluential": false,
          "contexts": [
            "To reduce false stereo correspondences and therefore enhance matching quality, temporal matching was aided by additional constraints (i.e., equations), such as epipolar and ordering constraints [85]."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Asynchronous Event-Based Binocular Stereo Matching",
            "abstract": "",
            "year": 2012,
            "venue": "IEEE Transactions on Neural Networks and Learning Systems",
            "authors": [
              {
                "authorId": "3121605",
                "name": "P. Rogister"
              },
              {
                "authorId": "1750848",
                "name": "R. Benosman"
              },
              {
                "authorId": "144975525",
                "name": "S. Ieng"
              },
              {
                "authorId": "1744964",
                "name": "P. Lichtsteiner"
              },
              {
                "authorId": "5548576",
                "name": "T. Delbruck"
              }
            ]
          }
        },
        {
          "citedcorpusid": 19091270,
          "isinfluential": false,
          "contexts": [
            "Accurate stereo correspondence between the event and frame-based camera would enable better alignment between them, thus producing a similar effect as a DAVIS camera [151] but with higher image quality and resolution."
          ],
          "intents": [
            "--"
          ],
          "cited_paper_info": {
            "title": "Front and Back Illuminated Dynamic and Active Pixel Vision Sensors Comparison",
            "abstract": "",
            "year": 2018,
            "venue": "IEEE Transactions on Circuits and Systems - II - Express Briefs",
            "authors": [
              {
                "authorId": "8742892",
                "name": "Gemma Taverni"
              },
              {
                "authorId": "133850011",
                "name": "Diederik Paul Moeys"
              },
              {
                "authorId": "49672818",
                "name": "Chenghan Li"
              },
              {
                "authorId": "40859499",
                "name": "C. Cavaco"
              },
              {
                "authorId": "30912796",
                "name": "V. Motsnyi"
              },
              {
                "authorId": "113215237",
                "name": "D. San Segundo Bello"
              },
              {
                "authorId": "5548576",
                "name": "T. Delbruck"
              }
            ]
          }
        },
        {
          "citedcorpusid": 25268038,
          "isinfluential": false,
          "contexts": [
            "…reason for the prevalence of cooperative networks for event-based stereo matching is that they can be realized as Spiking Neural Networks ( SNN s), which can be implemented on highly efficient neuromorphic hardware like SpiNNaker [110], ROLLS [111], Loihi [112], DYNAP [113] and TrueNorth [114]."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "The SpiNNaker Project",
            "abstract": "",
            "year": 2014,
            "venue": "Proceedings of the IEEE",
            "authors": [
              {
                "authorId": "144409615",
                "name": "S. Furber"
              },
              {
                "authorId": "3008126",
                "name": "F. Galluppi"
              },
              {
                "authorId": "143816983",
                "name": "S. Temple"
              },
              {
                "authorId": "3085921",
                "name": "L. Plana"
              }
            ]
          }
        },
        {
          "citedcorpusid": 27059477,
          "isinfluential": false,
          "contexts": [
            "Initial works [92, 97, 98, 105, 106] accumulated events into frames for compatibility with standard binocular vision techniques."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Bio-inspired Stereo Vision System with Silicon Retina Imagers",
            "abstract": "",
            "year": 2009,
            "venue": "International Conference on Virtual Storytelling",
            "authors": [
              {
                "authorId": "1824241",
                "name": "J. Kogler"
              },
              {
                "authorId": "1692498",
                "name": "C. Sulzbachner"
              },
              {
                "authorId": "1728811",
                "name": "W. Kubinger"
              }
            ]
          }
        },
        {
          "citedcorpusid": 34855834,
          "isinfluential": false,
          "contexts": [
            "Recent works have proven this idea on small-scale scenes with stationary cameras [43, 49, 63, 64].",
            "On the other hand, Dikov et al. [63] extended their previous cooperative method [69] by implementing it on SpiNNaker digital processor boards."
          ],
          "intents": [
            "['background']",
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Spiking Cooperative Stereo-Matching at 2 ms Latency with Neuromorphic Hardware",
            "abstract": "",
            "year": 2017,
            "venue": "Living Machines",
            "authors": [
              {
                "authorId": "46775745",
                "name": "G. Dikov"
              },
              {
                "authorId": "145885214",
                "name": "M. Firouzi"
              },
              {
                "authorId": "1685761",
                "name": "Florian Röhrbein"
              },
              {
                "authorId": "3302681",
                "name": "J. Conradt"
              },
              {
                "authorId": "2053647526",
                "name": "Christoph Richter"
              }
            ]
          }
        },
        {
          "citedcorpusid": 44969055,
          "isinfluential": false,
          "contexts": [
            "…(i.e., frame-based) paradigm that decouples the stereo depth estimation problem in two sequential steps: first, establishing stereo correspondences across image planes (“stereo matching”), and then back-projecting the correspondences to compute the associated 3D point (“triangulation”) [101].",
            "Under the static scene assumption, the warp is approximated by the motion field [101] produced by the camera velocity and candidate scene depth (per pixel)."
          ],
          "intents": [
            "['background']",
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Introductory techniques for 3-D computer vision",
            "abstract": "From the Publisher: \nFEATURES: \n \n \nProvides a guide to well-tested theory and algorithms including solutions of problems encountered in modern computer vision. \nContains many practical hints highlighted in the book. \nDevelops two parallel tracks in the presentation, showing how fundamental problems are solved using both intensity and range images, the most popular types of images used today. \nEach chapter contains notes on the literature, review questions, numerical exercises, and projects. \nProvides an Internet list for accessing links to test images, demos, archives and additional learning material.",
            "year": 1998,
            "venue": "",
            "authors": [
              {
                "authorId": "1750195",
                "name": "E. Trucco"
              },
              {
                "authorId": "1716824",
                "name": "A. Verri"
              }
            ]
          }
        },
        {
          "citedcorpusid": 46937991,
          "isinfluential": false,
          "contexts": [
            "For example, the DVS stereo dataset from Andreopoulos et al. [55] comprises a setup of stereo DAVIS240C cameras that is stationary, and therefore only perceives dynamic IMOs in the scene like a rotating fan and a toy butterfly.",
            "While the methods discussed so far employed some event pre-processing (like rectification) on conventional hardware, Andreopoulos et al. [55] implemented a full event-driven non-cooperative local stereo matching pipeline on IBM TrueNorth digital neuromorphic processors.",
            "This need has been identified in the past [55, 135], leading to proposal of a benchmark [135], but it has not been widely adopted."
          ],
          "intents": [
            "['methodology']",
            "['methodology']",
            "['background']"
          ],
          "cited_paper_info": {
            "title": "A Low Power, High Throughput, Fully Event-Based Stereo System",
            "abstract": "We introduce a stereo correspondence system implemented fully on event-based digital hardware, using a fully graph-based non von-Neumann computation model, where no frames, arrays, or any other such data-structures are used. This is the first time that an end-to-end stereo pipeline from image acquisition and rectification, multi-scale spatiotemporal stereo correspondence, winner-take-all, to disparity regularization is implemented fully on event-based hardware. Using a cluster of TrueNorth neurosynaptic processors, we demonstrate their ability to process bilateral event-based inputs streamed live by Dynamic Vision Sensors (DVS), at up to 2,000 disparity maps per second, producing high fidelity disparities which are in turn used to reconstruct, at low power, the depth of events produced from rapidly changing scenes. Experiments on real-world sequences demonstrate the ability of the system to take full advantage of the asynchronous and sparse nature of DVS sensors for low power depth reconstruction, in environments where conventional frame-based cameras connected to synchronous processors would be inefficient for rapidly moving objects. System evaluation on event-based sequences demonstrates a ~ 200 Ã— improvement in terms of power per pixel per disparity map compared to the closest state-of-the-art, and maximum latencies of up to 11ms from spike injection to disparity map ejection.",
            "year": 2018,
            "venue": "2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition",
            "authors": [
              {
                "authorId": "2542089",
                "name": "Alexander Andreopoulos"
              },
              {
                "authorId": "2353154",
                "name": "H. Kashyap"
              },
              {
                "authorId": "103221704",
                "name": "T. Nayak"
              },
              {
                "authorId": "1767364",
                "name": "A. Amir"
              },
              {
                "authorId": "1712991",
                "name": "M. Flickner"
              }
            ]
          }
        },
        {
          "citedcorpusid": 49864158,
          "isinfluential": false,
          "contexts": [
            "…resolution rather than HD cameras because they observed a “smearing effect” on top of the surface of active events, similar to problems reported by [126, 155], where motion blur in the event stream or timestamp delays was observed from sudden and significant contrast changes on DAVIS event cameras.",
            "Then, corners are tracked on the motion-compensated time surfaces using Arc ∗ [126], and used to triangulate for instantaneous stereo matching, as well as for temporal camera tracking."
          ],
          "intents": [
            "['background']",
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Asynchronous Corner Detection and Tracking for Event Cameras in Real Time",
            "abstract": "The recent emergence of bioinspired event cameras has opened up exciting new possibilities in high-frequency tracking, bringing robustness to common problems in traditional vision, such as lighting changes and motion blur. In order to leverage these attractive attributes of the event cameras, research has been focusing on understanding how to process their unusual output: an asynchronous stream of events. With the majority of existing techniques discretizing the event-stream essentially forming frames of events grouped according to their timestamp, we are still to exploit the power of these cameras. In this spirit, this letter proposes a new, purely event-based corner detector, and a novel corner tracker, demonstrating that it is possible to detect corners and track them directly on the event stream in real time. Evaluation on benchmarking datasets reveals a significant boost in the number of detected corners and the repeatability of such detections over the state of the art even in challenging scenarios with the proposed approach while enabling more than a 4$\\times$ speed-up when compared to the most efficient algorithm in the literature. The proposed pipeline detects and tracks corners at a rate of more than 7.5 million events per second, promising great impact in high-speed applications.",
            "year": 2018,
            "venue": "IEEE Robotics and Automation Letters",
            "authors": [
              {
                "authorId": "7278610",
                "name": "Ignacio Alzugaray"
              },
              {
                "authorId": "1885768",
                "name": "M. Chli"
              }
            ]
          }
        },
        {
          "citedcorpusid": 49877954,
          "isinfluential": true,
          "contexts": [
            "Time-surface matching subsumes the temporal coincidence assumption of stereo events and is a building block for finding stereo matches in established event-only stereo methods for VO/SLAM like [41, 58].",
            "This event stereo dataset [58] (Fig.",
            "R P G [ 58 ] N/A M V S E C [ 136 ] D S E C [ 45 ] T U M -",
            "Its mapping module is an improved version of the method in [58], where instantaneous depth is estimated by matching time surfaces across cameras (Fig."
          ],
          "intents": [
            "['background']",
            "--",
            "--",
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Semi-Dense 3D Reconstruction with a Stereo Event Camera",
            "abstract": "Event cameras are bio-inspired sensors that offer several advantages, such as low latency, high-speed and high dynamic range, to tackle challenging scenarios in computer vision. This paper presents a solution to the problem of 3D reconstruction from data captured by a stereo event-camera rig moving in a static scene, such as in the context of stereo Simultaneous Localization and Mapping. The proposed method consists of the optimization of an energy function designed to exploit small-baseline spatio-temporal consistency of events triggered across both stereo image planes. To improve the density of the reconstruction and to reduce the uncertainty of the estimation, a probabilistic depth-fusion strategy is also developed. The resulting method has no special requirements on either the motion of the stereo event-camera rig or on prior knowledge about the scene. Experiments demonstrate our method can deal with both texture-rich scenes as well as sparse scenes, outperforming state-of-the-art stereo methods based on event data image representations.",
            "year": 2018,
            "venue": "European Conference on Computer Vision",
            "authors": [
              {
                "authorId": null,
                "name": "Yi Zhou"
              },
              {
                "authorId": "144036711",
                "name": "Guillermo Gallego"
              },
              {
                "authorId": "3414274",
                "name": "Henri Rebecq"
              },
              {
                "authorId": "1727013",
                "name": "L. Kneip"
              },
              {
                "authorId": "40124570",
                "name": "Hongdong Li"
              },
              {
                "authorId": "2075371",
                "name": "D. Scaramuzza"
              }
            ]
          }
        },
        {
          "citedcorpusid": 73729084,
          "isinfluential": false,
          "contexts": [
            "However, it does not contain GT depth, so only qualitative evaluation of 3D reconstruction algorithms is possible."
          ],
          "intents": [
            "--"
          ],
          "cited_paper_info": {
            "title": "Group-Wise Correlation Stereo Network",
            "abstract": "Stereo matching estimates the disparity between a rectified image pair, which is of great importance to depth sensing, autonomous driving, and other related tasks. Previous works built cost volumes with cross-correlation or concatenation of left and right features across all disparity levels, and then a 2D or 3D convolutional neural network is utilized to regress the disparity maps. In this paper, we propose to construct the cost volume by group-wise correlation. The left features and the right features are divided into groups along the channel dimension, and correlation maps are computed among each group to obtain multiple matching cost proposals, which are then packed into a cost volume. Group-wise correlation provides efficient representations for measuring feature similarities and will not lose too much information like full correlation. It also preserves better performance when reducing parameters compared with previous methods. The 3D stacked hourglass network proposed in previous works is improved to boost the performance and decrease the inference computational cost. Experiment results show that our method outperforms previous methods on Scene Flow, KITTI 2012, and KITTI 2015 datasets.",
            "year": 2019,
            "venue": "Computer Vision and Pattern Recognition",
            "authors": [
              {
                "authorId": "49932298",
                "name": "Xiaoyang Guo"
              },
              {
                "authorId": "2118048994",
                "name": "Kai Yang"
              },
              {
                "authorId": "3432961",
                "name": "Wukui Yang"
              },
              {
                "authorId": "31843833",
                "name": "Xiaogang Wang"
              },
              {
                "authorId": "47893312",
                "name": "Hongsheng Li"
              }
            ]
          }
        },
        {
          "citedcorpusid": 84182058,
          "isinfluential": true,
          "contexts": [
            "6: Results of line-based stereo matching proposed in [54].",
            "Image courtesy of [54].",
            "In this regard, a line-based feature matching algorithm for depth estimation was proposed in [54] which showed promising results on short real-world scenarios with moving cameras (Fig."
          ],
          "intents": [
            "['background']",
            "--",
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Event-based Depth Reconstruction Using Stereo Dynamic Vision Sensors",
            "abstract": "In this thesis, different types of stereo matching algorithms for event-based vision sensors are developed and analysed. The algorithms are based on a) single-event matching, b) event-cloud alignment and c) feature extraction and matching. Analysis focusses on both accuracy as well as computational cost and suitability for real-time usage, and compares the results of the proposed methods with values published in current literature.",
            "year": 2018,
            "venue": "",
            "authors": [
              {
                "authorId": "2694609",
                "name": "Lukas Everding"
              }
            ]
          }
        },
        {
          "citedcorpusid": 119309624,
          "isinfluential": false,
          "contexts": [
            "This has connections with the Contrast Maximization ( CMax ) framework [123, 124]: the depth slices of the DSI can be interpreted as Images of Warped Events ( IWEs ) and recovery of the 3D scene structure amounts to finding the depth map whose IWE has maximum contrast (i.e., best event alignment,…"
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Focus Is All You Need: Loss Functions for Event-Based Vision",
            "abstract": "Event cameras are novel vision sensors that output pixel-level brightness changes (\"events\") instead of traditional video frames. These asynchronous sensors offer several advantages over traditional cameras, such as, high temporal resolution, very high dynamic range, and no motion blur. To unlock the potential of such sensors, motion compensation methods have been recently proposed. We present a collection and taxonomy of twenty two objective functions to analyze event alignment in motion compensation approaches. We call them focus loss functions since they have strong connections with functions used in traditional shape-from-focus applications. The proposed loss functions allow bringing mature computer vision tools to the realm of event cameras. We compare the accuracy and runtime performance of all loss functions on a publicly available dataset, and conclude that the variance, the gradient and the Laplacian magnitudes are among the best loss functions. The applicability of the loss functions is shown on multiple tasks: rotational motion, depth and optical flow estimation. The proposed focus loss functions allow to unlock the outstanding properties of event cameras.",
            "year": 2019,
            "venue": "Computer Vision and Pattern Recognition",
            "authors": [
              {
                "authorId": "144036711",
                "name": "Guillermo Gallego"
              },
              {
                "authorId": "8329387",
                "name": "Mathias Gehrig"
              },
              {
                "authorId": "2075371",
                "name": "D. Scaramuzza"
              }
            ]
          }
        },
        {
          "citedcorpusid": 157060825,
          "isinfluential": false,
          "contexts": [
            "Zou et al. [65] also demonstrated decent depth estimation results on sequences with ego-motion by stereo-matching sharp event images obtained by pixel-wise accumulation of events over their lifetime [108]."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "ROBUST DENSE DEPTH MAP ESTIMATION FROM SPARSE DVS STEREOS 3 2 Related Work",
            "abstract": "Real-world depth perception applications require precise reaction to fast motion, and the ability to operate in scenes which contain large intensity differences or high dynamic range. Standard CMOS cameras based methods for depth computing, such as stereo matching, run into the problem of huge power consuming at high frame-rates or inaccurate depths with noise or holes. The event camera, DVS (Dynamic Vision Sensor), aims to be robust to fast motion and light change with low power consumption and sparse representation, offering great potential to replace standard cameras for depth perception. However, it is not trivial to directly apply DVS for stereo matching due to its nature of low latency and sparsity which will result in extremely limited available information and imperfect imaging quality. To overcome these problems and make the DVS available for depth perception, this paper introduces a novel method which can enhance the stream of events and estimate the dense depth through event driven stereo matching. Our event stream enhancement algorithm efficiently buffers events according to time continuous rather than using artificially-chosen time intervals, and our stereo matching method can robust estimate depth for complex scenarios regardless of the motion or light changing. To the best of our knowledge, this is the first algorithm provably able to recover dense depth maps from sparse DVS stereos. Experiments in a variety of challenging conditions demonstrate the superior performance of our method. c © 2017. The copyright of this document resides with its authors. It may be distributed unchanged freely in print or electronic forms. 2 ZOU ET AL.: ROBUST DENSE DEPTH MAP ESTIMATION FROM SPARSE DVS STEREOS",
            "year": 2017,
            "venue": "",
            "authors": [
              {
                "authorId": "2780914",
                "name": "Dongqing Zou"
              },
              {
                "authorId": "2053742370",
                "name": "Feng Shi"
              },
              {
                "authorId": "2153282723",
                "name": "Qiang Wang"
              }
            ]
          }
        },
        {
          "citedcorpusid": 167210006,
          "isinfluential": false,
          "contexts": [
            "2019 [3] Bio-inspired stereo vision with event cameras; focus on cooperative networks.",
            "Steffen et al [3] review various bio-inspired aspects of event cameras and the stereo algorithms that may benefit from their novel data."
          ],
          "intents": [
            "--",
            "--"
          ],
          "cited_paper_info": {
            "title": "Neuromorphic Stereo Vision: A Survey of Bio-Inspired Sensors and Algorithms",
            "abstract": "Any visual sensor, whether artificial or biological, maps the 3D-world on a 2D-representation. The missing dimension is depth and most species use stereo vision to recover it. Stereo vision implies multiple perspectives and matching, hence it obtains depth from a pair of images. Algorithms for stereo vision are also used prosperously in robotics. Although, biological systems seem to compute disparities effortless, artificial methods suffer from high energy demands and latency. The crucial part is the correspondence problem; finding the matching points of two images. The development of event-based cameras, inspired by the retina, enables the exploitation of an additional physical constraint—time. Due to their asynchronous course of operation, considering the precise occurrence of spikes, Spiking Neural Networks take advantage of this constraint. In this work, we investigate sensors and algorithms for event-based stereo vision leading to more biologically plausible robots. Hereby, we focus mainly on binocular stereo vision.",
            "year": 2019,
            "venue": "Front. Neurorobot.",
            "authors": [
              {
                "authorId": "80693972",
                "name": "Lea Steffen"
              },
              {
                "authorId": "79456322",
                "name": "Daniel Reichard"
              },
              {
                "authorId": "80999556",
                "name": "Jakob Weinland"
              },
              {
                "authorId": "40647726",
                "name": "Jacques Kaiser"
              },
              {
                "authorId": "1382189271",
                "name": "A. Roennau"
              },
              {
                "authorId": "144427136",
                "name": "R. Dillmann"
              }
            ]
          }
        },
        {
          "citedcorpusid": 205698386,
          "isinfluential": true,
          "contexts": [
            "For example, in [64], the cooperative method is realized with a hierarchical SNN architecture of coincidence and disparity detector layers (Fig.",
            "Risi et al [43] further demonstrated the efficiency and performance of the DYNAP implementation of the cooperative network in [64] by evaluating it on complex real-world scenes from the DHP19 3D human pose tracking dataset.",
            "Recently, Kim et al [34] demonstrated improved power and hardware efficiency by trading off latency and accuracy while implementing the architecture of [64] entirely on Field Programmable Gate Arrays ( FPGA ).",
            "7: Asynchronous cooperative stereo method in [64].",
            "Image courtesy of [64]. of stereo algorithms known as “cooperative stereo” uses a dynamic network-based computational model of binocular stereopsis for finding correspondences, making it highly efficient for sparse asynchronous event-driven processing on specialized neuromorphic hardware (Fig.",
            "Recent works have proven this idea on small-scale scenes with stationary cameras [43, 49, 63, 64]."
          ],
          "intents": [
            "['methodology']",
            "['background']",
            "['background']",
            "--",
            "['background']",
            "['background']"
          ],
          "cited_paper_info": {
            "title": "A spiking neural network model of 3D perception for event-based neuromorphic stereo vision systems",
            "abstract": "Stereo vision is an important feature that enables machine vision systems to perceive their environment in 3D. While machine vision has spawned a variety of software algorithms to solve the stereo-correspondence problem, their implementation and integration in small, fast, and efficient hardware vision systems remains a difficult challenge. Recent advances made in neuromorphic engineering offer a possible solution to this problem, with the use of a new class of event-based vision sensors and neural processing devices inspired by the organizing principles of the brain. Here we propose a radically novel model that solves the stereo-correspondence problem with a spiking neural network that can be directly implemented with massively parallel, compact, low-latency and low-power neuromorphic engineering devices. We validate the model with experimental results, highlighting features that are in agreement with both computational neuroscience stereo vision theories and experimental findings. We demonstrate its features with a prototype neuromorphic hardware system and provide testable predictions on the role of spike-based representations and temporal dynamics in biological stereo vision processing systems.",
            "year": 2017,
            "venue": "Scientific Reports",
            "authors": [
              {
                "authorId": "2598816",
                "name": "Marc Osswald"
              },
              {
                "authorId": "144975525",
                "name": "S. Ieng"
              },
              {
                "authorId": "1750848",
                "name": "R. Benosman"
              },
              {
                "authorId": "1721210",
                "name": "G. Indiveri"
              }
            ]
          }
        },
        {
          "citedcorpusid": 238198645,
          "isinfluential": false,
          "contexts": [
            "StereoSpike [37] is the only SNN that effectively uses deep learning for stereo depth estimation."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "StereoSpike: Depth Learning With a Spiking Neural Network",
            "abstract": "Depth estimation is an important computer vision task, useful in particular for navigation in autonomous vehicles, or for object manipulation in robotics. Here, we propose to solve it using StereoSpike, an end-to-end neuromorphic approach, combining two event-based cameras and a Spiking Neural Network (SNN) with a modified U-Net-like encoder-decoder architecture. More specifically, we used the Multi Vehicle Stereo Event Camera Dataset (MVSEC). It provides a depth ground-truth, which was used to train StereoSpike in a supervised manner, using surrogate gradient descent. We propose a novel readout paradigm to obtain a dense analog prediction–the depth of each pixel– from the spikes of the decoder. We demonstrate that this architecture generalizes very well, even better than its non-spiking counterparts, leading to near state-of-the-art test accuracy. To the best of our knowledge, it is the first time that such a large-scale regression problem is solved by a fully spiking neural network. Finally, we show that very low firing rates (< 5%) can be obtained via regularization, with a minimal cost in accuracy. This means that StereoSpike could be efficiently implemented on neuromorphic chips, opening the door for low power and real time embedded systems.",
            "year": 2021,
            "venue": "IEEE Access",
            "authors": [
              {
                "authorId": "2130588381",
                "name": "Ulysse Rançon"
              },
              {
                "authorId": "2129996240",
                "name": "Javier Cuadrado-Anibarro"
              },
              {
                "authorId": "49953506",
                "name": "B. Cottereau"
              },
              {
                "authorId": "2441104",
                "name": "T. Masquelier"
              }
            ]
          }
        },
        {
          "citedcorpusid": 244306440,
          "isinfluential": true,
          "contexts": [
            "Image courtesy of [42]. frames are missing, it can still output disparity.",
            "11: Network structure of EI-Stereo [42].",
            "While the former is more efficient and was also used in EI-Stereo [42] and Conc-Net [30], the latter produced better overall performance on the DSEC disparity benchmark.",
            "The first learning-based stereo depth estimation using both frames and events was proposed in Event-Intensity Stereo ( EI-Stereo or EIS ) [42] (Fig."
          ],
          "intents": [
            "['background']",
            "--",
            "['methodology']",
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Event-Intensity Stereo: Estimating Depth by the Best of Both Worlds",
            "abstract": "Event cameras can report scene movements as an asynchronous stream of data called the events. Unlike traditional cameras, event cameras have very low latency (microseconds vs milliseconds) very high dynamic range (140 dB vs 60 dB), and low power consumption, as they report changes of a scene and not a complete frame. As they re- port per pixel feature-like events and not the whole intensity frame they are immune to motion blur. However, event cameras require movement between the scene and camera to fire events, i.e., they have no output when the scene is relatively static. Traditional cameras, however, report the whole frame of pixels at once in fixed intervals but have lower dynamic range and are prone to motion blur in case of rapid movements. We get the best from both worlds and use events and intensity images together in our complementary design and estimate dense disparity from this combination. The proposed end-to-end design combines events and images in a sequential manner and correlates them to estimate dense depth values. Our various experimental settings in real-world and simulated scenarios exploit the superiority of our method in predicting accurate depth values with fine details. We further extend our method to extreme cases of missing the left or right event or stereo pair and also investigate stereo depth estimation with inconsistent dynamic ranges or event thresholds on the left and right pairs.",
            "year": 2021,
            "venue": "IEEE International Conference on Computer Vision",
            "authors": [
              {
                "authorId": "114141661",
                "name": "Mohammad Mostafavi"
              },
              {
                "authorId": "51182421",
                "name": "Kuk-Jin Yoon"
              },
              {
                "authorId": "2119579051",
                "name": "Jonghyun Choi"
              }
            ]
          }
        },
        {
          "citedcorpusid": 246656358,
          "isinfluential": false,
          "contexts": [
            "Recently, Furmonas et al published a review article [4] that dives into various event-based depth estimation meth-ods, both monocular and stereo.",
            "2022 [4] Event-based monocular and stereo depth."
          ],
          "intents": [
            "['background']",
            "--"
          ],
          "cited_paper_info": {
            "title": "Analytical Review of Event-Based Camera Depth Estimation Methods and Systems",
            "abstract": "Event-based cameras have increasingly become more commonplace in the commercial space as the performance of these cameras has also continued to increase to the degree where they can exponentially outperform their frame-based counterparts in many applications. However, instantiations of event-based cameras for depth estimation are sparse. After a short introduction detailing the salient differences and features of an event-based camera compared to that of a traditional, frame-based one, this work summarizes the published event-based methods and systems known to date. An analytical review of these methods and systems is performed, justifying the conclusions drawn. This work is concluded with insights and recommendations for further development in the field of event-based camera depth estimation.",
            "year": 2022,
            "venue": "Italian National Conference on Sensors",
            "authors": [
              {
                "authorId": "2153753559",
                "name": "Justas Furmonas"
              },
              {
                "authorId": "2286357",
                "name": "J. Liobe"
              },
              {
                "authorId": "2781315",
                "name": "V. Barzdėnas"
              }
            ]
          }
        },
        {
          "citedcorpusid": 248227281,
          "isinfluential": false,
          "contexts": [
            "However, this limits the FOV of the sensors (59 ◦ HFOV/ 34 ◦ VFOV in [127]) and the light incident on each pixel.",
            "An alternative solution is using a beam splitter setup with separate frame-based and event-based cameras, like in [127, 145, 148, 149].",
            "More-over, only qualitative results are shown on depth estimation, which do not look accurate compared to monocular meth-ods like [127]."
          ],
          "intents": [
            "['background']",
            "['methodology']",
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Event-aided Direct Sparse Odometry",
            "abstract": "We introduce EDS, a direct monocular visual odometry using events and frames. Our algorithm leverages the event generation model to track the camera motion in the blind time between frames. The method formulates a direct probabilistic approach of observed brightness increments. Per-pixel brightness increments are predicted using a sparse number of selected 3D points and are compared to the events via the brightness increment error to estimate camera motion. The method recovers a semi-dense 3D map using photometric bundle adjustment. EDS is the first method to perform 6-DOF VO using events and frames with a direct approach. By design it overcomes the problem of changing appearance in indirect methods. Our results outperform all previous event-based odometry solutions. We also show that, for a target error performance, EDS can work at lower frame rates than state-of-the-art frame-based VO solutions. This opens the door to low-power motion-tracking applications where frames are sparingly triggered “on demand” and our method tracks the motion in between. We release code and datasets to the public.",
            "year": 2022,
            "venue": "Computer Vision and Pattern Recognition",
            "authors": [
              {
                "authorId": "2065112865",
                "name": "Javier Hidalgo-Carri'o"
              },
              {
                "authorId": "144036711",
                "name": "Guillermo Gallego"
              },
              {
                "authorId": "2075371",
                "name": "D. Scaramuzza"
              }
            ]
          }
        },
        {
          "citedcorpusid": 248572428,
          "isinfluential": false,
          "contexts": [
            "I M O 2 [ 139 ] V E C t o r [ 138 ] H K U V I O [ 25 ] N/A M 3 E D [ 140 ] S H E F [ 39 ] (a) Sensors National University targets the problem of stereo 3D reconstruction between a frame-based and an event-based camera.",
            "The EV-IMO2 dataset [139] (Fig."
          ],
          "intents": [
            "['background']",
            "--"
          ],
          "cited_paper_info": {
            "title": "EVIMO2: An Event Camera Dataset for Motion Segmentation, Optical Flow, Structure from Motion, and Visual Inertial Odometry in Indoor Scenes with Monocular or Stereo Algorithms",
            "abstract": "A new event camera dataset, EVIMO2, is introduced that improves on the popular EVIMO dataset by providing more data, from better cameras, in more complex scenarios. As with its predecessor, EVIMO2 provides labels in the form of per-pixel ground truth depth and segmentation as well as camera and object poses. All sequences use data from physical cameras and many sequences feature multiple independently moving objects. Typically, such labeled data is unavailable in physical event camera datasets. Thus, EVIMO2 will serve as a challenging benchmark for existing algorithms and rich training set for the development of new algorithms. In particular, EVIMO2 is suited for sup-porting research in motion and object segmentation, optical ﬂow, structure from motion, and visual (inertial) odometry in both monocular or stereo conﬁgurations. EVIMO2 consists of 41 minutes of data from three 640 × 480 event cameras, one 2080 × 1552 classical color camera, inertial measurements from two six axis inertial measurement units, and millimeter accurate object poses from a Vicon motion capture system. The dataset’s 173 sequences are arranged into three categories. 3.75 minutes of independently moving household objects, 22.55 minutes of static scenes, and 14.85 minutes of basic motions in shallow scenes. Some sequences were recorded in low-light condi-tions where conventional cameras fail. Depth and segmentation are provided at 60 Hz for the event cameras and 30 Hz for the classical camera. The masks can be regenerated using open-source code up to rates as high as 200 Hz. This technical report brieﬂy describes EVIMO2. The full documentation is available online 1 . Videos of individual sequences can be sampled on the download page 2 .",
            "year": 2022,
            "venue": "arXiv.org",
            "authors": [
              {
                "authorId": "2096137947",
                "name": "Levi Burner"
              },
              {
                "authorId": "144559298",
                "name": "A. Mitrokhin"
              },
              {
                "authorId": "3415312",
                "name": "Cornelia Fermuller"
              },
              {
                "authorId": "1697493",
                "name": "Y. Aloimonos"
              }
            ]
          }
        },
        {
          "citedcorpusid": 250127779,
          "isinfluential": false,
          "contexts": [
            "I M O 2 [ 139 ] V E C t o r [ 138 ] H K U V I O [ 25 ] N/A M 3 E D [ 140 ] S H E F [ 39 ] (a) Sensors National University targets the problem of stereo 3D reconstruction between a frame-based and an event-based camera.",
            "The VECtor benchmark dataset [138] (Fig.",
            "Moreover, it is inherently affected by shutter noise due to the DAVIS’ hybrid event-frame mode [138]."
          ],
          "intents": [
            "['background']",
            "--",
            "['background']"
          ],
          "cited_paper_info": {
            "title": "VECtor: A Versatile Event-Centric Benchmark for Multi-Sensor SLAM",
            "abstract": "Event cameras have recently gained in popularity as they hold strong potential to complement regular cameras in situations of high dynamics or challenging illumination. An important problem that may benefit from the addition of an event camera is given by Simultaneous Localization And Mapping (SLAM). However, in order to ensure progress on event-inclusive multi-sensor SLAM, novel benchmark sequences are needed. Our contribution is the first complete set of benchmark datasets captured with a multi-sensor setup containing an event-based stereo camera, a regular stereo camera, multiple depth sensors, and an inertial measurement unit. The setup is fully hardware-synchronized and underwent accurate extrinsic calibration. All sequences come with ground truth data captured by highly accurate external reference devices such as a motion capture system. Individual sequences include both small and large-scale environments, and cover the specific challenges targeted by dynamic vision sensors.",
            "year": 2022,
            "venue": "IEEE Robotics and Automation Letters",
            "authors": [
              {
                "authorId": "2148991481",
                "name": "Ling Gao"
              },
              {
                "authorId": "72322304",
                "name": "Y. Liang"
              },
              {
                "authorId": "1423718086",
                "name": "Jiaqi Yang"
              },
              {
                "authorId": "2174101946",
                "name": "Shaoxun Wu"
              },
              {
                "authorId": "2109502285",
                "name": "Chenyu Wang"
              },
              {
                "authorId": "2120262069",
                "name": "Jiaben Chen"
              },
              {
                "authorId": "1727013",
                "name": "L. Kneip"
              }
            ]
          }
        },
        {
          "citedcorpusid": 250607506,
          "isinfluential": false,
          "contexts": [
            "An alternative solution is using a beam splitter setup with separate frame-based and event-based cameras, like in [127, 145, 148, 149]."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Stereo Co-capture System for Recording and Tracking Fish with Frame- and Event Cameras",
            "abstract": "This work introduces a co-capture system for multi-animal visual data acquisition using conventional cameras and event cameras. Event cameras offer multiple advantages over frame-based cameras, such as a high temporal resolution and temporal redundancy suppression, which enable us to efficiently capture the fast and erratic movements of fish. We furthermore present an event-based multi-animal tracking algorithm, which proves the feasibility of the approach and sets the baseline for further exploration of combining the advantages of event cameras and conventional cameras for multi-animal tracking.",
            "year": 2022,
            "venue": "arXiv.org",
            "authors": [
              {
                "authorId": "1580402626",
                "name": "Friedhelm Hamann"
              },
              {
                "authorId": "144036711",
                "name": "Guillermo Gallego"
              }
            ]
          }
        },
        {
          "citedcorpusid": 250699235,
          "isinfluential": false,
          "contexts": [
            "Specifically, it proposes a temporal disparity consistency loss to align GT disparity maps from different time steps, on top of established paradigms of L 1 disparity loss and Contrast Maximization losses [12, 115, 116].",
            "Recently, Shiba et al. [12] extended the CMax framework to simultaneously estimate depth and ego-motion using an optical flow warp."
          ],
          "intents": [
            "--",
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Secrets of Event-Based Optical Flow",
            "abstract": "Event cameras respond to scene dynamics and offer advantages to estimate motion. Following recent image-based deep-learning achievements, optical flow estimation methods for event cameras have rushed to combine those image-based methods with event data. However, it requires several adaptations (data conversion, loss function, etc.) as they have very different properties. We develop a principled method to extend the Contrast Maximization framework to estimate optical flow from events alone. We investigate key elements: how to design the objective function to prevent overfitting, how to warp events to deal better with occlusions, and how to improve convergence with multi-scale raw events. With these key elements, our method ranks first among unsupervised methods on the MVSEC benchmark, and is competitive on the DSEC benchmark. Moreover, our method allows us to expose the issues of the ground truth flow in those benchmarks, and produces remarkable results when it is transferred to unsupervised learning settings. Our code is available at https://github.com/tub-rip/event_based_optical_flow",
            "year": 2022,
            "venue": "European Conference on Computer Vision",
            "authors": [
              {
                "authorId": "2066324243",
                "name": "Shintaro Shiba"
              },
              {
                "authorId": "1716469",
                "name": "Y. Aoki"
              },
              {
                "authorId": "144036711",
                "name": "Guillermo Gallego"
              }
            ]
          }
        },
        {
          "citedcorpusid": 250918780,
          "isinfluential": true,
          "contexts": [
            "Values for CopNet [62] and TSES [57] are taken from [57], whereas the others are from [29].",
            "However, as noticed in [29], the baseline of 11.84 cm is too big for the small depth range (nearest object ≈ 40cm away) in these sequences.",
            "Event-based stereo depth estimation has notable advantages over monocular (temporal) stereo [29]: higher accurate, faster mapping, outlier removal and absolute scale recovery.",
            "The advantages of stereo over monocular (e.g., [29] vs [121]) due to exploiting spatial parallax are consistently clear on both tables: mean errors decrease by 30–45%, and outliers also decrease (by more than half) while the number of recovered points remains.",
            "Images courtesy of [29, 121].",
            "The calib A sequences were better calibrated than the calib B ones [29].",
            "More details are provided in [29].",
            "Grid-like 3D event representations [29, 57, 115] like DSIs have been shown to preserve finer details in depth estimation.",
            "EMVS has been extended to the stereo setup in Multi-Camera ( MC )- EMVS [29], where DSIs computed from individual cameras are fused using element-wise operations like the harmonic mean, and the local maxima of the fused DSI yields the depth map (Fig."
          ],
          "intents": [
            "['background']",
            "['background']",
            "['background']",
            "['background']",
            "--",
            "['methodology']",
            "--",
            "['background']",
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Multi‐Event‐Camera Depth Estimation and Outlier Rejection by Refocused Events Fusion",
            "abstract": "Event cameras are bio‐inspired sensors that offer advantages over traditional cameras. They operate asynchronously, sampling the scene at microsecond resolution and producing a stream of brightness changes. This unconventional output has sparked novel computer vision methods to unlock the camera's potential. Here, the problem of event‐based stereo 3D reconstruction for SLAM is considered. Most event‐based stereo methods attempt to exploit the high temporal resolution of the camera and the simultaneity of events across cameras to establish matches and estimate depth. By contrast, this work investigates how to estimate depth without explicit data association by fusing disparity space images (DSIs) originated in efficient monocular methods. Fusion theory is developed and applied to design multi‐camera 3D reconstruction algorithms that produce state‐of‐the‐art results, as confirmed by comparisons with four baseline methods and tests on a variety of available datasets.",
            "year": 2022,
            "venue": "Advanced Intelligent Systems",
            "authors": [
              {
                "authorId": "2155615482",
                "name": "Suman Ghosh"
              },
              {
                "authorId": "144036711",
                "name": "Guillermo Gallego"
              }
            ]
          }
        },
        {
          "citedcorpusid": 252476994,
          "isinfluential": false,
          "contexts": [
            "To eliminate the cost of using two separate event cameras for stereo matching, [33] mounted a stereo lens on a single event camera (Fig.",
            "8: Setup using a stereo lens and an event camera [33]. approximating performance metrics (from data reported in the original papers on different evaluation datasets)."
          ],
          "intents": [
            "['background']",
            "['background']"
          ],
          "cited_paper_info": {
            "title": "3D Events with Stereoscopy with a single Dynamic Vision System",
            "abstract": "Considering the cost of a single event-based camera, classical 3D imaging approaches using two or more synchronized cameras may be cost-prohibitive. This work explores 3D imaging using a single stationary camera and a stereoscopic lens setup.",
            "year": 2022,
            "venue": "Imaging and Applied Optics Congress 2022 (3D, AOA, COSI, ISA, pcAOP)",
            "authors": [
              {
                "authorId": "2185897689",
                "name": "Raviv Ilani"
              },
              {
                "authorId": "2185904854",
                "name": "Adi Reich"
              },
              {
                "authorId": "2185904780",
                "name": "Moshik Schindelhaim"
              },
              {
                "authorId": "94314653",
                "name": "A. Stern"
              }
            ]
          }
        },
        {
          "citedcorpusid": 253651036,
          "isinfluential": false,
          "contexts": [
            "A stereo matching algorithm between a frame-based and an event camera has been proposed in [36]."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Real-Time Hetero-Stereo Matching for Event and Frame Camera With Aligned Events Using Maximum Shift Distance",
            "abstract": "Event cameras can show better performance than frame cameras in challenging scenarios, such as fast-moving environments or high-dynamic-range scenes. However, it is still difficult for event cameras to replace frame cameras in non-challenging normal scenarios. In order to leverage the advantages of both cameras, we conduct a study for the heterogeneous stereo camera system which employs both an event and a frame camera. The proposed system estimates the semi-dense disparity in real-time by matching heterogeneous data of an event and a frame camera in stereo. We propose an accurate, intuitive and efficient way to align events with 6-DOF camera motion, by suggesting the maximum shift distance method. The aligned event image shows high similarity to the edge image of the frame camera. The proposed method can estimate poses of an event camera and depth of events in a few frames, which can speed up the initialization of the event camera system. We verified our algorithm in the DSEC dataset. The proposed hetero-stereo matching outperformed other methods. For real-time operation, we implemented our code using parallel computation with CUDA and release our code open source:",
            "year": 2023,
            "venue": "IEEE Robotics and Automation Letters",
            "authors": [
              {
                "authorId": "2108880866",
                "name": "Haram Kim"
              },
              {
                "authorId": "2108077736",
                "name": "Sangil Lee"
              },
              {
                "authorId": "2125035466",
                "name": "Junha Kim"
              },
              {
                "authorId": "2161495857",
                "name": "H. J. Kim"
              }
            ]
          }
        },
        {
          "citedcorpusid": 254531210,
          "isinfluential": false,
          "contexts": [
            "Recently, Kim et al [34] demonstrated improved power and hardware efficiency by trading off latency and accuracy while implementing the architecture of [64] entirely on Field Programmable Gate Arrays ( FPGA ).",
            "This architecture was also subsequently adapted and implemented in neuromorphic hardware platforms [43, 49], and on FPGA [34]."
          ],
          "intents": [
            "['background']",
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Spiking Cooperative Network Implemented on FPGA for Real-Time Event-Based Stereo System",
            "abstract": "A hardware-efficient implementation of a spiking cooperative network (SCN) for a real-time event-based stereo correspondence system is presented. While fully utilizing the advantage of event data, the proposed SCN design significantly reduces the amount of hardware resources by utilizing distinct properties of the SCN, such as the repeatability of synaptic connections and operations, through physical constraints. A stereo system consisting of a field-programmable gate array (FPGA) and a pair of dynamic vision sensors (DVSs) is implemented to demonstrate the SCN design. Stereo livestreamed event data are generated from the DVSs, and the SCN is implemented on an FPGA chip to process the event data. The SCN system has four cores, each comprising an array of 32 Coincidence-Disparity units that calculate the 32-level disparity in a semi-parallel manner. The system performance was evaluated experimentally to estimate the depth of objects moving at different speeds. A rotating drum with a diameter of 8 cm was used in the test. The median relative error of the estimated depth at a rotation speed of 16.7 Hz ranged from 7.3% to 10.6%.",
            "year": 2022,
            "venue": "IEEE Access",
            "authors": [
              {
                "authorId": "2109982138",
                "name": "Jung-Gyun Kim"
              },
              {
                "authorId": "49077100",
                "name": "Donghwan Seo"
              },
              {
                "authorId": "40654693",
                "name": "Byung-geun Lee"
              }
            ]
          }
        },
        {
          "citedcorpusid": 255125395,
          "isinfluential": false,
          "contexts": [
            "Hence, Liu et al [22] have proposed a direct method of state estimation using stereo events and IMU without explicit feature tracking, referred to as direct-ESVIO .",
            "Event-based Stereo Visual Odometry ( ESVO ) [41] is an established system on which others build upon [19, 22, 125]."
          ],
          "intents": [
            "--",
            "--"
          ],
          "cited_paper_info": {
            "title": "ESVIO: Event-Based Stereo Visual Inertial Odometry",
            "abstract": "Event cameras that asynchronously output low-latency event streams provide great opportunities for state estimation under challenging situations. Despite event-based visual odometry having been extensively studied in recent years, most of them are based on the monocular, while few research on stereo event vision. In this letter, we present ESVIO, the first event-based stereo visual-inertial odometry, which leverages the complementary advantages of event streams, standard images, and inertial measurements. Our proposed pipeline includes the ESIO (purely event-based) and ESVIO (event with image-aided), which achieves spatial and temporal associations between consecutive stereo event streams. A well-design back-end tightly-coupled fused the multi-sensor measurement to obtain robust state estimation. We validate that both ESIO and ESVIO have superior performance compared with other image-based and event-based baseline methods on public and self-collected datasets. Furthermore, we use our pipeline to perform onboard quadrotor flights under low-light environments. Autonomous driving data sequences and real-world large-scale experiments are also conducted to demonstrate long-term effectiveness. We highlight that this work is a real-time, accurate system that is aimed at robust state estimation under challenging environments.",
            "year": 2022,
            "venue": "IEEE Robotics and Automation Letters",
            "authors": [
              {
                "authorId": "120125963",
                "name": "Pei-Ying Chen"
              },
              {
                "authorId": "31327386",
                "name": "W. Guan"
              },
              {
                "authorId": "2069299246",
                "name": "P. Lu"
              }
            ]
          }
        },
        {
          "citedcorpusid": 257019827,
          "isinfluential": true,
          "contexts": [
            "2023 [5] Deep-learning–based event-vision applications; benchmarking on image reconstruction and optical flow.",
            "However, the primary focus of [5] seems to be on learning methods for image reconstruction and optical flow estimation (probably due to these problems being more investigated than depth estimation in the context of learning).",
            "A survey on deep learning methods for event-based vision by Zheng et al. [5] dedicates a section to stereo depth estimation methods."
          ],
          "intents": [
            "--",
            "--",
            "--"
          ],
          "cited_paper_info": {
            "title": "Deep Learning for Event-based Vision: A Comprehensive Survey and Benchmarks",
            "abstract": "Event cameras are bio-inspired sensors that capture the per-pixel intensity changes asynchronously and produce event streams encoding the time, pixel position, and polarity (sign) of the intensity changes. Event cameras possess a myriad of advantages over canonical frame-based cameras, such as high temporal resolution, high dynamic range, low latency, etc. Being capable of capturing information in challenging visual conditions, event cameras have the potential to overcome the limitations of frame-based cameras in the computer vision and robotics community. In very recent years, deep learning (DL) has been brought to this emerging field and inspired active research endeavors in mining its potential. However, there is still a lack of taxonomies in DL techniques for event-based vision. We first scrutinize the typical event representations with quality enhancement methods as they play a pivotal role as inputs to the DL models. We then provide a comprehensive survey of existing DL-based methods by structurally grouping them into two major categories: 1) image/video reconstruction and restoration; 2) event-based scene understanding and 3D vision. We conduct benchmark experiments for the existing methods in some representative research directions, i.e., image reconstruction, deblurring, and object recognition, to identify some critical insights and problems. Finally, we have discussions regarding the challenges and provide new perspectives for inspiring more research studies.",
            "year": 2023,
            "venue": "arXiv.org",
            "authors": [
              {
                "authorId": "2142380383",
                "name": "Xueye Zheng"
              },
              {
                "authorId": "2198472382",
                "name": "Yexin Liu"
              },
              {
                "authorId": "2014217",
                "name": "Yunfan Lu"
              },
              {
                "authorId": "2207729945",
                "name": "Tongyan Hua"
              },
              {
                "authorId": "2207727652",
                "name": "Tianbo Pan"
              },
              {
                "authorId": "51027868",
                "name": "Weiming Zhang"
              },
              {
                "authorId": "2075330732",
                "name": "Dacheng Tao"
              },
              {
                "authorId": "2168616303",
                "name": "Lin Wang"
              }
            ]
          }
        },
        {
          "citedcorpusid": 258213006,
          "isinfluential": false,
          "contexts": [
            "Also, a survey on SLAM using event cameras by Wang et al. [6] discusses depth estimation and camera tracking using monocular and stereo setups; the main focus is on the monocular setup.",
            "2023 [6] Event-based SLAM."
          ],
          "intents": [
            "['methodology']",
            "--"
          ],
          "cited_paper_info": {
            "title": "Event-based Simultaneous Localization and Mapping: A Comprehensive Survey",
            "abstract": "In recent decades, visual simultaneous localization and mapping (vSLAM) has gained significant interest in both academia and industry. It estimates camera motion and reconstructs the environment concurrently using visual sensors on a moving robot. However, conventional cameras are limited by hardware, including motion blur and low dynamic range, which can negatively impact performance in challenging scenarios like high-speed motion and high dynamic range illumination. Recent studies have demonstrated that event cameras, a new type of bio-inspired visual sensor, offer advantages such as high temporal resolution, dynamic range, low power consumption, and low latency. This paper presents a timely and comprehensive review of event-based vSLAM algorithms that exploit the benefits of asynchronous and irregular event streams for localization and mapping tasks. The review covers the working principle of event cameras and various event representations for preprocessing event data. It also categorizes event-based vSLAM methods into four main categories: feature-based, direct, motion-compensation, and deep learning methods, with detailed discussions and practical guidance for each approach. Furthermore, the paper evaluates the state-of-the-art methods on various benchmarks, highlighting current challenges and future opportunities in this emerging research area. A public repository will be maintained to keep track of the rapid developments in this field at {\\url{https://github.com/kun150kun/ESLAM-survey}}.",
            "year": 2023,
            "venue": "arXiv.org",
            "authors": [
              {
                "authorId": "1915412351",
                "name": "Kunping Huang"
              },
              {
                "authorId": "2142418483",
                "name": "Sen Zhang"
              },
              {
                "authorId": "1519070643",
                "name": "Jing Zhang"
              },
              {
                "authorId": "2075330732",
                "name": "Dacheng Tao"
              }
            ]
          }
        },
        {
          "citedcorpusid": 259075396,
          "isinfluential": false,
          "contexts": [
            "Finally, Deep Hybrid Parallel Tracking and Mapping ( DH-PTAM ) [21] uses a hybrid approach, combining model-based estimation with deep learning, for solving SLAM with stereo pairs of event and frame-based cameras."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "DH-PTAM: A Deep Hybrid Stereo Events-Frames Parallel Tracking and Mapping System",
            "abstract": "This paper presents a robust approach for a visual parallel tracking and mapping (PTAM) system that excels in challenging environments. Our proposed method combines the strengths of heterogeneous multi-modal visual sensors, including stereo event-based and frame-based sensors, in a unified reference frame through a novel spatio-temporal synchronization approach. We employ deep learning-based feature extraction and description for estimation to enhance robustness further. We also introduce an end-to-end parallel tracking and mapping optimization layer complemented by a simple loop-closure algorithm for efficient SLAM behavior. Through comprehensive experiments on both small-scale and large-scale real-world sequences of VECtor and TUM-VIE benchmarks, our proposed method (DH-PTAM) demonstrates superior performance in terms of robustness and accuracy in adverse conditions, especially in large-scale HDR scenarios.",
            "year": 2023,
            "venue": "IEEE Transactions on Intelligent Vehicles",
            "authors": [
              {
                "authorId": "2173749707",
                "name": "Abanob Soliman"
              },
              {
                "authorId": "19212645",
                "name": "Fabien Bonardi"
              },
              {
                "authorId": "2291965664",
                "name": "D'esir'e Sidib'e"
              },
              {
                "authorId": "2415502",
                "name": "S. Bouchafa"
              }
            ]
          }
        },
        {
          "citedcorpusid": 260293142,
          "isinfluential": false,
          "contexts": [
            "Even though the dataset is aimed for robot control learning along with SLAM, its size is small compared to other such datasets recorded in constrained setups [159]."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "RT-2: Vision-Language-Action Models Transfer Web Knowledge to Robotic Control",
            "abstract": "We study how vision-language models trained on Internet-scale data can be incorporated directly into end-to-end robotic control to boost generalization and enable emergent semantic reasoning. Our goal is to enable a single end-to-end trained model to both learn to map robot observations to actions and enjoy the benefits of large-scale pretraining on language and vision-language data from the web. To this end, we propose to co-fine-tune state-of-the-art vision-language models on both robotic trajectory data and Internet-scale vision-language tasks, such as visual question answering. In contrast to other approaches, we propose a simple, general recipe to achieve this goal: in order to fit both natural language responses and robotic actions into the same format, we express the actions as text tokens and incorporate them directly into the training set of the model in the same way as natural language tokens. We refer to such category of models as vision-language-action models (VLA) and instantiate an example of such a model, which we call RT-2. Our extensive evaluation (6k evaluation trials) shows that our approach leads to performant robotic policies and enables RT-2 to obtain a range of emergent capabilities from Internet-scale training. This includes significantly improved generalization to novel objects, the ability to interpret commands not present in the robot training data (such as placing an object onto a particular number or icon), and the ability to perform rudimentary reasoning in response to user commands (such as picking up the smallest or largest object, or the one closest to another object). We further show that incorporating chain of thought reasoning allows RT-2 to perform multi-stage semantic reasoning, for example figuring out which object to pick up for use as an improvised hammer (a rock), or which type of drink is best suited for someone who is tired (an energy drink).",
            "year": 2023,
            "venue": "Conference on Robot Learning",
            "authors": [
              {
                "authorId": "118025075",
                "name": "Anthony Brohan"
              },
              {
                "authorId": "2161343011",
                "name": "Noah Brown"
              },
              {
                "authorId": "2196517336",
                "name": "Justice Carbajal"
              },
              {
                "authorId": "2527420",
                "name": "Yevgen Chebotar"
              },
              {
                "authorId": "1805203",
                "name": "K. Choromanski"
              },
              {
                "authorId": "95691186",
                "name": "Tianli Ding"
              },
              {
                "authorId": "2283848260",
                "name": "Danny Driess"
              },
              {
                "authorId": "89890133",
                "name": "Kumar Avinava Dubey"
              },
              {
                "authorId": "46881670",
                "name": "Chelsea Finn"
              },
              {
                "authorId": "47686265",
                "name": "Peter R. Florence"
              },
              {
                "authorId": "3430433",
                "name": "Chuyuan Fu"
              },
              {
                "authorId": "153134021",
                "name": "Montse Gonzalez Arenas"
              },
              {
                "authorId": "2161342233",
                "name": "K. Gopalakrishnan"
              },
              {
                "authorId": "2273880591",
                "name": "Kehang Han"
              },
              {
                "authorId": "1944801",
                "name": "Karol Hausman"
              },
              {
                "authorId": "1505793452",
                "name": "Alexander Herzog"
              },
              {
                "authorId": "2726592",
                "name": "Jasmine Hsu"
              },
              {
                "authorId": "2704814",
                "name": "Brian Ichter"
              },
              {
                "authorId": "17818078",
                "name": "A. Irpan"
              },
              {
                "authorId": "2052368480",
                "name": "Nikhil J. Joshi"
              },
              {
                "authorId": "144885996",
                "name": "Ryan C. Julian"
              },
              {
                "authorId": "48313860",
                "name": "Dmitry Kalashnikov"
              },
              {
                "authorId": "2161342687",
                "name": "Yuheng Kuang"
              },
              {
                "authorId": "2057988112",
                "name": "Isabel Leal"
              },
              {
                "authorId": "1736651",
                "name": "S. Levine"
              },
              {
                "authorId": "47407464",
                "name": "H. Michalewski"
              },
              {
                "authorId": "2080746",
                "name": "Igor Mordatch"
              },
              {
                "authorId": "31719101",
                "name": "Karl Pertsch"
              },
              {
                "authorId": "2251957",
                "name": "Kanishka Rao"
              },
              {
                "authorId": "2163522073",
                "name": "Krista Reymann"
              },
              {
                "authorId": "1766489",
                "name": "M. Ryoo"
              },
              {
                "authorId": "2196524735",
                "name": "Grecia Salazar"
              },
              {
                "authorId": "2840758",
                "name": "Pannag R. Sanketi"
              },
              {
                "authorId": "3142556",
                "name": "P. Sermanet"
              },
              {
                "authorId": "2196040785",
                "name": "Jaspiar Singh"
              },
              {
                "authorId": "2111007256",
                "name": "Anikait Singh"
              },
              {
                "authorId": "1737285",
                "name": "Radu Soricut"
              },
              {
                "authorId": "2195355151",
                "name": "Huong Tran"
              },
              {
                "authorId": "2657155",
                "name": "Vincent Vanhoucke"
              },
              {
                "authorId": "144579461",
                "name": "Q. Vuong"
              },
              {
                "authorId": "88728227",
                "name": "Ayzaan Wahid"
              },
              {
                "authorId": "69426588",
                "name": "Stefan Welker"
              },
              {
                "authorId": "3202367",
                "name": "Paul Wohlhart"
              },
              {
                "authorId": "9961095",
                "name": "Ted Xiao"
              },
              {
                "authorId": "10909315",
                "name": "Tianhe Yu"
              },
              {
                "authorId": "2196524598",
                "name": "Brianna Zitkovich"
              }
            ]
          }
        },
        {
          "citedcorpusid": 262638843,
          "isinfluential": true,
          "contexts": [
            "Events (on frames) DDES [51] DTC-SPADE [31] StereoFlow-Net[11] GT Events DTC-SPADE [31] Conc-Net[30] StereoFlow-Net[11] DTC-SPADE Se-CFF Ours Events Image Fig.",
            ") [51] was the first event-only stereo approach using deep learning.",
            "The Event-Image-Translation-Network ( EIT-Net ) [40] improves upon DDES [51] by reconstructing images from events and using them as a guiding signal for stereo matching.",
            "Many of the methods do not report results on split 2, citing poor generalization because of the difference in dynamic characteristics in training and testing events on that split, as mentioned in [40, 51].",
            "The output is then fed to the spatial embedding module, followed by the matching and regularization modules as in DDES [51].",
            "In contrast to SHEF [39], which uses simple model-based stereo matching on edge maps from both sensors, HDES aims to learn stereo depth by employing spatio-temporal input representations (same as [51]) and a novel hybrid pyramid attention module.",
            "4) is based on the protocol proposed in [51].",
            "Comparing the outputs of DDES [51] (2019) to StereoFlow-Net [11] (2024) we observe a notable improvement in depth accuracy and sharpness of object boundaries through the years.",
            "Mean depth error values are provided individually for flying1 , flying2 and flying3 sequences, as in [51].",
            "Another method for stereo depth in this category is proposed in Stereo Cross-Modality network ( SCM-Net ) [120], where an event-intensity fusion network is used to combine intensity features and event embeddings learned from the method proposed in DDES [51]."
          ],
          "intents": [
            "--",
            "['background']",
            "['methodology']",
            "['methodology']",
            "['methodology']",
            "['methodology']",
            "['background']",
            "['methodology']",
            "['methodology']",
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Learning an Event Sequence Embedding for Dense Event-Based Deep Stereo",
            "abstract": "Today, a frame-based camera is the sensor of choice for machine vision applications. However, these cameras, originally developed for acquisition of static images rather than for sensing of dynamic uncontrolled visual environments, suffer from high power consumption, data rate, latency and low dynamic range. An event-based image sensor addresses these drawbacks by mimicking a biological retina. Instead of measuring the intensity of every pixel in a fixed time-interval, it reports events of significant pixel intensity changes. Every such event is represented by its position, sign of change, and timestamp, accurate to the microsecond. Asynchronous event sequences require special handling, since traditional algorithms work only with synchronous, spatially gridded data. To address this problem we introduce a new module for event sequence embedding, for use in difference applications. The module builds a representation of an event sequence by firstly aggregating information locally across time, using a novel fully-connected layer for an irregularly sampled continuous domain, and then across discrete spatial domain. Based on this module, we design a deep learning-based stereo method for event-based cameras. The proposed method is the first learning-based stereo method for an event-based camera and the only method that produces dense results. We show that large performance increases on the Multi Vehicle Stereo Event Camera Dataset (MVSEC), which became the standard set for benchmarking of event-based stereo methods.",
            "year": 2019,
            "venue": "IEEE International Conference on Computer Vision",
            "authors": [
              {
                "authorId": "1823725",
                "name": "S. Tulyakov"
              },
              {
                "authorId": "2721983",
                "name": "F. Fleuret"
              },
              {
                "authorId": "40519282",
                "name": "Martin Kiefel"
              },
              {
                "authorId": "2871555",
                "name": "Peter Gehler"
              },
              {
                "authorId": "2058954687",
                "name": "Michael Hirsch"
              }
            ]
          }
        },
        {
          "citedcorpusid": 263339606,
          "isinfluential": false,
          "contexts": [
            "The ConvLSTM Event Stereo Network ( CES-Net ) [47] achieved the best results in mean average disparity error (MAE) in the DSEC disparity benchmark among the event-only methods at the CVPR Event Vision Workshop 2023."
          ],
          "intents": [
            "--"
          ],
          "cited_paper_info": {
            "title": "Spatio-temporal Event Feature Extractor for Event-based Stereo",
            "abstract": "This technical report is submitted for the DSEC [2] competition in CVPR 2021 Workshop on Event-based Vision. The goal of the competition is to estimate dense disparity maps from event stream data obtained by event cameras in a stereo manner. The event stream data and the provided ground truth disparity maps are temporally synchronized but not spatially aligned. We use the rectification map provided in the dataset to rectify and align the event stream data. We would like to further note that the DSEC dataset does provide RGB images for the scene; however, our method is an event-only method that does not need to use these RGB images. In this report, we primarily focus on the event embedding method and design a ConvLSTM [3]-based event feature extractor. The ConvLSTM module extracts not only spatial but also temporal information while processing the stacked event stream. We experimentally show that the designed event feature extractor can provide more valuable features to the stereo matching model than either the voxelbased [6] or queue-based method [4]. Furthermore, we employ a PSMNet as the backbone stereo matching model to improve performance. Ultimately, with only event stream data, we achieve 0.59 MAE in our train/validation split.",
            "year": 2021,
            "venue": "",
            "authors": [
              {
                "authorId": "101274182",
                "name": "H. Kweon"
              },
              {
                "authorId": "72286913",
                "name": "Jaeseok Jeong"
              },
              {
                "authorId": "2249857499",
                "name": "Sung-Hoon Yoon"
              },
              {
                "authorId": "2117242331",
                "name": "Yujeong Chae"
              },
              {
                "authorId": "2150155685",
                "name": "Kuk-Jin Yoon"
              }
            ]
          }
        },
        {
          "citedcorpusid": 265257632,
          "isinfluential": false,
          "contexts": [
            "It is similar to ASNet [17], except that the event stack lengths are decided on a per-pixel basis, giving it more flexibility.",
            "The Adaptive Stacks Depth Estimation Network ( ASNet ) [17] proposes the use of event-histogram stacks as input to an image-based stereo matching network (Mo-bileStereoNet)."
          ],
          "intents": [
            "['methodology']",
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Stereo Depth Estimation Based on Adaptive Stacks from Event Cameras",
            "abstract": "In recent years, the combination of event cameras and computer vision has shown increasingly excellent performance. Due to high sensitivity, event cameras are capable of addressing the issue of motion blur in conventional cameras, and are well-suited for analyzing fast-moving objects, making them highly suitable for depth estimation in UAV applications This paper focuses on methods for depth estimation using events generated by event cameras. Due to the asynchronicity of events, it is difficult to directly transmit events to the depth estimation network. So the method to preprocess events is important. Unlike existing processing methods, this paper creatively proposes the idea of adaptive stacks, which can change the size of weighted stacks in real time according to the events generation rate. In this way, we can solve the problems caused by traditional processing methods, and better utilize the effective information of events. Then, a depth estimation network corresponding to the adaptive stacks is designed to form a complete end-to-end events depth estimation model: Adaptive Stacks Depth Estimation Network (ASNet). Compared with other models, ASNet has demonstrated excellent depth estimation accuracy and has great application prospects.",
            "year": 2023,
            "venue": "Annual Conference of the IEEE Industrial Electronics Society",
            "authors": [
              {
                "authorId": "2267245054",
                "name": "Jianguo Zhu"
              },
              {
                "authorId": "2267343583",
                "name": "Pengfei Wang"
              },
              {
                "authorId": "2268299791",
                "name": "Sunan Huang"
              },
              {
                "authorId": "2268205977",
                "name": "Cheng Xiang"
              },
              {
                "authorId": "2268205873",
                "name": "Rodney Swee Huat Teo"
              }
            ]
          }
        },
        {
          "citedcorpusid": 265479838,
          "isinfluential": false,
          "contexts": [
            "Recently, Ghosh et al. [14] improved upon the SPADE framework by using a novel two-stage coarse-to-fine strategy for stereo matching."
          ],
          "intents": [
            "--"
          ],
          "cited_paper_info": {
            "title": "Two-stage cross-fusion network for stereo event-based depth estimation",
            "abstract": "",
            "year": 2023,
            "venue": "Expert systems with applications",
            "authors": [
              {
                "authorId": "67062511",
                "name": "Diponkar Ghosh"
              },
              {
                "authorId": "48895895",
                "name": "Yong Ju Jung"
              }
            ]
          }
        },
        {
          "citedcorpusid": 267212137,
          "isinfluential": false,
          "contexts": [
            "NSAVP [143] is a stereo event dataset that focuses on autonomous driving."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Dataset and Benchmark: Novel Sensors for Autonomous Vehicle Perception",
            "abstract": "Conventional cameras employed in autonomous vehicle (AV) systems support many perception tasks but are challenged by low-light or high dynamic range scenes, adverse weather, and fast motion. Novel sensors, such as event and thermal cameras, offer capabilities with the potential to address these scenarios, but they remain to be fully exploited. This paper introduces the Novel Sensors for Autonomous Vehicle Perception (NSAVP) dataset to facilitate future research on this topic. The dataset was captured with a platform including stereo event, thermal, monochrome, and RGB cameras as well as a high precision navigation system providing ground truth poses. The data was collected by repeatedly driving two ∼8 km routes and includes varied lighting conditions and opposing viewpoint perspectives. We provide benchmarking experiments on the task of place recognition to demonstrate challenges and opportunities for novel sensors to enhance critical AV perception tasks. To our knowledge, the NSAVP dataset is the first to include stereo thermal cameras together with stereo event and monochrome cameras. The dataset and supporting software suite is available at https://umautobots.github.io/nsavp.",
            "year": 2024,
            "venue": "Int. J. Robotics Res.",
            "authors": [
              {
                "authorId": "2281035023",
                "name": "Spencer Carmichael"
              },
              {
                "authorId": "2281036390",
                "name": "Austin Buchan"
              },
              {
                "authorId": "32890129",
                "name": "M. Ramanagopal"
              },
              {
                "authorId": "2281036489",
                "name": "Radhika Ravi"
              },
              {
                "authorId": "2238949806",
                "name": "Ram Vasudevan"
              },
              {
                "authorId": "32313805",
                "name": "K. Skinner"
              }
            ]
          }
        },
        {
          "citedcorpusid": 269137093,
          "isinfluential": false,
          "contexts": [
            "FusionPortablev2 [144] is a multi-sensor dataset for generalized SLAM across diverse environments and platforms."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "FusionPortableV2: A unified multi-sensor dataset for generalized SLAM across diverse platforms and scalable environments",
            "abstract": "Simultaneous Localization and Mapping (SLAM) has been widely applied in various robotic missions, from rescue operations to autonomous driving. However, the generalization of SLAM algorithms remains a significant challenge, as current datasets often lack scalability in terms of platforms and environments. To address this limitation, we present FusionPortableV2, a multi-sensor SLAM dataset featuring sensor diversity, varied motion patterns, and a wide range of environmental scenarios. Our dataset comprises 27 sequences, spanning over 2.5 hours and collected from four distinct platforms: a handheld suite, a legged robot, an unmanned ground vehicle (UGV), and a vehicle. These sequences cover diverse settings, including buildings, campuses, and urban areas, with a total length of 38.7 km. Additionally, the dataset includes ground truth (GT) trajectories and RGB point cloud maps covering approximately 0.3 km2. To validate the utility of our dataset in advancing SLAM research, we assess several state-of-the-art (SOTA) SLAM algorithms. Furthermore, we demonstrate the dataset’s broad application beyond traditional SLAM tasks by investigating its potential for monocular depth estimation. The complete dataset, including sensor data, GT, and calibration details, is accessible at https://fusionportable.github.io/dataset/fusionportable_v2.",
            "year": 2024,
            "venue": "Int. J. Robotics Res.",
            "authors": [
              {
                "authorId": "2115434835",
                "name": "Hexiang Wei"
              },
              {
                "authorId": "26944262",
                "name": "Jianhao Jiao"
              },
              {
                "authorId": "2148943939",
                "name": "Xiangcheng Hu"
              },
              {
                "authorId": "2180798953",
                "name": "Jingwen Yu"
              },
              {
                "authorId": "1636890268",
                "name": "Xupeng Xie"
              },
              {
                "authorId": "2110492616",
                "name": "Jin Wu"
              },
              {
                "authorId": "2296675649",
                "name": "Yilong Zhu"
              },
              {
                "authorId": "2254019927",
                "name": "Yuxuan Liu"
              },
              {
                "authorId": "2240390025",
                "name": "Lujia Wang"
              },
              {
                "authorId": "2369275207",
                "name": "Ming Liu"
              }
            ]
          }
        },
        {
          "citedcorpusid": 269614135,
          "isinfluential": false,
          "contexts": [
            "The latest work [15] in this category is a follow-up from the first author of the ESVO paper."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "IMU-Aided Event-based Stereo Visual Odometry",
            "abstract": "Direct methods for event-based visual odometry solve the mapping and camera pose tracking sub-problems by establishing implicit data association in a way that the generative model of events is exploited. The main bottlenecks faced by state-of-the-art work in this field include the high computational complexity of mapping and the limited accuracy of tracking. In this paper, we improve our previous direct pipeline Event-based Stereo Visual Odometry in terms of accuracy and efficiency. To speed up the mapping operation, we propose an efficient strategy of edge-pixel sampling according to the local dynamics of events. The mapping performance in terms of completeness and local smoothness is also improved by combining the temporal stereo results and the static stereo results. To circumvent the degeneracy issue of camera pose tracking in recovering the yaw component of general 6-DoF motion, we introduce as a prior the gyroscope measurements via pre-integration. Experiments on publicly available datasets justify our improvement. We release our pipeline as an open-source software for future research in this field.",
            "year": 2024,
            "venue": "IEEE International Conference on Robotics and Automation",
            "authors": [
              {
                "authorId": "2300175148",
                "name": "Junkai Niu"
              },
              {
                "authorId": "2300175187",
                "name": "Sheng Zhong"
              },
              {
                "authorId": "2268849289",
                "name": "Yi Zhou"
              }
            ]
          }
        },
        {
          "citedcorpusid": 270068050,
          "isinfluential": false,
          "contexts": [
            "A motion-decoupled event camera [133] that always produces sharp, high frame-rate, HDR edge maps may be a better fit for such architectures."
          ],
          "intents": [
            "--"
          ],
          "cited_paper_info": {
            "title": "Microsaccade-inspired event camera for robotics",
            "abstract": "Neuromorphic vision sensors or event cameras have made the visual perception of extremely low reaction time possible, opening new avenues for high-dynamic robotics applications. These event cameras’ output is dependent on both motion and texture. However, the event camera fails to capture object edges that are parallel to the camera motion. This is a problem intrinsic to the sensor and therefore challenging to solve algorithmically. Human vision deals with perceptual fading using the active mechanism of small involuntary eye movements, the most prominent ones called microsaccades. By moving the eyes constantly and slightly during fixation, microsaccades can substantially maintain texture stability and persistence. Inspired by microsaccades, we designed an event-based perception system capable of simultaneously maintaining low reaction time and stable texture. In this design, a rotating wedge prism was mounted in front of the aperture of an event camera to redirect light and trigger events. The geometrical optics of the rotating wedge prism allows for algorithmic compensation of the additional rotational motion, resulting in a stable texture appearance and high informational output independent of external motion. The hardware device and software solution are integrated into a system, which we call artificial microsaccade–enhanced event camera (AMI-EV). Benchmark comparisons validated the superior data quality of AMI-EV recordings in scenarios where both standard cameras and event cameras fail to deliver. Various real-world experiments demonstrated the potential of the system to facilitate robotics perception both for low-level and high-level vision tasks. An artificial microsaccade–enhanced event camera for varied vision tasks in challenging scenarios is proposed. Editor’s summary Event cameras are useful for sensing dynamic objects, but they are not optimized for maintaining stable and persistent texture in vision. Inspired by microsaccades, which are tiny involuntary eye movements generated during prolonged visual fixation to resolve objects, He et al. developed an enhanced event camera to address these challenges. The event camera contained a rotating wedge prism mounted in front of the aperture of the event camera to redirect light and stabilize texture. They demonstrated the ability of the enhanced event camera to acquire more information about the environment and estimate high-speed motion when compared with standard event cameras, with potential to be adopted for robot vision. —Amos Matsiko",
            "year": 2024,
            "venue": "Science Robotics",
            "authors": [
              {
                "authorId": "2296438839",
                "name": "Botao He"
              },
              {
                "authorId": "2303590299",
                "name": "Ze Wang"
              },
              {
                "authorId": "2293552465",
                "name": "Yuan Zhou"
              },
              {
                "authorId": "2688691",
                "name": "Jingxi Chen"
              },
              {
                "authorId": "35885710",
                "name": "Chahat Deep Singh"
              },
              {
                "authorId": "153175521",
                "name": "Haojia Li"
              },
              {
                "authorId": "1726030043",
                "name": "Yuman Gao"
              },
              {
                "authorId": "2269120949",
                "name": "Shaojie Shen"
              },
              {
                "authorId": "2209881109",
                "name": "Kaiwei Wang"
              },
              {
                "authorId": "48696300",
                "name": "Yanjun Cao"
              },
              {
                "authorId": "2260598951",
                "name": "Chao Xu"
              },
              {
                "authorId": "1697493",
                "name": "Y. Aloimonos"
              },
              {
                "authorId": "2156937708",
                "name": "Fei Gao"
              },
              {
                "authorId": "1759899",
                "name": "C. Fermüller"
              }
            ]
          }
        },
        {
          "citedcorpusid": 271892156,
          "isinfluential": false,
          "contexts": [
            "An alternative solution is using a beam splitter setup with separate frame-based and event-based cameras, like in [127, 145, 148, 149].",
            "CoSEC [145] uses a pair of beam splitters to perfectly align pixels between frame-based and event cameras (1-Mpx EVK4s) for multi-modal fusion."
          ],
          "intents": [
            "['methodology']",
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "CoSEC: A Coaxial Stereo Event Camera Dataset for Autonomous Driving",
            "abstract": "Conventional frame camera is the mainstream sensor of the autonomous driving scene perception, while it is limited in adverse conditions, such as low light. Event camera with high dynamic range has been applied in assisting frame camera for the multimodal fusion, which relies heavily on the pixel-level spatial alignment between various modalities. Typically, existing multimodal datasets mainly place event and frame cameras in parallel and directly align them spatially via warping operation. However, this parallel strategy is less effective for multimodal fusion, since the large disparity exacerbates spatial misalignment due to the large event-frame baseline. We argue that baseline minimization can reduce alignment error between event and frame cameras. In this work, we introduce hybrid coaxial event-frame devices to build the multimodal system, and propose a coaxial stereo event camera (CoSEC) dataset for autonomous driving. As for the multimodal system, we first utilize the microcontroller to achieve time synchronization, and then spatially calibrate different sensors, where we perform intra- and inter-calibration of stereo coaxial devices. As for the multimodal dataset, we filter LiDAR point clouds to generate depth and optical flow labels using reference depth, which is further improved by fusing aligned event and frame data in nighttime conditions. With the help of the coaxial device, the proposed dataset can promote the all-day pixel-level multimodal fusion. Moreover, we also conduct experiments to demonstrate that the proposed dataset can improve the performance and generalization of the multimodal fusion.",
            "year": 2024,
            "venue": "arXiv.org",
            "authors": [
              {
                "authorId": "2291076457",
                "name": "Shihan Peng"
              },
              {
                "authorId": "2111826988",
                "name": "Hanyu Zhou"
              },
              {
                "authorId": "2290967427",
                "name": "Hao Dong"
              },
              {
                "authorId": "2282279657",
                "name": "Zhiwei Shi"
              },
              {
                "authorId": "2282191166",
                "name": "Haoyue Liu"
              },
              {
                "authorId": "2282418648",
                "name": "Yuxing Duan"
              },
              {
                "authorId": "2131636344",
                "name": "Yi Chang"
              },
              {
                "authorId": "2282097801",
                "name": "Luxin Yan"
              }
            ]
          }
        },
        {
          "citedcorpusid": 274611240,
          "isinfluential": false,
          "contexts": [
            "However, it does not contain GT depth, so only qualitative evaluation of 3D reconstruction algorithms is possible."
          ],
          "intents": [
            "--"
          ],
          "cited_paper_info": {
            "title": "Edge-Guided Fusion and Motion Augmentation for Event-Image Stereo",
            "abstract": "",
            "year": 2024,
            "venue": "European Conference on Computer Vision",
            "authors": [
              {
                "authorId": "2335837652",
                "name": "Fengan Zhao"
              },
              {
                "authorId": "2211987004",
                "name": "Qianang Zhou"
              },
              {
                "authorId": "2334703157",
                "name": "Junlin Xiong"
              }
            ]
          }
        },
        {
          "citedcorpusid": 276652376,
          "isinfluential": false,
          "contexts": [
            "However, it does not contain GT depth, so only qualitative evaluation of 3D reconstruction algorithms is possible."
          ],
          "intents": [
            "--"
          ],
          "cited_paper_info": {
            "title": "NER-Net+: Seeing Motion at Nighttime With an Event Camera",
            "abstract": "We focus on a very challenging task: imaging at nighttime dynamic scenes. Conventional RGB cameras struggle with the trade-off between long exposure for low-light imaging and short exposure for capturing dynamic scenes. Event cameras react to dynamic changes, with their high temporal resolution (microsecond) and dynamic range (120 dB), and thus offer a promising alternative. However, existing methods are mostly based on simulated datasets due to the lack of paired event-clean image data for nighttime conditions, where the domain gap leads to performance limitations in real-world scenarios. Moreover, most existing event reconstruction methods are tailored for daytime data, overlooking issues unique to low-light events at night, such as strong noise, temporal trailing, and spatial non-uniformity, resulting in unsatisfactory reconstruction results. To address these challenges, we construct the first real paired low-light event dataset (RLED) through a co-axial imaging system, comprising 80,400 spatially and temporally aligned image GTs and low-light events, which provides a unified training and evaluation dataset for existing methods. We further conduct a comprehensive analysis of the causes and characteristics of strong noise, temporal trailing, and spatial non-uniformity in nighttime events, and propose a nighttime event reconstruction network (NER-Net+). It includes a learnable event timestamps calibration module (LETC) to correct the temporal trailing events and a non-stationary spatio-temporal information enhancement module (NSIE) to suppress sensor noise and spatial non-uniformity. Extensive experiments demonstrate that the proposed method outperforms state-of-the-art methods in visual quality and generalization on real-world nighttime datasets.",
            "year": 2025,
            "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
            "authors": [
              {
                "authorId": "2282191166",
                "name": "Haoyue Liu"
              },
              {
                "authorId": "2347771707",
                "name": "Jinghan Xu"
              },
              {
                "authorId": "2291076457",
                "name": "Shihan Peng"
              },
              {
                "authorId": "2131636344",
                "name": "Yi Chang"
              },
              {
                "authorId": "2111826988",
                "name": "Hanyu Zhou"
              },
              {
                "authorId": "2282418648",
                "name": "Yuxing Duan"
              },
              {
                "authorId": "2297235676",
                "name": "Lin Zhu"
              },
              {
                "authorId": "2275270705",
                "name": "Yonghong Tian"
              },
              {
                "authorId": "2282097801",
                "name": "Luxin Yan"
              }
            ]
          }
        },
        {
          "citedcorpusid": null,
          "isinfluential": false,
          "contexts": [
            "Since the seminal work [2] (2008) they have gained increasing interest due to their appealing properties, which allow them to perform well in challenging scenarios for traditional cameras, such as high-speed motion, high dynamic range (HDR) illumination, and low-power consumption."
          ],
          "intents": [
            "--"
          ],
          "cited_paper_info": {}
        },
        {
          "citedcorpusid": null,
          "isinfluential": false,
          "contexts": [
            "Over the years, several event camera designs have been investigated inspired by biological retinas.",
            "Steffen et al [3] review various bio-inspired aspects of event cameras and the stereo algorithms that may benefit from their novel data."
          ],
          "intents": [
            "--",
            "--"
          ],
          "cited_paper_info": {}
        },
        {
          "citedcorpusid": null,
          "isinfluential": false,
          "contexts": [
            "Graph created from a compiled list of papers, which is accessible in this spreadsheet."
          ],
          "intents": [
            "--"
          ],
          "cited_paper_info": {}
        },
        {
          "citedcorpusid": null,
          "isinfluential": false,
          "contexts": [
            "While NN methods like E2VID provide best reconstruction quality, less accurate but faster reconstruction techniques like simple image recon [160] are equality good at detecting high-contrast corners."
          ],
          "intents": [
            "--"
          ],
          "cited_paper_info": {}
        },
        {
          "citedcorpusid": null,
          "isinfluential": false,
          "contexts": [
            "Most of the methods in this category are still model-based."
          ],
          "intents": [
            "--"
          ],
          "cited_paper_info": {}
        },
        {
          "citedcorpusid": null,
          "isinfluential": false,
          "contexts": [
            "Most of the methods in this category are still model-based."
          ],
          "intents": [
            "--"
          ],
          "cited_paper_info": {}
        }
      ]
    },
    "278355101": {
      "citing_paper_info": {
        "title": "Event-Based Stereo Depth Estimation With Motion Guidance and Left–Right Consistency",
        "abstract": "Depth estimation, which directly captures the structure of observable environmental surfaces, plays a critical role in vision-based applications such as measurement, mapping, autonomous driving, and robot navigation. Specifically, event camera-based stereo depth estimation provides a novel solution to challenging conditions like rapid motion and extreme illumination variations. Currently, deep learning has become the dominant approach for event camera-based stereo depth estimation. However, these methods fail to fully exploit temporal cues in the event stream, resulting in suboptimal event representation clarity. Furthermore, there is room for further reduction in pixel shifts in feature maps before constructing the cost volume. This article presents a novel event-based stereo depth estimation method with motion guidance and left-right consistency. First, an edge-aware aggregation (EAA) module is proposed, which integrates event frames with motion confidence maps to generate a novel high-definition event representation. Then, a motion-guided attention (MGA) module is introduced, which leverages deformable transformer encoders guided by motion confidence maps to refine edge precision in feature maps. Finally, a census left-right consistency loss function is designed to enhance the left-right consistency of stereo event representation. Experiments conducted in challenging real-world driving scenarios demonstrate that the proposed method outperforms state-of-the-art (SOTA) methods in terms of mean absolute error (MAE) and root mean square error (RMSE) metrics.",
        "year": 2025,
        "venue": "IEEE Transactions on Instrumentation and Measurement",
        "authors": [
          {
            "authorId": "2107975649",
            "name": "Junjie Jiang"
          },
          {
            "authorId": "2229193295",
            "name": "Zhuang Hao"
          },
          {
            "authorId": "2152665453",
            "name": "Xinjie Huang"
          },
          {
            "authorId": "2052315650",
            "name": "Delei Kong"
          },
          {
            "authorId": "2165663624",
            "name": "Zheng Fang"
          }
        ]
      },
      "citation_summary": {
        "citation_count": 22,
        "unique_cited_count": 21,
        "influential_count": 1,
        "detailed_records_count": 22
      },
      "cited_papers": [
        "249980412",
        "226293853",
        "271736313",
        "234788196",
        "195859047",
        "246026914",
        "247143537",
        "13756489",
        "247213634",
        "220314130",
        "253121300",
        "216036364",
        "4252896",
        "267740607",
        "206770307",
        "118684904",
        "102352684",
        "247109597",
        "50773155",
        "248304816",
        "22889967"
      ],
      "citation_details": [
        {
          "citedcorpusid": 4252896,
          "isinfluential": false,
          "contexts": [
            "For deep learning-based approaches [37], [38], [39], the left and right images are used as feature maps in the network to construct the cost volume."
          ],
          "intents": [
            "--"
          ],
          "cited_paper_info": {
            "title": "Pyramid Stereo Matching Network",
            "abstract": "Recent work has shown that depth estimation from a stereo pair of images can be formulated as a supervised learning task to be resolved with convolutional neural networks (CNNs). However, current architectures rely on patch-based Siamese networks, lacking the means to exploit context information for finding correspondence in ill-posed regions. To tackle this problem, we propose PSMNet, a pyramid stereo matching network consisting of two main modules: spatial pyramid pooling and 3D CNN. The spatial pyramid pooling module takes advantage of the capacity of global context information by aggregating context in different scales and locations to form a cost volume. The 3D CNN learns to regularize cost volume using stacked multiple hourglass networks in conjunction with intermediate supervision. The proposed approach was evaluated on several benchmark datasets. Our method ranked first in the KITTI 2012 and 2015 leaderboards before March 18, 2018. The codes of PSMNet are available at: https://github.com/JiaRenChang/PSMNet.",
            "year": 2018,
            "venue": "2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition",
            "authors": [
              {
                "authorId": "2936466",
                "name": "Jia-Ren Chang"
              },
              {
                "authorId": "2143438143",
                "name": "Yonghao Chen"
              }
            ]
          }
        },
        {
          "citedcorpusid": 13756489,
          "isinfluential": false,
          "contexts": [
            "Unlike global attention in [41], deformable attention only samples K points around the reference point to calculate the attention result."
          ],
          "intents": [
            "--"
          ],
          "cited_paper_info": {
            "title": "Attention is All you Need",
            "abstract": "The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.",
            "year": 2017,
            "venue": "Neural Information Processing Systems",
            "authors": [
              {
                "authorId": "40348417",
                "name": "Ashish Vaswani"
              },
              {
                "authorId": "1846258",
                "name": "Noam M. Shazeer"
              },
              {
                "authorId": "3877127",
                "name": "Niki Parmar"
              },
              {
                "authorId": "39328010",
                "name": "Jakob Uszkoreit"
              },
              {
                "authorId": "145024664",
                "name": "Llion Jones"
              },
              {
                "authorId": "19177000",
                "name": "Aidan N. Gomez"
              },
              {
                "authorId": "40527594",
                "name": "Lukasz Kaiser"
              },
              {
                "authorId": "3443442",
                "name": "I. Polosukhin"
              }
            ]
          }
        },
        {
          "citedcorpusid": 22889967,
          "isinfluential": false,
          "contexts": [
            "The event camera simulates the functionality of the transient visual pathway of the biological retina, where each pixel independently and asynchronously responds to relative brightness changes with a time resolution at the microsecond-level [30], [31]."
          ],
          "intents": [
            "--"
          ],
          "cited_paper_info": {
            "title": "Integration of dynamic vision sensor with inertial measurement unit for electronically stabilized event-based vision",
            "abstract": "",
            "year": 2014,
            "venue": "International Symposium on Circuits and Systems",
            "authors": [
              {
                "authorId": "5548576",
                "name": "T. Delbruck"
              },
              {
                "authorId": "144670874",
                "name": "Vicente Villanueva"
              },
              {
                "authorId": "48762210",
                "name": "Luca Longinotti"
              }
            ]
          }
        },
        {
          "citedcorpusid": 50773155,
          "isinfluential": false,
          "contexts": [
            "For deep learning-based approaches [37], [38], [39], the left and right images are used as feature maps in the network to construct the cost volume."
          ],
          "intents": [
            "--"
          ],
          "cited_paper_info": {
            "title": "StereoNet: Guided Hierarchical Refinement for Real-Time Edge-Aware Depth Prediction",
            "abstract": "This paper presents StereoNet, the first end-to-end deep architecture for real-time stereo matching that runs at 60fps on an NVidia Titan X, producing high-quality, edge-preserved, quantization-free disparity maps. A key insight of this paper is that the network achieves a sub-pixel matching precision than is a magnitude higher than those of traditional stereo matching approaches. This allows us to achieve real-time performance by using a very low resolution cost volume that encodes all the information needed to achieve high disparity precision. Spatial precision is achieved by employing a learned edge-aware upsampling function. Our model uses a Siamese network to extract features from the left and right image. A first estimate of the disparity is computed in a very low resolution cost volume, then hierarchically the model re-introduces high-frequency details through a learned upsampling function that uses compact pixel-to-pixel refinement networks. Leveraging color input as a guide, this function is capable of producing high-quality edge-aware output. We achieve compelling results on multiple benchmarks, showing how the proposed method offers extreme flexibility at an acceptable computational budget.",
            "year": 2018,
            "venue": "European Conference on Computer Vision",
            "authors": [
              {
                "authorId": "2121982",
                "name": "S. Khamis"
              },
              {
                "authorId": "34772804",
                "name": "S. Fanello"
              },
              {
                "authorId": "2086328",
                "name": "Christoph Rhemann"
              },
              {
                "authorId": "2371390",
                "name": "Adarsh Kowdle"
              },
              {
                "authorId": "39596866",
                "name": "Julien P. C. Valentin"
              },
              {
                "authorId": "79406746",
                "name": "S. Izadi"
              }
            ]
          }
        },
        {
          "citedcorpusid": 102352684,
          "isinfluential": false,
          "contexts": [
            "Finally, a refinement module [29] is utilized to recover the multiscale predicted disparity maps scale.",
            "Finally, the winner-take-all (WTA) [28] and the refinement module [29] are employed to estimate the disparity."
          ],
          "intents": [
            "--",
            "--"
          ],
          "cited_paper_info": {
            "title": "StereoDRNet: Dilated Residual StereoNet",
            "abstract": "We propose a system that uses a convolution neural network (CNN) to estimate depth from a stereo pair followed by volumetric fusion of the predicted depth maps to produce a 3D reconstruction of a scene. Our proposed depth refinement architecture, predicts view-consistent disparity and occlusion maps that helps the fusion system to produce geometrically consistent reconstructions. We utilize 3D dilated convolutions in our proposed cost filtering network that yields better filtering while almost halving the computational cost in comparison to state of the art cost filtering architectures. For feature extraction we use the Vortex Pooling architecture. The proposed method achieves state of the art results in KITTI 2012, KITTI 2015 and ETH 3D stereo benchmarks. Finally, we demonstrate that our system is able to produce high fidelity 3D scene reconstructions that outperforms the state of the art stereo system.",
            "year": 2019,
            "venue": "Computer Vision and Pattern Recognition",
            "authors": [
              {
                "authorId": "3428200",
                "name": "Rohan Chabra"
              },
              {
                "authorId": "20128275",
                "name": "Julian Straub"
              },
              {
                "authorId": "40517410",
                "name": "Chris Sweeney"
              },
              {
                "authorId": "50366818",
                "name": "Richard A. Newcombe"
              },
              {
                "authorId": "145472944",
                "name": "H. Fuchs"
              }
            ]
          }
        },
        {
          "citedcorpusid": 118684904,
          "isinfluential": false,
          "contexts": [
            "Event cameras [7], [8], [9], [10], [11], [12] asynchronously capture submicrosecond brightness changes at each pixel, outputting pixel coordinates, timestamps, and polarities of these brightness changes."
          ],
          "intents": [
            "--"
          ],
          "cited_paper_info": {
            "title": "Event-Based Vision: A Survey",
            "abstract": "Event cameras are bio-inspired sensors that differ from conventional frame cameras: Instead of capturing images at a fixed rate, they asynchronously measure per-pixel brightness changes, and output a stream of events that encode the time, location and sign of the brightness changes. Event cameras offer attractive properties compared to traditional cameras: high temporal resolution (in the order of $\\mu$μs), very high dynamic range (140 dB versus 60 dB), low power consumption, and high pixel bandwidth (on the order of kHz) resulting in reduced motion blur. Hence, event cameras have a large potential for robotics and computer vision in challenging scenarios for traditional cameras, such as low-latency, high speed, and high dynamic range. However, novel methods are required to process the unconventional output of these sensors in order to unlock their potential. This paper provides a comprehensive overview of the emerging field of event-based vision, with a focus on the applications and the algorithms developed to unlock the outstanding properties of event cameras. We present event cameras from their working principle, the actual sensors that are available and the tasks that they have been used for, from low-level vision (feature detection and tracking, optic flow, etc.) to high-level vision (reconstruction, segmentation, recognition). We also discuss the techniques developed to process events, including learning-based techniques, as well as specialized processors for these novel sensors, such as spiking neural networks. Additionally, we highlight the challenges that remain to be tackled and the opportunities that lie ahead in the search for a more efficient, bio-inspired way for machines to perceive and interact with the world.",
            "year": 2019,
            "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
            "authors": [
              {
                "authorId": "144036711",
                "name": "Guillermo Gallego"
              },
              {
                "authorId": "1694635",
                "name": "T. Delbrück"
              },
              {
                "authorId": "33780923",
                "name": "G. Orchard"
              },
              {
                "authorId": "1897771",
                "name": "C. Bartolozzi"
              },
              {
                "authorId": "1736425",
                "name": "B. Taba"
              },
              {
                "authorId": "1860631",
                "name": "A. Censi"
              },
              {
                "authorId": "2864731",
                "name": "Stefan Leutenegger"
              },
              {
                "authorId": "2052135690",
                "name": "A. Davison"
              },
              {
                "authorId": "3302681",
                "name": "J. Conradt"
              },
              {
                "authorId": "1751586",
                "name": "Kostas Daniilidis"
              },
              {
                "authorId": "2075371",
                "name": "D. Scaramuzza"
              }
            ]
          }
        },
        {
          "citedcorpusid": 195859047,
          "isinfluential": false,
          "contexts": [
            "Finally, the winner-take-all (WTA) [28] and the refinement module [29] are employed to estimate the disparity."
          ],
          "intents": [
            "--"
          ],
          "cited_paper_info": {
            "title": "A Taxonomy and Evaluation of Dense Two-Frame Stereo Correspondence Algorithms",
            "abstract": "",
            "year": 2001,
            "venue": "Proceedings IEEE Workshop on Stereo and Multi-Baseline Vision (SMBV 2001)",
            "authors": [
              {
                "authorId": "1709053",
                "name": "D. Scharstein"
              },
              {
                "authorId": "1717841",
                "name": "R. Szeliski"
              }
            ]
          }
        },
        {
          "citedcorpusid": 206770307,
          "isinfluential": false,
          "contexts": [
            "2) Total Loss Function: Both the smooth L 1 loss function [43] and the left–right consistency census loss are used to construct the total loss function."
          ],
          "intents": [
            "--"
          ],
          "cited_paper_info": {
            "title": "Fast R-CNN",
            "abstract": "This paper proposes Fast R-CNN, a clean and fast framework for object detection. Compared to traditional R-CNN, and its accelerated version SPPnet, Fast R-CNN trains networks using a multi-task loss in a single training stage. The multi-task loss simplifies learning and improves detection accuracy. Unlike SPPnet, all network layers can be updated during fine-tuning. We show that this difference has practical ramifications for very deep networks, such as VGG16, where mAP suffers when only the fully-connected layers are updated. Compared to\"slow\"R-CNN, Fast R-CNN is 9x faster at training VGG16 for detection, 213x faster at test-time, and achieves a significantly higher mAP on PASCAL VOC 2012. Compared to SPPnet, Fast R-CNN trains VGG16 3x faster, tests 10x faster, and is more accurate. Fast R-CNN is implemented in Python and C++ and is available under the open-source MIT License at https://github.com/rbgirshick/fast-rcnn",
            "year": 2015,
            "venue": "",
            "authors": [
              {
                "authorId": "2983898",
                "name": "Ross B. Girshick"
              }
            ]
          }
        },
        {
          "citedcorpusid": 216036364,
          "isinfluential": false,
          "contexts": [
            "Next, a cost volume pyramid is constructed using the similarity (correlation) method, and the cost volumes are aggregated through the ISA and CSA [27].",
            "The intrascale aggregation (ISA) module and cross-scale aggregation (CSA) module proposed in [27] are utilized to construct the cost aggregation module."
          ],
          "intents": [
            "--",
            "--"
          ],
          "cited_paper_info": {
            "title": "AANet: Adaptive Aggregation Network for Efficient Stereo Matching",
            "abstract": "Despite the remarkable progress made by learning based stereo matching algorithms, one key challenge remains unsolved. Current state-of-the-art stereo models are mostly based on costly 3D convolutions, the cubic computational complexity and high memory consumption make it quite expensive to deploy in real-world applications. In this paper, we aim at completely replacing the commonly used 3D convolutions to achieve fast inference speed while maintaining comparable accuracy. To this end, we first propose a sparse points based intra-scale cost aggregation method to alleviate the well-known edge-fattening issue at disparity discontinuities. Further, we approximate traditional cross-scale cost aggregation algorithm with neural network layers to handle large textureless regions. Both modules are simple, lightweight, and complementary, leading to an effective and efficient architecture for cost aggregation. With these two modules, we can not only significantly speed up existing top-performing models (e.g., 41x than GC-Net, 4x than PSMNet and 38x than GA-Net), but also improve the performance of fast stereo models (e.g., StereoNet). We also achieve competitive results on Scene Flow and KITTI datasets while running at 62ms, demonstrating the versatility and high efficiency of the proposed method. Our full framework is available at https://github.com/haofeixu/aanet.",
            "year": 2020,
            "venue": "Computer Vision and Pattern Recognition",
            "authors": [
              {
                "authorId": "2108835907",
                "name": "Haofei Xu"
              },
              {
                "authorId": "2108487442",
                "name": "Juyong Zhang"
              }
            ]
          }
        },
        {
          "citedcorpusid": 220314130,
          "isinfluential": false,
          "contexts": [
            "Event cameras [7], [8], [9], [10], [11], [12] asynchronously capture submicrosecond brightness changes at each pixel, outputting pixel coordinates, timestamps, and polarities of these brightness changes."
          ],
          "intents": [
            "--"
          ],
          "cited_paper_info": {
            "title": "Event-Based Neuromorphic Vision for Autonomous Driving: A Paradigm Shift for Bio-Inspired Visual Sensing and Perception",
            "abstract": "As a bio-inspired and emerging sensor, an event-based neuromorphic vision sensor has a different working principle compared to the standard frame-based cameras, which leads to promising properties of low energy consumption, low latency, high dynamic range (HDR), and high temporal resolution. It poses a paradigm shift to sense and perceive the environment by capturing local pixel-level light intensity changes and producing asynchronous event streams. Advanced technologies for the visual sensing system of autonomous vehicles from standard computer vision to event-based neuromorphic vision have been developed. In this tutorial-like article, a comprehensive review of the emerging technology is given. First, the course of the development of the neuromorphic vision sensor that is derived from the understanding of biological retina is introduced. The signal processing techniques for event noise processing and event data representation are then discussed. Next, the signal processing algorithms and applications for event-based neuromorphic vision in autonomous driving and various assistance systems are reviewed. Finally, challenges and future research directions are pointed out. It is expected that this article will serve as a starting point for new researchers and engineers in the autonomous driving field and provide a bird's-eye view to both neuromorphic vision and autonomous driving research communities.",
            "year": 2020,
            "venue": "IEEE Signal Processing Magazine",
            "authors": [
              {
                "authorId": "143930563",
                "name": "Guang Chen"
              },
              {
                "authorId": "40223253",
                "name": "Hu Cao"
              },
              {
                "authorId": "3302681",
                "name": "J. Conradt"
              },
              {
                "authorId": "3134548",
                "name": "Huajin Tang"
              },
              {
                "authorId": "1789856900",
                "name": "Florian Rohrbein"
              },
              {
                "authorId": "2075424317",
                "name": "Alois Knoll"
              }
            ]
          }
        },
        {
          "citedcorpusid": 226293853,
          "isinfluential": false,
          "contexts": [
            "They improve the disparity estimation performance using knowledge distillation [23], [24], [25], [26] by incorporating future event information to reinforce the disparity estimation results based on only past event information."
          ],
          "intents": [
            "--"
          ],
          "cited_paper_info": {
            "title": "Compact StereoNet: Stereo Disparity Estimation via Knowledge Distillation and Compact Feature Extractor",
            "abstract": "Stereo disparity estimation is a difficult and crucial task in computer vision. Although many experimental techniques have been proposed in recent years with the flourishing of deep learning, very few studies take into account the optimization of computational complexity and memory consumption. Most previous works take advantage of stacked 3D convolutional block to generate fine disparity, but with a high computational cost and a large memory consumption. Considering the aforementioned problem, in this paper, we proposed an efficient convolutional neural architecture for stereo disparity estimation. In particular, a compact and efficient multi-scale extractor named MCliqueNet with stacked CliqueBlock was proposed to extract the more refined features for constructing multi-scale cost volume. In order to reduce the computational cost and maintain the accuracy of disparity, we utilized knowledge distillation scheme to transfer contextual features from a teacher network to a student network. Furthermore, we present a novel adaptive $Smooth_{L1}$ (ASL) Loss for calculating the similarity between the contextual features of the teacher network and those of the student network, resulting in a more robust distillation process. Experimental results have shown that our method achieves competitive performance on the challenging Scene Flow and KITTI benchmarks while maintaining a very fast running speed.",
            "year": 2020,
            "venue": "IEEE Access",
            "authors": [
              {
                "authorId": "40214487",
                "name": "Qinquan Gao"
              },
              {
                "authorId": "2008219246",
                "name": "Yuanbo Zhou"
              },
              {
                "authorId": "2108550321",
                "name": "Gen Li"
              },
              {
                "authorId": "144061555",
                "name": "T. Tong"
              }
            ]
          }
        },
        {
          "citedcorpusid": 234788196,
          "isinfluential": false,
          "contexts": [
            "Regarding the uncertainty, the method [49] is followed by applying dropout to the feature extraction component at run-time and measuring the mean µ uncert and variance σ uncert as the uncertainty."
          ],
          "intents": [
            "--"
          ],
          "cited_paper_info": {
            "title": "Machine Learning in Measurement Part 2: Uncertainty Quantification",
            "abstract": "In spite of the advent of Machine Learning (ML) and its successful deployment in measurement systems, little information can be found in the literature about uncertainty quantification in these systems [1]. Uncertainty is crucial for the adoption of ML in commercial products and services. Designers are now being encouraged to be upfront about the uncertainty in their ML systems, because products that specify their uncertainty can have a significant competitive advantage and can unlock new value, reduce risk, and improve usability [2]. In this article, we will describe uncertainty quantification in ML. Because there isn't enough room in one article to explain all ML methods, we concentrate on Deep Learning (DL), which is one of the most popular and effective ML methods in I&M [3]. Please note that this article follows and uses concepts from Part 1 [4], so readers are highly encouraged to first read that part. In addition, we assume the reader has a basic understanding of both DL and uncertainty. Readers for whom this assumption is false are encouraged to first read the brief introduction to DL and its applications in I&M presented in [3] as well as the uncertainty tutorial in [5].",
            "year": 2021,
            "venue": "IEEE Instrumentation & Measurement Magazine",
            "authors": [
              {
                "authorId": "1881977",
                "name": "Hussein Al Osman"
              },
              {
                "authorId": "1748276",
                "name": "S. Shirmohammadi"
              }
            ]
          }
        },
        {
          "citedcorpusid": 246026914,
          "isinfluential": false,
          "contexts": [
            "Event cameras [7], [8], [9], [10], [11], [12] asynchronously capture submicrosecond brightness changes at each pixel, outputting pixel coordinates, timestamps, and polarities of these brightness changes."
          ],
          "intents": [
            "--"
          ],
          "cited_paper_info": {
            "title": "A Novel Visual Object Detection and Distance Estimation Method for HDR Scenes based on Event Camera",
            "abstract": "The event camera is a new type of visual sensor inspired by the biological retina. It can efficiently capture the brightness changes of the scene (called events) in real-time and output a sparse asynchronous event stream with a microsecond resolution. Moreover, it has low latency, low bandwidth, Advantages such as high speed, and High Dynamic Range (HDR). In this article, we propose a novel event camera-based visual target detection and distance estimation method. First, use the DVS event camera to obtain a stable and clear cumulative event image; then, because of the characteristics of the cumulative event image, improve the YOLOv5s network structure to obtain the YOLOv5s (+detection) network so that the objects in the event stream data have better detection accuracy; Next, according to the relationship between the size of different objects in the pixel plane and the real distance, a small MLP network is trained to estimate the distance of the object; finally, in the low-light environment, the use of DVS event camera and RGB camera complete the effect of target detection and distance estimation. Experiments show that our proposed event-based method can achieve high detection and estimation accuracy under different lighting conditions.",
            "year": 2021,
            "venue": "International Conference on Innovative Computing and Cloud Computing",
            "authors": [
              {
                "authorId": "2150448879",
                "name": "Tianhao Wu"
              },
              {
                "authorId": "2151078779",
                "name": "Chengye Gong"
              },
              {
                "authorId": "2052315650",
                "name": "Delei Kong"
              },
              {
                "authorId": "2111043555",
                "name": "Shufang Xu"
              },
              {
                "authorId": "2333334",
                "name": "Qingjie Liu"
              }
            ]
          }
        },
        {
          "citedcorpusid": 247109597,
          "isinfluential": false,
          "contexts": [
            "A S A fundamental and crucial measurement task, depth estimation [2], [3], [4], [5], [6] has attracted significant research attention."
          ],
          "intents": [
            "--"
          ],
          "cited_paper_info": {
            "title": "Dense Depth-Map Estimation Based on Fusion of Event Camera and Sparse LiDAR",
            "abstract": "Depth-map estimation reflects the geometry of the visible surface in the environment directly and plays an important role in perception and decision for intelligent robots. However, sparse LiDAR only provides low-resolution depth information, which is a huge challenge for accurate sensing algorithms. To address this problem, this article proposes a novel fusion framework to generate dense depth-map based on event camera and sparse LiDAR. The approach uses the geometric information provided by the point cloud as prior knowledge and clusters point cloud data by an improved density clustering algorithm. Combined with the 3-D surface model of each cluster, the approach can provide 3-D reconstructions of the coordinate points of events and further obtain the dense-depth map by depth expansion and hole filling. Finally, we deploy our approach in MVSEC datasets and real-world applications. Experimental results show that, compared with other approaches, our approach can obtain more accurate depth information.",
            "year": 2022,
            "venue": "IEEE Transactions on Instrumentation and Measurement",
            "authors": [
              {
                "authorId": "2055098314",
                "name": "Mingyue Cui"
              },
              {
                "authorId": "2109438579",
                "name": "Yuzhang Zhu"
              },
              {
                "authorId": "2197320306",
                "name": "Yechang Liu"
              },
              {
                "authorId": "2117415209",
                "name": "Yun-Meng Liu"
              },
              {
                "authorId": "2146661429",
                "name": "Gang Chen"
              },
              {
                "authorId": "2112769025",
                "name": "Kai Huang"
              }
            ]
          }
        },
        {
          "citedcorpusid": 247143537,
          "isinfluential": false,
          "contexts": [
            "A S A fundamental and crucial measurement task, depth estimation [2], [3], [4], [5], [6] has attracted significant research attention."
          ],
          "intents": [
            "--"
          ],
          "cited_paper_info": {
            "title": "ORB-TEDM: An RGB-D SLAM Approach Fusing ORB Triangulation Estimates and Depth Measurements",
            "abstract": "3-D position estimates of feature points in traditional RGB-D simultaneous localization and mapping (SLAM) systems are directly obtained by depth measurements. However, the available information provided by the triangulation of feature points has not been involved. In this article, a novel RGB-D SLAM approach is proposed based on the ORB-SLAM2 system, by fusing the triangulation estimates and depth measurements of ORB features, termed ORB-TEDM. Specifically, uncertainties on both the pose estimate of the camera and triangulation of ORB features are rigorously computed in a closed form for the ORB-SLAM2 system using covariance propagation and implicit differentiation. On the other hand, the uncertainty is evaluated for the 3-D position estimate of each map point obtained by its depth measurement from the RGB-D camera. Then, the triangulation results and position estimates from depth measurements are further fused with the covariance intersection (CI) filter to generate a more precise and consistent map. In addition, a more flexible selection policy of map points and keyframes is designed based on the uncertainty evaluation results. As a consequence, a more accurate and robust RGB-D SLAM system can be achieved. It is worthwhile to point out that the obtained closed-form solution to uncertainties on estimates of the camera pose and feature points can facilitate further development of the ORB-SLAM framework, such as active SLAM and multisensor SLAM. The experimental results on public datasets and in real-world environments are presented to show the effectiveness of the proposed approach.",
            "year": 2022,
            "venue": "IEEE Transactions on Instrumentation and Measurement",
            "authors": [
              {
                "authorId": "145078779",
                "name": "Jing Yuan"
              },
              {
                "authorId": "2116750416",
                "name": "Shuhao Zhu"
              },
              {
                "authorId": "123240760",
                "name": "Kaitao Tang"
              },
              {
                "authorId": "46252752",
                "name": "Qinxuan Sun"
              }
            ]
          }
        },
        {
          "citedcorpusid": 247213634,
          "isinfluential": false,
          "contexts": [
            "Similar to frame-based stereo disparity learning [19], [20], the methods proposed in the above works generally adhere to the following pipeline: first, stereo features are extracted through a feature extractor."
          ],
          "intents": [
            "--"
          ],
          "cited_paper_info": {
            "title": "Adaptive Cost Volume Representation for Unsupervised High-Resolution Stereo Matching",
            "abstract": "Learning-based stereomatching methods have produced remarkable results in recent years. However, typical supervised learning-based methods always suffer from the non-negligible problem of costly and time-consuming depth annotations. To mitigate this issue, in this work, a multi-stage unsupervised stereo matching method based on the cascaded Siamese network is proposed. To obtain a better performance on depth annotations, the improvements of this work are as follows. Firstly, sparse costs are constructed to predict the coarse disparity, and an adaptive sampling strategy is developed to dynamically adjust the sampling interval and effectively narrow the disparity search range. The proposed cost sparse and sampling strategy can certainly guarantee the accuracy of disparity estimation under the limited memory requirements. Then, geometric constraints with left and right semantic features are integrated into the loss function to learn the inherent matching correspondences. Next, information entropy of the probability volume is used to measure the quality of estimated disparity and designed as weighted guidance for the photometric loss. Finally, a pixel-wise disparity refinement module is designed to achieve high-resolution disparity estimation at the final stage. Experimental results on the datasets, including SceneFlow and KITTI, show the effectiveness and practicability of our method with limited memory consumption and running time.",
            "year": 2023,
            "venue": "IEEE Transactions on Intelligent Vehicles",
            "authors": [
              {
                "authorId": "2202590650",
                "name": "Kevin W. Tong"
              },
              {
                "authorId": "2176334272",
                "name": "Poly Z. H. Sun"
              },
              {
                "authorId": "66121253",
                "name": "E. Wu"
              },
              {
                "authorId": "2157341467",
                "name": "Changxu Wu"
              },
              {
                "authorId": "98809975",
                "name": "Zhibin Jiang"
              }
            ]
          }
        },
        {
          "citedcorpusid": 248304816,
          "isinfluential": false,
          "contexts": [
            "Event cameras [7], [8], [9], [10], [11], [12] asynchronously capture submicrosecond brightness changes at each pixel, outputting pixel coordinates, timestamps, and polarities of these brightness changes."
          ],
          "intents": [
            "--"
          ],
          "cited_paper_info": {
            "title": "Event-VPR: End-to-End Weakly Supervised Deep Network Architecture for Visual Place Recognition Using Event-Based Vision Sensor",
            "abstract": "Traditional visual place recognition (VPR) methods generally use frame-based cameras, which will easily fail due to rapid illumination changes or fast motion. To overcome this, we propose an end-to-end VPR network using event cameras, which can achieve good recognition performance in challenging environments (e.g., large-scale driving scenes). The key idea of the proposed algorithm is first to characterize the event streams with the EST voxel grid representation, then extract features using a deep residual network, and, finally, aggregate features using an improved VLAD network to realize end-to-end VPR using event streams. To verify the effectiveness of the proposed algorithm, on the event-based driving datasets (MVSEC, DDD17, and Brisbane-Event-VPR) and the synthetic event datasets (Oxford RobotCar and CARLA), we analyze the performance of our proposed method on large-scale driving sequences, including cross-weather, cross-season, and illumination changing scenes, and then, we compare the proposed method with the state-of-the-art event-based VPR method (Ensemble-Event-VPR) to prove its advantages. Experimental results show that the performance of the proposed method is better than that of the event-based ensemble scheme in challenging scenarios. To the best of our knowledge, for the VPR task, this is the first end-to-end weakly supervised deep network architecture that directly processes event stream data.",
            "year": 2022,
            "venue": "IEEE Transactions on Instrumentation and Measurement",
            "authors": [
              {
                "authorId": "2052315650",
                "name": "Delei Kong"
              },
              {
                "authorId": "2072874946",
                "name": "Zheng Fang"
              },
              {
                "authorId": "2007715213",
                "name": "Kuanxu Hou"
              },
              {
                "authorId": "153175521",
                "name": "Haojia Li"
              },
              {
                "authorId": "2107975649",
                "name": "Junjie Jiang"
              },
              {
                "authorId": "2069121180",
                "name": "Sonya A. Coleman"
              },
              {
                "authorId": "35201456",
                "name": "D. Kerr"
              }
            ]
          }
        },
        {
          "citedcorpusid": 249980412,
          "isinfluential": true,
          "contexts": [
            "Previous event-based stereo disparity estimation pipelines using deep learning [13], [14], [15], [16], [17], [18] have made significant contributions to this emerging field, achieving outstanding results.",
            "Nam et al. [16] proposed a learning-based approach to generate event representations by learning from multiple event frames within various time windows.",
            "3) Evaluation Metrics: Prior works are referenced [13], [14], [16], [17], and evaluation metrics such as RMSE, MAE, 1-pixel error (1PE), and 2-pixel error (2PE) are utilized.",
            "First, events are converted into mixed-density event stacking (MES) [16].",
            "Smaller time windows offer more accurate edges but contain less event information and require higher computational resources [16].",
            "Learning-based event representation methods [14], [16], [17] can overcome the aforementioned limitations, yielding more compact representations.",
            "Nam et al. [16] utilized learning-based representation and employed knowledge transfer techniques to learn a model that incorporates both past and future information.",
            "Since a single event contains very little information and the event camera does not produce data when it is stationary, the MES [16] is used as the input to the network.",
            "Deep learning-based event stereo disparity estimation algorithms [6], [13], [14], [15], [16], [17], [18] typically extract features from event streams through a neural network and represent them as feature maps.",
            "1) Quantitative Analysis: The proposed method is compared with SOTA algorithms (DDES [13], E-Stereo [14], Concentration Net [16], and DTC-PDS [17]) on 1PE, 2PE, MAE, and RMSE metrics using the DSEC disparity benchmark dataset."
          ],
          "intents": [
            "--",
            "--",
            "--",
            "--",
            "--",
            "--",
            "--",
            "--",
            "--",
            "--"
          ],
          "cited_paper_info": {
            "title": "Stereo Depth from Events Cameras: Concentrate and Focus on the Future",
            "abstract": "Neuromorphic cameras or event cameras mimic human vision by reporting changes in the intensity in a scene, instead of reporting the whole scene at once in a form of an image frame as performed by conventional cameras. Events are streamed data that are often dense when either the scene changes or the camera moves rapidly. The rapid movement causes the events to be overridden or missed when creating a tensor for the machine to learn on. To alleviate the event missing or overriding issue, we propose to learn to concentrate on the dense events to produce a compact event representation with high details for depth estimation. Specifically, we learn a model with events from both past and future but infer only with past data with the predicted future. We initially estimate depth in an event-only setting but also propose to further incorporate images and events by a hier-archical event and intensity combination network for better depth estimation. By experiments in challenging real-world scenarios, we validate that our method outperforms prior arts even with low computational cost. Code is available at: https://github.com/yonseivnl/se-cff.",
            "year": 2022,
            "venue": "Computer Vision and Pattern Recognition",
            "authors": [
              {
                "authorId": "1830605424",
                "name": "Yeongwoo Nam"
              },
              {
                "authorId": "114141661",
                "name": "Mohammad Mostafavi"
              },
              {
                "authorId": "51182421",
                "name": "Kuk-Jin Yoon"
              },
              {
                "authorId": "2119579051",
                "name": "Jonghyun Choi"
              }
            ]
          }
        },
        {
          "citedcorpusid": 253121300,
          "isinfluential": false,
          "contexts": [
            "They improve the disparity estimation performance using knowledge distillation [23], [24], [25], [26] by incorporating future event information to reinforce the disparity estimation results based on only past event information."
          ],
          "intents": [
            "--"
          ],
          "cited_paper_info": {
            "title": "KD-MVS: Knowledge Distillation Based Self-supervised Learning for Multi-view Stereo",
            "abstract": "Supervised multi-view stereo (MVS) methods have achieved remarkable progress in terms of reconstruction quality, but suffer from the challenge of collecting large-scale ground-truth depth. In this paper, we propose a novel self-supervised training pipeline for MVS based on knowledge distillation, termed KD-MVS, which mainly consists of self-supervised teacher training and distillation-based student training. Specifically, the teacher model is trained in a self-supervised fashion using both photometric and featuremetric consistency. Then we distill the knowledge of the teacher model to the student model through probabilistic knowledge transferring. With the supervision of validated knowledge, the student model is able to outperform its teacher by a large margin. Extensive experiments performed on multiple datasets show our method can even outperform supervised methods.",
            "year": 2022,
            "venue": "European Conference on Computer Vision",
            "authors": [
              {
                "authorId": "2152155111",
                "name": "Yikang Ding"
              },
              {
                "authorId": "1409972081",
                "name": "Qingtian Zhu"
              },
              {
                "authorId": "2144225462",
                "name": "Xiangyue Liu"
              },
              {
                "authorId": null,
                "name": "Wentao Yuan"
              },
              {
                "authorId": "2300026392",
                "name": "Haotian Zhang"
              },
              {
                "authorId": null,
                "name": "Chi Zhang"
              }
            ]
          }
        },
        {
          "citedcorpusid": 267740607,
          "isinfluential": false,
          "contexts": [
            "In the future, frame-event fusion methods will be explored to achieve higher performance [50], as well as deep spike-based stereo depth estimation network architectures suitable for energy-efficient inference [51], [52], [53]."
          ],
          "intents": [
            "--"
          ],
          "cited_paper_info": {
            "title": "Spike-EVPR: Deep Spiking Residual Network with Cross-Representation Aggregation for Event-Based Visual Place Recognition",
            "abstract": "Event cameras have been successfully applied to visual place recognition (VPR) tasks by using deep artificial neural networks (ANNs) in recent years. However, previously proposed deep ANN architectures are often unable to harness the abundant temporal information presented in event streams. In contrast, deep spiking networks exhibit more intricate spatiotemporal dynamics and are inherently well-suited to process sparse asynchronous event streams. Unfortunately, directly inputting temporal-dense event volumes into the spiking network introduces excessive time steps, resulting in prohibitively high training costs for large-scale VPR tasks. To address the aforementioned issues, we propose a novel deep spiking network architecture called Spike-EVPR for event-based VPR tasks. First, we introduce two novel event representations tailored for SNN to fully exploit the spatio-temporal information from the event streams, and reduce the video memory occupation during training as much as possible. Then, to exploit the full potential of these two representations, we construct a Bifurcated Spike Residual Encoder (BSR-Encoder) with powerful representational capabilities to better extract the high-level features from the two event representations. Next, we introduce a Shared&Specific Descriptor Extractor (SSD-Extractor). This module is designed to extract features shared between the two representations and features specific to each. Finally, we propose a Cross-Descriptor Aggregation Module (CDA-Module) that fuses the above three features to generate a refined, robust global descriptor of the scene. Our experimental results indicate the superior performance of our Spike-EVPR compared to several existing EVPR pipelines on Brisbane-Event-VPR and DDD20 datasets, with the average Recall@1 increased by 7.61% on Brisbane and 13.20% on DDD20.",
            "year": 2024,
            "venue": "arXiv.org",
            "authors": [
              {
                "authorId": "2284672285",
                "name": "Chenming Hu"
              },
              {
                "authorId": "2165663624",
                "name": "Zheng Fang"
              },
              {
                "authorId": "2007715213",
                "name": "Kuanxu Hou"
              },
              {
                "authorId": "2052315650",
                "name": "Delei Kong"
              },
              {
                "authorId": "2107975649",
                "name": "Junjie Jiang"
              },
              {
                "authorId": "2229193295",
                "name": "Zhuang Hao"
              },
              {
                "authorId": "2284645461",
                "name": "Mingyuan Sun"
              },
              {
                "authorId": "2152665453",
                "name": "Xinjie Huang"
              }
            ]
          }
        },
        {
          "citedcorpusid": 271736313,
          "isinfluential": false,
          "contexts": [
            "Deep learning-based event stereo disparity estimation algorithms [6], [13], [14], [15], [16], [17], [18] typically extract features from event streams through a neural network and represent them as feature maps.",
            "A S A fundamental and crucial measurement task, depth estimation [2], [3], [4], [5], [6] has attracted significant research attention."
          ],
          "intents": [
            "--",
            "--"
          ],
          "cited_paper_info": {
            "title": "Real-Time and High-Accuracy Switchable Stereo Depth Estimation Method Utilizing Self-Supervised Online Learning Mechanism for MIS",
            "abstract": "In minimally invasive surgery (MIS), clinicians often rely on 2-D laparoscopic images to assess the size and distances between internal structures. However, this subjective estimation introduces significant uncertainty and can increase surgical risks while reducing efficiency. Modern 3-D laparoscopes offer improved stereoscopic perception and can incorporate stereo depth estimation methods for quantitative analysis. However, existing methods struggle with real-time and high-accuracy demands in diverse surgical scenarios. To address this issue, we propose a novel intraoperative stereo depth estimation framework termed metainitialized online learning (MIOL), aiming to assist surgeons in quantitatively controlling surgical targets during the procedure. This framework features two switchable modes and does not require annotated data. One mode enables rapid depth recovery through surgical videos, providing real-time 3-D reconstruction to help surgeons understand in vivo structures. The other mode achieves high-precision measurements of critical tissues in fixed frames, assisting in surgical decision-making. Our approach employs self-supervised adaptation to train a model specific to each stereo image, eliminating the need for generalization and achieving outstanding accuracy. The framework establishes a lightweight network that converges rapidly under self-supervised losses and incorporates meta-learning pretraining, sparse optical flow guidance, and effective region identification to ensure speed and accuracy. Extensive experiments on two public datasets demonstrate the superiority of our method over existing approaches. Furthermore, we implement an intraoperative measurement system and conduct clinical trials, confirming its practical utility. The code is available at https://github.com/Darcy-vision/MIOL.",
            "year": 2024,
            "venue": "IEEE Transactions on Instrumentation and Measurement",
            "authors": [
              {
                "authorId": "2315083400",
                "name": "Jieyu Zheng"
              },
              {
                "authorId": "2116339095",
                "name": "Xiaojian Li"
              },
              {
                "authorId": "2315082137",
                "name": "Xin Wang"
              },
              {
                "authorId": "2315067772",
                "name": "Haojun Wu"
              },
              {
                "authorId": "2052168258",
                "name": "Ling Li"
              },
              {
                "authorId": "2315070290",
                "name": "Xiang Ma"
              },
              {
                "authorId": "2314942246",
                "name": "Shanlin Yang"
              }
            ]
          }
        },
        {
          "citedcorpusid": null,
          "isinfluential": false,
          "contexts": [
            "The event camera simulates the functionality of the transient visual pathway of the biological retina, where each pixel independently and asynchronously responds to relative brightness changes with a time resolution at the microsecond-level [30], [31]."
          ],
          "intents": [
            "--"
          ],
          "cited_paper_info": {}
        }
      ]
    },
    "22158024": {
      "citing_paper_info": {
        "title": "Event-Based Stereo Depth Estimation Using Belief Propagation",
        "abstract": "Compared to standard frame-based cameras, biologically-inspired event-based sensors capture visual information with low latency and minimal redundancy. These event-based sensors are also far less prone to motion blur than traditional cameras, and still operate effectively in high dynamic range scenes. However, classical framed-based algorithms are not typically suitable for these event-based data and new processing algorithms are required. This paper focuses on the problem of depth estimation from a stereo pair of event-based sensors. A fully event-based stereo depth estimation algorithm which relies on message passing is proposed. The algorithm not only considers the properties of a single event but also uses a Markov Random Field (MRF) to consider the constraints between the nearby events, such as disparity uniqueness and depth continuity. The method is tested on five different scenes and compared to other state-of-art event-based stereo matching methods. The results show that the method detects more stereo matches than other methods, with each match having a higher accuracy. The method can operate in an event-driven manner where depths are reported for individual events as they are received, or the network can be queried at any time to generate a sparse depth frame which represents the current state of the network.",
        "year": 2017,
        "venue": "Frontiers in Neuroscience",
        "authors": [
          {
            "authorId": "47661053",
            "name": "Zhen Xie"
          },
          {
            "authorId": "1788427",
            "name": "Shengyong Chen"
          },
          {
            "authorId": "33780923",
            "name": "G. Orchard"
          }
        ]
      },
      "citation_summary": {
        "citation_count": 10,
        "unique_cited_count": 10,
        "influential_count": 5,
        "detailed_records_count": 10
      },
      "cited_papers": [
        "24007071",
        "7495827",
        "6258804",
        "4833834",
        "12986049",
        "1686141",
        "8385399",
        "21317717",
        "8702465",
        "17693733"
      ],
      "citation_details": [
        {
          "citedcorpusid": 1686141,
          "isinfluential": false,
          "contexts": [
            "The datasets can be used not only for stereo but also for scene ﬂow, SLAM, and other event-based applications.",
            "Recently, some datasets for event-based simultaneous localization and mapping (SLAM) (Kogler et al., 2013; Serrano-Gotarredona et al., 2013) have become available, but none of those are created for event based stereo matching and the above previous works do not release their test datasets.",
            "Recently, some datasets for eventbased simultaneous localization and mapping (SLAM) (Kogler et al., 2013; Serrano-Gotarredona et al., 2013) have become available, but none of those are created for event based stereo matching and the above previous works do not release their test datasets."
          ],
          "intents": [
            "--",
            "['background']",
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Improved contrast sensitivity DVS and its application to event-driven stereo vision",
            "abstract": "",
            "year": 2013,
            "venue": "International Symposium on Circuits and Systems",
            "authors": [
              {
                "authorId": "1397317865",
                "name": "T. Serrano-Gotarredona"
              },
              {
                "authorId": "144926159",
                "name": "Jongkil Park"
              },
              {
                "authorId": "1396468177",
                "name": "A. Linares-Barranco"
              },
              {
                "authorId": "1398437019",
                "name": "A. Jiménez-Fernandez"
              },
              {
                "authorId": "1750848",
                "name": "R. Benosman"
              },
              {
                "authorId": "1397317879",
                "name": "B. Linares-Barranco"
              }
            ]
          }
        },
        {
          "citedcorpusid": 4833834,
          "isinfluential": true,
          "contexts": [
            "The datasets used in previous works (Rogister et al., 2012; Camuñasmesa et al., 2014; Firouzi and Conradt, 2016) both assume the cameras are static.",
            "Inthis paper we have replicated Rogister’s and Firouzi’s algorithms.",
            "Firouzi and Conradt (2016) came up with the dynamic cooperative neural network to make use of the stream of the events.",
            "Cop-net is used to denote Firouzi’s cooperative network approach, and EMP is used to denote our Event-based Message Passing approach.",
            "There are some state-of-art event-based stereo matching algorithms like Rogister’s ( Rogister et al., 2012), Camuñasmesa (Camuñasmesa et al., 2014), and Firouzi’s (Firouzi and Conradt, 2016)."
          ],
          "intents": [
            "['methodology']",
            "--",
            "['background']",
            "--",
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Asynchronous Event-based Cooperative Stereo Matching Using Neuromorphic Silicon Retinas",
            "abstract": "Biologically-inspired event-driven silicon retinas, so called dynamic vision sensors (DVS), allow efficient solutions for various visual perception tasks, e.g. surveillance, tracking, or motion detection. Similar to retinal photoreceptors, any perceived light intensity change in the DVS generates an event at the corresponding pixel. The DVS thereby emits a stream of spatiotemporal events to encode visually perceived objects that in contrast to conventional frame-based cameras, is largely free of redundant background information. The DVS offers multiple additional advantages, but requires the development of radically new asynchronous, event-based information processing algorithms. In this paper we present a fully event-based disparity matching algorithm for reliable 3D depth perception using a dynamic cooperative neural network. The interaction between cooperative cells applies cross-disparity uniqueness-constraints and within-disparity continuity-constraints, to asynchronously extract disparity for each new event, without any need of buffering individual events. We have investigated the algorithm’s performance in several experiments; our results demonstrate smooth disparity maps computed in a purely event-based manner, even in the scenes with temporally-overlapping stimuli.",
            "year": 2016,
            "venue": "Neural Processing Letters",
            "authors": [
              {
                "authorId": "145885214",
                "name": "M. Firouzi"
              },
              {
                "authorId": "3302681",
                "name": "J. Conradt"
              }
            ]
          }
        },
        {
          "citedcorpusid": 6258804,
          "isinfluential": false,
          "contexts": [
            "The datasets can be used not only for stereo but also for scene ﬂow, SLAM, and other event-based applications.",
            "Recently, some datasets for event-based simultaneous localization and mapping (SLAM) (Kogler et al., 2013; Serrano-Gotarredona et al., 2013) have become available, but none of those are created for event based stereo matching and the above previous works do not release their test datasets."
          ],
          "intents": [
            "--",
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Ground Truth Evaluation for Event-Based Silicon Retina Stereo Data",
            "abstract": "",
            "year": 2013,
            "venue": "2013 IEEE Conference on Computer Vision and Pattern Recognition Workshops",
            "authors": [
              {
                "authorId": "1824241",
                "name": "J. Kogler"
              },
              {
                "authorId": "2435053",
                "name": "F. Eibensteiner"
              },
              {
                "authorId": "1721721",
                "name": "M. Humenberger"
              },
              {
                "authorId": "1990797",
                "name": "M. Gelautz"
              },
              {
                "authorId": "2662311",
                "name": "J. Scharinger"
              }
            ]
          }
        },
        {
          "citedcorpusid": 7495827,
          "isinfluential": true,
          "contexts": [
            "A schematic of the MRF connectivity and messages are shown in Figure 4C .",
            "Finding labels that minimize the cost corresponds to a maximum a posteriori (MAP) estimation problem in an appropriately deﬁned MRF (Sun et al., 2003 Felzenszwalb and Huttenlocher, 2004).",
            "BP is a message passing algorithm for performing inference on graphical models, such as Bayesian networks and Markov random ﬁelds (MRF).",
            "Our goal is to ﬁnd proper label for each pixel to minimize the cost, which corresponds to a maximum a posteriori estimation problem in an appropriately deﬁned Markov Random Field(MAP-MRF).",
            "However, as shown in Figure 1 , the classical BP does not work for event accumulated frames, so we have to construct a modiﬁed MRF to manage the event-driven input and formulate a dynamic updating mechanism to deal with the temporal correlation of the event stream.",
            "The max-product BP algorithm can be used to solve the MAP-MRF problem eﬃciently (Felzenszwalb and Huttenlocher, 2004).",
            "Finally, the disparity output stage estimates the disparity of each event and generates a semi-dense disparity map with the updated MRF."
          ],
          "intents": [
            "--",
            "['background']",
            "--",
            "--",
            "--",
            "--",
            "--"
          ],
          "cited_paper_info": {
            "title": "Stereo Matching Using Belief Propagation",
            "abstract": "In this paper, we formulate the stereo matching problem as a Markov network consisting of three coupled Markov random fields (MRF's). These three MRF's model a smooth field for depth/disparity, a line process for depth discontinuity and a binary process for occlusion, respectively. After eliminating the line process and the binary process by introducing two robust functions, we obtain the maximum a posteriori (MAP) estimation in the Markov network by applying a Bayesian belief propagation (BP) algorithm. Furthermore, we extend our basic stereo model to incorporate other visual cues (e.g., image segmentation) that are not modeled in the three MRF's, and again obtain the MAP solution. Experimental results demonstrate that our method outperforms the state-of-art stereo algorithms for most test cases.",
            "year": 2002,
            "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
            "authors": [
              {
                "authorId": null,
                "name": "Jian Sun"
              },
              {
                "authorId": "144154486",
                "name": "H. Shum"
              },
              {
                "authorId": "122737130",
                "name": "N. Zheng"
              }
            ]
          }
        },
        {
          "citedcorpusid": 8385399,
          "isinfluential": false,
          "contexts": [
            "Inspired by Cook et al. (2011) who was using message passing algorithm to jointly estimate ego-motion intensity and optical ﬂow, we explore message passing for stereo depth estimation."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Interacting maps for fast visual interpretation",
            "abstract": "",
            "year": 2011,
            "venue": "The 2011 International Joint Conference on Neural Networks",
            "authors": [
              {
                "authorId": "144828543",
                "name": "Matthew Cook"
              },
              {
                "authorId": "2112201",
                "name": "Luca Gugelmann"
              },
              {
                "authorId": "1773294",
                "name": "Florian Jug"
              },
              {
                "authorId": "3157063",
                "name": "Christoph Krautz"
              },
              {
                "authorId": "143618393",
                "name": "A. Steger"
              }
            ]
          }
        },
        {
          "citedcorpusid": 8702465,
          "isinfluential": true,
          "contexts": [
            "For the event-driven message passing framework, we follow the idea from Felzenszwalb and Huttenlocher (2004), which deﬁnes stereo matching as a labeling problem.",
            "One possible method for estimating stereo is to construct frames by accumulating events over a period of time, and then use the BPmethod (Felzenszwalb and Huttenlocher, 2004) to process in a frame-based manner.",
            "Finding labels that minimize the cost corresponds to a maximum a posteriori (MAP) estimation problem in an appropriately deﬁned MRF (Sun et al., 2003 Felzenszwalb and Huttenlocher, 2004).",
            "We the min convolution algorithm from Felzenszwalb and Huttenlocher (2004) to reduce the complexity of message updating to be linear rather than quadratic in the number of labels.",
            "The max-product BP algorithm can be used to solve the MAP-MRF problem efficiently (Felzenszwalb and Huttenlocher, 2004).",
            "One possible method for estimating stereo is to construct frames by accumulating events over a period of time, and then use the BP method (Felzenszwalb and Huttenlocher, 2004) to process in a frame-based manner.",
            "The max-product BP algorithm can be used to solve the MAP-MRF problem eﬃciently (Felzenszwalb and Huttenlocher, 2004)."
          ],
          "intents": [
            "['methodology']",
            "['methodology']",
            "['background']",
            "['methodology']",
            "['methodology']",
            "['methodology']",
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Efficient Belief Propagation for Early Vision",
            "abstract": "",
            "year": 2004,
            "venue": "Proceedings of the 2004 IEEE Computer Society Conference on Computer Vision and Pattern Recognition, 2004. CVPR 2004.",
            "authors": [
              {
                "authorId": "1685089",
                "name": "Pedro F. Felzenszwalb"
              },
              {
                "authorId": "1713089",
                "name": "D. Huttenlocher"
              }
            ]
          }
        },
        {
          "citedcorpusid": 12986049,
          "isinfluential": false,
          "contexts": [
            "BP as a global cost optimization method is used by some state-of-art frame-based stereo methods on the Middlebury and KITTI benchmarks.",
            "Traditional frame-based stereo vision systems continue to steadily mature, in part thanks to publicly available datasets, such as the Middlebury (Scharstein and Szeliski, 2002) and KITTI (Menze and Geiger, 2015) benchmarks."
          ],
          "intents": [
            "--",
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Object scene flow for autonomous vehicles",
            "abstract": "",
            "year": 2015,
            "venue": "Computer Vision and Pattern Recognition",
            "authors": [
              {
                "authorId": "101841672",
                "name": "Moritz Menze"
              },
              {
                "authorId": "47237027",
                "name": "Andreas Geiger"
              }
            ]
          }
        },
        {
          "citedcorpusid": 17693733,
          "isinfluential": true,
          "contexts": [
            "In the experiments, ST is used to denote Rogister’s method which enforces Space (epipolar) and Time constraints for stereo matching.",
            "The datasets used in previous works (Rogister et al., 2012; Camuñasmesa et al., 2014; Firouzi and Conradt, 2016) both assume the cameras are static.",
            "Inthis paper we have replicated Rogister’s and Firouzi’s algorithms.",
            "Rogister et al. used one moving pen and two simultaneously moving pens as stimulus and showed the detected disparity (Rogister et al., 2012), but the accuracy of the algorithm is not quantitatively analyzed.",
            "However, matching using temporal and polarity criterion alone is prone to errors because the latency of events varies (jitter) (Rogister et al., 2012).",
            "There are some state-of-art event-based stereo matching algorithms like Rogister’s ( Rogister et al., 2012), Camuñasmesa (Camuñasmesa et al., 2014), and Firouzi’s (Firouzi and Conradt, 2016)."
          ],
          "intents": [
            "--",
            "['methodology']",
            "--",
            "['background']",
            "['background']",
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Asynchronous Event-Based Binocular Stereo Matching",
            "abstract": "",
            "year": 2012,
            "venue": "IEEE Transactions on Neural Networks and Learning Systems",
            "authors": [
              {
                "authorId": "3121605",
                "name": "P. Rogister"
              },
              {
                "authorId": "1750848",
                "name": "R. Benosman"
              },
              {
                "authorId": "144975525",
                "name": "S. Ieng"
              },
              {
                "authorId": "1744964",
                "name": "P. Lichtsteiner"
              },
              {
                "authorId": "5548576",
                "name": "T. Delbruck"
              }
            ]
          }
        },
        {
          "citedcorpusid": 21317717,
          "isinfluential": false,
          "contexts": [
            "Event-based vision sensors loosely mimic biological retinas, asynchronously generating events in response to relative light intensity changes rather than absolute image intensity (Posch et al., 2011).",
            "Many researchers have explored event-based matching criterions for event-based cameras such as ATIS (Posch et al., 2011) and DVS (Lichtsteiner et al., 2008)."
          ],
          "intents": [
            "['background']",
            "['background']"
          ],
          "cited_paper_info": {
            "title": "A QVGA 143 dB Dynamic Range Frame-Free PWM Image Sensor With Lossless Pixel-Level Video Compression and Time-Domain CDS",
            "abstract": "",
            "year": 2011,
            "venue": "IEEE Journal of Solid-State Circuits",
            "authors": [
              {
                "authorId": "153466606",
                "name": "C. Posch"
              },
              {
                "authorId": "1758423",
                "name": "D. Matolin"
              },
              {
                "authorId": "2509695",
                "name": "R. Wohlgenannt"
              }
            ]
          }
        },
        {
          "citedcorpusid": 24007071,
          "isinfluential": true,
          "contexts": [
            "For the event-based stereo setup, we rely on two DAVIS240C (Brandli et al., 2014) sensors.",
            "Calibration is performed by using the frame-capture capability of the DAVIS240C to simultaneously record frames from both sensors, which can then be used with OpenCV to calibrate.",
            "Data from the DAVIS240C, ZED, and Vicon are simultaneously recorded using the Robot Operating System (ROS) 3 .",
            "Events are read out from each DAVIS240C sensor independently over two separate USB cables, but their timestamps are synchronized using the standard timestamp synchronization feature of the sensors (which relies on the audio cable seen in the ﬁgure).",
            "It consists of the ZED frame-based stereo sensor mounted below two event-based DAVIS240C sensors, all of which are held together with a 3D printed plastic mounting."
          ],
          "intents": [
            "['methodology']",
            "--",
            "--",
            "--",
            "--"
          ],
          "cited_paper_info": {
            "title": "A 240 × 180 130 dB 3 µs Latency Global Shutter Spatiotemporal Vision Sensor",
            "abstract": "",
            "year": 2014,
            "venue": "IEEE Journal of Solid-State Circuits",
            "authors": [
              {
                "authorId": "2239977",
                "name": "Christian Brandli"
              },
              {
                "authorId": "144246116",
                "name": "R. Berner"
              },
              {
                "authorId": "1779496",
                "name": "Minhao Yang"
              },
              {
                "authorId": "1704961",
                "name": "Shih-Chii Liu"
              },
              {
                "authorId": "5548576",
                "name": "T. Delbruck"
              }
            ]
          }
        }
      ]
    },
    "252918208": {
      "citing_paper_info": {
        "title": "Event-based Stereo Depth Estimation from Ego-motion using Ray Density Fusion",
        "abstract": ". Event cameras are bio-inspired sensors that mimic the human retina by responding to brightness changes in the scene. They generate asynchronous spike-based outputs at microsecond resolution, provid-ing advantages over traditional cameras like high dynamic range, low motion blur and power eﬃciency. Most event-based stereo methods attempt to exploit the high temporal resolution of the camera and the simultane-ity of events across cameras to establish matches and estimate depth. By contrast, this work investigates how to estimate depth from stereo event cameras without explicit data association by fusing back-projected ray densities, and demonstrates its eﬀectiveness on head-mounted camera data, which is recorded in an egocentric fashion. Code and video are available at https://github.com/tub-rip/dvs mcemvs",
        "year": 2022,
        "venue": "arXiv.org",
        "authors": [
          {
            "authorId": "2155615482",
            "name": "Suman Ghosh"
          },
          {
            "authorId": "144036711",
            "name": "Guillermo Gallego"
          }
        ]
      },
      "citation_summary": {
        "citation_count": 5,
        "unique_cited_count": 5,
        "influential_count": 0,
        "detailed_records_count": 5
      },
      "cited_papers": [
        "244920800",
        "9642065",
        "16588072",
        "237142365",
        "215799961"
      ],
      "citation_details": [
        {
          "citedcorpusid": 9642065,
          "isinfluential": false,
          "contexts": [
            "Additionally, we demonstrate the advantage of using events over standard frames for tackling HDR scenes in Fig.",
            "Moreover, the outstanding properties of event cameras, such as high dynamic range (HDR), high temporal resolution (≈ μs) and low power consumption, offer potential to tackle scenarios that are challenging for standard cameras (high speed and/or HDR) [1,3, 6, 7, 15,16,18].",
            "Moreover, the outstanding properties of event cameras, such as high dynamic range (HDR), high temporal resolution (≈ µs) and low power consumption, offer potential to tackle scenarios that are challenging for standard cameras (high speed and/or HDR) [1,3, 6, 7, 15,16,18]."
          ],
          "intents": [
            "--",
            "['background']",
            "--"
          ],
          "cited_paper_info": {
            "title": "Robotic goalie with 3 ms reaction time at 4% CPU load using event-based dynamic vision sensor",
            "abstract": "Conventional vision-based robotic systems that must operate quickly require high video frame rates and consequently high computational costs. Visual response latencies are lower-bound by the frame period, e.g., 20 ms for 50 Hz frame rate. This paper shows how an asynchronous neuromorphic dynamic vision sensor (DVS) silicon retina is used to build a fast self-calibrating robotic goalie, which offers high update rates and low latency at low CPU load. Independent and asynchronous per pixel illumination change events from the DVS signify moving objects and are used in software to track multiple balls. Motor actions to block the most “threatening” ball are based on measured ball positions and velocities. The goalie also sees its single-axis goalie arm and calibrates the motor output map during idle periods so that it can plan open-loop arm movements to desired visual locations. Blocking capability is about 80% for balls shot from 1 m from the goal even with the fastest-shots, and approaches 100% accuracy when the ball does not beat the limits of the servo motor to move the arm to the necessary position in time. Running with standard USB buses under a standard preemptive multitasking operating system (Windows), the goalie robot achieves median update rates of 550 Hz, with latencies of 2.2 ± 2 ms from ball movement to motor command at a peak CPU load of less than 4%. Practical observations and measurements of USB device latency are provided1.",
            "year": 2013,
            "venue": "Frontiers in Neuroscience",
            "authors": [
              {
                "authorId": "5548576",
                "name": "T. Delbruck"
              },
              {
                "authorId": "2353652387",
                "name": "M. Lang"
              }
            ]
          }
        },
        {
          "citedcorpusid": 16588072,
          "isinfluential": false,
          "contexts": [
            "Similar to the monocular method in [17], known camera poses are used to back-project events into space in the form of rays, referred to as a Disparity Space Image (DSI).",
            "Correspondencefree depth estimation for monocular event cameras has been shown to generate state-of-the-art results in visual odometry [17]."
          ],
          "intents": [
            "['methodology']",
            "['background']"
          ],
          "cited_paper_info": {
            "title": "EVO: A Geometric Approach to Event-Based 6-DOF Parallel Tracking and Mapping in Real Time",
            "abstract": "",
            "year": 2017,
            "venue": "IEEE Robotics and Automation Letters",
            "authors": [
              {
                "authorId": "3414274",
                "name": "Henri Rebecq"
              },
              {
                "authorId": "9676873",
                "name": "Timo Horstschaefer"
              },
              {
                "authorId": "144036711",
                "name": "Guillermo Gallego"
              },
              {
                "authorId": "2075371",
                "name": "D. Scaramuzza"
              }
            ]
          }
        },
        {
          "citedcorpusid": 215799961,
          "isinfluential": false,
          "contexts": [
            "Event cameras, such as the Dynamic Vision Sensor [4, 11] (DVS), mimic the transient visual pathway in humans.",
            "It is also the first public visual-inertial dataset with 1 Megapixel stereo event cameras [4]."
          ],
          "intents": [
            "['background']",
            "['background']"
          ],
          "cited_paper_info": {
            "title": "5.10 A 1280×720 Back-Illuminated Stacked Temporal Contrast Event-Based Vision Sensor with 4.86µm Pixels, 1.066GEPS Readout, Programmable Event-Rate Controller and Compressive Data-Formatting Pipeline",
            "abstract": "Event-based (EB) vision sensors pixel-individually detect temporal contrast exceeding a preset relative threshold [1], [2] to follow the temporal evolution of relative light changes (contrast detection, CD) and to define sampling points for frame-free pixel-level measurement of absolute intensity (exposure measurement, EM) [3], [4]. EB sensors gain popularity in high-speed low-power machine vision applications thanks to temporal precision of recorded data, inherent suppression of temporal redundancy resulting in reduced post-processing cost, and wide intra-scene dynamic range operation.",
            "year": 2020,
            "venue": "IEEE International Solid-State Circuits Conference",
            "authors": [
              {
                "authorId": "9413276",
                "name": "T. Finateu"
              },
              {
                "authorId": "38934516",
                "name": "Atsumi Niwa"
              },
              {
                "authorId": "1758423",
                "name": "D. Matolin"
              },
              {
                "authorId": "1637236417",
                "name": "Koya Tsuchimoto"
              },
              {
                "authorId": "49886151",
                "name": "A. Mascheroni"
              },
              {
                "authorId": "2083159785",
                "name": "Etienne Reynaud"
              },
              {
                "authorId": "2920529",
                "name": "P. Mostafalu"
              },
              {
                "authorId": "40792639",
                "name": "F. Brady"
              },
              {
                "authorId": "3444944",
                "name": "L. Chotard"
              },
              {
                "authorId": "94271083",
                "name": "F. L. Goff"
              },
              {
                "authorId": "38884358",
                "name": "H. Takahashi"
              },
              {
                "authorId": "145762314",
                "name": "H. Wakabayashi"
              },
              {
                "authorId": "3228095",
                "name": "Y. Oike"
              },
              {
                "authorId": "153466606",
                "name": "C. Posch"
              }
            ]
          }
        },
        {
          "citedcorpusid": 237142365,
          "isinfluential": false,
          "contexts": [
            "1 on a variety of datasets, here we present depth estimation results on the egocentric TUM-VIE dataset [10] for the sake of brevity.",
            "1 on TUM-VIE data [10] (1Mpix cameras)."
          ],
          "intents": [
            "['background']",
            "--"
          ],
          "cited_paper_info": {
            "title": "TUM-VIE: The TUM Stereo Visual-Inertial Event Dataset",
            "abstract": "Event cameras are bio-inspired vision sensors which measure per pixel brightness changes. They offer numerous benefits over traditional, frame-based cameras, including low latency, high dynamic range, high temporal resolution and low power consumption. Thus, these sensors are suited for robotics and virtual reality applications. To foster the development of 3D perception and navigation algorithms with event cameras, we present the TUM-VIE dataset. It consists of a large variety of handheld and head-mounted sequences in indoor and outdoor environments, including rapid motion during sports and high dynamic range scenarios. The dataset contains stereo event data, stereo grayscale frames at 20Hz as well as IMU data at 200Hz. Timestamps between all sensors are synchronized in hardware. The event cameras contain a large sensor of 1280x720 pixels, which is significantly larger than the sensors used in existing stereo event datasets (at least by a factor of ten). We provide ground truth poses from a motion capture system at 120Hz during the beginning and end of each sequence, which can be used for trajectory evaluation. TUM-VIE includes challenging sequences where state-of-the art visual SLAM algorithms either fail or result in large drift. Hence, our dataset can help to push the boundary of future research on event-based visual-inertial perception algorithms.",
            "year": 2021,
            "venue": "IEEE/RJS International Conference on Intelligent RObots and Systems",
            "authors": [
              {
                "authorId": "1966877",
                "name": "Simone Klenk"
              },
              {
                "authorId": "145333351",
                "name": "J. Chui"
              },
              {
                "authorId": "1979699",
                "name": "Nikolaus Demmel"
              },
              {
                "authorId": "1695302",
                "name": "D. Cremers"
              }
            ]
          }
        },
        {
          "citedcorpusid": 244920800,
          "isinfluential": false,
          "contexts": [
            "Additionally, we demonstrate the advantage of using events over standard frames for tackling HDR scenes in Fig.",
            "Moreover, the outstanding properties of event cameras, such as high dynamic range (HDR), high temporal resolution (≈ μs) and low power consumption, offer potential to tackle scenarios that are challenging for standard cameras (high speed and/or HDR) [1,3, 6, 7, 15,16,18].",
            "Moreover, the outstanding properties of event cameras, such as high dynamic range (HDR), high temporal resolution (≈ µs) and low power consumption, offer potential to tackle scenarios that are challenging for standard cameras (high speed and/or HDR) [1,3, 6, 7, 15,16,18]."
          ],
          "intents": [
            "--",
            "['background']",
            "--"
          ],
          "cited_paper_info": {
            "title": "E2(GO)MOTION: Motion Augmented Event Stream for Egocentric Action Recognition",
            "abstract": "Event cameras are novel bio-inspired sensors, which asynchronously capture pixel-level intensity changes in the form of “events”. Due to their sensing mechanism, event cameras have little to no motion blur, a very high temporal resolution and require significantly less power and memory than traditional frame-based cameras. These characteristics make them a perfect fit to several real-world applications such as egocentric action recognition on wearable devices, where fast camera motion and limited power challenge traditional vision sensors. However, the ever-growing field of event-based vision has, to date, overlooked the potential of event cameras in such applications. In this paper, we show that event data is a very valuable modality for egocentric action recognition. To do so, we introduce N-EPIC-Kitchens, the first event-based camera extension of the large-scale EPIC-Kitchens dataset. In this context, we propose two strategies: (i) directly processing event-camera data with traditional video-processing architectures (E2(GO)) and (ii) using event-data to distill optical flow information (E2(GO)MO). On our proposed benchmark, we show that event data provides a comparable performance to RGB and optical flow, yet without any additional flow computation at deploy time, and an improved performance of up to 4% with respect to RGB only information. The N-EPIC-Kitchens dataset is available at https://github.com/EgocentricVision/N-EPIC-Kitchens.",
            "year": 2021,
            "venue": "Computer Vision and Pattern Recognition",
            "authors": [
              {
                "authorId": "1879516321",
                "name": "Chiara Plizzari"
              },
              {
                "authorId": "51005155",
                "name": "Mirco Planamente"
              },
              {
                "authorId": "2143262888",
                "name": "Gabriele Goletto"
              },
              {
                "authorId": "46235249",
                "name": "Marco Cannici"
              },
              {
                "authorId": "2143268839",
                "name": "Emanuele Gusso"
              },
              {
                "authorId": "145927530",
                "name": "M. Matteucci"
              },
              {
                "authorId": "1752593147",
                "name": "Barbara Caputo"
              }
            ]
          }
        }
      ]
    },
    "276248927": {
      "citing_paper_info": {
        "title": "Unifying Event-based Flow, Stereo and Depth Estimation via Feature Similarity Matching",
        "abstract": "As an emerging vision sensor, the event camera has gained popularity in various vision tasks such as optical flow estimation, stereo matching, and depth estimation due to its high-speed, sparse, and asynchronous event streams. Unlike traditional approaches that use specialized architectures for each specific task, we propose a unified framework, EventMatch, that reformulates these tasks as an event-based dense correspondence matching problem, allowing them to be solved with a single model by directly comparing feature similarities. By utilizing a shared feature similarities module, which integrates knowledge from other event flows via temporal or spatial interactions, and distinct task heads, our network can concurrently perform optical flow estimation from temporal inputs (e.g., two segments of event streams in the temporal domain) and stereo matching from spatial inputs (e.g., two segments of event streams from different viewpoints in the spatial domain). Moreover, we further demonstrate that our unified model inherently supports cross-task transfer since the architecture and parameters are shared across tasks. Without the need for retraining on each task, our model can effectively handle both optical flow and disparity estimation simultaneously. The experiment conducted on the DSEC benchmark demonstrates that our model exhibits superior performance in both optical flow and disparity estimation tasks, outperforming existing state-of-the-art methods. Our unified approach not only advances event-based models but also opens new possibilities for cross-task transfer and inter-task fusion in both spatial and temporal dimensions. Our code will be available later.",
        "year": 2024,
        "venue": "arXiv.org",
        "authors": [
          {
            "authorId": "2314328191",
            "name": "Pengjie Zhang"
          },
          {
            "authorId": "2275786010",
            "name": "Lin Zhu"
          },
          {
            "authorId": "2258671866",
            "name": "Lizhi Wang"
          },
          {
            "authorId": "2271866232",
            "name": "Hua Huang"
          }
        ]
      },
      "citation_summary": {
        "citation_count": 27,
        "unique_cited_count": 27,
        "influential_count": 2,
        "detailed_records_count": 27
      },
      "cited_papers": [
        "17407641",
        "257631432",
        "1151030",
        "244306440",
        "11008141",
        "4597042",
        "56366093",
        "10280488",
        "262638843",
        "269525186",
        "12552176",
        "219037720",
        "118684904",
        "239049376",
        "2121536",
        "26169625",
        "216036364",
        "250602271",
        "1082643",
        "57573786",
        "189998802",
        "10716717",
        "257505349",
        "214667893",
        "244709323",
        "56475917",
        "3396150"
      ],
      "citation_details": [
        {
          "citedcorpusid": 1082643,
          "isinfluential": false,
          "contexts": [
            "Based on event cameras, there has been considerable research on computer vision algorithms, such as image reconstruction [9, 10], optical flow estimation [11, 12], depth estimation [13, 14], etc.",
            "For the latter, the objective of vision tasks is to comprehend the structure of every 3D object, such as stereo matching [19–21], depth estimation [14]."
          ],
          "intents": [
            "--",
            "--"
          ],
          "cited_paper_info": {
            "title": "EMVS: Event-Based Multi-View Stereo—3D Reconstruction with an Event Camera in Real-Time",
            "abstract": "",
            "year": 2018,
            "venue": "International Journal of Computer Vision",
            "authors": [
              {
                "authorId": "3414274",
                "name": "Henri Rebecq"
              },
              {
                "authorId": "144036711",
                "name": "Guillermo Gallego"
              },
              {
                "authorId": "144578041",
                "name": "Elias Mueggler"
              },
              {
                "authorId": "2075371",
                "name": "D. Scaramuzza"
              }
            ]
          }
        },
        {
          "citedcorpusid": 1151030,
          "isinfluential": false,
          "contexts": [
            "And many works propose their own insights [31–33] to improve model performance."
          ],
          "intents": [
            "--"
          ],
          "cited_paper_info": {
            "title": "Contour Motion Estimation for Asynchronous Event-Driven Cameras",
            "abstract": "",
            "year": 2014,
            "venue": "Proceedings of the IEEE",
            "authors": [
              {
                "authorId": "144484799",
                "name": "Francisco Barranco"
              },
              {
                "authorId": "1759899",
                "name": "C. Fermüller"
              },
              {
                "authorId": "1697493",
                "name": "Y. Aloimonos"
              }
            ]
          }
        },
        {
          "citedcorpusid": 2121536,
          "isinfluential": false,
          "contexts": [
            "Traditionally, optical flow estimation relies on variational approaches [1, 2], where it is commonly tackled as an energy minimization problem.",
            "Benosman et al. [29] design an event-based method to estimate optical flow with inspiration from the Lucas-Kanade algorithm [2].",
            "Based on different visual tasks, there are many classic vision algorithms, such as optical flow estimation [1, 2], stereo matching [3–5], depth estimation[6], etc."
          ],
          "intents": [
            "--",
            "--",
            "--"
          ],
          "cited_paper_info": {
            "title": "An Iterative Image Registration Technique with an Application to Stereo Vision",
            "abstract": "Image registration finds a variety of applications in computer vision. Unfortunately, traditional image registration techniques tend to be costly. We present a new image registration technique that makes use of the spatial intensity gradient of the images to find a good match using a type of Newton-Raphson iteration. Our technique is taster because it examines far fewer potential matches between the images than existing techniques Furthermore, this registration technique can be generalized to handle rotation, scaling and shearing. We show how our technique can be adapted tor use in a stereo vision system.",
            "year": 1981,
            "venue": "International Joint Conference on Artificial Intelligence",
            "authors": [
              {
                "authorId": "40588702",
                "name": "B. D. Lucas"
              },
              {
                "authorId": "1733113",
                "name": "T. Kanade"
              }
            ]
          }
        },
        {
          "citedcorpusid": 3396150,
          "isinfluential": false,
          "contexts": [
            "For example, EV-FlowNet [15, 34] applies a deep neural network to process two consecutive event streams as static frames, just like FlowNet [24].",
            "For the former, the purpose of vision algorithms is to understand motion in 3D scenes, such as optical flow [15–17], object tracking [18]."
          ],
          "intents": [
            "--",
            "--"
          ],
          "cited_paper_info": {
            "title": "EV-FlowNet: Self-Supervised Optical Flow Estimation for Event-based Cameras",
            "abstract": "Event-based cameras have shown great promise in a variety of situations where frame based cameras suffer, such as high speed motions and high dynamic range scenes. However, developing algorithms for event measurements requires a new class of hand crafted algorithms. Deep learning has shown great success in providing model free solutions to many problems in the vision community, but existing networks have been developed with frame based images in mind, and there does not exist the wealth of labeled data for events as there does for images for supervised training. To these points, we present EV-FlowNet, a novel self-supervised deep learning pipeline for optical flow estimation for event based cameras. In particular, we introduce an image based representation of a given event stream, which is fed into a self-supervised neural network as the sole input. The corresponding grayscale images captured from the same camera at the same time as the events are then used as a supervisory signal to provide a loss function at training time, given the estimated flow from the network. We show that the resulting network is able to accurately predict optical flow from events only in a variety of different scenes, with performance competitive to image based networks. This method not only allows for accurate estimation of dense optical flow, but also provides a framework for the transfer of other self-supervised methods to the event-based domain.",
            "year": 2018,
            "venue": "Robotics: Science and Systems",
            "authors": [
              {
                "authorId": "3385588",
                "name": "A. Z. Zhu"
              },
              {
                "authorId": "36001694",
                "name": "Liangzhe Yuan"
              },
              {
                "authorId": "20728097",
                "name": "Kenneth Chaney"
              },
              {
                "authorId": "1751586",
                "name": "Kostas Daniilidis"
              }
            ]
          }
        },
        {
          "citedcorpusid": 4597042,
          "isinfluential": false,
          "contexts": [
            "Based on event cameras, there has been considerable research on computer vision algorithms, such as image reconstruction [9, 10], optical flow estimation [11, 12], depth estimation [13, 14], etc.",
            "Gallego et al. [12, 30] propose a framework with the principle of contrast maximization.",
            "Gallego et al. [12, 47] have proposed a unifying framework with contrast maximization (CM) strategy, whose main idea is to find the point trajectories on the image plane that are best aligned with the event data."
          ],
          "intents": [
            "--",
            "--",
            "--"
          ],
          "cited_paper_info": {
            "title": "A Unifying Contrast Maximization Framework for Event Cameras, with Applications to Motion, Depth, and Optical Flow Estimation",
            "abstract": "We present a unifying framework to solve several computer vision problems with event cameras: motion, depth and optical flow estimation. The main idea of our framework is to find the point trajectories on the image plane that are best aligned with the event data by maximizing an objective function: the contrast of an image of warped events. Our method implicitly handles data association between the events, and therefore, does not rely on additional appearance information about the scene. In addition to accurately recovering the motion parameters of the problem, our framework produces motion-corrected edge-like images with high dynamic range that can be used for further scene analysis. The proposed method is not only simple, but more importantly, it is, to the best of our knowledge, the first method that can be successfully applied to such a diverse set of important vision tasks with event cameras.",
            "year": 2018,
            "venue": "2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition",
            "authors": [
              {
                "authorId": "144036711",
                "name": "Guillermo Gallego"
              },
              {
                "authorId": "3414274",
                "name": "Henri Rebecq"
              },
              {
                "authorId": "2075371",
                "name": "D. Scaramuzza"
              }
            ]
          }
        },
        {
          "citedcorpusid": 10280488,
          "isinfluential": false,
          "contexts": [
            "Based on event cameras, there has been considerable research on computer vision algorithms, such as image reconstruction [9, 10], optical flow estimation [11, 12], depth estimation [13, 14], etc."
          ],
          "intents": [
            "--"
          ],
          "cited_paper_info": {
            "title": "Simultaneous Optical Flow and Intensity Estimation from an Event Camera",
            "abstract": "",
            "year": 2016,
            "venue": "Computer Vision and Pattern Recognition",
            "authors": [
              {
                "authorId": "7642780",
                "name": "Patrick Bardow"
              },
              {
                "authorId": "2052135690",
                "name": "A. Davison"
              },
              {
                "authorId": "2864731",
                "name": "Stefan Leutenegger"
              }
            ]
          }
        },
        {
          "citedcorpusid": 10716717,
          "isinfluential": false,
          "contexts": [
            "Multi-scale feature pyramid is widely used in the field of computer vision, such as optical flow estimation [25] and object detection [52]."
          ],
          "intents": [
            "--"
          ],
          "cited_paper_info": {
            "title": "Feature Pyramid Networks for Object Detection",
            "abstract": "Feature pyramids are a basic component in recognition systems for detecting objects at different scales. But pyramid representations have been avoided in recent object detectors that are based on deep convolutional networks, partially because they are slow to compute and memory intensive. In this paper, we exploit the inherent multi-scale, pyramidal hierarchy of deep convolutional networks to construct feature pyramids with marginal extra cost. A top-down architecture with lateral connections is developed for building high-level semantic feature maps at all scales. This architecture, called a Feature Pyramid Network (FPN), shows significant improvement as a generic feature extractor in several applications. Using a basic Faster R-CNN system, our method achieves state-of-the-art single-model results on the COCO detection benchmark without bells and whistles, surpassing all existing single-model entries including those from the COCO 2016 challenge winners. In addition, our method can run at 5 FPS on a GPU and thus is a practical and accurate solution to multi-scale object detection. Code will be made publicly available.",
            "year": 2016,
            "venue": "Computer Vision and Pattern Recognition",
            "authors": [
              {
                "authorId": "33493200",
                "name": "Tsung-Yi Lin"
              },
              {
                "authorId": "3127283",
                "name": "Piotr Dollár"
              },
              {
                "authorId": "2983898",
                "name": "Ross B. Girshick"
              },
              {
                "authorId": "39353098",
                "name": "Kaiming He"
              },
              {
                "authorId": "1790580",
                "name": "Bharath Hariharan"
              },
              {
                "authorId": "50172592",
                "name": "Serge J. Belongie"
              }
            ]
          }
        },
        {
          "citedcorpusid": 11008141,
          "isinfluential": false,
          "contexts": [
            "Based on different visual tasks, there are many classic vision algorithms, such as optical flow estimation [1, 2], stereo matching [3–5], depth estimation[6], etc."
          ],
          "intents": [
            "--"
          ],
          "cited_paper_info": {
            "title": "A space-sweep approach to true multi-image matching",
            "abstract": "",
            "year": 1996,
            "venue": "Proceedings CVPR IEEE Computer Society Conference on Computer Vision and Pattern Recognition",
            "authors": [
              {
                "authorId": "143980462",
                "name": "R. Collins"
              }
            ]
          }
        },
        {
          "citedcorpusid": 12552176,
          "isinfluential": false,
          "contexts": [
            "For example, EV-FlowNet [15, 34] applies a deep neural network to process two consecutive event streams as static frames, just like FlowNet [24].",
            "Since the introduction of FlowNet [24], which pioneers the application of deep-learning methods for optical flow estimation, neural networks [25–28] trained on large-scale datasets have emerged as a new paradigm."
          ],
          "intents": [
            "--",
            "--"
          ],
          "cited_paper_info": {
            "title": "FlowNet: Learning Optical Flow with Convolutional Networks",
            "abstract": "Convolutional neural networks (CNNs) have recently been very successful in a variety of computer vision tasks, especially on those linked to recognition. Optical flow estimation has not been among the tasks CNNs succeeded at. In this paper we construct CNNs which are capable of solving the optical flow estimation problem as a supervised learning task. We propose and compare two architectures: a generic architecture and another one including a layer that correlates feature vectors at different image locations. Since existing ground truth data sets are not sufficiently large to train a CNN, we generate a large synthetic Flying Chairs dataset. We show that networks trained on this unrealistic data still generalize very well to existing datasets such as Sintel and KITTI, achieving competitive accuracy at frame rates of 5 to 10 fps.",
            "year": 2015,
            "venue": "IEEE International Conference on Computer Vision",
            "authors": [
              {
                "authorId": "2841331",
                "name": "Alexey Dosovitskiy"
              },
              {
                "authorId": "152702479",
                "name": "P. Fischer"
              },
              {
                "authorId": "48105320",
                "name": "Eddy Ilg"
              },
              {
                "authorId": "2880264",
                "name": "Philip Häusser"
              },
              {
                "authorId": "3322806",
                "name": "C. Hazirbas"
              },
              {
                "authorId": "2943639",
                "name": "Vladimir Golkov"
              },
              {
                "authorId": "1715782",
                "name": "Patrick van der Smagt"
              },
              {
                "authorId": "1695302",
                "name": "D. Cremers"
              },
              {
                "authorId": "1710872",
                "name": "T. Brox"
              }
            ]
          }
        },
        {
          "citedcorpusid": 17407641,
          "isinfluential": false,
          "contexts": [
            "Benosman et al. [29] design an event-based method to estimate optical flow with inspiration from the Lucas-Kanade algorithm [2]."
          ],
          "intents": [
            "--"
          ],
          "cited_paper_info": {
            "title": "Asynchronous frameless event-based optical flow",
            "abstract": "",
            "year": 2012,
            "venue": "Neural Networks",
            "authors": [
              {
                "authorId": "1750848",
                "name": "R. Benosman"
              },
              {
                "authorId": "144975525",
                "name": "S. Ieng"
              },
              {
                "authorId": "48762590",
                "name": "Charles Clercq"
              },
              {
                "authorId": "1897771",
                "name": "C. Bartolozzi"
              },
              {
                "authorId": "145339648",
                "name": "M. Srinivasan"
              }
            ]
          }
        },
        {
          "citedcorpusid": 26169625,
          "isinfluential": false,
          "contexts": [
            "SGM[3], PatchMatch[4], and ADCensus[5] are some classic algorithms among them, whose core idea is matching and optimization."
          ],
          "intents": [
            "--"
          ],
          "cited_paper_info": {
            "title": "PatchMatch: a randomized correspondence algorithm for structural image editing",
            "abstract": "",
            "year": 2009,
            "venue": "ACM Transactions on Graphics",
            "authors": [
              {
                "authorId": "2496412",
                "name": "Connelly Barnes"
              },
              {
                "authorId": "2177801",
                "name": "Eli Shechtman"
              },
              {
                "authorId": "37737599",
                "name": "Adam Finkelstein"
              },
              {
                "authorId": "1976171",
                "name": "Dan B. Goldman"
              }
            ]
          }
        },
        {
          "citedcorpusid": 56366093,
          "isinfluential": false,
          "contexts": [
            "HD3 [45] utilizes the match density estimation method to design task-specific decoders for optical flow and stereo matching."
          ],
          "intents": [
            "--"
          ],
          "cited_paper_info": {
            "title": "Hierarchical Discrete Distribution Decomposition for Match Density Estimation",
            "abstract": "Explicit representations of the global match distributions of pixel-wise correspondences between pairs of images are desirable for uncertainty estimation and downstream applications. However, the computation of the match density for each pixel may be prohibitively expensive due to the large number of candidates. In this paper, we propose Hierarchical Discrete Distribution Decomposition (HD^3), a framework suitable for learning probabilistic pixel correspondences in both optical flow and stereo matching. We decompose the full match density into multiple scales hierarchically, and estimate the local matching distributions at each scale conditioned on the matching and warping at coarser scales. The local distributions can then be composed together to form the global match density. Despite its simplicity, our probabilistic method achieves state-of-the-art results for both optical flow and stereo matching on established benchmarks. We also find the estimated uncertainty is a good indication of the reliability of the predicted correspondences.",
            "year": 2018,
            "venue": "Computer Vision and Pattern Recognition",
            "authors": [
              {
                "authorId": "2069532893",
                "name": "Zhichao Yin"
              },
              {
                "authorId": "1753210",
                "name": "Trevor Darrell"
              },
              {
                "authorId": "1807197",
                "name": "F. Yu"
              }
            ]
          }
        },
        {
          "citedcorpusid": 56475917,
          "isinfluential": false,
          "contexts": [
            "For example, EV-FlowNet [15, 34] applies a deep neural network to process two consecutive event streams as static frames, just like FlowNet [24].",
            "EV-FlowNet [34], an early deep learning network for event cameras, has been exploring the unification of flow, depth, and egomotion.",
            "In order to feed event data into a neural network, we divide event sequence E ( t s , t e ) into B temporal bins as the channel dimension of an event voxel[34]."
          ],
          "intents": [
            "--",
            "--",
            "--"
          ],
          "cited_paper_info": {
            "title": "Unsupervised Event-Based Learning of Optical Flow, Depth, and Egomotion",
            "abstract": "In this work, we propose a novel framework for unsupervised learning for event cameras that learns motion information from only the event stream. In particular, we propose an input representation of the events in the form of a discretized volume that maintains the temporal distribution of the events, which we pass through a neural network to predict the motion of the events. This motion is used to attempt to remove any motion blur in the event image. We then propose a loss function applied to the motion compensated event image that measures the motion blur in this image. We train two networks with this framework, one to predict optical flow, and one to predict egomotion and depths, and evaluate these networks on the Multi Vehicle Stereo Event Camera dataset, along with qualitative results from a variety of different scenes.",
            "year": 2018,
            "venue": "Computer Vision and Pattern Recognition",
            "authors": [
              {
                "authorId": "3385588",
                "name": "A. Z. Zhu"
              },
              {
                "authorId": "36001694",
                "name": "Liangzhe Yuan"
              },
              {
                "authorId": "20728097",
                "name": "Kenneth Chaney"
              },
              {
                "authorId": "1751586",
                "name": "Kostas Daniilidis"
              }
            ]
          }
        },
        {
          "citedcorpusid": 57573786,
          "isinfluential": false,
          "contexts": [
            "If multi-scale iterations are adopted, the feature extraction network can also output multi-scale features such as 1/4 and 1/16, using a weight-sharing convolution with different strides (similar to TridentNet [50])."
          ],
          "intents": [
            "--"
          ],
          "cited_paper_info": {
            "title": "Scale-Aware Trident Networks for Object Detection",
            "abstract": "Scale variation is one of the key challenges in object detection. In this work, we ﬁrst present a controlled experiment to investigate the effect of receptive ﬁelds for scale variation in object detection. Based on the ﬁndings from the exploration experiments, we propose a novel Trident Network (TridentNet) aiming to generate scale-speciﬁc feature maps with a uniform representational power. We construct a parallel multi-branch architecture in which each branch shares the same transformation parameters but with different receptive ﬁelds. Then, we adopt a scale-aware training scheme to specialize each branch by sampling object instances of proper scales for training. As a bonus, a fast approximation version of TridentNet could achieve signiﬁcant improvements without any additional parameters and computational cost compared with the vanilla detector. On the COCO dataset, our TridentNet with ResNet-101 backbone achieves state-of-the-art single-model results of 48.4 mAP. Codes are available at https://git.io/fj5vR.",
            "year": 2019,
            "venue": "IEEE International Conference on Computer Vision",
            "authors": [
              {
                "authorId": "2366569300",
                "name": "Yanghao Li"
              },
              {
                "authorId": "2798406",
                "name": "Yuntao Chen"
              },
              {
                "authorId": "48246959",
                "name": "Naiyan Wang"
              },
              {
                "authorId": "145274329",
                "name": "Zhaoxiang Zhang"
              }
            ]
          }
        },
        {
          "citedcorpusid": 118684904,
          "isinfluential": false,
          "contexts": [
            "Event camera is a novel neuromorphic visual sensor with the advantages of low latency, high temporal resolution, and high dynamic range [7, 8]."
          ],
          "intents": [
            "--"
          ],
          "cited_paper_info": {
            "title": "Event-Based Vision: A Survey",
            "abstract": "Event cameras are bio-inspired sensors that differ from conventional frame cameras: Instead of capturing images at a fixed rate, they asynchronously measure per-pixel brightness changes, and output a stream of events that encode the time, location and sign of the brightness changes. Event cameras offer attractive properties compared to traditional cameras: high temporal resolution (in the order of $\\mu$μs), very high dynamic range (140 dB versus 60 dB), low power consumption, and high pixel bandwidth (on the order of kHz) resulting in reduced motion blur. Hence, event cameras have a large potential for robotics and computer vision in challenging scenarios for traditional cameras, such as low-latency, high speed, and high dynamic range. However, novel methods are required to process the unconventional output of these sensors in order to unlock their potential. This paper provides a comprehensive overview of the emerging field of event-based vision, with a focus on the applications and the algorithms developed to unlock the outstanding properties of event cameras. We present event cameras from their working principle, the actual sensors that are available and the tasks that they have been used for, from low-level vision (feature detection and tracking, optic flow, etc.) to high-level vision (reconstruction, segmentation, recognition). We also discuss the techniques developed to process events, including learning-based techniques, as well as specialized processors for these novel sensors, such as spiking neural networks. Additionally, we highlight the challenges that remain to be tackled and the opportunities that lie ahead in the search for a more efficient, bio-inspired way for machines to perceive and interact with the world.",
            "year": 2019,
            "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
            "authors": [
              {
                "authorId": "144036711",
                "name": "Guillermo Gallego"
              },
              {
                "authorId": "1694635",
                "name": "T. Delbrück"
              },
              {
                "authorId": "33780923",
                "name": "G. Orchard"
              },
              {
                "authorId": "1897771",
                "name": "C. Bartolozzi"
              },
              {
                "authorId": "1736425",
                "name": "B. Taba"
              },
              {
                "authorId": "1860631",
                "name": "A. Censi"
              },
              {
                "authorId": "2864731",
                "name": "Stefan Leutenegger"
              },
              {
                "authorId": "2052135690",
                "name": "A. Davison"
              },
              {
                "authorId": "3302681",
                "name": "J. Conradt"
              },
              {
                "authorId": "1751586",
                "name": "Kostas Daniilidis"
              },
              {
                "authorId": "2075371",
                "name": "D. Scaramuzza"
              }
            ]
          }
        },
        {
          "citedcorpusid": 189998802,
          "isinfluential": false,
          "contexts": [
            "Based on event cameras, there has been considerable research on computer vision algorithms, such as image reconstruction [9, 10], optical flow estimation [11, 12], depth estimation [13, 14], etc."
          ],
          "intents": [
            "--"
          ],
          "cited_paper_info": {
            "title": "High Speed and High Dynamic Range Video with an Event Camera",
            "abstract": "Event cameras are novel sensors that report brightness changes in the form of a stream of asynchronous “events” instead of intensity frames. They offer significant advantages with respect to conventional cameras: high temporal resolution, high dynamic range, and no motion blur. While the stream of events encodes in principle the complete visual signal, the reconstruction of an intensity image from a stream of events is an ill-posed problem in practice. Existing reconstruction approaches are based on hand-crafted priors and strong assumptions about the imaging process as well as the statistics of natural images. In this work we propose to learn to reconstruct intensity images from event streams directly from data instead of relying on any hand-crafted priors. We propose a novel recurrent network to reconstruct videos from a stream of events, and train it on a large amount of simulated event data. During training we propose to use a perceptual loss to encourage reconstructions to follow natural image statistics. We further extend our approach to synthesize color images from color event streams. Our quantitative experiments show that our network surpasses state-of-the-art reconstruction methods by a large margin in terms of image quality (<inline-formula><tex-math notation=\"LaTeX\">$>\\!20\\%$</tex-math><alternatives><mml:math><mml:mrow><mml:mo>></mml:mo><mml:mspace width=\"-0.166667em\"/><mml:mn>20</mml:mn><mml:mo>%</mml:mo></mml:mrow></mml:math><inline-graphic xlink:href=\"rebecq-ieq1-2963386.gif\"/></alternatives></inline-formula>), while comfortably running in real-time. We show that the network is able to synthesize high framerate videos (<inline-formula><tex-math notation=\"LaTeX\">$>5,000$</tex-math><alternatives><mml:math><mml:mrow><mml:mo>></mml:mo><mml:mn>5</mml:mn><mml:mo>,</mml:mo><mml:mn>000</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href=\"rebecq-ieq2-2963386.gif\"/></alternatives></inline-formula> frames per second) of high-speed phenomena (e.g., a bullet hitting an object) and is able to provide high dynamic range reconstructions in challenging lighting conditions. As an additional contribution, we demonstrate the effectiveness of our reconstructions as an intermediate representation for event data. We show that off-the-shelf computer vision algorithms can be applied to our reconstructions for tasks such as object classification and visual-inertial odometry and that this strategy consistently outperforms algorithms that were specifically designed for event data. We release the reconstruction code, a pre-trained model and the datasets to enable further research.",
            "year": 2019,
            "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
            "authors": [
              {
                "authorId": "3414274",
                "name": "Henri Rebecq"
              },
              {
                "authorId": "2774325",
                "name": "René Ranftl"
              },
              {
                "authorId": "145231047",
                "name": "V. Koltun"
              },
              {
                "authorId": "2075371",
                "name": "D. Scaramuzza"
              }
            ]
          }
        },
        {
          "citedcorpusid": 214667893,
          "isinfluential": false,
          "contexts": [
            "As shown in Figure 4(b), the head is a GRU iterative structure similar to RAFT [26], which generates optimized residuals based on the current optical flow query and the corresponding cost volume.",
            "Then, with the inspiration of RAFT [26], Gehrig et al. [16] introduce correlation volume into the event-based optical flow model."
          ],
          "intents": [
            "--",
            "--"
          ],
          "cited_paper_info": {
            "title": "RAFT: Recurrent All-Pairs Field Transforms for Optical Flow",
            "abstract": "We introduce Recurrent All-Pairs Field Transforms (RAFT), a new deep network architecture for optical flow. RAFT extracts per-pixel features, builds multi-scale 4D correlation volumes for all pairs of pixels, and iteratively updates a flow field through a recurrent unit that performs lookups on the correlation volumes. RAFT achieves state-of-the-art performance on the KITTI and Sintel datasets. In addition, RAFT has strong cross-dataset generalization as well as high efficiency in inference time, training speed, and parameter count.",
            "year": 2020,
            "venue": "European Conference on Computer Vision",
            "authors": [
              {
                "authorId": "8048414",
                "name": "Zachary Teed"
              },
              {
                "authorId": "153302678",
                "name": "Jia Deng"
              }
            ]
          }
        },
        {
          "citedcorpusid": 216036364,
          "isinfluential": false,
          "contexts": [
            "But now deep learning-based methods [39–42] have gradually moved away from this framework."
          ],
          "intents": [
            "--"
          ],
          "cited_paper_info": {
            "title": "AANet: Adaptive Aggregation Network for Efficient Stereo Matching",
            "abstract": "Despite the remarkable progress made by learning based stereo matching algorithms, one key challenge remains unsolved. Current state-of-the-art stereo models are mostly based on costly 3D convolutions, the cubic computational complexity and high memory consumption make it quite expensive to deploy in real-world applications. In this paper, we aim at completely replacing the commonly used 3D convolutions to achieve fast inference speed while maintaining comparable accuracy. To this end, we first propose a sparse points based intra-scale cost aggregation method to alleviate the well-known edge-fattening issue at disparity discontinuities. Further, we approximate traditional cross-scale cost aggregation algorithm with neural network layers to handle large textureless regions. Both modules are simple, lightweight, and complementary, leading to an effective and efficient architecture for cost aggregation. With these two modules, we can not only significantly speed up existing top-performing models (e.g., 41x than GC-Net, 4x than PSMNet and 38x than GA-Net), but also improve the performance of fast stereo models (e.g., StereoNet). We also achieve competitive results on Scene Flow and KITTI datasets while running at 62ms, demonstrating the versatility and high efficiency of the proposed method. Our full framework is available at https://github.com/haofeixu/aanet.",
            "year": 2020,
            "venue": "Computer Vision and Pattern Recognition",
            "authors": [
              {
                "authorId": "2108835907",
                "name": "Haofei Xu"
              },
              {
                "authorId": "2108487442",
                "name": "Juyong Zhang"
              }
            ]
          }
        },
        {
          "citedcorpusid": 219037720,
          "isinfluential": false,
          "contexts": [
            "For the former, the purpose of vision algorithms is to understand motion in 3D scenes, such as optical flow [15–17], object tracking [18]."
          ],
          "intents": [
            "--"
          ],
          "cited_paper_info": {
            "title": "Object tracking on event cameras with offline-online learning",
            "abstract": "Compared with conventional image sensors, event cameras have been attracting attention thanks to their potential in environments under fast motion and high dynamic range (HDR). To tackle the lost-track issue due to fast illumination changes under HDR scene such as tunnels, an object tracking framework has been presented based on event count images from an event camera. The framework contains an offline-trained detector and an online-trained tracker which complement each other: The detector benefits from pre-labelled data during training, but may have false or missing detections; the tracker provides persistent results for each initialised object but may suffer from drifting issues or even failures. Besides, process and measurement equations have been modelled, and a Kalman fusion scheme has been proposed to incorporate measurements from the detector and the tracker. Self-initialisation and track maintenance in the fusion scheme ensure autonomous real-time tracking without user intervene. With self-collected event data in urban driving scenarios, experiments have been conducted to show the performance of the proposed framework and the fusion scheme.",
            "year": 2020,
            "venue": "CAAI Transactions on Intelligence Technology",
            "authors": [
              {
                "authorId": "2052890483",
                "name": "Rui Jiang"
              },
              {
                "authorId": "2068172",
                "name": "Xiaozheng Mou"
              },
              {
                "authorId": "2072684227",
                "name": "Shunshun Shi"
              },
              {
                "authorId": "2120777",
                "name": "Yueyin Zhou"
              },
              {
                "authorId": "49110724",
                "name": "Qinyi Wang"
              },
              {
                "authorId": "2069775764",
                "name": "Mengyuan Dong"
              },
              {
                "authorId": "50358115",
                "name": "Shoushun Chen"
              }
            ]
          }
        },
        {
          "citedcorpusid": 239049376,
          "isinfluential": true,
          "contexts": [
            "We assess the performance of several state-of-the-art methods for event-based optical flow estimation, including E-RAFT [16], TMA [17] and IDNet [54].",
            "For optical flow, we use the l 1 loss [16].",
            "Compared with ERAFT [16] and TMA [17], our model can capture and predict more detailed optical flow edges.",
            "For ERAFT[16] and IDNet[54] with the warm start strategy, our model also has advantages on 1PE and is competitive on EPE.",
            "Then, with the inspiration of RAFT [26], Gehrig et al. [16] introduce correlation volume into the event-based optical flow model.",
            "…matching is that the corresponding pixels must exist in both reference and target inputs, so matching may fail for occluded and out-of-boundary For each two columns, we display reference images and event frames, and compare our method with the state-of-the-art baseline E-RAFT [16] and TMA [17]."
          ],
          "intents": [
            "--",
            "--",
            "--",
            "--",
            "--",
            "--"
          ],
          "cited_paper_info": {
            "title": "E-RAFT: Dense Optical Flow from Event Cameras",
            "abstract": "We propose to incorporate feature correlation and sequential processing into dense optical flow estimation from event cameras. Modern frame-based optical flow methods heavily rely on matching costs computed from feature correlation. In contrast, there exists no optical flow method for event cameras that explicitly computes matching costs. Instead, learning-based approaches using events usually resort to the U-Net architecture to estimate optical flow sparsely. Our key finding is that the introduction of correlation features significantly improves results compared to previous methods that solely rely on convolution layers. Compared to the state-of-the-art, our proposed approach computes dense optical flow and reduces the end-point error by 23% on MVSEC. Furthermore, we show that all existing optical flow methods developed so far for event cameras have been evaluated on datasets with very small displacement fields with maximum flow magnitude of 10 pixels. Based on this observation, we introduce a new real-world dataset that exhibits displacement fields with magnitudes up to 210 pixels and 3 times higher camera resolution. Our proposed approach reduces the end-point error on this dataset by 66%.",
            "year": 2021,
            "venue": "International Conference on 3D Vision",
            "authors": [
              {
                "authorId": "8329387",
                "name": "Mathias Gehrig"
              },
              {
                "authorId": "2124709306",
                "name": "Mario Millhäusler"
              },
              {
                "authorId": "51152279",
                "name": "Daniel Gehrig"
              },
              {
                "authorId": "2075371",
                "name": "D. Scaramuzza"
              }
            ]
          }
        },
        {
          "citedcorpusid": 244306440,
          "isinfluential": false,
          "contexts": [
            "We also select several state-of-the-art methods for comparison, including DDES [19], E-stereo [55] and Concentration Net [21]."
          ],
          "intents": [
            "--"
          ],
          "cited_paper_info": {
            "title": "Event-Intensity Stereo: Estimating Depth by the Best of Both Worlds",
            "abstract": "Event cameras can report scene movements as an asynchronous stream of data called the events. Unlike traditional cameras, event cameras have very low latency (microseconds vs milliseconds) very high dynamic range (140 dB vs 60 dB), and low power consumption, as they report changes of a scene and not a complete frame. As they re- port per pixel feature-like events and not the whole intensity frame they are immune to motion blur. However, event cameras require movement between the scene and camera to fire events, i.e., they have no output when the scene is relatively static. Traditional cameras, however, report the whole frame of pixels at once in fixed intervals but have lower dynamic range and are prone to motion blur in case of rapid movements. We get the best from both worlds and use events and intensity images together in our complementary design and estimate dense disparity from this combination. The proposed end-to-end design combines events and images in a sequential manner and correlates them to estimate dense depth values. Our various experimental settings in real-world and simulated scenarios exploit the superiority of our method in predicting accurate depth values with fine details. We further extend our method to extreme cases of missing the left or right event or stereo pair and also investigate stereo depth estimation with inconsistent dynamic ranges or event thresholds on the left and right pairs.",
            "year": 2021,
            "venue": "IEEE International Conference on Computer Vision",
            "authors": [
              {
                "authorId": "114141661",
                "name": "Mohammad Mostafavi"
              },
              {
                "authorId": "51182421",
                "name": "Kuk-Jin Yoon"
              },
              {
                "authorId": "2119579051",
                "name": "Jonghyun Choi"
              }
            ]
          }
        },
        {
          "citedcorpusid": 244709323,
          "isinfluential": true,
          "contexts": [
            "As shown in Figure 4(a), we choose Transformer as the main architecture for feature enhancement, due to its adeptness in capturing the intricate interplay between two sets using the attention mechanism [22].",
            "For similarity matching methods [23, 22], the key to optical flow estimation and stereo matching all lies in finding dense correspondence between reference and target inputs.",
            "Therefore, we have added an additional self-attention layer that can propagate effective matching flows to occluded or out-of-boundary regions [22, 23].",
            "Inspired by global matching strategy [22, 23], we extract motion and the 3D position of the world from a pair of event streams via feature similarity matching."
          ],
          "intents": [
            "--",
            "--",
            "--",
            "--"
          ],
          "cited_paper_info": {
            "title": "GMFlow: Learning Optical Flow via Global Matching",
            "abstract": "Learning-based optical flow estimation has been dominated with the pipeline of cost volume with convolutions for flow regression, which is inherently limited to local correlations and thus is hard to address the long-standing challenge of large displacements. To alleviate this, the state-of-the-art framework RAFT gradually improves its prediction quality by using a large number of iterative refinements, achieving remarkable performance but introducing linearly increasing inference time. To enable both high accuracy and efficiency, we completely revamp the dominant flow regression pipeline by reformulating optical flow as a global matching problem, which identifies the correspondences by directly comparing feature similarities. Specifically, we propose a GMFlow framework, which consists of three main components: a customized Transformer for feature enhancement, a correlation and softmax layer for global feature matching, and a self-attention layer for flow propagation. We further introduce a refinement step that reuses GMFlow at higher feature resolution for residual flow prediction. Our new framework outperforms 31-refinements RAFT on the challenging Sintel benchmark, while using only one refinement and running faster, suggesting a new paradigm for accurate and efficient optical flow estimation. Code is available at https://github.com/haofeixu/gmflow.",
            "year": 2021,
            "venue": "Computer Vision and Pattern Recognition",
            "authors": [
              {
                "authorId": "2108835907",
                "name": "Haofei Xu"
              },
              {
                "authorId": "1519066969",
                "name": "Jing Zhang"
              },
              {
                "authorId": "1688642",
                "name": "Jianfei Cai"
              },
              {
                "authorId": "2307081678",
                "name": "Hamid Rezatofighi"
              },
              {
                "authorId": "143719920",
                "name": "D. Tao"
              }
            ]
          }
        },
        {
          "citedcorpusid": 250602271,
          "isinfluential": false,
          "contexts": [
            "Then, Zhang et al. [20] propose an abstracted model, discrete time convolution (DTC) to encode high dimensional spatial-temporal data."
          ],
          "intents": [
            "--"
          ],
          "cited_paper_info": {
            "title": "Discrete time convolution for fast event-based stereo",
            "abstract": "Inspired by biological retina, dynamical vision sensor transmits events of instantaneous changes of pixel intensity, giving it a series of advantages over traditional frame-based camera, such as high dynamical range, high temporal resolution and low power consumption. However, extracting information from highly asynchronous event data is a challenging task. Inspired by continuous dynamics of biological neuron models, we propose a novel encoding method for sparse events-continuous time convolution (CTC)-which learns to model the spatial feature of the data with intrinsic dynamics. Adopting channel-wise parameterization, temporal dynamics of the model is synchronized on the same feature map and diverges across different ones, enabling it to embed data in a variety of temporal scales. Abstracted from CTC, we further develop discrete time convolution (DTC) which accelerates the process with lower computational cost. We apply these methods to event-based multi- view stereo matching where they surpass state-of-the-art methods on benchmark criteria of the MVSEC dataset. Spatially sparse event data often leads to inaccurate estimation of edges and local contours. To address this problem, we propose a dual-path architecture in which the feature map is complemented by underlying edge information from original events extracted with spatially-adaptive denormal-ization. We demonstrate the superiority of our model in terms of speed (up to 110 FPS), accuracy and robustness, showing a great potential for real-time fast depth estimation. Finally, we perform experiments on the recent DSEC dataset to demonstrate the general usage of our model.",
            "year": 2022,
            "venue": "Computer Vision and Pattern Recognition",
            "authors": [
              {
                "authorId": "2152981298",
                "name": "Kai Zhang"
              },
              {
                "authorId": "2122908748",
                "name": "Kaiwei Che"
              },
              {
                "authorId": "2155240940",
                "name": "Jianguo Zhang"
              },
              {
                "authorId": "2163076178",
                "name": "Jie Cheng"
              },
              {
                "authorId": "2144371654",
                "name": "Ziyang Zhang"
              },
              {
                "authorId": "47747957",
                "name": "Qinghai Guo"
              },
              {
                "authorId": "48205902",
                "name": "Luziwei Leng"
              }
            ]
          }
        },
        {
          "citedcorpusid": 257505349,
          "isinfluential": false,
          "contexts": [
            "Recently, there have been several researches on network structures [35, 17] and large-scale datasets [36, 37] to further improve model performance."
          ],
          "intents": [
            "--"
          ],
          "cited_paper_info": {
            "title": "BlinkFlow: A Dataset to Push the Limits of Event-Based Optical Flow Estimation",
            "abstract": "Event cameras provide high temporal precision, low data rates, and high dynamic range visual perception, which are well-suited for optical flow estimation. While data-driven optical flow estimation has obtained great success in RGB cameras, its generalization performance is seriously hindered in event cameras mainly due to the limited and biased training data. In this paper, we present a novel simulator, BlinkSim, for the fast generation of large-scale data for event-based optical flow. BlinkSim incorporates a configurable rendering engine alongside an event simulation suite. By leveraging the wealth of current 3D assets, the rendering engine enables us to automatically build up thousands of scenes with different objects, textures, and motion patterns and render very high-frequency images for realistic event data simulation. Based on BlinkSim, we construct a large training dataset and evaluation benchmark BlinkFlow that contains sufficient, diversiform, and challenging event data with optical flow ground truth. Experiments show that BlinkFlow improves the generalization performance of state-of-the-art methods by more than 40% on average and up to 90%. Moreover, we further propose an Event-based optical Flow transFormer (E-FlowFormer) architecture. Powered by our BlinkFlow, E-FlowFormer outperforms the SOTA methods by up to 91% on the MVSEC dataset and 14% on the DSEC dataset and presents the best generalization performance. The source code and data are available at https://zju3dv.github.io/blinkflow/.",
            "year": 2023,
            "venue": "IEEE/RJS International Conference on Intelligent RObots and Systems",
            "authors": [
              {
                "authorId": "2110512150",
                "name": "Yijin Li"
              },
              {
                "authorId": "1830448350",
                "name": "Zhaoyang Huang"
              },
              {
                "authorId": "2116572341",
                "name": "Shuo Chen"
              },
              {
                "authorId": "2119204728",
                "name": "Xiaoyu Shi"
              },
              {
                "authorId": "49404547",
                "name": "Hongsheng Li"
              },
              {
                "authorId": "1679542",
                "name": "H. Bao"
              },
              {
                "authorId": "1813796",
                "name": "Zhaopeng Cui"
              },
              {
                "authorId": "32162658",
                "name": "Guofeng Zhang"
              }
            ]
          }
        },
        {
          "citedcorpusid": 257631432,
          "isinfluential": false,
          "contexts": [
            "For ERAFT[16] and IDNet[54] with the warm start strategy, our model also has advantages on 1PE and is competitive on EPE.",
            "We assess the performance of several state-of-the-art methods for event-based optical flow estimation, including E-RAFT [16], TMA [17] and IDNet [54]."
          ],
          "intents": [
            "--",
            "--"
          ],
          "cited_paper_info": {
            "title": "Lightweight Event-based Optical Flow Estimation via Iterative Deblurring",
            "abstract": "Inspired by frame-based methods, state-of-the-art event-based optical flow networks rely on the explicit construction of correlation volumes, which are expensive to compute and store, rendering them unsuitable for robotic applications with limited compute and energy budget. Moreover, correlation volumes scale poorly with resolution, prohibiting them from estimating high-resolution flow. We observe that the spatiotemporally continuous traces of events provide a natural search direction for seeking pixel correspondences, obviating the need to rely on gradients of explicit correlation volumes as such search directions. We introduce IDNet (Iterative Deblurring Network), a lightweight yet high-performing event-based optical flow network directly estimating flow from event traces without using correlation volumes. We further propose two iterative update schemes: \"ID\" which iterates over the same batch of events, and \"TID\" which iterates over time with streaming events in an online fashion. Our top-performing model (ID) sets a new state of the art on DSEC benchmark. Meanwhile, the base model (TID) is competitive with prior arts while using 80% fewer parameters, consuming 20x less memory footprint and running 40% faster on the NVidia Jetson Xavier NX. Furthermore, the TID scheme is even more efficient offering an additional 5x faster inference speed and 8 ms ultra-low latency at the cost of only a 9% performance drop, making it the only model among current literature capable of real-time operation while maintaining decent performance.Code: https://github.com/tudelft/idnet.",
            "year": 2022,
            "venue": "IEEE International Conference on Robotics and Automation",
            "authors": [
              {
                "authorId": "39200632",
                "name": "Yilun Wu"
              },
              {
                "authorId": "1411256751",
                "name": "F. Paredes-Vallés"
              },
              {
                "authorId": "102110243",
                "name": "G. D. Croon"
              }
            ]
          }
        },
        {
          "citedcorpusid": 262638843,
          "isinfluential": false,
          "contexts": [
            "We also select several state-of-the-art methods for comparison, including DDES [19], E-stereo [55] and Concentration Net [21].",
            "For the latter, the objective of vision tasks is to comprehend the structure of every 3D object, such as stereo matching [19–21], depth estimation [14].",
            "DDES [19] is the first learning-based stereo method for event cameras, which proposes a learnable representation for event sequences."
          ],
          "intents": [
            "--",
            "--",
            "--"
          ],
          "cited_paper_info": {
            "title": "Learning an Event Sequence Embedding for Dense Event-Based Deep Stereo",
            "abstract": "Today, a frame-based camera is the sensor of choice for machine vision applications. However, these cameras, originally developed for acquisition of static images rather than for sensing of dynamic uncontrolled visual environments, suffer from high power consumption, data rate, latency and low dynamic range. An event-based image sensor addresses these drawbacks by mimicking a biological retina. Instead of measuring the intensity of every pixel in a fixed time-interval, it reports events of significant pixel intensity changes. Every such event is represented by its position, sign of change, and timestamp, accurate to the microsecond. Asynchronous event sequences require special handling, since traditional algorithms work only with synchronous, spatially gridded data. To address this problem we introduce a new module for event sequence embedding, for use in difference applications. The module builds a representation of an event sequence by firstly aggregating information locally across time, using a novel fully-connected layer for an irregularly sampled continuous domain, and then across discrete spatial domain. Based on this module, we design a deep learning-based stereo method for event-based cameras. The proposed method is the first learning-based stereo method for an event-based camera and the only method that produces dense results. We show that large performance increases on the Multi Vehicle Stereo Event Camera Dataset (MVSEC), which became the standard set for benchmarking of event-based stereo methods.",
            "year": 2019,
            "venue": "IEEE International Conference on Computer Vision",
            "authors": [
              {
                "authorId": "1823725",
                "name": "S. Tulyakov"
              },
              {
                "authorId": "2721983",
                "name": "F. Fleuret"
              },
              {
                "authorId": "40519282",
                "name": "Martin Kiefel"
              },
              {
                "authorId": "2871555",
                "name": "Peter Gehler"
              },
              {
                "authorId": "2058954687",
                "name": "Michael Hirsch"
              }
            ]
          }
        },
        {
          "citedcorpusid": 269525186,
          "isinfluential": false,
          "contexts": [
            "Gallego et al. [12, 47] have proposed a unifying framework with contrast maximization (CM) strategy, whose main idea is to find the point trajectories on the image plane that are best aligned with the event data."
          ],
          "intents": [
            "--"
          ],
          "cited_paper_info": {
            "title": "Secrets of Event-Based Optical Flow, Depth and Ego-Motion Estimation by Contrast Maximization",
            "abstract": "Event cameras respond to scene dynamics and provide signals naturally suitable for motion estimation with advantages, such as high dynamic range. The emerging field of event-based vision motivates a revisit of fundamental computer vision tasks related to motion, such as optical flow and depth estimation. However, state-of-the-art event-based optical flow methods tend to originate in frame-based deep-learning methods, which require several adaptations (data conversion, loss function, etc.) as they have very different properties. We develop a principled method to extend the Contrast Maximization framework to estimate dense optical flow, depth, and ego-motion from events alone. The proposed method sensibly models the space-time properties of event data and tackles the event alignment problem. It designs the objective function to prevent overfitting, deals better with occlusions, and improves convergence using a multi-scale approach. With these key elements, our method ranks first among unsupervised methods on the MVSEC benchmark and is competitive on the DSEC benchmark. Moreover, it allows us to simultaneously estimate dense depth and ego-motion, exposes the limitations of current flow benchmarks, and produces remarkable results when it is transferred to unsupervised learning settings. Along with various downstream applications shown, we hope the proposed method becomes a cornerstone on event-based motion-related tasks.",
            "year": 2024,
            "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
            "authors": [
              {
                "authorId": "2066324243",
                "name": "Shintaro Shiba"
              },
              {
                "authorId": "2299517127",
                "name": "Yannick Klose"
              },
              {
                "authorId": "2267612969",
                "name": "Yoshimitsu Aoki"
              },
              {
                "authorId": "144036711",
                "name": "Guillermo Gallego"
              }
            ]
          }
        }
      ]
    },
    "267659950": {
      "citing_paper_info": {
        "title": "An Event-based Stereo 3D Mapping and Tracking Pipeline for Autonomous Vehicles",
        "abstract": "Event cameras are bio-inspired, motion-activated sensors which generate asynchronous events instead of intensity images at a fixed rate. These sensors have been shown to outperform traditional frame-based cameras by large margins, in case of high-speed motions and scenes with high dynamic range. Next-generation intelligent vehicles are expected to greatly benefit from these novel cameras, especially in adverse lighting conditions, and their potential is still largely untapped. In the last decade, the continuous stream of events produced by an event camera has been exploited in numerous 3D perception tasks (depth estimation, 6-DoF tracking, visual-inertial odometry, etc.). In this paper, we propose an event-based stereo pipeline for simultaneous 3D mapping and tracking. The mapping module relies on DSI (Disparity Space Image) fusion, and the tracking module makes use of time surfaces as anisotropic distance fields, to estimate the pose of the stereo camera. Numerical experiments with a publicly-available event dataset recorded by a car in different urban environments, show the effectiveness of the proposed architecture.",
        "year": 2023,
        "venue": "2023 IEEE 26th International Conference on Intelligent Transportation Systems (ITSC)",
        "authors": [
          {
            "authorId": "2284078734",
            "name": "A. E. Moudni"
          },
          {
            "authorId": "2284078683",
            "name": "Fabio Morbidi"
          },
          {
            "authorId": "2284077748",
            "name": "Sebastien Kramm"
          },
          {
            "authorId": "2737160",
            "name": "Rémi Boutteau"
          }
        ]
      },
      "citation_summary": {
        "citation_count": 10,
        "unique_cited_count": 10,
        "influential_count": 0,
        "detailed_records_count": 10
      },
      "cited_papers": [
        "11008141",
        "186689463",
        "248227281",
        "206594738",
        "49864158",
        "65040501",
        "26324573",
        "251765050",
        "212837417",
        "6178869"
      ],
      "citation_details": [
        {
          "citedcorpusid": 6178869,
          "isinfluential": false,
          "contexts": [
            "Probabilistic filters [27], [28] have been shown to be more suitable for this specific task (the estimated pose is updated every time a new event is triggered), while still preserving the asynchronous nature of the incoming stream of events."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Event-Based, 6-DOF Camera Tracking from Photometric Depth Maps",
            "abstract": "Event cameras are bio-inspired vision sensors that output pixel-level brightness changes instead of standard intensity frames. These cameras do not suffer from motion blur and have a very high dynamic range, which enables them to provide reliable visual information during high-speed motions or in scenes characterized by high dynamic range. These features, along with a very low power consumption, make event cameras an ideal complement to standard cameras for VR/AR and video game applications. With these applications in mind, this paper tackles the problem of accurate, low-latency tracking of an event camera from an existing photometric depth map (i.e., intensity plus depth information) built via classic dense reconstruction pipelines. Our approach tracks the 6-DOF pose of the event camera upon the arrival of each event, thus virtually eliminating latency. We successfully evaluate the method in both indoor and outdoor scenes and show that—because of the technological advantages of the event camera—our pipeline works in scenes characterized by high-speed motion, which are still inaccessible to standard cameras.",
            "year": 2016,
            "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
            "authors": [
              {
                "authorId": "144036711",
                "name": "Guillermo Gallego"
              },
              {
                "authorId": "31929293",
                "name": "Jon E. A. Lund"
              },
              {
                "authorId": "144578041",
                "name": "Elias Mueggler"
              },
              {
                "authorId": "3414274",
                "name": "Henri Rebecq"
              },
              {
                "authorId": "5548576",
                "name": "T. Delbruck"
              },
              {
                "authorId": "2075371",
                "name": "D. Scaramuzza"
              }
            ]
          }
        },
        {
          "citedcorpusid": 11008141,
          "isinfluential": false,
          "contexts": [
            "In classical computer vision, the space-sweep approach [31] allows to solve the multi-view stereo (MVS) problem, and a 3D reconstruction can be obtained without the need for matching or data association across cameras."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "A space-sweep approach to true multi-image matching",
            "abstract": "",
            "year": 1996,
            "venue": "Proceedings CVPR IEEE Computer Society Conference on Computer Vision and Pattern Recognition",
            "authors": [
              {
                "authorId": "143980462",
                "name": "R. Collins"
              }
            ]
          }
        },
        {
          "citedcorpusid": 26324573,
          "isinfluential": false,
          "contexts": [
            "As far as the camera tracking problem is concerned, a variant of the Kalman filter has been used in [14] to predict the 6-DoF camera motion."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Real-Time 3D Reconstruction and 6-DoF Tracking with an Event Camera",
            "abstract": "",
            "year": 2016,
            "venue": "European Conference on Computer Vision",
            "authors": [
              {
                "authorId": "3227772",
                "name": "Hanme Kim"
              },
              {
                "authorId": "2864731",
                "name": "Stefan Leutenegger"
              },
              {
                "authorId": "2052135690",
                "name": "A. Davison"
              }
            ]
          }
        },
        {
          "citedcorpusid": 49864158,
          "isinfluential": false,
          "contexts": [
            "In contrast, direct methods process single events, and satisfactory results, in terms of accuracy, are reported in [24]: however, the existing event-feature extractors and trackers [25], [26] cannot be easily adapted to a visual odometry problem, since they suffer from poor accuracy and stability."
          ],
          "intents": [
            "['result']"
          ],
          "cited_paper_info": {
            "title": "Asynchronous Corner Detection and Tracking for Event Cameras in Real Time",
            "abstract": "The recent emergence of bioinspired event cameras has opened up exciting new possibilities in high-frequency tracking, bringing robustness to common problems in traditional vision, such as lighting changes and motion blur. In order to leverage these attractive attributes of the event cameras, research has been focusing on understanding how to process their unusual output: an asynchronous stream of events. With the majority of existing techniques discretizing the event-stream essentially forming frames of events grouped according to their timestamp, we are still to exploit the power of these cameras. In this spirit, this letter proposes a new, purely event-based corner detector, and a novel corner tracker, demonstrating that it is possible to detect corners and track them directly on the event stream in real time. Evaluation on benchmarking datasets reveals a significant boost in the number of detected corners and the repeatability of such detections over the state of the art even in challenging scenarios with the proposed approach while enabling more than a 4$\\times$ speed-up when compared to the most efficient algorithm in the literature. The proposed pipeline detects and tracks corners at a rate of more than 7.5 million events per second, promising great impact in high-speed applications.",
            "year": 2018,
            "venue": "IEEE Robotics and Automation Letters",
            "authors": [
              {
                "authorId": "7278610",
                "name": "Ignacio Alzugaray"
              },
              {
                "authorId": "1885768",
                "name": "M. Chli"
              }
            ]
          }
        },
        {
          "citedcorpusid": 65040501,
          "isinfluential": false,
          "contexts": [
            "Other methods rely on data association [11], [12] (matching to calculate the disparity), or are correspondence-free, but the knowledge of camera’s trajectory is necessary to estimate the depth by performing a re-projection into a reference view [13].",
            "Belief propagation [11] and event-driven semi-global matching (ESGM) [12] are two methods that have been used to minimize such a cost function."
          ],
          "intents": [
            "['methodology']",
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Event-based stereo matching using semiglobal matching",
            "abstract": "In this article, we focus on the problem of depth estimation from a stereo pair of event-based sensors. These sensors asynchronously capture pixel-level brightness changes information (events) instead of standard intensity images at a specified frame rate. So, these sensors provide sparse data at low latency and high temporal resolution over a wide intrascene dynamic range. However, new asynchronous, event-based processing algorithms are required to process the event streams. We propose a fully event-based stereo three-dimensional depth estimation algorithm inspired by semiglobal matching. Our algorithm considers the smoothness constraints between the nearby events to remove the ambiguous and wrong matches when only using the properties of a single event or local features. Experimental validation and comparison with several state-of-the-art, event-based stereo matching methods are provided on five different scenes of event-based stereo data sets. The results show that our method can operate well in an event-driven way and has higher estimation accuracy.",
            "year": 2018,
            "venue": "",
            "authors": [
              {
                "authorId": "47661053",
                "name": "Zhen Xie"
              },
              {
                "authorId": "1739956171",
                "name": "Jianhua Zhang"
              },
              {
                "authorId": "2108238188",
                "name": "Pengfei Wang"
              }
            ]
          }
        },
        {
          "citedcorpusid": 186689463,
          "isinfluential": false,
          "contexts": [
            "This optimization problem is solved using the Lucas-Kanade method [29]."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Lucas-Kanade 20 Years On: A Unifying Framework",
            "abstract": "",
            "year": 2004,
            "venue": "International Journal of Computer Vision",
            "authors": [
              {
                "authorId": "145347688",
                "name": "Simon Baker"
              },
              {
                "authorId": "83165362",
                "name": "Iain Matthews"
              }
            ]
          }
        },
        {
          "citedcorpusid": 206594738,
          "isinfluential": false,
          "contexts": [
            "The edges in the depth maps generated by our method are sharper, which makes the detection (e.g., by an off-the-shelf deep learning algorithm such as YOLO [32]) of different objects in the scene (traffic light, tree, etc.), easier."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "You Only Look Once: Unified, Real-Time Object Detection",
            "abstract": "We present YOLO, a new approach to object detection. Prior work on object detection repurposes classifiers to perform detection. Instead, we frame object detection as a regression problem to spatially separated bounding boxes and associated class probabilities. A single neural network predicts bounding boxes and class probabilities directly from full images in one evaluation. Since the whole detection pipeline is a single network, it can be optimized end-to-end directly on detection performance. Our unified architecture is extremely fast. Our base YOLO model processes images in real-time at 45 frames per second. A smaller version of the network, Fast YOLO, processes an astounding 155 frames per second while still achieving double the mAP of other real-time detectors. Compared to state-of-the-art detection systems, YOLO makes more localization errors but is less likely to predict false positives on background. Finally, YOLO learns very general representations of objects. It outperforms other detection methods, including DPM and R-CNN, when generalizing from natural images to other domains like artwork.",
            "year": 2015,
            "venue": "Computer Vision and Pattern Recognition",
            "authors": [
              {
                "authorId": "40497777",
                "name": "Joseph Redmon"
              },
              {
                "authorId": "2038685",
                "name": "S. Divvala"
              },
              {
                "authorId": "2983898",
                "name": "Ross B. Girshick"
              },
              {
                "authorId": "143787583",
                "name": "Ali Farhadi"
              }
            ]
          }
        },
        {
          "citedcorpusid": 212837417,
          "isinfluential": false,
          "contexts": [
            "Modern cars are equipped with a suite of advanced sensors, such as multiple color cameras (8 cameras in Tesla Model Y), radars [2], lidars [3], RGB-D cameras [4], and, more recently, event cameras [5]."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "A RGB-D Based Real-Time Multiple Object Detection and Ranging System for Autonomous Driving",
            "abstract": "Real-time object detection and ranging of multiple objects on the road are the essential tasks in the field of autonomous driving. In this paper, we introduce a system for simultaneous detection and ranging of vehicles, people, non-motor vehicles and lanes based on RGB-D images. Among them, the detection of vehicles, people and non-motor vehicles belongs to general detection task and the lane detection belongs to segmentation task. In order to improve the accuracy and speed, we use two networks to complete these two tasks. We propose a real-time synchronization method with multi-GPU, which achieves separate training and simultaneous detection of lane detectior and vehicle, people and non-motor vehicle detector. We also propose a center-selective ranging module based on binocular ranging technology to distance the detected object. The system reaches nearly 15 FPS with four 1080Ti GPUs. We construct datasets about these problems including daytime and night in which the system achieves high accuracy. A real-time test of the system on the streets of Tianjin, China has been conducted by us, it has proved that the system can be applied to actual driving.",
            "year": 2020,
            "venue": "IEEE Sensors Journal",
            "authors": [
              {
                "authorId": "3021550",
                "name": "Jiachen Yang"
              },
              {
                "authorId": "2108755778",
                "name": "Chenguang Wang"
              },
              {
                "authorId": "2108868841",
                "name": "Huihui Wang"
              },
              {
                "authorId": "2146262616",
                "name": "Qiang Li"
              }
            ]
          }
        },
        {
          "citedcorpusid": 248227281,
          "isinfluential": false,
          "contexts": [
            "Event cameras are often combined with other incumbent sensors for more accurate and robust pose estimation: for example, an inertial measurement unit (IMU) for visual-inertial odometry [21], or a frame-based camera [22]."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Event-aided Direct Sparse Odometry",
            "abstract": "We introduce EDS, a direct monocular visual odometry using events and frames. Our algorithm leverages the event generation model to track the camera motion in the blind time between frames. The method formulates a direct probabilistic approach of observed brightness increments. Per-pixel brightness increments are predicted using a sparse number of selected 3D points and are compared to the events via the brightness increment error to estimate camera motion. The method recovers a semi-dense 3D map using photometric bundle adjustment. EDS is the first method to perform 6-DOF VO using events and frames with a direct approach. By design it overcomes the problem of changing appearance in indirect methods. Our results outperform all previous event-based odometry solutions. We also show that, for a target error performance, EDS can work at lower frame rates than state-of-the-art frame-based VO solutions. This opens the door to low-power motion-tracking applications where frames are sparingly triggered “on demand” and our method tracks the motion in between. We release code and datasets to the public.",
            "year": 2022,
            "venue": "Computer Vision and Pattern Recognition",
            "authors": [
              {
                "authorId": "2065112865",
                "name": "Javier Hidalgo-Carri'o"
              },
              {
                "authorId": "144036711",
                "name": "Guillermo Gallego"
              },
              {
                "authorId": "2075371",
                "name": "D. Scaramuzza"
              }
            ]
          }
        },
        {
          "citedcorpusid": 251765050,
          "isinfluential": false,
          "contexts": [
            "In the near future, we are going to build our own multi-modal dataset to test the proposed pipeline in different traffic and weather conditions, and we would like to fuse the event data with measurements coming from other on-board sensors (e.g., lidar [8], [33], RGB camera, IMU)."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "L2E: Lasers to Events for 6-DoF Extrinsic Calibration of Lidars and Event Cameras",
            "abstract": "As neuromorphic technology is maturing, its application to robotics and autonomous vehicle systems has become an area of active research. In particular, event cameras have emerged as a compelling alternative to frame-based cameras in low-power and latency-demanding applications. To enable event cameras to operate alongside staple sensors like lidar in perception tasks, we propose a direct, temporally-decoupled extrinsic calibration method between event cameras and lidars. The high dynamic range, high temporal resolution, and low-latency operation of event cameras are exploited to directly register lidar laser returns, allowing information-based correlation methods to optimize for the 6- DoF extrinsic calibration between the two sensors. This paper presents the first direct calibration method between event cameras and lidars, removing dependencies on frame-based camera intermediaries and/or highly-accurate hand measurements. Code: https://github.com/kev-in-ta/12e",
            "year": 2022,
            "venue": "IEEE International Conference on Robotics and Automation",
            "authors": [
              {
                "authorId": "2174736815",
                "name": "Kevin Ta"
              },
              {
                "authorId": "2158510239",
                "name": "David Brüggemann"
              },
              {
                "authorId": "2175117292",
                "name": "Tim Brödermann"
              },
              {
                "authorId": "7593607",
                "name": "Christos Sakaridis"
              },
              {
                "authorId": "1681236",
                "name": "L. Gool"
              }
            ]
          }
        }
      ]
    },
    "65040501": {
      "citing_paper_info": {
        "title": "Event-based stereo matching using semiglobal matching",
        "abstract": "In this article, we focus on the problem of depth estimation from a stereo pair of event-based sensors. These sensors asynchronously capture pixel-level brightness changes information (events) instead of standard intensity images at a specified frame rate. So, these sensors provide sparse data at low latency and high temporal resolution over a wide intrascene dynamic range. However, new asynchronous, event-based processing algorithms are required to process the event streams. We propose a fully event-based stereo three-dimensional depth estimation algorithm inspired by semiglobal matching. Our algorithm considers the smoothness constraints between the nearby events to remove the ambiguous and wrong matches when only using the properties of a single event or local features. Experimental validation and comparison with several state-of-the-art, event-based stereo matching methods are provided on five different scenes of event-based stereo data sets. The results show that our method can operate well in an event-driven way and has higher estimation accuracy.",
        "year": 2018,
        "venue": "",
        "authors": [
          {
            "authorId": "47661053",
            "name": "Zhen Xie"
          },
          {
            "authorId": "1739956171",
            "name": "Jianhua Zhang"
          },
          {
            "authorId": "2108238188",
            "name": "Pengfei Wang"
          }
        ]
      },
      "citation_summary": {
        "citation_count": 4,
        "unique_cited_count": 4,
        "influential_count": 0,
        "detailed_records_count": 4
      },
      "cited_papers": [
        "21317717",
        "18161107",
        "24007071",
        "37664826"
      ],
      "citation_details": [
        {
          "citedcorpusid": 18161107,
          "isinfluential": false,
          "contexts": [
            "The difference can be defined in different forms, such as absolute gray-scale value, mutual information,(22) and census transform.(23) The penalties depend on the difference to the neighborhood disparities."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Nonlinear Shape Statistics in Mumford-Shah Based Segmentation",
            "abstract": "",
            "year": 2002,
            "venue": "European Conference on Computer Vision",
            "authors": [
              {
                "authorId": "1695302",
                "name": "D. Cremers"
              },
              {
                "authorId": "1749805",
                "name": "Timo Kohlberger"
              },
              {
                "authorId": "1679944",
                "name": "C. Schnörr"
              }
            ]
          }
        },
        {
          "citedcorpusid": 21317717,
          "isinfluential": false,
          "contexts": [
            "The bioinspired vision sensor or called event-based camera mimics retinas to asynchronously generate the response to relative light intensity variations rather than the actual image intensity.(3) Event-based cameras are datadriven sensors that have advantages of low redundancy, high temporal resolution (in the order of microseconds), low latency, and high dynamic range (130 dB compared with 60 dB of standard cameras)."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "A QVGA 143 dB Dynamic Range Frame-Free PWM Image Sensor With Lossless Pixel-Level Video Compression and Time-Domain CDS",
            "abstract": "",
            "year": 2011,
            "venue": "IEEE Journal of Solid-State Circuits",
            "authors": [
              {
                "authorId": "153466606",
                "name": "C. Posch"
              },
              {
                "authorId": "1758423",
                "name": "D. Matolin"
              },
              {
                "authorId": "2509695",
                "name": "R. Wohlgenannt"
              }
            ]
          }
        },
        {
          "citedcorpusid": 24007071,
          "isinfluential": false,
          "contexts": [
            "In this work, DAVIS is used, which is an extension of the dynamic vision sensor (DVS)(7) with higher resolution (240 180) and an additional frame-based intensity readout (not used in this work).(8) Each event is presented with a quadruplet eðt; x; y; pÞ; t is the time stamp, ðx; yÞ is the image coordinates, and p means polarity (ON/OFF) which indicates the luminance increase (ON) or decrease (OFF)."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "A 240 × 180 130 dB 3 µs Latency Global Shutter Spatiotemporal Vision Sensor",
            "abstract": "",
            "year": 2014,
            "venue": "IEEE Journal of Solid-State Circuits",
            "authors": [
              {
                "authorId": "2239977",
                "name": "Christian Brandli"
              },
              {
                "authorId": "144246116",
                "name": "R. Berner"
              },
              {
                "authorId": "1779496",
                "name": "Minhao Yang"
              },
              {
                "authorId": "1704961",
                "name": "Shih-Chii Liu"
              },
              {
                "authorId": "5548576",
                "name": "T. Delbruck"
              }
            ]
          }
        },
        {
          "citedcorpusid": 37664826,
          "isinfluential": false,
          "contexts": [
            "Recently, Eibensteiner et al.15 implemented an event-based stereo matching algorithm in hardware on a field-programmable gate array.",
            "Recently, Eibensteiner et al.(15) implemented an event-based stereo matching algorithm in hardware on a field-programmable gate array."
          ],
          "intents": [
            "--",
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Event-driven stereo vision algorithm based on silicon retina sensors",
            "abstract": "",
            "year": 2017,
            "venue": "International Conference Radioelektronika",
            "authors": [
              {
                "authorId": "2435053",
                "name": "F. Eibensteiner"
              },
              {
                "authorId": "1747978",
                "name": "H. Brachtendorf"
              },
              {
                "authorId": "2662311",
                "name": "J. Scharinger"
              }
            ]
          }
        }
      ]
    },
    "245300947": {
      "citing_paper_info": {
        "title": "Accurate depth estimation from a hybrid event-RGB stereo setup",
        "abstract": "Event-based visual perception is becoming increasingly popular owing to interesting sensor characteristics enabling the handling of difficult conditions such as highly dynamic motion or challenging illumination. The mostly complementary nature of event cameras however still means that best results are achieved if the sensor is paired with a regular frame-based sensor. The present work aims at answering a simple question: Assuming that both cameras do not share a common optical center, is it possible to exploit the hybrid stereo setup's baseline to perform accurate stereo depth estimation? We present a learning based solution to this problem leveraging modern spatio-temporal input representations as well as a novel hybrid pyramid attention module. Results on real data demonstrate competitive performance against pure frame-based stereo alternatives as well as the ability to maintain the advantageous properties of event-based sensors.",
        "year": 2021,
        "venue": "IEEE/RJS International Conference on Intelligent RObots and Systems",
        "authors": [
          {
            "authorId": "2136622420",
            "name": "Yihao Zuo"
          },
          {
            "authorId": "2114843191",
            "name": "Li Cui"
          },
          {
            "authorId": "3428000",
            "name": "Xin-Zhong Peng"
          },
          {
            "authorId": "2110198892",
            "name": "Yanyu Xu"
          },
          {
            "authorId": "1702868",
            "name": "Shenghua Gao"
          },
          {
            "authorId": "2155465459",
            "name": "Xia Wang"
          },
          {
            "authorId": "1727013",
            "name": "L. Kneip"
          }
        ]
      },
      "citation_summary": {
        "citation_count": 18,
        "unique_cited_count": 18,
        "influential_count": 0,
        "detailed_records_count": 18
      },
      "cited_papers": [
        "59222403",
        "131774014",
        "3719281",
        "157060825",
        "1408596",
        "202565789",
        "221846159",
        "119309624",
        "220545977",
        "145916256",
        "1680724",
        "7151414",
        "189998802",
        "27059477",
        "226292034",
        "17693733",
        "121601380",
        "226291858"
      ],
      "citation_details": [
        {
          "citedcorpusid": 1408596,
          "isinfluential": false,
          "contexts": [
            "Novel local event context descriptors are proposed to robustly match events in a temporal-spatial window [26], [27]."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Context-aware event-driven stereo matching",
            "abstract": "",
            "year": 2016,
            "venue": "International Conference on Information Photonics",
            "authors": [
              {
                "authorId": "2780914",
                "name": "Dongqing Zou"
              },
              {
                "authorId": "2075394158",
                "name": "Ping Guo"
              },
              {
                "authorId": "145805403",
                "name": "Qiang Wang"
              },
              {
                "authorId": "2108031895",
                "name": "Xiaotao Wang"
              },
              {
                "authorId": "2055021575",
                "name": "Guangqi Shao"
              },
              {
                "authorId": "2053742370",
                "name": "Feng Shi"
              },
              {
                "authorId": "2118372356",
                "name": "Jia Li"
              },
              {
                "authorId": "13975243",
                "name": "P. Park"
              }
            ]
          }
        },
        {
          "citedcorpusid": 1680724,
          "isinfluential": false,
          "contexts": [
            "COPNET [48], SEMI-DENSE 3D [7], SGM∗ [19][7], FCV F∗ [49][7]",
            "Following [8], we furthermore compare against SGM∗ and FCVF∗, two traditional methods [19][49] adjusted",
            "As can be observed, our proposed method HDES outperforms TSES [30], CopNet [48], Semi-Dense 3D [7], SGM∗ [19][7] and FCV F∗ [49][7]."
          ],
          "intents": [
            "--",
            "['methodology']",
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Fast cost-volume filtering for visual correspondence and beyond",
            "abstract": "",
            "year": 2011,
            "venue": "Computer Vision and Pattern Recognition",
            "authors": [
              {
                "authorId": "2086328",
                "name": "Christoph Rhemann"
              },
              {
                "authorId": "3115485",
                "name": "A. Hosni"
              },
              {
                "authorId": "2873656",
                "name": "M. Bleyer"
              },
              {
                "authorId": "1756036",
                "name": "C. Rother"
              },
              {
                "authorId": "1990797",
                "name": "M. Gelautz"
              }
            ]
          }
        },
        {
          "citedcorpusid": 3719281,
          "isinfluential": false,
          "contexts": [
            "Our architecture is a fully convolutional neural network which is based on the Unet architecture [41]."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "U-Net: Convolutional Networks for Biomedical Image Segmentation",
            "abstract": "There is large consent that successful training of deep networks requires many thousand annotated training samples. In this paper, we present a network and training strategy that relies on the strong use of data augmentation to use the available annotated samples more efficiently. The architecture consists of a contracting path to capture context and a symmetric expanding path that enables precise localization. We show that such a network can be trained end-to-end from very few images and outperforms the prior best method (a sliding-window convolutional network) on the ISBI challenge for segmentation of neuronal structures in electron microscopic stacks. Using the same network trained on transmitted light microscopy images (phase contrast and DIC) we won the ISBI cell tracking challenge 2015 in these categories by a large margin. Moreover, the network is fast. Segmentation of a 512x512 image takes less than a second on a recent GPU. The full implementation (based on Caffe) and the trained networks are available at http://lmb.informatik.uni-freiburg.de/people/ronneber/u-net .",
            "year": 2015,
            "venue": "International Conference on Medical Image Computing and Computer-Assisted Intervention",
            "authors": [
              {
                "authorId": "1737326",
                "name": "O. Ronneberger"
              },
              {
                "authorId": "152702479",
                "name": "P. Fischer"
              },
              {
                "authorId": "1710872",
                "name": "T. Brox"
              }
            ]
          }
        },
        {
          "citedcorpusid": 7151414,
          "isinfluential": false,
          "contexts": [
            "As can be observed, our proposed method HDES outperforms TSES [30], CopNet [48], Semi-Dense 3D [7], SGM ∗ [19][7] and FCVF ∗ [49][7].",
            "R ESULTS FOR HDES ( OUR PROPOSED METHOD ) TSES [30], C OP N ET [48], S EMI -",
            "Our ﬁnal experiments compare our hybrid method HDES against the purely event-based methods DDES [8]—a state-of-the-art learning-based method—and Semi-Dense 3D [7], TSES [30], and CopNet [48], which are more traditional methods."
          ],
          "intents": [
            "['methodology']",
            "['background']",
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Improved Cooperative Stereo Matching for Dynamic Vision Sensors with Ground Truth Evaluation",
            "abstract": "",
            "year": 2017,
            "venue": "2017 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)",
            "authors": [
              {
                "authorId": "47105977",
                "name": "Ewa Piatkowska"
              },
              {
                "authorId": "1824241",
                "name": "J. Kogler"
              },
              {
                "authorId": "1768812",
                "name": "A. Belbachir"
              },
              {
                "authorId": "1990797",
                "name": "M. Gelautz"
              }
            ]
          }
        },
        {
          "citedcorpusid": 17693733,
          "isinfluential": false,
          "contexts": [
            "Rogister et al. [24] ﬁnd matches under the assumption that correlated events are likely to appear within a small time interval and on the same epipolar line."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Asynchronous Event-Based Binocular Stereo Matching",
            "abstract": "",
            "year": 2012,
            "venue": "IEEE Transactions on Neural Networks and Learning Systems",
            "authors": [
              {
                "authorId": "3121605",
                "name": "P. Rogister"
              },
              {
                "authorId": "1750848",
                "name": "R. Benosman"
              },
              {
                "authorId": "144975525",
                "name": "S. Ieng"
              },
              {
                "authorId": "1744964",
                "name": "P. Lichtsteiner"
              },
              {
                "authorId": "5548576",
                "name": "T. Delbruck"
              }
            ]
          }
        },
        {
          "citedcorpusid": 27059477,
          "isinfluential": false,
          "contexts": [
            "Original work [21], [22] accumulates events to generate event-images such that a frame-based stereo matching al-gorithm can be applied."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Bio-inspired Stereo Vision System with Silicon Retina Imagers",
            "abstract": "",
            "year": 2009,
            "venue": "International Conference on Virtual Storytelling",
            "authors": [
              {
                "authorId": "1824241",
                "name": "J. Kogler"
              },
              {
                "authorId": "1692498",
                "name": "C. Sulzbachner"
              },
              {
                "authorId": "1728811",
                "name": "W. Kubinger"
              }
            ]
          }
        },
        {
          "citedcorpusid": 59222403,
          "isinfluential": false,
          "contexts": [
            "Such methods have been developed to solve many traditional vision problems such as optical flow estimation [31], [32], intensity frame reconstruction [33], [5], action recognition [34], and object tracking [35]."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Unsupervised Event-Based Optical Flow Using Motion Compensation",
            "abstract": "",
            "year": 2018,
            "venue": "ECCV Workshops",
            "authors": [
              {
                "authorId": "3385588",
                "name": "A. Z. Zhu"
              },
              {
                "authorId": "36001694",
                "name": "Liangzhe Yuan"
              },
              {
                "authorId": "20728097",
                "name": "Kenneth Chaney"
              },
              {
                "authorId": "1751586",
                "name": "Kostas Daniilidis"
              }
            ]
          }
        },
        {
          "citedcorpusid": 119309624,
          "isinfluential": false,
          "contexts": [
            "Event cameras hold several important beneﬁts over standard cameras such as low latency ( ∼ 1 µ s), absence of motion blur, high dynamic range (140 dB vs 60 dB for traditional cameras [1]), and low power consumption."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Focus Is All You Need: Loss Functions for Event-Based Vision",
            "abstract": "Event cameras are novel vision sensors that output pixel-level brightness changes (\"events\") instead of traditional video frames. These asynchronous sensors offer several advantages over traditional cameras, such as, high temporal resolution, very high dynamic range, and no motion blur. To unlock the potential of such sensors, motion compensation methods have been recently proposed. We present a collection and taxonomy of twenty two objective functions to analyze event alignment in motion compensation approaches. We call them focus loss functions since they have strong connections with functions used in traditional shape-from-focus applications. The proposed loss functions allow bringing mature computer vision tools to the realm of event cameras. We compare the accuracy and runtime performance of all loss functions on a publicly available dataset, and conclude that the variance, the gradient and the Laplacian magnitudes are among the best loss functions. The applicability of the loss functions is shown on multiple tasks: rotational motion, depth and optical flow estimation. The proposed focus loss functions allow to unlock the outstanding properties of event cameras.",
            "year": 2019,
            "venue": "Computer Vision and Pattern Recognition",
            "authors": [
              {
                "authorId": "144036711",
                "name": "Guillermo Gallego"
              },
              {
                "authorId": "8329387",
                "name": "Mathias Gehrig"
              },
              {
                "authorId": "2075371",
                "name": "D. Scaramuzza"
              }
            ]
          }
        },
        {
          "citedcorpusid": 121601380,
          "isinfluential": false,
          "contexts": [
            "Piatkowska et al. [28] and Firouzi et al. [29] model event-driven stereo matching by a cooperative network for reliable 3D depth perception."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Cooperative and asynchronous stereo vision for dynamic vision sensors",
            "abstract": "",
            "year": 2014,
            "venue": "",
            "authors": [
              {
                "authorId": "47105977",
                "name": "Ewa Piatkowska"
              },
              {
                "authorId": "1768812",
                "name": "A. Belbachir"
              },
              {
                "authorId": "1990797",
                "name": "M. Gelautz"
              }
            ]
          }
        },
        {
          "citedcorpusid": 131774014,
          "isinfluential": false,
          "contexts": [
            "Sharp intensity images are changed into blurry intensity images following [46]."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "DAVANet: Stereo Deblurring With View Aggregation",
            "abstract": "Nowadays stereo cameras are more commonly adopted in emerging devices such as dual-lens smartphones and unmanned aerial vehicles. However, they also suffer from blurry images in dynamic scenes which leads to visual discomfort and hampers further image processing. Previous works have succeeded in monocular deblurring, yet there are few studies on deblurring for stereoscopic images. By exploiting the two-view nature of stereo images, we propose a novel stereo image deblurring network with Depth Awareness and View Aggregation, named DAVANet. In our proposed network, 3D scene cues from the depth and varying information from two views are incorporated, which help to remove complex spatially-varying blur in dynamic scenes. Specifically, with our proposed fusion network, we integrate the bidirectional disparities estimation and deblurring into a unified framework. Moreover, we present a large-scale multi-scene dataset for stereo deblurring, containing 20,637 blurry-sharp stereo image pairs from 135 diverse sequences and their corresponding bidirectional disparities. The experimental results on our dataset demonstrate that DAVANet outperforms state-of-the-art methods in terms of accuracy, speed, and model size.",
            "year": 2019,
            "venue": "Computer Vision and Pattern Recognition",
            "authors": [
              {
                "authorId": "7523259",
                "name": "Shangchen Zhou"
              },
              {
                "authorId": "1519062623",
                "name": "Jiawei Zhang"
              },
              {
                "authorId": "1724520",
                "name": "W. Zuo"
              },
              {
                "authorId": "3451627",
                "name": "Haozhe Xie"
              },
              {
                "authorId": "9416881",
                "name": "Jin-shan Pan"
              },
              {
                "authorId": "145335572",
                "name": "Jimmy S. J. Ren"
              }
            ]
          }
        },
        {
          "citedcorpusid": 145916256,
          "isinfluential": false,
          "contexts": [
            "Such methods have been developed to solve many traditional vision problems such as optical ﬂow estimation [31], [32], intensity frame reconstruction [33], [5], action recognition [34], and object tracking [35]."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Neuromorphic Vision Sensing for CNN-based Action Recognition",
            "abstract": "Neuromorphic vision sensing (NVS) hardware is now gaining traction as a low-power/high-speed visual sensing technology that circumvents the limitations of conventional active pixel sensing (APS) cameras. While object detection and tracking models have been investigated in conjunction with NVS, there is currently little work on NVS for higher-level semantic tasks, such as action recognition. Contrary to recent work that considers homogeneous transfer between flow domains (optical flow to motion vectors), we propose to embed an NVS emulator into a multi-modal transfer learning framework that carries out heterogeneous transfer from optical flow to NVS. The potential of our framework is showcased by the fact that, for the first time, our NVS-based results achieve comparable action recognition performance to motion-vector or optical-flow based methods (i.e., accuracy on UCF-101 within 8.8% of I3D with optical flow), with the NVS emulator and NVS camera hardware offering 3 to 6 orders of magnitude faster frame generation (respectively) compared to standard Brox optical flow. Beyond this significant advantage, our CNN processing is found to have the lowest total GFLOP count against all competing methods (up to 7.7 times complexity saving compared to I3D with optical flow).",
            "year": 2019,
            "venue": "IEEE International Conference on Acoustics, Speech, and Signal Processing",
            "authors": [
              {
                "authorId": "33998511",
                "name": "Aaron Chadha"
              },
              {
                "authorId": "2057245555",
                "name": "Yin Bi"
              },
              {
                "authorId": "2822935",
                "name": "Alhabib Abbas"
              },
              {
                "authorId": "2747620",
                "name": "Y. Andreopoulos"
              }
            ]
          }
        },
        {
          "citedcorpusid": 157060825,
          "isinfluential": false,
          "contexts": [
            "Novel local event context descriptors are proposed to robustly match events in a temporal-spatial window [26], [27]."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "ROBUST DENSE DEPTH MAP ESTIMATION FROM SPARSE DVS STEREOS 3 2 Related Work",
            "abstract": "Real-world depth perception applications require precise reaction to fast motion, and the ability to operate in scenes which contain large intensity differences or high dynamic range. Standard CMOS cameras based methods for depth computing, such as stereo matching, run into the problem of huge power consuming at high frame-rates or inaccurate depths with noise or holes. The event camera, DVS (Dynamic Vision Sensor), aims to be robust to fast motion and light change with low power consumption and sparse representation, offering great potential to replace standard cameras for depth perception. However, it is not trivial to directly apply DVS for stereo matching due to its nature of low latency and sparsity which will result in extremely limited available information and imperfect imaging quality. To overcome these problems and make the DVS available for depth perception, this paper introduces a novel method which can enhance the stream of events and estimate the dense depth through event driven stereo matching. Our event stream enhancement algorithm efficiently buffers events according to time continuous rather than using artificially-chosen time intervals, and our stereo matching method can robust estimate depth for complex scenarios regardless of the motion or light changing. To the best of our knowledge, this is the first algorithm provably able to recover dense depth maps from sparse DVS stereos. Experiments in a variety of challenging conditions demonstrate the superior performance of our method. c © 2017. The copyright of this document resides with its authors. It may be distributed unchanged freely in print or electronic forms. 2 ZOU ET AL.: ROBUST DENSE DEPTH MAP ESTIMATION FROM SPARSE DVS STEREOS",
            "year": 2017,
            "venue": "",
            "authors": [
              {
                "authorId": "2780914",
                "name": "Dongqing Zou"
              },
              {
                "authorId": "2053742370",
                "name": "Feng Shi"
              },
              {
                "authorId": "2153282723",
                "name": "Qiang Wang"
              }
            ]
          }
        },
        {
          "citedcorpusid": 189998802,
          "isinfluential": false,
          "contexts": [
            "…on event camera based computer vision, the reader is kindly referred to Gallego et al.’s survey [2] offering a good summary of several important contributions on problems such as object tracking [3], pattern recognition [4], intensity image reconstruction [5] and stereo depth estimation [6], [7].",
            "Such methods have been developed to solve many traditional vision problems such as optical ﬂow estimation [31], [32], intensity frame reconstruction [33], [5], action recognition [34], and object tracking [35]."
          ],
          "intents": [
            "['background']",
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "High Speed and High Dynamic Range Video with an Event Camera",
            "abstract": "Event cameras are novel sensors that report brightness changes in the form of a stream of asynchronous “events” instead of intensity frames. They offer significant advantages with respect to conventional cameras: high temporal resolution, high dynamic range, and no motion blur. While the stream of events encodes in principle the complete visual signal, the reconstruction of an intensity image from a stream of events is an ill-posed problem in practice. Existing reconstruction approaches are based on hand-crafted priors and strong assumptions about the imaging process as well as the statistics of natural images. In this work we propose to learn to reconstruct intensity images from event streams directly from data instead of relying on any hand-crafted priors. We propose a novel recurrent network to reconstruct videos from a stream of events, and train it on a large amount of simulated event data. During training we propose to use a perceptual loss to encourage reconstructions to follow natural image statistics. We further extend our approach to synthesize color images from color event streams. Our quantitative experiments show that our network surpasses state-of-the-art reconstruction methods by a large margin in terms of image quality (<inline-formula><tex-math notation=\"LaTeX\">$>\\!20\\%$</tex-math><alternatives><mml:math><mml:mrow><mml:mo>></mml:mo><mml:mspace width=\"-0.166667em\"/><mml:mn>20</mml:mn><mml:mo>%</mml:mo></mml:mrow></mml:math><inline-graphic xlink:href=\"rebecq-ieq1-2963386.gif\"/></alternatives></inline-formula>), while comfortably running in real-time. We show that the network is able to synthesize high framerate videos (<inline-formula><tex-math notation=\"LaTeX\">$>5,000$</tex-math><alternatives><mml:math><mml:mrow><mml:mo>></mml:mo><mml:mn>5</mml:mn><mml:mo>,</mml:mo><mml:mn>000</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href=\"rebecq-ieq2-2963386.gif\"/></alternatives></inline-formula> frames per second) of high-speed phenomena (e.g., a bullet hitting an object) and is able to provide high dynamic range reconstructions in challenging lighting conditions. As an additional contribution, we demonstrate the effectiveness of our reconstructions as an intermediate representation for event data. We show that off-the-shelf computer vision algorithms can be applied to our reconstructions for tasks such as object classification and visual-inertial odometry and that this strategy consistently outperforms algorithms that were specifically designed for event data. We release the reconstruction code, a pre-trained model and the datasets to enable further research.",
            "year": 2019,
            "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
            "authors": [
              {
                "authorId": "3414274",
                "name": "Henri Rebecq"
              },
              {
                "authorId": "2774325",
                "name": "René Ranftl"
              },
              {
                "authorId": "145231047",
                "name": "V. Koltun"
              },
              {
                "authorId": "2075371",
                "name": "D. Scaramuzza"
              }
            ]
          }
        },
        {
          "citedcorpusid": 202565789,
          "isinfluential": false,
          "contexts": [
            "DeepPruner [16] improves on the complexity and runtime induced by building the cost volume and the 3D convolutions."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "DeepPruner: Learning Efficient Stereo Matching via Differentiable PatchMatch",
            "abstract": "Our goal is to significantly speed up the runtime of current state-of-the-art stereo algorithms to enable real-time inference. Towards this goal, we developed a differentiable PatchMatch module that allows us to discard most disparities without requiring full cost volume evaluation. We then exploit this representation to learn which range to prune for each pixel. By progressively reducing the search space and effectively propagating such information, we are able to efficiently compute the cost volume for high likelihood hypotheses and achieve savings in both memory and computation.Finally, an image guided refinement module is exploited to further improve the performance. Since all our components are differentiable, the full network can be trained end-to-end. Our experiments show that our method achieves competitive results on KITTI and SceneFlow datasets while running in real-time at 62ms.",
            "year": 2019,
            "venue": "IEEE International Conference on Computer Vision",
            "authors": [
              {
                "authorId": "26928334",
                "name": "Shivam Duggal"
              },
              {
                "authorId": "1892247",
                "name": "Shenlong Wang"
              },
              {
                "authorId": "2650832",
                "name": "Wei-Chiu Ma"
              },
              {
                "authorId": "2067788292",
                "name": "Rui Hu"
              },
              {
                "authorId": "2422559",
                "name": "R. Urtasun"
              }
            ]
          }
        },
        {
          "citedcorpusid": 220545977,
          "isinfluential": false,
          "contexts": [
            "Events and frames are utilized as a bi-modal input to recover sharp, high quality images in [36] and [33]."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Event Enhanced High-Quality Image Recovery",
            "abstract": "With extremely high temporal resolution, event cameras have a large potential for robotics and computer vision. However, their asynchronous imaging mechanism often aggravates the measurement sensitivity to noises and brings a physical burden to increase the image spatial resolution. To recover high-quality intensity images, one should address both denoising and super-resolution problems for event cameras. Since events depict brightness changes, with the enhanced degeneration model by the events, the clear and sharp high-resolution latent images can be recovered from the noisy, blurry and low-resolution intensity observations. Exploiting the framework of sparse learning, the events and the low-resolution intensity observations can be jointly considered. Based on this, we propose an explainable network, an event-enhanced sparse learning network (eSL-Net), to recover the high-quality images from event cameras. After training with a synthetic dataset, the proposed eSL-Net can largely improve the performance of the state-of-the-art by 7-12 dB. Furthermore, without additional training process, the proposed eSL-Net can be easily extended to generate continuous frames with frame-rate as high as the events.",
            "year": 2020,
            "venue": "European Conference on Computer Vision",
            "authors": [
              {
                "authorId": "14630719",
                "name": "Bishan Wang"
              },
              {
                "authorId": "47752316",
                "name": "Jingwei He"
              },
              {
                "authorId": "46275044",
                "name": "Lei Yu"
              },
              {
                "authorId": "51280933",
                "name": "Guisong Xia"
              },
              {
                "authorId": "49230398",
                "name": "Wen Yang"
              }
            ]
          }
        },
        {
          "citedcorpusid": 221846159,
          "isinfluential": false,
          "contexts": [
            "’s survey [2] offering a good summary of several important contributions on problems such as object tracking [3], pattern recognition [4], intensity image reconstruction [5] and stereo depth estimation [6], [7]."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Asynchronous event-based clustering and tracking for intrusion monitoring in UAS",
            "abstract": "Automatic surveillance and monitoring using Unmanned Aerial Systems (UAS) require the development of perception systems that robustly work under different illumination conditions. Event cameras are neuromorphic sensors that capture the illumination changes in the scene with very low latency and high dynamic range. Although recent advances in eventbased vision have explored the use of event cameras onboard UAS, most techniques group events in frames and, therefore, do not fully exploit the sequential and asynchronous nature of the event stream. This paper proposes a fully asynchronous scheme for intruder monitoring using UAS. It employs efficient event clustering and feature tracking modules and includes a sampling mechanism to cope with the computational cost of event-by-event processing adapting to on-board hardware computational constraints. The proposed scheme was tested on a real multirotor in challenging scenarios showing significant accuracy and robustness to lighting conditions.",
            "year": 2020,
            "venue": "IEEE International Conference on Robotics and Automation",
            "authors": [
              {
                "authorId": "1412532578",
                "name": "J. P. Rodríguez-Gómez"
              },
              {
                "authorId": "9617212",
                "name": "A. G. Eguíluz"
              },
              {
                "authorId": "2717222",
                "name": "J. R. M. Dios"
              },
              {
                "authorId": "144405166",
                "name": "A. Ollero"
              }
            ]
          }
        },
        {
          "citedcorpusid": 226291858,
          "isinfluential": false,
          "contexts": [
            "Events and frames are utilized as a bi-modal input to recover sharp, high quality images in [36] and [33].",
            "Such methods have been developed to solve many traditional vision problems such as optical ﬂow estimation [31], [32], intensity frame reconstruction [33], [5], action recognition [34], and object tracking [35]."
          ],
          "intents": [
            "['background']",
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Learning Event-Driven Video Deblurring and Interpolation",
            "abstract": "",
            "year": 2020,
            "venue": "European Conference on Computer Vision",
            "authors": [
              {
                "authorId": "32764534",
                "name": "Songnan Lin"
              },
              {
                "authorId": "1519062623",
                "name": "Jiawei Zhang"
              },
              {
                "authorId": "9416881",
                "name": "Jin-shan Pan"
              },
              {
                "authorId": "2112762498",
                "name": "Zhe Jiang"
              },
              {
                "authorId": "2780914",
                "name": "Dongqing Zou"
              },
              {
                "authorId": "1692621",
                "name": "Yongtian Wang"
              },
              {
                "authorId": "47739910",
                "name": "J. Chen"
              },
              {
                "authorId": "145335572",
                "name": "Jimmy S. J. Ren"
              }
            ]
          }
        },
        {
          "citedcorpusid": 226292034,
          "isinfluential": false,
          "contexts": [
            "Such methods have been developed to solve many traditional vision problems such as optical flow estimation [31], [32], intensity frame reconstruction [33], [5], action recognition [34], and object tracking [35]."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Jointly Learning Visual Motion and Confidence from Local Patches in Event Cameras",
            "abstract": "",
            "year": 2020,
            "venue": "European Conference on Computer Vision",
            "authors": [
              {
                "authorId": "48167512",
                "name": "Daniel R. Kepple"
              },
              {
                "authorId": "153619516",
                "name": "Daewon Lee"
              },
              {
                "authorId": "2008508852",
                "name": "Colin Prepsius"
              },
              {
                "authorId": "1698835",
                "name": "Volkan Isler"
              },
              {
                "authorId": "41159691",
                "name": "Il-Su Park"
              },
              {
                "authorId": "2124216992",
                "name": "Daniel D. Lee"
              }
            ]
          }
        }
      ]
    },
    "233033658": {
      "citing_paper_info": {
        "title": "Instantaneous Stereo Depth Estimation of Real-World Stimuli with a Neuromorphic Stereo-Vision Setup",
        "abstract": "The stereo-matching problem, i.e., matching corresponding features in two different views to reconstruct depth, is efficiently solved in biology. Yet, it remains the computational bottleneck for classical machine vision approaches. By exploiting the properties of event cameras, recently proposed Spiking Neural Network (SNN) architectures for stereo vision have the potential of simplifying the stereo-matching problem. Several solutions that combine event cameras with spike-based neuromorphic processors already exist. However, they are either simulated on digital hardware or tested on simplified stimuli. In this work, we use the Dynamic Vision Sensor 3D Human Pose Dataset (DHP19) to validate a brain-inspired event-based stereo-matching architecture implemented on a mixed-signal neuromorphic processor with real-world data. Our experiments show that this SNN architecture, composed of coincidence detectors and disparity sensitive neurons, is able to provide a coarse estimate of the input disparity instantaneously, thereby detecting the presence of a stimulus moving in depth in real-time.",
        "year": 2021,
        "venue": "International Symposium on Circuits and Systems",
        "authors": [
          {
            "authorId": "46242134",
            "name": "Nicoletta Risi"
          },
          {
            "authorId": "2053001729",
            "name": "Enrico Calabrese"
          },
          {
            "authorId": "46494726",
            "name": "G. Indiveri"
          }
        ]
      },
      "citation_summary": {
        "citation_count": 8,
        "unique_cited_count": 8,
        "influential_count": 0,
        "detailed_records_count": 8
      },
      "cited_papers": [
        "226308033",
        "2497402",
        "14878668",
        "13373696",
        "3416874",
        "118684904",
        "25268038",
        "14072069"
      ],
      "citation_details": [
        {
          "citedcorpusid": 2497402,
          "isinfluential": false,
          "contexts": [
            "By providing DVS input data combined with precise, yet sparse, 3D ground-truth information, the DVS 3D Human Pose Dataset (DHP19) [16] offers suitable samples for small-scale neuromorphic architectures of coarse stereo vision.",
            "By contrast, biologically inspired neuromorphic event cameras, such as the Dynamic Vision Sensor (DVS) [1], transmit asynchronous streams of events generated by individual pixels in response to perceived brightness changes [2]–[4].",
            "Despite the lack of standard benchmarks for this problem domain, two datasets for event-based stereo have recently been proposed: The Multi Vehicle Stereo Event Camera (MVSEC) Dataset [15], consisting of indoor and outdoor sequences recorded in a variety of illuminations and speeds, and the DVS stereo dataset [10], with two real-world sets of sequences (a fast rotating fan and a rotating toy butterfly)."
          ],
          "intents": [
            "--",
            "['background']",
            "--"
          ],
          "cited_paper_info": {
            "title": "> Replace This Line with Your Paper Identification Number (double-click Here to Edit) < 1",
            "abstract": "—This paper describes a CMOS vision sensor which is inspired by biological visual systems. Each pixel independently and in continuous time quantizes local relative intensity changes to generate spike events. These events appear at the output of the sensor as an asynchronous stream of digital pixel addresses. These address-events signify scene reflectance change and have sub-millisecond timing precision. The output data rate depends on the dynamic content of the scene and is typically orders of magnitude lower than those of conventional frame-based imagers. By combining an active front-end logarithmic photoreceptor running in continuous time with a self-timed switched-capacitor differencing circuit, the sensor achieves an array mismatch of 2.1% in relative intensity event threshold and a pixel bandwidth of 3 kHz under 1 klux scene illumination. Dynamic range is >120 dB and chip power consumption is 23 mW. Event latency shows weak light dependency and decreases to 15 us at >1 klux pixel illumination. The sensor is built in a 0.35u 4M2P process yielding 40x40 um 2 pixels with 9.4% fill-factor. By providing high pixel bandwidth, wide dynamic range, and precisely-timed sparse digital output, this neuromorphic silicon retina provides an attractive combination of characteristics for low-latency dynamic vision under uncontrolled illumination with low post-processing requirements.",
            "year": null,
            "venue": "",
            "authors": []
          }
        },
        {
          "citedcorpusid": 3416874,
          "isinfluential": false,
          "contexts": [
            "Unlike datasets such as [10] and [15], which can be used to compute the ground truth on a per-event basis, the Vicon marker-based motion capture system provides ground-truth depth information directly with sparse data linked to point labels attached to specific body parts.",
            "By providing DVS input data combined with precise, yet sparse, 3D ground-truth information, the DVS 3D Human Pose Dataset (DHP19) [16] offers suitable samples for small-scale neuromorphic architectures of coarse stereo vision.",
            "Despite the lack of standard benchmarks for this problem domain, two datasets for event-based stereo have recently been proposed: The Multi Vehicle Stereo Event Camera (MVSEC) Dataset [15], consisting of indoor and outdoor sequences recorded in a variety of illuminations and speeds, and the DVS stereo dataset [10], with two real-world sets of sequences (a fast rotating fan and a rotating toy butterfly)."
          ],
          "intents": [
            "['methodology']",
            "--",
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "The Multivehicle Stereo Event Camera Dataset: An Event Camera Dataset for 3D Perception",
            "abstract": "Event-based cameras are a new passive sensing modality with a number of benefits over traditional cameras, including extremely low latency, asynchronous data acquisition, high dynamic range, and very low power consumption. There has been a lot of recent interest and development in applying algorithms to use the events to perform a variety of three-dimensional perception tasks, such as feature tracking, visual odometry, and stereo depth estimation. However, there currently lacks the wealth of labeled data that exists for traditional cameras to be used for both testing and development. In this letter, we present a large dataset with a synchronized stereo pair event based camera system, carried on a handheld rig, flown by a hexacopter, driven on top of a car, and mounted on a motorcycle, in a variety of different illumination levels and environments. From each camera, we provide the event stream, grayscale images, and inertial measurement unit (IMU) readings. In addition, we utilize a combination of IMU, a rigidly mounted lidar system, indoor and outdoor motion capture, and GPS to provide accurate pose and depth images for each camera at up to 100 Hz. For comparison, we also provide synchronized grayscale images and IMU readings from a frame-based stereo camera system.",
            "year": 2018,
            "venue": "IEEE Robotics and Automation Letters",
            "authors": [
              {
                "authorId": "3385588",
                "name": "A. Z. Zhu"
              },
              {
                "authorId": "144964367",
                "name": "Dinesh Thakur"
              },
              {
                "authorId": "2520604",
                "name": "Tolga Özaslan"
              },
              {
                "authorId": "39832696",
                "name": "Bernd Pfrommer"
              },
              {
                "authorId": "37956314",
                "name": "Vijay R. Kumar"
              },
              {
                "authorId": "1751586",
                "name": "Kostas Daniilidis"
              }
            ]
          }
        },
        {
          "citedcorpusid": 13373696,
          "isinfluential": false,
          "contexts": [
            "For each column, the top row shows the events of both cameras, depicted as time surfaces [22] with rectified polarities, together with the projected marker locations, and their corresponding disparity over time."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "HOTS: A Hierarchy of Event-Based Time-Surfaces for Pattern Recognition",
            "abstract": "",
            "year": 2017,
            "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
            "authors": [
              {
                "authorId": "1807856",
                "name": "Xavier Lagorce"
              },
              {
                "authorId": "33780923",
                "name": "G. Orchard"
              },
              {
                "authorId": "3008126",
                "name": "F. Galluppi"
              },
              {
                "authorId": "2075335081",
                "name": "Bertram E. Shi"
              },
              {
                "authorId": "1750848",
                "name": "R. Benosman"
              }
            ]
          }
        },
        {
          "citedcorpusid": 14072069,
          "isinfluential": false,
          "contexts": [
            "By contrast, the stimulus disparity encoded by the SNN was defined as the firing-rate weighted average of the encoded disparity dn for each neuron in C and D, or population Center of Mass (CoM) [21]:"
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Theoretical Neuroscience: Computational and Mathematical Modeling of Neural Systems",
            "abstract": "Theoretical neuroscience provides a quantitative basis for describing what nervous systems do, determining how they function, and uncovering the general principles by which they operate. This text introduces the basic mathematical and computational methods of theoretical neuroscience and presents applications in a variety of areas including vision, sensory-motor integration, development, learning, and memory. The book is divided into three parts. Part I discusses the relationship between sensory stimuli and neural responses, focusing on the representation of information by the spiking activity of neurons. Part II discusses the modeling of neurons and neural circuits on the basis of cellular and synaptic biophysics. Part III analyzes the role of plasticity in development and learning. An appendix covers the mathematical methods used, and exercises are available on the book's Web site.",
            "year": 2001,
            "venue": "",
            "authors": [
              {
                "authorId": "1790646",
                "name": "P. Dayan"
              },
              {
                "authorId": "145293717",
                "name": "L. Abbott"
              }
            ]
          }
        },
        {
          "citedcorpusid": 14878668,
          "isinfluential": false,
          "contexts": [
            "Digital peripheral asynchronous input/output logic circuits are used to receive and transmit spikes via an Address Event Representation (AER) communication protocol [20]."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "A pulse-coded communications infrastructure for neuromorphic systems",
            "abstract": "Neuromorphic engineering [Mead, 1989, Mead, 1990, Douglas et al., 1995] applies the computational principles used by biological nervous systems to those tasks that biological systems perform easily, but which have proved difficult to do using traditional engineering techniques. These problems include visual and auditory perceptive processing, navigation, and locomotion. Typically, current neuromorphic systems are hybrid analog-digital electronic systems fabricated using CMOS VLSI technology [Mead, 1989, Douglas and Mahowald, 1995]. Research has focused on the sub-threshold analog operation of these circuits, because in this regime it is possible to construct compact analog circuits that compute various biologically relevant operations such as logarithms, exponents and hyperbolic tangents.",
            "year": 1999,
            "venue": "",
            "authors": [
              {
                "authorId": "47429806",
                "name": "S. Deiss"
              },
              {
                "authorId": "1742758",
                "name": "R. Douglas"
              },
              {
                "authorId": "1737986",
                "name": "A. Whatley"
              }
            ]
          }
        },
        {
          "citedcorpusid": 25268038,
          "isinfluential": false,
          "contexts": [
            "Following the pioneering work of Misha\nMahowald [6], several Spiking Neural Networks (SNNs) that reconstruct 3D information on a per-event basis have been recently deployed on fully digital, as well as mixed-signals neuromorphic architectures: Spinnaker [7], [8], True North [9], [10], ROLLS [11], [12], and DYNAP [13], [14].",
            "Following the pioneering work of Misha Mahowald [6], several Spiking Neural Networks (SNNs) that reconstruct 3D information on a per-event basis have been recently deployed on fully digital, as well as mixed-signals neuromorphic architectures: Spinnaker [7], [8], True North [9], [10], ROLLS [11], [12], and DYNAP [13], [14]."
          ],
          "intents": [
            "--",
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "The SpiNNaker Project",
            "abstract": "",
            "year": 2014,
            "venue": "Proceedings of the IEEE",
            "authors": [
              {
                "authorId": "144409615",
                "name": "S. Furber"
              },
              {
                "authorId": "3008126",
                "name": "F. Galluppi"
              },
              {
                "authorId": "143816983",
                "name": "S. Temple"
              },
              {
                "authorId": "3085921",
                "name": "L. Plana"
              }
            ]
          }
        },
        {
          "citedcorpusid": 118684904,
          "isinfluential": false,
          "contexts": [
            "Indeed, a novel class of event-based algorithms for stereo vision, also referred to as instantaneous stereo, extracts depth information by exploiting the inter-ocular spatio-temporal correlation of spike trains from event cameras [2].",
            "By contrast, biologically inspired neuromorphic event cameras, such as the Dynamic Vision Sensor (DVS) [1], transmit asynchronous streams of events generated by individual pixels in response to perceived brightness changes [2]–[4]."
          ],
          "intents": [
            "['background']",
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Event-Based Vision: A Survey",
            "abstract": "Event cameras are bio-inspired sensors that differ from conventional frame cameras: Instead of capturing images at a fixed rate, they asynchronously measure per-pixel brightness changes, and output a stream of events that encode the time, location and sign of the brightness changes. Event cameras offer attractive properties compared to traditional cameras: high temporal resolution (in the order of $\\mu$μs), very high dynamic range (140 dB versus 60 dB), low power consumption, and high pixel bandwidth (on the order of kHz) resulting in reduced motion blur. Hence, event cameras have a large potential for robotics and computer vision in challenging scenarios for traditional cameras, such as low-latency, high speed, and high dynamic range. However, novel methods are required to process the unconventional output of these sensors in order to unlock their potential. This paper provides a comprehensive overview of the emerging field of event-based vision, with a focus on the applications and the algorithms developed to unlock the outstanding properties of event cameras. We present event cameras from their working principle, the actual sensors that are available and the tasks that they have been used for, from low-level vision (feature detection and tracking, optic flow, etc.) to high-level vision (reconstruction, segmentation, recognition). We also discuss the techniques developed to process events, including learning-based techniques, as well as specialized processors for these novel sensors, such as spiking neural networks. Additionally, we highlight the challenges that remain to be tackled and the opportunities that lie ahead in the search for a more efficient, bio-inspired way for machines to perceive and interact with the world.",
            "year": 2019,
            "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
            "authors": [
              {
                "authorId": "144036711",
                "name": "Guillermo Gallego"
              },
              {
                "authorId": "1694635",
                "name": "T. Delbrück"
              },
              {
                "authorId": "33780923",
                "name": "G. Orchard"
              },
              {
                "authorId": "1897771",
                "name": "C. Bartolozzi"
              },
              {
                "authorId": "1736425",
                "name": "B. Taba"
              },
              {
                "authorId": "1860631",
                "name": "A. Censi"
              },
              {
                "authorId": "2864731",
                "name": "Stefan Leutenegger"
              },
              {
                "authorId": "2052135690",
                "name": "A. Davison"
              },
              {
                "authorId": "3302681",
                "name": "J. Conradt"
              },
              {
                "authorId": "1751586",
                "name": "Kostas Daniilidis"
              },
              {
                "authorId": "2075371",
                "name": "D. Scaramuzza"
              }
            ]
          }
        },
        {
          "citedcorpusid": 226308033,
          "isinfluential": false,
          "contexts": [
            "Following the pioneering work of Misha Mahowald [6], several Spiking Neural Networks (SNNs) that reconstruct 3D information on a per-event basis have been recently deployed on fully digital, as well as mixed-signals neuromorphic architectures: Spinnaker [7], [8], True North [9], [10], ROLLS [11], [12], and DYNAP [13], [14].",
            "Finally, for each input sample, we estimated the power consumption of the mixed-signal neuromorphic implementation as described in [14].",
            "Thus, in this work, we use the DHP19 dataset to assess the robustness of the event-based approach for neuromorphic, on-chip depth estimation recently presented in [14].",
            "The Spike-Based Neuromorphic Architecture The spike-based neuromorphic architecture used to extract disparity information is based on the hardwired topology proposed in [14].",
            "By contrast, [12] and [14] use mixed-signal analog/digital neuromorphic circuits that directly emulate the dynamics of the neural computing primitives used in biology to perform stereo vision.",
            "2, is adapted from the structure presented in [12], [14].",
            "See [14] for a comprehensive description of the architecture."
          ],
          "intents": [
            "['methodology']",
            "['methodology']",
            "['methodology']",
            "['methodology']",
            "['background']",
            "['background']",
            "['background']"
          ],
          "cited_paper_info": {
            "title": "A Spike-Based Neuromorphic Architecture of Stereo Vision",
            "abstract": "The problem of finding stereo correspondences in binocular vision is solved effortlessly in nature and yet it is still a critical bottleneck for artificial machine vision systems. As temporal information is a crucial feature in this process, the advent of event-based vision sensors and dedicated event-based processors promises to offer an effective approach to solving the stereo matching problem. Indeed, event-based neuromorphic hardware provides an optimal substrate for fast, asynchronous computation, that can make explicit use of precise temporal coincidences. However, although several biologically-inspired solutions have already been proposed, the performance benefits of combining event-based sensing with asynchronous and parallel computation are yet to be explored. Here we present a hardware spike-based stereo-vision system that leverages the advantages of brain-inspired neuromorphic computing by interfacing two event-based vision sensors to an event-based mixed-signal analog/digital neuromorphic processor. We describe a prototype interface designed to enable the emulation of a stereo-vision system on neuromorphic hardware and we quantify the stereo matching performance with two datasets. Our results provide a path toward the realization of low-latency, end-to-end event-based, neuromorphic architectures for stereo vision.",
            "year": 2020,
            "venue": "Frontiers in Neurorobotics",
            "authors": [
              {
                "authorId": "46242134",
                "name": "Nicoletta Risi"
              },
              {
                "authorId": "2064094007",
                "name": "Alessandro Aimar"
              },
              {
                "authorId": "1452347065",
                "name": "Elisa Donati"
              },
              {
                "authorId": "2470088",
                "name": "S. Solinas"
              },
              {
                "authorId": "1721210",
                "name": "G. Indiveri"
              }
            ]
          }
        }
      ]
    },
    "268277996": {
      "citing_paper_info": {
        "title": "Depth from Asymmetric Frame-Event Stereo: A Divide-and-Conquer Approach",
        "abstract": "Event cameras asynchronously measure brightness changes in a scene without motion blur or saturation, while frame cameras capture images with dense intensity and fine details at a fixed rate. The exclusive advantages of the two modalities make depth estimation from Stereo Asymmetric Frame-Event (SAFE) systems appealing. However, due to the inevitable information absence of one modality in certain challenging regions, existing stereo matching methods lose efficacy for asymmetric inputs from SAFE systems. In this paper, we propose a divide-and-conquer approach that decomposes depth estimation from SAFE systems into three sub-tasks, i.e., frame-event stereo matching, frame-based Structure-from-Motion (SfM), and event-based SfM. In this way, the above challenging regions are addressed by monocular SfM, which estimates robust depth with two views belonging to the same functioning modality. Moreover, we propose a dual sampling strategy to construct cost volumes with identical spatial locations and depth hypotheses for different sub-tasks, which enables sub-task fusion at the cost volume level. To tackle the occlusion issue raised by the sampling strategy, we further introduce a temporal fusion scheme to utilize long-term sequential inputs with multi-view information. Experimental results validate the superior performance of our method over existing solutions.",
        "year": 2024,
        "venue": "IEEE Workshop/Winter Conference on Applications of Computer Vision",
        "authors": [
          {
            "authorId": "2290357966",
            "name": "Xihao Chen"
          },
          {
            "authorId": "2107017232",
            "name": "Wenming Weng"
          },
          {
            "authorId": "2145912767",
            "name": "Yueyi Zhang"
          },
          {
            "authorId": "2250301651",
            "name": "Zhiwei Xiong"
          }
        ]
      },
      "citation_summary": {
        "citation_count": 14,
        "unique_cited_count": 14,
        "influential_count": 1,
        "detailed_records_count": 14
      },
      "cited_papers": [
        "118684904",
        "53749928",
        "53073405",
        "3719281",
        "212675709",
        "245300947",
        "4252896",
        "232478376",
        "249980412",
        "253513043",
        "29158639",
        "232170230",
        "253651036",
        "4712004"
      ],
      "citation_details": [
        {
          "citedcorpusid": 3719281,
          "isinfluential": false,
          "contexts": [
            "…camera systems, consisting of an event camera and a frame camera , to solve long-standing challenges in various applications, including de-blur [23], HDR imaging [10], SLAM [30], etc. Recently, depth estimation from SAFE systems [32, 42] has also been explored, which aims to estimate…"
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "U-Net: Convolutional Networks for Biomedical Image Segmentation",
            "abstract": "There is large consent that successful training of deep networks requires many thousand annotated training samples. In this paper, we present a network and training strategy that relies on the strong use of data augmentation to use the available annotated samples more efficiently. The architecture consists of a contracting path to capture context and a symmetric expanding path that enables precise localization. We show that such a network can be trained end-to-end from very few images and outperforms the prior best method (a sliding-window convolutional network) on the ISBI challenge for segmentation of neuronal structures in electron microscopic stacks. Using the same network trained on transmitted light microscopy images (phase contrast and DIC) we won the ISBI cell tracking challenge 2015 in these categories by a large margin. Moreover, the network is fast. Segmentation of a 512x512 image takes less than a second on a recent GPU. The full implementation (based on Caffe) and the trained networks are available at http://lmb.informatik.uni-freiburg.de/people/ronneber/u-net .",
            "year": 2015,
            "venue": "International Conference on Medical Image Computing and Computer-Assisted Intervention",
            "authors": [
              {
                "authorId": "1737326",
                "name": "O. Ronneberger"
              },
              {
                "authorId": "152702479",
                "name": "P. Fischer"
              },
              {
                "authorId": "1710872",
                "name": "T. Brox"
              }
            ]
          }
        },
        {
          "citedcorpusid": 4252896,
          "isinfluential": true,
          "contexts": [
            "…of an event camera and a frame camera , to solve long-standing challenges in various applications, including de-blur [23], HDR imaging [10], SLAM [30], etc. Recently, depth estimation from SAFE systems [32, 42] has also been explored, which aims to estimate accurate depth in various conditions.",
            "In a SAFE system, a frame camera and an event camera are used to perceive scenes with different modalities.",
            "These advantages make event cameras a promising alternative to conventional frame cameras in challenging scenarios, such as challenging illumination or high-speed situations.",
            "An event is triggered by a pixel intensity change above a certain threshold and characterized by the corresponding pixel location, timestamp, and polarity.",
            "Depth estimation from SAFE systems is expected to be accurate in various scenarios, because the event camera provides high-quality signals even in high dynamic range or high-speed regions while the frame camera provides clear intensity signals in most regions."
          ],
          "intents": [
            "['background']",
            "['methodology']",
            "['background']",
            "['background']",
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Pyramid Stereo Matching Network",
            "abstract": "Recent work has shown that depth estimation from a stereo pair of images can be formulated as a supervised learning task to be resolved with convolutional neural networks (CNNs). However, current architectures rely on patch-based Siamese networks, lacking the means to exploit context information for finding correspondence in ill-posed regions. To tackle this problem, we propose PSMNet, a pyramid stereo matching network consisting of two main modules: spatial pyramid pooling and 3D CNN. The spatial pyramid pooling module takes advantage of the capacity of global context information by aggregating context in different scales and locations to form a cost volume. The 3D CNN learns to regularize cost volume using stacked multiple hourglass networks in conjunction with intermediate supervision. The proposed approach was evaluated on several benchmark datasets. Our method ranked first in the KITTI 2012 and 2015 leaderboards before March 18, 2018. The codes of PSMNet are available at: https://github.com/JiaRenChang/PSMNet.",
            "year": 2018,
            "venue": "2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition",
            "authors": [
              {
                "authorId": "2936466",
                "name": "Jia-Ren Chang"
              },
              {
                "authorId": "2143438143",
                "name": "Yonghao Chen"
              }
            ]
          }
        },
        {
          "citedcorpusid": 4712004,
          "isinfluential": false,
          "contexts": [
            "Experiments",
            "Typically, to boost the estimation of one reference view, multiple source views are matched with the reference jointly [34,35] or respectively [12,18].",
            "We expect our method could generalize to other stereo asymmetric systems and leave it as a future work."
          ],
          "intents": [
            "--",
            "['background']",
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "MVSNet: Depth Inference for Unstructured Multi-view Stereo",
            "abstract": "We present an end-to-end deep learning architecture for depth map inference from multi-view images. In the network, we first extract deep visual image features, and then build the 3D cost volume upon the reference camera frustum via the differentiable homography warping. Next, we apply 3D convolutions to regularize and regress the initial depth map, which is then refined with the reference image to generate the final output. Our framework flexibly adapts arbitrary N-view inputs using a variance-based cost metric that maps multiple features into one cost feature. The proposed MVSNet is demonstrated on the large-scale indoor DTU dataset. With simple post-processing, our method not only significantly outperforms previous state-of-the-arts, but also is several times faster in runtime. We also evaluate MVSNet on the complex outdoor Tanks and Temples dataset, where our method ranks first before April 18, 2018 without any fine-tuning, showing the strong generalization ability of MVSNet.",
            "year": 2018,
            "venue": "European Conference on Computer Vision",
            "authors": [
              {
                "authorId": "145923104",
                "name": "Yao Yao"
              },
              {
                "authorId": "9484005",
                "name": "Zixin Luo"
              },
              {
                "authorId": "144013684",
                "name": "Shiwei Li"
              },
              {
                "authorId": "3406486",
                "name": "Tian Fang"
              },
              {
                "authorId": "144645904",
                "name": "Long Quan"
              }
            ]
          }
        },
        {
          "citedcorpusid": 29158639,
          "isinfluential": false,
          "contexts": [
            "Compared with cross-modal symmetric stereo systems, asymmetric stereo systems with a single modality on one side, e.g., frame-event [9,32,42] and RGB-NIR [16,38] systems, possess the same sensing capabilities and come with half costs.",
            "For example, edge images and temporal gradient images are adopted to normalize frame and event images [15, 32], while transformation networks are proposed to make up the photometric inconsistency of RGB and NIR images [16, 38]."
          ],
          "intents": [
            "['background']",
            "--"
          ],
          "cited_paper_info": {
            "title": "Deep Material-Aware Cross-Spectral Stereo Matching",
            "abstract": "Cross-spectral imaging provides strong benefits for recognition and detection tasks. Often, multiple cameras are used for cross-spectral imaging, thus requiring image alignment, or disparity estimation in a stereo setting. Increasingly, multi-camera cross-spectral systems are embedded in active RGBD devices (e.g. RGB-NIR cameras in Kinect and iPhone X). Hence, stereo matching also provides an opportunity to obtain depth without an active projector source. However, matching images from different spectral bands is challenging because of large appearance variations. We develop a novel deep learning framework to simultaneously transform images across spectral bands and estimate disparity. A material-aware loss function is incorporated within the disparity prediction network to handle regions with unreliable matching such as light sources, glass windshields and glossy surfaces. No depth supervision is required by our method. To evaluate our method, we used a vehicle-mounted RGB-NIR stereo system to collect 13.7 hours of video data across a range of areas in and around a city. Experiments show that our method achieves strong performance and reaches real-time speed.",
            "year": 2018,
            "venue": "2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition",
            "authors": [
              {
                "authorId": "7353963",
                "name": "Tiancheng Zhi"
              },
              {
                "authorId": "144588179",
                "name": "B. Pires"
              },
              {
                "authorId": "145670946",
                "name": "M. Hebert"
              },
              {
                "authorId": "1779052",
                "name": "S. Narasimhan"
              }
            ]
          }
        },
        {
          "citedcorpusid": 53073405,
          "isinfluential": false,
          "contexts": [
            "Typically, to boost the estimation of one reference view, multiple source views are matched with the reference jointly [34,35] or respectively [12,18]."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "DPSNet: End-to-end Deep Plane Sweep Stereo",
            "abstract": "Multiview stereo aims to reconstruct scene depth from images acquired by a camera under arbitrary motion. Recent methods address this problem through deep learning, which can utilize semantic cues to deal with challenges such as textureless and reflective regions. In this paper, we present a convolutional neural network called DPSNet (Deep Plane Sweep Network) whose design is inspired by best practices of traditional geometry-based approaches for dense depth reconstruction. Rather than directly estimating depth and/or optical flow correspondence from image pairs as done in many previous deep learning methods, DPSNet takes a plane sweep approach that involves building a cost volume from deep features using the plane sweep algorithm, regularizing the cost volume via a context-aware cost aggregation, and regressing the dense depth map from the cost volume. The cost volume is constructed using a differentiable warping process that allows for end-to-end training of the network. Through the effective incorporation of conventional multiview stereo concepts within a deep learning framework, DPSNet achieves state-of-the-art reconstruction results on a variety of challenging datasets.",
            "year": 2019,
            "venue": "International Conference on Learning Representations",
            "authors": [
              {
                "authorId": "37579351",
                "name": "Sunghoon Im"
              },
              {
                "authorId": "39060641",
                "name": "Hae-Gon Jeon"
              },
              {
                "authorId": "145676588",
                "name": "Stephen Lin"
              },
              {
                "authorId": "2398271",
                "name": "In-So Kweon"
              }
            ]
          }
        },
        {
          "citedcorpusid": 53749928,
          "isinfluential": false,
          "contexts": [
            "…camera systems, consisting of an event camera and a frame camera , to solve long-standing challenges in various applications, including de-blur [23], HDR imaging [10], SLAM [30], etc. Recently, depth estimation from SAFE systems [32, 42] has also been explored, which aims to estimate accurate…"
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Bringing a Blurry Frame Alive at High Frame-Rate With an Event Camera",
            "abstract": "Event-based cameras can measure intensity changes (called ‘events’) with microsecond accuracy under high-speed motion and challenging lighting conditions. With the active pixel sensor (APS), the event camera allows simultaneous output of the intensity frames. However, the output images are captured at a relatively low frame-rate and often suffer from motion blur. A blurry image can be regarded as the integral of a sequence of latent images, while the events indicate the changes between the latent images. Therefore, we are able to model the blur-generation process by associating event data to a latent image. In this paper, we propose a simple and effective approach, the Event-based Double Integral (EDI) model, to reconstruct a high frame-rate, sharp video from a single blurry frame and its event data. The video generation is based on solving a simple non-convex optimization problem in a single scalar variable. Experimental results on both synthetic and real images demonstrate the superiority of our EDI model and optimization method in comparison to the state-of-the-art.",
            "year": 2018,
            "venue": "Computer Vision and Pattern Recognition",
            "authors": [
              {
                "authorId": "4589043",
                "name": "Liyuan Pan"
              },
              {
                "authorId": "51939028",
                "name": "C. Scheerlinck"
              },
              {
                "authorId": "1490933487",
                "name": "Xin Yu"
              },
              {
                "authorId": "143750012",
                "name": "R. Hartley"
              },
              {
                "authorId": "65795460",
                "name": "Miaomiao Liu"
              },
              {
                "authorId": "1681554",
                "name": "Yuchao Dai"
              }
            ]
          }
        },
        {
          "citedcorpusid": 118684904,
          "isinfluential": false,
          "contexts": [
            "Due to the unique working principle, event cameras present several attractive advantages, including high dynamic range ( > 120 dB), high temporal resolution (in the order of microsecond), etc [6]."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Event-Based Vision: A Survey",
            "abstract": "Event cameras are bio-inspired sensors that differ from conventional frame cameras: Instead of capturing images at a fixed rate, they asynchronously measure per-pixel brightness changes, and output a stream of events that encode the time, location and sign of the brightness changes. Event cameras offer attractive properties compared to traditional cameras: high temporal resolution (in the order of $\\mu$μs), very high dynamic range (140 dB versus 60 dB), low power consumption, and high pixel bandwidth (on the order of kHz) resulting in reduced motion blur. Hence, event cameras have a large potential for robotics and computer vision in challenging scenarios for traditional cameras, such as low-latency, high speed, and high dynamic range. However, novel methods are required to process the unconventional output of these sensors in order to unlock their potential. This paper provides a comprehensive overview of the emerging field of event-based vision, with a focus on the applications and the algorithms developed to unlock the outstanding properties of event cameras. We present event cameras from their working principle, the actual sensors that are available and the tasks that they have been used for, from low-level vision (feature detection and tracking, optic flow, etc.) to high-level vision (reconstruction, segmentation, recognition). We also discuss the techniques developed to process events, including learning-based techniques, as well as specialized processors for these novel sensors, such as spiking neural networks. Additionally, we highlight the challenges that remain to be tackled and the opportunities that lie ahead in the search for a more efficient, bio-inspired way for machines to perceive and interact with the world.",
            "year": 2019,
            "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
            "authors": [
              {
                "authorId": "144036711",
                "name": "Guillermo Gallego"
              },
              {
                "authorId": "1694635",
                "name": "T. Delbrück"
              },
              {
                "authorId": "33780923",
                "name": "G. Orchard"
              },
              {
                "authorId": "1897771",
                "name": "C. Bartolozzi"
              },
              {
                "authorId": "1736425",
                "name": "B. Taba"
              },
              {
                "authorId": "1860631",
                "name": "A. Censi"
              },
              {
                "authorId": "2864731",
                "name": "Stefan Leutenegger"
              },
              {
                "authorId": "2052135690",
                "name": "A. Davison"
              },
              {
                "authorId": "3302681",
                "name": "J. Conradt"
              },
              {
                "authorId": "1751586",
                "name": "Kostas Daniilidis"
              },
              {
                "authorId": "2075371",
                "name": "D. Scaramuzza"
              }
            ]
          }
        },
        {
          "citedcorpusid": 212675709,
          "isinfluential": false,
          "contexts": [
            "We expect our method could generalize to other stereo asymmetric systems and leave it as a future work."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Softmax Splatting for Video Frame Interpolation",
            "abstract": "Differentiable image sampling in the form of backward warping has seen broad adoption in tasks like depth estimation and optical flow prediction. In contrast, how to perform forward warping has seen less attention, partly due to additional challenges such as resolving the conflict of mapping multiple pixels to the same target location in a differentiable way. We propose softmax splatting to address this paradigm shift and show its effectiveness on the application of frame interpolation. Specifically, given two input frames, we forward-warp the frames and their feature pyramid representations based on an optical flow estimate using softmax splatting. In doing so, the softmax splatting seamlessly handles cases where multiple source pixels map to the same target location. We then use a synthesis network to predict the interpolation result from the warped representations. Our softmax splatting allows us to not only interpolate frames at an arbitrary time but also to fine tune the feature pyramid and the optical flow. We show that our synthesis approach, empowered by softmax splatting, achieves new state-of-the-art results for video frame interpolation.",
            "year": 2020,
            "venue": "Computer Vision and Pattern Recognition",
            "authors": [
              {
                "authorId": "39644974",
                "name": "Simon Niklaus"
              },
              {
                "authorId": "40513795",
                "name": "Feng Liu"
              }
            ]
          }
        },
        {
          "citedcorpusid": 232170230,
          "isinfluential": false,
          "contexts": [
            "However, modality asymmetry can not be eliminated or even mitigated in certain challenging regions due to the inevitable information absence of one modality, e.g. , high dynamic range regions for frame cameras and regions inside object contours for event cameras."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "DSEC: A Stereo Event Camera Dataset for Driving Scenarios",
            "abstract": "Once an academic venture, autonomous driving has received unparalleled corporate funding in the last decade. Still, operating conditions of current autonomous cars are mostly restricted to ideal scenarios. This means that driving in challenging illumination conditions such as night, sunrise, and sunset remains an open problem. In these cases, standard cameras are being pushed to their limits in terms of low light and high dynamic range performance. To address these challenges, we propose, DSEC, a new dataset that contains such demanding illumination conditions and provides a rich set of sensory data. DSEC offers data from a wide-baseline stereo setup of two color frame cameras and two high-resolution monochrome event cameras. In addition, we collect lidar data and RTK GPS measurements, both hardware synchronized with all camera data. One of the distinctive features of this dataset is the inclusion of high-resolution event cameras. Event cameras have received increasing attention for their high temporal resolution and high dynamic range performance. However, due to their novelty, event camera datasets in driving scenarios are rare. This work presents the first high resolution, large scale stereo dataset with event cameras. The dataset contains 53 sequences collected by driving in a variety of illumination conditions and provides ground truth disparity for the development and evaluation of event-based stereo algorithms.",
            "year": 2021,
            "venue": "IEEE Robotics and Automation Letters",
            "authors": [
              {
                "authorId": "8329387",
                "name": "Mathias Gehrig"
              },
              {
                "authorId": "2052356146",
                "name": "Willem Aarents"
              },
              {
                "authorId": "51152279",
                "name": "Daniel Gehrig"
              },
              {
                "authorId": "2075371",
                "name": "D. Scaramuzza"
              }
            ]
          }
        },
        {
          "citedcorpusid": 232478376,
          "isinfluential": false,
          "contexts": [
            "E-SfM with two source views ( E Rt and E Rt − 1 ) could be recognized as a two-view SfM problem [31] with known camera poses ( e.g. , from the inertial measurement unit, IMU)."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Deep Two-View Structure-from-Motion Revisited",
            "abstract": "Two-view structure-from-motion (SfM) is the cornerstone of 3D reconstruction and visual SLAM. Existing deep learning-based approaches formulate the problem by either recovering absolute pose scales from two consecutive frames or predicting a depth map from a single image, both of which are ill-posed problems. In contrast, we propose to revisit the problem of deep two-view SfM by leveraging the well-posedness of the classic pipeline. Our method consists of 1) an optical flow estimation network that predicts dense correspondences between two frames; 2) a normalized pose estimation module that computes relative camera poses from the 2D optical flow correspondences, and 3) a scale-invariant depth estimation network that leverages epipolar geometry to reduce the search space, refine the dense correspondences, and estimate relative depth maps. Extensive experiments show that our method outperforms all state-of-the-art two-view SfM methods by a clear margin on KITTI depth, KITTI VO, MVS, Scenes11, and SUN3D datasets in both relative pose and depth estimation.",
            "year": 2021,
            "venue": "Computer Vision and Pattern Recognition",
            "authors": [
              {
                "authorId": "1832343458",
                "name": "Jianyuan Wang"
              },
              {
                "authorId": "2015152",
                "name": "Yiran Zhong"
              },
              {
                "authorId": "1681554",
                "name": "Yuchao Dai"
              },
              {
                "authorId": "2238841",
                "name": "Stan Birchfield"
              },
              {
                "authorId": "3397429",
                "name": "Kaihao Zhang"
              },
              {
                "authorId": "2887475",
                "name": "Nikolai Smolyanskiy"
              },
              {
                "authorId": "46382489",
                "name": "Hongdong Li"
              }
            ]
          }
        },
        {
          "citedcorpusid": 245300947,
          "isinfluential": false,
          "contexts": [
            "…of an event camera and a frame camera , to solve long-standing challenges in various applications, including de-blur [23], HDR imaging [10], SLAM [30], etc. Recently, depth estimation from SAFE systems [32, 42] has also been explored, which aims to estimate accurate depth in various conditions.",
            "Compared with cross-modal symmetric stereo systems, asymmetric stereo systems with a single modality on one side, e.g., frame-event [9,32,42] and RGB-NIR [16,38] systems, possess the same sensing capabilities and come with half costs.",
            "Therefore, FF-PSMNet can not reveal the 3D geometry in these regions as accurately as SAFE-Ours and EE-DDES."
          ],
          "intents": [
            "['background']",
            "['background']",
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Accurate depth estimation from a hybrid event-RGB stereo setup",
            "abstract": "Event-based visual perception is becoming increasingly popular owing to interesting sensor characteristics enabling the handling of difficult conditions such as highly dynamic motion or challenging illumination. The mostly complementary nature of event cameras however still means that best results are achieved if the sensor is paired with a regular frame-based sensor. The present work aims at answering a simple question: Assuming that both cameras do not share a common optical center, is it possible to exploit the hybrid stereo setup's baseline to perform accurate stereo depth estimation? We present a learning based solution to this problem leveraging modern spatio-temporal input representations as well as a novel hybrid pyramid attention module. Results on real data demonstrate competitive performance against pure frame-based stereo alternatives as well as the ability to maintain the advantageous properties of event-based sensors.",
            "year": 2021,
            "venue": "IEEE/RJS International Conference on Intelligent RObots and Systems",
            "authors": [
              {
                "authorId": "2136622420",
                "name": "Yihao Zuo"
              },
              {
                "authorId": "2114843191",
                "name": "Li Cui"
              },
              {
                "authorId": "3428000",
                "name": "Xin-Zhong Peng"
              },
              {
                "authorId": "2110198892",
                "name": "Yanyu Xu"
              },
              {
                "authorId": "1702868",
                "name": "Shenghua Gao"
              },
              {
                "authorId": "2155465459",
                "name": "Xia Wang"
              },
              {
                "authorId": "1727013",
                "name": "L. Kneip"
              }
            ]
          }
        },
        {
          "citedcorpusid": 249980412,
          "isinfluential": false,
          "contexts": [
            "More recently, cross-modal symmetric stereo systems with cameras of different modalities on both sides have been proposed and demonstrate distinct performance in various scenarios [4,20,21]."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Stereo Depth from Events Cameras: Concentrate and Focus on the Future",
            "abstract": "Neuromorphic cameras or event cameras mimic human vision by reporting changes in the intensity in a scene, instead of reporting the whole scene at once in a form of an image frame as performed by conventional cameras. Events are streamed data that are often dense when either the scene changes or the camera moves rapidly. The rapid movement causes the events to be overridden or missed when creating a tensor for the machine to learn on. To alleviate the event missing or overriding issue, we propose to learn to concentrate on the dense events to produce a compact event representation with high details for depth estimation. Specifically, we learn a model with events from both past and future but infer only with past data with the predicted future. We initially estimate depth in an event-only setting but also propose to further incorporate images and events by a hier-archical event and intensity combination network for better depth estimation. By experiments in challenging real-world scenarios, we validate that our method outperforms prior arts even with low computational cost. Code is available at: https://github.com/yonseivnl/se-cff.",
            "year": 2022,
            "venue": "Computer Vision and Pattern Recognition",
            "authors": [
              {
                "authorId": "1830605424",
                "name": "Yeongwoo Nam"
              },
              {
                "authorId": "114141661",
                "name": "Mohammad Mostafavi"
              },
              {
                "authorId": "51182421",
                "name": "Kuk-Jin Yoon"
              },
              {
                "authorId": "2119579051",
                "name": "Jonghyun Choi"
              }
            ]
          }
        },
        {
          "citedcorpusid": 253513043,
          "isinfluential": false,
          "contexts": [
            "More recently, cross-modal symmetric stereo systems with cameras of different modalities on both sides have been proposed and demonstrate distinct performance in various scenarios [4,20,21]."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Selection and Cross Similarity for Event-Image Deep Stereo",
            "abstract": "",
            "year": 2022,
            "venue": "European Conference on Computer Vision",
            "authors": [
              {
                "authorId": "2148275179",
                "name": "Hoonhee Cho"
              },
              {
                "authorId": "51182421",
                "name": "Kuk-Jin Yoon"
              }
            ]
          }
        },
        {
          "citedcorpusid": 253651036,
          "isinfluential": false,
          "contexts": [
            "To conduct stereo matching from a pair of frame and event images (converted from event streams) with signiﬁcant asymmetry, existing methods pro-pose to normalize [32] or transform [15] different modalities to a uniﬁed form.",
            "For example, edge images and temporal gradient images are adopted to normalize frame and event images [15, 32], while transformation networks are proposed to make up the photometric inconsistency of RGB and NIR images [16, 38]."
          ],
          "intents": [
            "['methodology']",
            "--"
          ],
          "cited_paper_info": {
            "title": "Real-Time Hetero-Stereo Matching for Event and Frame Camera With Aligned Events Using Maximum Shift Distance",
            "abstract": "Event cameras can show better performance than frame cameras in challenging scenarios, such as fast-moving environments or high-dynamic-range scenes. However, it is still difficult for event cameras to replace frame cameras in non-challenging normal scenarios. In order to leverage the advantages of both cameras, we conduct a study for the heterogeneous stereo camera system which employs both an event and a frame camera. The proposed system estimates the semi-dense disparity in real-time by matching heterogeneous data of an event and a frame camera in stereo. We propose an accurate, intuitive and efficient way to align events with 6-DOF camera motion, by suggesting the maximum shift distance method. The aligned event image shows high similarity to the edge image of the frame camera. The proposed method can estimate poses of an event camera and depth of events in a few frames, which can speed up the initialization of the event camera system. We verified our algorithm in the DSEC dataset. The proposed hetero-stereo matching outperformed other methods. For real-time operation, we implemented our code using parallel computation with CUDA and release our code open source:",
            "year": 2023,
            "venue": "IEEE Robotics and Automation Letters",
            "authors": [
              {
                "authorId": "2108880866",
                "name": "Haram Kim"
              },
              {
                "authorId": "2108077736",
                "name": "Sangil Lee"
              },
              {
                "authorId": "2125035466",
                "name": "Junha Kim"
              },
              {
                "authorId": "2161495857",
                "name": "H. J. Kim"
              }
            ]
          }
        }
      ]
    },
    "277994207": {
      "citing_paper_info": {
        "title": "DERD-Net: Learning Depth from Event-based Ray Densities",
        "abstract": "Event cameras offer a promising avenue for multi-view stereo depth estimation and Simultaneous Localization And Mapping (SLAM) due to their ability to detect blur-free 3D edges at high-speed and over broad illumination conditions. However, traditional deep learning frameworks designed for conventional cameras struggle with the asynchronous, stream-like nature of event data, as their architectures are optimized for discrete, image-like inputs. We propose a scalable, flexible and adaptable framework for pixel-wise depth estimation with event cameras in both monocular and stereo setups. The 3D scene structure is encoded into disparity space images (DSIs), representing spatial densities of rays obtained by back-projecting events into space via known camera poses. Our neural network processes local subregions of the DSIs combining 3D convolutions and a recurrent structure to recognize valuable patterns for depth prediction. Local processing enables fast inference with full parallelization and ensures constant ultra-low model complexity and memory costs, regardless of camera resolution. Experiments on standard benchmarks (MVSEC and DSEC datasets) demonstrate unprecedented effectiveness: (i) using purely monocular data, our method achieves comparable results to existing stereo methods; (ii) when applied to stereo data, it strongly outperforms all state-of-the-art (SOTA) approaches, reducing the mean absolute error by at least 42%; (iii) our method also allows for increases in depth completeness by more than 3-fold while still yielding a reduction in median absolute error of at least 30%. Given its remarkable performance and effective processing of event-data, our framework holds strong potential to become a standard approach for using deep learning for event-based depth estimation and SLAM. Project page: https://github.com/tub-rip/DERD-Net",
        "year": 2025,
        "venue": "arXiv.org",
        "authors": [
          {
            "authorId": "2356787934",
            "name": "Diego de Oliveira Hitzges"
          },
          {
            "authorId": "2155615482",
            "name": "Suman Ghosh"
          },
          {
            "authorId": "2349236480",
            "name": "Guillermo Gallego"
          }
        ]
      },
      "citation_summary": {
        "citation_count": 13,
        "unique_cited_count": 12,
        "influential_count": 3,
        "detailed_records_count": 13
      },
      "cited_papers": [
        "250374739",
        "250918780",
        "262638843",
        "3416874",
        "198229801",
        "49554392",
        "215799961",
        "4412139",
        "226298400",
        "6539071",
        "219303641",
        "250699235"
      ],
      "citation_details": [
        {
          "citedcorpusid": 3416874,
          "isinfluential": true,
          "contexts": [
            "Qualitative comparison of depth estimated using the MC-EMVS method [13], applying it to the new selected pixels F denser and our method DERD-Net, for the MVSEC indoor flying [48] (top 3 rows) and DSEC zurich city 04 a (bottom row) sequences.",
            "In this section, we describe the experiments conducted on the indoor flying sequences 1,2,3 of the MVSEC dataset [48] to evaluate the performance of the proposed depth estimation method.",
            "Following prior protocols, we conduct experiments on the MVSEC [48] and the DSEC [10] datasets.",
            "Moreover, the scarcity of event camera datasets with ground truth depth [14, 48] results in limited training data, which can lead to overfitting [38].",
            "• Comprehensive Experiments : We evaluate our model using both monocular and stereo data from standard datasets MVSEC [48] and DSEC [10], employing cross-validation, outperforming the state of the art by a large margin on ten figures of merit."
          ],
          "intents": [
            "--",
            "--",
            "--",
            "--",
            "--"
          ],
          "cited_paper_info": {
            "title": "The Multivehicle Stereo Event Camera Dataset: An Event Camera Dataset for 3D Perception",
            "abstract": "Event-based cameras are a new passive sensing modality with a number of benefits over traditional cameras, including extremely low latency, asynchronous data acquisition, high dynamic range, and very low power consumption. There has been a lot of recent interest and development in applying algorithms to use the events to perform a variety of three-dimensional perception tasks, such as feature tracking, visual odometry, and stereo depth estimation. However, there currently lacks the wealth of labeled data that exists for traditional cameras to be used for both testing and development. In this letter, we present a large dataset with a synchronized stereo pair event based camera system, carried on a handheld rig, flown by a hexacopter, driven on top of a car, and mounted on a motorcycle, in a variety of different illumination levels and environments. From each camera, we provide the event stream, grayscale images, and inertial measurement unit (IMU) readings. In addition, we utilize a combination of IMU, a rigidly mounted lidar system, indoor and outdoor motion capture, and GPS to provide accurate pose and depth images for each camera at up to 100 Hz. For comparison, we also provide synchronized grayscale images and IMU readings from a frame-based stereo camera system.",
            "year": 2018,
            "venue": "IEEE Robotics and Automation Letters",
            "authors": [
              {
                "authorId": "3385588",
                "name": "A. Z. Zhu"
              },
              {
                "authorId": "144964367",
                "name": "Dinesh Thakur"
              },
              {
                "authorId": "2520604",
                "name": "Tolga Özaslan"
              },
              {
                "authorId": "39832696",
                "name": "Bernd Pfrommer"
              },
              {
                "authorId": "37956314",
                "name": "Vijay R. Kumar"
              },
              {
                "authorId": "1751586",
                "name": "Kostas Daniilidis"
              }
            ]
          }
        },
        {
          "citedcorpusid": 4412139,
          "isinfluential": false,
          "contexts": [
            "Recent approaches have addressed stereo event-based 3D reconstruction for VO and SLAM [7, 15, 35, 45–47]."
          ],
          "intents": [
            "--"
          ],
          "cited_paper_info": {
            "title": "Realtime Time Synchronized Event-based Stereo",
            "abstract": "In this work, we propose a novel event based stereo method which addresses the problem of motion blur for a moving event camera. Our method uses the velocity of the camera and a range of disparities to synchronize the positions of the events, as if they were captured at a single point in time. We represent these events using a pair of novel time synchronized event disparity volumes, which we show remove motion blur for pixels at the correct disparity in the volume, while further blurring pixels at the wrong disparity. We then apply a novel matching cost over these time synchronized event disparity volumes, which both rewards similarity between the volumes while penalizing blurriness. We show that our method outperforms more expensive, smoothing based event stereo methods, by evaluating on the Multi Vehicle Stereo Event Camera dataset.",
            "year": 2018,
            "venue": "European Conference on Computer Vision",
            "authors": [
              {
                "authorId": "3385588",
                "name": "A. Z. Zhu"
              },
              {
                "authorId": "2116435960",
                "name": "Yibo Chen"
              },
              {
                "authorId": "1751586",
                "name": "Kostas Daniilidis"
              }
            ]
          }
        },
        {
          "citedcorpusid": 6539071,
          "isinfluential": false,
          "contexts": [
            "Stereo depth estimation with event cameras has been an intriguing problem since the invention of the first event camera by Mahowald and Mead in the 1990s [9, 14, 26] due to their potential for high temporal resolution and robustness to motion blur."
          ],
          "intents": [
            "--"
          ],
          "cited_paper_info": {
            "title": "Cooperative",
            "abstract": "",
            "year": 1949,
            "venue": "",
            "authors": [
              {
                "authorId": "2345879664",
                "name": "Zhou Meng"
              },
              {
                "authorId": "2344759731",
                "name": "Wang Chang"
              },
              {
                "authorId": "2344811636",
                "name": "Wang Jing"
              },
              {
                "authorId": "2344761276",
                "name": "Wang Li"
              }
            ]
          }
        },
        {
          "citedcorpusid": 49554392,
          "isinfluential": false,
          "contexts": [
            "The Generalized Time-Based Stereovision (GTS) method [18] utilizes a two-step process: first performing stereo matching based on a time-consistency score for each event, followed by depth estimation through triangulation."
          ],
          "intents": [
            "--"
          ],
          "cited_paper_info": {
            "title": "Neuromorphic Event-Based Generalized Time-Based Stereovision",
            "abstract": "3D reconstruction from multiple viewpoints is an important problem in machine vision that allows recovering tridimensional structures from multiple two-dimensional views of a given scene. Reconstructions from multiple views are conventionally achieved through a process of pixel luminance-based matching between different views. Unlike conventional machine vision methods that solve matching ambiguities by operating only on spatial constraints and luminance, this paper introduces a fully time-based solution to stereovision using the high temporal resolution of neuromorphic asynchronous event-based cameras. These cameras output dynamic visual information in the form of what is known as “change events” that encode the time, the location and the sign of the luminance changes. A more advanced event-based camera, the Asynchronous Time-based Image Sensor (ATIS), in addition of change events, encodes absolute luminance as time differences. The stereovision problem can then be formulated solely in the time domain as a problem of events coincidences detection problem. This work is improving existing event-based stereovision techniques by adding luminance information that increases the matching reliability. It also introduces a formulation that does not require to build local frames (though it is still possible) from the luminances which can be costly to implement. Finally, this work also introduces a methodology for time based stereovision in the context of binocular and trinocular configurations using time based event matching criterion combining for the first time all together: space, time, luminance, and motion.",
            "year": 2018,
            "venue": "Frontiers in Neuroscience",
            "authors": [
              {
                "authorId": "144975525",
                "name": "S. Ieng"
              },
              {
                "authorId": "2057119545",
                "name": "J. Carneiro"
              },
              {
                "authorId": "2598816",
                "name": "Marc Osswald"
              },
              {
                "authorId": "1750848",
                "name": "R. Benosman"
              }
            ]
          }
        },
        {
          "citedcorpusid": 198229801,
          "isinfluential": false,
          "contexts": [
            "Deep learning has significantly advanced depth estimation in traditional monocular and stereo camera setups, achieving remarkable results [20, 21, 37]."
          ],
          "intents": [
            "--"
          ],
          "cited_paper_info": {
            "title": "From Big to Small: Multi-Scale Local Planar Guidance for Monocular Depth Estimation",
            "abstract": "Estimating accurate depth from a single image is challenging because it is an ill-posed problem as infinitely many 3D scenes can be projected to the same 2D scene. However, recent works based on deep convolutional neural networks show great progress with plausible results. The convolutional neural networks are generally composed of two parts: an encoder for dense feature extraction and a decoder for predicting the desired depth. In the encoder-decoder schemes, repeated strided convolution and spatial pooling layers lower the spatial resolution of transitional outputs, and several techniques such as skip connections or multi-layer deconvolutional networks are adopted to recover back to the original resolution for effective dense prediction. In this paper, for more effective guidance of densely encoded features to the desired depth prediction, we propose a network architecture that utilizes novel local planar guidance layers located at multiple stages in the decoding phase. We show that the proposed method outperforms the state-of-the-art works with significant margin evaluating on challenging benchmarks. We also provide results from an ablation study to validate the effectiveness of the proposed method.",
            "year": 2019,
            "venue": "arXiv.org",
            "authors": [
              {
                "authorId": "2108559570",
                "name": "Jin Han Lee"
              },
              {
                "authorId": "145312236",
                "name": "Myung-Kyu Han"
              },
              {
                "authorId": "2284233",
                "name": "D. W. Ko"
              },
              {
                "authorId": "8574373",
                "name": "I. Suh"
              }
            ]
          }
        },
        {
          "citedcorpusid": 215799961,
          "isinfluential": false,
          "contexts": [
            "Unlike conventional cameras, event cameras operate asynchronously, detecting per-pixel brightness changes [8, 9, 22]."
          ],
          "intents": [
            "--"
          ],
          "cited_paper_info": {
            "title": "5.10 A 1280×720 Back-Illuminated Stacked Temporal Contrast Event-Based Vision Sensor with 4.86µm Pixels, 1.066GEPS Readout, Programmable Event-Rate Controller and Compressive Data-Formatting Pipeline",
            "abstract": "Event-based (EB) vision sensors pixel-individually detect temporal contrast exceeding a preset relative threshold [1], [2] to follow the temporal evolution of relative light changes (contrast detection, CD) and to define sampling points for frame-free pixel-level measurement of absolute intensity (exposure measurement, EM) [3], [4]. EB sensors gain popularity in high-speed low-power machine vision applications thanks to temporal precision of recorded data, inherent suppression of temporal redundancy resulting in reduced post-processing cost, and wide intra-scene dynamic range operation.",
            "year": 2020,
            "venue": "IEEE International Solid-State Circuits Conference",
            "authors": [
              {
                "authorId": "9413276",
                "name": "T. Finateu"
              },
              {
                "authorId": "38934516",
                "name": "Atsumi Niwa"
              },
              {
                "authorId": "1758423",
                "name": "D. Matolin"
              },
              {
                "authorId": "1637236417",
                "name": "Koya Tsuchimoto"
              },
              {
                "authorId": "49886151",
                "name": "A. Mascheroni"
              },
              {
                "authorId": "2083159785",
                "name": "Etienne Reynaud"
              },
              {
                "authorId": "2920529",
                "name": "P. Mostafalu"
              },
              {
                "authorId": "40792639",
                "name": "F. Brady"
              },
              {
                "authorId": "3444944",
                "name": "L. Chotard"
              },
              {
                "authorId": "94271083",
                "name": "F. L. Goff"
              },
              {
                "authorId": "38884358",
                "name": "H. Takahashi"
              },
              {
                "authorId": "145762314",
                "name": "H. Wakabayashi"
              },
              {
                "authorId": "3228095",
                "name": "Y. Oike"
              },
              {
                "authorId": "153466606",
                "name": "C. Posch"
              }
            ]
          }
        },
        {
          "citedcorpusid": 219303641,
          "isinfluential": false,
          "contexts": [
            "Deep learning has significantly advanced depth estimation in traditional monocular and stereo camera setups, achieving remarkable results [20, 21, 37]."
          ],
          "intents": [
            "--"
          ],
          "cited_paper_info": {
            "title": "A Survey on Deep Learning Techniques for Stereo-Based Depth Estimation",
            "abstract": "Estimating depth from RGB images is a long-standing ill-posed problem, which has been explored for decades by the computer vision, graphics, and machine learning communities. Among the existing techniques, stereo matching remains one of the most widely used in the literature due to its strong connection to the human binocular system. Traditionally, stereo-based depth estimation has been addressed through matching hand-crafted features across multiple images. Despite the extensive amount of research, these traditional techniques still suffer in the presence of highly textured areas, large uniform regions, and occlusions. Motivated by their growing success in solving various 2D and 3D vision problems, deep learning for stereo-based depth estimation has attracted a growing interest from the community, with more than 150 papers published in this area between 2014 and 2019. This new generation of methods has demonstrated a significant leap in performance, enabling applications such as autonomous driving and augmented reality. In this paper, we provide a comprehensive survey of this new and continuously growing field of research, summarize the most commonly used pipelines, and discuss their benefits and limitations. In retrospect of what has been achieved so far, we also conjecture what the future may hold for deep learning-based stereo for depth estimation research.",
            "year": 2020,
            "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
            "authors": [
              {
                "authorId": "47028380",
                "name": "Hamid Laga"
              },
              {
                "authorId": "51895170",
                "name": "Laurent Valentin Jospin"
              },
              {
                "authorId": "2795743",
                "name": "F. Boussaïd"
              },
              {
                "authorId": "1698675",
                "name": "Bennamoun"
              }
            ]
          }
        },
        {
          "citedcorpusid": 226298400,
          "isinfluential": false,
          "contexts": [
            "Finally, we report δ -accuracy values, which indicate the percentage of points whose estimated depth falls within specified limits relative to GT [41]."
          ],
          "intents": [
            "--"
          ],
          "cited_paper_info": {
            "title": "Unsupervised Learning of Dense Optical Flow, Depth and Egomotion with Event-Based Sensors",
            "abstract": "We present an unsupervised learning pipeline for dense depth, optical flow and egomotion estimation for autonomous driving applications, using the event-based output of the Dynamic Vision Sensor (DVS) as input. The backbone of our pipeline is a bioinspired encoder-decoder neural network architecture - ECN. To train the pipeline, we introduce a covariance normalization technique which resembles the lateral inhibition mechanism found in animal neural systems.Our work is the first monocular pipeline that generates dense depth and optical flow from sparse event data only, and is able to transfer from day to night scenes without any additional training. The network works in self-supervised mode and has just 150k parameters. We evaluate our pipeline on the MVSEC self driving dataset and present results for depth, optical flow and and egomotion estimation. Thanks to the efficient design, we are able to achieve inference rates of 300 FPS on a single Nvidia 1080Ti GPU. Our experiments demonstrate significant improvements upon works that used deep learning on event data, as well as the ability to perform well during both day and night.",
            "year": 2020,
            "venue": "IEEE/RJS International Conference on Intelligent RObots and Systems",
            "authors": [
              {
                "authorId": "3300969",
                "name": "Chengxi Ye"
              },
              {
                "authorId": "144559298",
                "name": "A. Mitrokhin"
              },
              {
                "authorId": "1759899",
                "name": "C. Fermüller"
              },
              {
                "authorId": "9861772",
                "name": "J. Yorke"
              },
              {
                "authorId": "1697493",
                "name": "Y. Aloimonos"
              }
            ]
          }
        },
        {
          "citedcorpusid": 250374739,
          "isinfluential": false,
          "contexts": [
            "For stereo depth estimation, [38, 40] present two pio-neering studies.",
            "Specifically, DDES [38] introduced the first deep-learning–based supervised stereo-matching method, while [40] proposed the first unsupervised learning framework."
          ],
          "intents": [
            "--",
            "--"
          ],
          "cited_paper_info": {
            "title": "Unsupervised Deep Event Stereo for Depth Estimation",
            "abstract": "Bio-inspired event cameras have been considered effective alternatives to traditional frame-based cameras for stereo depth estimation, especially in challenging conditions such as low-light or high-speed environments. Recently, deep learning-based supervised event stereo matching methods have achieved significant performance improvements over the traditional event stereo methods. However, the supervised methods depend on ground-truth disparity maps for training, and it is difficult to secure a large amount of ground-truth disparity maps. A feasible alternative is to devise an unsupervised event stereo method that can be trained without ground-truth disparity maps. To this end, we propose the first unsupervised event stereo matching method that can predict dense disparity maps, and is trained by transforming the depth estimation problem into a warping-based reconstruction problem. We propose a novel unsupervised loss function that enforces the network to minimize the feature-level epipolar correlation difference between the ground-truth intensity images and warped images. Moreover, we propose a novel event embedding mechanism that utilizes both temporal and spatial neighboring events to capture spatio-temporal relationships among the events for stereo matching. Experimental results reveal that the proposed method outperforms the baseline unsupervised methods by significant margins (e.g., up to 16.88% improvement) and achieves comparable results with the existing supervised methods. Extensive ablation studies validate the efficacy of the proposed modules and architectural choices.",
            "year": 2022,
            "venue": "IEEE transactions on circuits and systems for video technology (Print)",
            "authors": [
              {
                "authorId": "40621769",
                "name": "S M Nadim Uddin"
              },
              {
                "authorId": "2175478044",
                "name": "Soikat Hasan Ahmed"
              },
              {
                "authorId": "48895895",
                "name": "Yong Ju Jung"
              }
            ]
          }
        },
        {
          "citedcorpusid": 250699235,
          "isinfluential": false,
          "contexts": [
            "Recent approaches have addressed stereo event-based 3D reconstruction for VO and SLAM [7, 15, 35, 45–47]."
          ],
          "intents": [
            "--"
          ],
          "cited_paper_info": {
            "title": "Secrets of Event-Based Optical Flow",
            "abstract": "Event cameras respond to scene dynamics and offer advantages to estimate motion. Following recent image-based deep-learning achievements, optical flow estimation methods for event cameras have rushed to combine those image-based methods with event data. However, it requires several adaptations (data conversion, loss function, etc.) as they have very different properties. We develop a principled method to extend the Contrast Maximization framework to estimate optical flow from events alone. We investigate key elements: how to design the objective function to prevent overfitting, how to warp events to deal better with occlusions, and how to improve convergence with multi-scale raw events. With these key elements, our method ranks first among unsupervised methods on the MVSEC benchmark, and is competitive on the DSEC benchmark. Moreover, our method allows us to expose the issues of the ground truth flow in those benchmarks, and produces remarkable results when it is transferred to unsupervised learning settings. Our code is available at https://github.com/tub-rip/event_based_optical_flow",
            "year": 2022,
            "venue": "European Conference on Computer Vision",
            "authors": [
              {
                "authorId": "2066324243",
                "name": "Shintaro Shiba"
              },
              {
                "authorId": "1716469",
                "name": "Y. Aoki"
              },
              {
                "authorId": "144036711",
                "name": "Guillermo Gallego"
              }
            ]
          }
        },
        {
          "citedcorpusid": 250918780,
          "isinfluential": true,
          "contexts": [
            "For each interval, we construct two DSIs (one for each camera) and fuse them by applying voxel-wise metrics (e.g., harmonic mean) as described in [13].",
            "MC-EMVS [13] introduced a novel stereo approach for depth estimation which does not require explicit data association, using DSIs generated from stereo events cameras.",
            "A confidence map is generated by projecting the DSI onto a 2D grid of size W × H , where each pixel’s value represents the maximum ray density among all depth levels [13] (pixel selection map in Fig.",
            "The performance of the networks is evaluated using ten standard metrics commonly employed in depth estimation tasks [13].",
            "To test this hypothesis, we used a larger filter window size of 9 × 9 px and a subtractive constant of C denser = − 10 , re-5 Scene Pixel selection map MC-EMVS [13] MC-EMVS + F denser DERD-Net (Ours) Ground truth (GT) Figure 3.",
            "Subsequently, we retrained it on stereo DSIs fused via the harmonic mean and compared its performance to MC-EMVS [13].",
            "The Multi-Camera Event-based Multi-View Stereo (MC-EMVS) method has recently produced state-of-the-art (SOTA) results, outperforming other techniques in depth benchmarks across several metrics [13].",
            "The two closest baseline methods for performance comparison of our method are EMVS for monocular vision [31] and MC-EMVS for stereo vision [13].",
            "The objective is to capture local geometrical patterns in the Sub-DSI to extract more relevant depth information from it than the SOTA argmax approach used in [13, 31].",
            "Following [13, 31], this stream is sliced by dividing time into intervals."
          ],
          "intents": [
            "--",
            "--",
            "--",
            "--",
            "--",
            "--",
            "--",
            "--",
            "--",
            "--"
          ],
          "cited_paper_info": {
            "title": "Multi‐Event‐Camera Depth Estimation and Outlier Rejection by Refocused Events Fusion",
            "abstract": "Event cameras are bio‐inspired sensors that offer advantages over traditional cameras. They operate asynchronously, sampling the scene at microsecond resolution and producing a stream of brightness changes. This unconventional output has sparked novel computer vision methods to unlock the camera's potential. Here, the problem of event‐based stereo 3D reconstruction for SLAM is considered. Most event‐based stereo methods attempt to exploit the high temporal resolution of the camera and the simultaneity of events across cameras to establish matches and estimate depth. By contrast, this work investigates how to estimate depth without explicit data association by fusing disparity space images (DSIs) originated in efficient monocular methods. Fusion theory is developed and applied to design multi‐camera 3D reconstruction algorithms that produce state‐of‐the‐art results, as confirmed by comparisons with four baseline methods and tests on a variety of available datasets.",
            "year": 2022,
            "venue": "Advanced Intelligent Systems",
            "authors": [
              {
                "authorId": "2155615482",
                "name": "Suman Ghosh"
              },
              {
                "authorId": "144036711",
                "name": "Guillermo Gallego"
              }
            ]
          }
        },
        {
          "citedcorpusid": 262638843,
          "isinfluential": true,
          "contexts": [
            "For stereo depth estimation, [38, 40] present two pio-neering studies.",
            "No other method reports good generalization on “split 2” of MVSEC because of the difference in dynamic characteris-Method tics of events in training and testing on that split (as mentioned in [1, 38]).",
            "Specifically, DDES [38] introduced the first deep-learning–based supervised stereo-matching method, while [40] proposed the first unsupervised learning framework.",
            "Moreover, the scarcity of event camera datasets with ground truth depth [14, 48] results in limited training data, which can lead to overfitting [38]."
          ],
          "intents": [
            "--",
            "--",
            "--",
            "--"
          ],
          "cited_paper_info": {
            "title": "Learning an Event Sequence Embedding for Dense Event-Based Deep Stereo",
            "abstract": "Today, a frame-based camera is the sensor of choice for machine vision applications. However, these cameras, originally developed for acquisition of static images rather than for sensing of dynamic uncontrolled visual environments, suffer from high power consumption, data rate, latency and low dynamic range. An event-based image sensor addresses these drawbacks by mimicking a biological retina. Instead of measuring the intensity of every pixel in a fixed time-interval, it reports events of significant pixel intensity changes. Every such event is represented by its position, sign of change, and timestamp, accurate to the microsecond. Asynchronous event sequences require special handling, since traditional algorithms work only with synchronous, spatially gridded data. To address this problem we introduce a new module for event sequence embedding, for use in difference applications. The module builds a representation of an event sequence by firstly aggregating information locally across time, using a novel fully-connected layer for an irregularly sampled continuous domain, and then across discrete spatial domain. Based on this module, we design a deep learning-based stereo method for event-based cameras. The proposed method is the first learning-based stereo method for an event-based camera and the only method that produces dense results. We show that large performance increases on the Multi Vehicle Stereo Event Camera Dataset (MVSEC), which became the standard set for benchmarking of event-based stereo methods.",
            "year": 2019,
            "venue": "IEEE International Conference on Computer Vision",
            "authors": [
              {
                "authorId": "1823725",
                "name": "S. Tulyakov"
              },
              {
                "authorId": "2721983",
                "name": "F. Fleuret"
              },
              {
                "authorId": "40519282",
                "name": "Martin Kiefel"
              },
              {
                "authorId": "2871555",
                "name": "Peter Gehler"
              },
              {
                "authorId": "2058954687",
                "name": "Michael Hirsch"
              }
            ]
          }
        },
        {
          "citedcorpusid": null,
          "isinfluential": false,
          "contexts": [
            "Unlike conventional cameras, event cameras operate asynchronously, detecting per-pixel brightness changes [8, 9, 22]."
          ],
          "intents": [
            "--"
          ],
          "cited_paper_info": {}
        }
      ]
    },
    "258869142": {
      "citing_paper_info": {
        "title": "Frame of Events: A Low-latency Resource-efficient Approach for Stereo Depth Maps",
        "abstract": "Computer vision traditionally uses cameras that capture visual information as frames at periodic intervals. On the other hand, Dynamic Vision Sensors (DVS) capture temporal contrast (TC) in each pixel asynchronously and stream them serially. This paper proposes a hybrid approach to generate input visual data as ‘frame of events’ for a stereo vision pipeline. We demonstrate that using hybrid vision sensors that produce frames made up of TC events can achieve superior results in terms of low latency, less compute and low memory footprint as compared to the traditional cameras and the event-based DVS. The frame-of-events approach eliminates the latency and memory resources involved in the accumulation of asynchronous events into synchronous frames, while generating acceptable disparity maps for depth estimation. Benchmarking results show that the frame-of-events pipeline outperforms others with the least average latency per frame of 3.8 ms and least average memory usage per frame of 112.4 Kb, which amounts to 7.32% and 9.75% reduction when compared to traditional frame-based pipeline. Hence, the proposed method is suitable for missioncritical robotics applications that involve path planning and localization mapping in a resource-constrained environment, such as drone navigation and autonomous vehicles.",
        "year": 2023,
        "venue": "International Conference on Automation, Robotics and Applications",
        "authors": [
          {
            "authorId": "2183256618",
            "name": "Shanmuga Venkatachalam"
          },
          {
            "authorId": "150334818",
            "name": "Vijay Shankaran Vivekanand"
          },
          {
            "authorId": "3459360",
            "name": "R. Kubendran"
          }
        ]
      },
      "citation_summary": {
        "citation_count": 5,
        "unique_cited_count": 5,
        "influential_count": 0,
        "detailed_records_count": 5
      },
      "cited_papers": [
        "147709",
        "15357188",
        "23913692",
        "209202615",
        "2070927"
      ],
      "citation_details": [
        {
          "citedcorpusid": 147709,
          "isinfluential": false,
          "contexts": [
            "This step is based on the concept of epipolar geometry [1] which constructs an epipolar plane which primarily includes epipolar lines across the x coordinates.",
            "The traditional camera based stereo vision [1], [12], has seen more advancements in the recent works assisted by deep learning to achieve high resolution binocular vision",
            "The concept of stereo vision deeply relies on epipolar geometry [1] which will be briefly described in the following calibration step."
          ],
          "intents": [
            "['background']",
            "['methodology']",
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Spacetime stereo: a unifying framework for depth from triangulation",
            "abstract": "",
            "year": 2005,
            "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
            "authors": [
              {
                "authorId": "2111092372",
                "name": "James Davis"
              },
              {
                "authorId": "1752236",
                "name": "R. Ramamoorthi"
              },
              {
                "authorId": "7723706",
                "name": "S. Rusinkiewicz"
              }
            ]
          }
        },
        {
          "citedcorpusid": 2070927,
          "isinfluential": false,
          "contexts": [
            "The eDVS setup records the event-based data and yields two files in the aedat4 [2] format, each for the left and right cameras."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Frame-free dynamic digital vision",
            "abstract": "Conventional image sensors produce massive amounts of redundant data and are limited in temporal resolution by the frame rate. This paper reviews our recent breakthrough in the development of a high- performance spike-event based dynamic vision sensor (DVS) that discards the frame concept entirely, and then describes novel digital methods for efficient low-level filtering and feature extraction and high-level object tracking that are based on the DVS spike events. These methods filter events, label them, or use them for object tracking. Filtering reduces the number of events but improves the ratio of informative events. Labeling attaches additional interpretation to the events, e.g. orientation or local optical flow. Tracking uses the events to track moving objects. Processing occurs on an event-by-event basis and uses the event time and identity as the basis for computation. A common memory object for filtering and labeling is a spatial map of most recent past event times. Processing methods typically use these past event times together with the present event in integer branching logic to filter, label, or synthesize new events. These methods are straightforwardly computed on serial digital hardware, resulting in a new event- and timing-based approach for visual computation that efficiently integrates a neural style of computation with digital hardware. All code is open- sourced in the jAER project (jaer.wiki.sourceforge.net).",
            "year": 2008,
            "venue": "",
            "authors": [
              {
                "authorId": "1694635",
                "name": "T. Delbrück"
              }
            ]
          }
        },
        {
          "citedcorpusid": 15357188,
          "isinfluential": false,
          "contexts": [
            "In contrast, Dynamic Vision Sensors (DVS) [9] encode temporal contrast (TC), i."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "A 128×128 120db 30mw Asynchronous Vision Sensor That Responds to Relative Intensity Change",
            "abstract": "The frame-based architectures of most imagers are natural for making movies and pictures, they have significant drawbacks for machine vision. Short-latency vision problems require high frame rate, producing massive readout (e.g., >1GB/s from 352×288 pixels at 10kFrames/s [1]). Reducing the output to a manageable rate by using region-of-interest readout usually requires complex control strategies. Readout and processing of largely redundant data ultimately limit reductions in computational effort and power consumption. In this paper, a vision sensor is presented whose pixels asynchronously respond to events that represent relative changes in intensity. It operates largely independent of scene illumination, directly encodes object reflectance, and reduces redundancy while preserving precise timing information. Because output bandwidth is automatically dedicated to dynamic parts of the scene, the sensor is suitable for applications in surveillance and motion analysis. It improves on prior frame-based temporal difference detection imagers (e.g., [2]) by asynchronously responding to temporal contrast rather than absolute illumination, and on prior event-based imagers because they either do not reduce redundancy at all [3], reduce only spatial redundancy [4], have large FPN, slow response, and limited dynamic range [5], or have low contrast sensitivity [6].",
            "year": 2005,
            "venue": "",
            "authors": [
              {
                "authorId": "1744964",
                "name": "P. Lichtsteiner"
              },
              {
                "authorId": "2368354141",
                "name": "C. Posch"
              },
              {
                "authorId": "5548576",
                "name": "T. Delbruck"
              }
            ]
          }
        },
        {
          "citedcorpusid": 23913692,
          "isinfluential": false,
          "contexts": [
            "Stereo vision is the ability to construct a unified view of an object or scene, with two different vision sensors/cameras [11].",
            "This requires identifying and matching features in the left and right images of the same point in the visual scene that helps to derive information about how far away objects are, based solely on the relative positions of the object in the two sensors [11]."
          ],
          "intents": [
            "['background']",
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Stereo vision and strabismus",
            "abstract": "",
            "year": 2015,
            "venue": "Eye",
            "authors": [
              {
                "authorId": "1719063",
                "name": "J. Read"
              }
            ]
          }
        },
        {
          "citedcorpusid": 209202615,
          "isinfluential": false,
          "contexts": [
            "[7], render high quality 3-D reconstruction of stereo images [13] or MRI images [6], [10], etc."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Deep-Learning Assisted High-Resolution Binocular Stereo Depth Reconstruction",
            "abstract": "This work presents dense stereo reconstruction using high-resolution images for infrastructure inspections. The state-of-the-art stereo reconstruction methods, both learning and non-learning ones, consume too much computational resource on high-resolution data. Recent learning-based methods achieve top ranks on most benchmarks. However, they suffer from the generalization issue due to lack of task-specific training data. We propose to use a less resource demanding non-learning method, guided by a learning-based model, to handle high-resolution images and achieve accurate stereo reconstruction. The deep-learning model produces an initial disparity prediction with uncertainty for each pixel of the down-sampled stereo image pair. The uncertainty serves as a self-measurement of its generalization ability and the perpixel searching range around the initially predicted disparity. The downstream process performs a modified version of the Semi-Global Block Matching method with the up-sampled perpixel searching range. The proposed deep-learning assisted method is evaluated on the Middlebury dataset and high-resolution stereo images collected by our customized binocular stereo camera. The combination of learning and non-learning methods achieves better performance on 12 out of 15 cases of the Middlebury dataset. In our infrastructure inspection experiments, the average 3D reconstruction error is less than 0.004m.",
            "year": 2019,
            "venue": "IEEE International Conference on Robotics and Automation",
            "authors": [
              {
                "authorId": "2113665545",
                "name": "Yaoyu Hu"
              },
              {
                "authorId": "2893099",
                "name": "Weikun Zhen"
              },
              {
                "authorId": "32634992",
                "name": "S. Scherer"
              }
            ]
          }
        }
      ]
    },
    "53086261": {
      "citing_paper_info": {
        "title": "An Active Approach to Solving the Stereo Matching Problem using Event-Based Sensors",
        "abstract": "",
        "year": 2018,
        "venue": "International Symposium on Circuits and Systems",
        "authors": [
          {
            "authorId": "144247981",
            "name": "Julien N. P. Martel"
          },
          {
            "authorId": "2110565676",
            "name": "Jonathan Müller"
          },
          {
            "authorId": "3302681",
            "name": "J. Conradt"
          },
          {
            "authorId": "145809681",
            "name": "Yulia Sandamirskaya"
          }
        ]
      },
      "citation_summary": {
        "citation_count": 6,
        "unique_cited_count": 6,
        "influential_count": 1,
        "detailed_records_count": 6
      },
      "cited_papers": [
        "23276048",
        "10712214",
        "261497446",
        "22296005",
        "15357188",
        "3494469"
      ],
      "citation_details": [
        {
          "citedcorpusid": 3494469,
          "isinfluential": false,
          "contexts": [
            "Those can estimate depth from binocular parallax using two cameras [2] or motion [3], [4], from focus [5], [6], from shading [7], from occlusions [8], from linear perspective [9], and many others."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Real-Time Depth From Focus on a Programmable Focal Plane Processor",
            "abstract": "",
            "year": 2018,
            "venue": "IEEE Transactions on Circuits and Systems Part 1: Regular Papers",
            "authors": [
              {
                "authorId": "144247981",
                "name": "Julien N. P. Martel"
              },
              {
                "authorId": "3444613",
                "name": "Lorenz K. Muller"
              },
              {
                "authorId": "2691859",
                "name": "S. Carey"
              },
              {
                "authorId": "2110565676",
                "name": "Jonathan Müller"
              },
              {
                "authorId": "145809681",
                "name": "Yulia Sandamirskaya"
              },
              {
                "authorId": "1900897",
                "name": "P. Dudek"
              }
            ]
          }
        },
        {
          "citedcorpusid": 10712214,
          "isinfluential": false,
          "contexts": [
            ", patches in images [12] or time surfaces in event-based systems [13]).",
            "To help placing our system in the vast landscape of eventbased stereo vision approaches that have been devised, note that 1) it does not rely on the precise spatio-temporal matching of event time surfaces such as in [13], [25]–[28], and that 2) even though it is an active approach using the emission of light in the scene, it is not using the idea of analyzing the deformations of an a-priori known pattern of light such as in structured light (that often make use of a single sensor) [29]– [31], nor using any time-of-flight information [32]."
          ],
          "intents": [
            "['background']",
            "--"
          ],
          "cited_paper_info": {
            "title": "Event-based 3D reconstruction from neuromorphic retinas",
            "abstract": "",
            "year": 2013,
            "venue": "Neural Networks",
            "authors": [
              {
                "authorId": "2057119545",
                "name": "J. Carneiro"
              },
              {
                "authorId": "144975525",
                "name": "S. Ieng"
              },
              {
                "authorId": "153466606",
                "name": "C. Posch"
              },
              {
                "authorId": "1750848",
                "name": "R. Benosman"
              }
            ]
          }
        },
        {
          "citedcorpusid": 15357188,
          "isinfluential": true,
          "contexts": [
            "Thus, a natural sensor to use to detect such a change is a Dynamic Vision Sensor (DVS) [20]–[22].",
            "In the DAVIS240C sensor we use, each pixel contains both an APS and DVS circuit sharing the same photodiode.",
            "Thus, we can calibrate our system using the grayscale images produced by the APS part and use the estimated parameters for further operations involving events produced by the DVS part.",
            "DAVIS sensors contain both a Dynamic Vision Sensor (DVS) and an Active Pixel Sensor (APS) circuit in each pixel.",
            "In this paper, we demonstrated how combining a laser that can quickly scan a scene with a pair of DVS allows us to create\na stereo vision system, in which the stereo-matching problem is alleviated.",
            "Hence, whenever the laser blinks or moves to a position, it creates events that are captured in the two DVS (as seen in Figure 3).",
            "A DVS is a vision chip that reports events indicating changes in brightness for each pixel asynchronously."
          ],
          "intents": [
            "['background']",
            "--",
            "--",
            "--",
            "--",
            "--",
            "--"
          ],
          "cited_paper_info": {
            "title": "A 128×128 120db 30mw Asynchronous Vision Sensor That Responds to Relative Intensity Change",
            "abstract": "The frame-based architectures of most imagers are natural for making movies and pictures, they have significant drawbacks for machine vision. Short-latency vision problems require high frame rate, producing massive readout (e.g., >1GB/s from 352×288 pixels at 10kFrames/s [1]). Reducing the output to a manageable rate by using region-of-interest readout usually requires complex control strategies. Readout and processing of largely redundant data ultimately limit reductions in computational effort and power consumption. In this paper, a vision sensor is presented whose pixels asynchronously respond to events that represent relative changes in intensity. It operates largely independent of scene illumination, directly encodes object reflectance, and reduces redundancy while preserving precise timing information. Because output bandwidth is automatically dedicated to dynamic parts of the scene, the sensor is suitable for applications in surveillance and motion analysis. It improves on prior frame-based temporal difference detection imagers (e.g., [2]) by asynchronously responding to temporal contrast rather than absolute illumination, and on prior event-based imagers because they either do not reduce redundancy at all [3], reduce only spatial redundancy [4], have large FPN, slow response, and limited dynamic range [5], or have low contrast sensitivity [6].",
            "year": 2005,
            "venue": "",
            "authors": [
              {
                "authorId": "1744964",
                "name": "P. Lichtsteiner"
              },
              {
                "authorId": "2368354141",
                "name": "C. Posch"
              },
              {
                "authorId": "5548576",
                "name": "T. Delbruck"
              }
            ]
          }
        },
        {
          "citedcorpusid": 22296005,
          "isinfluential": false,
          "contexts": [
            "The difficulty of stereo-matching, whether it is area based or feature based, is illustrated by the numerous hardware accelerators that have been devised to perform these operations [15]–[19]."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "High-Performance SIFT Hardware Accelerator for Real-Time Image Feature Extraction",
            "abstract": "",
            "year": 2012,
            "venue": "IEEE transactions on circuits and systems for video technology (Print)",
            "authors": [
              {
                "authorId": "2154878",
                "name": "Feng-Cheng Huang"
              },
              {
                "authorId": "1730018",
                "name": "Shi-Yu Huang"
              },
              {
                "authorId": "2936346",
                "name": "J. Ker"
              },
              {
                "authorId": "103218849",
                "name": "Yung-Chang Chen"
              }
            ]
          }
        },
        {
          "citedcorpusid": 23276048,
          "isinfluential": false,
          "contexts": [
            "To help placing our system in the vast landscape of eventbased stereo vision approaches that have been devised, note that 1) it does not rely on the precise spatio-temporal matching of event time surfaces such as in [13], [25]–[28], and that 2) even though it is an active approach using the emission of light in the scene, it is not using the idea of analyzing the deformations of an a-priori known pattern of light such as in structured light (that often make use of a single sensor) [29]– [31], nor using any time-of-flight information [32]."
          ],
          "intents": [
            "--"
          ],
          "cited_paper_info": {
            "title": "Arrayable Voltage-Controlled Ring-Oscillator for Direct Time-of-Flight Image Sensors",
            "abstract": "Direct time-of-flight (d-ToF) estimation with high frame rate requires the incorporation of a time-to-digital converter (TDC) at pixel level. A feasible approach to a compact implementation of the TDC is to use the multiple phases of a voltage-controlled ring-oscillator (VCRO) for the finest bits. The VCRO becomes central in determining the performance parameters of a d-ToF image sensor. In this paper, we are covering the modeling, design, and measurement of a CMOS pseudo-differential VCRO. The oscillation frequency, the jitter due to mismatches and noise and the power consumption are analytically evaluated. This design has been incorporated into a <inline-formula> <tex-math notation=\"LaTeX\">$64\\times 64$ </tex-math></inline-formula>-pixel array. It has been fabricated in a 0.18 <inline-formula> <tex-math notation=\"LaTeX\">$\\mu \\text{m}$ </tex-math></inline-formula> standard CMOS technology. Occupation area is <inline-formula> <tex-math notation=\"LaTeX\">$28\\!\\!\\times \\!\\!29~\\mu \\text{m}^{2}$ </tex-math></inline-formula> and power consumption is 1.17 mW at 850 MHz. The measured gain of the VCRO is of 477 MHz/V with a frequency tuning range of 53%. Moreover, it features a linearity of 99.4% over a wide range of control frequencies, namely, from 400 to 850 MHz. The phase noise is of -102 dBc/Hz at 2 MHz offset frequency from 850 MHz. The influence of these parameters in the performance of the TDC has been measured. The minimum time bin of the TDC is 147 ps with a rms DNL/ INL of 0.13/ 1.7LSB.",
            "year": 2017,
            "venue": "IEEE Transactions on Circuits and Systems Part 1: Regular Papers",
            "authors": [
              {
                "authorId": "3194101",
                "name": "I. Vornicu"
              },
              {
                "authorId": "1403824726",
                "name": "R. Carmona-Galán"
              },
              {
                "authorId": "1403293719",
                "name": "Á. Rodríguez-Vázquez"
              }
            ]
          }
        },
        {
          "citedcorpusid": 261497446,
          "isinfluential": false,
          "contexts": [
            "By observing a scene with two sensors separated by some baseline, the third dimension – depth – can be recovered by triangulation [10].",
            "Triangulation is performed using a Direct Linear Transform [10] after refining the twodimensional coordinates of our points using constraints imposed by the geometry of our setup (epipolar constraints)."
          ],
          "intents": [
            "['background']",
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Multiple View Geometry in Computer Vision",
            "abstract": "",
            "year": 2001,
            "venue": "Künstliche Intell.",
            "authors": [
              {
                "authorId": "2237471376",
                "name": "Bernhard P. Wrobel"
              }
            ]
          }
        }
      ]
    },
    "8305136": {
      "citing_paper_info": {
        "title": "Event-Based 3D Motion Flow Estimation Using 4D Spatio Temporal Subspaces Properties",
        "abstract": "State of the art scene flow estimation techniques are based on projections of the 3D motion on image using luminance—sampled at the frame rate of the cameras—as the principal source of information. We introduce in this paper a pure time based approach to estimate the flow from 3D point clouds primarily output by neuromorphic event-based stereo camera rigs, or by any existing 3D depth sensor even if it does not provide nor use luminance. This method formulates the scene flow problem by applying a local piecewise regularization of the scene flow. The formulation provides a unifying framework to estimate scene flow from synchronous and asynchronous 3D point clouds. It relies on the properties of 4D space time using a decomposition into its subspaces. This method naturally exploits the properties of the neuromorphic asynchronous event based vision sensors that allows continuous time 3D point clouds reconstruction. The approach can also handle the motion of deformable object. Experiments using different 3D sensors are presented.",
        "year": 2017,
        "venue": "Frontiers in Neuroscience",
        "authors": [
          {
            "authorId": "144975525",
            "name": "S. Ieng"
          },
          {
            "authorId": "2057119545",
            "name": "J. Carneiro"
          },
          {
            "authorId": "1750848",
            "name": "R. Benosman"
          }
        ]
      },
      "citation_summary": {
        "citation_count": 7,
        "unique_cited_count": 7,
        "influential_count": 0,
        "detailed_records_count": 7
      },
      "cited_papers": [
        "1846045",
        "9208584",
        "1864608",
        "18164747",
        "2610586",
        "10712214",
        "21874346"
      ],
      "citation_details": [
        {
          "citedcorpusid": 1846045,
          "isinfluential": false,
          "contexts": [
            "Patch based techniques have also been used in Popham et al. (2010) and Cagniart et al. (2010) to split complex surfaces into simpler ones."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Free-form mesh tracking: A patch-based approach",
            "abstract": "",
            "year": 2010,
            "venue": "2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition",
            "authors": [
              {
                "authorId": "2782397",
                "name": "Cedric Cagniart"
              },
              {
                "authorId": "1719388",
                "name": "Edmond Boyer"
              },
              {
                "authorId": "46505857",
                "name": "Slobodan Ilic"
              }
            ]
          }
        },
        {
          "citedcorpusid": 1864608,
          "isinfluential": false,
          "contexts": [
            "Scene ﬂow can also be computed from local descriptors of reconstructed surfaces such as surfel that encodes the local geometry and the reﬂectance information of the shapes (Carceroni and Kutulakos, 2002)."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Multi-View Scene Capture by Surfel Sampling: From Video Streams to Non-Rigid 3D Motion, Shape and Reflectance",
            "abstract": "",
            "year": 2002,
            "venue": "International Journal of Computer Vision",
            "authors": [
              {
                "authorId": "1831412",
                "name": "R. Carceroni"
              },
              {
                "authorId": "1734027",
                "name": "Kiriakos N. Kutulakos"
              }
            ]
          }
        },
        {
          "citedcorpusid": 2610586,
          "isinfluential": false,
          "contexts": [
            "Regularization is often performed by minimizing an energy function with variational formulations (Zhang et al., 2001; Min and Sohn, 2006; Huguet and Devernay, 2007)."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "A Variational Method for Scene Flow Estimation from Stereo Sequences",
            "abstract": "",
            "year": 2007,
            "venue": "IEEE International Conference on Computer Vision",
            "authors": [
              {
                "authorId": "24920638",
                "name": "F. Huguet"
              },
              {
                "authorId": "1909232",
                "name": "Frederic Devernay"
              }
            ]
          }
        },
        {
          "citedcorpusid": 9208584,
          "isinfluential": false,
          "contexts": [
            "However, SFM’s high vulnerability to images’ noise and to camera calibration errors raised questions regarding its applicability in real-world scenarios (Tomasi and Zhang, 1995)."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Is Structure-from-Motion Worth Pursuing?",
            "abstract": "",
            "year": 1996,
            "venue": "",
            "authors": [
              {
                "authorId": "145086151",
                "name": "Carlo Tomasi"
              },
              {
                "authorId": "2108092355",
                "name": "John Zhang"
              }
            ]
          }
        },
        {
          "citedcorpusid": 10712214,
          "isinfluential": false,
          "contexts": [
            "The algorithm is applied to two sources of 3D data: a Microsoft Kinect (an RGBD sensor that outputs frames of 3D points aligned with RGB information) and an asynchronous event-based 3D reconstruction system as introduced in Carneiro et al. (2013).",
            "This subsection provides the 3D scene ﬂow using event-based cameras (DVS) as described in Carneiro et al. (2013).",
            "As introduced in Rogister et al. (2012) and Carneiro et al. (2013), event-based cameras allow to estimate depth and produce 3D point clouds at unprecedented accuracy ( > 1 kHz in real-time) at very low computational and energy cost using conventional processing hardware."
          ],
          "intents": [
            "['methodology']",
            "['methodology']",
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Event-based 3D reconstruction from neuromorphic retinas",
            "abstract": "",
            "year": 2013,
            "venue": "Neural Networks",
            "authors": [
              {
                "authorId": "2057119545",
                "name": "J. Carneiro"
              },
              {
                "authorId": "144975525",
                "name": "S. Ieng"
              },
              {
                "authorId": "153466606",
                "name": "C. Posch"
              },
              {
                "authorId": "1750848",
                "name": "R. Benosman"
              }
            ]
          }
        },
        {
          "citedcorpusid": 18164747,
          "isinfluential": false,
          "contexts": [
            "This approach parametrizes the motion problem on the image plane, i.e., in 2D and is the most commonly found in the existing literature (Vedula et al., 1999; Zhang et al., 2001; Isard and MacCormick, 2006; Wedel et al., 2011)."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Dense Motion and Disparity Estimation Via Loopy Belief Propagation",
            "abstract": "",
            "year": 2006,
            "venue": "Asian Conference on Computer Vision",
            "authors": [
              {
                "authorId": "2090818",
                "name": "M. Isard"
              },
              {
                "authorId": "1698422",
                "name": "J. MacCormick"
              }
            ]
          }
        },
        {
          "citedcorpusid": 21874346,
          "isinfluential": false,
          "contexts": [
            "It is also called the mean closest point between both points clouds and is a dissimilarity measure often used for example in the Iterative Closest Point (ICP) problem (Besl and McKay, 1992)."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "A Method for Registration of 3-D Shapes",
            "abstract": "The authors describe a general-purpose, representation-independent method for the accurate and computationally efficient registration of 3-D shapes including free-form curves and surfaces. The method handles the full six degrees of freedom and is based on the iterative closest point (ICP) algorithm, which requires only a procedure to find the closest point on a geometric entity to a given point. The ICP algorithm always converges monotonically to the nearest local minimum of a mean-square distance metric, and the rate of convergence is rapid during the first few iterations. Therefore, given an adequate set of initial rotations and translations for a particular class of objects with a certain level of 'shape complexity', one can globally minimize the mean-square distance metric over all six degrees of freedom by testing each initial registration. One important application of this method is to register sensed data from unfixtured rigid objects with an ideal geometric model, prior to shape inspection. Experimental results show the capabilities of the registration algorithm on point sets, curves, and surfaces. >",
            "year": 1992,
            "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
            "authors": [
              {
                "authorId": "3050737",
                "name": "P. Besl"
              },
              {
                "authorId": "144856117",
                "name": "Neil D. McKay"
              }
            ]
          }
        }
      ]
    },
    "197431184": {
      "citing_paper_info": {
        "title": "Stereo Event Lifetime and Disparity Estimation for Dynamic Vision Sensors",
        "abstract": "Event-based cameras are biologically inspired sensors that output asynchronous pixel-wise brightness changes in the scene called events. They have a high dynamic range and temporal resolution of a microsecond, opposed to standard cameras that output frames at fixed frame rates and suffer from motion blur. Forming stereo pairs of such cameras can open novel application possibilities, since for each event depth can be readily estimated; however, to fully exploit asynchronous nature of the sensor and avoid fixed time interval event accumulation, stereo event lifetime estimation should be employed. In this paper, we propose a novel method for event lifetime estimation of stereo event-cameras, allowing generation of sharp gradient images of events that serve as input to disparity estimation methods. Since a single brightness change triggers events in both event-camera sensors, we propose a method for single shot event lifetime and disparity estimation, with association via stereo matching. The proposed method is approximately twice as fast and more accurate than if lifetimes were estimated separately for each sensor and then stereo matched. Results are validated on realworld data through multiple stereo event-camera experiments.",
        "year": 2019,
        "venue": "European Conference on Mobile Robots",
        "authors": [
          {
            "authorId": "51450968",
            "name": "Antea Hadviger"
          },
          {
            "authorId": "2346045",
            "name": "Ivan Marković"
          },
          {
            "authorId": "143888607",
            "name": "I. Petrović"
          }
        ]
      },
      "citation_summary": {
        "citation_count": 9,
        "unique_cited_count": 9,
        "influential_count": 3,
        "detailed_records_count": 9
      },
      "cited_papers": [
        "115151433",
        "6324125",
        "157060825",
        "44623261",
        "10280488",
        "24236495",
        "24007071",
        "2497402",
        "12212153"
      ],
      "citation_details": [
        {
          "citedcorpusid": 2497402,
          "isinfluential": true,
          "contexts": [
            "DVS yield sparse event data in case of a motion occurring within the area of very similar brightness intensity, which the sensor is unable to discern, resulting in poor results of event lifetime and disparity estimation.",
            "Event-based vision sensors [1], such as the dynamic vision sensor (DVS) [2] and dynamic and active-pixel vision sensor (DAVIS) [3], are relatively novel biologically inspired cameras that output pixel-wise changes in brightness intensity in the scene, which are referred to as events due to their asynchronous nature.",
            "Index Terms—event-based cameras, stereo vision, event lifetime estimation, disparity estimation\nI. INTRODUCTION\nEvent-based vision sensors [1], such as the dynamic vision sensor (DVS) [2] and dynamic and active-pixel vision sensor (DAVIS) [3], are relatively novel biologically inspired cameras that output pixel-wise changes in brightness intensity in the scene, which are referred to as events due to their asynchronous nature.",
            "A recent survey on event-based vision applications and methods can be found in [4] and an example of a processed DVS output can be seen in Fig."
          ],
          "intents": [
            "--",
            "['background']",
            "--",
            "--"
          ],
          "cited_paper_info": {
            "title": "> Replace This Line with Your Paper Identification Number (double-click Here to Edit) < 1",
            "abstract": "—This paper describes a CMOS vision sensor which is inspired by biological visual systems. Each pixel independently and in continuous time quantizes local relative intensity changes to generate spike events. These events appear at the output of the sensor as an asynchronous stream of digital pixel addresses. These address-events signify scene reflectance change and have sub-millisecond timing precision. The output data rate depends on the dynamic content of the scene and is typically orders of magnitude lower than those of conventional frame-based imagers. By combining an active front-end logarithmic photoreceptor running in continuous time with a self-timed switched-capacitor differencing circuit, the sensor achieves an array mismatch of 2.1% in relative intensity event threshold and a pixel bandwidth of 3 kHz under 1 klux scene illumination. Dynamic range is >120 dB and chip power consumption is 23 mW. Event latency shows weak light dependency and decreases to 15 us at >1 klux pixel illumination. The sensor is built in a 0.35u 4M2P process yielding 40x40 um 2 pixels with 9.4% fill-factor. By providing high pixel bandwidth, wide dynamic range, and precisely-timed sparse digital output, this neuromorphic silicon retina provides an attractive combination of characteristics for low-latency dynamic vision under uncontrolled illumination with low post-processing requirements.",
            "year": null,
            "venue": "",
            "authors": []
          }
        },
        {
          "citedcorpusid": 6324125,
          "isinfluential": false,
          "contexts": [
            "The implementation of the proposed method is done in C++ in ROS environment [20]."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "ROS: an open-source Robot Operating System",
            "abstract": "—This paper gives an overview of ROS, an open-source robot operating system. ROS is not an operating system in the traditional sense of process management and scheduling; rather, it provides a structured communications layer above the host operating systems of a heterogenous compute cluster. In this paper, we discuss how ROS relates to existing robot software frameworks, and brieﬂy overview some of the available application software which uses ROS.",
            "year": 2009,
            "venue": "IEEE International Conference on Robotics and Automation",
            "authors": [
              {
                "authorId": "39100828",
                "name": "M. Quigley"
              }
            ]
          }
        },
        {
          "citedcorpusid": 10280488,
          "isinfluential": false,
          "contexts": [
            "Accumulation can be done directly by choosing a fixed time interval and this approach has been used for tracking and optical flow estimation [7], [8]."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Simultaneous Optical Flow and Intensity Estimation from an Event Camera",
            "abstract": "",
            "year": 2016,
            "venue": "Computer Vision and Pattern Recognition",
            "authors": [
              {
                "authorId": "7642780",
                "name": "Patrick Bardow"
              },
              {
                "authorId": "2052135690",
                "name": "A. Davison"
              },
              {
                "authorId": "2864731",
                "name": "Stefan Leutenegger"
              }
            ]
          }
        },
        {
          "citedcorpusid": 12212153,
          "isinfluential": false,
          "contexts": [
            "We use an event feature descriptor introduced by [16] that supports event-driven stereo matching based on distance transform [17] which describes the context of a pixel in binary images by calculating the distance from the pixel to the nearest active pixel."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Distance Transforms of Sampled Functions",
            "abstract": "We describe linear-time algorithms for solving a class of problems that involve transforming a cost function on a grid using spatial information. These problems can be viewed as a generalization of classical distance transforms of binary images, where the binary image is replaced by an arbitrary function on a grid. Alternatively they can be viewed in terms of the minimum convolution of two functions, which is an important operation in grayscale morphology. A consequence of our techniques is a simple and fast method for computing the Euclidean distance transform of a binary image. Our algorithms are also applicable to Viterbi decoding, belief propagation, and optimal control.",
            "year": 2012,
            "venue": "Theory of Computing",
            "authors": [
              {
                "authorId": "1685089",
                "name": "Pedro F. Felzenszwalb"
              },
              {
                "authorId": "1713089",
                "name": "D. Huttenlocher"
              }
            ]
          }
        },
        {
          "citedcorpusid": 24007071,
          "isinfluential": true,
          "contexts": [
            "Experiments were conducted with two DAVIS240 sensors mounted on a stereo rig with a baseline of 10 cm as shown in Fig.",
            "Stream of events from DAVIS240 is fetched in grouped sequences through a ROS topic.",
            "Index Terms—event-based cameras, stereo vision, event lifetime estimation, disparity estimation\nI. INTRODUCTION\nEvent-based vision sensors [1], such as the dynamic vision sensor (DVS) [2] and dynamic and active-pixel vision sensor (DAVIS) [3], are relatively novel biologically inspired cameras that output pixel-wise changes in brightness intensity in the scene, which are referred to as events due to their asynchronous nature.",
            "We used the DAVIS240 standard camera frames for calibration, but our proposed method operates on events only.",
            "Event-based vision sensors [1], such as the dynamic vision sensor (DVS) [2] and dynamic and active-pixel vision sensor (DAVIS) [3], are relatively novel biologically inspired cameras that output pixel-wise changes in brightness intensity in the scene, which are referred to as events due to their asynchronous nature."
          ],
          "intents": [
            "--",
            "--",
            "--",
            "--",
            "['background']"
          ],
          "cited_paper_info": {
            "title": "A 240 × 180 130 dB 3 µs Latency Global Shutter Spatiotemporal Vision Sensor",
            "abstract": "",
            "year": 2014,
            "venue": "IEEE Journal of Solid-State Circuits",
            "authors": [
              {
                "authorId": "2239977",
                "name": "Christian Brandli"
              },
              {
                "authorId": "144246116",
                "name": "R. Berner"
              },
              {
                "authorId": "1779496",
                "name": "Minhao Yang"
              },
              {
                "authorId": "1704961",
                "name": "Shih-Chii Liu"
              },
              {
                "authorId": "5548576",
                "name": "T. Delbruck"
              }
            ]
          }
        },
        {
          "citedcorpusid": 24236495,
          "isinfluential": false,
          "contexts": [
            "In [12], [13] authors used a fixed event accumulation interval to generate artificial frames, while [14] generated different frames based on event timestamps."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Dynamic stereo vision system for real-time tracking",
            "abstract": "",
            "year": 2010,
            "venue": "Proceedings of 2010 IEEE International Symposium on Circuits and Systems",
            "authors": [
              {
                "authorId": "2521747",
                "name": "S. Schraml"
              },
              {
                "authorId": "1768812",
                "name": "A. Belbachir"
              },
              {
                "authorId": "152832347",
                "name": "Nenad Milosevic"
              },
              {
                "authorId": "1680865",
                "name": "Peter Schön"
              }
            ]
          }
        },
        {
          "citedcorpusid": 44623261,
          "isinfluential": false,
          "contexts": [
            "In [12], [13] authors used a fixed event accumulation interval to generate artificial frames, while [14] generated different frames based on event timestamps."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Address-Event Based Stereo Vision with Bio-Inspired Silicon Retina Imagers",
            "abstract": "Several industry, home, or automotive applications need 3D or at least range data of the observed environment to operate. Such applications are, e.g., driver assistance systems, home care systems, or 3D sensing and measurement for industrial production. State-of-the-art range sensors are laser range finders or laser scanners (LIDAR, light detection and ranging), time-of-flight (TOF) cameras, and ultrasonic sound sensors. All of them are embedded, which means that the sensors operate independently and have an integrated processing unit. This is advantageous because the processing power in the mentioned applications is limited and they are computationally intensive anyway. Another benefits of embedded systems are a low power consumption and a small form factor. Furthermore, embedded systems are full customizable by the developer and can be adapted to the specific application in an optimal way. A promising alternative to the mentioned sensors is stereo vision. Classic stereo vision uses a stereo camera setup, which is built up of two cameras (stereo camera head), mounted in parallel and separated by the baseline. It captures a synchronized stereo pair consisting of the left camera’s image and the right camera’s image. The main challenge of stereo vision is the reconstruction of 3D information of a scene captured from two different points of view. Each visible scene point is projected on the image planes of the cameras. Pixels which represent the same scene points on different image planes correspond to each other. These correspondences can then be used to determine the three dimensional position of the projected scene point in a defined coordinate system. In more detail, the horizontal displacement, called the disparity, is inverse proportional to the scene point’s depth. With this information and the camera’s intrinsic parameters (principal point and focal length), the 3D position can be reconstructed. Fig. 1 shows a typical stereo camera setup. The projections of scene point P are pl and pr. Once the correspondences are found, the disparity is calculated with",
            "year": 2011,
            "venue": "",
            "authors": [
              {
                "authorId": "1824241",
                "name": "J. Kogler"
              },
              {
                "authorId": "1692498",
                "name": "C. Sulzbachner"
              },
              {
                "authorId": "1721721",
                "name": "M. Humenberger"
              },
              {
                "authorId": "2435053",
                "name": "F. Eibensteiner"
              }
            ]
          }
        },
        {
          "citedcorpusid": 115151433,
          "isinfluential": false,
          "contexts": [
            "Recently, significant efforts have been made in the area of intensity image reconstruction from events [5], [6]; however, for some applications complex image intensity reconstruction"
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Events-To-Video: Bringing Modern Computer Vision to Event Cameras",
            "abstract": "Event cameras are novel sensors that report brightness changes in the form of asynchronous \"events\" instead of intensity frames. They have significant advantages over conventional cameras: high temporal resolution, high dynamic range, and no motion blur. Since the output of event cameras is fundamentally different from conventional cameras, it is commonly accepted that they require the development of specialized algorithms to accommodate the particular nature of events. In this work, we take a different view and propose to apply existing, mature computer vision techniques to videos reconstructed from event data. We propose a novel, recurrent neural network to reconstruct videos from a stream of events and train it on a large amount of simulated event data. Our experiments show that our approach surpasses state-of-the-art reconstruction methods by a large margin (> 20%) in terms of image quality. We further apply off-the-shelf computer vision algorithms to videos reconstructed from event data on tasks such as object classification and visual-inertial odometry, and show that this strategy consistently outperforms algorithms that were specifically designed for event data. We believe that our approach opens the door to bringing the outstanding properties of event cameras to an entirely new range of tasks.",
            "year": 2019,
            "venue": "Computer Vision and Pattern Recognition",
            "authors": [
              {
                "authorId": "3414274",
                "name": "Henri Rebecq"
              },
              {
                "authorId": "2774325",
                "name": "René Ranftl"
              },
              {
                "authorId": "145231047",
                "name": "V. Koltun"
              },
              {
                "authorId": "2075371",
                "name": "D. Scaramuzza"
              }
            ]
          }
        },
        {
          "citedcorpusid": 157060825,
          "isinfluential": true,
          "contexts": [
            "We use an event feature descriptor introduced by [16] that supports event-driven stereo matching based on distance transform [17] which describes the context of a pixel in binary images by calculating the distance from the pixel to the nearest active pixel.",
            "We compare the proposed method in terms of both execution time and accuracy with the method proposed in [16] that first executes lifetime estimation separately for both sensors and then follows up with disparity estimation (referred to as the decoupled method).",
            "However, opposed to choosing a fixed time event accumulation interval, enhancing the event stream with event lifetimes yields more accurate disparity maps [16].",
            "Although [16] proposed a descriptor invariant to scale and rotation, we do not implement those features since they are not needed for stereo matching on rectified images.",
            "disparity estimation method is computationally more efficient and accurate in comparison to the method that decouples event lifetime and disparity estimation [16].",
            "Cost aggregation is performed as suggested in [16], in a fixed Algorithm 1 Stereo event lifetime (sensorA, sensorB)"
          ],
          "intents": [
            "['methodology']",
            "['methodology']",
            "['background']",
            "['background']",
            "['methodology']",
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "ROBUST DENSE DEPTH MAP ESTIMATION FROM SPARSE DVS STEREOS 3 2 Related Work",
            "abstract": "Real-world depth perception applications require precise reaction to fast motion, and the ability to operate in scenes which contain large intensity differences or high dynamic range. Standard CMOS cameras based methods for depth computing, such as stereo matching, run into the problem of huge power consuming at high frame-rates or inaccurate depths with noise or holes. The event camera, DVS (Dynamic Vision Sensor), aims to be robust to fast motion and light change with low power consumption and sparse representation, offering great potential to replace standard cameras for depth perception. However, it is not trivial to directly apply DVS for stereo matching due to its nature of low latency and sparsity which will result in extremely limited available information and imperfect imaging quality. To overcome these problems and make the DVS available for depth perception, this paper introduces a novel method which can enhance the stream of events and estimate the dense depth through event driven stereo matching. Our event stream enhancement algorithm efficiently buffers events according to time continuous rather than using artificially-chosen time intervals, and our stereo matching method can robust estimate depth for complex scenarios regardless of the motion or light changing. To the best of our knowledge, this is the first algorithm provably able to recover dense depth maps from sparse DVS stereos. Experiments in a variety of challenging conditions demonstrate the superior performance of our method. c © 2017. The copyright of this document resides with its authors. It may be distributed unchanged freely in print or electronic forms. 2 ZOU ET AL.: ROBUST DENSE DEPTH MAP ESTIMATION FROM SPARSE DVS STEREOS",
            "year": 2017,
            "venue": "",
            "authors": [
              {
                "authorId": "2780914",
                "name": "Dongqing Zou"
              },
              {
                "authorId": "2053742370",
                "name": "Feng Shi"
              },
              {
                "authorId": "2153282723",
                "name": "Qiang Wang"
              }
            ]
          }
        }
      ]
    },
    "3416874": {
      "citing_paper_info": {
        "title": "The Multivehicle Stereo Event Camera Dataset: An Event Camera Dataset for 3D Perception",
        "abstract": "Event-based cameras are a new passive sensing modality with a number of benefits over traditional cameras, including extremely low latency, asynchronous data acquisition, high dynamic range, and very low power consumption. There has been a lot of recent interest and development in applying algorithms to use the events to perform a variety of three-dimensional perception tasks, such as feature tracking, visual odometry, and stereo depth estimation. However, there currently lacks the wealth of labeled data that exists for traditional cameras to be used for both testing and development. In this letter, we present a large dataset with a synchronized stereo pair event based camera system, carried on a handheld rig, flown by a hexacopter, driven on top of a car, and mounted on a motorcycle, in a variety of different illumination levels and environments. From each camera, we provide the event stream, grayscale images, and inertial measurement unit (IMU) readings. In addition, we utilize a combination of IMU, a rigidly mounted lidar system, indoor and outdoor motion capture, and GPS to provide accurate pose and depth images for each camera at up to 100 Hz. For comparison, we also provide synchronized grayscale images and IMU readings from a frame-based stereo camera system.",
        "year": 2018,
        "venue": "IEEE Robotics and Automation Letters",
        "authors": [
          {
            "authorId": "3385588",
            "name": "A. Z. Zhu"
          },
          {
            "authorId": "144964367",
            "name": "Dinesh Thakur"
          },
          {
            "authorId": "2520604",
            "name": "Tolga Özaslan"
          },
          {
            "authorId": "39832696",
            "name": "Bernd Pfrommer"
          },
          {
            "authorId": "37956314",
            "name": "Vijay R. Kumar"
          },
          {
            "authorId": "1751586",
            "name": "Kostas Daniilidis"
          }
        ]
      },
      "citation_summary": {
        "citation_count": 14,
        "unique_cited_count": 14,
        "influential_count": 1,
        "detailed_records_count": 14
      },
      "cited_papers": [
        "7151414",
        "1408596",
        "12339854",
        "26324573",
        "277804",
        "204780933",
        "17693733",
        "18612391",
        "120110206",
        "109416659",
        "7884141",
        "15778738",
        "11177597",
        "396580"
      ],
      "citation_details": [
        {
          "citedcorpusid": 277804,
          "isinfluential": false,
          "contexts": [
            "The camera intrinsics and extrinsics are estimated using a grid of AprilTags [35] that is moved in front of the sensor rig and calibrated using Kalibr."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "AprilTag: A robust and flexible visual fiducial system",
            "abstract": "",
            "year": 2011,
            "venue": "IEEE International Conference on Robotics and Automation",
            "authors": [
              {
                "authorId": "144776513",
                "name": "Edwin Olson"
              }
            ]
          }
        },
        {
          "citedcorpusid": 396580,
          "isinfluential": false,
          "contexts": [
            "[5] provide a large dataset of a DAVIS 346B mounted behind the windshield of a car, with 12 hours of driving, intended for end to end learning of various driving related tasks."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "DDD17: End-To-End DAVIS Driving Dataset",
            "abstract": "Event cameras, such as dynamic vision sensors (DVS), and dynamic and active-pixel vision sensors (DAVIS) can supplement other autonomous driving sensors by providing a concurrent stream of standard active pixel sensor (APS) images and DVS temporal contrast events. The APS stream is a sequence of standard grayscale global-shutter image sensor frames. The DVS events represent brightness changes occurring at a particular moment, with a jitter of about a millisecond under most lighting conditions. They have a dynamic range of >120 dB and effective frame rates >1 kHz at data rates comparable to 30 fps (frames/second) image sensors. To overcome some of the limitations of current image acquisition technology, we investigate in this work the use of the combined DVS and APS streams in end-to-end driving applications. The dataset DDD17 accompanying this paper is the first open dataset of annotated DAVIS driving recordings. DDD17 has over 12 h of a 346x260 pixel DAVIS sensor recording highway and city driving in daytime, evening, night, dry and wet weather conditions, along with vehicle speed, GPS position, driver steering, throttle, and brake captured from the car's on-board diagnostics interface. As an example application, we performed a preliminary end-to-end learning study of using a convolutional neural network that is trained to predict the instantaneous steering angle from DVS and APS visual data.",
            "year": 2017,
            "venue": "arXiv.org",
            "authors": [
              {
                "authorId": "1737610",
                "name": "Jonathan Binas"
              },
              {
                "authorId": "145243593",
                "name": "Daniel Neil"
              },
              {
                "authorId": "1704961",
                "name": "Shih-Chii Liu"
              },
              {
                "authorId": "5548576",
                "name": "T. Delbruck"
              }
            ]
          }
        },
        {
          "citedcorpusid": 1408596,
          "isinfluential": false,
          "contexts": [
            "In [15], the authors propose a novel context descriptor to perform matching, and the authors in [16] use a stereo event camera undergoing pure rotation to perform depth estimation and panoramic stitching."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Context-aware event-driven stereo matching",
            "abstract": "",
            "year": 2016,
            "venue": "International Conference on Information Photonics",
            "authors": [
              {
                "authorId": "2780914",
                "name": "Dongqing Zou"
              },
              {
                "authorId": "2075394158",
                "name": "Ping Guo"
              },
              {
                "authorId": "145805403",
                "name": "Qiang Wang"
              },
              {
                "authorId": "2108031895",
                "name": "Xiaotao Wang"
              },
              {
                "authorId": "2055021575",
                "name": "Guangqi Shao"
              },
              {
                "authorId": "2053742370",
                "name": "Feng Shi"
              },
              {
                "authorId": "2118372356",
                "name": "Jia Li"
              },
              {
                "authorId": "13975243",
                "name": "P. Park"
              }
            ]
          }
        },
        {
          "citedcorpusid": 7151414,
          "isinfluential": false,
          "contexts": [
            "Later works in [8], [9] and [10] have adapted cooperative methods for stereo depth to event based cameras, due to their applicability to asynchronous, point based measurements."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Improved Cooperative Stereo Matching for Dynamic Vision Sensors with Ground Truth Evaluation",
            "abstract": "",
            "year": 2017,
            "venue": "2017 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)",
            "authors": [
              {
                "authorId": "47105977",
                "name": "Ewa Piatkowska"
              },
              {
                "authorId": "1824241",
                "name": "J. Kogler"
              },
              {
                "authorId": "1768812",
                "name": "A. Belbachir"
              },
              {
                "authorId": "1990797",
                "name": "M. Gelautz"
              }
            ]
          }
        },
        {
          "citedcorpusid": 7884141,
          "isinfluential": false,
          "contexts": [
            "The authors in [17] and [18] proposed novel methods to perform feature tracking in the event space, which they extended in [19] and [20] to perform visual and visual inertial odometry, respectively."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Low-latency visual odometry using event-based feature tracks",
            "abstract": "",
            "year": 2016,
            "venue": "IEEE/RJS International Conference on Intelligent RObots and Systems",
            "authors": [
              {
                "authorId": "2076484536",
                "name": "Beat Kueng"
              },
              {
                "authorId": "144578041",
                "name": "Elias Mueggler"
              },
              {
                "authorId": "144036711",
                "name": "Guillermo Gallego"
              },
              {
                "authorId": "2075371",
                "name": "D. Scaramuzza"
              }
            ]
          }
        },
        {
          "citedcorpusid": 11177597,
          "isinfluential": false,
          "contexts": [
            "Early works in [6], [7] present stereo depth estimation results with a number of spatial and temporal costs."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Event-Based Stereo Matching Approaches for Frameless Address Event Stereo Data",
            "abstract": "",
            "year": 2011,
            "venue": "International Symposium on Visual Computing",
            "authors": [
              {
                "authorId": "1824241",
                "name": "J. Kogler"
              },
              {
                "authorId": "1721721",
                "name": "M. Humenberger"
              },
              {
                "authorId": "1692498",
                "name": "C. Sulzbachner"
              }
            ]
          }
        },
        {
          "citedcorpusid": 12339854,
          "isinfluential": false,
          "contexts": [
            "The transformation that takes a point from the lidar frame to the left DAVIS frame was initially calibrated using the Camera and Range Calibration Toolbox [33].",
            "lidar are calibrated using the Camera and Range Calibration Toolbox4 [33], and fine tuned manually, and the hand eye calibration between the mocap model pose in the motion capture",
            "The camera intrinsics, stereo extrinsics, and camera-IMU extrinsics are calibrated using the Kalibr toolbox3 [30], [31], [32], the extrinsics between the left DAVIS camera and Velodyne lidar are calibrated using the Camera and Range Calibration Toolbox4 [33], and fine tuned manually, and the hand eye calibration between the mocap model pose in the motion capture world frame and the left DAVIS camera pose is performed using CamOdoCal5 [34]."
          ],
          "intents": [
            "['methodology']",
            "['methodology']",
            "--"
          ],
          "cited_paper_info": {
            "title": "Automatic camera and range sensor calibration using a single shot",
            "abstract": "",
            "year": 2012,
            "venue": "IEEE International Conference on Robotics and Automation",
            "authors": [
              {
                "authorId": "47237027",
                "name": "Andreas Geiger"
              },
              {
                "authorId": "3128253",
                "name": "Frank Moosmann"
              },
              {
                "authorId": "2738114",
                "name": "O. Car"
              },
              {
                "authorId": "2057784291",
                "name": "Bernhard Schuster"
              }
            ]
          }
        },
        {
          "citedcorpusid": 15778738,
          "isinfluential": false,
          "contexts": [
            "The camera intrinsics, stereo extrinsics, and camera-IMU extrinsics are calibrated using the Kalibr toolbox3 [30], [31], [32], the extrinsics between the left DAVIS camera and Velodyne"
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Unified temporal and spatial calibration for multi-sensor systems",
            "abstract": "",
            "year": 2013,
            "venue": "2013 IEEE/RSJ International Conference on Intelligent Robots and Systems",
            "authors": [
              {
                "authorId": "1792387",
                "name": "P. Furgale"
              },
              {
                "authorId": "2864524",
                "name": "J. Rehder"
              },
              {
                "authorId": "1720483",
                "name": "R. Siegwart"
              }
            ]
          }
        },
        {
          "citedcorpusid": 17693733,
          "isinfluential": false,
          "contexts": [
            "Similarly, [11] and [12] apply a set of temporal, epipolar, ordering and polarity constraints to determine matches, while [13] compare this with matching based on the output of a bank of orientation filters."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Asynchronous Event-Based Binocular Stereo Matching",
            "abstract": "",
            "year": 2012,
            "venue": "IEEE Transactions on Neural Networks and Learning Systems",
            "authors": [
              {
                "authorId": "3121605",
                "name": "P. Rogister"
              },
              {
                "authorId": "1750848",
                "name": "R. Benosman"
              },
              {
                "authorId": "144975525",
                "name": "S. Ieng"
              },
              {
                "authorId": "1744964",
                "name": "P. Lichtsteiner"
              },
              {
                "authorId": "5548576",
                "name": "T. Delbruck"
              }
            ]
          }
        },
        {
          "citedcorpusid": 18612391,
          "isinfluential": false,
          "contexts": [
            "For each sequence with lidar measurements, we run the Lidar Odometry and Mapping (LOAM) algorithm [29] to generate dense 3D local maps, which are projected into each DAVIS camera to generate dense depth images at 20 Hz, and to provide 3D pose for the handheld sequences."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "LOAM: Lidar Odometry and Mapping in Real-time",
            "abstract": "We propose a real-time method for odometry and mapping using range measurements from a 2-axis lidar moving in 6-DOF. The problem is hard because the range measurements are received at different times, and errors in motion estimation can cause mis-registration of the resulting point cloud. To date, coherent 3D maps can be built by off-line batch methods, often using loop closure to correct for drift over time. Our method achieves both low-drift and low-computational complexity without the need for high accuracy ranging or inertial measurements. The key idea in obtaining this level of performance is the division of the complex problem of simultaneous localization and mapping, which seeks to optimize a large number of variables simultaneously, by two algorithms. One algorithm performs odometry at a high frequency but low fidelity to estimate velocity of the lidar. Another algorithm runs at a frequency of an order of magnitude lower for fine matching and registration of the point cloud. Combination of the two algorithms allows the method to map in real-time. The method has been evaluated by a large set of experiments as well as on the KITTI odometry benchmark. The results indicate that the method can achieve accuracy at the level of state of the art offline batch methods.",
            "year": 2014,
            "venue": "Robotics: Science and Systems",
            "authors": [
              {
                "authorId": "2116921472",
                "name": "Ji Zhang"
              },
              {
                "authorId": "2118413567",
                "name": "Sanjiv Singh"
              }
            ]
          }
        },
        {
          "citedcorpusid": 26324573,
          "isinfluential": false,
          "contexts": [
            "The authors in [21] use events to estimate angular velocity of a camera, while [22] and [23] perform visual odometry by building an up to a scale map."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Real-Time 3D Reconstruction and 6-DoF Tracking with an Event Camera",
            "abstract": "",
            "year": 2016,
            "venue": "European Conference on Computer Vision",
            "authors": [
              {
                "authorId": "3227772",
                "name": "Hanme Kim"
              },
              {
                "authorId": "2864731",
                "name": "Stefan Leutenegger"
              },
              {
                "authorId": "2052135690",
                "name": "A. Davison"
              }
            ]
          }
        },
        {
          "citedcorpusid": 109416659,
          "isinfluential": true,
          "contexts": [
            "• Images and IMU measurements from the VI Sensor.",
            "The timestamps for the VI Sensor messages in the dataset are then modified to compensate for this offset.",
            "In comparison, this dataset provides event streams from two synchronized and calibrated Dynamic Vision and Active Pixel Sensors (DAVISm346b), with long indoor and outdoor sequences in a variety of illuminations and speeds, along with accurate depth images and pose at up to 100Hz, generated from a lidar system rigidly mounted on top of the cameras, as in Fig 1, along with motion capture and GPS.",
            "In addition, we have mounted a VI Sensor [27], originally",
            "In addition, we calibrate the temporal offset between the DAVIS stereo pair and the VI Sensor by finding the temporal offset that maximizes the cross correlation between the magnitude of the gyroscope angular velocities from the IMUs of the left DAVIS and the VI Sensor.",
            "In this section, we describe the various steps performed to calibrate the intrinsic parameters of each DAVIS and VI-Sensor camera, as well as the extrinsic transformations between each of the cameras, IMUs and the lidar.",
            "For high speed sequences, the DAVIS stereo rig and VI Sensor are mounted on the handlebar of a motorcycle (Fig 2d), along with the GPS device.",
            "In addition, we have mounted a VI Sensor [27], originally developed by Skybotix for comparison with frame based methods."
          ],
          "intents": [
            "--",
            "--",
            "--",
            "['methodology']",
            "--",
            "--",
            "--",
            "--"
          ],
          "cited_paper_info": {
            "title": "A synchronized visual-inertial sensor system with FPGA pre-processing for accurate real-time SLAM",
            "abstract": "",
            "year": 2014,
            "venue": "IEEE International Conference on Robotics and Automation",
            "authors": [
              {
                "authorId": "40274270",
                "name": "J. Nikolic"
              },
              {
                "authorId": "50529675",
                "name": "J. Rehder"
              },
              {
                "authorId": "1767631",
                "name": "M. Burri"
              },
              {
                "authorId": "2617164",
                "name": "Pascal Gohl"
              },
              {
                "authorId": "2864731",
                "name": "Stefan Leutenegger"
              },
              {
                "authorId": "1792387",
                "name": "P. Furgale"
              },
              {
                "authorId": "1720483",
                "name": "R. Siegwart"
              }
            ]
          }
        },
        {
          "citedcorpusid": 120110206,
          "isinfluential": false,
          "contexts": [
            "The camera intrinsics, stereo extrinsics, and camera-IMU extrinsics are calibrated using the Kalibr toolbox3 [30], [31], [32], the extrinsics between the left DAVIS camera and Velodyne"
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Self-supervised calibration for robotic systems",
            "abstract": "",
            "year": 2013,
            "venue": "2013 IEEE Intelligent Vehicles Symposium (IV)",
            "authors": [
              {
                "authorId": "32524746",
                "name": "J. Maye"
              },
              {
                "authorId": "1792387",
                "name": "P. Furgale"
              },
              {
                "authorId": "1720483",
                "name": "R. Siegwart"
              }
            ]
          }
        },
        {
          "citedcorpusid": 204780933,
          "isinfluential": false,
          "contexts": [
            "In addition, [24] and [25] also fuse events with measurements from an IMU to perform visual inertial odometry."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "2018 1 Problem Statement Continuous-Time Visual-Inertial Trajectory Estimation with Event Cameras",
            "abstract": "Continuous-Time Visual-Inertial Trajectory Estimation with Event Cameras by Elias Mueggler et al. [4] details the combination of a spline based trajectory estimation with visual-inertial measurements through an IMU and an event based camera. This continuous-time based system is appealing since it allows the incorporation of asynchronous measurements and high frequency measurements without increasing the overall complexity of the system.",
            "year": 2018,
            "venue": "",
            "authors": [
              {
                "authorId": "20944397",
                "name": "Patrick Geneva"
              }
            ]
          }
        }
      ]
    },
    "221543284": {
      "citing_paper_info": {
        "title": "Stereo dense depth tracking based on optical flow using frames and events",
        "abstract": "ABSTRACT Event cameras are biologically inspired sensors that asynchronously detect brightness changes in the scene independently for each pixel. Their output is a stream of events which is reported with a low latency and high temporal resolution of a microsecond, making them superior to standard cameras in highly dynamic scenarios when they are sensitive to motion blur. Event cameras can be used in a wide range of applications, one of them being depth estimation, in both stereo and monocular settings. However, most known event-based depth estimation methods yield sparse depth maps due to the nature of the sparse event stream. We present a novel method that fuses information from both events and standard frames, as well as odometry, to exploit the advantages of both sensors. We propose to estimate dense disparity from standard frames at the point of their availability, predict the disparity using odometry information, and track the disparity asynchronously using optical flow of events between the standard frames. We present the performance of the method through several experiments in various setups, including synthetic data, KITTI dataset enhanced with events, MVSEC dataset, as well as our own stereo event camera recordings. GRAPHICAL ABSTRACT",
        "year": 2020,
        "venue": "Adv. Robotics",
        "authors": [
          {
            "authorId": "51450968",
            "name": "Antea Hadviger"
          },
          {
            "authorId": "2346045",
            "name": "Ivan Marković"
          },
          {
            "authorId": "143888607",
            "name": "I. Petrović"
          }
        ]
      },
      "citation_summary": {
        "citation_count": 12,
        "unique_cited_count": 12,
        "influential_count": 2,
        "detailed_records_count": 12
      },
      "cited_papers": [
        "3892441",
        "6724907",
        "24007071",
        "10817557",
        "38870956",
        "15002233",
        "53107219",
        "118684904",
        "65040501",
        "18327083",
        "214743146",
        "157060825"
      ],
      "citation_details": [
        {
          "citedcorpusid": 3892441,
          "isinfluential": false,
          "contexts": [
            "The work in [21] aims to reduce the disparity search space based on the prediction and use pixel-wise Kalman filtering to combine the predicted disparity and the newly matched one, whereas [22] extends the method by exploiting the difference between the predicted and matched disparity to detect moving objects."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Dense Disparity Estimation in Ego-motion Reduced Search Space",
            "abstract": "",
            "year": 2017,
            "venue": "arXiv.org",
            "authors": [
              {
                "authorId": "24072976",
                "name": "Luka Fucek"
              },
              {
                "authorId": "2346045",
                "name": "Ivan Marković"
              },
              {
                "authorId": "2720665",
                "name": "Igor Cvisic"
              },
              {
                "authorId": "143888607",
                "name": "I. Petrović"
              }
            ]
          }
        },
        {
          "citedcorpusid": 6724907,
          "isinfluential": false,
          "contexts": [
            "To demonstrate the performance of our method in a real-world scenario, we used the well-known KITTI dataset [28]."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Are we ready for autonomous driving? The KITTI vision benchmark suite",
            "abstract": "",
            "year": 2012,
            "venue": "2012 IEEE Conference on Computer Vision and Pattern Recognition",
            "authors": [
              {
                "authorId": "47237027",
                "name": "Andreas Geiger"
              },
              {
                "authorId": "37108776",
                "name": "Philip Lenz"
              },
              {
                "authorId": "2422559",
                "name": "R. Urtasun"
              }
            ]
          }
        },
        {
          "citedcorpusid": 10817557,
          "isinfluential": false,
          "contexts": [
            "Prior to synthesizing events, the standard frames of the KITTI dataset that were captured with the frequency of 10 Hz were upsampled with a frame interpolation package (also available in the said tool) based on [30] to achieve better results."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Super SloMo: High Quality Estimation of Multiple Intermediate Frames for Video Interpolation",
            "abstract": "Given two consecutive frames, video interpolation aims at generating intermediate frame(s) to form both spatially and temporally coherent video sequences. While most existing methods focus on single-frame interpolation, we propose an end-to-end convolutional neural network for variable-length multi-frame video interpolation, where the motion interpretation and occlusion reasoning are jointly modeled. We start by computing bi-directional optical flow between the input images using a U-Net architecture. These flows are then linearly combined at each time step to approximate the intermediate bi-directional optical flows. These approximate flows, however, only work well in locally smooth regions and produce artifacts around motion boundaries. To address this shortcoming, we employ another U-Net to refine the approximated flow and also predict soft visibility maps. Finally, the two input images are warped and linearly fused to form each intermediate frame. By applying the visibility maps to the warped images before fusion, we exclude the contribution of occluded pixels to the interpolated intermediate frame to avoid artifacts. Since none of our learned network parameters are time-dependent, our approach is able to produce as many intermediate frames as needed. To train our network, we use 1,132 240-fps video clips, containing 300K individual video frames. Experimental results on several datasets, predicting different numbers of interpolated frames, demonstrate that our approach performs consistently better than existing methods.",
            "year": 2017,
            "venue": "2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition",
            "authors": [
              {
                "authorId": "40175280",
                "name": "Huaizu Jiang"
              },
              {
                "authorId": "3232265",
                "name": "Deqing Sun"
              },
              {
                "authorId": "2745026",
                "name": "V. Jampani"
              },
              {
                "authorId": "37144787",
                "name": "Ming-Hsuan Yang"
              },
              {
                "authorId": "1389846455",
                "name": "E. Learned-Miller"
              },
              {
                "authorId": "1690538",
                "name": "J. Kautz"
              }
            ]
          }
        },
        {
          "citedcorpusid": 15002233,
          "isinfluential": false,
          "contexts": [
            "…4 × 4 transformation matrix deﬁned as: where Γ is the projection matrix that transforms the coordinates ω between the disparity space and the coordinates M in the Euclidean space as follows: with f and b being the focal length of the cameras and the baseline of the stereo rig, respectively [24]."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Motion estimation from disparity images",
            "abstract": "A new method for 3D rigid motion estimation from stereo is proposed in this paper. The appealing feature of this method is that it directly uses the disparity images obtained from stereo matching. We assume that the stereo rig has parallel cameras and show, in that case, the geometric and topological properties of the disparity images. Then we introduce a rigid transformation (called d-motion) that maps two disparity images of a rigidly moving object. We show how it is related to the Euclidean rigid motion and a motion estimation algorithm is derived. We show with experiments that our approach is simple and more accurate than standard approaches.",
            "year": 2001,
            "venue": "Proceedings Eighth IEEE International Conference on Computer Vision. ICCV 2001",
            "authors": [
              {
                "authorId": "2444581",
                "name": "D. Demirdjian"
              },
              {
                "authorId": "1753210",
                "name": "Trevor Darrell"
              }
            ]
          }
        },
        {
          "citedcorpusid": 18327083,
          "isinfluential": true,
          "contexts": [
            "We used our implementation of semiglobal matching (SGM) for a stereo setup presented in [22], which supports AVX2 instructions and multithreading to support real-time performance.",
            ", SGM [18], and use events to track the disparity asynchronously between the frames, an illus-",
            "We suggest to use standard frames to perform dense disparity estimation with any standard stereo matching method, e.g., SGM [18], and use events to track the disparity asynchronously between the frames, an illus-\ntration of which is shown in Figure 1.",
            "Dense disparity was estimated using SGM, shown in (a) and (f).",
            "SGM introduces parameters P1 and P2 as discontinuity penalties in order to obtain smooth disparity maps.",
            "The method performs in real-time in a single thread, with the ability to run our implementation of SGM multithreaded, making it suitable for implementation in autonomous systems that operate in highly dynamic scenarios or have significant computational constraints."
          ],
          "intents": [
            "--",
            "['background']",
            "--",
            "--",
            "--",
            "--"
          ],
          "cited_paper_info": {
            "title": "Ieee Transactions on Pattern Analysis and Machine Intelligence 1 Stereo Processing by Semi-global Matching and Mutual Information",
            "abstract": "",
            "year": null,
            "venue": "",
            "authors": [
              {
                "authorId": "3335378",
                "name": "H. Hirschmüller"
              }
            ]
          }
        },
        {
          "citedcorpusid": 24007071,
          "isinfluential": true,
          "contexts": [
            "We have demonstrated the performance of the method through several experiments that cover different scenarios, including synthetic data from the event simulator, KITTI dataset enhanced with events, MVSEC dataset, and our own stereo DAVIS recordings.",
            "4.4 Stereo DAVIS346 Experiments\nExperimental data was collected with two DAVIS346 sensors mounted on a stereo rig with a baseline of 10 cm.",
            "This dataset provides multiple stereo event sequences collected with two DAVIS346 cameras, including ground truth information about pose and depth obtained by lidar SLAM.",
            "Event cameras, such as the dynamic and active-pixel vision sensor (DAVIS) [1], are biologically inspired sensors that operate fundamentally different than standard cameras."
          ],
          "intents": [
            "--",
            "--",
            "--",
            "['background']"
          ],
          "cited_paper_info": {
            "title": "A 240 × 180 130 dB 3 µs Latency Global Shutter Spatiotemporal Vision Sensor",
            "abstract": "",
            "year": 2014,
            "venue": "IEEE Journal of Solid-State Circuits",
            "authors": [
              {
                "authorId": "2239977",
                "name": "Christian Brandli"
              },
              {
                "authorId": "144246116",
                "name": "R. Berner"
              },
              {
                "authorId": "1779496",
                "name": "Minhao Yang"
              },
              {
                "authorId": "1704961",
                "name": "Shih-Chii Liu"
              },
              {
                "authorId": "5548576",
                "name": "T. Delbruck"
              }
            ]
          }
        },
        {
          "citedcorpusid": 38870956,
          "isinfluential": false,
          "contexts": [
            "A similar method that rejects unreliable predictions in the areas where moving objects are detected was proposed in [20]."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Disparity prediction between adjacent frames for dynamic scenes",
            "abstract": "",
            "year": 2014,
            "venue": "Neurocomputing",
            "authors": [
              {
                "authorId": "2116325958",
                "name": "Jun Jiang"
              },
              {
                "authorId": "11729006",
                "name": "Jun Cheng"
              },
              {
                "authorId": "9376787",
                "name": "Baowen Chen"
              },
              {
                "authorId": "1730308",
                "name": "Xinyu Wu"
              }
            ]
          }
        },
        {
          "citedcorpusid": 53107219,
          "isinfluential": false,
          "contexts": [
            "Experiments were conducted on the data obtained with ESIM: an Open Event Camera Simulator [27]."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "ESIM: an Open Event Camera Simulator",
            "abstract": ": Event cameras are revolutionary sensors that work radically differently from standard cameras. Instead of capturing intensity images at a ﬁxed rate, event cameras measure changes of intensity asynchronously , in the form of a stream of events , which encode per-pixel brightness changes. In the last few years, their outstanding properties (asynchronous sensing, no motion blur, high dynamic range) have led to exciting vision applications, with very low-latency and high robustness. However, these sensors are still scarce and expensive to get, slowing down progress of the research community. To address these issues, there is a huge demand for cheap, high-quality synthetic, labeled event for algorithm prototyping, deep learning and algorithm benchmarking. The development of such a simulator, however, is not trivial since event cameras work fundamentally differently from frame-based cameras. We present the ﬁrst event camera simulator that can generate a large amount of reliable event data. The key component of our simulator is a theoretically sound, adaptive rendering scheme that only samples frames when necessary, through a tight coupling between the rendering engine and the event simulator. We release an open source implementation of our simulator. We release ESIM as open source: http://rpg.ifi.uzh.ch/esim .",
            "year": 2018,
            "venue": "Conference on Robot Learning",
            "authors": [
              {
                "authorId": "3414274",
                "name": "Henri Rebecq"
              },
              {
                "authorId": "51152279",
                "name": "Daniel Gehrig"
              },
              {
                "authorId": "2075371",
                "name": "D. Scaramuzza"
              }
            ]
          }
        },
        {
          "citedcorpusid": 65040501,
          "isinfluential": false,
          "contexts": [
            "In order to get smoother depth maps, global approaches incorporate smoothness constraints across neighboring points, typically demonstrated in scenes with static cameras and few moving objects [7, 8]."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Event-based stereo matching using semiglobal matching",
            "abstract": "In this article, we focus on the problem of depth estimation from a stereo pair of event-based sensors. These sensors asynchronously capture pixel-level brightness changes information (events) instead of standard intensity images at a specified frame rate. So, these sensors provide sparse data at low latency and high temporal resolution over a wide intrascene dynamic range. However, new asynchronous, event-based processing algorithms are required to process the event streams. We propose a fully event-based stereo three-dimensional depth estimation algorithm inspired by semiglobal matching. Our algorithm considers the smoothness constraints between the nearby events to remove the ambiguous and wrong matches when only using the properties of a single event or local features. Experimental validation and comparison with several state-of-the-art, event-based stereo matching methods are provided on five different scenes of event-based stereo data sets. The results show that our method can operate well in an event-driven way and has higher estimation accuracy.",
            "year": 2018,
            "venue": "",
            "authors": [
              {
                "authorId": "47661053",
                "name": "Zhen Xie"
              },
              {
                "authorId": "1739956171",
                "name": "Jianhua Zhang"
              },
              {
                "authorId": "2108238188",
                "name": "Pengfei Wang"
              }
            ]
          }
        },
        {
          "citedcorpusid": 118684904,
          "isinfluential": false,
          "contexts": [
            "A recent survey on event cameras oﬀers an exhausting overview of all state-of-the-art event-based applications and methods [2]."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Event-Based Vision: A Survey",
            "abstract": "Event cameras are bio-inspired sensors that differ from conventional frame cameras: Instead of capturing images at a fixed rate, they asynchronously measure per-pixel brightness changes, and output a stream of events that encode the time, location and sign of the brightness changes. Event cameras offer attractive properties compared to traditional cameras: high temporal resolution (in the order of $\\mu$μs), very high dynamic range (140 dB versus 60 dB), low power consumption, and high pixel bandwidth (on the order of kHz) resulting in reduced motion blur. Hence, event cameras have a large potential for robotics and computer vision in challenging scenarios for traditional cameras, such as low-latency, high speed, and high dynamic range. However, novel methods are required to process the unconventional output of these sensors in order to unlock their potential. This paper provides a comprehensive overview of the emerging field of event-based vision, with a focus on the applications and the algorithms developed to unlock the outstanding properties of event cameras. We present event cameras from their working principle, the actual sensors that are available and the tasks that they have been used for, from low-level vision (feature detection and tracking, optic flow, etc.) to high-level vision (reconstruction, segmentation, recognition). We also discuss the techniques developed to process events, including learning-based techniques, as well as specialized processors for these novel sensors, such as spiking neural networks. Additionally, we highlight the challenges that remain to be tackled and the opportunities that lie ahead in the search for a more efficient, bio-inspired way for machines to perceive and interact with the world.",
            "year": 2019,
            "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
            "authors": [
              {
                "authorId": "144036711",
                "name": "Guillermo Gallego"
              },
              {
                "authorId": "1694635",
                "name": "T. Delbrück"
              },
              {
                "authorId": "33780923",
                "name": "G. Orchard"
              },
              {
                "authorId": "1897771",
                "name": "C. Bartolozzi"
              },
              {
                "authorId": "1736425",
                "name": "B. Taba"
              },
              {
                "authorId": "1860631",
                "name": "A. Censi"
              },
              {
                "authorId": "2864731",
                "name": "Stefan Leutenegger"
              },
              {
                "authorId": "2052135690",
                "name": "A. Davison"
              },
              {
                "authorId": "3302681",
                "name": "J. Conradt"
              },
              {
                "authorId": "1751586",
                "name": "Kostas Daniilidis"
              },
              {
                "authorId": "2075371",
                "name": "D. Scaramuzza"
              }
            ]
          }
        },
        {
          "citedcorpusid": 157060825,
          "isinfluential": false,
          "contexts": [
            "The work in [14] attempts to recover semi-dense disparity by fusing information from multiple camera viewpoints, while [5] proposes to estimate dense disparity by performing interpolation in the areas uncovered by events.",
            "Some works proposed to estimate event lifetimes prior to event aggregation in order to yield more accurate results [5, 6]."
          ],
          "intents": [
            "['background']",
            "['background']"
          ],
          "cited_paper_info": {
            "title": "ROBUST DENSE DEPTH MAP ESTIMATION FROM SPARSE DVS STEREOS 3 2 Related Work",
            "abstract": "Real-world depth perception applications require precise reaction to fast motion, and the ability to operate in scenes which contain large intensity differences or high dynamic range. Standard CMOS cameras based methods for depth computing, such as stereo matching, run into the problem of huge power consuming at high frame-rates or inaccurate depths with noise or holes. The event camera, DVS (Dynamic Vision Sensor), aims to be robust to fast motion and light change with low power consumption and sparse representation, offering great potential to replace standard cameras for depth perception. However, it is not trivial to directly apply DVS for stereo matching due to its nature of low latency and sparsity which will result in extremely limited available information and imperfect imaging quality. To overcome these problems and make the DVS available for depth perception, this paper introduces a novel method which can enhance the stream of events and estimate the dense depth through event driven stereo matching. Our event stream enhancement algorithm efficiently buffers events according to time continuous rather than using artificially-chosen time intervals, and our stereo matching method can robust estimate depth for complex scenarios regardless of the motion or light changing. To the best of our knowledge, this is the first algorithm provably able to recover dense depth maps from sparse DVS stereos. Experiments in a variety of challenging conditions demonstrate the superior performance of our method. c © 2017. The copyright of this document resides with its authors. It may be distributed unchanged freely in print or electronic forms. 2 ZOU ET AL.: ROBUST DENSE DEPTH MAP ESTIMATION FROM SPARSE DVS STEREOS",
            "year": 2017,
            "venue": "",
            "authors": [
              {
                "authorId": "2780914",
                "name": "Dongqing Zou"
              },
              {
                "authorId": "2053742370",
                "name": "Feng Shi"
              },
              {
                "authorId": "2153282723",
                "name": "Qiang Wang"
              }
            ]
          }
        },
        {
          "citedcorpusid": 214743146,
          "isinfluential": false,
          "contexts": [
            "However, we generated synthetic event data using the open-source Video to Events tool provided by [29]."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Video to Events: Recycling Video Datasets for Event Cameras",
            "abstract": "Event cameras are novel sensors that output brightness changes in the form of a stream of asynchronous \"events\" instead of intensity frames. They offer significant advantages with respect to conventional cameras: high dynamic range (HDR), high temporal resolution, and no motion blur. Recently, novel learning approaches operating on event data have achieved impressive results. Yet, these methods require a large amount of event data for training, which is hardly available due the novelty of event sensors in computer vision research. In this paper, we present a method that addresses these needs by converting any existing video dataset recorded with conventional cameras to synthetic event data. This unlocks the use of a virtually unlimited number of existing video datasets for training networks designed for real event data. We evaluate our method on two relevant vision tasks, i.e., object recognition and semantic segmentation, and show that models trained on synthetic events have several benefits: (i) they generalize well to real event data, even in scenarios where standard-camera images are blurry or overexposed, by inheriting the outstanding properties of event cameras; (ii) they can be used for fine-tuning on real data to improve over state-of-the-art for both classification and semantic segmentation.",
            "year": 2019,
            "venue": "Computer Vision and Pattern Recognition",
            "authors": [
              {
                "authorId": "51152279",
                "name": "Daniel Gehrig"
              },
              {
                "authorId": "8329387",
                "name": "Mathias Gehrig"
              },
              {
                "authorId": "2065112865",
                "name": "Javier Hidalgo-Carri'o"
              },
              {
                "authorId": "2075371",
                "name": "D. Scaramuzza"
              }
            ]
          }
        }
      ]
    },
    "238198645": {
      "citing_paper_info": {
        "title": "StereoSpike: Depth Learning With a Spiking Neural Network",
        "abstract": "Depth estimation is an important computer vision task, useful in particular for navigation in autonomous vehicles, or for object manipulation in robotics. Here, we propose to solve it using StereoSpike, an end-to-end neuromorphic approach, combining two event-based cameras and a Spiking Neural Network (SNN) with a modified U-Net-like encoder-decoder architecture. More specifically, we used the Multi Vehicle Stereo Event Camera Dataset (MVSEC). It provides a depth ground-truth, which was used to train StereoSpike in a supervised manner, using surrogate gradient descent. We propose a novel readout paradigm to obtain a dense analog prediction–the depth of each pixel– from the spikes of the decoder. We demonstrate that this architecture generalizes very well, even better than its non-spiking counterparts, leading to near state-of-the-art test accuracy. To the best of our knowledge, it is the first time that such a large-scale regression problem is solved by a fully spiking neural network. Finally, we show that very low firing rates (< 5%) can be obtained via regularization, with a minimal cost in accuracy. This means that StereoSpike could be efficiently implemented on neuromorphic chips, opening the door for low power and real time embedded systems.",
        "year": 2021,
        "venue": "IEEE Access",
        "authors": [
          {
            "authorId": "2130588381",
            "name": "Ulysse Rançon"
          },
          {
            "authorId": "2129996240",
            "name": "Javier Cuadrado-Anibarro"
          },
          {
            "authorId": "49953506",
            "name": "B. Cottereau"
          },
          {
            "authorId": "2441104",
            "name": "T. Masquelier"
          }
        ]
      },
      "citation_summary": {
        "citation_count": 16,
        "unique_cited_count": 16,
        "influential_count": 5,
        "detailed_records_count": 16
      },
      "cited_papers": [
        "241440878",
        "4694685",
        "3416874",
        "223957202",
        "81981856",
        "102496818",
        "235359262",
        "212414806",
        "226976144",
        "118684904",
        "207935891",
        "6195748",
        "3608458",
        "4572038",
        "7151414",
        "86423050"
      ],
      "citation_details": [
        {
          "citedcorpusid": 3416874,
          "isinfluential": false,
          "contexts": [
            "We trained and tested our network on the Multi Vehicle Stereo Event Camera (MVSEC) dataset (Zhu et al. 2018b)."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "The Multivehicle Stereo Event Camera Dataset: An Event Camera Dataset for 3D Perception",
            "abstract": "Event-based cameras are a new passive sensing modality with a number of benefits over traditional cameras, including extremely low latency, asynchronous data acquisition, high dynamic range, and very low power consumption. There has been a lot of recent interest and development in applying algorithms to use the events to perform a variety of three-dimensional perception tasks, such as feature tracking, visual odometry, and stereo depth estimation. However, there currently lacks the wealth of labeled data that exists for traditional cameras to be used for both testing and development. In this letter, we present a large dataset with a synchronized stereo pair event based camera system, carried on a handheld rig, flown by a hexacopter, driven on top of a car, and mounted on a motorcycle, in a variety of different illumination levels and environments. From each camera, we provide the event stream, grayscale images, and inertial measurement unit (IMU) readings. In addition, we utilize a combination of IMU, a rigidly mounted lidar system, indoor and outdoor motion capture, and GPS to provide accurate pose and depth images for each camera at up to 100 Hz. For comparison, we also provide synchronized grayscale images and IMU readings from a frame-based stereo camera system.",
            "year": 2018,
            "venue": "IEEE Robotics and Automation Letters",
            "authors": [
              {
                "authorId": "3385588",
                "name": "A. Z. Zhu"
              },
              {
                "authorId": "144964367",
                "name": "Dinesh Thakur"
              },
              {
                "authorId": "2520604",
                "name": "Tolga Özaslan"
              },
              {
                "authorId": "39832696",
                "name": "Bernd Pfrommer"
              },
              {
                "authorId": "37956314",
                "name": "Vijay R. Kumar"
              },
              {
                "authorId": "1751586",
                "name": "Kostas Daniilidis"
              }
            ]
          }
        },
        {
          "citedcorpusid": 3608458,
          "isinfluential": true,
          "contexts": [
            "Implemented on dedicated chips such as Intel Loihi [5], IBM TrueNorth [2], Brainchip Akida [40] or Tianjic [34], these models could become a new paradigm for ultra-low power computation in the coming years.",
            "As a result, it is essentially implementable on dedicated neuromorphic hardware, such as Intel Loihi [7], IBM TrueNorth [8].",
            "Implemented on dedicated chips such as Intel Loihi [7], IBM TrueNorth [8], Brainchip Akida [9] or",
            "As a result, it is essentially implementable on dedicated neuromorphic hardware, such as Intel Loihi [5], IBM TrueNorth [2]."
          ],
          "intents": [
            "--",
            "['methodology']",
            "['methodology']",
            "--"
          ],
          "cited_paper_info": {
            "title": "Loihi: A Neuromorphic Manycore Processor with On-Chip Learning",
            "abstract": "Loihi is a 60-mm2 chip fabricated in Intels 14-nm process that advances the state-of-the-art modeling of spiking neural networks in silicon. It integrates a wide range of novel features for the field, such as hierarchical connectivity, dendritic compartments, synaptic delays, and, most importantly, programmable synaptic learning rules. Running a spiking convolutional form of the Locally Competitive Algorithm, Loihi can solve LASSO optimization problems with over three orders of magnitude superior energy-delay-product compared to conventional solvers running on a CPU iso-process/voltage/area. This provides an unambiguous example of spike-based computation, outperforming all known conventional solutions.",
            "year": 2018,
            "venue": "IEEE Micro",
            "authors": [
              {
                "authorId": "2087005207",
                "name": "Mike Davies"
              },
              {
                "authorId": "1753812",
                "name": "N. Srinivasa"
              },
              {
                "authorId": "1740851",
                "name": "Tsung-Han Lin"
              },
              {
                "authorId": "3198756",
                "name": "Gautham N. Chinya"
              },
              {
                "authorId": "39098303",
                "name": "Yongqiang Cao"
              },
              {
                "authorId": "2572988",
                "name": "S. H. Choday"
              },
              {
                "authorId": "50280933",
                "name": "G. Dimou"
              },
              {
                "authorId": "144594117",
                "name": "Prasad Joshi"
              },
              {
                "authorId": "39536844",
                "name": "N. Imam"
              },
              {
                "authorId": "2116971410",
                "name": "Shweta Jain"
              },
              {
                "authorId": "2781201",
                "name": "Yuyun Liao"
              },
              {
                "authorId": "3312263",
                "name": "Chit-Kwan Lin"
              },
              {
                "authorId": "145166778",
                "name": "Andrew Lines"
              },
              {
                "authorId": "35804101",
                "name": "Ruokun Liu"
              },
              {
                "authorId": "1710935",
                "name": "D. Mathaikutty"
              },
              {
                "authorId": "2070135234",
                "name": "Steve McCoy"
              },
              {
                "authorId": "2150514037",
                "name": "Arnab Paul"
              },
              {
                "authorId": "2306147",
                "name": "Jonathan Tse"
              },
              {
                "authorId": "35865638",
                "name": "Guruguhanathan Venkataramanan"
              },
              {
                "authorId": "31953633",
                "name": "Y. Weng"
              },
              {
                "authorId": "1665915600",
                "name": "Andreas Wild"
              },
              {
                "authorId": "2108648646",
                "name": "Yoonseok Yang"
              },
              {
                "authorId": "49528487",
                "name": "Hong Wang"
              }
            ]
          }
        },
        {
          "citedcorpusid": 4572038,
          "isinfluential": false,
          "contexts": [
            "The model in [7] was the ﬁrst successful multi-scale architecture designed for depth estimation from RGB images, and was consequently followed by advances based on similar approaches [24] [23] [14]."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "MegaDepth: Learning Single-View Depth Prediction from Internet Photos",
            "abstract": "Single-view depth prediction is a fundamental problem in computer vision. Recently, deep learning methods have led to significant progress, but such methods are limited by the available training data. Current datasets based on 3D sensors have key limitations, including indoor-only images (NYU), small numbers of training examples (Make3D), and sparse sampling (KITTI). We propose to use multi-view Internet photo collections, a virtually unlimited data source, to generate training data via modern structure-from-motion and multi-view stereo (MVS) methods, and present a large depth dataset called MegaDepth based on this idea. Data derived from MVS comes with its own challenges, including noise and unreconstructable objects. We address these challenges with new data cleaning methods, as well as automatically augmenting our data with ordinal depth relations generated using semantic segmentation. We validate the use of large amounts of Internet data by showing that models trained on MegaDepth exhibit strong generalization-not only to novel scenes, but also to other diverse datasets including Make3D, KITTI, and DIW, even when no images from those datasets are seen during training.1",
            "year": 2018,
            "venue": "2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition",
            "authors": [
              {
                "authorId": "2145369560",
                "name": "Zhengqi Li"
              },
              {
                "authorId": "1830653",
                "name": "Noah Snavely"
              }
            ]
          }
        },
        {
          "citedcorpusid": 4694685,
          "isinfluential": false,
          "contexts": [
            "Depth is an important feature of the surrounding space whose estimation finds its place in various tasks across many different fields [1]."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Application of Stereo-Imaging Technology to Medical Field",
            "abstract": "Objectives There has been continuous development in the area of stereoscopic medical imaging devices, and many stereoscopic imaging devices have been realized and applied in the medical field. In this article, we review past and current trends pertaining to the application stereo-imaging technologies in the medical field. Methods We describe the basic principles of stereo vision and visual issues related to it, including visual discomfort, binocular disparities, vergence-accommodation mismatch, and visual fatigue. We also present a brief history of medical applications of stereo-imaging techniques, examples of recently developed stereoscopic medical devices, and patent application trends as they pertain to stereo-imaging medical devices. Results Three-dimensional (3D) stereo-imaging technology can provide more realistic depth perception to the viewer than conventional two-dimensional imaging technology. Therefore, it allows for a more accurate understanding and analysis of the morphology of an object. Based on these advantages, the significance of stereoscopic imaging in the medical field increases in accordance with the increase in the number of laparoscopic surgeries, and stereo-imaging technology plays a key role in the diagnoses of the detailed morphologies of small biological specimens. Conclusions The application of 3D stereo-imaging technology to the medical field will help improve surgical accuracy, reduce operation times, and enhance patient safety. Therefore, it is important to develop more enhanced stereoscopic medical devices.",
            "year": 2012,
            "venue": "Healthcare Informatics Research",
            "authors": [
              {
                "authorId": "1769975",
                "name": "K. Nam"
              },
              {
                "authorId": "6374399",
                "name": "Jeongyun Park"
              },
              {
                "authorId": "38288252",
                "name": "I. Kim"
              },
              {
                "authorId": "1942531",
                "name": "K. Kim"
              }
            ]
          }
        },
        {
          "citedcorpusid": 6195748,
          "isinfluential": false,
          "contexts": [
            "Such neurons are inexpensive to simulate and can be deployed in large models, whereas more complex ones such as Hodgkin-Huxley [16], Izhikevich [18] or even SRM [13] are still too computationally expensive to be trained on modern hardware."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Why spikes? Hebbian learning and retrieval of time-resolved excitation patterns",
            "abstract": "",
            "year": 1993,
            "venue": "Biological cybernetics",
            "authors": [
              {
                "authorId": "1708945",
                "name": "W. Gerstner"
              },
              {
                "authorId": "30521249",
                "name": "R. Ritz"
              },
              {
                "authorId": "49939454",
                "name": "J. V. van Hemmen"
              }
            ]
          }
        },
        {
          "citedcorpusid": 7151414,
          "isinfluential": false,
          "contexts": [
            "7 TSES* [38] 36 44 36 CopNet* [31] 61 100 64",
            "StereoSpike also outperforms TSES [38] and CopNet [31], which only predict depth at event positions."
          ],
          "intents": [
            "--",
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Improved Cooperative Stereo Matching for Dynamic Vision Sensors with Ground Truth Evaluation",
            "abstract": "",
            "year": 2017,
            "venue": "2017 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)",
            "authors": [
              {
                "authorId": "47105977",
                "name": "Ewa Piatkowska"
              },
              {
                "authorId": "1824241",
                "name": "J. Kogler"
              },
              {
                "authorId": "1768812",
                "name": "A. Belbachir"
              },
              {
                "authorId": "1990797",
                "name": "M. Gelautz"
              }
            ]
          }
        },
        {
          "citedcorpusid": 81981856,
          "isinfluential": true,
          "contexts": [
            "DTC-SPADE [44] is not represented here as the authors did not use the same colormap as other studies and did not publish their code.",
            "The model in [22] used the same input embedding and backbone as [21], but proposed a preliminary network using spatially-adaptive normalization (SPADE) [23] to reconstruct grayscale intensity images jointly with depth maps.",
            "DTC-SPADE [44] has managed to incorporate elements of the latter for the beneﬁt of accuracy and without exploding algorithmic size.",
            "[44] also used a similar matching backbone as well as SPADE, but differed in its input encoding.",
            "The model in [1] used the same input embedding and backbone as [39], but proposed a preliminary network using spatially-adaptive normalization (SPADE) [33] to reconstruct grayscale intensity images jointly with depth maps."
          ],
          "intents": [
            "--",
            "['methodology']",
            "--",
            "--",
            "--"
          ],
          "cited_paper_info": {
            "title": "Semantic Image Synthesis With Spatially-Adaptive Normalization",
            "abstract": "We propose spatially-adaptive normalization, a simple but effective layer for synthesizing photorealistic images given an input semantic layout. Previous methods directly feed the semantic layout as input to the network, forcing the network to memorize the information throughout all the layers. Instead, we propose using the input layout for modulating the activations in normalization layers through a spatially-adaptive, learned affine transformation. Experiments on several challenging datasets demonstrate the superiority of our method compared to existing approaches, regarding both visual fidelity and alignment with input layouts. Finally, our model allows users to easily control the style and content of image synthesis results as well as create multi-modal results. Code is available upon publication.",
            "year": 2019,
            "venue": "Computer Vision and Pattern Recognition",
            "authors": [
              {
                "authorId": "2071929129",
                "name": "Taesung Park"
              },
              {
                "authorId": "39793900",
                "name": "Ming-Yu Liu"
              },
              {
                "authorId": "2195314",
                "name": "Ting-Chun Wang"
              },
              {
                "authorId": "2436356",
                "name": "Jun-Yan Zhu"
              }
            ]
          }
        },
        {
          "citedcorpusid": 86423050,
          "isinfluential": false,
          "contexts": [
            "From a visual neuroscience perspective, there is strong evidence that binocular cues are mixed in a similar manner at very early stages of the visual system [41]."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Foundations of Vision",
            "abstract": "For people with intact vision, it would be hard to imagine what life would be like without it. Vision is the sense that we rely on most to perform everyday tasks. Imagine if instead you had to accomplish all of your daily routines while blindfolded. We depend on vision whenever we navigate to work by foot or by car, search for our favorite snack in the grocery aisle, or scan the words on a printed page trying to extract their underlying meaning. For many mammals and especially for higher primates, vision is essential for survival, allowing us to reliably identify objects, food sources, conspecifics, and the layout of the surrounding environment. Beyond its survival value, our visual sense provides us with an intrinsic source of beauty and pleasure, a tapestry of richly detailed experiences. We may find ourselves captivated by an expansive view from a seaside cliff, a swirl of colors in an abstract oil painting, or an endearing smile from a close friend. The power of vision lies in the dense array of information that it provides about the surrounding environment, from distances near and far, registered by the geometry of light patterns projected onto the backs of the eyes. It is commonly said that a picture is worth a thousand words. Consider for a moment the chirping activity of the ganglion cells in your retinae right now, and their outgoing bundle of roughly 1 million axonal fibers through each optic tract. Following each glance or microsaccade, a new pattern of activity is registered by the photoreceptors, then processed by the bipolar neurons and the ganglion cells, after which these high-bandwidth signals are relayed to the lateral geniculate nucleus and ultimately to the visual cortex for in-depth analysis. Psychologists and neuroscientists have made remarkable advances in understanding the functional organization of the visual system, uncovering important clues about its perceptual mechanisms and underlying neural codes. Computational neuroscientist David Marr (1982) once quipped that the function of vision is “to know what is where by looking.” As Marr well appreciated, the problem underlying vision is far easier to summarize than it is to solve. Our visual system does a remarkably good job of solving this problem, getting things pretty much right about 99.9% of the time. On those rare occasions where the visual system seems to come up with “the wrong answer,” as in the case of visual illusions, scientists can gain insight into the powerful computations that underlie the automatic inferences made by the visual system.",
            "year": 2018,
            "venue": "",
            "authors": [
              {
                "authorId": "145794570",
                "name": "F. Tong"
              }
            ]
          }
        },
        {
          "citedcorpusid": 102496818,
          "isinfluential": false,
          "contexts": [
            "The model in [7] was the ﬁrst successful multi-scale architecture designed for depth estimation from RGB images, and was consequently followed by advances based on similar approaches [24] [23] [14]."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Predicting Depth, Surface Normals and Semantic Labels with a Common Multi-scale Convolutional Architecture",
            "abstract": "In this paper we address three different computer vision tasks using a single basic architecture: depth prediction, surface normal estimation, and semantic labeling. We use a multiscale convolutional network that is able to adapt easily to each task using only small modifications, regressing from the input image to the output map directly. Our method progressively refines predictions using a sequence of scales, and captures many image details without any superpixels or low-level segmentation. We achieve state-of-the-art performance on benchmarks for all three tasks.",
            "year": 2014,
            "venue": "IEEE International Conference on Computer Vision",
            "authors": [
              {
                "authorId": "2060028",
                "name": "D. Eigen"
              },
              {
                "authorId": "2276554",
                "name": "R. Fergus"
              }
            ]
          }
        },
        {
          "citedcorpusid": 118684904,
          "isinfluential": false,
          "contexts": [
            "Dynamic Vision Sensors (DVS) have recently gathered the interest of scientists and industrial actors, thanks to a growing number of research papers explaining how to process their output [11]."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Event-Based Vision: A Survey",
            "abstract": "Event cameras are bio-inspired sensors that differ from conventional frame cameras: Instead of capturing images at a fixed rate, they asynchronously measure per-pixel brightness changes, and output a stream of events that encode the time, location and sign of the brightness changes. Event cameras offer attractive properties compared to traditional cameras: high temporal resolution (in the order of $\\mu$μs), very high dynamic range (140 dB versus 60 dB), low power consumption, and high pixel bandwidth (on the order of kHz) resulting in reduced motion blur. Hence, event cameras have a large potential for robotics and computer vision in challenging scenarios for traditional cameras, such as low-latency, high speed, and high dynamic range. However, novel methods are required to process the unconventional output of these sensors in order to unlock their potential. This paper provides a comprehensive overview of the emerging field of event-based vision, with a focus on the applications and the algorithms developed to unlock the outstanding properties of event cameras. We present event cameras from their working principle, the actual sensors that are available and the tasks that they have been used for, from low-level vision (feature detection and tracking, optic flow, etc.) to high-level vision (reconstruction, segmentation, recognition). We also discuss the techniques developed to process events, including learning-based techniques, as well as specialized processors for these novel sensors, such as spiking neural networks. Additionally, we highlight the challenges that remain to be tackled and the opportunities that lie ahead in the search for a more efficient, bio-inspired way for machines to perceive and interact with the world.",
            "year": 2019,
            "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
            "authors": [
              {
                "authorId": "144036711",
                "name": "Guillermo Gallego"
              },
              {
                "authorId": "1694635",
                "name": "T. Delbrück"
              },
              {
                "authorId": "33780923",
                "name": "G. Orchard"
              },
              {
                "authorId": "1897771",
                "name": "C. Bartolozzi"
              },
              {
                "authorId": "1736425",
                "name": "B. Taba"
              },
              {
                "authorId": "1860631",
                "name": "A. Censi"
              },
              {
                "authorId": "2864731",
                "name": "Stefan Leutenegger"
              },
              {
                "authorId": "2052135690",
                "name": "A. Davison"
              },
              {
                "authorId": "3302681",
                "name": "J. Conradt"
              },
              {
                "authorId": "1751586",
                "name": "Kostas Daniilidis"
              },
              {
                "authorId": "2075371",
                "name": "D. Scaramuzza"
              }
            ]
          }
        },
        {
          "citedcorpusid": 207935891,
          "isinfluential": false,
          "contexts": [
            "Implemented on dedicated chips such as Intel Loihi [5], IBM TrueNorth [2], Brainchip Akida [40] or Tianjic [34], these models could become a new paradigm for ultra-low power computation in the coming years.",
            "We therefore consider that Brainchip’s Akida chip [35] is a good fit.",
            "Implemented on dedicated chips such as Intel Loihi [3], IBM TrueNorth [1], Brainchip Akida [35] or Tianjic [29], these models could become a new paradigm for ultra-low power computation in the coming years."
          ],
          "intents": [
            "--",
            "['background']",
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "A Hardware-Deployable Neuromorphic Solution for Encoding and Classification of Electronic Nose Data",
            "abstract": "In several application domains, electronic nose systems employing conventional data processing approaches incur substantial power and computational costs and limitations, such as significant latency and poor accuracy for classification. Recent developments in spike-based bio-inspired approaches have delivered solutions for the highly accurate classification of multivariate sensor data with minimized computational and power requirements. Although these methods have addressed issues related to efficient data processing and classification accuracy, other areas, such as reducing the processing latency to support real-time application and deploying spike-based solutions on supported hardware, have yet to be studied in detail. Through this investigation, we proposed a spiking neural network (SNN)-based classifier, implemented in a chip-emulation-based development environment, that can be seamlessly deployed on a neuromorphic system-on-a-chip (NSoC). Under three different scenarios of increasing complexity, the SNN was determined to be able to classify real-valued sensor data with greater than 90% accuracy and with a maximum latency of 3 s on the software-based platform. Highlights of this work included the design and implementation of a novel encoder for artificial olfactory systems, implementation of unsupervised spike-timing-dependent plasticity (STDP) for learning, and a foundational study on early classification capability using the SNN-based classifier.",
            "year": 2019,
            "venue": "Italian National Conference on Sensors",
            "authors": [
              {
                "authorId": "4075505",
                "name": "Anup Vanarse"
              },
              {
                "authorId": "1936403",
                "name": "A. Osseiran"
              },
              {
                "authorId": "145651396",
                "name": "A. Rassau"
              },
              {
                "authorId": "2150795598",
                "name": "P. van der Made"
              }
            ]
          }
        },
        {
          "citedcorpusid": 212414806,
          "isinfluential": false,
          "contexts": [
            "A notable exception is (Gehrig et al. 2020), but they only regressed 3 variables, while we propose here to regress the values of 260× 346 = 89960 pixels."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Event-Based Angular Velocity Regression with Spiking Networks",
            "abstract": "Spiking Neural Networks (SNNs) are bio-inspired networks that process information conveyed as temporal spikes rather than numeric values. An example of a sensor providing such data is the event-camera. It only produces an event when a pixel reports a significant brightness change. Similarly, the spiking neuron of an SNN only produces a spike whenever a significant number of spikes occur within a short period of time. Due to their spike-based computational model, SNNs can process output from event-based, asynchronous sensors without any pre-processing at extremely lower power unlike standard artificial neural networks. This is possible due to specialized neuromorphic hardware that implements the highly-parallelizable concept of SNNs in silicon. Yet, SNNs have not enjoyed the same rise of popularity as artificial neural networks. This not only stems from the fact that their input format is rather unconventional but also due to the challenges in training spiking networks. Despite their temporal nature and recent algorithmic advances, they have been mostly evaluated on classification problems. We propose, for the first time, a temporal regression problem of numerical values given events from an event-camera. We specifically investigate the prediction of the 3- DOF angular velocity of a rotating event-camera with an SNN. The difficulty of this problem arises from the prediction of angular velocities continuously in time directly from irregular, asynchronous event-based input. Directly utilising the output of event-cameras without any pre-processing ensures that we inherit all the benefits that they provide over conventional cameras. That is high-temporal resolution, high-dynamic range and no motion blur. To assess the performance of SNNs on this task, we introduce a synthetic event-camera dataset generated from real-world panoramic images and show that we can successfully train an SNN to perform angular velocity regression.",
            "year": 2020,
            "venue": "IEEE International Conference on Robotics and Automation",
            "authors": [
              {
                "authorId": "8329387",
                "name": "Mathias Gehrig"
              },
              {
                "authorId": "144419894",
                "name": "S. Shrestha"
              },
              {
                "authorId": "1388053822",
                "name": "Daniel Mouritzen"
              },
              {
                "authorId": "2075371",
                "name": "D. Scaramuzza"
              }
            ]
          }
        },
        {
          "citedcorpusid": 223957202,
          "isinfluential": true,
          "contexts": [
            "We then investigated outdoor scenarios by following the same training, test and validation splits as [19].",
            "In outdoor scenarios, our model outperforms E2Depth [19] by a large margin at all cutoff distances, and with 25× fewer parameters.",
            "divide by the maximum number of spike count) for an easier-to-learn distribution of data and better generalization [19].",
            "However, we argue that the task of depth recovery from events has a minor temporal component and can be solved by a fully feedforward model with minimal temporal knowledge; therefore the use of convLSTMs in [19] is suboptimal and unnecessarily costly in terms of computation.",
            "Finally, in [19], dense metric depth was recovered from only one camera, and showed good performances with a recurrent, monocular encoder-decoder architecture on outdoor sequences.",
            "As a counterbalance measure, we used data augmentation; as [19].",
            "As in [19], we used a combination of a regression loss with a regularization loss.",
            "Please note that [19] also used simulated data that did not come from MVSEC, but this does not prevent StereoSpike from outperforming this competitor by a large margin with substantially less parameters (cf.",
            "According to [19], the minimization of this term encourages smooth depth changes as well as sharp depth discontinuities in the depth map prediction, hence helping the network to represent objects that stand out of the background of the scene (e.",
            "This is not due to the fact that [19] takes input from only one camera as the monocular version of StereoSpike still remains consistently superior to this competitor."
          ],
          "intents": [
            "['methodology']",
            "['methodology']",
            "['background']",
            "['background']",
            "['background']",
            "['methodology']",
            "['methodology']",
            "['methodology']",
            "['background']",
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Learning Monocular Dense Depth from Events",
            "abstract": "Event cameras are novel sensors that output brightness changes in the form of a stream of asynchronous ”events” instead of intensity frames. Compared to conventional image sensors, they offer significant advantages: high temporal resolution, high dynamic range, no motion blur, and much lower bandwidth. Recently, learning-based approaches have been applied to event-based data, thus unlocking their potential and making significant progress in a variety of tasks, such as monocular depth prediction. Most existing approaches use standard feed-forward architectures to generate network predictions, which do not leverage the temporal consistency presents in the event stream. We propose a recurrent architecture to solve this task and show significant improvement over standard feed-forward methods. In particular, our method generates dense depth predictions using a monocular setup, which has not been shown previously. We pretrain our model using a new dataset containing events and depth maps recorded in the CARLA simulator. We test our method on the Multi Vehicle Stereo Event Camera Dataset (MVSEC). Quantitative experiments show up to 50% improvement in average depth error with respect to previous event-based methods. Code and dataset are available at: http://rpg.ifi.uzh.ch/e2depth",
            "year": 2020,
            "venue": "International Conference on 3D Vision",
            "authors": [
              {
                "authorId": "2065112865",
                "name": "Javier Hidalgo-Carri'o"
              },
              {
                "authorId": "51152279",
                "name": "Daniel Gehrig"
              },
              {
                "authorId": "2075371",
                "name": "D. Scaramuzza"
              }
            ]
          }
        },
        {
          "citedcorpusid": 226976144,
          "isinfluential": true,
          "contexts": [
            "While SNNs generally remain less accurate than their analog counterpart (i.e., Analog Neural Networks or ANNs), the gap in accuracy is decreasing, even on challenging problems like ImageNet [9].",
            "So far, SNNs have been used for classification tasks like image recognition [8], object detection [18], or motion segmentation [27].",
            "We use the derivative of the arctan function as our surrogate gradient in this paper, as suggested in [8].",
            ", Analog Neural Networks or ANNs), the gap in accuracy is decreasing, even on challenging problems like ImageNet [8]."
          ],
          "intents": [
            "--",
            "['methodology']",
            "['methodology']",
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Incorporating Learnable Membrane Time Constant to Enhance Learning of Spiking Neural Networks",
            "abstract": "Spiking Neural Networks (SNNs) have attracted enormous research interest due to temporal information processing capability, low power consumption, and high biological plausibility. However, the formulation of efficient and high-performance learning algorithms for SNNs is still challenging. Most existing learning methods learn weights only, and require manual tuning of the membrane-related parameters that determine the dynamics of a single spiking neuron. These parameters are typically chosen to be the same for all neurons, which limits the diversity of neurons and thus the expressiveness of the resulting SNNs. In this paper, we take inspiration from the observation that membrane-related parameters are different across brain regions, and propose a training algorithm that is capable of learning not only the synaptic weights but also the membrane time constants of SNNs. We show that incorporating learnable membrane time constants can make the network less sensitive to initial values and can speed up learning. In addition, we reevaluate the pooling methods in SNNs and find that max-pooling will not lead to significant information loss and have the advantage of low computation cost and binary compatibility. We evaluate the proposed method for image classification tasks on both traditional static MNIST, Fashion-MNIST, CIFAR-10 datasets, and neuromorphic N-MNIST, CIFAR10-DVS, DVS128 Gesture datasets. The experiment results show that the proposed method outperforms the state-of-the-art accuracy on nearly all datasets, using fewer time-steps. Our codes are available at https://github.com/fangwei123456/Parametric-Leaky-Integrate-and-Fire-Spiking-Neuron.",
            "year": 2020,
            "venue": "IEEE International Conference on Computer Vision",
            "authors": [
              {
                "authorId": "2087000501",
                "name": "Wei Fang"
              },
              {
                "authorId": "1746114",
                "name": "Zhaofei Yu"
              },
              {
                "authorId": "2115935255",
                "name": "Yanqing Chen"
              },
              {
                "authorId": "2441104",
                "name": "T. Masquelier"
              },
              {
                "authorId": "34097174",
                "name": "Tiejun Huang"
              },
              {
                "authorId": "40161651",
                "name": "Yonghong Tian"
              }
            ]
          }
        },
        {
          "citedcorpusid": 235359262,
          "isinfluential": true,
          "contexts": [
            "The bottleneck consisted in 2 SEWResBlocks [9] following each other and with ADD connect function.",
            "While SNNs generally remain less accurate than their analog counterpart (i.e., Analog Neural Networks or ANNs), the gap in accuracy is decreasing, even on challenging problems like ImageNet [9].",
            "So far, SNNs have been used for classification tasks like image recognition [9,10], object detection [3,20], or motion segmentation [31].",
            ", Analog Neural Networks or ANNs), the gap in accuracy is decreasing, even on challenging problems like ImageNet [9]."
          ],
          "intents": [
            "['background']",
            "--",
            "['methodology']",
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Deep Residual Learning in Spiking Neural Networks",
            "abstract": "Deep Spiking Neural Networks (SNNs) present optimization difficulties for gradient-based approaches due to discrete binary activation and complex spatial-temporal dynamics. Considering the huge success of ResNet in deep learning, it would be natural to train deep SNNs with residual learning. Previous Spiking ResNet mimics the standard residual block in ANNs and simply replaces ReLU activation layers with spiking neurons, which suffers the degradation problem and can hardly implement residual learning. In this paper, we propose the spike-element-wise (SEW) ResNet to realize residual learning in deep SNNs. We prove that the SEW ResNet can easily implement identity mapping and overcome the vanishing/exploding gradient problems of Spiking ResNet. We evaluate our SEW ResNet on ImageNet, DVS Gesture, and CIFAR10-DVS datasets, and show that SEW ResNet outperforms the state-of-the-art directly trained SNNs in both accuracy and time-steps. Moreover, SEW ResNet can achieve higher performance by simply adding more layers, providing a simple method to train deep SNNs. To our best knowledge, this is the first time that directly training deep SNNs with more than 100 layers becomes possible. Our codes are available at https://github.com/fangwei123456/Spike-Element-Wise-ResNet.",
            "year": 2021,
            "venue": "Neural Information Processing Systems",
            "authors": [
              {
                "authorId": "2087000501",
                "name": "Wei Fang"
              },
              {
                "authorId": "1746114",
                "name": "Zhaofei Yu"
              },
              {
                "authorId": "2115935255",
                "name": "Yanqing Chen"
              },
              {
                "authorId": "34097174",
                "name": "Tiejun Huang"
              },
              {
                "authorId": "2441104",
                "name": "T. Masquelier"
              },
              {
                "authorId": "40161651",
                "name": "Yonghong Tian"
              }
            ]
          }
        },
        {
          "citedcorpusid": 241440878,
          "isinfluential": false,
          "contexts": [
            "While SNNs generally remain less accurate than their analog counterpart (i.e., Analog Neural Networks or ANNs), the gap in accuracy is decreasing, even on challenging problems like ImageNet [9].",
            "So far, SNNs have been used for classiﬁcation tasks like image recognition [9,10], object detection [3,20], or motion segmentation [31]."
          ],
          "intents": [
            "['background']",
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Residual",
            "abstract": "The use of lime in No-Tillage System is an issue that still raises many questions regarding the efficiency of the use of this correction in surface application, as well as their answer over time. Thus the present study aimed to test different methods of application and reapplication of lime in corn in an area with ten years of tillage. The experimental design was a completely randomized block design with four replications in a 3x4 factorial design, ie three application modes (lime incorporated in coverage and absence of lime) and four times reapplication over the ten year. Six doses of lime (0.0, 0.5, 1.0, 1.5, 2.0 and 2.5 times the indicated dose) were also tested. The study area was cultivated for ten years in the SPD and from deployment (season 99/00)",
            "year": 2020,
            "venue": "Encyclopedic Dictionary of Archaeology",
            "authors": [
              {
                "authorId": "82855837",
                "name": "T. Parente"
              },
              {
                "authorId": "86912758",
                "name": "E. Lazarini"
              },
              {
                "authorId": "2095469002",
                "name": "Sheila Caioni"
              },
              {
                "authorId": "89070540",
                "name": "Raul Sobrinho Pivetta"
              },
              {
                "authorId": "90658307",
                "name": "Wandemberg Assis Silva Oliveira"
              }
            ]
          }
        }
      ]
    },
    "271855627": {
      "citing_paper_info": {
        "title": "EV-MGDispNet: Motion-Guided Event-Based Stereo Disparity Estimation Network with Left-Right Consistency",
        "abstract": "Event cameras have the potential to revolutionize the field of robot vision, particularly in areas like stereo disparity estimation, owing to their high temporal resolution and high dynamic range. Many studies use deep learning for event camera stereo disparity estimation. However, these methods fail to fully exploit the temporal information in the event stream to acquire clear event representations. Additionally, there is room for further reduction in pixel shifts in the feature maps before constructing the cost volume. In this paper, we propose EV-MGDispNet, a novel event-based stereo disparity estimation method. Firstly, we propose an edge-aware aggregation (EAA) module, which fuses event frames and motion confidence maps to generate a novel clear event representation. Then, we propose a motion-guided attention (MGA) module, where motion confidence maps utilize deformable transformer encoders to enhance the feature map with more accurate edges. Finally, we also add a census left-right consistency loss function to enhance the left-right consistency of stereo event representation. Through conducting experiments within challenging real-world driving scenarios, we validate that our method outperforms currently known state-of-the-art methods in terms of mean absolute error (MAE) and root mean square error (RMSE) metrics.",
        "year": 2024,
        "venue": "arXiv.org",
        "authors": [
          {
            "authorId": "2107975649",
            "name": "Junjie Jiang"
          },
          {
            "authorId": "2229193295",
            "name": "Zhuang Hao"
          },
          {
            "authorId": "2152665453",
            "name": "Xinjie Huang"
          },
          {
            "authorId": "2052315650",
            "name": "Delei Kong"
          },
          {
            "authorId": "2165663624",
            "name": "Zheng Fang"
          }
        ]
      },
      "citation_summary": {
        "citation_count": 17,
        "unique_cited_count": 16,
        "influential_count": 2,
        "detailed_records_count": 17
      },
      "cited_papers": [
        "222208633",
        "54115081",
        "13373696",
        "247593948",
        "246834559",
        "20954901",
        "14193490",
        "195859047",
        "206770307",
        "118684904",
        "244306440",
        "207761262",
        "13756489",
        "235306612",
        "4252896",
        "102352684"
      ],
      "citation_details": [
        {
          "citedcorpusid": 4252896,
          "isinfluential": false,
          "contexts": [
            "For deep learning-based approaches [40]–[42], the left and right images are used as feature maps in the network to construct the cost volume."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Pyramid Stereo Matching Network",
            "abstract": "Recent work has shown that depth estimation from a stereo pair of images can be formulated as a supervised learning task to be resolved with convolutional neural networks (CNNs). However, current architectures rely on patch-based Siamese networks, lacking the means to exploit context information for finding correspondence in ill-posed regions. To tackle this problem, we propose PSMNet, a pyramid stereo matching network consisting of two main modules: spatial pyramid pooling and 3D CNN. The spatial pyramid pooling module takes advantage of the capacity of global context information by aggregating context in different scales and locations to form a cost volume. The 3D CNN learns to regularize cost volume using stacked multiple hourglass networks in conjunction with intermediate supervision. The proposed approach was evaluated on several benchmark datasets. Our method ranked first in the KITTI 2012 and 2015 leaderboards before March 18, 2018. The codes of PSMNet are available at: https://github.com/JiaRenChang/PSMNet.",
            "year": 2018,
            "venue": "2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition",
            "authors": [
              {
                "authorId": "2936466",
                "name": "Jia-Ren Chang"
              },
              {
                "authorId": "2143438143",
                "name": "Yonghao Chen"
              }
            ]
          }
        },
        {
          "citedcorpusid": 13373696,
          "isinfluential": false,
          "contexts": [
            "This method is inspired by [38], but differs in the definition of t max , which represents the maximum timestamp in the event stream."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "HOTS: A Hierarchy of Event-Based Time-Surfaces for Pattern Recognition",
            "abstract": "",
            "year": 2017,
            "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
            "authors": [
              {
                "authorId": "1807856",
                "name": "Xavier Lagorce"
              },
              {
                "authorId": "33780923",
                "name": "G. Orchard"
              },
              {
                "authorId": "3008126",
                "name": "F. Galluppi"
              },
              {
                "authorId": "2075335081",
                "name": "Bertram E. Shi"
              },
              {
                "authorId": "1750848",
                "name": "R. Benosman"
              }
            ]
          }
        },
        {
          "citedcorpusid": 13756489,
          "isinfluential": false,
          "contexts": [
            "Unlike global attention in [44], deformable attention only samples K points around the reference point to calculate the attention result."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Attention is All you Need",
            "abstract": "The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.",
            "year": 2017,
            "venue": "Neural Information Processing Systems",
            "authors": [
              {
                "authorId": "40348417",
                "name": "Ashish Vaswani"
              },
              {
                "authorId": "1846258",
                "name": "Noam M. Shazeer"
              },
              {
                "authorId": "3877127",
                "name": "Niki Parmar"
              },
              {
                "authorId": "39328010",
                "name": "Jakob Uszkoreit"
              },
              {
                "authorId": "145024664",
                "name": "Llion Jones"
              },
              {
                "authorId": "19177000",
                "name": "Aidan N. Gomez"
              },
              {
                "authorId": "40527594",
                "name": "Lukasz Kaiser"
              },
              {
                "authorId": "3443442",
                "name": "I. Polosukhin"
              }
            ]
          }
        },
        {
          "citedcorpusid": 14193490,
          "isinfluential": false,
          "contexts": [
            "It possesses extensive applications in the tasks of robotics [13], autonomous driving [14], [15] and virtual augmented reality (VR/AR) [16], offering a broad range of possibilities."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Stereo vision based indoor/outdoor navigation for flying robots",
            "abstract": "",
            "year": 2013,
            "venue": "2013 IEEE/RSJ International Conference on Intelligent Robots and Systems",
            "authors": [
              {
                "authorId": "40499793",
                "name": "Korbinian Schmid"
              },
              {
                "authorId": "34598337",
                "name": "Teodor Tomic"
              },
              {
                "authorId": "32882253",
                "name": "Felix Ruess"
              },
              {
                "authorId": "3335378",
                "name": "H. Hirschmüller"
              },
              {
                "authorId": "1787158",
                "name": "M. Suppa"
              }
            ]
          }
        },
        {
          "citedcorpusid": 20954901,
          "isinfluential": false,
          "contexts": [
            "The underlying principle of stereo disparity estimation relies on the assumption that the left and right images demonstrate left-right consistency [37].",
            "In stereo disparity estimation, accurate disparity estimation relies on input images with clear object boundaries [35]– [37]."
          ],
          "intents": [
            "['background']",
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Accomplishments and challenges of computer stereo vision",
            "abstract": "",
            "year": 2010,
            "venue": "Proceedings Elmar",
            "authors": [
              {
                "authorId": "9458111",
                "name": "M. Gosta"
              },
              {
                "authorId": "1755973",
                "name": "M. Grgic"
              }
            ]
          }
        },
        {
          "citedcorpusid": 54115081,
          "isinfluential": false,
          "contexts": [
            "It possesses extensive applications in the tasks of robotics [13], autonomous driving [14], [15] and virtual augmented reality (VR/AR) [16], offering a broad range of possibilities."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Depth from motion for smartphone AR",
            "abstract": "Augmented reality (AR) for smartphones has matured from a technology for earlier adopters, available only on select high-end phones, to one that is truly available to the general public. One of the key breakthroughs has been in low-compute methods for six degree of freedom (6DoF) tracking on phones using only the existing hardware (camera and inertial sensors). 6DoF tracking is the cornerstone of smartphone AR allowing virtual content to be precisely locked on top of the real world. However, to really give users the impression of believable AR, one requires mobile depth. Without depth, even simple effects such as a virtual object being correctly occluded by the real-world is impossible. However, requiring a mobile depth sensor would severely restrict the access to such features. In this article, we provide a novel pipeline for mobile depth that supports a wide array of mobile phones, and uses only the existing monocular color sensor. Through several technical contributions, we provide the ability to compute low latency dense depth maps using only a single CPU core of a wide range of (medium-high) mobile phones. We demonstrate the capabilities of our approach on high-level AR applications including real-time navigation and shopping.",
            "year": 2018,
            "venue": "ACM Transactions on Graphics",
            "authors": [
              {
                "authorId": "39596866",
                "name": "Julien P. C. Valentin"
              },
              {
                "authorId": "2371390",
                "name": "Adarsh Kowdle"
              },
              {
                "authorId": "50329510",
                "name": "J. Barron"
              },
              {
                "authorId": "34004812",
                "name": "N. Wadhwa"
              },
              {
                "authorId": "52113705",
                "name": "Max Dzitsiuk"
              },
              {
                "authorId": "2076750024",
                "name": "Michael Schoenberg"
              },
              {
                "authorId": "2124843633",
                "name": "V. Verma"
              },
              {
                "authorId": "52100919",
                "name": "Ambrus Csaszar"
              },
              {
                "authorId": "2061819196",
                "name": "Eric Turner"
              },
              {
                "authorId": "1786152",
                "name": "Ivan Dryanovski"
              },
              {
                "authorId": "2053591286",
                "name": "João Afonso"
              },
              {
                "authorId": "144894826",
                "name": "J. Pascoal"
              },
              {
                "authorId": "1702044",
                "name": "Konstantine Tsotsos"
              },
              {
                "authorId": "46192307",
                "name": "Mira Leung"
              },
              {
                "authorId": "46624426",
                "name": "Mirko Schmidt"
              },
              {
                "authorId": "2818379",
                "name": "O. Guleryuz"
              },
              {
                "authorId": "2121982",
                "name": "S. Khamis"
              },
              {
                "authorId": "3440762",
                "name": "V. Tankovich"
              },
              {
                "authorId": "34772804",
                "name": "S. Fanello"
              },
              {
                "authorId": "79406746",
                "name": "S. Izadi"
              },
              {
                "authorId": "2086328",
                "name": "Christoph Rhemann"
              }
            ]
          }
        },
        {
          "citedcorpusid": 102352684,
          "isinfluential": false,
          "contexts": [
            "Finally, we utilize the winner-take-all (WTA) [12] and refinement module [32] to estimate disparity.",
            "Finally, we utilize a refinement module [32] to recover the multi-scale predicted disparity maps scale."
          ],
          "intents": [
            "['methodology']",
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "StereoDRNet: Dilated Residual StereoNet",
            "abstract": "We propose a system that uses a convolution neural network (CNN) to estimate depth from a stereo pair followed by volumetric fusion of the predicted depth maps to produce a 3D reconstruction of a scene. Our proposed depth refinement architecture, predicts view-consistent disparity and occlusion maps that helps the fusion system to produce geometrically consistent reconstructions. We utilize 3D dilated convolutions in our proposed cost filtering network that yields better filtering while almost halving the computational cost in comparison to state of the art cost filtering architectures. For feature extraction we use the Vortex Pooling architecture. The proposed method achieves state of the art results in KITTI 2012, KITTI 2015 and ETH 3D stereo benchmarks. Finally, we demonstrate that our system is able to produce high fidelity 3D scene reconstructions that outperforms the state of the art stereo system.",
            "year": 2019,
            "venue": "Computer Vision and Pattern Recognition",
            "authors": [
              {
                "authorId": "3428200",
                "name": "Rohan Chabra"
              },
              {
                "authorId": "20128275",
                "name": "Julian Straub"
              },
              {
                "authorId": "40517410",
                "name": "Chris Sweeney"
              },
              {
                "authorId": "50366818",
                "name": "Richard A. Newcombe"
              },
              {
                "authorId": "145472944",
                "name": "H. Fuchs"
              }
            ]
          }
        },
        {
          "citedcorpusid": 118684904,
          "isinfluential": false,
          "contexts": [
            "E VENT cameras [2]–[7], inspired by the neural structure of the human eye in response to the changes of illumination, are a type of neuromorphic vision sensor [8]."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Event-Based Vision: A Survey",
            "abstract": "Event cameras are bio-inspired sensors that differ from conventional frame cameras: Instead of capturing images at a fixed rate, they asynchronously measure per-pixel brightness changes, and output a stream of events that encode the time, location and sign of the brightness changes. Event cameras offer attractive properties compared to traditional cameras: high temporal resolution (in the order of $\\mu$μs), very high dynamic range (140 dB versus 60 dB), low power consumption, and high pixel bandwidth (on the order of kHz) resulting in reduced motion blur. Hence, event cameras have a large potential for robotics and computer vision in challenging scenarios for traditional cameras, such as low-latency, high speed, and high dynamic range. However, novel methods are required to process the unconventional output of these sensors in order to unlock their potential. This paper provides a comprehensive overview of the emerging field of event-based vision, with a focus on the applications and the algorithms developed to unlock the outstanding properties of event cameras. We present event cameras from their working principle, the actual sensors that are available and the tasks that they have been used for, from low-level vision (feature detection and tracking, optic flow, etc.) to high-level vision (reconstruction, segmentation, recognition). We also discuss the techniques developed to process events, including learning-based techniques, as well as specialized processors for these novel sensors, such as spiking neural networks. Additionally, we highlight the challenges that remain to be tackled and the opportunities that lie ahead in the search for a more efficient, bio-inspired way for machines to perceive and interact with the world.",
            "year": 2019,
            "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
            "authors": [
              {
                "authorId": "144036711",
                "name": "Guillermo Gallego"
              },
              {
                "authorId": "1694635",
                "name": "T. Delbrück"
              },
              {
                "authorId": "33780923",
                "name": "G. Orchard"
              },
              {
                "authorId": "1897771",
                "name": "C. Bartolozzi"
              },
              {
                "authorId": "1736425",
                "name": "B. Taba"
              },
              {
                "authorId": "1860631",
                "name": "A. Censi"
              },
              {
                "authorId": "2864731",
                "name": "Stefan Leutenegger"
              },
              {
                "authorId": "2052135690",
                "name": "A. Davison"
              },
              {
                "authorId": "3302681",
                "name": "J. Conradt"
              },
              {
                "authorId": "1751586",
                "name": "Kostas Daniilidis"
              },
              {
                "authorId": "2075371",
                "name": "D. Scaramuzza"
              }
            ]
          }
        },
        {
          "citedcorpusid": 195859047,
          "isinfluential": false,
          "contexts": [
            "Stereo depth estimation [9], [10] stands as a fundamental task in computer vision [11] and robotics [12].",
            "Finally, we utilize the winner-take-all (WTA) [12] and refinement module [32] to estimate disparity."
          ],
          "intents": [
            "['background']",
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "A Taxonomy and Evaluation of Dense Two-Frame Stereo Correspondence Algorithms",
            "abstract": "",
            "year": 2001,
            "venue": "Proceedings IEEE Workshop on Stereo and Multi-Baseline Vision (SMBV 2001)",
            "authors": [
              {
                "authorId": "1709053",
                "name": "D. Scharstein"
              },
              {
                "authorId": "1717841",
                "name": "R. Szeliski"
              }
            ]
          }
        },
        {
          "citedcorpusid": 206770307,
          "isinfluential": false,
          "contexts": [
            "2) Total Loss Function: We utilize both the smooth L 1 loss function [46] and the left-right consistency census loss to construct the total loss function."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Fast R-CNN",
            "abstract": "This paper proposes Fast R-CNN, a clean and fast framework for object detection. Compared to traditional R-CNN, and its accelerated version SPPnet, Fast R-CNN trains networks using a multi-task loss in a single training stage. The multi-task loss simplifies learning and improves detection accuracy. Unlike SPPnet, all network layers can be updated during fine-tuning. We show that this difference has practical ramifications for very deep networks, such as VGG16, where mAP suffers when only the fully-connected layers are updated. Compared to\"slow\"R-CNN, Fast R-CNN is 9x faster at training VGG16 for detection, 213x faster at test-time, and achieves a significantly higher mAP on PASCAL VOC 2012. Compared to SPPnet, Fast R-CNN trains VGG16 3x faster, tests 10x faster, and is more accurate. Fast R-CNN is implemented in Python and C++ and is available under the open-source MIT License at https://github.com/rbgirshick/fast-rcnn",
            "year": 2015,
            "venue": "",
            "authors": [
              {
                "authorId": "2983898",
                "name": "Ross B. Girshick"
              }
            ]
          }
        },
        {
          "citedcorpusid": 207761262,
          "isinfluential": false,
          "contexts": [
            "In Table IV, we employ two fundamental metrics, namely Peak Signal-to-Noise Ratio (PSNR) and Structural Similarity (SSIM) [47], to evaluate the efficiency of the different configurations which sequentially remove the left-right consistency census loss and EAA modules from EV-MGDispNet."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Image quality assessment: from error visibility to structural similarity",
            "abstract": "",
            "year": 2004,
            "venue": "IEEE Transactions on Image Processing",
            "authors": [
              {
                "authorId": "41210105",
                "name": "Zhou Wang"
              },
              {
                "authorId": "1747569",
                "name": "A. Bovik"
              },
              {
                "authorId": "2387140",
                "name": "H. Sheikh"
              },
              {
                "authorId": "1689350",
                "name": "Eero P. Simoncelli"
              }
            ]
          }
        },
        {
          "citedcorpusid": 222208633,
          "isinfluential": false,
          "contexts": [
            "Then, we input the X into the deformable encoder, which is applied as follows: where f deform-attn ( · ) denotes deformable attention [43], f LN ( · ) denotes layernorm layer and f FFN ( · ) denotes feed-forward network consisting of two MLP layers."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Deformable DETR: Deformable Transformers for End-to-End Object Detection",
            "abstract": "DETR has been recently proposed to eliminate the need for many hand-designed components in object detection while demonstrating good performance. However, it suffers from slow convergence and limited feature spatial resolution, due to the limitation of Transformer attention modules in processing image feature maps. To mitigate these issues, we proposed Deformable DETR, whose attention modules only attend to a small set of key sampling points around a reference. Deformable DETR can achieve better performance than DETR (especially on small objects) with 10$\\times$ less training epochs. Extensive experiments on the COCO benchmark demonstrate the effectiveness of our approach. Code shall be released.",
            "year": 2020,
            "venue": "International Conference on Learning Representations",
            "authors": [
              {
                "authorId": "2578924",
                "name": "Xizhou Zhu"
              },
              {
                "authorId": "145499378",
                "name": "Weijie Su"
              },
              {
                "authorId": "152309485",
                "name": "Lewei Lu"
              },
              {
                "authorId": "2183101614",
                "name": "Bin Li"
              },
              {
                "authorId": "93768810",
                "name": "Xiaogang Wang"
              },
              {
                "authorId": "3304536",
                "name": "Jifeng Dai"
              }
            ]
          }
        },
        {
          "citedcorpusid": 235306612,
          "isinfluential": true,
          "contexts": [
            "[19] reconstructs event features as images and combines them with frame image features using the conditional normalization method.",
            "[17], [19], [22] transforms events into an event queue, initially proposed by Tulyakov et al. [17] , for stereo disparity estimation.",
            "Hand-crafted event representations can preserve a greater amount of raw event data and exhibit more stable outcomes compared to reconstructed images [17], [19].",
            "Previous works [19], [21] introduce conditional normalization to address this issue by adjusting the distribution of feature maps to better align with the edges of scenes.",
            "A naive approach to represent event streams is to reconstruct the events into images [18], [19], and yet the quality of reconstructed images significantly impacts the accuracy of stereo disparity estimation."
          ],
          "intents": [
            "['methodology']",
            "['background']",
            "['background']",
            "['background']",
            "--"
          ],
          "cited_paper_info": {
            "title": "Deep Event Stereo Leveraged by Event-to-Image Translation",
            "abstract": "Depth estimation in real-world applications requires precise responses to fast motion and challenging lighting conditions. Event cameras use bio-inspired event-driven sensors that provide instantaneous and asynchronous information of pixel-level log intensity changes, which makes them suitable for depth estimation in such challenging conditions. However, as the event cameras primarily provide asynchronous and spatially sparse event data, it is hard to provide accurate dense disparity map in stereo event camera setups - especially in estimating disparities on local structures or edges. In this study, we develop a novel deep event stereo network that reconstructs spatial intensity image features from embedded event streams and leverages the event features using the reconstructed image features to compute dense disparity maps. To this end, we propose a novel event-to-image translation network with a cross-semantic attention mechanism that calculates the global semantic context of the event features for the intensity image reconstruction. In addition, a feature aggregation module is developed for accurate disparity estimation, which modulates the event features with the reconstructed image features by a stacked dilated spatially-adaptive denormalization mechanism. Experimental results reveal that our method can outperform the state-of-the-art methods by significant margins both in quantitative and qualitative measures.",
            "year": 2021,
            "venue": "AAAI Conference on Artificial Intelligence",
            "authors": [
              {
                "authorId": "2175478044",
                "name": "Soikat Hasan Ahmed"
              },
              {
                "authorId": "1696425398",
                "name": "Hae Woong Jang"
              },
              {
                "authorId": "40621769",
                "name": "S M Nadim Uddin"
              },
              {
                "authorId": "48895895",
                "name": "Yong Ju Jung"
              }
            ]
          }
        },
        {
          "citedcorpusid": 244306440,
          "isinfluential": true,
          "contexts": [
            "Previous works [18], [20] generate event representation through neural networks, but the generated event representation suffers from pixel shifts.",
            "[18] proposes the earliest deep learning-based method that fuses event and frame image data for stereo disparity estimation.",
            "3) Evaluation Metrics: we reference prior works [17], [18], [20], [21] and utilize evaluation metrics such as Root Mean Squared Error (RMSE), Mean Absolute Error (MAE), 1-Pixel Error (1PE) and 2-Pixel Error (2PE).",
            "Learning-based event representation methods [18], [20], [21] can overcome the aforementioned limitations, yielding more compact representations.",
            "We believe that the improved accuracy [17], [18], [20], [21] ON THE DSEC [1] of fine structures may be attributed to the integration of motion information in the EAA module, which allows the generated aggregated edge-modulated event frame to possess more accurate edges and contours.",
            "A naive approach to represent event streams is to reconstruct the events into images [18], [19], and yet the quality of reconstructed images significantly impacts the accuracy of stereo disparity estimation.",
            "1) Quantitative Analysis: We compare our EV-MGDispNet with state-of-the-art algorithms (DDES [17], E-Stereo [18], Concentration Net [20], DTC-PDS [21]) on 1PE, 2PE, MAE and RMSE metrics using the DSEC disparity benchmark dataset.",
            "The existing works [18], [20], [21] mainly rely on components from the field of image-based stereo disparity estimation to perform basic functionalities such as cost volume computation, volume aggregation and disparity estimation.",
            "Mostafavi et al. [18] proposes a method that combines event stream within a time window with images using image reconstruction to generate reconstructed images."
          ],
          "intents": [
            "['background']",
            "['background']",
            "['methodology']",
            "['background']",
            "['background']",
            "--",
            "['methodology']",
            "['methodology']",
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Event-Intensity Stereo: Estimating Depth by the Best of Both Worlds",
            "abstract": "Event cameras can report scene movements as an asynchronous stream of data called the events. Unlike traditional cameras, event cameras have very low latency (microseconds vs milliseconds) very high dynamic range (140 dB vs 60 dB), and low power consumption, as they report changes of a scene and not a complete frame. As they re- port per pixel feature-like events and not the whole intensity frame they are immune to motion blur. However, event cameras require movement between the scene and camera to fire events, i.e., they have no output when the scene is relatively static. Traditional cameras, however, report the whole frame of pixels at once in fixed intervals but have lower dynamic range and are prone to motion blur in case of rapid movements. We get the best from both worlds and use events and intensity images together in our complementary design and estimate dense disparity from this combination. The proposed end-to-end design combines events and images in a sequential manner and correlates them to estimate dense depth values. Our various experimental settings in real-world and simulated scenarios exploit the superiority of our method in predicting accurate depth values with fine details. We further extend our method to extreme cases of missing the left or right event or stereo pair and also investigate stereo depth estimation with inconsistent dynamic ranges or event thresholds on the left and right pairs.",
            "year": 2021,
            "venue": "IEEE International Conference on Computer Vision",
            "authors": [
              {
                "authorId": "114141661",
                "name": "Mohammad Mostafavi"
              },
              {
                "authorId": "51182421",
                "name": "Kuk-Jin Yoon"
              },
              {
                "authorId": "2119579051",
                "name": "Jonghyun Choi"
              }
            ]
          }
        },
        {
          "citedcorpusid": 246834559,
          "isinfluential": false,
          "contexts": [
            "Stereo depth estimation [9], [10] stands as a fundamental task in computer vision [11] and robotics [12]."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Optimization of 3-D Pose Measurement Method Based on Binocular Vision",
            "abstract": "To improve the accuracy and speed of existing binocular vision methods in practical environments, this article presents an optimization method for accurate and stable 3-D pose measurement based on binocular vision. First, in the stereo-image-matching stage, we introduce a guide-point definition and propose an optimal path-searching method for dynamic programming (DP). Second, a distance-based adaptive-filtering method is added to remove noise points and outliers around the target so that the environmental noise interference in practical environments is reduced before registration. Third, an adaptive-sampling method based on distance is proposed to extract the key points of the target point-cloud, and finally a target surface model based on a hash table is established as an offline matching data structure of point-cloud registration to improve the matching speed and accuracy of the 3-D pose calculation. A series of experiments showed the translation error of the proposed method was less than 1 cm relative to the measurement of 100 cm, and the rotation error was less than 2° relative to the measurement of 180°. Compared with traditional point-cloud registration algorithms, our method exhibited higher accuracy and stability, indicating great potential for use in unstructured environments.",
            "year": 2022,
            "venue": "IEEE Transactions on Instrumentation and Measurement",
            "authors": [
              {
                "authorId": "38028558",
                "name": "Yangjie Wei"
              },
              {
                "authorId": "2056825783",
                "name": "Yao Xi"
              }
            ]
          }
        },
        {
          "citedcorpusid": 247593948,
          "isinfluential": false,
          "contexts": [
            "They improve the disparity estimation performance using knowledge distillation [27]–[30] by incorporating future event information to reinforce the disparity estimation results based on only past event information."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "ESS: Learning Event-based Semantic Segmentation from Still Images",
            "abstract": "Retrieving accurate semantic information in challenging high dynamic range (HDR) and high-speed conditions remains an open challenge for image-based algorithms due to severe image degradations. Event cameras promise to address these challenges since they feature a much higher dynamic range and are resilient to motion blur. Nonetheless, semantic segmentation with event cameras is still in its infancy which is chiefly due to the lack of high-quality, labeled datasets. In this work, we introduce ESS (Event-based Semantic Segmentation), which tackles this problem by directly transferring the semantic segmentation task from existing labeled image datasets to unlabeled events via unsupervised domain adaptation (UDA). Compared to existing UDA methods, our approach aligns recurrent, motion-invariant event embeddings with image embeddings. For this reason, our method neither requires video data nor per-pixel alignment between images and events and, crucially, does not need to hallucinate motion from still images. Additionally, we introduce DSEC-Semantic, the first large-scale event-based dataset with fine-grained labels. We show that using image labels alone, ESS outperforms existing UDA approaches, and when combined with event labels, it even outperforms state-of-the-art supervised approaches on both DDD17 and DSEC-Semantic. Finally, ESS is general-purpose, which unlocks the vast amount of existing labeled image datasets and paves the way for new and exciting research directions in new fields previously inaccessible for event cameras.",
            "year": 2022,
            "venue": "European Conference on Computer Vision",
            "authors": [
              {
                "authorId": "2118548390",
                "name": "Zhaoning Sun"
              },
              {
                "authorId": "29989106",
                "name": "Nico Messikommer"
              },
              {
                "authorId": "51152279",
                "name": "Daniel Gehrig"
              },
              {
                "authorId": "2075371",
                "name": "D. Scaramuzza"
              }
            ]
          }
        },
        {
          "citedcorpusid": null,
          "isinfluential": false,
          "contexts": [
            "The event camera simulates the functionality of the transient visual pathway of the biological retina, where each pixel independently and asynchronously responds to relative brightness changes with a time resolution at the microsecond-level [33], [34]."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {}
        }
      ]
    },
    "269525186": {
      "citing_paper_info": {
        "title": "Secrets of Event-Based Optical Flow, Depth and Ego-Motion Estimation by Contrast Maximization",
        "abstract": "Event cameras respond to scene dynamics and provide signals naturally suitable for motion estimation with advantages, such as high dynamic range. The emerging field of event-based vision motivates a revisit of fundamental computer vision tasks related to motion, such as optical flow and depth estimation. However, state-of-the-art event-based optical flow methods tend to originate in frame-based deep-learning methods, which require several adaptations (data conversion, loss function, etc.) as they have very different properties. We develop a principled method to extend the Contrast Maximization framework to estimate dense optical flow, depth, and ego-motion from events alone. The proposed method sensibly models the space-time properties of event data and tackles the event alignment problem. It designs the objective function to prevent overfitting, deals better with occlusions, and improves convergence using a multi-scale approach. With these key elements, our method ranks first among unsupervised methods on the MVSEC benchmark and is competitive on the DSEC benchmark. Moreover, it allows us to simultaneously estimate dense depth and ego-motion, exposes the limitations of current flow benchmarks, and produces remarkable results when it is transferred to unsupervised learning settings. Along with various downstream applications shown, we hope the proposed method becomes a cornerstone on event-based motion-related tasks.",
        "year": 2024,
        "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
        "authors": [
          {
            "authorId": "2066324243",
            "name": "Shintaro Shiba"
          },
          {
            "authorId": "2299517127",
            "name": "Yannick Klose"
          },
          {
            "authorId": "2267612969",
            "name": "Yoshimitsu Aoki"
          },
          {
            "authorId": "144036711",
            "name": "Guillermo Gallego"
          }
        ]
      },
      "citation_summary": {
        "citation_count": 41,
        "unique_cited_count": 26,
        "influential_count": 9,
        "detailed_records_count": 41
      },
      "cited_papers": [
        "257505349",
        "11977588",
        "119096559",
        "3738244",
        "24007071",
        "268379520",
        "229211559",
        "226298400",
        "56475917",
        "9729856",
        "9865213",
        "3845250",
        "355163",
        "52283776",
        "1082643",
        "267945091",
        "3396150",
        "236574",
        "254591426",
        "257637142",
        "16638035",
        "16219282",
        "239049376",
        "13360027",
        "250408092",
        "19091270"
      ],
      "citation_details": [
        {
          "citedcorpusid": 236574,
          "isinfluential": false,
          "contexts": [
            "Restrictions apply. the method of characteristics [64].",
            "The PDEs have advection terms and others that resemble those of the inviscid Burgers’ equation [64] since the ﬂow is transporting itself."
          ],
          "intents": [
            "--",
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Partial Differential Equations",
            "abstract": "",
            "year": 2002,
            "venue": "",
            "authors": [
              {
                "authorId": "87932203",
                "name": "J. B. Seaborn"
              }
            ]
          }
        },
        {
          "citedcorpusid": 355163,
          "isinfluential": false,
          "contexts": [
            "One could use other clustering algorithms, such as DBSCAN [83], to treat such interpolation effects as outliers."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "A Density-Based Algorithm for Discovering Clusters in Large Spatial Databases with Noise",
            "abstract": "Clustering algorithms are attractive for the task of class identification in spatial databases. However, the application to large spatial databases rises the following requirements for clustering algorithms: minimal requirements of domain knowledge to determine the input parameters, discovery of clusters with arbitrary shape and good efficiency on large databases. The well-known clustering algorithms offer no solution to the combination of these requirements. In this paper, we present the new clustering algorithm DBSCAN relying on a density-based notion of clusters which is designed to discover clusters of arbitrary shape. DBSCAN requires only one input parameter and supports the user in determining an appropriate value for it. We performed an experimental evaluation of the effectiveness and efficiency of DBSCAN using synthetic data and real data of the SEQUOIA 2000 benchmark. The results of our experiments demonstrate that (1) DBSCAN is significantly more effective in discovering clusters of arbitrary shape than the well-known algorithm CLARANS, and that (2) DBSCAN outperforms CLARANS by a factor of more than 100 in terms of efficiency.",
            "year": 1996,
            "venue": "Knowledge Discovery and Data Mining",
            "authors": [
              {
                "authorId": "1766588",
                "name": "M. Ester"
              },
              {
                "authorId": "1688561",
                "name": "H. Kriegel"
              },
              {
                "authorId": "144815274",
                "name": "J. Sander"
              },
              {
                "authorId": "2145783722",
                "name": "Xiaowei Xu"
              }
            ]
          }
        },
        {
          "citedcorpusid": 1082643,
          "isinfluential": false,
          "contexts": [
            "…evaluations, we provide exhaustive comparisons across the existing methods to date: a model-based method where the pose information is given (EMVS) [23], a supervised-learning method [61] trained on real data (outdoor_day2, denoted “SL (R)”) or in simulation (“SL (S)”), and two…",
            "…[8] is a powerful framework that allows us to tackle multiple motion estimation problems (rotational motion [9], [10], [11], [12], homographic motion [7], [13], [14], feature ﬂow estimation [15], [16], [17], [18], motion segmentation [19], [20], [21], [22], and also reconstruction [7], [23], [24]).",
            "The problem is difﬁcult, and often one settles for estimating depth alone, with or without knowledge of the camera motion [23], [61], [62]."
          ],
          "intents": [
            "['methodology']",
            "['background']",
            "['background']"
          ],
          "cited_paper_info": {
            "title": "EMVS: Event-Based Multi-View Stereo—3D Reconstruction with an Event Camera in Real-Time",
            "abstract": "",
            "year": 2018,
            "venue": "International Journal of Computer Vision",
            "authors": [
              {
                "authorId": "3414274",
                "name": "Henri Rebecq"
              },
              {
                "authorId": "144036711",
                "name": "Guillermo Gallego"
              },
              {
                "authorId": "144578041",
                "name": "Elias Mueggler"
              },
              {
                "authorId": "2075371",
                "name": "D. Scaramuzza"
              }
            ]
          }
        },
        {
          "citedcorpusid": 3396150,
          "isinfluential": true,
          "contexts": [
            "In terms of architectures, the three most common ones are U-Net [34], [49], FireNet [28], and RAFT [44], [50].",
            "The dataset was extended in [34] to provide ground truth (GT) optical ﬂow, computed as the motion ﬁeld [6] given the camera velocity and the depth of the scene.",
            "Some learning-based works [27], [28], [34] also have a multi-scale component, inherited from the use of a U-Net architecture.",
            "Zhu et al. [27] extended EV-FlowNet [34] to the unsupervised setting using a motion-compensation loss inspired by the average timestamp images in [19].",
            "[34] change and its polarity p k ∈ { +1 , − 1 } .",
            "EV-FlowNet [34] pioneered these approaches.",
            "Semi-supervised methods use the grayscale images from a colocated camera (e.g., DAVIS [56]) as a supervisory signal: images are warped using the ﬂow predicted by the ANN and their photometric consistency is used as loss function [34], [45], [46].",
            "Datasets: First, we evaluate our method on sequences from the MVSEC dataset [4], [34], which is the de facto standard dataset used by prior works to benchmark optical ﬂow.",
            "Current state-of-the-art approaches are ANNs [27], [30], [34], [44], [45], [46], largely inspired by frame-based optical ﬂow architectures [47], [48].",
            "The results of our experimental evaluation are surprising: the above design choices are key to our simple, model-based tile-based method achieving the best accuracy among all state-of-the-art methods, including supervised-learning ones, on the de facto benchmark of MVSEC indoor sequences [34]."
          ],
          "intents": [
            "['background']",
            "['methodology']",
            "['background']",
            "['methodology']",
            "--",
            "--",
            "['methodology']",
            "['methodology']",
            "['background']",
            "['background']"
          ],
          "cited_paper_info": {
            "title": "EV-FlowNet: Self-Supervised Optical Flow Estimation for Event-based Cameras",
            "abstract": "Event-based cameras have shown great promise in a variety of situations where frame based cameras suffer, such as high speed motions and high dynamic range scenes. However, developing algorithms for event measurements requires a new class of hand crafted algorithms. Deep learning has shown great success in providing model free solutions to many problems in the vision community, but existing networks have been developed with frame based images in mind, and there does not exist the wealth of labeled data for events as there does for images for supervised training. To these points, we present EV-FlowNet, a novel self-supervised deep learning pipeline for optical flow estimation for event based cameras. In particular, we introduce an image based representation of a given event stream, which is fed into a self-supervised neural network as the sole input. The corresponding grayscale images captured from the same camera at the same time as the events are then used as a supervisory signal to provide a loss function at training time, given the estimated flow from the network. We show that the resulting network is able to accurately predict optical flow from events only in a variety of different scenes, with performance competitive to image based networks. This method not only allows for accurate estimation of dense optical flow, but also provides a framework for the transfer of other self-supervised methods to the event-based domain.",
            "year": 2018,
            "venue": "Robotics: Science and Systems",
            "authors": [
              {
                "authorId": "3385588",
                "name": "A. Z. Zhu"
              },
              {
                "authorId": "36001694",
                "name": "Liangzhe Yuan"
              },
              {
                "authorId": "20728097",
                "name": "Kenneth Chaney"
              },
              {
                "authorId": "1751586",
                "name": "Kostas Daniilidis"
              }
            ]
          }
        },
        {
          "citedcorpusid": 3738244,
          "isinfluential": false,
          "contexts": [
            "The ECD dataset [63] is a lower resolution, standard dataset to assess camera ego-motion [9], [16], [25], [69], [70], [71], [72]."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Ultimate SLAM? Combining Events, Images, and IMU for Robust Visual SLAM in HDR and High-Speed Scenarios",
            "abstract": "Event cameras are bioinspired vision sensors that output pixel-level brightness changes instead of standard intensity frames. These cameras do not suffer from motion blur and have a very high dynamic range, which enables them to provide reliable visual information during high-speed motions or in scenes characterized by high dynamic range. However, event cameras output only little information when the amount of motion is limited, such as in the case of almost still motion. Conversely, standard cameras provide instant and rich information about the environment most of the time (in low-speed and good lighting scenarios), but they fail severely in case of fast motions, or difficult lighting such as high dynamic range or low light scenes. In this letter, we present the first state estimation pipeline that leverages the complementary advantages of these two sensors by fusing in a tightly coupled manner events, standard frames, and inertial measurements. We show on the publicly available Event Camera Dataset that our hybrid pipeline leads to an accuracy improvement of 130% over event-only pipelines, and 85% over standard-frames-only visual-inertial systems, while still being computationally tractable. Furthermore, we use our pipeline to demonstrate—to the best of our knowledge—the first autonomous quadrotor flight using an event camera for state estimation, unlocking flight scenarios that were not reachable with traditional visual-inertial odometry, such as low-light environments and high dynamic range scenes. Videos of the experiments: http://rpg.ifi.uzh.ch/ultimateslam.html",
            "year": 2017,
            "venue": "IEEE Robotics and Automation Letters",
            "authors": [
              {
                "authorId": "26430800",
                "name": "Antoni Rosinol Vidal"
              },
              {
                "authorId": "3414274",
                "name": "Henri Rebecq"
              },
              {
                "authorId": "9676873",
                "name": "Timo Horstschaefer"
              },
              {
                "authorId": "2075371",
                "name": "D. Scaramuzza"
              }
            ]
          }
        },
        {
          "citedcorpusid": 3845250,
          "isinfluential": false,
          "contexts": [
            "Zhu et al. [27] extended EV-FlowNet [34] to the unsupervised setting using a motion-compensation loss inspired by the average timestamp images in [19].",
            "…[8] is a powerful framework that allows us to tackle multiple motion estimation problems (rotational motion [9], [10], [11], [12], homographic motion [7], [13], [14], feature ﬂow estimation [15], [16], [17], [18], motion segmentation [19], [20], [21], [22], and also reconstruction [7], [23], [24])."
          ],
          "intents": [
            "['methodology']",
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Event-Based Moving Object Detection and Tracking",
            "abstract": "Event-based vision sensors, such as the Dynamic Vision Sensor (DVS), are ideally suited for real-time motion analysis. The unique properties encompassed in the readings of such sensors provide high temporal resolution, superior sensitivity to light and low latency. These properties provide the grounds to estimate motion efficiently and reliably in the most sophisticated scenarios, but these advantages come at a price - modern event-based vision sensors have extremely low resolution, produce a lot of noise and require the development of novel algorithms to handle the asynchronous event stream. This paper presents a new, efficient approach to object tracking with asynchronous cameras. We present a novel event stream representation which enables us to utilize information about the dynamic (temporal)component of the event stream. The 3D geometry of the event stream is approximated with a parametric model to motion-compensate for the camera (without feature tracking or explicit optical flow computation), and then moving objects that don't conform to the model are detected in an iterative process. We demonstrate our framework on the task of independent motion detection and tracking, where we use the temporal model inconsistencies to locate differently moving objects in challenging situations of very fast motion.",
            "year": 2018,
            "venue": "IEEE/RJS International Conference on Intelligent RObots and Systems",
            "authors": [
              {
                "authorId": "144559298",
                "name": "A. Mitrokhin"
              },
              {
                "authorId": "1759899",
                "name": "C. Fermüller"
              },
              {
                "authorId": "38904651",
                "name": "Chethan Parameshwara"
              },
              {
                "authorId": "1697493",
                "name": "Y. Aloimonos"
              }
            ]
          }
        },
        {
          "citedcorpusid": 9729856,
          "isinfluential": false,
          "contexts": [
            "The ECD dataset [63] is a lower resolution, standard dataset to assess camera ego-motion [9], [16], [25], [69], [70], [71], [72]."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Continuous-Time Visual-Inertial Odometry for Event Cameras",
            "abstract": "Event cameras are bioinspired vision sensors that output pixel-level brightness changes instead of standard intensity frames. They offer significant advantages over standard cameras, namely a very high dynamic range, no motion blur, and a latency in the order of microseconds. However, due to the fundamentally different structure of the sensor's output, new algorithms that exploit the high temporal resolution and the asynchronous nature of the sensor are required. Recent work has shown that a continuous-time representation of the event camera pose can deal with the high temporal resolution and asynchronous nature of this sensor in a principled way. In this paper, we leverage such a continuous-time representation to perform visual-inertial odometry with an event camera. This representation allows direct integration of the asynchronous events with microsecond accuracy and the inertial measurements at high frequency. The event camera trajectory is approximated by a smooth curve in the space of rigid-body motions using cubic splines. This formulation significantly reduces the number of variables in trajectory estimation problems. We evaluate our method on real data from several scenes and compare the results against ground truth from a motion-capture system. We show that our method provides improved accuracy over the result of a state-of-the-art visual odometry method for event cameras. We also show that both the map orientation and scale can be recovered accurately by fusing events and inertial data. To the best of our knowledge, this is the first work on visual-inertial fusion with event cameras using a continuous-time framework.",
            "year": 2017,
            "venue": "IEEE Transactions on robotics",
            "authors": [
              {
                "authorId": "144578041",
                "name": "Elias Mueggler"
              },
              {
                "authorId": "144036711",
                "name": "Guillermo Gallego"
              },
              {
                "authorId": "3414274",
                "name": "Henri Rebecq"
              },
              {
                "authorId": "2075371",
                "name": "D. Scaramuzza"
              }
            ]
          }
        },
        {
          "citedcorpusid": 9865213,
          "isinfluential": true,
          "contexts": [
            "Depth and Ego-motion estimation for the slider_depth sequence (real data) from the ECD dataset [63].",
            "The advantages of the time-aware warp (8) over (4) to produce better IWEs (higher FWL) are most noticeable on sequences like slider_depth [63] and DSEC (see Fig.",
            "4 illustrates this with an occlusion example taken from the slider_depth sequence [63].",
            "To assess the effect of the proposed time-aware warp (8), we conducted experiments on MVSEC, DSEC and ECD [63] datasets.",
            "The ECD dataset [63] is a lower resolution, standard dataset to assess camera ego-motion [9], [16], [25], [69], [70], [71], [72].",
            "2) Results on ECD: Depth and ego-motion estimation results on the slider_depth sequence from the ECD dataset [63] are shown on Fig.",
            "At occlusions (dartboard in slider_depth [63] and garage door in DSEC [5]), upwind and Burgers’ produce sharper IWEs.",
            "17 shows the results on a synthetic sequence from [63].",
            "Depth and Ego-motion estimation for the simulation_3planes sequence from the ECD dataset [63]."
          ],
          "intents": [
            "['methodology']",
            "['background']",
            "['methodology']",
            "['methodology']",
            "['methodology']",
            "['methodology']",
            "['background']",
            "['background']",
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "The event-camera dataset and simulator: Event-based data for pose estimation, visual odometry, and SLAM",
            "abstract": "New vision sensors, such as the dynamic and active-pixel vision sensor (DAVIS), incorporate a conventional global-shutter camera and an event-based sensor in the same pixel array. These sensors have great potential for high-speed robotics and computer vision because they allow us to combine the benefits of conventional cameras with those of event-based sensors: low latency, high temporal resolution, and very high dynamic range. However, new algorithms are required to exploit the sensor characteristics and cope with its unconventional output, which consists of a stream of asynchronous brightness changes (called “events”) and synchronous grayscale frames. For this purpose, we present and release a collection of datasets captured with a DAVIS in a variety of synthetic and real environments, which we hope will motivate research on new algorithms for high-speed and high-dynamic-range robotics and computer-vision applications. In addition to global-shutter intensity images and asynchronous events, we provide inertial measurements and ground-truth camera poses from a motion-capture system. The latter allows comparing the pose accuracy of ego-motion estimation algorithms quantitatively. All the data are released both as standard text files and binary files (i.e. rosbag). This paper provides an overview of the available data and describes a simulator that we release open-source to create synthetic event-camera data.",
            "year": 2016,
            "venue": "Int. J. Robotics Res.",
            "authors": [
              {
                "authorId": "144578041",
                "name": "Elias Mueggler"
              },
              {
                "authorId": "3414274",
                "name": "Henri Rebecq"
              },
              {
                "authorId": "144036711",
                "name": "Guillermo Gallego"
              },
              {
                "authorId": "1694635",
                "name": "T. Delbrück"
              },
              {
                "authorId": "2075371",
                "name": "D. Scaramuzza"
              }
            ]
          }
        },
        {
          "citedcorpusid": 11977588,
          "isinfluential": false,
          "contexts": [
            "For depth accuracy evaluation, we use standard metrics following previous work on monocular depth estimation [57], [74]."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Unsupervised Learning of Depth and Ego-Motion from Video",
            "abstract": "We present an unsupervised learning framework for the task of monocular depth and camera motion estimation from unstructured video sequences. In common with recent work [10, 14, 16], we use an end-to-end learning approach with view synthesis as the supervisory signal. In contrast to the previous work, our method is completely unsupervised, requiring only monocular video sequences for training. Our method uses single-view depth and multiview pose networks, with a loss based on warping nearby views to the target using the computed depth and pose. The networks are thus coupled by the loss during training, but can be applied independently at test time. Empirical evaluation on the KITTI dataset demonstrates the effectiveness of our approach: 1) monocular depth performs comparably with supervised methods that use either ground-truth pose or depth for training, and 2) pose estimation performs favorably compared to established SLAM systems under comparable input settings.",
            "year": 2017,
            "venue": "Computer Vision and Pattern Recognition",
            "authors": [
              {
                "authorId": "1822702",
                "name": "Tinghui Zhou"
              },
              {
                "authorId": "144735785",
                "name": "Matthew A. Brown"
              },
              {
                "authorId": "1830653",
                "name": "Noah Snavely"
              },
              {
                "authorId": "35238678",
                "name": "D. Lowe"
              }
            ]
          }
        },
        {
          "citedcorpusid": 13360027,
          "isinfluential": false,
          "contexts": [
            "Prior work has proposed adaptations of frame-based approaches (block matching [36], Lucas-Kanade [37]), ﬁlter-banks [38], [39], spatio-temporal plane-ﬁtting [40], [41], time surface matching [42], variational optimization on voxelized events [43], and feature-based contrast maximization [7], [15].",
            "…[8] is a powerful framework that allows us to tackle multiple motion estimation problems (rotational motion [9], [10], [11], [12], homographic motion [7], [13], [14], feature ﬂow estimation [15], [16], [17], [18], motion segmentation [19], [20], [21], [22], and also reconstruction [7], [23], [24])."
          ],
          "intents": [
            "['methodology']",
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Event-based feature tracking with probabilistic data association",
            "abstract": "",
            "year": 2017,
            "venue": "IEEE International Conference on Robotics and Automation",
            "authors": [
              {
                "authorId": "3385588",
                "name": "A. Z. Zhu"
              },
              {
                "authorId": "50365495",
                "name": "Nikolay A. Atanasov"
              },
              {
                "authorId": "1751586",
                "name": "Kostas Daniilidis"
              }
            ]
          }
        },
        {
          "citedcorpusid": 16219282,
          "isinfluential": false,
          "contexts": [
            "To mitigate isolated patches with very large depth values we apply median ﬁlters [35] and a Charbonnier loss [67] for regularization.",
            "Because of the above, we believe that the proposed design choices deserve to be called “secrets” [35]."
          ],
          "intents": [
            "['methodology']",
            "['background']"
          ],
          "cited_paper_info": {
            "title": "A Quantitative Analysis of Current Practices in Optical Flow Estimation and the Principles Behind Them",
            "abstract": "The accuracy of optical flow estimation algorithms has been improving steadily as evidenced by results on the Middlebury optical flow benchmark. The typical formulation, however, has changed little since the work of Horn and Schunck. We attempt to uncover what has made recent advances possible through a thorough analysis of how the objective function, the optimization method, and modern implementation practices influence accuracy. We discover that “classical” flow formulations perform surprisingly well when combined with modern optimization and implementation techniques. One key implementation detail is the median filtering of intermediate flow fields during optimization. While this improves the robustness of classical methods it actually leads to higher energy solutions, meaning that these methods are not optimizing the original objective function. To understand the principles behind this phenomenon, we derive a new objective function that formalizes the median filtering heuristic. This objective function includes a non-local smoothness term that robustly integrates flow estimates over large spatial neighborhoods. By modifying this new term to include information about flow and image boundaries we develop a method that can better preserve motion details. To take advantage of the trend towards video in wide-screen format, we further introduce an asymmetric pyramid downsampling scheme that enables the estimation of longer range horizontal motions. The methods are evaluated on the Middlebury, MPI Sintel, and KITTI datasets using the same parameter settings.",
            "year": 2013,
            "venue": "International Journal of Computer Vision",
            "authors": [
              {
                "authorId": "3232265",
                "name": "Deqing Sun"
              },
              {
                "authorId": "145920814",
                "name": "S. Roth"
              },
              {
                "authorId": "2105795",
                "name": "Michael J. Black"
              }
            ]
          }
        },
        {
          "citedcorpusid": 16638035,
          "isinfluential": false,
          "contexts": [
            "Prior work has proposed adaptations of frame-based approaches (block matching [36], Lucas-Kanade [37]), ﬁlter-banks [38], [39], spatio-temporal plane-ﬁtting [40], [41], time surface matching [42], variational optimization on voxelized events [43], and feature-based contrast maximization [7], [15]."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "On event-based optical flow detection",
            "abstract": "Event-based sensing, i.e., the asynchronous detection of luminance changes, promises low-energy, high dynamic range, and sparse sensing. This stands in contrast to whole image frame-wise acquisition by standard cameras. Here, we systematically investigate the implications of event-based sensing in the context of visual motion, or flow, estimation. Starting from a common theoretical foundation, we discuss different principal approaches for optical flow detection ranging from gradient-based methods over plane-fitting to filter based methods and identify strengths and weaknesses of each class. Gradient-based methods for local motion integration are shown to suffer from the sparse encoding in address-event representations (AER). Approaches exploiting the local plane like structure of the event cloud, on the other hand, are shown to be well suited. Within this class, filter based approaches are shown to define a proper detection scheme which can also deal with the problem of representing multiple motions at a single location (motion transparency). A novel biologically inspired efficient motion detector is proposed, analyzed and experimentally validated. Furthermore, a stage of surround normalization is incorporated. Together with the filtering this defines a canonical circuit for motion feature detection. The theoretical analysis shows that such an integrated circuit reduces motion ambiguity in addition to decorrelating the representation of motion related activations.",
            "year": 2015,
            "venue": "Frontiers in Neuroscience",
            "authors": [
              {
                "authorId": "2256291",
                "name": "T. Brosch"
              },
              {
                "authorId": "2381710",
                "name": "Stephan Tschechne"
              },
              {
                "authorId": "144718494",
                "name": "H. Neumann"
              }
            ]
          }
        },
        {
          "citedcorpusid": 19091270,
          "isinfluential": false,
          "contexts": [
            "Semi-supervised methods use the grayscale images from a colocated camera (e.g., DAVIS [56]) as a supervisory signal: images are warped using the ﬂow predicted by the ANN and their photometric consistency is used as loss function [34], [45], [46].",
            "The event camera has 346 × 260 pixel resolution [56]."
          ],
          "intents": [
            "['methodology']",
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Front and Back Illuminated Dynamic and Active Pixel Vision Sensors Comparison",
            "abstract": "",
            "year": 2018,
            "venue": "IEEE Transactions on Circuits and Systems - II - Express Briefs",
            "authors": [
              {
                "authorId": "8742892",
                "name": "Gemma Taverni"
              },
              {
                "authorId": "133850011",
                "name": "Diederik Paul Moeys"
              },
              {
                "authorId": "49672818",
                "name": "Chenghan Li"
              },
              {
                "authorId": "40859499",
                "name": "C. Cavaco"
              },
              {
                "authorId": "30912796",
                "name": "V. Motsnyi"
              },
              {
                "authorId": "113215237",
                "name": "D. San Segundo Bello"
              },
              {
                "authorId": "5548576",
                "name": "T. Delbruck"
              }
            ]
          }
        },
        {
          "citedcorpusid": 24007071,
          "isinfluential": false,
          "contexts": [
            "Each sequence provides events, frames, calibration information, and IMU data from a DAVIS240 C camera ( 240 × 180 pixels [73]), as well as ground truth camera poses from a motion capture system (at 200 Hz)."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "A 240 × 180 130 dB 3 µs Latency Global Shutter Spatiotemporal Vision Sensor",
            "abstract": "",
            "year": 2014,
            "venue": "IEEE Journal of Solid-State Circuits",
            "authors": [
              {
                "authorId": "2239977",
                "name": "Christian Brandli"
              },
              {
                "authorId": "144246116",
                "name": "R. Berner"
              },
              {
                "authorId": "1779496",
                "name": "Minhao Yang"
              },
              {
                "authorId": "1704961",
                "name": "Shih-Chii Liu"
              },
              {
                "authorId": "5548576",
                "name": "T. Delbruck"
              }
            ]
          }
        },
        {
          "citedcorpusid": 52283776,
          "isinfluential": false,
          "contexts": [
            "Prior work has proposed adaptations of frame-based approaches (block matching [36], Lucas-Kanade [37]), ﬁlter-banks [38], [39], spatio-temporal plane-ﬁtting [40], [41], time surface matching [42], variational optimization on voxelized events [43], and feature-based contrast maximization [7], [15]."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Adaptive Time-Slice Block-Matching Optical Flow Algorithm for Dynamic Vision Sensors",
            "abstract": "Dynamic Vision Sensors (DVS) output asynchronous log intensity change events. They have potential applications in high-speed robotics, autonomous cars and drones. The precise event timing, sparse output, and wide dynamic range of the events are well suited for optical ﬂow, but conventional optical ﬂow (OF) algorithms are not well matched to the event stream data. This paper proposes an event-driven OF algorithm called adaptive block-matching optical ﬂow (ABMOF). ABMOF uses time slices of accumulated DVS events. The time slices are adaptively rotated based on the input events and OF results. Compared with other methods such as gradient-based OF, ABMOF can efﬁciently be implemented in compact logic circuits. We developed both ABMOF and Lucas-Kanade (LK) algorithms using our adapted slices. Results shows that ABMOF accuracy is comparable with LK accuracy on natural scene data including sparse and dense texture, high dynamic range, and fast motion exceeding 30,000 pixels per second.",
            "year": 2018,
            "venue": "British Machine Vision Conference",
            "authors": [
              {
                "authorId": "2108480076",
                "name": "Min Liu"
              },
              {
                "authorId": "1694635",
                "name": "T. Delbrück"
              }
            ]
          }
        },
        {
          "citedcorpusid": 56475917,
          "isinfluential": true,
          "contexts": [
            "…the scene complexity, the large pixel displacement and the high dynamic range. been tackled by changing the objective function, from contrast to the energy of an average timestamp image [27], [28], but this loss is not straightforward to interpret [8], [29], and is not without its problems [30].",
            "Zhu et al. [27] report that the contrast objective (variance) overﬁts to the events.",
            "Remark: Warping to two reference times (min and max) was proposed in [27], but with important differences: (i) it was done for the average timestamp loss, hence it did not consider the effect on contrast or focus functions [8], and (ii) it had a completely different motivation: to lessen a…",
            "Zhu et al. [27] extended EV-FlowNet [34] to the unsupervised setting using a motion-compensation loss inspired by the average timestamp images in [19].",
            "Closest to our work are [27], [57] because they estimate a depth-parameterized motion ﬁeld that best ﬁts the event data.",
            "Preliminary work on applying CM to estimate optical ﬂow has reported event collapse [25], [26], producing ﬂows at undesired optima that warp events to few pixels or lines [27].",
            "Their loss function consists of an event alignment error using the ﬂow predicted by the ANN [27], [28], [30], [57], [58], [59].",
            "The loss functions are based on the energy of an average timestamp image [27] or on the photometric consistency of edge-maps warped by the predicted ﬂow [57].",
            "For dense optical ﬂow motion, the warp used is [27], [28] x where θ = { v ( x ) } x ∈ Ω is a ﬂow ﬁeld on the image plane Ω at a set time, e.g., t ref .",
            "Current state-of-the-art approaches are ANNs [27], [30], [34], [44], [45], [46], largely inspired by frame-based optical ﬂow architectures [47], [48]."
          ],
          "intents": [
            "['background']",
            "['background']",
            "['methodology']",
            "['methodology']",
            "['background']",
            "['background']",
            "['methodology']",
            "['background']",
            "['methodology']",
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Unsupervised Event-Based Learning of Optical Flow, Depth, and Egomotion",
            "abstract": "In this work, we propose a novel framework for unsupervised learning for event cameras that learns motion information from only the event stream. In particular, we propose an input representation of the events in the form of a discretized volume that maintains the temporal distribution of the events, which we pass through a neural network to predict the motion of the events. This motion is used to attempt to remove any motion blur in the event image. We then propose a loss function applied to the motion compensated event image that measures the motion blur in this image. We train two networks with this framework, one to predict optical flow, and one to predict egomotion and depths, and evaluate these networks on the Multi Vehicle Stereo Event Camera dataset, along with qualitative results from a variety of different scenes.",
            "year": 2018,
            "venue": "Computer Vision and Pattern Recognition",
            "authors": [
              {
                "authorId": "3385588",
                "name": "A. Z. Zhu"
              },
              {
                "authorId": "36001694",
                "name": "Liangzhe Yuan"
              },
              {
                "authorId": "20728097",
                "name": "Kenneth Chaney"
              },
              {
                "authorId": "1751586",
                "name": "Kostas Daniilidis"
              }
            ]
          }
        },
        {
          "citedcorpusid": 119096559,
          "isinfluential": false,
          "contexts": [
            "Noticethat[27]wastrainedontheout-door_day2 sequence, which is a similar driving sequence to the testone,whiletheothermethodsweretrainedondronedata [81]."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Are We Ready for Autonomous Drone Racing? The UZH-FPV Drone Racing Dataset",
            "abstract": "Despite impressive results in visual-inertial state estimation in recent years, high speed trajectories with six degree of freedom motion remain challenging for existing estimation algorithms. Aggressive trajectories feature large accelerations and rapid rotational motions, and when they pass close to objects in the environment, this induces large apparent motions in the vision sensors, all of which increase the difficulty in estimation. Existing benchmark datasets do not address these types of trajectories, instead focusing on slow speed or constrained trajectories, targeting other tasks such as inspection or driving. We introduce the UZH-FPV Drone Racing dataset, consisting of over 27 sequences, with more than 10 km of flight distance, captured on a first-person-view (FPV) racing quadrotor flown by an expert pilot. The dataset features camera images, inertial measurements, event-camera data, and precise ground truth poses. These sequences are faster and more challenging, in terms of apparent scene motion, than any existing dataset. Our goal is to enable advancement of the state of the art in aggressive motion estimation by providing a dataset that is beyond the capabilities of existing state estimation algorithms.",
            "year": 2019,
            "venue": "IEEE International Conference on Robotics and Automation",
            "authors": [
              {
                "authorId": "1770833",
                "name": "J. Delmerico"
              },
              {
                "authorId": "2017998",
                "name": "Titus Cieslewski"
              },
              {
                "authorId": "3414274",
                "name": "Henri Rebecq"
              },
              {
                "authorId": "36984610",
                "name": "Matthias Faessler"
              },
              {
                "authorId": "2075371",
                "name": "D. Scaramuzza"
              }
            ]
          }
        },
        {
          "citedcorpusid": 226298400,
          "isinfluential": true,
          "contexts": [
            "As discussed in (Section IV-D1 and Table IV), ECN [57] might have overﬁt to this outdoor sequence that reports a very small error (0.7m/s).",
            "…exhaustive comparisons across the existing methods to date: a model-based method where the pose information is given (EMVS) [23], a supervised-learning method [61] trained on real data (outdoor_day2, denoted “SL (R)”) or in simulation (“SL (S)”), and two unsupervised-learning methods [27], [57].",
            "Closest to our work are [27], [57] because they estimate a depth-parameterized motion ﬁeld that best ﬁts the event data.",
            "The proposed methods achieve overall better accuracy on the indoor sequences and competitive results on the outdoor sequence compared with ECN [57], the closest work to ours.",
            "For depth accuracy evaluation, we use standard metrics following previous work on monocular depth estimation [57], [74].",
            "Their loss function consists of an event alignment error using the ﬂow predicted by the ANN [27], [28], [30], [57], [58], [59].",
            "The loss functions are based on the energy of an average timestamp image [27] or on the photometric consistency of edge-maps warped by the predicted ﬂow [57].",
            "Following the convention [57], we report the metrics for indoor as the average of the three indoor sequences."
          ],
          "intents": [
            "['background']",
            "['methodology']",
            "['background']",
            "['methodology']",
            "['methodology']",
            "['methodology']",
            "['background']",
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Unsupervised Learning of Dense Optical Flow, Depth and Egomotion with Event-Based Sensors",
            "abstract": "We present an unsupervised learning pipeline for dense depth, optical flow and egomotion estimation for autonomous driving applications, using the event-based output of the Dynamic Vision Sensor (DVS) as input. The backbone of our pipeline is a bioinspired encoder-decoder neural network architecture - ECN. To train the pipeline, we introduce a covariance normalization technique which resembles the lateral inhibition mechanism found in animal neural systems.Our work is the first monocular pipeline that generates dense depth and optical flow from sparse event data only, and is able to transfer from day to night scenes without any additional training. The network works in self-supervised mode and has just 150k parameters. We evaluate our pipeline on the MVSEC self driving dataset and present results for depth, optical flow and and egomotion estimation. Thanks to the efficient design, we are able to achieve inference rates of 300 FPS on a single Nvidia 1080Ti GPU. Our experiments demonstrate significant improvements upon works that used deep learning on event data, as well as the ability to perform well during both day and night.",
            "year": 2020,
            "venue": "IEEE/RJS International Conference on Intelligent RObots and Systems",
            "authors": [
              {
                "authorId": "3300969",
                "name": "Chengxi Ye"
              },
              {
                "authorId": "144559298",
                "name": "A. Mitrokhin"
              },
              {
                "authorId": "1759899",
                "name": "C. Fermüller"
              },
              {
                "authorId": "9861772",
                "name": "J. Yorke"
              },
              {
                "authorId": "1697493",
                "name": "Y. Aloimonos"
              }
            ]
          }
        },
        {
          "citedcorpusid": 229211559,
          "isinfluential": true,
          "contexts": [
            "Firstrow: corridor sequencefrom[21].",
            "…[8] is a powerful framework that allows us to tackle multiple motion estimation problems (rotational motion [9], [10], [11], [12], homographic motion [7], [13], [14], feature ﬂow estimation [15], [16], [17], [18], motion segmentation [19], [20], [21], [22], and also reconstruction [7], [23], [24]).",
            "The sequences in EMSGC [21] are recorded with a hand-held DAVIS346 camera ( 346 × 260 pixels).",
            "Finally, we also test sequences from two motion segmentation datasets [20], [21].",
            "To this end, we show results on three sequences from [20], [21] in Fig."
          ],
          "intents": [
            "--",
            "['background']",
            "['methodology']",
            "['methodology']",
            "--"
          ],
          "cited_paper_info": {
            "title": "Event-Based Motion Segmentation With Spatio-Temporal Graph Cuts",
            "abstract": "Identifying independently moving objects is an essential task for dynamic scene understanding. However, traditional cameras used in dynamic scenes may suffer from motion blur or exposure artifacts due to their sampling principle. By contrast, event-based cameras are novel bio-inspired sensors that offer advantages to overcome such limitations. They report pixel-wise intensity changes asynchronously, which enables them to acquire visual information at exactly the same rate as the scene dynamics. We develop a method to identify independently moving objects acquired with an event-based camera, that is, to solve the event-based motion segmentation problem. We cast the problem as an energy minimization one involving the fitting of multiple motion models. We jointly solve two sub-problems, namely event-cluster assignment (labeling) and motion model fitting, in an iterative manner by exploiting the structure of the input event data in the form of a spatio-temporal graph. Experiments on available datasets demonstrate the versatility of the method in scenes with different motion patterns and number of moving objects. The evaluation shows state-of-the-art results without having to predetermine the number of expected moving objects. We release the software and dataset under an open source license to foster research in the emerging topic of event-based motion segmentation.",
            "year": 2020,
            "venue": "IEEE Transactions on Neural Networks and Learning Systems",
            "authors": [
              {
                "authorId": null,
                "name": "Yi Zhou"
              },
              {
                "authorId": "144036711",
                "name": "Guillermo Gallego"
              },
              {
                "authorId": "2110036504",
                "name": "Xiuyuan Lu"
              },
              {
                "authorId": "2108637011",
                "name": "Siqi Liu"
              },
              {
                "authorId": "3225993",
                "name": "S. Shen"
              }
            ]
          }
        },
        {
          "citedcorpusid": 239049376,
          "isinfluential": true,
          "contexts": [
            "In terms of architectures, the three most common ones are U-Net [34], [49], FireNet [28], and RAFT [44], [50].",
            "The proposed meth-ods are compared with an unsupervised-learning method [59] (Section II) and a supervised-learning method E-RAFT [44].",
            "Notice that both DNN methods [44], [59] train and evaluate on the DSEC dataset, which is dominantly forward driving motion.",
            "Current state-of-the-art approaches are ANNs [27], [30], [34], [44], [45], [46], largely inspired by frame-based optical ﬂow architectures [47], [48].",
            "Since GT is missing at IMOs and points outside the LiDAR’s FOV, the supervised method [44] may provide inaccurate predictions around IMOs and road points closetothecamera,whereasourmethodproducessharpedges.",
            "Wealso evaluate on a recent dataset that provides ground truth ﬂow: DSEC [44].",
            "Supervised methods train ANNs in simulation and/or real-data [44], [49], [50], [51], [52], [53], [54]."
          ],
          "intents": [
            "['background']",
            "['methodology']",
            "['methodology']",
            "['background']",
            "['methodology']",
            "['background']",
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "E-RAFT: Dense Optical Flow from Event Cameras",
            "abstract": "We propose to incorporate feature correlation and sequential processing into dense optical flow estimation from event cameras. Modern frame-based optical flow methods heavily rely on matching costs computed from feature correlation. In contrast, there exists no optical flow method for event cameras that explicitly computes matching costs. Instead, learning-based approaches using events usually resort to the U-Net architecture to estimate optical flow sparsely. Our key finding is that the introduction of correlation features significantly improves results compared to previous methods that solely rely on convolution layers. Compared to the state-of-the-art, our proposed approach computes dense optical flow and reduces the end-point error by 23% on MVSEC. Furthermore, we show that all existing optical flow methods developed so far for event cameras have been evaluated on datasets with very small displacement fields with maximum flow magnitude of 10 pixels. Based on this observation, we introduce a new real-world dataset that exhibits displacement fields with magnitudes up to 210 pixels and 3 times higher camera resolution. Our proposed approach reduces the end-point error on this dataset by 66%.",
            "year": 2021,
            "venue": "International Conference on 3D Vision",
            "authors": [
              {
                "authorId": "8329387",
                "name": "Mathias Gehrig"
              },
              {
                "authorId": "2124709306",
                "name": "Mario Millhäusler"
              },
              {
                "authorId": "51152279",
                "name": "Daniel Gehrig"
              },
              {
                "authorId": "2075371",
                "name": "D. Scaramuzza"
              }
            ]
          }
        },
        {
          "citedcorpusid": 250408092,
          "isinfluential": true,
          "contexts": [
            "The ECD dataset [63] is a lower resolution, standard dataset to assess camera ego-motion [9], [16], [25], [69], [70], [71], [72].",
            "This is in part because the warp (4) can describe very complex ﬂow ﬁelds, which can push the events to accumulate in few pixels (i.e., event collapse [25], [26]).",
            "The latter may be due to event collapse [25], but given recent advances on overcoming this issue [31], we show it is possible to succeed.",
            "Preliminary work on applying CM to estimate optical ﬂow has reported event collapse [25], [26], producing ﬂows at undesired optima that warp events to few pixels or lines [27]."
          ],
          "intents": [
            "['methodology']",
            "['background']",
            "['background']",
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Event Collapse in Contrast Maximization Frameworks",
            "abstract": "Contrast maximization (CMax) is a framework that provides state-of-the-art results on several event-based computer vision tasks, such as ego-motion or optical flow estimation. However, it may suffer from a problem called event collapse, which is an undesired solution where events are warped into too few pixels. As prior works have largely ignored the issue or proposed workarounds, it is imperative to analyze this phenomenon in detail. Our work demonstrates event collapse in its simplest form and proposes collapse metrics by using first principles of space–time deformation based on differential geometry and physics. We experimentally show on publicly available datasets that the proposed metrics mitigate event collapse and do not harm well-posed warps. To the best of our knowledge, regularizers based on the proposed metrics are the only effective solution against event collapse in the experimental settings considered, compared with other methods. We hope that this work inspires further research to tackle more complex warp models.",
            "year": 2022,
            "venue": "Italian National Conference on Sensors",
            "authors": [
              {
                "authorId": "2066324243",
                "name": "Shintaro Shiba"
              },
              {
                "authorId": "1716469",
                "name": "Y. Aoki"
              },
              {
                "authorId": "144036711",
                "name": "Guillermo Gallego"
              }
            ]
          }
        },
        {
          "citedcorpusid": 254591426,
          "isinfluential": false,
          "contexts": [
            "Due to the regularizer in [24], the very ﬁne structure (e.g., the poster contents) might not be crisp.",
            "These two quantities are entangled, and it is possible to use computed optical ﬂow to recover brightness, i.e., reconstruct intensity images [24].",
            "…[8] is a powerful framework that allows us to tackle multiple motion estimation problems (rotational motion [9], [10], [11], [12], homographic motion [7], [13], [14], feature ﬂow estimation [15], [16], [17], [18], motion segmentation [19], [20], [21], [22], and also reconstruction [7], [23], [24])."
          ],
          "intents": [
            "['background']",
            "['background']",
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Formulating Event-Based Image Reconstruction as a Linear Inverse Problem With Deep Regularization Using Optical Flow",
            "abstract": "Event cameras are novel bio-inspired sensors that measure per-pixel brightness differences asynchronously. Recovering brightness from events is appealing since the reconstructed images inherit the high dynamic range (HDR) and high-speed properties of events; hence they can be used in many robotic vision applications and to generate slow-motion HDR videos. However, state-of-the-art methods tackle this problem by training an event-to-image Recurrent Neural Network (RNN), which lacks explainability and is difficult to tune. In this work we show, for the first time, how tackling the combined problem of motion and brightness estimation leads us to formulate event-based image reconstruction as a linear inverse problem that can be solved without training an image reconstruction RNN. Instead, classical and learning-based regularizers are used to solve the problem and remove artifacts from the reconstructed images. The experiments show that the proposed approach generates images with visual quality on par with state-of-the-art methods despite only using data from a short time interval. State-of-the-art results are achieved using an image denoising Convolutional Neural Network (CNN) as the regularization function. The proposed regularized formulation and solvers have a unifying character because they can be applied also to reconstruct brightness from the second derivative. Additionally, the formulation is attractive because it can be naturally combined with super-resolution, motion-segmentation and color demosaicing. Code is available at https://github.com/tub-rip/event_based_image_rec_inverse_problem",
            "year": 2021,
            "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
            "authors": [
              {
                "authorId": "2134911269",
                "name": "Ze-Feng Zhang"
              },
              {
                "authorId": "144599246",
                "name": "A. Yezzi"
              },
              {
                "authorId": "144036711",
                "name": "Guillermo Gallego"
              }
            ]
          }
        },
        {
          "citedcorpusid": 257505349,
          "isinfluential": false,
          "contexts": [
            "While this is not a problem in simulation, it incurs a performance gap when trained models are used to predict ﬂow on real data, due to often a large domain gap between training and test data [52], [55].",
            "As a result, these learning-based methods may overﬁt to the driving data (i.e., tend to predict forward motion) and fail to produce good results in other motions and datasets [55] (e.g., see E-RAFT rows on the MVSEC indoor seqs. in Table I)."
          ],
          "intents": [
            "['background']",
            "['background']"
          ],
          "cited_paper_info": {
            "title": "BlinkFlow: A Dataset to Push the Limits of Event-Based Optical Flow Estimation",
            "abstract": "Event cameras provide high temporal precision, low data rates, and high dynamic range visual perception, which are well-suited for optical flow estimation. While data-driven optical flow estimation has obtained great success in RGB cameras, its generalization performance is seriously hindered in event cameras mainly due to the limited and biased training data. In this paper, we present a novel simulator, BlinkSim, for the fast generation of large-scale data for event-based optical flow. BlinkSim incorporates a configurable rendering engine alongside an event simulation suite. By leveraging the wealth of current 3D assets, the rendering engine enables us to automatically build up thousands of scenes with different objects, textures, and motion patterns and render very high-frequency images for realistic event data simulation. Based on BlinkSim, we construct a large training dataset and evaluation benchmark BlinkFlow that contains sufficient, diversiform, and challenging event data with optical flow ground truth. Experiments show that BlinkFlow improves the generalization performance of state-of-the-art methods by more than 40% on average and up to 90%. Moreover, we further propose an Event-based optical Flow transFormer (E-FlowFormer) architecture. Powered by our BlinkFlow, E-FlowFormer outperforms the SOTA methods by up to 91% on the MVSEC dataset and 14% on the DSEC dataset and presents the best generalization performance. The source code and data are available at https://zju3dv.github.io/blinkflow/.",
            "year": 2023,
            "venue": "IEEE/RJS International Conference on Intelligent RObots and Systems",
            "authors": [
              {
                "authorId": "2110512150",
                "name": "Yijin Li"
              },
              {
                "authorId": "1830448350",
                "name": "Zhaoyang Huang"
              },
              {
                "authorId": "2116572341",
                "name": "Shuo Chen"
              },
              {
                "authorId": "2119204728",
                "name": "Xiaoyu Shi"
              },
              {
                "authorId": "49404547",
                "name": "Hongsheng Li"
              },
              {
                "authorId": "1679542",
                "name": "H. Bao"
              },
              {
                "authorId": "1813796",
                "name": "Zhaopeng Cui"
              },
              {
                "authorId": "32162658",
                "name": "Guofeng Zhang"
              }
            ]
          }
        },
        {
          "citedcorpusid": 257637142,
          "isinfluential": false,
          "contexts": [
            "Supervised methods train ANNs in simulation and/or real-data [44], [49], [50], [51], [52], [53], [54].",
            "In terms of architectures, the three most common ones are U-Net [34], [49], FireNet [28], and RAFT [44], [50]."
          ],
          "intents": [
            "['methodology']",
            "['background']"
          ],
          "cited_paper_info": {
            "title": "TMA: Temporal Motion Aggregation for Event-based Optical Flow",
            "abstract": "Event cameras have the ability to record continuous and detailed trajectories of objects with high temporal resolution, thereby providing intuitive motion cues for optical flow estimation. Nevertheless, most existing learning-based approaches for event optical flow estimation directly remould the paradigm of conventional images by representing the consecutive event stream as static frames, ignoring the inherent temporal continuity of event data. In this paper, we argue that temporal continuity is a vital element of event-based optical flow and propose a novel Temporal Motion Aggregation (TMA) approach to unlock its potential. Technically, TMA comprises three components: an event splitting strategy to incorporate intermediate motion information underlying the temporal context, a linear lookup strategy to align temporally fine-grained motion features and a novel motion pattern aggregation module to emphasize consistent patterns for motion feature enhancement. By incorporating temporally fine-grained motion information, TMA can derive better flow estimates than existing methods at early stages, which not only enables TMA to obtain more accurate final predictions, but also greatly reduces the demand for a number of refinements. Extensive experiments on DSEC-Flow and MVSEC datasets verify the effectiveness and superiority of our TMA. Remarkably, compared to E-RAFT, TMA achieves a 6% improvement in accuracy and a 40% reduction in inference time on DSEC-Flow. Code will be available at https://github.com/ispc-lab/TMA.",
            "year": 2023,
            "venue": "IEEE International Conference on Computer Vision",
            "authors": [
              {
                "authorId": null,
                "name": "Haotian Liu"
              },
              {
                "authorId": "143930563",
                "name": "Guang Chen"
              },
              {
                "authorId": "52532366",
                "name": "Sanqing Qu"
              },
              {
                "authorId": "2135958284",
                "name": "Yanping Zhang"
              },
              {
                "authorId": "1860369299",
                "name": "Zhijun Li"
              },
              {
                "authorId": "2075424317",
                "name": "Alois Knoll"
              },
              {
                "authorId": "2115484883",
                "name": "Changjun Jiang"
              }
            ]
          }
        },
        {
          "citedcorpusid": 267945091,
          "isinfluential": false,
          "contexts": [
            "We develop two explicit methods to solve the PDEs, one with upwind differences and one with a conservative scheme adapted to Burgers’ terms [65]."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Level Set Methods And Fast Marching Methods",
            "abstract": "Thank you very much for downloading level set methods and fast marching methods. Maybe you have knowledge that, people have search numerous times for their favorite novels like this level set methods and fast marching methods, but end up in infectious downloads. Rather than reading a good book with a cup of tea in the afternoon, instead they cope with some harmful virus inside their laptop. level set methods and fast marching methods is available in our digital library an online access to it is set as public so you can get it instantly. Our book servers hosts in multiple locations, allowing you to get the most less latency time to download any of our books like this one. Kindly say, the level set methods and fast marching methods is universally compatible with any devices to read.",
            "year": 2016,
            "venue": "",
            "authors": [
              {
                "authorId": "2286956819",
                "name": "Klaudia Kaiser"
              }
            ]
          }
        },
        {
          "citedcorpusid": 268379520,
          "isinfluential": false,
          "contexts": [
            "…Maximization (CM) [7], [8] is a powerful framework that allows us to tackle multiple motion estimation problems (rotational motion [9], [10], [11], [12], homographic motion [7], [13], [14], feature ﬂow estimation [15], [16], [17], [18], motion segmentation [19], [20], [21], [22], and also…"
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "CMax-SLAM: Event-Based Rotational-Motion Bundle Adjustment and SLAM System Using Contrast Maximization",
            "abstract": "Event cameras are bioinspired visual sensors that capture pixelwise intensity changes and output asynchronous event streams. They show great potential over conventional cameras to handle challenging scenarios in robotics and computer vision, such as high speed and high dynamic range. This article considers the problem of rotational motion estimation using event cameras. Several event-based rotation estimation methods have been developed in the past decade, but their performance has not been evaluated and compared under unified criteria yet. In addition, these prior works do not consider a global refinement step. To this end, we conduct a systematic study of this problem with two objectives in mind: Summarizing previous works and presenting our own solution. First, we compare prior works both theoretically and experimentally. Second, we propose the first event-based rotation-only bundle adjustment (BA) approach. We formulate it leveraging the state-of-the-art contrast maximization (CMax) framework, which is principled and avoids the need to convert events into frames. Third, we use the proposed BA to build CMax-simultaneous localization and mapping (SLAM), the first event-based rotation-only SLAM system comprising a front-end and a back-end. Our BA is able to run both offline (trajectory smoothing) and online (CMax-SLAM back-end). To demonstrate the performance and versatility of our method, we present comprehensive experiments on synthetic and real-world datasets, including indoor, outdoor, and space scenarios. We discuss the pitfalls of real-world evaluation and propose a proxy for the reprojection error as the figure of merit to evaluate event-based rotation BA methods. We release the source code and novel data sequences to benefit the community. We hope this work leads to a better understanding and fosters further research on event-based egomotion estimation.",
            "year": 2024,
            "venue": "IEEE Transactions on robotics",
            "authors": [
              {
                "authorId": "2291222487",
                "name": "Shuang Guo"
              },
              {
                "authorId": "144036711",
                "name": "Guillermo Gallego"
              }
            ]
          }
        },
        {
          "citedcorpusid": null,
          "isinfluential": false,
          "contexts": [
            "Current state-of-the-art approaches are ANNs [27], [30], [34], [44], [45], [46], largely inspired by frame-based optical ﬂow architectures [47], [48].",
            "Semi-supervised methods use the grayscale images from a colocated camera (e.g., DAVIS [56]) as a supervisory signal: images are warped using the ﬂow predicted by the ANN and their photometric consistency is used as loss function [34], [45], [46]."
          ],
          "intents": [
            "['background']",
            "['methodology']"
          ],
          "cited_paper_info": {}
        },
        {
          "citedcorpusid": null,
          "isinfluential": false,
          "contexts": [
            "Supervised methods train ANNs in simulation and/or real-data [44], [49], [50], [51], [52], [53], [54].",
            "In terms of architectures, the three most common ones are U-Net [34], [49], FireNet [28], and RAFT [44], [50]."
          ],
          "intents": [
            "--",
            "--"
          ],
          "cited_paper_info": {}
        },
        {
          "citedcorpusid": null,
          "isinfluential": false,
          "contexts": [
            "Supervised methods train ANNs in simulation and/or real-data [44], [49], [50], [51], [52], [53], [54]."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {}
        },
        {
          "citedcorpusid": null,
          "isinfluential": false,
          "contexts": [
            "The main difference between our ﬂow (Section III-C) and concurrent proposals [53], [59], [82] is the motion hypothesis and its underlying assumptions: (7) assumes that the ﬂow is constant along its streamlines within short time intervals, which produces linear motion trajectories (Fig."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {}
        },
        {
          "citedcorpusid": null,
          "isinfluential": false,
          "contexts": [
            "The ECD dataset [63] is a lower resolution, standard dataset to assess camera ego-motion [9], [16], [25], [69], [70], [71], [72].",
            "…work, Contrast Maximization (CM) [7], [8] is a powerful framework that allows us to tackle multiple motion estimation problems (rotational motion [9], [10], [11], [12], homographic motion [7], [13], [14], feature ﬂow estimation [15], [16], [17], [18], motion segmentation [19], [20], [21], [22],…"
          ],
          "intents": [
            "['methodology']",
            "['background']"
          ],
          "cited_paper_info": {}
        },
        {
          "citedcorpusid": null,
          "isinfluential": false,
          "contexts": [
            "Current state-of-the-art approaches are ANNs [27], [30], [34], [44], [45], [46], largely inspired by frame-based optical ﬂow architectures [47], [48].",
            "This version of RAFT [48] was introduced along with the DSEC ﬂow benchmark and showed it can estimate pixel correspondences for large displacements."
          ],
          "intents": [
            "['background']",
            "['methodology']"
          ],
          "cited_paper_info": {}
        },
        {
          "citedcorpusid": null,
          "isinfluential": true,
          "contexts": [
            "Among prior work, Contrast Maximization (CM) [7], [8] is a powerful framework that allows us to tackle multiple motion estimation problems (rotational motion [9], [10], [11], [12], homographic motion [7], [13], [14], feature ﬂow estimation [15], [16], [17], [18], motion segmentation [19], [20],…",
            "…[8] is a powerful framework that allows us to tackle multiple motion estimation problems (rotational motion [9], [10], [11], [12], homographic motion [7], [13], [14], feature ﬂow estimation [15], [16], [17], [18], motion segmentation [19], [20], [21], [22], and also reconstruction [7], [23], [24]).",
            "The CM framework [7] assumes events E .",
            "Prior work has proposed adaptations of frame-based approaches (block matching [36], Lucas-Kanade [37]), ﬁlter-banks [38], [39], spatio-temporal plane-ﬁtting [40], [41], time surface matching [42], variational optimization on voxelized events [43], and feature-based contrast maximization [7], [15].",
            "Each scale of our method has the same computational complexity as CM [7], O ( N e + N p ) because the multi-reference warps yield a constant scaling factor.",
            "1) Objective Functions Based on the IWE Gradient: Among the contrast functions proposed in [7], [8], we use two functions based on the gradient of the IWE with q = 1 (the L 1 -norm) and q = 2 (the squared L 2 -norm).",
            "We compare the gradient-based functions ( L 1 and L 2 ), image variance [7], average timestamp [27], and normalized average timestamp [30]."
          ],
          "intents": [
            "['background']",
            "['background']",
            "['background']",
            "['methodology']",
            "['methodology']",
            "['methodology']",
            "['methodology']"
          ],
          "cited_paper_info": {}
        },
        {
          "citedcorpusid": null,
          "isinfluential": false,
          "contexts": [
            "This poses the challenge of rethinking visual processing [2], [3]: motion patterns (i.e., optical ﬂow ) are no longer obtained by analyzing the intensities of images captured at regular intervals, but by analyzing the stream of per-pixel brightness changes produced by the event camera."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {}
        },
        {
          "citedcorpusid": null,
          "isinfluential": false,
          "contexts": [
            "The ﬂow is obtained as the solution to the problem where, λ > 0 is the regularizer weight, and we use the total variation (TV) [66] as regularizer."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {}
        },
        {
          "citedcorpusid": null,
          "isinfluential": false,
          "contexts": [
            "…[8] is a powerful framework that allows us to tackle multiple motion estimation problems (rotational motion [9], [10], [11], [12], homographic motion [7], [13], [14], feature ﬂow estimation [15], [16], [17], [18], motion segmentation [19], [20], [21], [22], and also reconstruction [7], [23], [24])."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {}
        },
        {
          "citedcorpusid": null,
          "isinfluential": true,
          "contexts": [
            "…but with important differences: (i) it was done for the average timestamp loss, hence it did not consider the effect on contrast or focus functions [8], and (ii) it had a completely different motivation: to lessen a back-propagation scaling problem, so that the gradients of the loss would not…",
            "Among prior work, Contrast Maximization (CM) [7], [8] is a powerful framework that allows us to tackle multiple motion estimation problems (rotational motion [9], [10], [11], [12], homographic motion [7], [13], [14], feature ﬂow estimation [15], [16], [17], [18], motion segmentation [19], [20],…",
            "…the scene complexity, the large pixel displacement and the high dynamic range. been tackled by changing the objective function, from contrast to the energy of an average timestamp image [27], [28], but this loss is not straightforward to interpret [8], [29], and is not without its problems [30].",
            "Stemming from an accurate and spatially-dependent contrast loss (the gradient magnitude [8]), we model the problem using a tile of patches (in ﬂow or depth parameters) and propose solutions to several problems: overﬁtting, occlusions, and convergence.",
            "Event alignment is measured by the strength of the edges of the IWE, which is directly related to image contrast [8].",
            "1) Objective Functions Based on the IWE Gradient: Among the contrast functions proposed in [7], [8], we use two functions based on the gradient of the IWE with q = 1 (the L 1 -norm) and q = 2 (the squared L 2 -norm)."
          ],
          "intents": [
            "['background']",
            "['background']",
            "['background']",
            "['methodology']",
            "['background']",
            "['methodology']"
          ],
          "cited_paper_info": {}
        },
        {
          "citedcorpusid": null,
          "isinfluential": false,
          "contexts": [
            "Current state-of-the-art approaches are ANNs [27], [30], [34], [44], [45], [46], largely inspired by frame-based optical ﬂow architectures [47], [48].",
            "Semi-supervised methods use the grayscale images from a colocated camera (e.g., DAVIS [56]) as a supervisory signal: images are warped using the ﬂow predicted by the ANN and their photometric consistency is used as loss function [34], [45], [46]."
          ],
          "intents": [
            "['background']",
            "['methodology']"
          ],
          "cited_paper_info": {}
        },
        {
          "citedcorpusid": null,
          "isinfluential": false,
          "contexts": [
            "Datasets: First, we evaluate our method on sequences from the MVSEC dataset [4], [34], which is the de facto standard dataset used by prior works to benchmark optical ﬂow.",
            "Ground truth (GT) in de facto standard datasets [4], [5] is obtained by the motion ﬁeld [6] given additional depth sensors and camera motion.",
            "It provides events, grayscale frames, IMU data, camera poses, and scene depth from a LiDAR [4]."
          ],
          "intents": [
            "['methodology']",
            "['background']",
            "['background']"
          ],
          "cited_paper_info": {}
        },
        {
          "citedcorpusid": null,
          "isinfluential": false,
          "contexts": [
            "Prior work has proposed adaptations of frame-based approaches (block matching [36], Lucas-Kanade [37]), ﬁlter-banks [38], [39], spatio-temporal plane-ﬁtting [40], [41], time surface matching [42], variational optimization on voxelized events [43], and feature-based contrast maximization [7], [15]."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {}
        },
        {
          "citedcorpusid": null,
          "isinfluential": false,
          "contexts": [
            "Prior work has proposed adaptations of frame-based approaches (block matching [36], Lucas-Kanade [37]), ﬁlter-banks [38], [39], spatio-temporal plane-ﬁtting [40], [41], time surface matching [42], variational optimization on voxelized events [43], and feature-based contrast maximization [7], [15]."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {}
        }
      ]
    },
    "271974279": {
      "citing_paper_info": {
        "title": "ES-PTAM: Event-based Stereo Parallel Tracking and Mapping",
        "abstract": "Visual Odometry (VO) and SLAM are fundamental components for spatial perception in mobile robots. Despite enormous progress in the field, current VO/SLAM systems are limited by their sensors' capability. Event cameras are novel visual sensors that offer advantages to overcome the limitations of standard cameras, enabling robots to expand their operating range to challenging scenarios, such as high-speed motion and high dynamic range illumination. We propose a novel event-based stereo VO system by combining two ideas: a correspondence-free mapping module that estimates depth by maximizing ray density fusion and a tracking module that estimates camera poses by maximizing edge-map alignment. We evaluate the system comprehensively on five real-world datasets, spanning a variety of camera types (manufacturers and spatial resolutions) and scenarios (driving, flying drone, hand-held, egocentric, etc). The quantitative and qualitative results demonstrate that our method outperforms the state of the art in majority of the test sequences by a margin, e.g., trajectory error reduction of 45% on RPG dataset, 61% on DSEC dataset, and 21% on TUM-VIE dataset. To benefit the community and foster research on event-based perception systems, we release the source code and results: https://github.com/tub-rip/ES-PTAM",
        "year": 2024,
        "venue": "ECCV Workshops",
        "authors": [
          {
            "authorId": "2155615482",
            "name": "Suman Ghosh"
          },
          {
            "authorId": "2317114102",
            "name": "Valentina Cavinato"
          },
          {
            "authorId": "144036711",
            "name": "Guillermo Gallego"
          }
        ]
      },
      "citation_summary": {
        "citation_count": 17,
        "unique_cited_count": 15,
        "influential_count": 4,
        "detailed_records_count": 17
      },
      "cited_papers": [
        "266335686",
        "11008141",
        "16588072",
        "56475917",
        "49877954",
        "6324125",
        "229211559",
        "206775501",
        "259380779",
        "260164484",
        "10712214",
        "250918780",
        "3299195",
        "232170230",
        "248572428"
      ],
      "citation_details": [
        {
          "citedcorpusid": 3299195,
          "isinfluential": false,
          "contexts": [
            "Similar to the classification employed for frame-based systems [9], they can be either feature-based (indirect) [18,33] or direct [8,15,37]."
          ],
          "intents": [
            "['result']"
          ],
          "cited_paper_info": {
            "title": "Direct Sparse Odometry",
            "abstract": "Direct Sparse Odometry (DSO) is a visual odometry method based on a novel, highly accurate sparse and direct structure and motion formulation. It combines a fully direct probabilistic model (minimizing a photometric error) with consistent, joint optimization of all model parameters, including geometry-represented as inverse depth in a reference frame-and camera motion. This is achieved in real time by omitting the smoothness prior used in other direct methods and instead sampling pixels evenly throughout the images. Since our method does not depend on keypoint detectors or descriptors, it can naturally sample pixels from across all image regions that have intensity gradient, including edges or smooth intensity variations on essentially featureless walls. The proposed model integrates a full photometric calibration, accounting for exposure time, lens vignetting, and non-linear response functions. We thoroughly evaluate our method on three different datasets comprising several hours of video. The experiments show that the presented approach significantly outperforms state-of-the-art direct and indirect methods in a variety of real-world settings, both in terms of tracking accuracy and robustness.",
            "year": 2016,
            "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
            "authors": [
              {
                "authorId": "35152266",
                "name": "Jakob J. Engel"
              },
              {
                "authorId": "145231047",
                "name": "V. Koltun"
              },
              {
                "authorId": "1695302",
                "name": "D. Cremers"
              }
            ]
          }
        },
        {
          "citedcorpusid": 6324125,
          "isinfluential": false,
          "contexts": [
            "2) via ROS [27]."
          ],
          "intents": [
            "--"
          ],
          "cited_paper_info": {
            "title": "ROS: an open-source Robot Operating System",
            "abstract": "—This paper gives an overview of ROS, an open-source robot operating system. ROS is not an operating system in the traditional sense of process management and scheduling; rather, it provides a structured communications layer above the host operating systems of a heterogenous compute cluster. In this paper, we discuss how ROS relates to existing robot software frameworks, and brieﬂy overview some of the available application software which uses ROS.",
            "year": 2009,
            "venue": "IEEE International Conference on Robotics and Automation",
            "authors": [
              {
                "authorId": "39100828",
                "name": "M. Quigley"
              }
            ]
          }
        },
        {
          "citedcorpusid": 10712214,
          "isinfluential": false,
          "contexts": [
            "Adding more than two cameras to a sensor setup can improve 3D reconstruction by better handling occlusions and removing ambiguities [5]."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Event-based 3D reconstruction from neuromorphic retinas",
            "abstract": "",
            "year": 2013,
            "venue": "Neural Networks",
            "authors": [
              {
                "authorId": "2057119545",
                "name": "J. Carneiro"
              },
              {
                "authorId": "144975525",
                "name": "S. Ieng"
              },
              {
                "authorId": "153466606",
                "name": "C. Posch"
              },
              {
                "authorId": "1750848",
                "name": "R. Benosman"
              }
            ]
          }
        },
        {
          "citedcorpusid": 11008141,
          "isinfluential": false,
          "contexts": [
            "Events are back-projected through 3D space at their corresponding camera location by shooting fictitious rays through 3D space following a space-sweep approach [7]."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "A space-sweep approach to true multi-image matching",
            "abstract": "",
            "year": 1996,
            "venue": "Proceedings CVPR IEEE Computer Society Conference on Computer Vision and Pattern Recognition",
            "authors": [
              {
                "authorId": "143980462",
                "name": "R. Collins"
              }
            ]
          }
        },
        {
          "citedcorpusid": 16588072,
          "isinfluential": true,
          "contexts": [
            "Since we aim to demonstrate the effectiveness of the fusion-based mapping module for visual odometry, we use an event-based camera tracker that works on the produced semi-dense 3D maps, by leveraging the idea of edge-map alignment [29].",
            "Progress in event-based VO/SLAM has been witnessed in monocular [20, 22, 29], and stereo setups [8, 15, 18, 33, 37, 39], possibly aided by other sensors (e.g., depth and/or IMU sensor fusion) [19,30,42].",
            "In this work, we compare the camera tracking performance of our system with the state-of-the-art direct event-only stereo visual odometry method ESVO [37] and with monocular system EVO [29].",
            "All the values for EVO [29] are taken from [22].",
            "We combine this method with a simple, fast and effective event camera tracker grounded on the idea of (semi-dense) edge-map alignment [29].",
            "One possibility is, as in [29], initialization by classical epipolar-geometry methods on event images.",
            "The tracking module is derived from EVO [29], which is grounded on the idea of global image alignment [1].",
            "Whereas the map in EVO [29] is given by EMVS [28], our depth estimates come from a mapping module which extends [28] to multi-camera data fusion via the harmonic mean."
          ],
          "intents": [
            "['methodology']",
            "['background']",
            "['methodology']",
            "['background']",
            "['methodology']",
            "['background']",
            "['methodology']",
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "EVO: A Geometric Approach to Event-Based 6-DOF Parallel Tracking and Mapping in Real Time",
            "abstract": "",
            "year": 2017,
            "venue": "IEEE Robotics and Automation Letters",
            "authors": [
              {
                "authorId": "3414274",
                "name": "Henri Rebecq"
              },
              {
                "authorId": "9676873",
                "name": "Timo Horstschaefer"
              },
              {
                "authorId": "144036711",
                "name": "Guillermo Gallego"
              },
              {
                "authorId": "2075371",
                "name": "D. Scaramuzza"
              }
            ]
          }
        },
        {
          "citedcorpusid": 49877954,
          "isinfluential": false,
          "contexts": [
            "We evaluate the performance of our stereo VO pipeline on sequences from five publicly available datasets [3,14,21,36,40] with varying camera resolutions depicting a wide range of scenarios on different mobile platforms.",
            "This dataset was recorded with stereo DAVIS cameras with 240 × 180 px resolution mounted on a handheld rig [36]."
          ],
          "intents": [
            "['methodology']",
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Semi-Dense 3D Reconstruction with a Stereo Event Camera",
            "abstract": "Event cameras are bio-inspired sensors that offer several advantages, such as low latency, high-speed and high dynamic range, to tackle challenging scenarios in computer vision. This paper presents a solution to the problem of 3D reconstruction from data captured by a stereo event-camera rig moving in a static scene, such as in the context of stereo Simultaneous Localization and Mapping. The proposed method consists of the optimization of an energy function designed to exploit small-baseline spatio-temporal consistency of events triggered across both stereo image planes. To improve the density of the reconstruction and to reduce the uncertainty of the estimation, a probabilistic depth-fusion strategy is also developed. The resulting method has no special requirements on either the motion of the stereo event-camera rig or on prior knowledge about the scene. Experiments demonstrate our method can deal with both texture-rich scenes as well as sparse scenes, outperforming state-of-the-art stereo methods based on event data image representations.",
            "year": 2018,
            "venue": "European Conference on Computer Vision",
            "authors": [
              {
                "authorId": null,
                "name": "Yi Zhou"
              },
              {
                "authorId": "144036711",
                "name": "Guillermo Gallego"
              },
              {
                "authorId": "3414274",
                "name": "Henri Rebecq"
              },
              {
                "authorId": "1727013",
                "name": "L. Kneip"
              },
              {
                "authorId": "40124570",
                "name": "Hongdong Li"
              },
              {
                "authorId": "2075371",
                "name": "D. Scaramuzza"
              }
            ]
          }
        },
        {
          "citedcorpusid": 56475917,
          "isinfluential": false,
          "contexts": [
            "Our work deviates from Zhu et al. [41], which proposes a supervised learning-based system that is trained on stereo data to predict ego-motion and depth, but that at inference time works only on monocular setups."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Unsupervised Event-Based Learning of Optical Flow, Depth, and Egomotion",
            "abstract": "In this work, we propose a novel framework for unsupervised learning for event cameras that learns motion information from only the event stream. In particular, we propose an input representation of the events in the form of a discretized volume that maintains the temporal distribution of the events, which we pass through a neural network to predict the motion of the events. This motion is used to attempt to remove any motion blur in the event image. We then propose a loss function applied to the motion compensated event image that measures the motion blur in this image. We train two networks with this framework, one to predict optical flow, and one to predict egomotion and depths, and evaluate these networks on the Multi Vehicle Stereo Event Camera dataset, along with qualitative results from a variety of different scenes.",
            "year": 2018,
            "venue": "Computer Vision and Pattern Recognition",
            "authors": [
              {
                "authorId": "3385588",
                "name": "A. Z. Zhu"
              },
              {
                "authorId": "36001694",
                "name": "Liangzhe Yuan"
              },
              {
                "authorId": "20728097",
                "name": "Kenneth Chaney"
              },
              {
                "authorId": "1751586",
                "name": "Kostas Daniilidis"
              }
            ]
          }
        },
        {
          "citedcorpusid": 206775501,
          "isinfluential": false,
          "contexts": [
            "In such cases, initial poses may be obtained from running a frame-based method (e.g., SVO [10]) on the DAVIS frames, by IMU dead-reckoning or by a VIO method (e.g., EVIO [38], ROVIO [2])."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "SVO: Semidirect Visual Odometry for Monocular and Multicamera Systems",
            "abstract": "",
            "year": 2017,
            "venue": "IEEE Transactions on robotics",
            "authors": [
              {
                "authorId": "144789467",
                "name": "Christian Forster"
              },
              {
                "authorId": "3414578",
                "name": "Zichao Zhang"
              },
              {
                "authorId": "2072349393",
                "name": "M. Gassner"
              },
              {
                "authorId": "2886023",
                "name": "Manuel Werlberger"
              },
              {
                "authorId": "2075371",
                "name": "D. Scaramuzza"
              }
            ]
          }
        },
        {
          "citedcorpusid": 229211559,
          "isinfluential": false,
          "contexts": [
            "Dealing with them, possibly via a combination with motion segmentation [35], is still an unexplored topic in the incipient field of event-based SLAM."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Event-Based Motion Segmentation With Spatio-Temporal Graph Cuts",
            "abstract": "Identifying independently moving objects is an essential task for dynamic scene understanding. However, traditional cameras used in dynamic scenes may suffer from motion blur or exposure artifacts due to their sampling principle. By contrast, event-based cameras are novel bio-inspired sensors that offer advantages to overcome such limitations. They report pixel-wise intensity changes asynchronously, which enables them to acquire visual information at exactly the same rate as the scene dynamics. We develop a method to identify independently moving objects acquired with an event-based camera, that is, to solve the event-based motion segmentation problem. We cast the problem as an energy minimization one involving the fitting of multiple motion models. We jointly solve two sub-problems, namely event-cluster assignment (labeling) and motion model fitting, in an iterative manner by exploiting the structure of the input event data in the form of a spatio-temporal graph. Experiments on available datasets demonstrate the versatility of the method in scenes with different motion patterns and number of moving objects. The evaluation shows state-of-the-art results without having to predetermine the number of expected moving objects. We release the software and dataset under an open source license to foster research in the emerging topic of event-based motion segmentation.",
            "year": 2020,
            "venue": "IEEE Transactions on Neural Networks and Learning Systems",
            "authors": [
              {
                "authorId": null,
                "name": "Yi Zhou"
              },
              {
                "authorId": "144036711",
                "name": "Guillermo Gallego"
              },
              {
                "authorId": "2110036504",
                "name": "Xiuyuan Lu"
              },
              {
                "authorId": "2108637011",
                "name": "Siqi Liu"
              },
              {
                "authorId": "3225993",
                "name": "S. Shen"
              }
            ]
          }
        },
        {
          "citedcorpusid": 232170230,
          "isinfluential": true,
          "contexts": [
            "Where camera poses are not available, like in DSEC [14], we use LiDAR-inertial odometry data provided by the authors as ground truth.",
            "Figure 3 compares the proposed strategy to that in MC-EMVS [15] on a section of the DSEC driving dataset [14], showing a clear improvement.",
            "1a), which is particularly suitable for automotive applications, supported by the introduction of new datasets [6,14,24,40], because it can recover the absolute scale of the scene and produces fast depth estimates due to spatial parallax.",
            "The sequences in the DSEC dataset [14] were recorded with VGA resolution (640 × 480 px) stereo event cameras on a car driven through the streets of Switzerland.",
            "We evaluate the performance of our stereo VO pipeline on sequences from five publicly available datasets [3,14,21,36,40] with varying camera resolutions depicting a wide range of scenarios on different mobile platforms."
          ],
          "intents": [
            "['methodology']",
            "['methodology']",
            "['background']",
            "['methodology']",
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "DSEC: A Stereo Event Camera Dataset for Driving Scenarios",
            "abstract": "Once an academic venture, autonomous driving has received unparalleled corporate funding in the last decade. Still, operating conditions of current autonomous cars are mostly restricted to ideal scenarios. This means that driving in challenging illumination conditions such as night, sunrise, and sunset remains an open problem. In these cases, standard cameras are being pushed to their limits in terms of low light and high dynamic range performance. To address these challenges, we propose, DSEC, a new dataset that contains such demanding illumination conditions and provides a rich set of sensory data. DSEC offers data from a wide-baseline stereo setup of two color frame cameras and two high-resolution monochrome event cameras. In addition, we collect lidar data and RTK GPS measurements, both hardware synchronized with all camera data. One of the distinctive features of this dataset is the inclusion of high-resolution event cameras. Event cameras have received increasing attention for their high temporal resolution and high dynamic range performance. However, due to their novelty, event camera datasets in driving scenarios are rare. This work presents the first high resolution, large scale stereo dataset with event cameras. The dataset contains 53 sequences collected by driving in a variety of illumination conditions and provides ground truth disparity for the development and evaluation of event-based stereo algorithms.",
            "year": 2021,
            "venue": "IEEE Robotics and Automation Letters",
            "authors": [
              {
                "authorId": "8329387",
                "name": "Mathias Gehrig"
              },
              {
                "authorId": "2052356146",
                "name": "Willem Aarents"
              },
              {
                "authorId": "51152279",
                "name": "Daniel Gehrig"
              },
              {
                "authorId": "2075371",
                "name": "D. Scaramuzza"
              }
            ]
          }
        },
        {
          "citedcorpusid": 248572428,
          "isinfluential": false,
          "contexts": [
            "We evaluate the performance of our stereo VO pipeline on sequences from five publicly available datasets [3,14,21,36,40] with varying camera resolutions depicting a wide range of scenarios on different mobile platforms.",
            "We present results on the only existing trinocular event dataset EVIMO2 [3], recorded using a handheld rig with three VGA resolution event cameras in an indoor environment."
          ],
          "intents": [
            "['methodology']",
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "EVIMO2: An Event Camera Dataset for Motion Segmentation, Optical Flow, Structure from Motion, and Visual Inertial Odometry in Indoor Scenes with Monocular or Stereo Algorithms",
            "abstract": "A new event camera dataset, EVIMO2, is introduced that improves on the popular EVIMO dataset by providing more data, from better cameras, in more complex scenarios. As with its predecessor, EVIMO2 provides labels in the form of per-pixel ground truth depth and segmentation as well as camera and object poses. All sequences use data from physical cameras and many sequences feature multiple independently moving objects. Typically, such labeled data is unavailable in physical event camera datasets. Thus, EVIMO2 will serve as a challenging benchmark for existing algorithms and rich training set for the development of new algorithms. In particular, EVIMO2 is suited for sup-porting research in motion and object segmentation, optical ﬂow, structure from motion, and visual (inertial) odometry in both monocular or stereo conﬁgurations. EVIMO2 consists of 41 minutes of data from three 640 × 480 event cameras, one 2080 × 1552 classical color camera, inertial measurements from two six axis inertial measurement units, and millimeter accurate object poses from a Vicon motion capture system. The dataset’s 173 sequences are arranged into three categories. 3.75 minutes of independently moving household objects, 22.55 minutes of static scenes, and 14.85 minutes of basic motions in shallow scenes. Some sequences were recorded in low-light condi-tions where conventional cameras fail. Depth and segmentation are provided at 60 Hz for the event cameras and 30 Hz for the classical camera. The masks can be regenerated using open-source code up to rates as high as 200 Hz. This technical report brieﬂy describes EVIMO2. The full documentation is available online 1 . Videos of individual sequences can be sampled on the download page 2 .",
            "year": 2022,
            "venue": "arXiv.org",
            "authors": [
              {
                "authorId": "2096137947",
                "name": "Levi Burner"
              },
              {
                "authorId": "144559298",
                "name": "A. Mitrokhin"
              },
              {
                "authorId": "3415312",
                "name": "Cornelia Fermuller"
              },
              {
                "authorId": "1697493",
                "name": "Y. Aloimonos"
              }
            ]
          }
        },
        {
          "citedcorpusid": 250918780,
          "isinfluential": true,
          "contexts": [
            "Among the multiple fusion schemes available in MC-EMVS we choose a point-wise (i.e., voxel-wise) harmonic mean fusion across cameras and no extra fusion in time due to its high accuracy and speed [15].",
            "MC-EMVS [15] circumvented this issue by always setting the DSI RV at the end of the camera trajectory in forward moving scenes, but this strategy does not work for arbitrary motions (e.g., moving backwards).",
            "In contrast to the pioneering work Event-based Stereo Visual Odometry (ESVO) [37], the MC-EMVS mapper offers several advantages, such as sharper edges, higher accuracy and improved depth map completion [15].",
            "Progress in event-based VO/SLAM has been witnessed in monocular [20, 22, 29], and stereo setups [8, 15, 18, 33, 37, 39], possibly aided by other sensors (e.g., depth and/or IMU sensor fusion) [19,30,42].",
            "Specifically, we improve upon the state-of-the-art mapping method (Multi-Camera Event-based Multi-View Stereo – MC-EMVS) [15], capable of generating semi-dense depth maps from synchronized event cameras.",
            "The mapper processes events in batches of fixed number of events N e (instead of a constant time window [15]).",
            "The mapping module is based on MC-EMVS [15], which demonstrated the state-of-the-art depth estimation capabilities when using ground truth (GT) poses.",
            "Improvements with respect to MC-EMVS [15].",
            "Similar to the classification employed for frame-based systems [9], they can be either feature-based (indirect) [18,33] or direct [8,15,37].",
            "Our mapper works on the principle of ray density fusion across stereo cameras [15]."
          ],
          "intents": [
            "['methodology']",
            "['background']",
            "['result']",
            "['background']",
            "['methodology']",
            "['background']",
            "['methodology']",
            "--",
            "['result']",
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Multi‐Event‐Camera Depth Estimation and Outlier Rejection by Refocused Events Fusion",
            "abstract": "Event cameras are bio‐inspired sensors that offer advantages over traditional cameras. They operate asynchronously, sampling the scene at microsecond resolution and producing a stream of brightness changes. This unconventional output has sparked novel computer vision methods to unlock the camera's potential. Here, the problem of event‐based stereo 3D reconstruction for SLAM is considered. Most event‐based stereo methods attempt to exploit the high temporal resolution of the camera and the simultaneity of events across cameras to establish matches and estimate depth. By contrast, this work investigates how to estimate depth without explicit data association by fusing disparity space images (DSIs) originated in efficient monocular methods. Fusion theory is developed and applied to design multi‐camera 3D reconstruction algorithms that produce state‐of‐the‐art results, as confirmed by comparisons with four baseline methods and tests on a variety of available datasets.",
            "year": 2022,
            "venue": "Advanced Intelligent Systems",
            "authors": [
              {
                "authorId": "2155615482",
                "name": "Suman Ghosh"
              },
              {
                "authorId": "144036711",
                "name": "Guillermo Gallego"
              }
            ]
          }
        },
        {
          "citedcorpusid": 259380779,
          "isinfluential": false,
          "contexts": [
            "1a), which is particularly suitable for automotive applications, supported by the introduction of new datasets [6,14,24,40], because it can recover the absolute scale of the scene and produces fast depth estimates due to spatial parallax."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "M3ED: Multi-Robot, Multi-Sensor, Multi-Environment Event Dataset",
            "abstract": "We present M3ED, the first multi-sensor event camera dataset focused on high-speed dynamic motions in robotics applications. M3ED provides high-quality synchronized and labeled data from multiple platforms, including ground vehicles, legged robots, and aerial robots, operating in challenging conditions such as driving along off-road trails, navigating through dense forests, and performing aggressive flight maneuvers. Our dataset also covers demanding operational scenarios for event cameras, such as scenes with high egomotion and multiple independently moving objects. The sensor suite used to collect M3ED includes high-resolution stereo event cameras (1280×720), grayscale imagers, an RGB imager, a high-quality IMU, a 64-beam LiDAR, and RTK localization. This dataset aims to accelerate the development of event-based algorithms and methods for edge cases encountered by autonomous systems in dynamic environments.",
            "year": 2023,
            "venue": "2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)",
            "authors": [
              {
                "authorId": "20728097",
                "name": "Kenneth Chaney"
              },
              {
                "authorId": "3163734",
                "name": "Fernando Cladera Ojeda"
              },
              {
                "authorId": "2117966677",
                "name": "Ziyun Wang"
              },
              {
                "authorId": "30814125",
                "name": "Anthony Bisulco"
              },
              {
                "authorId": "144062128",
                "name": "M. A. Hsieh"
              },
              {
                "authorId": "2311918",
                "name": "C. Korpela"
              },
              {
                "authorId": "37956314",
                "name": "Vijay R. Kumar"
              },
              {
                "authorId": "31589308",
                "name": "C. J. Taylor"
              },
              {
                "authorId": "1751586",
                "name": "Kostas Daniilidis"
              }
            ]
          }
        },
        {
          "citedcorpusid": 260164484,
          "isinfluential": false,
          "contexts": [
            "1 because code was unavailable [8, 18, 33] and/or trajectory errors were either not reported [8], or did not use the same standard metrics as in ESVO (e.g., absolute errors) [18].",
            "Similar to the classification employed for frame-based systems [9], they can be either feature-based (indirect) [18,33] or direct [8,15,37].",
            "Progress in event-based VO/SLAM has been witnessed in monocular [20, 22, 29], and stereo setups [8, 15, 18, 33, 37, 39], possibly aided by other sensors (e.g., depth and/or IMU sensor fusion) [19,30,42]."
          ],
          "intents": [
            "['methodology']",
            "['result']",
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Event-Based Stereo Visual Odometry With Native Temporal Resolution via Continuous-Time Gaussian Process Regression",
            "abstract": "Event-based cameras asynchronously capture individual visual changes in a scene. This makes them more robust than traditional frame-based cameras to highly dynamic motions and poor illumination. It also means that every measurement in a scene can occur at a unique time. Handling these different measurement times is a major challenge of using event-based cameras. It is often addressed in visual odometry (VO) pipelines by approximating temporally close measurements as occurring at one common time. This grouping simplifies the estimation problem but, absent additional sensors, sacrifices the inherent temporal resolution of event-based cameras. This paper instead presents a complete stereo VO pipeline that estimates directly with individual event-measurement times without requiring any grouping or approximation in the estimation state. It uses continuous-time trajectory estimation to maintain the temporal fidelity and asynchronous nature of event-based cameras through Gaussian process regression with a physically motivated prior. Its performance is evaluated on the MVSEC dataset, where it achieves $7.9\\cdot 10^{-3}$ and $5.9\\cdot 10^{-3}$ RMS relative error on two independent sequences, outperforming the existing publicly available event-based stereo VO pipeline by two and four times, respectively.",
            "year": 2023,
            "venue": "IEEE Robotics and Automation Letters",
            "authors": [
              {
                "authorId": "2218932886",
                "name": "Jianeng Wang"
              },
              {
                "authorId": "1735544",
                "name": "J. Gammell"
              }
            ]
          }
        },
        {
          "citedcorpusid": 266335686,
          "isinfluential": true,
          "contexts": [
            "Similar to recent event-based VO methods in the literature [22,25,37], we perform quantitative evaluation by computing the root-mean-square (RMSE) Absolute Trajectory Errors (ATE) and Absolute Rotation Errors (ARE) on tracked camera poses using the tool in [34] (Tab.",
            "Progress in event-based VO/SLAM has been witnessed in monocular [20, 22, 29], and stereo setups [8, 15, 18, 33, 37, 39], possibly aided by other sensors (e.g., depth and/or IMU sensor fusion) [19,30,42].",
            "Moreover, we consider the event-only setting, as our objective is to advance knowledge in purely event-based processing systems, expanding their capabilities while avoiding potential bottlenecks associated with paired sensors [22].",
            "Since ARE is missing from that paper, we take them from [22].",
            "Rotation errors on the RPG dataset were taken from [22].",
            "All the values for EVO [29] are taken from [22]."
          ],
          "intents": [
            "['methodology']",
            "['background']",
            "['background']",
            "['background']",
            "['methodology']",
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Deep Event Visual Odometry",
            "abstract": "Event cameras offer the exciting possibility of tracking the camera’s pose during high-speed motion and in adverse lighting conditions. Despite this promise, existing event-based monocular visual odometry (VO) approaches demonstrate limited performance on recent benchmarks. To address this limitation, some methods resort to additional sensors such as IMUs, stereo event cameras, or frame-based cameras. Nonetheless, these additional sensors limit the application of event cameras in real-world devices since they increase cost and complicate system requirements. Moreover, relying on a frame-based camera makes the system susceptible to motion blur and HDR. To remove the dependency on additional sensors and to push the limits of using only a single event camera, we present Deep Event VO (DEVO), the first monocular event-only system with strong performance on a large number of real-world benchmarks. DEVO sparsely tracks selected event patches over time. A key component of DEVO is a novel deep patch selection mechanism tailored to event data. We significantly decrease the state-of-the-art pose tracking error on seven real-world benchmarks by up to 97% compared to event-only methods and often surpass or are close to stereo or inertial methods.",
            "year": 2023,
            "venue": "International Conference on 3D Vision",
            "authors": [
              {
                "authorId": "1966877",
                "name": "Simone Klenk"
              },
              {
                "authorId": "2274932273",
                "name": "Marvin Motzet"
              },
              {
                "authorId": "1990661065",
                "name": "Lukas Koestler"
              },
              {
                "authorId": "2297191654",
                "name": "Daniel Cremers"
              }
            ]
          }
        },
        {
          "citedcorpusid": null,
          "isinfluential": false,
          "contexts": [
            "…at a high temporal resolution ( µ s), offering low power consumption, high dynamic range and sparsity for robust and efficient VO/SLAM [11,13,23]. for the TUM-VIE mocap-desk sequence [21], along with a snapshot of events, confidence map and the projected point cloud (overlaid on the events).",
            "Event cameras, such as the Dynamic Vision Sensor (DVS) [23], transmit pixel-wise brightness changes asynchronously, in the form of “events”."
          ],
          "intents": [
            "['background']",
            "['background']"
          ],
          "cited_paper_info": {}
        },
        {
          "citedcorpusid": null,
          "isinfluential": false,
          "contexts": [
            "In such cases, initial poses may be obtained from running a frame-based method (e.g., SVO [10]) on the DAVIS frames, by IMU dead-reckoning or by a VIO method (e.g., EVIO [38], ROVIO [2])."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {}
        }
      ]
    },
    "259338957": {
      "citing_paper_info": {
        "title": "Improved Event-Based Dense Depth Estimation via Optical Flow Compensation",
        "abstract": "Event cameras have the potential to overcome the limitations of classical computer vision in real-world applications. Depth estimation is a crucial step for high-level robotics tasks and has attracted much attention from the community. In this paper, we propose an event-based dense depth estimation architecture, Mixed-EF2DNet, which firstly predicts inter-grid optical flow to compensate for lost temporal information, and then estimates multiple contextual depth maps that are fused to generate a robust depth estimation map. To supervise the network training, we further design a smoothing loss function used to smooth local depth estimates and facilitate estimating reasonable depth for pixels without events. In addition, we introduce SE-resblocks in the depth network to enhance the network representation by selecting feature channels. Experimental evaluations on both real-world and synthetic datasets show that our method performs better in terms of accuracy when compared to state-of-the-art algorithms, especially in scene detail estimation. Besides, our method demonstrates excellent generalization in cross-dataset tasks.",
        "year": 2023,
        "venue": "IEEE International Conference on Robotics and Automation",
        "authors": [
          {
            "authorId": "2147239941",
            "name": "Dian-xi Shi"
          },
          {
            "authorId": "2147240285",
            "name": "Luoxi Jing"
          },
          {
            "authorId": "3376633",
            "name": "Ruihao Li"
          },
          {
            "authorId": "47781621",
            "name": "Zhe Liu"
          },
          {
            "authorId": "46660076",
            "name": "L. Wang"
          },
          {
            "authorId": "2156139003",
            "name": "Huachi Xu"
          },
          {
            "authorId": "2153915370",
            "name": "Yi Zhang"
          }
        ]
      },
      "citation_summary": {
        "citation_count": 9,
        "unique_cited_count": 8,
        "influential_count": 3,
        "detailed_records_count": 9
      },
      "cited_papers": [
        "4572038",
        "140309863",
        "231951439",
        "3416874",
        "223957202",
        "91183976",
        "38030033",
        "214667893"
      ],
      "citation_details": [
        {
          "citedcorpusid": 3416874,
          "isinfluential": true,
          "contexts": [
            "We first evaluate our proposed Mixed-EF2DNet on MVSEC Dataset to validate the effectiveness of our method in real-world environments.",
            "Qualitative comparison for MVSEC dataset.",
            "The representative outdoor day1 and outdoor night1 sequences of MVSEC are used for testing to verify the effectiveness of the depth estimation algorithm in the day and night environments.",
            "Our proposed method provides a more complete and accurate depth estimation of objects, such as trees and bus stations. samples of DENSE and then finetune the network by the combination train split of MVSEC and DENSE.",
            "In this section, we implement the experiments on the real-world event dataset MVSEC [24] and synthetic event dataset DENSE [18] to prove the effectiveness of our proposed method.",
            "To further demonstrate the effectiveness of the proposed method, we conduct an ablation study on the MVSEC dataset.",
            "The Multi-Vehicle Stereo Event Camera Dataset (MVSEC) is a popular real-world dataset recorded with mDAVIS346 sensors [24].",
            "world event dataset MVSEC [24] and synthetic event dataset DENSE [18] to prove the effectiveness of our proposed method.",
            "We further evaluate the models trained on DENSE in the testing split of MVSEC to investigate the generalization of methods."
          ],
          "intents": [
            "--",
            "--",
            "--",
            "--",
            "--",
            "--",
            "--",
            "['methodology']",
            "--"
          ],
          "cited_paper_info": {
            "title": "The Multivehicle Stereo Event Camera Dataset: An Event Camera Dataset for 3D Perception",
            "abstract": "Event-based cameras are a new passive sensing modality with a number of benefits over traditional cameras, including extremely low latency, asynchronous data acquisition, high dynamic range, and very low power consumption. There has been a lot of recent interest and development in applying algorithms to use the events to perform a variety of three-dimensional perception tasks, such as feature tracking, visual odometry, and stereo depth estimation. However, there currently lacks the wealth of labeled data that exists for traditional cameras to be used for both testing and development. In this letter, we present a large dataset with a synchronized stereo pair event based camera system, carried on a handheld rig, flown by a hexacopter, driven on top of a car, and mounted on a motorcycle, in a variety of different illumination levels and environments. From each camera, we provide the event stream, grayscale images, and inertial measurement unit (IMU) readings. In addition, we utilize a combination of IMU, a rigidly mounted lidar system, indoor and outdoor motion capture, and GPS to provide accurate pose and depth images for each camera at up to 100 Hz. For comparison, we also provide synchronized grayscale images and IMU readings from a frame-based stereo camera system.",
            "year": 2018,
            "venue": "IEEE Robotics and Automation Letters",
            "authors": [
              {
                "authorId": "3385588",
                "name": "A. Z. Zhu"
              },
              {
                "authorId": "144964367",
                "name": "Dinesh Thakur"
              },
              {
                "authorId": "2520604",
                "name": "Tolga Özaslan"
              },
              {
                "authorId": "39832696",
                "name": "Bernd Pfrommer"
              },
              {
                "authorId": "37956314",
                "name": "Vijay R. Kumar"
              },
              {
                "authorId": "1751586",
                "name": "Kostas Daniilidis"
              }
            ]
          }
        },
        {
          "citedcorpusid": 4572038,
          "isinfluential": true,
          "contexts": [
            "The metrics values for MonoDepth and Zhu et al. method are taken from [15] and those for MegaDepth, MegaDepth+, and E2Depth from [18].",
            "Besides, Mixed-EF2DNet also compares against well known image-based depth estimation algorithms, MonoDepth [8] 1 and MegaDepth [9], to verify the superiority of algorithms specifically designed for events.",
            "The predictions from MegaDepth might omit the scene information lost by images due to low lighting (e.g., the traffic light), and they have texture edges that break the continuity of depth (e.g., textures on the ground).",
            "For day environments, predicted depth maps from MegaDepth have artifacts in the sky.",
            "5, we visualize the depth maps predicted by the image-based method MegaDepth, event-based method E2Depth, and Mixed-EF2DNet.",
            "The quantitative results are presented in Table I. MegaDepth+ indicates that events are reconstructed into images using E2VID [27] then taken by MegaDepth to estimate depth."
          ],
          "intents": [
            "--",
            "['methodology']",
            "--",
            "--",
            "--",
            "--"
          ],
          "cited_paper_info": {
            "title": "MegaDepth: Learning Single-View Depth Prediction from Internet Photos",
            "abstract": "Single-view depth prediction is a fundamental problem in computer vision. Recently, deep learning methods have led to significant progress, but such methods are limited by the available training data. Current datasets based on 3D sensors have key limitations, including indoor-only images (NYU), small numbers of training examples (Make3D), and sparse sampling (KITTI). We propose to use multi-view Internet photo collections, a virtually unlimited data source, to generate training data via modern structure-from-motion and multi-view stereo (MVS) methods, and present a large depth dataset called MegaDepth based on this idea. Data derived from MVS comes with its own challenges, including noise and unreconstructable objects. We address these challenges with new data cleaning methods, as well as automatically augmenting our data with ordinal depth relations generated using semantic segmentation. We validate the use of large amounts of Internet data by showing that models trained on MegaDepth exhibit strong generalization-not only to novel scenes, but also to other diverse datasets including Make3D, KITTI, and DIW, even when no images from those datasets are seen during training.1",
            "year": 2018,
            "venue": "2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition",
            "authors": [
              {
                "authorId": "2145369560",
                "name": "Zhengqi Li"
              },
              {
                "authorId": "1830653",
                "name": "Noah Snavely"
              }
            ]
          }
        },
        {
          "citedcorpusid": 38030033,
          "isinfluential": false,
          "contexts": [
            "where ρ(x)= √ x2 +σ2 denotes the Chabonnier loss function [23] and N (u) denotes the 4 neighbouring pixels around the pixel u."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Two deterministic half-quadratic regularization algorithms for computed imaging",
            "abstract": "",
            "year": 1994,
            "venue": "Proceedings of 1st International Conference on Image Processing",
            "authors": [
              {
                "authorId": "2951325",
                "name": "P. Charbonnier"
              },
              {
                "authorId": "1389816580",
                "name": "L. Blanc-Féraud"
              },
              {
                "authorId": "31641158",
                "name": "G. Aubert"
              },
              {
                "authorId": "144459223",
                "name": "M. Barlaud"
              }
            ]
          }
        },
        {
          "citedcorpusid": 91183976,
          "isinfluential": false,
          "contexts": [
            "low power consumption, and no motion blur, event cameras offer a potential choice for the scenarios where conventional cameras are challenged [2], [3]."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Event-Based Motion Segmentation by Motion Compensation",
            "abstract": "In contrast to traditional cameras, whose pixels have a common exposure time, event-based cameras are novel bio-inspired sensors whose pixels work independently and asynchronously output intensity changes (called \"events\"), with microsecond resolution. Since events are caused by the apparent motion of objects, event-based cameras sample visual information based on the scene dynamics and are, therefore, a more natural fit than traditional cameras to acquire motion, especially at high speeds, where traditional cameras suffer from motion blur. However, distinguishing between events caused by different moving objects and by the camera's ego-motion is a challenging task. We present the first per-event segmentation method for splitting a scene into independently moving objects. Our method jointly estimates the event-object associations (i.e., segmentation) and the motion parameters of the objects (or the background) by maximization of an objective function, which builds upon recent results on event-based motion-compensation. We provide a thorough evaluation of our method on a public dataset, outperforming the state-of-the-art by as much as 10%. We also show the first quantitative evaluation of a segmentation algorithm for event cameras, yielding around 90% accuracy at 4 pixels relative displacement.",
            "year": 2019,
            "venue": "IEEE International Conference on Computer Vision",
            "authors": [
              {
                "authorId": "40037881",
                "name": "Timo Stoffregen"
              },
              {
                "authorId": "144036711",
                "name": "Guillermo Gallego"
              },
              {
                "authorId": "144418842",
                "name": "T. Drummond"
              },
              {
                "authorId": "46322392",
                "name": "L. Kleeman"
              },
              {
                "authorId": "2075371",
                "name": "D. Scaramuzza"
              }
            ]
          }
        },
        {
          "citedcorpusid": 140309863,
          "isinfluential": false,
          "contexts": [
            "In each SE-resblock, the SE block [22] performs a squeeze and excitation operation on the feature map U output by two convolutions and gets a collection of per-channel modulation weights W ."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Squeeze-and-Excitation Networks",
            "abstract": "Convolutional neural networks are built upon the convolution operation, which extracts informative features by fusing spatial and channel-wise information together within local receptive fields. In order to boost the representational power of a network, several recent approaches have shown the benefit of enhancing spatial encoding. In this work, we focus on the channel relationship and propose a novel architectural unit, which we term the \"Squeeze-and-Excitation\" (SE) block, that adaptively recalibrates channel-wise feature responses by explicitly modelling interdependencies between channels. We demonstrate that by stacking these blocks together, we can construct SENet architectures that generalise extremely well across challenging datasets. Crucially, we find that SE blocks produce significant performance improvements for existing state-of-the-art deep architectures at minimal additional computational cost. SENets formed the foundation of our ILSVRC 2017 classification submission which won first place and significantly reduced the top-5 error to 2.251%, achieving a ~25% relative improvement over the winning entry of 2016. Code and models are available at https://github.com/hujie-frank/SENet.",
            "year": 2017,
            "venue": "2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition",
            "authors": [
              {
                "authorId": "145815850",
                "name": "Jie Hu"
              },
              {
                "authorId": "152148573",
                "name": "Li Shen"
              },
              {
                "authorId": "7641268",
                "name": "Samuel Albanie"
              },
              {
                "authorId": "2087137982",
                "name": "Gang Sun"
              },
              {
                "authorId": "145344139",
                "name": "E. Wu"
              }
            ]
          }
        },
        {
          "citedcorpusid": 214667893,
          "isinfluential": false,
          "contexts": [
            "To quickly verify the effectiveness of introducing optical flow for our task, we use a mature image-based optical flow network RAFT [19] to estimate the optical flow between voxel grids.",
            "The central voxel grid V c and a side voxel grid V s i are passed into RAFT to output the optical flow Op s i from the central voxel grid to the side one."
          ],
          "intents": [
            "['methodology']",
            "--"
          ],
          "cited_paper_info": {
            "title": "RAFT: Recurrent All-Pairs Field Transforms for Optical Flow",
            "abstract": "We introduce Recurrent All-Pairs Field Transforms (RAFT), a new deep network architecture for optical flow. RAFT extracts per-pixel features, builds multi-scale 4D correlation volumes for all pairs of pixels, and iteratively updates a flow field through a recurrent unit that performs lookups on the correlation volumes. RAFT achieves state-of-the-art performance on the KITTI and Sintel datasets. In addition, RAFT has strong cross-dataset generalization as well as high efficiency in inference time, training speed, and parameter count.",
            "year": 2020,
            "venue": "European Conference on Computer Vision",
            "authors": [
              {
                "authorId": "8048414",
                "name": "Zachary Teed"
              },
              {
                "authorId": "153302678",
                "name": "Jia Deng"
              }
            ]
          }
        },
        {
          "citedcorpusid": 223957202,
          "isinfluential": true,
          "contexts": [
            "The metrics values for MonoDepth and Zhu et al. method are taken from [15] and those for MegaDepth, MegaDepth+, and E2Depth from [18].",
            "The most related to our method is [18], where a recurrent encoder-decoder style framework is used to perform dense depth estimation from monocular event stream by leveraging the temporal consistency.",
            "The values for E2Depth here are taken from [18].",
            "In this section, we implement the experiments on the real-world event dataset MVSEC [24] and synthetic event dataset DENSE [18] to prove the effectiveness of our proposed method.",
            "Following the prior work [18], we pre-train Mixed-EF2DNet with the first 1000 training (e) Ours (f) Ground Truth Fig.",
            "Our method solves the same task by a supervised joint network that further introduces optical flow compensating temporal relationship lost by event pre-processing than [18].",
            "We compare our proposed Mixed-EF2DNet with event-based depth estimation algorithms, including the method proposed by Zhu et al. [15] and E2Depth [18]."
          ],
          "intents": [
            "['methodology']",
            "['methodology']",
            "['background']",
            "['methodology']",
            "['methodology']",
            "['methodology']",
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Learning Monocular Dense Depth from Events",
            "abstract": "Event cameras are novel sensors that output brightness changes in the form of a stream of asynchronous ”events” instead of intensity frames. Compared to conventional image sensors, they offer significant advantages: high temporal resolution, high dynamic range, no motion blur, and much lower bandwidth. Recently, learning-based approaches have been applied to event-based data, thus unlocking their potential and making significant progress in a variety of tasks, such as monocular depth prediction. Most existing approaches use standard feed-forward architectures to generate network predictions, which do not leverage the temporal consistency presents in the event stream. We propose a recurrent architecture to solve this task and show significant improvement over standard feed-forward methods. In particular, our method generates dense depth predictions using a monocular setup, which has not been shown previously. We pretrain our model using a new dataset containing events and depth maps recorded in the CARLA simulator. We test our method on the Multi Vehicle Stereo Event Camera Dataset (MVSEC). Quantitative experiments show up to 50% improvement in average depth error with respect to previous event-based methods. Code and dataset are available at: http://rpg.ifi.uzh.ch/e2depth",
            "year": 2020,
            "venue": "International Conference on 3D Vision",
            "authors": [
              {
                "authorId": "2065112865",
                "name": "Javier Hidalgo-Carri'o"
              },
              {
                "authorId": "51152279",
                "name": "Daniel Gehrig"
              },
              {
                "authorId": "2075371",
                "name": "D. Scaramuzza"
              }
            ]
          }
        },
        {
          "citedcorpusid": 231951439,
          "isinfluential": false,
          "contexts": [
            "[17] used events and intensity images to complement each other and further estimated dense depth maps."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Combining Events and Frames Using Recurrent Asynchronous Multimodal Networks for Monocular Depth Prediction",
            "abstract": "Event cameras are novel vision sensors that report per-pixel brightness changes as a stream of asynchronous “events”. They offer significant advantages compared to standard cameras due to their high temporal resolution, high dynamic range and lack of motion blur. However, events only measure the varying component of the visual signal, which limits their ability to encode scene context. By contrast, standard cameras measure absolute intensity frames, which capture a much richer representation of the scene. Both sensors are thus complementary. However, due to the asynchronous nature of events, combining them with synchronous images remains challenging, especially for learning-based methods. This is because traditional recurrent neural networks (RNNs) are not designed for asynchronous and irregular data from additional sensors. To address this challenge, we introduce Recurrent Asynchronous Multimodal (RAM) networks, which generalize traditional RNNs to handle asynchronous and irregular data from multiple sensors. Inspired by traditional RNNs, RAM networks maintain a hidden state that is updated asynchronously and can be queried at any time to generate a prediction. We apply this novel architecture to monocular depth estimation with events and frames where we show an improvement over state-of-the-art methods by up to 30% in terms of mean absolute depth error. To enable further research on multimodal learning with events, we release EventScape, a new dataset with events, intensity frames, semantic labels, and depth maps recorded in the CARLA simulator.",
            "year": 2021,
            "venue": "IEEE Robotics and Automation Letters",
            "authors": [
              {
                "authorId": "51152279",
                "name": "Daniel Gehrig"
              },
              {
                "authorId": "2051531912",
                "name": "Michelle Rüegg"
              },
              {
                "authorId": "8329387",
                "name": "Mathias Gehrig"
              },
              {
                "authorId": "1406402485",
                "name": "Javier Hidalgo-Carrió"
              },
              {
                "authorId": "2075371",
                "name": "D. Scaramuzza"
              }
            ]
          }
        },
        {
          "citedcorpusid": null,
          "isinfluential": false,
          "contexts": [
            "Bio-inspired dynamic vision sensors called event cameras, such as Dynamic Vision Sensor (DVS) [1], are novel eventdriven devices that only report the pixel where illumination intensity has changed beyond a set threshold.",
            "Bio-inspired dynamic vision sensors called event cameras, such as Dynamic Vision Sensor (DVS) [1], are novel event-driven devices that only report the pixel where illumination intensity has changed beyond a set threshold."
          ],
          "intents": [
            "['background']",
            "--"
          ],
          "cited_paper_info": {}
        }
      ]
    },
    "260164484": {
      "citing_paper_info": {
        "title": "Event-Based Stereo Visual Odometry With Native Temporal Resolution via Continuous-Time Gaussian Process Regression",
        "abstract": "Event-based cameras asynchronously capture individual visual changes in a scene. This makes them more robust than traditional frame-based cameras to highly dynamic motions and poor illumination. It also means that every measurement in a scene can occur at a unique time. Handling these different measurement times is a major challenge of using event-based cameras. It is often addressed in visual odometry (VO) pipelines by approximating temporally close measurements as occurring at one common time. This grouping simplifies the estimation problem but, absent additional sensors, sacrifices the inherent temporal resolution of event-based cameras. This paper instead presents a complete stereo VO pipeline that estimates directly with individual event-measurement times without requiring any grouping or approximation in the estimation state. It uses continuous-time trajectory estimation to maintain the temporal fidelity and asynchronous nature of event-based cameras through Gaussian process regression with a physically motivated prior. Its performance is evaluated on the MVSEC dataset, where it achieves $7.9\\cdot 10^{-3}$ and $5.9\\cdot 10^{-3}$ RMS relative error on two independent sequences, outperforming the existing publicly available event-based stereo VO pipeline by two and four times, respectively.",
        "year": 2023,
        "venue": "IEEE Robotics and Automation Letters",
        "authors": [
          {
            "authorId": "2218932886",
            "name": "Jianeng Wang"
          },
          {
            "authorId": "1735544",
            "name": "J. Gammell"
          }
        ]
      },
      "citation_summary": {
        "citation_count": 17,
        "unique_cited_count": 17,
        "influential_count": 4,
        "detailed_records_count": 17
      },
      "cited_papers": [
        "27987704",
        "235794981",
        "3299195",
        "220870707",
        "49864158",
        "3738244",
        "221112528",
        "12475678",
        "9729856",
        "16284071",
        "221670108",
        "65172180",
        "195496021",
        "14925984",
        "837271",
        "1143169",
        "26324573"
      ],
      "citation_details": [
        {
          "citedcorpusid": 837271,
          "isinfluential": true,
          "contexts": [
            "This process is iterated until convergence to find the velocity best describing the initial inlier set found by fast MC-RANSAC, ¯ ϖ ← ¯ ϖ + δ ϖ ∗ .",
            "The fast version of MC-RANSAC (Sec.",
            "This iterative process uses the velocities found during MC-RANSAC as an initial condition.",
            "The inlier tracklets from MC-RANSAC are used to define the trajectory optimization problem (Fig.",
            "It also uses Motion-Compensated RANSAC (MC-RANSAC) [10] to consider the unique measurement times during outlier rejection and independently provide better tracklets and initial conditions for estimation.",
            "The largest inlier set found is used as the initial segmentation for the full iterative MC-RANSAC.",
            "In contrast to these existing works, it maintains temporal resolution with either frame-based or event-based feature detection and tracking and with a RANSAC formulation [10] that separates outlier rejection from estimation.",
            "Both versions of MC-RANSAC are compared to traditional RANSAC in [10].",
            "This process is repeated a user-specified number of times and then the largest inlier set is refined using the full iterative MC-RANSAC (Sec.",
            "This assumption is incorrect for asynchronous event tracklets and this paper instead uses MC-RANSAC [10], which makes no assumptions about common state times and uses a constant-velocity model in SE(3).",
            "This assumption is incorrect for asynchronous event tracklets and this paper instead uses MC-RANSAC [10], which makes no assumptions about common state times and uses a constant-velocity model in SE (3) .",
            "This is more accurate outlier rejection than using fast MC-RANSAC alone.",
            "Outlier rejection was done with 10000 iterations of fast MC-RANSAC followed with one call of iterative MC-RANSAC, both using an inlier threshold of 5%.",
            "MC-RANSAC segments these tracklets into inliers and outliers by finding the most tracklets that can be explained by a single velocity.",
            "2) Iterative MC-RANSAC: A more accurate iterative MC-RANSAC approach minimizes the reprojection error of each tracklet in image space with the cost function, where R j,k is the covariance matrix for the measurements of the j th tracklet and M fast is the number of inliers found by the fast MC-RANSAC."
          ],
          "intents": [
            "--",
            "--",
            "--",
            "--",
            "['methodology']",
            "--",
            "['result']",
            "['methodology']",
            "--",
            "['methodology']",
            "--",
            "--",
            "--",
            "--",
            "--"
          ],
          "cited_paper_info": {
            "title": "RANSAC for motion-distorted 3D visual sensors",
            "abstract": "",
            "year": 2013,
            "venue": "2013 IEEE/RSJ International Conference on Intelligent Robots and Systems",
            "authors": [
              {
                "authorId": "92114892",
                "name": "S. Anderson"
              },
              {
                "authorId": "87222581",
                "name": "T. Barfoot"
              }
            ]
          }
        },
        {
          "citedcorpusid": 1143169,
          "isinfluential": false,
          "contexts": [
            "Traditional VO pipeline uses Random Sample Consensus (RANSAC) [33] to remove tracklet outliers before estimation."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Visual odometry based on stereo image sequences with RANSAC-based outlier rejection scheme",
            "abstract": "",
            "year": 2010,
            "venue": "IEEE Intelligent Vehicles Symposium",
            "authors": [
              {
                "authorId": "1767366",
                "name": "B. Kitt"
              },
              {
                "authorId": "47237027",
                "name": "Andreas Geiger"
              },
              {
                "authorId": "1779883",
                "name": "Henning Lategahn"
              }
            ]
          }
        },
        {
          "citedcorpusid": 3299195,
          "isinfluential": false,
          "contexts": [
            "V ISUAL Odometry (VO) is a technique to estimate egomotion in robotics [1], [2], [3], [4], [5]."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Direct Sparse Odometry",
            "abstract": "Direct Sparse Odometry (DSO) is a visual odometry method based on a novel, highly accurate sparse and direct structure and motion formulation. It combines a fully direct probabilistic model (minimizing a photometric error) with consistent, joint optimization of all model parameters, including geometry-represented as inverse depth in a reference frame-and camera motion. This is achieved in real time by omitting the smoothness prior used in other direct methods and instead sampling pixels evenly throughout the images. Since our method does not depend on keypoint detectors or descriptors, it can naturally sample pixels from across all image regions that have intensity gradient, including edges or smooth intensity variations on essentially featureless walls. The proposed model integrates a full photometric calibration, accounting for exposure time, lens vignetting, and non-linear response functions. We thoroughly evaluate our method on three different datasets comprising several hours of video. The experiments show that the presented approach significantly outperforms state-of-the-art direct and indirect methods in a variety of real-world settings, both in terms of tracking accuracy and robustness.",
            "year": 2016,
            "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
            "authors": [
              {
                "authorId": "35152266",
                "name": "Jakob J. Engel"
              },
              {
                "authorId": "145231047",
                "name": "V. Koltun"
              },
              {
                "authorId": "1695302",
                "name": "D. Cremers"
              }
            ]
          }
        },
        {
          "citedcorpusid": 3738244,
          "isinfluential": false,
          "contexts": [
            "Many pipelines do this by grouping similar feature times to a common time [7], [8], [9].",
            "Ultimate-SLAM [7] extends [17] to use the IMU to generate motion-compensated event frames and reformulates the cost function for camera egomotion estimation.",
            "[18] extract features using an asynchronous feature detector (Arc* [19]) from a stereo pair of event cameras and adopt an estimation pipeline similar to [7]."
          ],
          "intents": [
            "['background']",
            "['methodology']",
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Ultimate SLAM? Combining Events, Images, and IMU for Robust Visual SLAM in HDR and High-Speed Scenarios",
            "abstract": "Event cameras are bioinspired vision sensors that output pixel-level brightness changes instead of standard intensity frames. These cameras do not suffer from motion blur and have a very high dynamic range, which enables them to provide reliable visual information during high-speed motions or in scenes characterized by high dynamic range. However, event cameras output only little information when the amount of motion is limited, such as in the case of almost still motion. Conversely, standard cameras provide instant and rich information about the environment most of the time (in low-speed and good lighting scenarios), but they fail severely in case of fast motions, or difficult lighting such as high dynamic range or low light scenes. In this letter, we present the first state estimation pipeline that leverages the complementary advantages of these two sensors by fusing in a tightly coupled manner events, standard frames, and inertial measurements. We show on the publicly available Event Camera Dataset that our hybrid pipeline leads to an accuracy improvement of 130% over event-only pipelines, and 85% over standard-frames-only visual-inertial systems, while still being computationally tractable. Furthermore, we use our pipeline to demonstrate—to the best of our knowledge—the first autonomous quadrotor flight using an event camera for state estimation, unlocking flight scenarios that were not reachable with traditional visual-inertial odometry, such as low-light environments and high dynamic range scenes. Videos of the experiments: http://rpg.ifi.uzh.ch/ultimateslam.html",
            "year": 2017,
            "venue": "IEEE Robotics and Automation Letters",
            "authors": [
              {
                "authorId": "26430800",
                "name": "Antoni Rosinol Vidal"
              },
              {
                "authorId": "3414274",
                "name": "Henri Rebecq"
              },
              {
                "authorId": "9676873",
                "name": "Timo Horstschaefer"
              },
              {
                "authorId": "2075371",
                "name": "D. Scaramuzza"
              }
            ]
          }
        },
        {
          "citedcorpusid": 9729856,
          "isinfluential": false,
          "contexts": [
            "Mueggler et al. [27] use a continuous-time pose estimation framework that uses IMU measurements and represents the trajectory as cumulative cubic B-splines."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Continuous-Time Visual-Inertial Odometry for Event Cameras",
            "abstract": "Event cameras are bioinspired vision sensors that output pixel-level brightness changes instead of standard intensity frames. They offer significant advantages over standard cameras, namely a very high dynamic range, no motion blur, and a latency in the order of microseconds. However, due to the fundamentally different structure of the sensor's output, new algorithms that exploit the high temporal resolution and the asynchronous nature of the sensor are required. Recent work has shown that a continuous-time representation of the event camera pose can deal with the high temporal resolution and asynchronous nature of this sensor in a principled way. In this paper, we leverage such a continuous-time representation to perform visual-inertial odometry with an event camera. This representation allows direct integration of the asynchronous events with microsecond accuracy and the inertial measurements at high frequency. The event camera trajectory is approximated by a smooth curve in the space of rigid-body motions using cubic splines. This formulation significantly reduces the number of variables in trajectory estimation problems. We evaluate our method on real data from several scenes and compare the results against ground truth from a motion-capture system. We show that our method provides improved accuracy over the result of a state-of-the-art visual odometry method for event cameras. We also show that both the map orientation and scale can be recovered accurately by fusing events and inertial data. To the best of our knowledge, this is the first work on visual-inertial fusion with event cameras using a continuous-time framework.",
            "year": 2017,
            "venue": "IEEE Transactions on robotics",
            "authors": [
              {
                "authorId": "144578041",
                "name": "Elias Mueggler"
              },
              {
                "authorId": "144036711",
                "name": "Guillermo Gallego"
              },
              {
                "authorId": "3414274",
                "name": "Henri Rebecq"
              },
              {
                "authorId": "2075371",
                "name": "D. Scaramuzza"
              }
            ]
          }
        },
        {
          "citedcorpusid": 12475678,
          "isinfluential": true,
          "contexts": [
            "Features are detected and tracked in the event frames and each feature is assigned an event time from the SAE.",
            "1) Event Clustering: The stereo event stream is rectified and clustered to construct new SAEs and new binary event frames (Fig.",
            "The system takes an asynchronous stereo event stream and clusters the events into event frames and SAEs.",
            "The timestamp of each tracklet state is assigned from the nearest event in the associated SAE.",
            "The temporal resolution of event cameras is maintained by assigning (possibly unique) times to each feature from the corresponding event in a Surface of Active Events (SAE) [31].",
            "The SAE records the most recent event timestamp of each pixel location and is used to maintain the asynchronous nature of the event camera."
          ],
          "intents": [
            "--",
            "--",
            "--",
            "--",
            "['background']",
            "--"
          ],
          "cited_paper_info": {
            "title": "Event-Based Visual Flow",
            "abstract": "",
            "year": 2014,
            "venue": "IEEE Transactions on Neural Networks and Learning Systems",
            "authors": [
              {
                "authorId": "1750848",
                "name": "R. Benosman"
              },
              {
                "authorId": "48762590",
                "name": "Charles Clercq"
              },
              {
                "authorId": "1807856",
                "name": "Xavier Lagorce"
              },
              {
                "authorId": "144975525",
                "name": "S. Ieng"
              },
              {
                "authorId": "1897771",
                "name": "C. Bartolozzi"
              }
            ]
          }
        },
        {
          "citedcorpusid": 14925984,
          "isinfluential": false,
          "contexts": [
            "[23] present a feature-based stereo VO pipeline using events, which group events into frames and then adopts a similar estimation framework to a traditional frame-based stereo VO pipeline [3].",
            "V ISUAL Odometry (VO) is a technique to estimate egomotion in robotics [1], [2], [3], [4], [5]."
          ],
          "intents": [
            "['methodology']",
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Visual odometry on the Mars Exploration Rovers",
            "abstract": "",
            "year": 2005,
            "venue": "IEEE International Conference on Systems, Man and Cybernetics",
            "authors": [
              {
                "authorId": "144858616",
                "name": "Yang Cheng"
              },
              {
                "authorId": "1781507",
                "name": "M. Maimone"
              },
              {
                "authorId": "1782162",
                "name": "L. Matthies"
              }
            ]
          }
        },
        {
          "citedcorpusid": 16284071,
          "isinfluential": false,
          "contexts": [
            "The sliding window width is five and LIBVISO2 is run on both full-and half-resolution images.",
            "A sliding-window version of the system is implemented in MATLAB using LIBVISO2 [36] for feature detection and tracking."
          ],
          "intents": [
            "--",
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "StereoScan: Dense 3d reconstruction in real-time",
            "abstract": "",
            "year": 2011,
            "venue": "2011 IEEE Intelligent Vehicles Symposium (IV)",
            "authors": [
              {
                "authorId": "47237027",
                "name": "Andreas Geiger"
              },
              {
                "authorId": "144003762",
                "name": "Julius Ziegler"
              },
              {
                "authorId": "1760556",
                "name": "C. Stiller"
              }
            ]
          }
        },
        {
          "citedcorpusid": 26324573,
          "isinfluential": false,
          "contexts": [
            "[21] estimate SE(3) motion using only eventbased cameras."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Real-Time 3D Reconstruction and 6-DoF Tracking with an Event Camera",
            "abstract": "",
            "year": 2016,
            "venue": "European Conference on Computer Vision",
            "authors": [
              {
                "authorId": "3227772",
                "name": "Hanme Kim"
              },
              {
                "authorId": "2864731",
                "name": "Stefan Leutenegger"
              },
              {
                "authorId": "2052135690",
                "name": "A. Davison"
              }
            ]
          }
        },
        {
          "citedcorpusid": 27987704,
          "isinfluential": false,
          "contexts": [
            "It is compared quantitatively to other estimation techniques in [24, 35].",
            "This interpolation can also be used to define the estimation states at a subset of the measurement times [24, 35].",
            "The system’s computational performance should be improved by implementing it in a more efficient language, such as C++, and using keytimes to reduce the number of the estimation states [24, 35]."
          ],
          "intents": [
            "['methodology']",
            "['methodology']",
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Gaussian Process Gauss–Newton for non-parametric simultaneous localization and mapping",
            "abstract": "",
            "year": 2013,
            "venue": "Int. J. Robotics Res.",
            "authors": [
              {
                "authorId": "2399448",
                "name": "Chi Hay Tong"
              },
              {
                "authorId": "1792387",
                "name": "P. Furgale"
              },
              {
                "authorId": "87222581",
                "name": "T. Barfoot"
              }
            ]
          }
        },
        {
          "citedcorpusid": 49864158,
          "isinfluential": false,
          "contexts": [
            "[18] extract features using an asynchronous feature detector (Arc* [19]) from a stereo pair of event cameras and adopt an estimation pipeline similar to [7]."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Asynchronous Corner Detection and Tracking for Event Cameras in Real Time",
            "abstract": "The recent emergence of bioinspired event cameras has opened up exciting new possibilities in high-frequency tracking, bringing robustness to common problems in traditional vision, such as lighting changes and motion blur. In order to leverage these attractive attributes of the event cameras, research has been focusing on understanding how to process their unusual output: an asynchronous stream of events. With the majority of existing techniques discretizing the event-stream essentially forming frames of events grouped according to their timestamp, we are still to exploit the power of these cameras. In this spirit, this letter proposes a new, purely event-based corner detector, and a novel corner tracker, demonstrating that it is possible to detect corners and track them directly on the event stream in real time. Evaluation on benchmarking datasets reveals a significant boost in the number of detected corners and the repeatability of such detections over the state of the art even in challenging scenarios with the proposed approach while enabling more than a 4$\\times$ speed-up when compared to the most efficient algorithm in the literature. The proposed pipeline detects and tracks corners at a rate of more than 7.5 million events per second, promising great impact in high-speed applications.",
            "year": 2018,
            "venue": "IEEE Robotics and Automation Letters",
            "authors": [
              {
                "authorId": "7278610",
                "name": "Ignacio Alzugaray"
              },
              {
                "authorId": "1885768",
                "name": "M. Chli"
              }
            ]
          }
        },
        {
          "citedcorpusid": 65172180,
          "isinfluential": true,
          "contexts": [
            "…γ k , can be defined as a continuous-time function with respect to the global trajectory state, , where T k ( τ ) is the pose at time t k ≤ τ ≤ t k +1 , J ( · ) − 1 is the inverse left Jacobian function, ln( · ) is the inverse exponential map, and ( · ) ∨ is the inverse lifting operator [34].",
            "…result equal to zero gives the perturbation that minimizes the linearization, where H j,k is the Jacobian of error function in (7), where the partial derivative of the sensor model is evaluated at the nominal value, T k,k ′ is the adjoint of SE (3) and J k,k ′ is the left Jacobian of SE (3) [34].",
            "…δ x = { δ ξ , δ ϖ , δ ζ } , linearizing (10) as where where P j,k and P k are matrices to pick the specific components of the total perturbation, G j,k is the Jacobian of (13), and E k is the Jacobian of the prior error function in (11), where and ( · ) ⋏ is the R 6 × 1 to R 6 × 6 operator [34].",
            "Substituting (3) into (2) approximates the error term as, where [34].",
            "This constant velocity over a time, ∆ t , gives the relative SE (3) transformation, where exp( · ) is the matrix exponential and ( · ) ∧ is the lifting operator that converts R 6 × 1 to R 4 × 4 [34]."
          ],
          "intents": [
            "['background']",
            "['background']",
            "['background']",
            "['background']",
            "['background']"
          ],
          "cited_paper_info": {
            "title": "State Estimation for Robotics",
            "abstract": "A key aspect of robotics today is estimating the state (e.g., position and orientation) of a robot, based on noisy sensor data. This book targets students and practitioners of robotics by presenting classical state estimation methods (e.g., the Kalman filter) but also important modern topics such as batch estimation, Bayes filter, sigmapoint and particle filters, robust estimation for outlier rejection, and continuous-time trajectory estimation and its connection to Gaussian-process regression. Since most robots operate in a three-dimensional world, common sensor models (e.g., camera, laser rangefinder) are provided followed by practical advice on how to carry out state estimation for rotational state variables. The book covers robotic applications such as point-cloud alignment, pose-graph relaxation, bundle adjustment, and simultaneous localization and mapping. Highlights of this expanded second edition include a new chapter on variational inference, a new section on inertial navigation, more introductory material on probability, and a primer on matrix calculus.",
            "year": 2017,
            "venue": "",
            "authors": [
              {
                "authorId": "87222581",
                "name": "T. Barfoot"
              }
            ]
          }
        },
        {
          "citedcorpusid": 195496021,
          "isinfluential": false,
          "contexts": [
            "The trajectory is initialized with an Ackermann motion model [29], and globally optimized with a B-spline-based continuous-time estimation framework."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Motion Estimation of Non-Holonomic Ground Vehicles From a Single Feature Correspondence Measured Over N Views",
            "abstract": "The planar motion of ground vehicles is often non-holonomic, which enables a solution of the two-view relative pose problem from a single point feature correspondence. Man-made environments such as underground parking lots are however dominated by line features. Inspired by the planar tri-focal tensor and its ability to handle lines, we establish an n-linear constraint on the locally circular motion of non-holonomic vehicles able to handle an arbitrarily large and dense window of views. We prove that this stays a uni-variate problem under the assumption of locally constant vehicle speed, and it can transparently handle both point and vertical line correspondences. In particular, we prove that an application of Viète's formulas for extrapolating trigonometric functions of angle multiples and the Weierstrass substitution casts the problem as one that merely seeks the roots of a uni-variate polynomial. We present the complete theory of this novel solver, and test it on both simulated and real data. Our results prove that it successfully handles a variety of relevant scenarios, eventually outperforming the 1-point two-view solver.",
            "year": 2019,
            "venue": "Computer Vision and Pattern Recognition",
            "authors": [
              {
                "authorId": "2112705724",
                "name": "Kun Huang"
              },
              {
                "authorId": "2115568923",
                "name": "Yifu Wang"
              },
              {
                "authorId": "1727013",
                "name": "L. Kneip"
              }
            ]
          }
        },
        {
          "citedcorpusid": 220870707,
          "isinfluential": true,
          "contexts": [
            "It is evaluated on the publicly available Multi Vehicle Stereo Event Camera (MVSEC) dataset [11], where it obtains a more accurate and smoother trajectory estimate than the state-of-theart Event-based Stereo Visual Odometry (ESVO) [9].",
            "The presented pipeline is evaluated on the MVSEC dataset [11] and compared against the publicly available ESVO [9], a discrete event-based stereo VO pipeline.",
            "Many pipelines do this by grouping similar feature times to a common time [7], [8], [9].",
            "ESVO [9] uses parallel tracking and mapping to estimate the egomotion trajectory and a semidense 3D scene reconstruction."
          ],
          "intents": [
            "['methodology']",
            "['methodology']",
            "['background']",
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Event-Based Stereo Visual Odometry",
            "abstract": "Event-based cameras are bioinspired vision sensors whose pixels work independently from each other and respond asynchronously to brightness changes, with microsecond resolution. Their advantages make it possible to tackle challenging scenarios in robotics, such as high-speed and high dynamic range scenes. We present a solution to the problem of visual odometry from the data acquired by a stereo event-based camera rig. Our system follows a parallel tracking-and-mapping approach, where novel solutions to each subproblem (three-dimensional (3-D) reconstruction and camera pose estimation) are developed with two objectives in mind: being principled and efficient, for real-time operation with commodity hardware. To this end, we seek to maximize the spatio-temporal consistency of stereo event-based data while using a simple and efficient representation. Specifically, the mapping module builds a semidense 3-D map of the scene by fusing depth estimates from multiple viewpoints (obtained by spatio-temporal consistency) in a probabilistic fashion. The tracking module recovers the pose of the stereo rig by solving a registration problem that naturally arises due to the chosen map and event data representation. Experiments on publicly available datasets and on our own recordings demonstrate the versatility of the proposed method in natural scenes with general 6-DoF motion. The system successfully leverages the advantages of event-based cameras to perform visual odometry in challenging illumination conditions, such as low-light and high dynamic range, while running in real-time on a standard CPU. We release the software and dataset under an open source license to foster research in the emerging topic of event-based simultaneous localization and mapping.",
            "year": 2020,
            "venue": "IEEE Transactions on robotics",
            "authors": [
              {
                "authorId": null,
                "name": "Yi Zhou"
              },
              {
                "authorId": "144036711",
                "name": "Guillermo Gallego"
              },
              {
                "authorId": "3225993",
                "name": "S. Shen"
              }
            ]
          }
        },
        {
          "citedcorpusid": 221112528,
          "isinfluential": false,
          "contexts": [
            "IMU Dynamic Vision Sensor Odometry using Lines (IDOL) [20] uses an alternative VIO paradigm."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "IDOL: A Framework for IMU-DVS Odometry using Lines",
            "abstract": "In this paper, we introduce IDOL, an optimization-based framework for IMU-DVS Odometry using Lines. Event cameras, also called Dynamic Vision Sensors (DVSs), generate highly asynchronous streams of events triggered upon illumination changes for each individual pixel. This novel paradigm presents advantages in low illumination conditions and high-speed motions. Nonetheless, this unconventional sensing modality brings new challenges to perform scene reconstruction or motion estimation. The proposed method offers to leverage a continuous-time representation of the inertial readings to associate each event with timely accurate inertial data. The method's front-end extracts event clusters that belong to line segments in the environment whereas the back-end estimates the system's trajectory alongside the lines' 3D position by minimizing point-to-line distances between individual events and the lines' projection in the image space. A novel attraction/repulsion mechanism is presented to accurately estimate the lines' extremities, avoiding their explicit detection in the event data. The proposed method is benchmarked against a state-of-the-art frame-based visual-inertial odometry framework using public datasets. The results show that IDOL performs at the same order of magnitude on most datasets and even shows better orientation estimates. These findings can have a great impact on new algorithms for DVS.",
            "year": 2020,
            "venue": "IEEE/RJS International Conference on Intelligent RObots and Systems",
            "authors": [
              {
                "authorId": "51295768",
                "name": "C. Gentil"
              },
              {
                "authorId": "40637345",
                "name": "Florian Tschopp"
              },
              {
                "authorId": "7278610",
                "name": "Ignacio Alzugaray"
              },
              {
                "authorId": "1401939307",
                "name": "Teresa Vidal-Calleja"
              },
              {
                "authorId": "1720483",
                "name": "R. Siegwart"
              },
              {
                "authorId": "144147879",
                "name": "Juan I. Nieto"
              }
            ]
          }
        },
        {
          "citedcorpusid": 221670108,
          "isinfluential": false,
          "contexts": [
            "It could also be directly replaced with event-based methods, e.g., Arc* [19] or HASTE [32]."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "HASTE: multi-Hypothesis Asynchronous Speeded-up Tracking of Events",
            "abstract": "Feature tracking using event cameras has experienced signiﬁcant progress lately, with methods achieving comparable performance to feature trackers using traditional frame-based cameras, even outperforming them on certain challenging scenarios. Most of the event-based trackers, however, still operate on intermediate, frame-like representations generated from accumulated events, on which traditional frame-based techniques can be adopted. Attempting to harness the sparsity and asynchronicity of the event stream, other approaches have emerged to process each event individually, but they lack both in accuracy and efﬁciency in comparison to the event-based, frame-like alternatives. Aiming to address this shortcoming of asynchronous approaches, in this paper, we propose an asynchronous patch-feature tracker that relies solely on events and processes each event individually as soon as it gets generated. We report signiﬁcant improvements in tracking quality over the state of the art in publicly available datasets, while performing an order of magnitude more efﬁciently than similar asynchronous tracking approaches.",
            "year": 2020,
            "venue": "British Machine Vision Conference",
            "authors": [
              {
                "authorId": "7278610",
                "name": "Ignacio Alzugaray"
              },
              {
                "authorId": "1885768",
                "name": "M. Chli"
              }
            ]
          }
        },
        {
          "citedcorpusid": 235794981,
          "isinfluential": false,
          "contexts": [
            "Hadviger et al. [23] present a feature-based stereo VO pipeline using events, which group events into frames and then adopts a similar estimation framework to a traditional frame-based stereo VO pipeline [3]."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Feature-based Event Stereo Visual Odometry",
            "abstract": "Event-based cameras are biologically inspired sensors that output events, i.e., asynchronous pixel-wise brightness changes in the scene. Their high dynamic range and temporal resolution of a microsecond makes them more reliable than standard cameras in environments of challenging illumination and in high-speed scenarios, thus developing odometry algorithms based solely on event cameras offers exciting new possibilities for autonomous systems and robots. In this paper, we propose a novel stereo visual odometry method for event cameras based on feature detection and matching with careful feature management, while pose estimation is done by feature reprojection error minimization. We evaluate the performance of the proposed method on two publicly available datasets: MVSEC sequences captured by an indoor flying drone and DSEC outdoor driving sequences. MVSEC offers accurate ground truth from motion capture, while for DSEC, which does not offer ground truth, in order to obtain a reference trajectory on the standard camera frames we used our SOFT visual odometry, one of the highest ranking algorithms on the KITTI scoreboards. We compared our method to the ESVO method, which is the first and still the only stereo event odometry method, showing on par performance on both MVSEC and DSEC sequences. Furthermore, two important advantages of our method over ESVO are that it adapts tracking frequency to the asynchronous event rate and does not require initialization.",
            "year": 2021,
            "venue": "European Conference on Mobile Robots",
            "authors": [
              {
                "authorId": "51450968",
                "name": "Antea Hadviger"
              },
              {
                "authorId": "2720665",
                "name": "Igor Cvisic"
              },
              {
                "authorId": "2053721089",
                "name": "Ivan Markovi'c"
              },
              {
                "authorId": "3237756",
                "name": "Sacha Vrazic"
              },
              {
                "authorId": "2064601815",
                "name": "Ivan Petrovi'c"
              }
            ]
          }
        }
      ]
    },
    "246656358": {
      "citing_paper_info": {
        "title": "Analytical Review of Event-Based Camera Depth Estimation Methods and Systems",
        "abstract": "Event-based cameras have increasingly become more commonplace in the commercial space as the performance of these cameras has also continued to increase to the degree where they can exponentially outperform their frame-based counterparts in many applications. However, instantiations of event-based cameras for depth estimation are sparse. After a short introduction detailing the salient differences and features of an event-based camera compared to that of a traditional, frame-based one, this work summarizes the published event-based methods and systems known to date. An analytical review of these methods and systems is performed, justifying the conclusions drawn. This work is concluded with insights and recommendations for further development in the field of event-based camera depth estimation.",
        "year": 2022,
        "venue": "Italian National Conference on Sensors",
        "authors": [
          {
            "authorId": "2153753559",
            "name": "Justas Furmonas"
          },
          {
            "authorId": "2286357",
            "name": "J. Liobe"
          },
          {
            "authorId": "2781315",
            "name": "V. Barzdėnas"
          }
        ]
      },
      "citation_summary": {
        "citation_count": 26,
        "unique_cited_count": 25,
        "influential_count": 4,
        "detailed_records_count": 26
      },
      "cited_papers": [
        "3993392",
        "197634653",
        "30913835",
        "605892",
        "21317717",
        "20873334",
        "15357188",
        "4412139",
        "208098355",
        "22158024",
        "6079544",
        "18123440",
        "34855834",
        "25268038",
        "203162947",
        "3719281",
        "1226657",
        "2792722",
        "206596513",
        "814743",
        "15373627",
        "185541",
        "26324573",
        "220314130",
        "7151414"
      ],
      "citation_details": [
        {
          "citedcorpusid": 185541,
          "isinfluential": false,
          "contexts": [
            "[42] ++ + + + +++ [43] + ++ +++ + +++ [44] ++ +++ [51] + ++ + ++ [35] + + +++ ++ [49] + +++ [48] 1 + + +++ [47] 2 ++ [40] + + ++ + + [53] + ++ + +++ + [45] + ++ + +++ [55] + + ++ ++ [41] + ++ ++ ++ + [52] +++ +++ + +++ [28] ++ + ++ +++ + [57] +++ +++ + [50] ++ ++ + ++"
          ],
          "intents": [
            "--"
          ],
          "cited_paper_info": {
            "title": "Event-driven stereo matching for real-time 3D panoramic vision",
            "abstract": "",
            "year": 2015,
            "venue": "Computer Vision and Pattern Recognition",
            "authors": [
              {
                "authorId": "2521747",
                "name": "S. Schraml"
              },
              {
                "authorId": "1768812",
                "name": "A. Belbachir"
              },
              {
                "authorId": "144746444",
                "name": "H. Bischof"
              }
            ]
          }
        },
        {
          "citedcorpusid": 605892,
          "isinfluential": false,
          "contexts": [
            "Neuromorphic computing consists of a variety of brain-inspired computers, devices, and models that contrast the pervasive von Neumann computer architecture [3].",
            "Moreover, neuromorphic computing has reached some significant milestones, which have been well summarized in [3]."
          ],
          "intents": [
            "['background']",
            "['background']"
          ],
          "cited_paper_info": {
            "title": "A Survey of Neuromorphic Computing and Neural Networks in Hardware",
            "abstract": "Neuromorphic computing has come to refer to a variety of brain-inspired computers, devices, and models that contrast the pervasive von Neumann computer architecture. This biologically inspired approach has created highly connected synthetic neurons and synapses that can be used to model neuroscience theories as well as solve challenging machine learning problems. The promise of the technology is to create a brain-like ability to learn and adapt, but the technical challenges are significant, starting with an accurate neuroscience model of how the brain works, to finding materials and engineering breakthroughs to build devices to support these models, to creating a programming framework so the systems can learn, to creating applications with brain-like capabilities. In this work, we provide a comprehensive survey of the research and motivations for neuromorphic computing over its history. We begin with a 35-year review of the motivations and drivers of neuromorphic computing, then look at the major research areas of the field, which we define as neuro-inspired models, algorithms and learning approaches, hardware and devices, supporting systems, and finally applications. We conclude with a broad discussion on the major research topics that need to be addressed in the coming years to see the promise of neuromorphic computing fulfilled. The goals of this work are to provide an exhaustive review of the research conducted in neuromorphic computing since the inception of the term, and to motivate further work by illuminating gaps in the field where new research is needed.",
            "year": 2017,
            "venue": "arXiv.org",
            "authors": [
              {
                "authorId": "2318902",
                "name": "Catherine D. Schuman"
              },
              {
                "authorId": "1771895",
                "name": "T. Potok"
              },
              {
                "authorId": "36395353",
                "name": "R. Patton"
              },
              {
                "authorId": "145961053",
                "name": "J. Birdwell"
              },
              {
                "authorId": "49313983",
                "name": "Mark E. Dean"
              },
              {
                "authorId": "145103402",
                "name": "G. Rose"
              },
              {
                "authorId": "1711281",
                "name": "J. Plank"
              }
            ]
          }
        },
        {
          "citedcorpusid": 814743,
          "isinfluential": false,
          "contexts": [
            "An intermediate complexity choice is the Izhikevich model [31]."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Simple model of spiking neurons",
            "abstract": "A model is presented that reproduces spiking and bursting behavior of known types of cortical neurons. The model combines the biologically plausibility of Hodgkin-Huxley-type dynamics and the computational efficiency of integrate-and-fire neurons. Using this model, one can simulate tens of thousands of spiking cortical neurons in real time (1 ms resolution) using a desktop PC.",
            "year": 2003,
            "venue": "IEEE Trans. Neural Networks",
            "authors": [
              {
                "authorId": "1754233",
                "name": "E. Izhikevich"
              }
            ]
          }
        },
        {
          "citedcorpusid": 1226657,
          "isinfluential": false,
          "contexts": [
            "It eliminates the dependency on absolute lighting level and instead the receptors respond to changes in the incident light (also known as temporal contrast) [13,34]."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Eye Smarter than Scientists Believed: Neural Computations in Circuits of the Retina",
            "abstract": "",
            "year": 2010,
            "venue": "Neuron",
            "authors": [
              {
                "authorId": "2987085",
                "name": "T. Gollisch"
              },
              {
                "authorId": "143936459",
                "name": "M. Meister"
              }
            ]
          }
        },
        {
          "citedcorpusid": 2792722,
          "isinfluential": true,
          "contexts": [
            "The latency between triggered events has dropped to less than 0.5 µ s [5,6]."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Live demonstration: A 768 × 640 pixels 200Meps dynamic vision sensor",
            "abstract": "",
            "year": 2017,
            "venue": "International Symposium on Circuits and Systems",
            "authors": [
              {
                "authorId": "2998931",
                "name": "M. Guo"
              },
              {
                "authorId": "2145740196",
                "name": "Jing Huang"
              },
              {
                "authorId": "50358115",
                "name": "Shoushun Chen"
              }
            ]
          }
        },
        {
          "citedcorpusid": 3719281,
          "isinfluential": false,
          "contexts": [
            "The depth prediction is done with a fully convolutional neural network, based on the UNet architecture [59]."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "U-Net: Convolutional Networks for Biomedical Image Segmentation",
            "abstract": "There is large consent that successful training of deep networks requires many thousand annotated training samples. In this paper, we present a network and training strategy that relies on the strong use of data augmentation to use the available annotated samples more efficiently. The architecture consists of a contracting path to capture context and a symmetric expanding path that enables precise localization. We show that such a network can be trained end-to-end from very few images and outperforms the prior best method (a sliding-window convolutional network) on the ISBI challenge for segmentation of neuronal structures in electron microscopic stacks. Using the same network trained on transmitted light microscopy images (phase contrast and DIC) we won the ISBI cell tracking challenge 2015 in these categories by a large margin. Moreover, the network is fast. Segmentation of a 512x512 image takes less than a second on a recent GPU. The full implementation (based on Caffe) and the trained networks are available at http://lmb.informatik.uni-freiburg.de/people/ronneber/u-net .",
            "year": 2015,
            "venue": "International Conference on Medical Image Computing and Computer-Assisted Intervention",
            "authors": [
              {
                "authorId": "1737326",
                "name": "O. Ronneberger"
              },
              {
                "authorId": "152702479",
                "name": "P. Fischer"
              },
              {
                "authorId": "1710872",
                "name": "T. Brox"
              }
            ]
          }
        },
        {
          "citedcorpusid": 3993392,
          "isinfluential": false,
          "contexts": [
            "Later, time surfaces (hierarchy of time surfaces (HOTS) and histograms of averaged time surfaces (HATS)) were presented, which used the spatiotemporal resolution provided by the event-based cameras to generate 3D maps of event data [26,27]."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "HATS: Histograms of Averaged Time Surfaces for Robust Event-Based Object Classification",
            "abstract": "Event-based cameras have recently drawn the attention of the Computer Vision community thanks to their advantages in terms of high temporal resolution, low power consumption and high dynamic range, compared to traditional frame-based cameras. These properties make event-based cameras an ideal choice for autonomous vehicles, robot navigation or UAV vision, among others. However, the accuracy of event-based object classification algorithms, which is of crucial importance for any reliable system working in real-world conditions, is still far behind their frame-based counterparts. Two main reasons for this performance gap are: 1. The lack of effective low-level representations and architectures for event-based object classification and 2. The absence of large real-world event-based datasets. In this paper we address both problems. First, we introduce a novel event-based feature representation together with a new machine learning architecture. Compared to previous approaches, we use local memory units to efficiently leverage past temporal information and build a robust event-based representation. Second, we release the first large real-world event-based dataset for object classification. We compare our method to the state-of-the-art with extensive experiments, showing better classification performance and real-time computation.",
            "year": 2018,
            "venue": "2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition",
            "authors": [
              {
                "authorId": "2861464",
                "name": "A. Sironi"
              },
              {
                "authorId": "32122487",
                "name": "Manuele Brambilla"
              },
              {
                "authorId": "2359111",
                "name": "Nicolas Bourdis"
              },
              {
                "authorId": "1807856",
                "name": "Xavier Lagorce"
              },
              {
                "authorId": "1750848",
                "name": "R. Benosman"
              }
            ]
          }
        },
        {
          "citedcorpusid": 4412139,
          "isinfluential": false,
          "contexts": [
            "The authors of [52] showed that motion blur can be solved by having a range of disparities to synchronize the position of events."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Realtime Time Synchronized Event-based Stereo",
            "abstract": "In this work, we propose a novel event based stereo method which addresses the problem of motion blur for a moving event camera. Our method uses the velocity of the camera and a range of disparities to synchronize the positions of the events, as if they were captured at a single point in time. We represent these events using a pair of novel time synchronized event disparity volumes, which we show remove motion blur for pixels at the correct disparity in the volume, while further blurring pixels at the wrong disparity. We then apply a novel matching cost over these time synchronized event disparity volumes, which both rewards similarity between the volumes while penalizing blurriness. We show that our method outperforms more expensive, smoothing based event stereo methods, by evaluating on the Multi Vehicle Stereo Event Camera dataset.",
            "year": 2018,
            "venue": "European Conference on Computer Vision",
            "authors": [
              {
                "authorId": "3385588",
                "name": "A. Z. Zhu"
              },
              {
                "authorId": "2116435960",
                "name": "Yibo Chen"
              },
              {
                "authorId": "1751586",
                "name": "Kostas Daniilidis"
              }
            ]
          }
        },
        {
          "citedcorpusid": 6079544,
          "isinfluential": false,
          "contexts": [
            "Going further, a spatiotemporal adaptive cooperative approach was used to calculate disparities for each incoming event in [44,45]."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Asynchronous Stereo Vision for Event-Driven Dynamic Stereo Sensor Using an Adaptive Cooperative Approach",
            "abstract": "",
            "year": 2013,
            "venue": "2013 IEEE International Conference on Computer Vision Workshops",
            "authors": [
              {
                "authorId": "47105977",
                "name": "Ewa Piatkowska"
              },
              {
                "authorId": "1768812",
                "name": "A. Belbachir"
              },
              {
                "authorId": "1990797",
                "name": "M. Gelautz"
              }
            ]
          }
        },
        {
          "citedcorpusid": 7151414,
          "isinfluential": false,
          "contexts": [
            "Going further, a spatiotemporal adaptive cooperative approach was used to calculate disparities for each incoming event in [44,45]."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Improved Cooperative Stereo Matching for Dynamic Vision Sensors with Ground Truth Evaluation",
            "abstract": "",
            "year": 2017,
            "venue": "2017 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)",
            "authors": [
              {
                "authorId": "47105977",
                "name": "Ewa Piatkowska"
              },
              {
                "authorId": "1824241",
                "name": "J. Kogler"
              },
              {
                "authorId": "1768812",
                "name": "A. Belbachir"
              },
              {
                "authorId": "1990797",
                "name": "M. Gelautz"
              }
            ]
          }
        },
        {
          "citedcorpusid": 15357188,
          "isinfluential": false,
          "contexts": [
            "[42] [63] Embedded Blackfin BF537 DSP Stereo 3."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "A 128×128 120db 30mw Asynchronous Vision Sensor That Responds to Relative Intensity Change",
            "abstract": "The frame-based architectures of most imagers are natural for making movies and pictures, they have significant drawbacks for machine vision. Short-latency vision problems require high frame rate, producing massive readout (e.g., >1GB/s from 352×288 pixels at 10kFrames/s [1]). Reducing the output to a manageable rate by using region-of-interest readout usually requires complex control strategies. Readout and processing of largely redundant data ultimately limit reductions in computational effort and power consumption. In this paper, a vision sensor is presented whose pixels asynchronously respond to events that represent relative changes in intensity. It operates largely independent of scene illumination, directly encodes object reflectance, and reduces redundancy while preserving precise timing information. Because output bandwidth is automatically dedicated to dynamic parts of the scene, the sensor is suitable for applications in surveillance and motion analysis. It improves on prior frame-based temporal difference detection imagers (e.g., [2]) by asynchronously responding to temporal contrast rather than absolute illumination, and on prior event-based imagers because they either do not reduce redundancy at all [3], reduce only spatial redundancy [4], have large FPN, slow response, and limited dynamic range [5], or have low contrast sensitivity [6].",
            "year": 2005,
            "venue": "",
            "authors": [
              {
                "authorId": "1744964",
                "name": "P. Lichtsteiner"
              },
              {
                "authorId": "2368354141",
                "name": "C. Posch"
              },
              {
                "authorId": "5548576",
                "name": "T. Delbruck"
              }
            ]
          }
        },
        {
          "citedcorpusid": 15373627,
          "isinfluential": true,
          "contexts": [
            "An event-based camera combined with TrueNorth has successfully demonstrated the ability of a fully event-based stereo system [41].",
            "In [41], the program that is running on TrueNorth was written in the Corelet programming language that was speciﬁcally created by IBM for this hardware platform.",
            "At the time of publication for this paper, a few neuromorphic processors have already been manufactured and well-utilized: TrueNorth [36], Loihi [37], SpiNNaker [38], and some others still in development [39].",
            "This neuromorphic chip consists of systems of equations that deﬁne the behaviour of TrueNorth’s neurons.",
            "The second method utilizing a TrueNorth processor realized a fully event-based system that achieved high computational efﬁciency and low power consumption.",
            "It used an event-based camera, and its output data was connected to nine TrueNorth chips.",
            "Table 3 presents seventeen methods that use standard CPUs, GPUs, or embedded platforms, only two methods that use FPGAs and one method that utilizes a TrueNorth processor."
          ],
          "intents": [
            "--",
            "--",
            "['methodology']",
            "--",
            "--",
            "--",
            "--"
          ],
          "cited_paper_info": {
            "title": "TrueNorth: Design and Tool Flow of a 65 mW 1 Million Neuron Programmable Neurosynaptic Chip",
            "abstract": "",
            "year": 2015,
            "venue": "IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems",
            "authors": [
              {
                "authorId": "3349882",
                "name": "Filipp Akopyan"
              },
              {
                "authorId": "48775866",
                "name": "J. Sawada"
              },
              {
                "authorId": "34019307",
                "name": "A. Cassidy"
              },
              {
                "authorId": "1393944534",
                "name": "Rodrigo Alvarez-Icaza"
              },
              {
                "authorId": "2100344",
                "name": "J. Arthur"
              },
              {
                "authorId": "2408151",
                "name": "P. Merolla"
              },
              {
                "authorId": "39536844",
                "name": "N. Imam"
              },
              {
                "authorId": "2109993277",
                "name": "Yutaka Nakamura"
              },
              {
                "authorId": "2111766588",
                "name": "Pallab Datta"
              },
              {
                "authorId": "145198179",
                "name": "Gi-Joon Nam"
              },
              {
                "authorId": "1736425",
                "name": "B. Taba"
              },
              {
                "authorId": "1882932",
                "name": "Michael P. Beakes"
              },
              {
                "authorId": "2938842",
                "name": "B. Brezzo"
              },
              {
                "authorId": "35121277",
                "name": "J. B. Kuang"
              },
              {
                "authorId": "144576833",
                "name": "R. Manohar"
              },
              {
                "authorId": "2904790",
                "name": "W. Risk"
              },
              {
                "authorId": "3234142",
                "name": "Bryan L. Jackson"
              },
              {
                "authorId": "1944330",
                "name": "D. Modha"
              }
            ]
          }
        },
        {
          "citedcorpusid": 18123440,
          "isinfluential": false,
          "contexts": [
            "Some solutions exist that can remove or mitigate motion blur or substitute it with motion flow using deep neural networks [2]."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "From Motion Blur to Motion Flow: A Deep Learning Solution for Removing Heterogeneous Motion Blur",
            "abstract": "Removing pixel-wise heterogeneous motion blur is challenging due to the ill-posed nature of the problem. The predominant solution is to estimate the blur kernel by adding a prior, but extensive literature on the subject indicates the difficulty in identifying a prior which is suitably informative, and general. Rather than imposing a prior based on theory, we propose instead to learn one from the data. Learning a prior over the latent image would require modeling all possible image content. The critical observation underpinning our approach, however, is that learning the motion flow instead allows the model to focus on the cause of the blur, irrespective of the image content. This is a much easier learning task, but it also avoids the iterative process through which latent image priors are typically applied. Our approach directly estimates the motion flow from the blurred image through a fully-convolutional deep neural network (FCN) and recovers the unblurred image from the estimated motion flow. Our FCN is the first universal end-to-end mapping from the blurred image to the dense motion flow. To train the FCN, we simulate motion flows to generate synthetic blurred-image-motion-flow pairs thus avoiding the need for human labeling. Extensive experiments on challenging realistic blurred images demonstrate that the proposed method outperforms the state-of-the-art.",
            "year": 2016,
            "venue": "Computer Vision and Pattern Recognition",
            "authors": [
              {
                "authorId": "145542268",
                "name": "Dong Gong"
              },
              {
                "authorId": "1688428",
                "name": "Jie Yang"
              },
              {
                "authorId": "2161037",
                "name": "Lingqiao Liu"
              },
              {
                "authorId": "2047640322",
                "name": "Yanning Zhang"
              },
              {
                "authorId": "145950884",
                "name": "I. Reid"
              },
              {
                "authorId": "1780381",
                "name": "Chunhua Shen"
              },
              {
                "authorId": "5546141",
                "name": "A. Hengel"
              },
              {
                "authorId": "3177281",
                "name": "Javen Qinfeng Shi"
              }
            ]
          }
        },
        {
          "citedcorpusid": 20873334,
          "isinfluential": false,
          "contexts": [
            "This is one of the ﬁrst models that describes the behaviour of biological neurons [30]."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "A quantitative description of membrane current and its application to conduction and excitation in nerve",
            "abstract": "",
            "year": 1952,
            "venue": "Journal of Physiology",
            "authors": [
              {
                "authorId": "2251398256",
                "name": "Hodgkin Al"
              },
              {
                "authorId": "2251411021",
                "name": "Huxley Af"
              }
            ]
          }
        },
        {
          "citedcorpusid": 21317717,
          "isinfluential": false,
          "contexts": [
            "The non-linear dynamic range is fairly the same in all the presented research papers with a maximum value up to 143 dB [7]."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "A QVGA 143 dB Dynamic Range Frame-Free PWM Image Sensor With Lossless Pixel-Level Video Compression and Time-Domain CDS",
            "abstract": "",
            "year": 2011,
            "venue": "IEEE Journal of Solid-State Circuits",
            "authors": [
              {
                "authorId": "153466606",
                "name": "C. Posch"
              },
              {
                "authorId": "1758423",
                "name": "D. Matolin"
              },
              {
                "authorId": "2509695",
                "name": "R. Wohlgenannt"
              }
            ]
          }
        },
        {
          "citedcorpusid": 22158024,
          "isinfluential": true,
          "contexts": [
            "In another work, a belief propagation neural network, which is deﬁned in [54], was used and a mean depth error of 61.14–92%, with a maximum depth of 5 m was reported [55].",
            "Monocular depth estimation meth-ods [55,57] have shown good performance in outdoor scenes up to thirty meters."
          ],
          "intents": [
            "['methodology']",
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Event-Based Stereo Depth Estimation Using Belief Propagation",
            "abstract": "Compared to standard frame-based cameras, biologically-inspired event-based sensors capture visual information with low latency and minimal redundancy. These event-based sensors are also far less prone to motion blur than traditional cameras, and still operate effectively in high dynamic range scenes. However, classical framed-based algorithms are not typically suitable for these event-based data and new processing algorithms are required. This paper focuses on the problem of depth estimation from a stereo pair of event-based sensors. A fully event-based stereo depth estimation algorithm which relies on message passing is proposed. The algorithm not only considers the properties of a single event but also uses a Markov Random Field (MRF) to consider the constraints between the nearby events, such as disparity uniqueness and depth continuity. The method is tested on five different scenes and compared to other state-of-art event-based stereo matching methods. The results show that the method detects more stereo matches than other methods, with each match having a higher accuracy. The method can operate in an event-driven manner where depths are reported for individual events as they are received, or the network can be queried at any time to generate a sparse depth frame which represents the current state of the network.",
            "year": 2017,
            "venue": "Frontiers in Neuroscience",
            "authors": [
              {
                "authorId": "47661053",
                "name": "Zhen Xie"
              },
              {
                "authorId": "1788427",
                "name": "Shengyong Chen"
              },
              {
                "authorId": "33780923",
                "name": "G. Orchard"
              }
            ]
          }
        },
        {
          "citedcorpusid": 25268038,
          "isinfluential": false,
          "contexts": [
            "At the time of publication for this paper, a few neuromorphic processors have already been manufactured and well-utilized: TrueNorth [36], Loihi [37], SpiNNaker [38], and some others still in development [39]."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "The SpiNNaker Project",
            "abstract": "",
            "year": 2014,
            "venue": "Proceedings of the IEEE",
            "authors": [
              {
                "authorId": "144409615",
                "name": "S. Furber"
              },
              {
                "authorId": "3008126",
                "name": "F. Galluppi"
              },
              {
                "authorId": "143816983",
                "name": "S. Temple"
              },
              {
                "authorId": "3085921",
                "name": "L. Plana"
              }
            ]
          }
        },
        {
          "citedcorpusid": 26324573,
          "isinfluential": false,
          "contexts": [
            "The study in [47] was the ﬁrst (to the best of authors’ knowledge) to provide a 3D reconstruction of a scene based on 6-DOF camera tracking.",
            "Nevertheless, [47] trail-blazed monocular event-based depth perception and 3D reconstruction for the next generation of efforts in this area."
          ],
          "intents": [
            "['background']",
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Real-Time 3D Reconstruction and 6-DoF Tracking with an Event Camera",
            "abstract": "",
            "year": 2016,
            "venue": "European Conference on Computer Vision",
            "authors": [
              {
                "authorId": "3227772",
                "name": "Hanme Kim"
              },
              {
                "authorId": "2864731",
                "name": "Stefan Leutenegger"
              },
              {
                "authorId": "2052135690",
                "name": "A. Davison"
              }
            ]
          }
        },
        {
          "citedcorpusid": 30913835,
          "isinfluential": false,
          "contexts": [
            "One of the ﬁrst to solve the correspondence problem with event-based vision sensors was [42].",
            "The researchers in [42] used an embedded device with a DSP, which utilized a simple algo-rithm for stereo processing— camera calibration and rectiﬁcation, stereo correspondence calculation and reconstruction (disparity map)."
          ],
          "intents": [
            "['background']",
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Smartcam for real-time stereo vision - address-event based embedded system",
            "abstract": "We present a novel real-time stereo smart camera for sparse disparity (depth) map estimation of moving objects at up to 200 frames/sec. It is based on a 128x128 pixel asynchronous optical transient sensor, using address-event representation (AER) protocol. An address-event based algorithm for stereo depth calculation including calibration, correspondence and reconstruction processing steps is also presented. Due to the onchip data pre-processing the algorithm can be implemented on a single low-power digital signal processor.",
            "year": 2007,
            "venue": "International Conference on Computer Vision Theory and Applications",
            "authors": [
              {
                "authorId": "2521747",
                "name": "S. Schraml"
              },
              {
                "authorId": "1680865",
                "name": "Peter Schön"
              },
              {
                "authorId": "152832347",
                "name": "Nenad Milosevic"
              }
            ]
          }
        },
        {
          "citedcorpusid": 34855834,
          "isinfluential": false,
          "contexts": [
            "The correspondence problem with a neuromorphic computing platform, SpiNNaker, was ﬁrst solved by [40]."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Spiking Cooperative Stereo-Matching at 2 ms Latency with Neuromorphic Hardware",
            "abstract": "",
            "year": 2017,
            "venue": "Living Machines",
            "authors": [
              {
                "authorId": "46775745",
                "name": "G. Dikov"
              },
              {
                "authorId": "145885214",
                "name": "M. Firouzi"
              },
              {
                "authorId": "1685761",
                "name": "Florian Röhrbein"
              },
              {
                "authorId": "3302681",
                "name": "J. Conradt"
              },
              {
                "authorId": "2053647526",
                "name": "Christoph Richter"
              }
            ]
          }
        },
        {
          "citedcorpusid": 197634653,
          "isinfluential": true,
          "contexts": [
            "The latency between triggered events has dropped to less than 0.5 µ s [5,6]."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Live Demonstration: CeleX-V: A 1M Pixel Multi-Mode Event-Based Sensor",
            "abstract": "We demonstrate a new generation smart image sensor, CeleX-V. With 1280×800 pixels, 9.8um pitch, the sensor integrates several vision functions into one chip, such as full-array-parallel motion detection and on-chip optical flow extraction. CeleX-V is also capable of producing high-quality full-frame pictures and thus is compatible with traditional picture-based algorithms. The sensor supports both MIPI and parallel interface, with typical 400mW power consumption.",
            "year": 2019,
            "venue": "2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)",
            "authors": [
              {
                "authorId": "50358115",
                "name": "Shoushun Chen"
              },
              {
                "authorId": "2998931",
                "name": "M. Guo"
              }
            ]
          }
        },
        {
          "citedcorpusid": 203162947,
          "isinfluential": false,
          "contexts": [
            "However, this noise can be filtered out using background filtering techniques [32]."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Low Latency Event-Based Filtering and Feature Extraction for Dynamic Vision Sensors in Real-Time FPGA Applications",
            "abstract": "Dynamic Vision Sensor (DVS) pixels produce an asynchronous variable-rate address-event output that represents brightness changes at the pixel. Since these sensors produce frame-free output, they are ideal for real-time dynamic vision applications with real-time latency and power system constraints. Event-based filtering algorithms have been proposed to post-process the asynchronous event output to reduce sensor noise, extract low level features, and track objects, among others. These postprocessing algorithms help to increase the performance and accuracy of further processing for tasks such as classification using spike-based learning (ie. ConvNets), stereo vision, and visually-servoed robots, etc. This paper presents an FPGA-based library of these postprocessing event-based algorithms with implementation details; specifically background activity (noise) filtering, pixel masking, object motion detection and object tracking. The latencies of these filters on the Field Programmable Gate Array (FPGA) platform are below 300ns with an average latency reduction of 188% (maximum of 570%) over the software versions running on a desktop PC CPU. This open-source event-based filter IP library for FPGA has been tested on two different platforms and scenarios using different synthesis and implementation tools for Lattice and Xilinx vendors.",
            "year": 2019,
            "venue": "IEEE Access",
            "authors": [
              {
                "authorId": "1396468177",
                "name": "A. Linares-Barranco"
              },
              {
                "authorId": "1403824561",
                "name": "Fernando Perez-Peña"
              },
              {
                "authorId": "2837457",
                "name": "Diederik Paul Moeys"
              },
              {
                "authorId": "52355887",
                "name": "F. Gomez-Rodriguez"
              },
              {
                "authorId": "1398437010",
                "name": "G. Jiménez-Moreno"
              },
              {
                "authorId": "1704961",
                "name": "Shih-Chii Liu"
              },
              {
                "authorId": "5548576",
                "name": "T. Delbruck"
              }
            ]
          }
        },
        {
          "citedcorpusid": 206596513,
          "isinfluential": false,
          "contexts": [
            "The results from the MVSEC dataset are compared with frame-based state-of-the-art methods [61,62], and with another monocular event-based method [58]."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Unsupervised Monocular Depth Estimation with Left-Right Consistency",
            "abstract": "Learning based methods have shown very promising results for the task of depth estimation in single images. However, most existing approaches treat depth prediction as a supervised regression problem and as a result, require vast quantities of corresponding ground truth depth data for training. Just recording quality depth data in a range of environments is a challenging problem. In this paper, we innovate beyond existing approaches, replacing the use of explicit depth data during training with easier-to-obtain binocular stereo footage. We propose a novel training objective that enables our convolutional neural network to learn to perform single image depth estimation, despite the absence of ground truth depth data. Ex-ploiting epipolar geometry constraints, we generate disparity images by training our network with an image reconstruction loss. We show that solving for image reconstruction alone results in poor quality depth images. To overcome this problem, we propose a novel training loss that enforces consistency between the disparities produced relative to both the left and right images, leading to improved performance and robustness compared to existing approaches. Our method produces state of the art results for monocular depth estimation on the KITTI driving dataset, even outperforming supervised methods that have been trained with ground truth depth.",
            "year": 2016,
            "venue": "Computer Vision and Pattern Recognition",
            "authors": [
              {
                "authorId": "31082236",
                "name": "Clément Godard"
              },
              {
                "authorId": "2918822",
                "name": "Oisin Mac Aodha"
              },
              {
                "authorId": "3309893",
                "name": "G. Brostow"
              }
            ]
          }
        },
        {
          "citedcorpusid": 208098355,
          "isinfluential": false,
          "contexts": [
            "The main differences between SNNs and ANNs are well summarized in [29]:"
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Learning in Energy‐Efficient Neuromorphic Computing",
            "abstract": "This book focuses on how to build energy-efficient hardware for neural networks with learning capabilities—and provides co-design and co-optimization methodologies for building hardware neural networks that can learn. Presenting a complete picture from high-level algorithm to low-level implementation details, Learning in Energy-Efficient Neuromorphic Computing: Algorithm and Architecture CoDesign also covers many fundamentals and essentials in neural networks (e.g., deep learning), as well as hardware implementation of neural networks.",
            "year": 2019,
            "venue": "",
            "authors": [
              {
                "authorId": "48870767",
                "name": "Nan Zheng"
              },
              {
                "authorId": "1751607",
                "name": "P. Mazumder"
              }
            ]
          }
        },
        {
          "citedcorpusid": 220314130,
          "isinfluential": false,
          "contexts": [
            "Hence, it can be extensively used in safety-critical applications like those related to the automotive industry [33]."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Event-Based Neuromorphic Vision for Autonomous Driving: A Paradigm Shift for Bio-Inspired Visual Sensing and Perception",
            "abstract": "As a bio-inspired and emerging sensor, an event-based neuromorphic vision sensor has a different working principle compared to the standard frame-based cameras, which leads to promising properties of low energy consumption, low latency, high dynamic range (HDR), and high temporal resolution. It poses a paradigm shift to sense and perceive the environment by capturing local pixel-level light intensity changes and producing asynchronous event streams. Advanced technologies for the visual sensing system of autonomous vehicles from standard computer vision to event-based neuromorphic vision have been developed. In this tutorial-like article, a comprehensive review of the emerging technology is given. First, the course of the development of the neuromorphic vision sensor that is derived from the understanding of biological retina is introduced. The signal processing techniques for event noise processing and event data representation are then discussed. Next, the signal processing algorithms and applications for event-based neuromorphic vision in autonomous driving and various assistance systems are reviewed. Finally, challenges and future research directions are pointed out. It is expected that this article will serve as a starting point for new researchers and engineers in the autonomous driving field and provide a bird's-eye view to both neuromorphic vision and autonomous driving research communities.",
            "year": 2020,
            "venue": "IEEE Signal Processing Magazine",
            "authors": [
              {
                "authorId": "143930563",
                "name": "Guang Chen"
              },
              {
                "authorId": "40223253",
                "name": "Hu Cao"
              },
              {
                "authorId": "3302681",
                "name": "J. Conradt"
              },
              {
                "authorId": "3134548",
                "name": "Huajin Tang"
              },
              {
                "authorId": "1789856900",
                "name": "Florian Rohrbein"
              },
              {
                "authorId": "2075424317",
                "name": "Alois Knoll"
              }
            ]
          }
        },
        {
          "citedcorpusid": null,
          "isinfluential": false,
          "contexts": [
            "5 [24] 320 × 262 1000 >100 13 × 13 70 [8] 132 × 1024 10 × 10 0."
          ],
          "intents": [
            "--"
          ],
          "cited_paper_info": {}
        }
      ]
    },
    "255125395": {
      "citing_paper_info": {
        "title": "ESVIO: Event-Based Stereo Visual Inertial Odometry",
        "abstract": "Event cameras that asynchronously output low-latency event streams provide great opportunities for state estimation under challenging situations. Despite event-based visual odometry having been extensively studied in recent years, most of them are based on the monocular, while few research on stereo event vision. In this letter, we present ESVIO, the first event-based stereo visual-inertial odometry, which leverages the complementary advantages of event streams, standard images, and inertial measurements. Our proposed pipeline includes the ESIO (purely event-based) and ESVIO (event with image-aided), which achieves spatial and temporal associations between consecutive stereo event streams. A well-design back-end tightly-coupled fused the multi-sensor measurement to obtain robust state estimation. We validate that both ESIO and ESVIO have superior performance compared with other image-based and event-based baseline methods on public and self-collected datasets. Furthermore, we use our pipeline to perform onboard quadrotor flights under low-light environments. Autonomous driving data sequences and real-world large-scale experiments are also conducted to demonstrate long-term effectiveness. We highlight that this work is a real-time, accurate system that is aimed at robust state estimation under challenging environments.",
        "year": 2022,
        "venue": "IEEE Robotics and Automation Letters",
        "authors": [
          {
            "authorId": "120125963",
            "name": "Pei-Ying Chen"
          },
          {
            "authorId": "31327386",
            "name": "W. Guan"
          },
          {
            "authorId": "2069299246",
            "name": "P. Lu"
          }
        ]
      },
      "citation_summary": {
        "citation_count": 13,
        "unique_cited_count": 11,
        "influential_count": 5,
        "detailed_records_count": 13
      },
      "cited_papers": [
        "232170230",
        "3738244",
        "2121536",
        "9729856",
        "3416874",
        "220713377",
        "118684904",
        "250127779",
        "249980412",
        "50775406",
        "235794981"
      ],
      "citation_details": [
        {
          "citedcorpusid": 2121536,
          "isinfluential": false,
          "contexts": [
            "For the new arrival event streams, the existing event-corner features are firstly temporally tracked by the LK optical approach [20] and then spatially matched in left and right event streams."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "An Iterative Image Registration Technique with an Application to Stereo Vision",
            "abstract": "Image registration finds a variety of applications in computer vision. Unfortunately, traditional image registration techniques tend to be costly. We present a new image registration technique that makes use of the spatial intensity gradient of the images to find a good match using a type of Newton-Raphson iteration. Our technique is taster because it examines far fewer potential matches between the images than existing techniques Furthermore, this registration technique can be generalized to handle rotation, scaling and shearing. We show how our technique can be adapted tor use in a stereo vision system.",
            "year": 1981,
            "venue": "International Joint Conference on Artificial Intelligence",
            "authors": [
              {
                "authorId": "40588702",
                "name": "B. D. Lucas"
              },
              {
                "authorId": "1733113",
                "name": "T. Kanade"
              }
            ]
          }
        },
        {
          "citedcorpusid": 3416874,
          "isinfluential": true,
          "contexts": [
            "What’s more, as can be seen from the video record of the evaluation using our ESVIO (take the school-scooter 3 in Vector and indoor flying 1 4 in MVSEC [7] as examples).",
            "In Section IV-B, we compare our methods with other event-based and image-based methods on two publicly available datasets: MVSEC [23] and VECtor [24].",
            "In subsection IV-B, we compare our methods with other event-based and image-based methods on two publicly available datasets: MVSEC [23] and VECtor [24].",
            "For the MVSEC [23], we select the sequence captured in the indoor flying room."
          ],
          "intents": [
            "--",
            "['methodology']",
            "--",
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "The Multivehicle Stereo Event Camera Dataset: An Event Camera Dataset for 3D Perception",
            "abstract": "Event-based cameras are a new passive sensing modality with a number of benefits over traditional cameras, including extremely low latency, asynchronous data acquisition, high dynamic range, and very low power consumption. There has been a lot of recent interest and development in applying algorithms to use the events to perform a variety of three-dimensional perception tasks, such as feature tracking, visual odometry, and stereo depth estimation. However, there currently lacks the wealth of labeled data that exists for traditional cameras to be used for both testing and development. In this letter, we present a large dataset with a synchronized stereo pair event based camera system, carried on a handheld rig, flown by a hexacopter, driven on top of a car, and mounted on a motorcycle, in a variety of different illumination levels and environments. From each camera, we provide the event stream, grayscale images, and inertial measurement unit (IMU) readings. In addition, we utilize a combination of IMU, a rigidly mounted lidar system, indoor and outdoor motion capture, and GPS to provide accurate pose and depth images for each camera at up to 100 Hz. For comparison, we also provide synchronized grayscale images and IMU readings from a frame-based stereo camera system.",
            "year": 2018,
            "venue": "IEEE Robotics and Automation Letters",
            "authors": [
              {
                "authorId": "3385588",
                "name": "A. Z. Zhu"
              },
              {
                "authorId": "144964367",
                "name": "Dinesh Thakur"
              },
              {
                "authorId": "2520604",
                "name": "Tolga Özaslan"
              },
              {
                "authorId": "39832696",
                "name": "Bernd Pfrommer"
              },
              {
                "authorId": "37956314",
                "name": "Vijay R. Kumar"
              },
              {
                "authorId": "1751586",
                "name": "Kostas Daniilidis"
              }
            ]
          }
        },
        {
          "citedcorpusid": 3738244,
          "isinfluential": true,
          "contexts": [
            "Available: https://github.com/MichaelGrupp /evo [26] C. Campos, R. Elvira, J. J. G. Rodríguez, J. M. Montiel, and J. D. Tars, “ORB-SLAM3: An accurate open-source library for visual, visual– inertial, and multimap SLAM,” IEEE Trans.",
            "While the traditional event-based methods [2], [3], [5] failed in most of the sequences in these two datasets.",
            "Index Terms—Visual-Inertial SLAM, sensor fusion, aerial systems: perception and autonomy.",
            "Available: https://github.\ncom/arclab-hku/Event_based_VO-VIO-SLAM/tree/main/ES VIO/supply.",
            "[3] A. R. Vidal, H. Rebecq, T. Horstschaefer, and D. Scaramuzza, “Ultimate SLAM? combining events, images, and IMU for robust visual SLAM in HDR and high-speed scenarios,” IEEE Robot.",
            "[24] L. Gao et al., “VECtor: A versatile event-centric benchmark for multisensor SLAM,” IEEE Robot.",
            "Due to motion blur, both ORB-SLAM3 and VINS-Fusion fail to extract reliable features in hku_agg_walk sequence, resulting in system failure.",
            "Most of the existing event-based visual odometers (VO) use monocular event camera [2], [3], [4], while few research on visual odometry based on stereo event cameras [5], [6].",
            "As for the MRE evaluation criterion, our ESVIO shows significant improvement compared to other advanced algorithms, e.g. the average MRE of ESVIO is 0.033◦/m while the value of ORB-SLAM3 is 0.12◦/m.",
            "In the hku_dark_normal sequence, ORB-SLAM3 cannot extract any feature due to poor light conditions (more qualitative details is in the supplementary material).",
            "[25] M. Grupp, “EVO: Python package for the evaluation of odometry and SLAM,” 2017.",
            "Available: https://github.com/arclabhku/Event_based_VO-VIO-SLAM.",
            "Although the MPE criterion of ORB-SLAM3 is slightly better than ours in some sequences (e.g. robot-normal, desk-normal, mountain-normal), our ESVIO provides more reliable and accurate results in most of the sequences under harsh situations with HDR or aggressive motion.",
            "However, most of the recent research on stereo event cameras has focused on depth estimation and constructing semi-dense or dense maps [16], [17], with less research on VO/SLAM fields.",
            "Our ESIO has good performance, especially for the sequence hku_agg_walk and hku_dark_normal, our ESIO still can produce reliable and accurate pose estimation even when the state-of-the-art image-based VIO method, ORB-SLAM3, fails.",
            "Observing this complementarity, leveraging both of the advantages of the aforementioned different sensors in combination with an inertial measurement unit (IMU) results in a robust and accurate visual-inertial odometry (VIO) pipeline [3], [4].",
            "Ultimate SLAM [3] furthered the aforementioned research by combining event streams, image frames, and IMU measurements with nonlinear optimization, which leverages the complementary advantages of event cameras and standard cameras."
          ],
          "intents": [
            "--",
            "['methodology']",
            "--",
            "--",
            "--",
            "--",
            "--",
            "['background']",
            "--",
            "--",
            "--",
            "--",
            "--",
            "--",
            "--",
            "['background']",
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Ultimate SLAM? Combining Events, Images, and IMU for Robust Visual SLAM in HDR and High-Speed Scenarios",
            "abstract": "Event cameras are bioinspired vision sensors that output pixel-level brightness changes instead of standard intensity frames. These cameras do not suffer from motion blur and have a very high dynamic range, which enables them to provide reliable visual information during high-speed motions or in scenes characterized by high dynamic range. However, event cameras output only little information when the amount of motion is limited, such as in the case of almost still motion. Conversely, standard cameras provide instant and rich information about the environment most of the time (in low-speed and good lighting scenarios), but they fail severely in case of fast motions, or difficult lighting such as high dynamic range or low light scenes. In this letter, we present the first state estimation pipeline that leverages the complementary advantages of these two sensors by fusing in a tightly coupled manner events, standard frames, and inertial measurements. We show on the publicly available Event Camera Dataset that our hybrid pipeline leads to an accuracy improvement of 130% over event-only pipelines, and 85% over standard-frames-only visual-inertial systems, while still being computationally tractable. Furthermore, we use our pipeline to demonstrate—to the best of our knowledge—the first autonomous quadrotor flight using an event camera for state estimation, unlocking flight scenarios that were not reachable with traditional visual-inertial odometry, such as low-light environments and high dynamic range scenes. Videos of the experiments: http://rpg.ifi.uzh.ch/ultimateslam.html",
            "year": 2017,
            "venue": "IEEE Robotics and Automation Letters",
            "authors": [
              {
                "authorId": "26430800",
                "name": "Antoni Rosinol Vidal"
              },
              {
                "authorId": "3414274",
                "name": "Henri Rebecq"
              },
              {
                "authorId": "9676873",
                "name": "Timo Horstschaefer"
              },
              {
                "authorId": "2075371",
                "name": "D. Scaramuzza"
              }
            ]
          }
        },
        {
          "citedcorpusid": 9729856,
          "isinfluential": false,
          "contexts": [
            "[11] adopted a continuous-time framework based on cubic spline for smooth trajectory estimation and fused both event streams and IMU together."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Continuous-Time Visual-Inertial Odometry for Event Cameras",
            "abstract": "Event cameras are bioinspired vision sensors that output pixel-level brightness changes instead of standard intensity frames. They offer significant advantages over standard cameras, namely a very high dynamic range, no motion blur, and a latency in the order of microseconds. However, due to the fundamentally different structure of the sensor's output, new algorithms that exploit the high temporal resolution and the asynchronous nature of the sensor are required. Recent work has shown that a continuous-time representation of the event camera pose can deal with the high temporal resolution and asynchronous nature of this sensor in a principled way. In this paper, we leverage such a continuous-time representation to perform visual-inertial odometry with an event camera. This representation allows direct integration of the asynchronous events with microsecond accuracy and the inertial measurements at high frequency. The event camera trajectory is approximated by a smooth curve in the space of rigid-body motions using cubic splines. This formulation significantly reduces the number of variables in trajectory estimation problems. We evaluate our method on real data from several scenes and compare the results against ground truth from a motion-capture system. We show that our method provides improved accuracy over the result of a state-of-the-art visual odometry method for event cameras. We also show that both the map orientation and scale can be recovered accurately by fusing events and inertial data. To the best of our knowledge, this is the first work on visual-inertial fusion with event cameras using a continuous-time framework.",
            "year": 2017,
            "venue": "IEEE Transactions on robotics",
            "authors": [
              {
                "authorId": "144578041",
                "name": "Elias Mueggler"
              },
              {
                "authorId": "144036711",
                "name": "Guillermo Gallego"
              },
              {
                "authorId": "3414274",
                "name": "Henri Rebecq"
              },
              {
                "authorId": "2075371",
                "name": "D. Scaramuzza"
              }
            ]
          }
        },
        {
          "citedcorpusid": 50775406,
          "isinfluential": false,
          "contexts": [
            "[14] D. Gehrig, H. Rebecq, G. Gallego, and D. Scaramuzza, “EKLT: Asynchronous photometric feature tracking using events and frames,” Int.",
            "EKLT-VIO [13] integrated an accurate state-of-the-art event-based feature tracker EKLT [14] with EKF backend to achieve event-based state estimation on Mars-like datasets."
          ],
          "intents": [
            "--",
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "EKLT: Asynchronous Photometric Feature Tracking Using Events and Frames",
            "abstract": "We present EKLT, a feature tracking method that leverages the complementarity of event cameras and standard cameras to track visual features with high temporal resolution. Event cameras are novel sensors that output pixel-level brightness changes, called “events”. They offer significant advantages over standard cameras, namely a very high dynamic range, no motion blur, and a latency in the order of microseconds. However, because the same scene pattern can produce different events depending on the motion direction, establishing event correspondences across time is challenging. By contrast, standard cameras provide intensity measurements (frames) that do not depend on motion direction. Our method extracts features on frames and subsequently tracks them asynchronously using events, thereby exploiting the best of both types of data: the frames provide a photometric representation that does not depend on motion direction and the events provide updates with high temporal resolution. In contrast to previous works, which are based on heuristics, this is the first principled method that uses intensity measurements directly, based on a generative event model within a maximum-likelihood framework. As a result, our method produces feature tracks that are more accurate than the state of the art, across a wide variety of scenes.",
            "year": 2018,
            "venue": "International Journal of Computer Vision",
            "authors": [
              {
                "authorId": "51152279",
                "name": "Daniel Gehrig"
              },
              {
                "authorId": "3414274",
                "name": "Henri Rebecq"
              },
              {
                "authorId": "144036711",
                "name": "Guillermo Gallego"
              },
              {
                "authorId": "2075371",
                "name": "D. Scaramuzza"
              }
            ]
          }
        },
        {
          "citedcorpusid": 118684904,
          "isinfluential": false,
          "contexts": [
            "EVENT cameras are novel bio-inspired sensors [1], which have a high dynamic range (140 dB compared to 60 dB of standard cameras) to handle broad illumination conditions."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Event-Based Vision: A Survey",
            "abstract": "Event cameras are bio-inspired sensors that differ from conventional frame cameras: Instead of capturing images at a fixed rate, they asynchronously measure per-pixel brightness changes, and output a stream of events that encode the time, location and sign of the brightness changes. Event cameras offer attractive properties compared to traditional cameras: high temporal resolution (in the order of $\\mu$μs), very high dynamic range (140 dB versus 60 dB), low power consumption, and high pixel bandwidth (on the order of kHz) resulting in reduced motion blur. Hence, event cameras have a large potential for robotics and computer vision in challenging scenarios for traditional cameras, such as low-latency, high speed, and high dynamic range. However, novel methods are required to process the unconventional output of these sensors in order to unlock their potential. This paper provides a comprehensive overview of the emerging field of event-based vision, with a focus on the applications and the algorithms developed to unlock the outstanding properties of event cameras. We present event cameras from their working principle, the actual sensors that are available and the tasks that they have been used for, from low-level vision (feature detection and tracking, optic flow, etc.) to high-level vision (reconstruction, segmentation, recognition). We also discuss the techniques developed to process events, including learning-based techniques, as well as specialized processors for these novel sensors, such as spiking neural networks. Additionally, we highlight the challenges that remain to be tackled and the opportunities that lie ahead in the search for a more efficient, bio-inspired way for machines to perceive and interact with the world.",
            "year": 2019,
            "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
            "authors": [
              {
                "authorId": "144036711",
                "name": "Guillermo Gallego"
              },
              {
                "authorId": "1694635",
                "name": "T. Delbrück"
              },
              {
                "authorId": "33780923",
                "name": "G. Orchard"
              },
              {
                "authorId": "1897771",
                "name": "C. Bartolozzi"
              },
              {
                "authorId": "1736425",
                "name": "B. Taba"
              },
              {
                "authorId": "1860631",
                "name": "A. Censi"
              },
              {
                "authorId": "2864731",
                "name": "Stefan Leutenegger"
              },
              {
                "authorId": "2052135690",
                "name": "A. Davison"
              },
              {
                "authorId": "3302681",
                "name": "J. Conradt"
              },
              {
                "authorId": "1751586",
                "name": "Kostas Daniilidis"
              },
              {
                "authorId": "2075371",
                "name": "D. Scaramuzza"
              }
            ]
          }
        },
        {
          "citedcorpusid": 220713377,
          "isinfluential": false,
          "contexts": [
            "[10] obtains a discrete number of states based on a spatio-temporal window of event streams, and introduces virtual event frames to achieve nonlinear optimization that refines estimated poses.",
            "Sequence ORB-SLAM3 [26] Stereo VIO VINS-Fusion [22] Stereo VIO USLAM [10] Mono EIO USLAM [3] Mono EVIO PL-EVIO [4] Mono EVIO Our ESIO Stereo EIO Our ESIO+ Stereo EIO Our ESVIO Stereo EVIO"
          ],
          "intents": [
            "['background']",
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "ORB-SLAM3: An Accurate Open-Source Library for Visual, Visual–Inertial, and Multimap SLAM",
            "abstract": "This article presents ORB-SLAM3, the first system able to perform visual, visual-inertial and multimap SLAM with monocular, stereo and RGB-D cameras, using pin-hole and fisheye lens models. The first main novelty is a tightly integrated visual-inertial SLAM system that fully relies on maximum a posteriori (MAP) estimation, even during IMU initialization, resulting in real-time robust operation in small and large, indoor and outdoor environments, being two to ten times more accurate than previous approaches. The second main novelty is a multiple map system relying on a new place recognition method with improved recall that lets ORB-SLAM3 survive to long periods of poor visual information: when it gets lost, it starts a new map that will be seamlessly merged with previous maps when revisiting them. Compared with visual odometry systems that only use information from the last few seconds, ORB-SLAM3 is the first system able to reuse in all the algorithm stages all previous information from high parallax co-visible keyframes, even if they are widely separated in time or come from previous mapping sessions, boosting accuracy. Our experiments show that, in all sensor configurations, ORB-SLAM3 is as robust as the best systems available in the literature and significantly more accurate. Notably, our stereo-inertial SLAM achieves an average accuracy of 3.5 cm in the EuRoC drone and 9 mm under quick hand-held motions in the room of TUM-VI dataset, representative of AR/VR scenarios. For the benefit of the community we make public the source code.",
            "year": 2020,
            "venue": "IEEE Transactions on robotics",
            "authors": [
              {
                "authorId": "2057451667",
                "name": "C. Campos"
              },
              {
                "authorId": "1388060898",
                "name": "Richard Elvira"
              },
              {
                "authorId": "2116899285",
                "name": "J. Rodr'iguez"
              },
              {
                "authorId": "2142960324",
                "name": "José M. M. Montiel"
              },
              {
                "authorId": "2142960326",
                "name": "Juan D. Tardós"
              }
            ]
          }
        },
        {
          "citedcorpusid": 232170230,
          "isinfluential": true,
          "contexts": [
            "[28] M. Gehrig, W. Aarents, D. Gehrig, and D. Scaramuzza, “DSEC: A stereo event camera dataset for driving scenarios,” IEEE Robot.",
            "1) DSEC Dataset: DSEC [28] is collected by high-resolution stereo event cameras (640 × 480) under driving scenarios, which is challenging for event-based sensors, as forward motions typically produce considerably fewer events at the center.",
            "Since the DSEC dataset does not provide the ground truth 6-DoF poses, we only show the estimated trajectory and the tracking performance of our event-based and image-based features."
          ],
          "intents": [
            "--",
            "['background']",
            "--"
          ],
          "cited_paper_info": {
            "title": "DSEC: A Stereo Event Camera Dataset for Driving Scenarios",
            "abstract": "Once an academic venture, autonomous driving has received unparalleled corporate funding in the last decade. Still, operating conditions of current autonomous cars are mostly restricted to ideal scenarios. This means that driving in challenging illumination conditions such as night, sunrise, and sunset remains an open problem. In these cases, standard cameras are being pushed to their limits in terms of low light and high dynamic range performance. To address these challenges, we propose, DSEC, a new dataset that contains such demanding illumination conditions and provides a rich set of sensory data. DSEC offers data from a wide-baseline stereo setup of two color frame cameras and two high-resolution monochrome event cameras. In addition, we collect lidar data and RTK GPS measurements, both hardware synchronized with all camera data. One of the distinctive features of this dataset is the inclusion of high-resolution event cameras. Event cameras have received increasing attention for their high temporal resolution and high dynamic range performance. However, due to their novelty, event camera datasets in driving scenarios are rare. This work presents the first high resolution, large scale stereo dataset with event cameras. The dataset contains 53 sequences collected by driving in a variety of illumination conditions and provides ground truth disparity for the development and evaluation of event-based stereo algorithms.",
            "year": 2021,
            "venue": "IEEE Robotics and Automation Letters",
            "authors": [
              {
                "authorId": "8329387",
                "name": "Mathias Gehrig"
              },
              {
                "authorId": "2052356146",
                "name": "Willem Aarents"
              },
              {
                "authorId": "51152279",
                "name": "Daniel Gehrig"
              },
              {
                "authorId": "2075371",
                "name": "D. Scaramuzza"
              }
            ]
          }
        },
        {
          "citedcorpusid": 235794981,
          "isinfluential": false,
          "contexts": [
            "[6] adopted stereo feature detection and matching with the geometry method, which adopts reprojection error minimization to achieve pose estimation.",
            "Most of the existing event-based visual odometers (VO) use monocular event camera [2], [3], [4], while few research on visual odometry based on stereo event cameras [5], [6]."
          ],
          "intents": [
            "['methodology']",
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Feature-based Event Stereo Visual Odometry",
            "abstract": "Event-based cameras are biologically inspired sensors that output events, i.e., asynchronous pixel-wise brightness changes in the scene. Their high dynamic range and temporal resolution of a microsecond makes them more reliable than standard cameras in environments of challenging illumination and in high-speed scenarios, thus developing odometry algorithms based solely on event cameras offers exciting new possibilities for autonomous systems and robots. In this paper, we propose a novel stereo visual odometry method for event cameras based on feature detection and matching with careful feature management, while pose estimation is done by feature reprojection error minimization. We evaluate the performance of the proposed method on two publicly available datasets: MVSEC sequences captured by an indoor flying drone and DSEC outdoor driving sequences. MVSEC offers accurate ground truth from motion capture, while for DSEC, which does not offer ground truth, in order to obtain a reference trajectory on the standard camera frames we used our SOFT visual odometry, one of the highest ranking algorithms on the KITTI scoreboards. We compared our method to the ESVO method, which is the first and still the only stereo event odometry method, showing on par performance on both MVSEC and DSEC sequences. Furthermore, two important advantages of our method over ESVO are that it adapts tracking frequency to the asynchronous event rate and does not require initialization.",
            "year": 2021,
            "venue": "European Conference on Mobile Robots",
            "authors": [
              {
                "authorId": "51450968",
                "name": "Antea Hadviger"
              },
              {
                "authorId": "2720665",
                "name": "Igor Cvisic"
              },
              {
                "authorId": "2053721089",
                "name": "Ivan Markovi'c"
              },
              {
                "authorId": "3237756",
                "name": "Sacha Vrazic"
              },
              {
                "authorId": "2064601815",
                "name": "Ivan Petrovi'c"
              }
            ]
          }
        },
        {
          "citedcorpusid": 249980412,
          "isinfluential": false,
          "contexts": [
            "However, most of the recent research on stereo event cameras has focused on depth estimation and constructing semi-dense or dense maps [16], [17], with less research on VO/SLAM fields."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Stereo Depth from Events Cameras: Concentrate and Focus on the Future",
            "abstract": "Neuromorphic cameras or event cameras mimic human vision by reporting changes in the intensity in a scene, instead of reporting the whole scene at once in a form of an image frame as performed by conventional cameras. Events are streamed data that are often dense when either the scene changes or the camera moves rapidly. The rapid movement causes the events to be overridden or missed when creating a tensor for the machine to learn on. To alleviate the event missing or overriding issue, we propose to learn to concentrate on the dense events to produce a compact event representation with high details for depth estimation. Specifically, we learn a model with events from both past and future but infer only with past data with the predicted future. We initially estimate depth in an event-only setting but also propose to further incorporate images and events by a hier-archical event and intensity combination network for better depth estimation. By experiments in challenging real-world scenarios, we validate that our method outperforms prior arts even with low computational cost. Code is available at: https://github.com/yonseivnl/se-cff.",
            "year": 2022,
            "venue": "Computer Vision and Pattern Recognition",
            "authors": [
              {
                "authorId": "1830605424",
                "name": "Yeongwoo Nam"
              },
              {
                "authorId": "114141661",
                "name": "Mohammad Mostafavi"
              },
              {
                "authorId": "51182421",
                "name": "Kuk-Jin Yoon"
              },
              {
                "authorId": "2119579051",
                "name": "Jonghyun Choi"
              }
            ]
          }
        },
        {
          "citedcorpusid": 250127779,
          "isinfluential": true,
          "contexts": [
            "In Section IV-B, we compare our methods with other event-based and image-based methods on two publicly available datasets: MVSEC [23] and VECtor [24].",
            "The VECtor [24] dataset consists of a hardwaresynchronized sensor suite that includes stereo event cameras, stereo standard cameras, an RGB-D sensor, a LiDAR, and an IMU.",
            "In subsection IV-B, we compare our methods with other event-based and image-based methods on two publicly available datasets: MVSEC [23] and VECtor [24].",
            "The VECtor [24] dataset consists of a hardware-synchronized sensor suite that includes stereo event cameras, stereo standard cameras, an RGB-D sensor, a LiDAR, and an IMU."
          ],
          "intents": [
            "['methodology']",
            "['methodology']",
            "--",
            "--"
          ],
          "cited_paper_info": {
            "title": "VECtor: A Versatile Event-Centric Benchmark for Multi-Sensor SLAM",
            "abstract": "Event cameras have recently gained in popularity as they hold strong potential to complement regular cameras in situations of high dynamics or challenging illumination. An important problem that may benefit from the addition of an event camera is given by Simultaneous Localization And Mapping (SLAM). However, in order to ensure progress on event-inclusive multi-sensor SLAM, novel benchmark sequences are needed. Our contribution is the first complete set of benchmark datasets captured with a multi-sensor setup containing an event-based stereo camera, a regular stereo camera, multiple depth sensors, and an inertial measurement unit. The setup is fully hardware-synchronized and underwent accurate extrinsic calibration. All sequences come with ground truth data captured by highly accurate external reference devices such as a motion capture system. Individual sequences include both small and large-scale environments, and cover the specific challenges targeted by dynamic vision sensors.",
            "year": 2022,
            "venue": "IEEE Robotics and Automation Letters",
            "authors": [
              {
                "authorId": "2148991481",
                "name": "Ling Gao"
              },
              {
                "authorId": "72322304",
                "name": "Y. Liang"
              },
              {
                "authorId": "1423718086",
                "name": "Jiaqi Yang"
              },
              {
                "authorId": "2174101946",
                "name": "Shaoxun Wu"
              },
              {
                "authorId": "2109502285",
                "name": "Chenyu Wang"
              },
              {
                "authorId": "2120262069",
                "name": "Jiaben Chen"
              },
              {
                "authorId": "1727013",
                "name": "L. Kneip"
              }
            ]
          }
        },
        {
          "citedcorpusid": null,
          "isinfluential": false,
          "contexts": [
            "The accuracy is measured with mean position error (MPE, %) and mean rotation error (MRE, ◦/m) aligning the estimated trajectory with ground truth using 6-DOF transformation (in SE3), which is calculated by the tool [25]."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {}
        },
        {
          "citedcorpusid": null,
          "isinfluential": true,
          "contexts": [
            "While the traditional event-based methods [2] [3] [5] failed in most of the sequences in these two datasets.",
            "Note that we also evaluate EVO [2] and ESVO [5] in our self-collected datasets, but they failed in all sequences.",
            "EVO [2] proposed a monocular event-based parallel tracking-and-mapping philosophy which applies the image-to-model alignment for tracking and Event-based Multi-View Stereo (EMVS) [8] for mapping.",
            "Besides, we emphasize real-time performance when evaluating our methods, while the computational burden of EVO [2] and ESVO [3] is so large that we had to slow down the rosbag such as × 0 .",
            "Most of the existing event-based visual odometers (VO) use monocular event camera [2]–[4], while few research on visual odometry based on stereo event cameras [5] [6].",
            "However, the generalization capability of [2] and [5] is slightly poor."
          ],
          "intents": [
            "['methodology']",
            "['methodology']",
            "['background']",
            "['methodology']",
            "['background']",
            "['background']"
          ],
          "cited_paper_info": {}
        }
      ]
    },
    "254531210": {
      "citing_paper_info": {
        "title": "Spiking Cooperative Network Implemented on FPGA for Real-Time Event-Based Stereo System",
        "abstract": "A hardware-efficient implementation of a spiking cooperative network (SCN) for a real-time event-based stereo correspondence system is presented. While fully utilizing the advantage of event data, the proposed SCN design significantly reduces the amount of hardware resources by utilizing distinct properties of the SCN, such as the repeatability of synaptic connections and operations, through physical constraints. A stereo system consisting of a field-programmable gate array (FPGA) and a pair of dynamic vision sensors (DVSs) is implemented to demonstrate the SCN design. Stereo livestreamed event data are generated from the DVSs, and the SCN is implemented on an FPGA chip to process the event data. The SCN system has four cores, each comprising an array of 32 Coincidence-Disparity units that calculate the 32-level disparity in a semi-parallel manner. The system performance was evaluated experimentally to estimate the depth of objects moving at different speeds. A rotating drum with a diameter of 8 cm was used in the test. The median relative error of the estimated depth at a rotation speed of 16.7 Hz ranged from 7.3% to 10.6%.",
        "year": 2022,
        "venue": "IEEE Access",
        "authors": [
          {
            "authorId": "2109982138",
            "name": "Jung-Gyun Kim"
          },
          {
            "authorId": "49077100",
            "name": "Donghwan Seo"
          },
          {
            "authorId": "40654693",
            "name": "Byung-geun Lee"
          }
        ]
      },
      "citation_summary": {
        "citation_count": 8,
        "unique_cited_count": 8,
        "influential_count": 2,
        "detailed_records_count": 8
      },
      "cited_papers": [
        "232152677",
        "1680724",
        "30913835",
        "11395394",
        "458430",
        "4833834",
        "205698386",
        "8688550"
      ],
      "citation_details": [
        {
          "citedcorpusid": 458430,
          "isinfluential": false,
          "contexts": [
            "The ability to process the stereo correspondence problem in real time, which solves the input data without storing it for later processing, is crucial in machine-vision systems such as robots and autonomous vehicles [1].",
            "The disparity maps were computed using frame-based approaches [1], [9]."
          ],
          "intents": [
            "['background']",
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "A fast stereo matching algorithm suitable for embedded real-time systems",
            "abstract": "",
            "year": 2010,
            "venue": "Computer Vision and Image Understanding",
            "authors": [
              {
                "authorId": "1721721",
                "name": "M. Humenberger"
              },
              {
                "authorId": "144904089",
                "name": "C. Zinner"
              },
              {
                "authorId": "2110622261",
                "name": "Michael Weber"
              },
              {
                "authorId": "1728811",
                "name": "W. Kubinger"
              },
              {
                "authorId": "1742533",
                "name": "M. Vincze"
              }
            ]
          }
        },
        {
          "citedcorpusid": 1680724,
          "isinfluential": false,
          "contexts": [
            "The disparity maps were computed using frame-based approaches [1], [9]."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Fast cost-volume filtering for visual correspondence and beyond",
            "abstract": "",
            "year": 2011,
            "venue": "Computer Vision and Pattern Recognition",
            "authors": [
              {
                "authorId": "2086328",
                "name": "Christoph Rhemann"
              },
              {
                "authorId": "3115485",
                "name": "A. Hosni"
              },
              {
                "authorId": "2873656",
                "name": "M. Bleyer"
              },
              {
                "authorId": "1756036",
                "name": "C. Rother"
              },
              {
                "authorId": "1990797",
                "name": "M. Gelautz"
              }
            ]
          }
        },
        {
          "citedcorpusid": 4833834,
          "isinfluential": false,
          "contexts": [
            "In [14], a time-correlated kernel was used for event-driven operations, but this method requires recalculating the synaptic weights for each event input, which leads to excessive computation."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Asynchronous Event-based Cooperative Stereo Matching Using Neuromorphic Silicon Retinas",
            "abstract": "Biologically-inspired event-driven silicon retinas, so called dynamic vision sensors (DVS), allow efficient solutions for various visual perception tasks, e.g. surveillance, tracking, or motion detection. Similar to retinal photoreceptors, any perceived light intensity change in the DVS generates an event at the corresponding pixel. The DVS thereby emits a stream of spatiotemporal events to encode visually perceived objects that in contrast to conventional frame-based cameras, is largely free of redundant background information. The DVS offers multiple additional advantages, but requires the development of radically new asynchronous, event-based information processing algorithms. In this paper we present a fully event-based disparity matching algorithm for reliable 3D depth perception using a dynamic cooperative neural network. The interaction between cooperative cells applies cross-disparity uniqueness-constraints and within-disparity continuity-constraints, to asynchronously extract disparity for each new event, without any need of buffering individual events. We have investigated the algorithm’s performance in several experiments; our results demonstrate smooth disparity maps computed in a purely event-based manner, even in the scenes with temporally-overlapping stimuli.",
            "year": 2016,
            "venue": "Neural Processing Letters",
            "authors": [
              {
                "authorId": "145885214",
                "name": "M. Firouzi"
              },
              {
                "authorId": "3302681",
                "name": "J. Conradt"
              }
            ]
          }
        },
        {
          "citedcorpusid": 8688550,
          "isinfluential": false,
          "contexts": [
            "In frame-based stereo systems, most stereo correspondence algorithms ﬁnd a matching point by comparing the similarity between the visual features of the left and right frame images and calculate the distance along the camera geometry through the disparity between the two matching points [2]."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Review of stereo vision algorithms and their suitability for resource-limited systems",
            "abstract": "",
            "year": 2012,
            "venue": "Journal of Real-Time Image Processing",
            "authors": [
              {
                "authorId": "2473595",
                "name": "Beau J. Tippetts"
              },
              {
                "authorId": "144131326",
                "name": "Dah-Jye Lee"
              },
              {
                "authorId": "2437538",
                "name": "Kirt D. Lillywhite"
              },
              {
                "authorId": "2448126",
                "name": "J. Archibald"
              }
            ]
          }
        },
        {
          "citedcorpusid": 11395394,
          "isinfluential": false,
          "contexts": [
            "A similar idea was adopted for event-based stereo systems in previous studies [3], [4], [5], [6].",
            "The methods reported in [5] and [6] do not generate frame images to compute the matching cost but compare each event with the collected events of another DVS pair within a speciﬁed spatiotemporal window.",
            "Thus, stereo systems utilizing DVSs have recently gained popularity and have been reported in the literature [3], [4], [5], [6], [7], [8]."
          ],
          "intents": [
            "['methodology']",
            "['methodology']",
            "--"
          ],
          "cited_paper_info": {
            "title": "A High-Performance Hardware Architecture for a Frameless Stereo Vision Algorithm Implemented on a FPGA Platform",
            "abstract": "",
            "year": 2014,
            "venue": "2014 IEEE Conference on Computer Vision and Pattern Recognition Workshops",
            "authors": [
              {
                "authorId": "2435053",
                "name": "F. Eibensteiner"
              },
              {
                "authorId": "1824241",
                "name": "J. Kogler"
              },
              {
                "authorId": "2662311",
                "name": "J. Scharinger"
              }
            ]
          }
        },
        {
          "citedcorpusid": 30913835,
          "isinfluential": true,
          "contexts": [
            "A similar idea was adopted for event-based stereo systems in previous studies [3], [4], [5], [6].",
            "Compared with other ASIC approaches [3], [17], our FPGA-based implementation achieved considerable accuracy and power performance with lesser hardware.",
            "In [3], grayscale images were generated by collecting spike events over 20 ms.",
            "Thus, stereo systems utilizing DVSs have recently gained popularity and have been reported in the literature [3], [4], [5], [6], [7], [8]."
          ],
          "intents": [
            "['methodology']",
            "['result']",
            "['methodology']",
            "--"
          ],
          "cited_paper_info": {
            "title": "Smartcam for real-time stereo vision - address-event based embedded system",
            "abstract": "We present a novel real-time stereo smart camera for sparse disparity (depth) map estimation of moving objects at up to 200 frames/sec. It is based on a 128x128 pixel asynchronous optical transient sensor, using address-event representation (AER) protocol. An address-event based algorithm for stereo depth calculation including calibration, correspondence and reconstruction processing steps is also presented. Due to the onchip data pre-processing the algorithm can be implemented on a single low-power digital signal processor.",
            "year": 2007,
            "venue": "International Conference on Computer Vision Theory and Applications",
            "authors": [
              {
                "authorId": "2521747",
                "name": "S. Schraml"
              },
              {
                "authorId": "1680865",
                "name": "Peter Schön"
              },
              {
                "authorId": "152832347",
                "name": "Nenad Milosevic"
              }
            ]
          }
        },
        {
          "citedcorpusid": 205698386,
          "isinfluential": true,
          "contexts": [
            "Since the disparity data generated from SCN is frameless, the latency was estimated using the amount of SCN spikes and rate of change of edges as proposed in [7].",
            "In most previous studies, a processor was used for SCN design [7], [8], [11], [17].",
            "In [7], another method was proposed for constructing the network, which separates the SCN into two layers as follows : When a coincidence neuron is activated, it excites the disparity-layer neurons according to the continuity constraint.",
            "Thus, stereo systems utilizing DVSs have recently gained popularity and have been reported in the literature [3], [4], [5], [6], [7], [8].",
            "In [7] and [8], a cooperative stereo algorithm employing a spiking-neuron model was presented."
          ],
          "intents": [
            "['methodology']",
            "['methodology']",
            "['methodology']",
            "--",
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "A spiking neural network model of 3D perception for event-based neuromorphic stereo vision systems",
            "abstract": "Stereo vision is an important feature that enables machine vision systems to perceive their environment in 3D. While machine vision has spawned a variety of software algorithms to solve the stereo-correspondence problem, their implementation and integration in small, fast, and efficient hardware vision systems remains a difficult challenge. Recent advances made in neuromorphic engineering offer a possible solution to this problem, with the use of a new class of event-based vision sensors and neural processing devices inspired by the organizing principles of the brain. Here we propose a radically novel model that solves the stereo-correspondence problem with a spiking neural network that can be directly implemented with massively parallel, compact, low-latency and low-power neuromorphic engineering devices. We validate the model with experimental results, highlighting features that are in agreement with both computational neuroscience stereo vision theories and experimental findings. We demonstrate its features with a prototype neuromorphic hardware system and provide testable predictions on the role of spike-based representations and temporal dynamics in biological stereo vision processing systems.",
            "year": 2017,
            "venue": "Scientific Reports",
            "authors": [
              {
                "authorId": "2598816",
                "name": "Marc Osswald"
              },
              {
                "authorId": "144975525",
                "name": "S. Ieng"
              },
              {
                "authorId": "1750848",
                "name": "R. Benosman"
              },
              {
                "authorId": "1721210",
                "name": "G. Indiveri"
              }
            ]
          }
        },
        {
          "citedcorpusid": 232152677,
          "isinfluential": false,
          "contexts": [
            "Neu-romorphic computing systems utilizing ﬁeld-programmable analog array or ﬁeld-programmable gate array (FPGA) devices have been successfully demonstrated the advantages of asynchronous computations [26], [27], [28]."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "A Fast and Energy-Efficient SNN Processor With Adaptive Clock/Event-Driven Computation Scheme and Online Learning",
            "abstract": "In the recent years, the spiking neural network (SNN) has attracted increasing attention due to its low energy consumption and online learning potential. However, the design of SNN processor has not been thoroughly investigated in the past, resulting in limited performance and energy consumption. In this work, a fast and energy-efficient SNN processor with adaptive clock/event-driven computation scheme and online learning capability has been proposed. Several techniques have been proposed to reduce the computation time and energy consumption, including Adaptive Clock- and Event-Driven Computing Scheme, Neighboring PE Borrowing Technique, Compressed Spike Routing Technique and Reconfigurable PE for Inference and Learning. Implemented on a Virtex-7 FPGA, the proposed design achieves computation time of 3.15 ms/image, inference energy consumption of <inline-formula> <tex-math notation=\"LaTeX\">$0.028~\\mu $ </tex-math></inline-formula> J/synapse/image and online learning energy consumption of <inline-formula> <tex-math notation=\"LaTeX\">$0.297~\\mu $ </tex-math></inline-formula> J/synapse/image for the MNIST 10-class dataset, which outperform several state-of-the-art SNN processors. The proposed SNN processor is suitable for real-time and energy-constrained applications.",
            "year": 2021,
            "venue": "IEEE Transactions on Circuits and Systems Part 1: Regular Papers",
            "authors": [
              {
                "authorId": "10439301",
                "name": "Sixu Li"
              },
              {
                "authorId": null,
                "name": "Zhaomin Zhang"
              },
              {
                "authorId": "40996145",
                "name": "R. Mao"
              },
              {
                "authorId": "3757226",
                "name": "Jianbiao Xiao"
              },
              {
                "authorId": "2108316483",
                "name": "L. Chang"
              },
              {
                "authorId": "2151548897",
                "name": "Jun Zhou"
              }
            ]
          }
        }
      ]
    },
    "250602271": {
      "citing_paper_info": {
        "title": "Discrete time convolution for fast event-based stereo",
        "abstract": "Inspired by biological retina, dynamical vision sensor transmits events of instantaneous changes of pixel intensity, giving it a series of advantages over traditional frame-based camera, such as high dynamical range, high temporal resolution and low power consumption. However, extracting information from highly asynchronous event data is a challenging task. Inspired by continuous dynamics of biological neuron models, we propose a novel encoding method for sparse events-continuous time convolution (CTC)-which learns to model the spatial feature of the data with intrinsic dynamics. Adopting channel-wise parameterization, temporal dynamics of the model is synchronized on the same feature map and diverges across different ones, enabling it to embed data in a variety of temporal scales. Abstracted from CTC, we further develop discrete time convolution (DTC) which accelerates the process with lower computational cost. We apply these methods to event-based multi- view stereo matching where they surpass state-of-the-art methods on benchmark criteria of the MVSEC dataset. Spatially sparse event data often leads to inaccurate estimation of edges and local contours. To address this problem, we propose a dual-path architecture in which the feature map is complemented by underlying edge information from original events extracted with spatially-adaptive denormal-ization. We demonstrate the superiority of our model in terms of speed (up to 110 FPS), accuracy and robustness, showing a great potential for real-time fast depth estimation. Finally, we perform experiments on the recent DSEC dataset to demonstrate the general usage of our model.",
        "year": 2022,
        "venue": "Computer Vision and Pattern Recognition",
        "authors": [
          {
            "authorId": "2152981298",
            "name": "Kai Zhang"
          },
          {
            "authorId": "2122908748",
            "name": "Kaiwei Che"
          },
          {
            "authorId": "2155240940",
            "name": "Jianguo Zhang"
          },
          {
            "authorId": "2163076178",
            "name": "Jie Cheng"
          },
          {
            "authorId": "2144371654",
            "name": "Ziyang Zhang"
          },
          {
            "authorId": "47747957",
            "name": "Qinghai Guo"
          },
          {
            "authorId": "48205902",
            "name": "Luziwei Leng"
          }
        ]
      },
      "citation_summary": {
        "citation_count": 24,
        "unique_cited_count": 24,
        "influential_count": 3,
        "detailed_records_count": 24
      },
      "cited_papers": [
        "52814827",
        "1561703",
        "4412139",
        "1234009",
        "196016124",
        "56475917",
        "214605597",
        "247675601",
        "226976144",
        "119297695",
        "211126617",
        "1753085",
        "213704910",
        "25268038",
        "216036364",
        "13373696",
        "3416874",
        "119304432",
        "211258776",
        "235078812",
        "485828",
        "231759393",
        "222319014",
        "203593170"
      ],
      "citation_details": [
        {
          "citedcorpusid": 485828,
          "isinfluential": true,
          "contexts": [
            "Inspired by convolutional RNNs [4, 29, 55] and continuous temporal dynamics of biological neuron models [18, 30], we propose a novel event processing method combining merits of both sides.",
            "However, fully-connected RNNs are not efficient for information extraction of images.",
            "An alternative encoding method for events is to use recurrent neural networks (RNNs), given their inherent ability to encode temporal sequences.",
            "Different embodiments of RCNN include convolutional long short-term memory (ConvLSTM) [55] and convolutional gated recurrent units (ConvGRU) [4], where additional gating variables were used for memory modulation.",
            "A natural thought is to incorporate RNNs into convolutional operations.",
            "Different from traditional RNNs constructed with artificial neurons, spiking neural networks (SNNs) [30] uses spiking neuron models inspired by biology with inherent self-recurrence."
          ],
          "intents": [
            "['methodology']",
            "--",
            "--",
            "['methodology']",
            "--",
            "--"
          ],
          "cited_paper_info": {
            "title": "Delving Deeper into Convolutional Networks for Learning Video Representations",
            "abstract": "We propose an approach to learn spatio-temporal features in videos from intermediate visual representations we call \"percepts\" using Gated-Recurrent-Unit Recurrent Networks (GRUs).Our method relies on percepts that are extracted from all level of a deep convolutional network trained on the large ImageNet dataset. While high-level percepts contain highly discriminative information, they tend to have a low-spatial resolution. Low-level percepts, on the other hand, preserve a higher spatial resolution from which we can model finer motion patterns. Using low-level percepts can leads to high-dimensionality video representations. To mitigate this effect and control the model number of parameters, we introduce a variant of the GRU model that leverages the convolution operations to enforce sparse connectivity of the model units and share parameters across the input spatial locations. \nWe empirically validate our approach on both Human Action Recognition and Video Captioning tasks. In particular, we achieve results equivalent to state-of-art on the YouTube2Text dataset using a simpler text-decoder model and without extra 3D CNN features.",
            "year": 2015,
            "venue": "International Conference on Learning Representations",
            "authors": [
              {
                "authorId": "2482072",
                "name": "Nicolas Ballas"
              },
              {
                "authorId": "145095579",
                "name": "L. Yao"
              },
              {
                "authorId": "1972076",
                "name": "C. Pal"
              },
              {
                "authorId": "1760871",
                "name": "Aaron C. Courville"
              }
            ]
          }
        },
        {
          "citedcorpusid": 1234009,
          "isinfluential": false,
          "contexts": [
            "There has been an increasing number of applications of SNNs in deep learning [5, 12, 21, 22, 28, 38, 45, 50, 54, 60], and the network’s asynchronous nature makes it an ideal solution for event-based tasks [7, 8, 23, 27, 41, 57]."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "HFirst: A Temporal Approach to Object Recognition",
            "abstract": "This paper introduces a spiking hierarchical model for object recognition which utilizes the precise timing information inherently present in the output of biologically inspired asynchronous address event representation (AER) vision sensors. The asynchronous nature of these systems frees computation and communication from the rigid predetermined timing enforced by system clocks in conventional systems. Freedom from rigid timing constraints opens the possibility of using true timing to our advantage in computation. We show not only how timing can be used in object recognition, but also how it can in fact simplify computation. Specifically, we rely on a simple temporal-winner-take-all rather than more computationally intensive synchronous operations typically used in biologically inspired neural networks for object recognition. This approach to visual computation represents a major paradigm shift from conventional clocked systems and can find application in other sensory modalities and computational tasks. We showcase effectiveness of the approach by achieving the highest reported accuracy to date (97.5% ± 3.5%) for a previously published four class card pip recognition task and an accuracy of 84.9% ± 1.9% for a new more difficult 36 class character recognition task.",
            "year": 2015,
            "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
            "authors": [
              {
                "authorId": "33780923",
                "name": "G. Orchard"
              },
              {
                "authorId": "48849443",
                "name": "Cedric Meyer"
              },
              {
                "authorId": "1398026219",
                "name": "R. Etienne-Cummings"
              },
              {
                "authorId": "153466606",
                "name": "C. Posch"
              },
              {
                "authorId": "145146201",
                "name": "N. Thakor"
              },
              {
                "authorId": "1750848",
                "name": "R. Benosman"
              }
            ]
          }
        },
        {
          "citedcorpusid": 1561703,
          "isinfluential": false,
          "contexts": [
            "[39] applied a modified version of LSTM [19] to event-based recognition, but the model was not specifically designed to preserve spatial information."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Phased LSTM: Accelerating Recurrent Network Training for Long or Event-based Sequences",
            "abstract": "Recurrent Neural Networks (RNNs) have become the state-of-the-art choice for extracting patterns from temporal sequences. Current RNN models are ill suited to process irregularly sampled data triggered by events generated in continuous time by sensors or other neurons. Such data can occur, for example, when the input comes from novel event-driven artificial sensors which generate sparse, asynchronous streams of events or from multiple conventional sensors with different update intervals. In this work, we introduce the Phased LSTM model, which extends the LSTM unit by adding a new time gate. This gate is controlled by a parametrized oscillation with a frequency range which require updates of the memory cell only during a small percentage of the cycle. Even with the sparse updates imposed by the oscillation, the Phased LSTM network achieves faster convergence than regular LSTMs on tasks which require learning of long sequences. The model naturally integrates inputs from sensors of arbitrary sampling rates, thereby opening new areas of investigation for processing asynchronous sensory events that carry timing information. It also greatly improves the performance of LSTMs in standard RNN applications, and does so with an order-of-magnitude fewer computes.",
            "year": 2016,
            "venue": "Neural Information Processing Systems",
            "authors": [
              {
                "authorId": "145243593",
                "name": "Daniel Neil"
              },
              {
                "authorId": "144578436",
                "name": "Michael Pfeiffer"
              },
              {
                "authorId": "1704961",
                "name": "Shih-Chii Liu"
              }
            ]
          }
        },
        {
          "citedcorpusid": 1753085,
          "isinfluential": false,
          "contexts": [
            "Inspired by convolutional RNNs [4, 29, 55] and continuous temporal dynamics of biological neuron models [18, 30], we propose a novel event processing method combining merits of both sides.",
            "Different from traditional RNNs constructed with artificial neurons, spiking neural networks (SNNs) [30] uses spiking neuron models inspired by biology with inherent self-recurrence."
          ],
          "intents": [
            "['methodology']",
            "--"
          ],
          "cited_paper_info": {
            "title": "Networks of Spiking Neurons: The Third Generation of Neural Network Models",
            "abstract": "",
            "year": 1996,
            "venue": "Electron. Colloquium Comput. Complex.",
            "authors": [
              {
                "authorId": "145247053",
                "name": "W. Maass"
              }
            ]
          }
        },
        {
          "citedcorpusid": 3416874,
          "isinfluential": true,
          "contexts": [
            "To our best knowledge, our models are the first to perform streaming experiments for dense disparity estimation on the MVSEC dataset.",
            "We demonstrated the superiority of our model over existing state-of-the-art works on both the MVSEC and the DSEC dataset.",
            "We demonstrated their advantages over other methods on the event-based stereo matching task on the MVSEC dataset.",
            "We split and preprocess the Indoor Flying dataset from the MVSEC using the same setting as [2, 51, 62].",
            "We conduct our experiments on the MVSEC dataset [63], which contains depth information collected by a LIDAR sensor and event streams obtained from two event cameras with corresponding 20 Hz intensity images at 346×260 resolution.",
            "We demonstrate the advantage of CTC and DTC over other event encoding methods on a set of criteria of event-based stereo matching, on the Multi Vehicle Stereo Event Camera (MVSEC) [63] dataset."
          ],
          "intents": [
            "--",
            "--",
            "--",
            "--",
            "['methodology']",
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "The Multivehicle Stereo Event Camera Dataset: An Event Camera Dataset for 3D Perception",
            "abstract": "Event-based cameras are a new passive sensing modality with a number of benefits over traditional cameras, including extremely low latency, asynchronous data acquisition, high dynamic range, and very low power consumption. There has been a lot of recent interest and development in applying algorithms to use the events to perform a variety of three-dimensional perception tasks, such as feature tracking, visual odometry, and stereo depth estimation. However, there currently lacks the wealth of labeled data that exists for traditional cameras to be used for both testing and development. In this letter, we present a large dataset with a synchronized stereo pair event based camera system, carried on a handheld rig, flown by a hexacopter, driven on top of a car, and mounted on a motorcycle, in a variety of different illumination levels and environments. From each camera, we provide the event stream, grayscale images, and inertial measurement unit (IMU) readings. In addition, we utilize a combination of IMU, a rigidly mounted lidar system, indoor and outdoor motion capture, and GPS to provide accurate pose and depth images for each camera at up to 100 Hz. For comparison, we also provide synchronized grayscale images and IMU readings from a frame-based stereo camera system.",
            "year": 2018,
            "venue": "IEEE Robotics and Automation Letters",
            "authors": [
              {
                "authorId": "3385588",
                "name": "A. Z. Zhu"
              },
              {
                "authorId": "144964367",
                "name": "Dinesh Thakur"
              },
              {
                "authorId": "2520604",
                "name": "Tolga Özaslan"
              },
              {
                "authorId": "39832696",
                "name": "Bernd Pfrommer"
              },
              {
                "authorId": "37956314",
                "name": "Vijay R. Kumar"
              },
              {
                "authorId": "1751586",
                "name": "Kostas Daniilidis"
              }
            ]
          }
        },
        {
          "citedcorpusid": 4412139,
          "isinfluential": false,
          "contexts": [
            "It also decreases data storage space for potential hardware applications, similar approaches were taken in [62].",
            "We split and preprocess the Indoor Flying dataset from the MVSEC using the same setting as [2, 51, 62].",
            "TSES [62] utilized the velocity of the camera to approximate optical flow and build time synchronized event disparity volumes."
          ],
          "intents": [
            "['methodology']",
            "['methodology']",
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Realtime Time Synchronized Event-based Stereo",
            "abstract": "In this work, we propose a novel event based stereo method which addresses the problem of motion blur for a moving event camera. Our method uses the velocity of the camera and a range of disparities to synchronize the positions of the events, as if they were captured at a single point in time. We represent these events using a pair of novel time synchronized event disparity volumes, which we show remove motion blur for pixels at the correct disparity in the volume, while further blurring pixels at the wrong disparity. We then apply a novel matching cost over these time synchronized event disparity volumes, which both rewards similarity between the volumes while penalizing blurriness. We show that our method outperforms more expensive, smoothing based event stereo methods, by evaluating on the Multi Vehicle Stereo Event Camera dataset.",
            "year": 2018,
            "venue": "European Conference on Computer Vision",
            "authors": [
              {
                "authorId": "3385588",
                "name": "A. Z. Zhu"
              },
              {
                "authorId": "2116435960",
                "name": "Yibo Chen"
              },
              {
                "authorId": "1751586",
                "name": "Kostas Daniilidis"
              }
            ]
          }
        },
        {
          "citedcorpusid": 13373696,
          "isinfluential": false,
          "contexts": [
            "The so-called handcrafted methods [25, 31, 35, 36, 40, 53, 58, 64] directly convert events to event frames based on the four dimensional information of each event.",
            "[25, 31] stored histograms of events of different polarities in different channels to avoid information loss due to polarity cancellation."
          ],
          "intents": [
            "['methodology']",
            "['background']"
          ],
          "cited_paper_info": {
            "title": "HOTS: A Hierarchy of Event-Based Time-Surfaces for Pattern Recognition",
            "abstract": "",
            "year": 2017,
            "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
            "authors": [
              {
                "authorId": "1807856",
                "name": "Xavier Lagorce"
              },
              {
                "authorId": "33780923",
                "name": "G. Orchard"
              },
              {
                "authorId": "3008126",
                "name": "F. Galluppi"
              },
              {
                "authorId": "2075335081",
                "name": "Bertram E. Shi"
              },
              {
                "authorId": "1750848",
                "name": "R. Benosman"
              }
            ]
          }
        },
        {
          "citedcorpusid": 25268038,
          "isinfluential": false,
          "contexts": [
            "An extension of our principles to SNNs implemented in neuromorphic hardware [10, 11, 14, 24, 33, 43] could further lead to super fast event-based stereo system [3]."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "The SpiNNaker Project",
            "abstract": "",
            "year": 2014,
            "venue": "Proceedings of the IEEE",
            "authors": [
              {
                "authorId": "144409615",
                "name": "S. Furber"
              },
              {
                "authorId": "3008126",
                "name": "F. Galluppi"
              },
              {
                "authorId": "143816983",
                "name": "S. Temple"
              },
              {
                "authorId": "3085921",
                "name": "L. Plana"
              }
            ]
          }
        },
        {
          "citedcorpusid": 52814827,
          "isinfluential": false,
          "contexts": [
            "The so-called handcrafted methods [25, 31, 35, 36, 40, 53, 58, 64] directly convert events to event frames based on the four dimensional information of each event."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Unsupervised Learning of Dense Optical Flow and Depth from Sparse Event Data",
            "abstract": "In this work we present unsupervised learning of depth and motion from sparse event data generated by a Dynamic Vision Sensor (DVS). To tackle this low level vision task, we use a novel encoder-decoder neural network architecture that aggregates multi-level features and addresses the problem at multiple resolutions. A feature decorrelation technique is introduced to improve the training of the network. A non-local sparse smoothness constraint is used to alleviate the challenge of data sparsity. Our work is the first that generates dense depth and optical flow information from sparse event data. Our results show significant improvements upon previous works that used deep learning for flow estimation from both images and events.",
            "year": 2018,
            "venue": "arXiv.org",
            "authors": [
              {
                "authorId": "3300969",
                "name": "Chengxi Ye"
              },
              {
                "authorId": "144559298",
                "name": "A. Mitrokhin"
              },
              {
                "authorId": "38904651",
                "name": "Chethan Parameshwara"
              },
              {
                "authorId": "1759899",
                "name": "C. Fermüller"
              },
              {
                "authorId": "9861772",
                "name": "J. Yorke"
              },
              {
                "authorId": "1697493",
                "name": "Y. Aloimonos"
              }
            ]
          }
        },
        {
          "citedcorpusid": 56475917,
          "isinfluential": false,
          "contexts": [
            "[65] created voxel grids by interpolation based on timestamps of events."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Unsupervised Event-Based Learning of Optical Flow, Depth, and Egomotion",
            "abstract": "In this work, we propose a novel framework for unsupervised learning for event cameras that learns motion information from only the event stream. In particular, we propose an input representation of the events in the form of a discretized volume that maintains the temporal distribution of the events, which we pass through a neural network to predict the motion of the events. This motion is used to attempt to remove any motion blur in the event image. We then propose a loss function applied to the motion compensated event image that measures the motion blur in this image. We train two networks with this framework, one to predict optical flow, and one to predict egomotion and depths, and evaluate these networks on the Multi Vehicle Stereo Event Camera dataset, along with qualitative results from a variety of different scenes.",
            "year": 2018,
            "venue": "Computer Vision and Pattern Recognition",
            "authors": [
              {
                "authorId": "3385588",
                "name": "A. Z. Zhu"
              },
              {
                "authorId": "36001694",
                "name": "Liangzhe Yuan"
              },
              {
                "authorId": "20728097",
                "name": "Kenneth Chaney"
              },
              {
                "authorId": "1751586",
                "name": "Kostas Daniilidis"
              }
            ]
          }
        },
        {
          "citedcorpusid": 119297695,
          "isinfluential": false,
          "contexts": [
            "An extension of our principles to SNNs implemented in neuromorphic hardware [10, 11, 14, 24, 33, 43] could further lead to super fast event-based stereo system [3]."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Accelerated Physical Emulation of Bayesian Inference in Spiking Neural Networks",
            "abstract": "The massively parallel nature of biological information processing plays an important role due to its superiority in comparison to human-engineered computing devices. In particular, it may hold the key to overcoming the von Neumann bottleneck that limits contemporary computer architectures. Physical-model neuromorphic devices seek to replicate not only this inherent parallelism, but also aspects of its microscopic dynamics in analog circuits emulating neurons and synapses. However, these machines require network models that are not only adept at solving particular tasks, but that can also cope with the inherent imperfections of analog substrates. We present a spiking network model that performs Bayesian inference through sampling on the BrainScaleS neuromorphic platform, where we use it for generative and discriminative computations on visual data. By illustrating its functionality on this platform, we implicitly demonstrate its robustness to various substrate-specific distortive effects, as well as its accelerated capability for computation. These results showcase the advantages of brain-inspired physical computation and provide important building blocks for large-scale neuromorphic applications.",
            "year": 2018,
            "venue": "Frontiers in Neuroscience",
            "authors": [
              {
                "authorId": "51135867",
                "name": "Á. F. Kungl"
              },
              {
                "authorId": "1780662217",
                "name": "Sebastian Schmitt"
              },
              {
                "authorId": "46275173",
                "name": "Johann Klähn"
              },
              {
                "authorId": "2074163152",
                "name": "Paul Müller"
              },
              {
                "authorId": "46708045",
                "name": "A. Baumbach"
              },
              {
                "authorId": "19322337",
                "name": "Dominik Dold"
              },
              {
                "authorId": "51117259",
                "name": "Alexander Kugele"
              },
              {
                "authorId": "143980412",
                "name": "Eric Müller"
              },
              {
                "authorId": "6862145",
                "name": "Christoph Koke"
              },
              {
                "authorId": "9919190",
                "name": "Mitja Kleider"
              },
              {
                "authorId": "46562690",
                "name": "Christian Mauch"
              },
              {
                "authorId": "2377454",
                "name": "O. Breitwieser"
              },
              {
                "authorId": "48205902",
                "name": "Luziwei Leng"
              },
              {
                "authorId": "50057572",
                "name": "Nico Gürtler"
              },
              {
                "authorId": "50199029",
                "name": "Maurice Güttler"
              },
              {
                "authorId": "49759974",
                "name": "D. Husmann"
              },
              {
                "authorId": "40566007",
                "name": "Kai Husmann"
              },
              {
                "authorId": "2064791913",
                "name": "Andreas Hartel"
              },
              {
                "authorId": "9917967",
                "name": "V. Karasenko"
              },
              {
                "authorId": "34988192",
                "name": "Andreas Grübl"
              },
              {
                "authorId": "2138905",
                "name": "J. Schemmel"
              },
              {
                "authorId": "36245042",
                "name": "K. Meier"
              },
              {
                "authorId": "2238320235",
                "name": "Mihai A. Petrovici"
              }
            ]
          }
        },
        {
          "citedcorpusid": 119304432,
          "isinfluential": false,
          "contexts": [
            "Most of the works in stereo matching based on deep learning are established on image datasets [20, 32, 52, 56, 59]."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "GA-Net: Guided Aggregation Net for End-To-End Stereo Matching",
            "abstract": "In the stereo matching task, matching cost aggregation is crucial in both traditional methods and deep neural network models in order to accurately estimate disparities. We propose two novel neural net layers, aimed at capturing local and the whole-image cost dependencies respectively. The first is a semi-global aggregation layer which is a differentiable approximation of the semi-global matching, the second is the local guided aggregation layer which follows a traditional cost filtering strategy to refine thin structures. These two layers can be used to replace the widely used 3D convolutional layer which is computationally costly and memory-consuming as it has cubic computational/memory complexity. In the experiments, we show that nets with a two-layer guided aggregation block easily outperform the state-of-the-art GC-Net which has nineteen 3D convolutional layers. We also train a deep guided aggregation network (GA-Net) which gets better accuracies than state-of-the-art methods on both Scene Flow dataset and KITTI benchmarks.",
            "year": 2019,
            "venue": "Computer Vision and Pattern Recognition",
            "authors": [
              {
                "authorId": "2144025110",
                "name": "Feihu Zhang"
              },
              {
                "authorId": "2824784",
                "name": "V. Prisacariu"
              },
              {
                "authorId": "38958903",
                "name": "Ruigang Yang"
              },
              {
                "authorId": "143635540",
                "name": "Philip H. S. Torr"
              }
            ]
          }
        },
        {
          "citedcorpusid": 196016124,
          "isinfluential": false,
          "contexts": [
            "The model can be viewed as a non-spiking form of the leaky integrate and fire (LIF) neuron with conductance synapse [44], without specifically defining its synaptic dynamics."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Form Versus Function: Theory and Models for Neuronal Substrates",
            "abstract": "The quest for endowing form with function represents the fundamental motivation behind all neural network modeling. In this thesis, we discuss various functional neuronal architectures and their implementation in silico, both on conventional computer systems and on neuromorpic devices. Necessarily, such casting to a particular substrate will constrain their form, either by requiring a simplified description of neuronal dynamics and interactions or by imposing physical limitations on important characteristics such as network connectivity or parameter precision. While our main focus lies on the computational properties of the studied models, we augment our discussion with rigorous mathematical formalism. We start by investigating the behavior of point neurons under synaptic bombardment and provide analytical predictions of single-unit and ensemble statistics. These considerations later become useful when moving to the functional network level, where we study the effects of an imperfect physical substrate on the computational properties of several cortical networks. Finally, we return to the single neuron level to discuss a novel interpretation of spiking activity in the context of probabilistic inference through sampling. We provide analytical derivations for the translation of this ``neural sampling'' framework to networks of biologically plausible and hardware-compatible neurons and later take this concept beyond the realm of brain science when we discuss applications in machine learning and analogies to solid-state systems.",
            "year": 2016,
            "venue": "",
            "authors": [
              {
                "authorId": "2238320235",
                "name": "Mihai A. Petrovici"
              }
            ]
          }
        },
        {
          "citedcorpusid": 203593170,
          "isinfluential": false,
          "contexts": [
            "There has been an increasing number of applications of SNNs in deep learning [5, 12, 21, 22, 28, 38, 45, 50, 54, 60], and the network’s asynchronous nature makes it an ideal solution for event-based tasks [7, 8, 23, 27, 41, 57]."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "DashNet: A Hybrid Artificial and Spiking Neural Network for High-speed Object Tracking",
            "abstract": "Computer-science-oriented artificial neural networks (ANNs) have achieved tremendous success in a variety of scenarios via powerful feature extraction and high-precision data operations. It is well known, however, that ANNs usually suffer from expensive processing resources and costs. In contrast, neuroscience-oriented spiking neural networks (SNNs) are promising for energy-efficient information processing benefit from the event-driven spike activities, whereas, they are yet be evidenced to achieve impressive effectiveness on real complicated tasks. How to combine the advantage of these two model families is an open question of great interest. Two significant challenges need to be addressed: (1) lack of benchmark datasets including both ANN-oriented (frames) and SNN-oriented (spikes) signal resources; (2) the difficulty in jointly processing the synchronous activation from ANNs and event-driven spikes from SNNs. In this work, we proposed a hybrid paradigm, named as DashNet, to demonstrate the advantages of combining ANNs and SNNs in a single model. A simulator and benchmark dataset NFS-DAVIS is built, and a temporal complementary filter (TCF) and attention module are designed to address the two mentioned challenges, respectively. In this way, it is shown that DashNet achieves the record-breaking speed of 2083FPS on neuromorphic chips and the best tracking performance on NFS-DAVIS and PRED18 datasets. To the best of our knowledge, DashNet is the first framework that can integrate and process ANNs and SNNs in a hybrid paradigm, which provides a novel solution to achieve both effectiveness and efficiency for high-speed object tracking.",
            "year": 2019,
            "venue": "arXiv.org",
            "authors": [
              {
                "authorId": "92403059",
                "name": "Zheyu Yang"
              },
              {
                "authorId": "72500851",
                "name": "Yujie Wu"
              },
              {
                "authorId": "80816621",
                "name": "Guanrui Wang"
              },
              {
                "authorId": "2108581965",
                "name": "Yukuan Yang"
              },
              {
                "authorId": "1730243",
                "name": "Guoqi Li"
              },
              {
                "authorId": "143895326",
                "name": "Lei Deng"
              },
              {
                "authorId": "2146279632",
                "name": "Jun Zhu"
              },
              {
                "authorId": "29889772",
                "name": "Luping Shi"
              }
            ]
          }
        },
        {
          "citedcorpusid": 211126617,
          "isinfluential": false,
          "contexts": [
            "[9] used time-surface with linear time decay to construct event images."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "End-to-end Learning of Object Motion Estimation from Retinal Events for Event-based Object Tracking",
            "abstract": "Event cameras, which are asynchronous bio-inspired vision sensors, have shown great potential in computer vision and artificial intelligence. However, the application of event cameras to object-level motion estimation or tracking is still in its infancy. The main idea behind this work is to propose a novel deep neural network to learn and regress a parametric object-level motion/transform model for event-based object tracking. To achieve this goal, we propose a synchronous Time-Surface with Linear Time Decay (TSLTD) representation, which effectively encodes the spatio-temporal information of asynchronous retinal events into TSLTD frames with clear motion patterns. We feed the sequence of TSLTD frames to a novel Retinal Motion Regression Network (RMRNet) to perform an end-to-end 5-DoF object motion regression. Our method is compared with state-of-the-art object tracking methods, that are based on conventional cameras or event cameras. The experimental results show the superiority of our method in handling various challenging environments such as fast motion and low illumination conditions.",
            "year": 2020,
            "venue": "AAAI Conference on Artificial Intelligence",
            "authors": [
              {
                "authorId": "101709917",
                "name": "Haosheng Chen"
              },
              {
                "authorId": "50592201",
                "name": "D. Suter"
              },
              {
                "authorId": "8673702",
                "name": "Qiangqiang Wu"
              },
              {
                "authorId": "2113288945",
                "name": "Hanzi Wang"
              }
            ]
          }
        },
        {
          "citedcorpusid": 211258776,
          "isinfluential": false,
          "contexts": [
            "There has been an increasing number of applications of SNNs in deep learning [5, 12, 21, 22, 28, 38, 45, 50, 54, 60], and the network’s asynchronous nature makes it an ideal solution for event-based tasks [7, 8, 23, 27, 41, 57]."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Temporal Spike Sequence Learning via Backpropagation for Deep Spiking Neural Networks",
            "abstract": "Spiking neural networks (SNNs) are well suited for spatio-temporal learning and implementations on energy-efficient event-driven neuromorphic processors. However, existing SNN error backpropagation (BP) methods lack proper handling of spiking discontinuities and suffer from low performance compared with the BP methods for traditional artificial neural networks. In addition, a large number of time steps are typically required to achieve decent performance, leading to high latency and rendering spike-based computation unscalable to deep architectures. We present a novel Temporal Spike Sequence Learning Backpropagation (TSSL-BP) method for training deep SNNs, which breaks down error backpropagation across two types of inter-neuron and intra-neuron dependencies and leads to improved temporal learning precision. It captures inter-neuron dependencies through presynaptic firing times by considering the all-or-none characteristics of firing activities and captures intra-neuron dependencies by handling the internal evolution of each neuronal state in time. TSSL-BP efficiently trains deep SNNs within a much shortened temporal window of a few steps while improving the accuracy for various image classification datasets including CIFAR10.",
            "year": 2020,
            "venue": "Neural Information Processing Systems",
            "authors": [
              {
                "authorId": "49039404",
                "name": "Wenrui Zhang"
              },
              {
                "authorId": "2149247484",
                "name": "Peng Li"
              }
            ]
          }
        },
        {
          "citedcorpusid": 213704910,
          "isinfluential": false,
          "contexts": [
            "There has been an increasing number of applications of SNNs in deep learning [5, 12, 21, 22, 28, 38, 45, 50, 54, 60], and the network’s asynchronous nature makes it an ideal solution for event-based tasks [7, 8, 23, 27, 41, 57]."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Spiking-YOLO: Spiking Neural Network for Energy-Efficient Object Detection",
            "abstract": "Over the past decade, deep neural networks (DNNs) have demonstrated remarkable performance in a variety of applications. As we try to solve more advanced problems, increasing demands for computing and power resources has become inevitable. Spiking neural networks (SNNs) have attracted widespread interest as the third-generation of neural networks due to their event-driven and low-powered nature. SNNs, however, are difficult to train, mainly owing to their complex dynamics of neurons and non-differentiable spike operations. Furthermore, their applications have been limited to relatively simple tasks such as image classification. In this study, we investigate the performance degradation of SNNs in a more challenging regression problem (i.e., object detection). Through our in-depth analysis, we introduce two novel methods: channel-wise normalization and signed neuron with imbalanced threshold, both of which provide fast and accurate information transmission for deep SNNs. Consequently, we present a first spiked-based object detection model, called Spiking-YOLO. Our experiments show that Spiking-YOLO achieves remarkable results that are comparable (up to 98%) to those of Tiny YOLO on non-trivial datasets, PASCAL VOC and MS COCO. Furthermore, Spiking-YOLO on a neuromorphic chip consumes approximately 280 times less energy than Tiny YOLO and converges 2.3 to 4 times faster than previous SNN conversion methods.",
            "year": 2019,
            "venue": "AAAI Conference on Artificial Intelligence",
            "authors": [
              {
                "authorId": "153274617",
                "name": "Seijoon Kim"
              },
              {
                "authorId": "2267522",
                "name": "Seongsik Park"
              },
              {
                "authorId": "2972978",
                "name": "Byunggook Na"
              },
              {
                "authorId": "2999019",
                "name": "Sungroh Yoon"
              }
            ]
          }
        },
        {
          "citedcorpusid": 214605597,
          "isinfluential": false,
          "contexts": [
            "[34, 47] applied specially designed asynchronous convolution for sparse events data."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Event-based Asynchronous Sparse Convolutional Networks",
            "abstract": "Event cameras are bio-inspired sensors that respond to per-pixel brightness changes in the form of asynchronous and sparse \"events\". Recently, pattern recognition algorithms, such as learning-based methods, have made significant progress with event cameras by converting events into synchronous dense, image-like representations and applying traditional machine learning methods developed for standard cameras. However, these approaches discard the spatial and temporal sparsity inherent in event data at the cost of higher computational complexity and latency. In this work, we present a general framework for converting models trained on synchronous image-like event representations into asynchronous models with identical output, thus directly leveraging the intrinsic asynchronous and sparse nature of the event data. We show both theoretically and experimentally that this drastically reduces the computational complexity and latency of high-capacity, synchronous neural networks without sacrificing accuracy. In addition, our framework has several desirable characteristics: (i) it exploits spatio-temporal sparsity of events explicitly, (ii) it is agnostic to the event representation, network architecture, and task, and (iii) it does not require any train-time change, since it is compatible with the standard neural networks' training process. We thoroughly validate the proposed framework on two computer vision tasks: object detection and object recognition. In these tasks, we reduce the computational complexity up to 20 times with respect to high-latency neural networks. At the same time, we outperform state-of-the-art asynchronous approaches up to 24% in prediction accuracy.",
            "year": 2020,
            "venue": "European Conference on Computer Vision",
            "authors": [
              {
                "authorId": "29989106",
                "name": "Nico Messikommer"
              },
              {
                "authorId": "51152279",
                "name": "Daniel Gehrig"
              },
              {
                "authorId": "20580939",
                "name": "Antonio Loquercio"
              },
              {
                "authorId": "2075371",
                "name": "D. Scaramuzza"
              }
            ]
          }
        },
        {
          "citedcorpusid": 216036364,
          "isinfluential": false,
          "contexts": [
            "Most of the works in stereo matching based on deep learning are established on image datasets [20, 32, 52, 56, 59]."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "AANet: Adaptive Aggregation Network for Efficient Stereo Matching",
            "abstract": "Despite the remarkable progress made by learning based stereo matching algorithms, one key challenge remains unsolved. Current state-of-the-art stereo models are mostly based on costly 3D convolutions, the cubic computational complexity and high memory consumption make it quite expensive to deploy in real-world applications. In this paper, we aim at completely replacing the commonly used 3D convolutions to achieve fast inference speed while maintaining comparable accuracy. To this end, we first propose a sparse points based intra-scale cost aggregation method to alleviate the well-known edge-fattening issue at disparity discontinuities. Further, we approximate traditional cross-scale cost aggregation algorithm with neural network layers to handle large textureless regions. Both modules are simple, lightweight, and complementary, leading to an effective and efficient architecture for cost aggregation. With these two modules, we can not only significantly speed up existing top-performing models (e.g., 41x than GC-Net, 4x than PSMNet and 38x than GA-Net), but also improve the performance of fast stereo models (e.g., StereoNet). We also achieve competitive results on Scene Flow and KITTI datasets while running at 62ms, demonstrating the versatility and high efficiency of the proposed method. Our full framework is available at https://github.com/haofeixu/aanet.",
            "year": 2020,
            "venue": "Computer Vision and Pattern Recognition",
            "authors": [
              {
                "authorId": "2108835907",
                "name": "Haofei Xu"
              },
              {
                "authorId": "2108487442",
                "name": "Juyong Zhang"
              }
            ]
          }
        },
        {
          "citedcorpusid": 222319014,
          "isinfluential": true,
          "contexts": [
            "LTC further enhances its ability by integrating f into the time constant of the system:\ndx(t)\ndt = −\n[ 1\nτ + f(x(t), I(t), t, θ)\n] x(t)+\nf(x(t), I(t), t, θ)A\n(3)\nwhere the system time constant becomes an inputdependent term τ1+τf(x(t),I(t),t,θ) and A is a scale parameter.",
            "The liquid time-constant network (LTC) [18,26], an expansion of the continuous time RNN [13], circumvents this problem by using continuous valued activation functions for its neuron, whose dynamics is modulated by an inputdependent system time constant.",
            "We term both convolution LTC (convLTC) and convolution LTC without reversal potential (convLTCOR) as continuous time convolution (CTC).",
            "Note that for CTC, we use simulation results from the convLTCOR model.",
            "The dynamics of the convLTCOR model is mainly characterized by its membrane time constant τm, abstracted from this intuition, we develop the discrete time convolution model (DTC), formulated as:\nxtcij = σ(τcx t−1 cij + Icij(t)) (9)\nwhere Icij(t) is defined the same as in Eq.",
            "The LTC network [18] is an expansion of continuoustime RNN (CT-RNN) [13], which can be described by an ordinary differential equation (ODE):\ndx(t) dt = −x(t) τ + f(x(t), I(t), t, θ) (2)\nwhere τ characterizes the speed and the coupling sensitivity of the dynamical system, x(t) is the hidden state, I(t) is the input, t represents time and f is a neural network parameterized by θ.",
            "In the fully connected structure, the synaptic input of an LTC neuron contains inputs from all the other neurons.",
            "The LTC network was only applied for low dimensional temporal sequence modeling.",
            "Empirically we found that the training of the convLTC model was unstable, during which gradients sometimes tended to vanish.",
            "The resulting convolution LTC neuron and its simplified version can be formulated as:\ndxcij(t)\ndt =−\n[ 1\nτm,c +\nIcij(t)\nCm,c\n] xcij(t)\n+ Icij(t)\nCm,c Erev,c + Eleak,c τm,c\n(6)\ndxcij(t)\ndt = Eleak,c − xcij(t) τm,c + Icij(t) Cm,c (7) Icij(t) = ∑ h ∑ k wchkP t h+i,k+j (8)\nwith Wg(t) in the previous section specified by Icij(t), which represents the convolution input on channel c at location i, j from the event frame pre-processed by SBT, h and k are spatial coordinates on the input plane.",
            "However, the LTC was only applied for low dimensional temporal sequence modeling and it lacks the ability to encode high dimensional spatial features.",
            "The output of the LTC neuron is normalized by a parametrical sigmoid function σ(xcij) = 1/(1 + exp(γc(μc − xcij))), where γc and μc are trainable parameters that scale and shift xcij .",
            "In the original work, the LTC neuron was evolved at a frequency six times higher than the input sampling rate, leading to a six times slower output rate for an equal temporal span of the input.",
            "We develop continuous time convolution (CTC), an expansion of LTC, for encoding high dimensional spatial-temporal data."
          ],
          "intents": [
            "--",
            "['background']",
            "--",
            "--",
            "--",
            "--",
            "--",
            "--",
            "--",
            "--",
            "--",
            "--",
            "--",
            "--"
          ],
          "cited_paper_info": {
            "title": "Neural circuit policies enabling auditable autonomy",
            "abstract": "",
            "year": 2020,
            "venue": "Nature Machine Intelligence",
            "authors": [
              {
                "authorId": "39083616",
                "name": "Mathias Lechner"
              },
              {
                "authorId": "8252176",
                "name": "Ramin M. Hasani"
              },
              {
                "authorId": "2056330",
                "name": "Alexander Amini"
              },
              {
                "authorId": "1710285",
                "name": "T. Henzinger"
              },
              {
                "authorId": "145944286",
                "name": "D. Rus"
              },
              {
                "authorId": "1787208",
                "name": "R. Grosu"
              }
            ]
          }
        },
        {
          "citedcorpusid": 226976144,
          "isinfluential": false,
          "contexts": [
            "There has been an increasing number of applications of SNNs in deep learning [5, 12, 21, 22, 28, 38, 45, 50, 54, 60], and the network’s asynchronous nature makes it an ideal solution for event-based tasks [7, 8, 23, 27, 41, 57]."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Incorporating Learnable Membrane Time Constant to Enhance Learning of Spiking Neural Networks",
            "abstract": "Spiking Neural Networks (SNNs) have attracted enormous research interest due to temporal information processing capability, low power consumption, and high biological plausibility. However, the formulation of efficient and high-performance learning algorithms for SNNs is still challenging. Most existing learning methods learn weights only, and require manual tuning of the membrane-related parameters that determine the dynamics of a single spiking neuron. These parameters are typically chosen to be the same for all neurons, which limits the diversity of neurons and thus the expressiveness of the resulting SNNs. In this paper, we take inspiration from the observation that membrane-related parameters are different across brain regions, and propose a training algorithm that is capable of learning not only the synaptic weights but also the membrane time constants of SNNs. We show that incorporating learnable membrane time constants can make the network less sensitive to initial values and can speed up learning. In addition, we reevaluate the pooling methods in SNNs and find that max-pooling will not lead to significant information loss and have the advantage of low computation cost and binary compatibility. We evaluate the proposed method for image classification tasks on both traditional static MNIST, Fashion-MNIST, CIFAR-10 datasets, and neuromorphic N-MNIST, CIFAR10-DVS, DVS128 Gesture datasets. The experiment results show that the proposed method outperforms the state-of-the-art accuracy on nearly all datasets, using fewer time-steps. Our codes are available at https://github.com/fangwei123456/Parametric-Leaky-Integrate-and-Fire-Spiking-Neuron.",
            "year": 2020,
            "venue": "IEEE International Conference on Computer Vision",
            "authors": [
              {
                "authorId": "2087000501",
                "name": "Wei Fang"
              },
              {
                "authorId": "1746114",
                "name": "Zhaofei Yu"
              },
              {
                "authorId": "2115935255",
                "name": "Yanqing Chen"
              },
              {
                "authorId": "2441104",
                "name": "T. Masquelier"
              },
              {
                "authorId": "34097174",
                "name": "Tiejun Huang"
              },
              {
                "authorId": "40161651",
                "name": "Yonghong Tian"
              }
            ]
          }
        },
        {
          "citedcorpusid": 231759393,
          "isinfluential": false,
          "contexts": [
            "Inspired by recent studies [2,6,42,48], we further develop a dual-path structure for feature embedding fused by SPADE with multi-scale dilated convolution."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "SPADE-E2VID: Spatially-Adaptive Denormalization for Event-Based Video Reconstruction",
            "abstract": "Event-based cameras have several advantages over traditional cameras that shoot videos in frames. Event cameras have a high temporal resolution, high dynamic range, and almost non-existence of blurriness. The data that is produced by event sensors forms a chain of events when a change in brightness is reported in each pixel. This feature makes it difficult to directly apply existing algorithms and take advantage of the event camera data. Due to the developments in neural networks, important advances were made in event-based image reconstruction. Even though these neural networks achieve precise reconstructions while preserving most of the properties of the event cameras, there is still an initialization time that needs to have the highest possible quality in the reconstructed frames. In this work, we present the SPADE-E2VID neural network model that improves the quality of early frames in an event-based reconstructed video, as well as the overall contrast. The SPADE-E2VID model improves the quality of the first reconstructed frames by 15.87% for MSE error, 4.15% for SSIM, and 2.5% in LPIPS. In addition, the SPADE layer in our model allows training our model to reconstruct videos without a temporal loss function. Another advantage of our model is that it has a faster training time. In a many-to-one training style, we avoid running the loss function at each step, executing the loss function at the end of each loop only once. In the present work, we also carried out experiments with event cameras that do not have polarity data. Our model produces quality video reconstructions with non-polarity events in HD resolution (1200 $\\times $ 800). The Video, the code, and the datasets will be available at: https://github.com/RodrigoGantier/SPADE_E2VID.",
            "year": 2021,
            "venue": "IEEE Transactions on Image Processing",
            "authors": [
              {
                "authorId": "1443786569",
                "name": "Pablo Rodrigo Gantier Cadena"
              },
              {
                "authorId": "22187872",
                "name": "Yeqiang Qian"
              },
              {
                "authorId": "47073793",
                "name": "Chunxiang Wang"
              },
              {
                "authorId": "50367252",
                "name": "Ming Yang"
              }
            ]
          }
        },
        {
          "citedcorpusid": 235078812,
          "isinfluential": false,
          "contexts": [
            "An extension of our principles to SNNs implemented in neuromorphic hardware [10, 11, 14, 24, 33, 43] could further lead to super fast event-based stereo system [3]."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Surrogate gradients for analog neuromorphic computing",
            "abstract": "Significance Neuromorphic systems aim to accomplish efficient computation in electronics by mirroring neurobiological principles. Taking advantage of neuromorphic technologies requires effective learning algorithms capable of instantiating high-performing neural networks, while also dealing with inevitable manufacturing variations of individual components, such as memristors or analog neurons. We present a learning framework resulting in bioinspired spiking neural networks with high performance, low inference latency, and sparse spike-coding schemes, which also self-corrects for device mismatch. We validate our approach on the BrainScaleS-2 analog spiking neuromorphic system, demonstrating state-of-the-art accuracy, low latency, and energy efficiency. Our work sketches a path for building powerful neuromorphic processors that take advantage of emerging analog technologies. To rapidly process temporal information at a low metabolic cost, biological neurons integrate inputs as an analog sum, but communicate with spikes, binary events in time. Analog neuromorphic hardware uses the same principles to emulate spiking neural networks with exceptional energy efficiency. However, instantiating high-performing spiking networks on such hardware remains a significant challenge due to device mismatch and the lack of efficient training algorithms. Surrogate gradient learning has emerged as a promising training strategy for spiking networks, but its applicability for analog neuromorphic systems has not been demonstrated. Here, we demonstrate surrogate gradient learning on the BrainScaleS-2 analog neuromorphic system using an in-the-loop approach. We show that learning self-corrects for device mismatch, resulting in competitive spiking network performance on both vision and speech benchmarks. Our networks display sparse spiking activity with, on average, less than one spike per hidden neuron and input, perform inference at rates of up to 85,000 frames per second, and consume less than 200 mW. In summary, our work sets several benchmarks for low-energy spiking network processing on analog neuromorphic hardware and paves the way for future on-chip learning algorithms.",
            "year": 2020,
            "venue": "Proceedings of the National Academy of Sciences of the United States of America",
            "authors": [
              {
                "authorId": "2065211145",
                "name": "Benjamin Cramer"
              },
              {
                "authorId": "2332613",
                "name": "Sebastian Billaudelle"
              },
              {
                "authorId": "2095219712",
                "name": "Simeon Kanya"
              },
              {
                "authorId": "1748958360",
                "name": "Aron Leibfried"
              },
              {
                "authorId": "1748957198",
                "name": "Andreas Grubl"
              },
              {
                "authorId": "9917967",
                "name": "V. Karasenko"
              },
              {
                "authorId": "51506711",
                "name": "Christian Pehle"
              },
              {
                "authorId": "51006272",
                "name": "Korbinian Schreiber"
              },
              {
                "authorId": "51498544",
                "name": "Yannik Stradmann"
              },
              {
                "authorId": "2062874332",
                "name": "Johannes Weis"
              },
              {
                "authorId": "2138905",
                "name": "J. Schemmel"
              },
              {
                "authorId": "2915923",
                "name": "F T Zenke"
              }
            ]
          }
        },
        {
          "citedcorpusid": 247675601,
          "isinfluential": false,
          "contexts": [
            "There has been an increasing number of applications of SNNs in deep learning [5, 12, 21, 22, 28, 38, 45, 50, 54, 60], and the network’s asynchronous nature makes it an ideal solution for event-based tasks [7, 8, 23, 27, 41, 57]."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Cortical oscillations support sampling-based computations in spiking neural networks",
            "abstract": "Being permanently confronted with an uncertain world, brains have faced evolutionary pressure to represent this uncertainty in order to respond appropriately. Often, this requires visiting multiple interpretations of the available information or multiple solutions to an encountered problem. This gives rise to the so-called mixing problem: since all of these “valid” states represent powerful attractors, but between themselves can be very dissimilar, switching between such states can be difficult. We propose that cortical oscillations can be effectively used to overcome this challenge. By acting as an effective temperature, background spiking activity modulates exploration. Rhythmic changes induced by cortical oscillations can then be interpreted as a form of simulated tempering. We provide a rigorous mathematical discussion of this link and study some of its phenomenological implications in computer simulations. This identifies a new computational role of cortical oscillations and connects them to various phenomena in the brain, such as sampling-based probabilistic inference, memory replay, multisensory cue combination, and place cell flickering.",
            "year": 2022,
            "venue": "PLoS Comput. Biol.",
            "authors": [
              {
                "authorId": "1644760229",
                "name": "Agnes Korcsak-Gorzo"
              },
              {
                "authorId": "1683088024",
                "name": "Michael G. Müller"
              },
              {
                "authorId": "46708045",
                "name": "A. Baumbach"
              },
              {
                "authorId": "48205902",
                "name": "Luziwei Leng"
              },
              {
                "authorId": "2377454",
                "name": "O. Breitwieser"
              },
              {
                "authorId": "3077630",
                "name": "Sacha Jennifer van Albada"
              },
              {
                "authorId": "1719636",
                "name": "W. Senn"
              },
              {
                "authorId": "36245042",
                "name": "K. Meier"
              },
              {
                "authorId": "2073142",
                "name": "R. Legenstein"
              },
              {
                "authorId": "2238320235",
                "name": "Mihai A. Petrovici"
              }
            ]
          }
        }
      ]
    },
    "46937991": {
      "citing_paper_info": {
        "title": "A Low Power, High Throughput, Fully Event-Based Stereo System",
        "abstract": "We introduce a stereo correspondence system implemented fully on event-based digital hardware, using a fully graph-based non von-Neumann computation model, where no frames, arrays, or any other such data-structures are used. This is the first time that an end-to-end stereo pipeline from image acquisition and rectification, multi-scale spatiotemporal stereo correspondence, winner-take-all, to disparity regularization is implemented fully on event-based hardware. Using a cluster of TrueNorth neurosynaptic processors, we demonstrate their ability to process bilateral event-based inputs streamed live by Dynamic Vision Sensors (DVS), at up to 2,000 disparity maps per second, producing high fidelity disparities which are in turn used to reconstruct, at low power, the depth of events produced from rapidly changing scenes. Experiments on real-world sequences demonstrate the ability of the system to take full advantage of the asynchronous and sparse nature of DVS sensors for low power depth reconstruction, in environments where conventional frame-based cameras connected to synchronous processors would be inefficient for rapidly moving objects. System evaluation on event-based sequences demonstrates a ~ 200 Ã— improvement in terms of power per pixel per disparity map compared to the closest state-of-the-art, and maximum latencies of up to 11ms from spike injection to disparity map ejection.",
        "year": 2018,
        "venue": "2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition",
        "authors": [
          {
            "authorId": "2542089",
            "name": "Alexander Andreopoulos"
          },
          {
            "authorId": "2353154",
            "name": "H. Kashyap"
          },
          {
            "authorId": "103221704",
            "name": "T. Nayak"
          },
          {
            "authorId": "1767364",
            "name": "A. Amir"
          },
          {
            "authorId": "1712991",
            "name": "M. Flickner"
          }
        ]
      },
      "citation_summary": {
        "citation_count": 24,
        "unique_cited_count": 24,
        "influential_count": 3,
        "detailed_records_count": 24
      },
      "cited_papers": [
        "15357188",
        "11759366",
        "8920227",
        "45560068",
        "8415966",
        "15077875",
        "11395394",
        "24007071",
        "16160208",
        "121601380",
        "6079544",
        "185541",
        "21539113",
        "14915763",
        "12248226",
        "35157264",
        "30913835",
        "24236495",
        "12986049",
        "14542261",
        "34855834",
        "195859047",
        "6913648",
        "22330500"
      ],
      "citation_details": [
        {
          "citedcorpusid": 185541,
          "isinfluential": false,
          "contexts": [
            "[59, 57] propose a cost function for the rotating stereo panorama setup in [7] based on temporal event difference."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Event-driven stereo matching for real-time 3D panoramic vision",
            "abstract": "",
            "year": 2015,
            "venue": "Computer Vision and Pattern Recognition",
            "authors": [
              {
                "authorId": "2521747",
                "name": "S. Schraml"
              },
              {
                "authorId": "1768812",
                "name": "A. Belbachir"
              },
              {
                "authorId": "144746444",
                "name": "H. Bischof"
              }
            ]
          }
        },
        {
          "citedcorpusid": 6079544,
          "isinfluential": false,
          "contexts": [
            "However, most of the recent event-based implementations of the cooperative algorithm do not consider depth gradients [47, 48, 23, 17]."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Asynchronous Stereo Vision for Event-Driven Dynamic Stereo Sensor Using an Adaptive Cooperative Approach",
            "abstract": "",
            "year": 2013,
            "venue": "2013 IEEE International Conference on Computer Vision Workshops",
            "authors": [
              {
                "authorId": "47105977",
                "name": "Ewa Piatkowska"
              },
              {
                "authorId": "1768812",
                "name": "A. Belbachir"
              },
              {
                "authorId": "1990797",
                "name": "M. Gelautz"
              }
            ]
          }
        },
        {
          "citedcorpusid": 6913648,
          "isinfluential": false,
          "contexts": [
            "CNNs [35] have been used to learn stereo matching cost [66, 46]."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Stereo Matching by Training a Convolutional Neural Network to Compare Image Patches",
            "abstract": "We present a method for extracting depth information from a rectified image pair. Our approach focuses on the first stage of many stereo algorithms: the matching cost computation. We approach the problem by learning a similarity measure on small image patches using a convolutional neural network. Training is carried out in a supervised manner by constructing a binary classification data set with examples of similar and dissimilar pairs of patches. We examine two network architectures for this task: one tuned for speed, the other for accuracy. The output of the convolutional neural network is used to initialize the stereo matching cost. A series of post-processing steps follow: cross-based cost aggregation, semiglobal matching, a left-right consistency check, subpixel enhancement, a median filter, and a bilateral filter. We evaluate our method on the KITTI 2012, KITTI 2015, and Middlebury stereo data sets and show that it outperforms other approaches on all three data sets.",
            "year": 2015,
            "venue": "Journal of machine learning research",
            "authors": [
              {
                "authorId": "3105120",
                "name": "Jure Zbontar"
              },
              {
                "authorId": "1688882",
                "name": "Yann LeCun"
              }
            ]
          }
        },
        {
          "citedcorpusid": 8415966,
          "isinfluential": false,
          "contexts": [
            "[59, 57] propose a cost function for the rotating stereo panorama setup in [7] based on temporal event difference.",
            "[7] use a rotating pair of event-based line (vertical) sensors in static scenes and render events from each rotation to an edge map [33], which is subsequently processed using a frame-based panoramic stereo algorithm [36]."
          ],
          "intents": [
            "['background']",
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "A Novel HDR Depth Camera for Real-Time 3D 360° Panoramic Vision",
            "abstract": "",
            "year": 2014,
            "venue": "2014 IEEE Conference on Computer Vision and Pattern Recognition Workshops",
            "authors": [
              {
                "authorId": "1768812",
                "name": "A. Belbachir"
              },
              {
                "authorId": "2521747",
                "name": "S. Schraml"
              },
              {
                "authorId": "20621831",
                "name": "Manfred Mayerhofer"
              },
              {
                "authorId": "145863753",
                "name": "M. Hofstätter"
              }
            ]
          }
        },
        {
          "citedcorpusid": 8920227,
          "isinfluential": false,
          "contexts": [
            "While the successful artificial neural networks may not operate the same way as the brain, both of them utilize highly parallel and hierarchical architectures that gradually abstract input data to more meaningful concepts [8, 51, 16]."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Hierarchical models of object recognition in cortex",
            "abstract": "",
            "year": 1999,
            "venue": "Nature Neuroscience",
            "authors": [
              {
                "authorId": "1996960",
                "name": "M. Riesenhuber"
              },
              {
                "authorId": "1685292",
                "name": "T. Poggio"
              }
            ]
          }
        },
        {
          "citedcorpusid": 11395394,
          "isinfluential": false,
          "contexts": [
            "The proposed method and its FPGA implementations [20, 19] are equivalent to the cooperative stereo algorithm [42] with noisy time difference inputs."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "A High-Performance Hardware Architecture for a Frameless Stereo Vision Algorithm Implemented on a FPGA Platform",
            "abstract": "",
            "year": 2014,
            "venue": "2014 IEEE Conference on Computer Vision and Pattern Recognition Workshops",
            "authors": [
              {
                "authorId": "2435053",
                "name": "F. Eibensteiner"
              },
              {
                "authorId": "1824241",
                "name": "J. Kogler"
              },
              {
                "authorId": "2662311",
                "name": "J. Scharinger"
              }
            ]
          }
        },
        {
          "citedcorpusid": 11759366,
          "isinfluential": false,
          "contexts": [
            "This consists of systems of equations defining the behavior of TrueNorth neurons, encased in modules called corelets [1], and the subsequent composition of the inputs and outputs of these modules.",
            "TrueNorth programs are written in the Corelet Programming Language — a hierarchical, compositional, object-oriented language [1]."
          ],
          "intents": [
            "['background']",
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Et al",
            "abstract": "",
            "year": 2008,
            "venue": "Archives de pédiatrie",
            "authors": [
              {
                "authorId": "2059358552",
                "name": "P. Cochat"
              },
              {
                "authorId": "13267685",
                "name": "L. Vaucoret"
              },
              {
                "authorId": "2097644863",
                "name": "J. Sarles"
              }
            ]
          }
        },
        {
          "citedcorpusid": 12248226,
          "isinfluential": false,
          "contexts": [
            "For autonomous vehicles, drones, and satellites, energy consumption is a challenge [6]."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Autonomous Vehicle Technologies for Small Fixed-Wing UAVs",
            "abstract": "Autonomous unmanned air vehicle ∞ight control systems require robust path generation to account for terrain obstructions, weather, and moving threats such as radar, jammers, and unfriendly aircraft. In this paper, we outline a feasible, hierarchal approach for real-time motion planning of small autonomous flxed-wing UAVs. The approach divides the trajectory generation into four tasks: waypoint path planning, dynamic trajectory smoothing, trajectory tracking, and low-level autopilot compensation. The waypoint path planner determines the vehicle’s route without regard for the dynamic constraints of the vehicle. This results in a signiflcant reduction in the path search space, enabling the generation of complicated paths that account for pop-up and dynamically moving threats. Kinematic constraints are satisfled using a trajectory smoother which has the same kinematic structure as the physical vehicle. The third step of the approach uses a novel tracking algorithm to generate a feasible state trajectory that can be followed by a standard autopilot. Monte-Carlo simulations were done to analyze the performance and feasibility of the approach and determine real-time computation requirements. A planar version of the algorithm has also been implemented and tested in a low-cost micro-controller. The paper describes a custom UAV built to test the algorithms.",
            "year": 2003,
            "venue": "Journal of Aerospace Computing Information and Communication",
            "authors": [
              {
                "authorId": "1762477",
                "name": "R. Beard"
              },
              {
                "authorId": "6041339",
                "name": "Derek B. Kingston"
              },
              {
                "authorId": "39100828",
                "name": "M. Quigley"
              },
              {
                "authorId": "34583609",
                "name": "D. Snyder"
              },
              {
                "authorId": "143950515",
                "name": "Reed Christiansen"
              },
              {
                "authorId": "2116428309",
                "name": "Walt Johnson"
              },
              {
                "authorId": "3248423",
                "name": "T. McLain"
              },
              {
                "authorId": "7410831",
                "name": "M. Goodrich"
              }
            ]
          }
        },
        {
          "citedcorpusid": 12986049,
          "isinfluential": false,
          "contexts": [
            "Ground truth disparity maps from benchmark frame-based datasets [27, 54, 26, 43] are used to train these Figure 1."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Object scene flow for autonomous vehicles",
            "abstract": "",
            "year": 2015,
            "venue": "Computer Vision and Pattern Recognition",
            "authors": [
              {
                "authorId": "101841672",
                "name": "Moritz Menze"
              },
              {
                "authorId": "47237027",
                "name": "Andreas Geiger"
              }
            ]
          }
        },
        {
          "citedcorpusid": 14542261,
          "isinfluential": false,
          "contexts": [
            "CNNs [35] have been used to learn stereo matching cost [66, 46]."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Gradient-based learning applied to document recognition",
            "abstract": "Multilayer neural networks trained with the back-propagation algorithm constitute the best example of a successful gradient based learning technique. Given an appropriate network architecture, gradient-based learning algorithms can be used to synthesize a complex decision surface that can classify high-dimensional patterns, such as handwritten characters, with minimal preprocessing. This paper reviews various methods applied to handwritten character recognition and compares them on a standard handwritten digit recognition task. Convolutional neural networks, which are specifically designed to deal with the variability of 2D shapes, are shown to outperform all other techniques. Real-life document recognition systems are composed of multiple modules including field extraction, segmentation recognition, and language modeling. A new learning paradigm, called graph transformer networks (GTN), allows such multimodule systems to be trained globally using gradient-based methods so as to minimize an overall performance measure. Two systems for online handwriting recognition are described. Experiments demonstrate the advantage of global training, and the flexibility of graph transformer networks. A graph transformer network for reading a bank cheque is also described. It uses convolutional neural network character recognizers combined with global training techniques to provide record accuracy on business and personal cheques. It is deployed commercially and reads several million cheques per day.",
            "year": 1998,
            "venue": "Proceedings of the IEEE",
            "authors": [
              {
                "authorId": "1688882",
                "name": "Yann LeCun"
              },
              {
                "authorId": "52184096",
                "name": "L. Bottou"
              },
              {
                "authorId": "1751762",
                "name": "Yoshua Bengio"
              },
              {
                "authorId": "1721248",
                "name": "P. Haffner"
              }
            ]
          }
        },
        {
          "citedcorpusid": 14915763,
          "isinfluential": false,
          "contexts": [
            "Ground truth disparity maps from benchmark frame-based datasets [27, 54, 26, 43] are used to train these Figure 1."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "High-Resolution Stereo Datasets with Subpixel-Accurate Ground Truth",
            "abstract": "",
            "year": 2014,
            "venue": "German Conference on Pattern Recognition",
            "authors": [
              {
                "authorId": "1709053",
                "name": "D. Scharstein"
              },
              {
                "authorId": "3335378",
                "name": "H. Hirschmüller"
              },
              {
                "authorId": "10396330",
                "name": "York Kitajima"
              },
              {
                "authorId": "2537271",
                "name": "Greg Krathwohl"
              },
              {
                "authorId": "3669531",
                "name": "Nera Nesic"
              },
              {
                "authorId": "2108249608",
                "name": "Xi Wang"
              },
              {
                "authorId": "47986907",
                "name": "P. Westling"
              }
            ]
          }
        },
        {
          "citedcorpusid": 15077875,
          "isinfluential": false,
          "contexts": [
            "Mahowald and Delbrück [41] implemented the Marr and Poggio cooperative stereo algorithm [42], a global approach, in an analog VLSI circuit.",
            "Mahowald and Delbrück [41] implemented the Marr and Poggio cooperative stereo algorithm [42], a global approach, in an analog VLSI circuit."
          ],
          "intents": [
            "--",
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Cooperative Stereo Matching Using Static and Dynamic Image Features",
            "abstract": "",
            "year": 1989,
            "venue": "Analog VLSI Implementation of Neural Systems",
            "authors": [
              {
                "authorId": "38086808",
                "name": "M. Mahowald"
              },
              {
                "authorId": "1694635",
                "name": "T. Delbrück"
              }
            ]
          }
        },
        {
          "citedcorpusid": 15357188,
          "isinfluential": true,
          "contexts": [
            "[60, 58] propose DSP implementation of a spatiotemporal similarity method using two live event sensors [37].",
            "Recently developed event-based cameras such as Dynamic Vision Sensor (DVS) [37, 10] and ATIS [50], inspired by the biological retina, encode pixel illumination changes as events.",
            "Using a cluster of TrueNorth neurosynaptic processors, we demonstrate their ability to process bilateral event-based inputs streamed live by Dynamic Vision Sensors (DVS), at up to 2,000 disparity maps per second, producing high fidelity disparities which are in turn used to reconstruct, at low power, the depth of events produced from rapidly changing scenes.",
            "The proposed event-based disparity method is implemented using a stereo pair of DAVIS sensors [10] (a version of DVS) and nine TrueNorth NS1e boards [53].",
            "A live-feed version of the system running on nine TrueNorth chips is shown to calculate 400 disparity maps per second, and the ability to increase this up to 2,000 disparities per second (subject to certain trade-offs) is demonstrated, for use with high speed event cameras, such as DVS.",
            "When the data in a cycle is sparse, as is the case with a DVS sensor, most neurons would not compute for most of the time, resulting in low active power [44].",
            "Experiments on real-world sequences demonstrate the ability of the system to take full advantage of the asynchronous and sparse nature of DVS sensors for low power depth reconstruction, in environments where conventional frame-based cameras connected to synchronous processors would be inefficient for rapidly moving objects.",
            "The system is highly parameterized and can operate with other event based sensors such as ATIS [50] or DVS [37]."
          ],
          "intents": [
            "['methodology']",
            "['background']",
            "--",
            "--",
            "--",
            "--",
            "--",
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "A 128×128 120db 30mw Asynchronous Vision Sensor That Responds to Relative Intensity Change",
            "abstract": "The frame-based architectures of most imagers are natural for making movies and pictures, they have significant drawbacks for machine vision. Short-latency vision problems require high frame rate, producing massive readout (e.g., >1GB/s from 352×288 pixels at 10kFrames/s [1]). Reducing the output to a manageable rate by using region-of-interest readout usually requires complex control strategies. Readout and processing of largely redundant data ultimately limit reductions in computational effort and power consumption. In this paper, a vision sensor is presented whose pixels asynchronously respond to events that represent relative changes in intensity. It operates largely independent of scene illumination, directly encodes object reflectance, and reduces redundancy while preserving precise timing information. Because output bandwidth is automatically dedicated to dynamic parts of the scene, the sensor is suitable for applications in surveillance and motion analysis. It improves on prior frame-based temporal difference detection imagers (e.g., [2]) by asynchronously responding to temporal contrast rather than absolute illumination, and on prior event-based imagers because they either do not reduce redundancy at all [3], reduce only spatial redundancy [4], have large FPN, slow response, and limited dynamic range [5], or have low contrast sensitivity [6].",
            "year": 2005,
            "venue": "",
            "authors": [
              {
                "authorId": "1744964",
                "name": "P. Lichtsteiner"
              },
              {
                "authorId": "2368354141",
                "name": "C. Posch"
              },
              {
                "authorId": "5548576",
                "name": "T. Delbruck"
              }
            ]
          }
        },
        {
          "citedcorpusid": 16160208,
          "isinfluential": false,
          "contexts": [
            "[52, 14] propose to use event-toevent constraints for calculating matching cost, such as time window, distance to the epipolar line, ordering constraint, and polarity."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Asynchronous Event-Based 3D Reconstruction From Neuromorphic Retinas",
            "abstract": "This paper presents a novel N-ocular 3D reconstruction algorithm for event-based vision data from bio-inspired artiﬁcial retina sensors. Artiﬁcial resti-nas capture visual information asynchronously and encode it into streams of asynchronous spike-like pulse signals carrying information on e.g. temporal contrast events in the scene. The precise time of the occurence of these visual features are implicitly encoded in the spike timings. Due to the high temporal resolution of the asynchronous visual information acquisition, the output of these sensors is ideally suited for dynamic 3D reconstruction. The presented technique takes full beneﬁt of the event-driven operation, i.e. events are processed individually at the moment they arrive. This strategy allows to preserve the original dynamics of the scene, hence allowing for more robust 3D reconstructions. As opposed to existing techniques, this algorithm is based on geometric and time constraints alone, making it particularly simple to implement and largely linear.",
            "year": 2013,
            "venue": "",
            "authors": [
              {
                "authorId": "2057119545",
                "name": "J. Carneiro"
              },
              {
                "authorId": "144975525",
                "name": "S. Ieng"
              },
              {
                "authorId": "153466606",
                "name": "C. Posch"
              },
              {
                "authorId": "1750848",
                "name": "R. Benosman"
              }
            ]
          }
        },
        {
          "citedcorpusid": 21539113,
          "isinfluential": false,
          "contexts": [
            "[59, 57] propose a cost function for the rotating stereo panorama setup in [7] based on temporal event difference.",
            "The main advantages of the proposed method, compared to the related work [17, 49, 45, 52, 57], are simultaneous end-to-end neuromorphic disparity calculation, low power, high throughput, low latency (9-11 ms), and linear scalability to multiple neuromorphic processors for larger input sizes.",
            "Local methods can be parallelized and find corresponding events using either local features over a spatiotemporal window or event-to-event features [13, 58, 52, 32, 57]."
          ],
          "intents": [
            "['background']",
            "['methodology']",
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "An Event-Driven Stereo System for Real-Time 3-D 360° Panoramic Vision",
            "abstract": "",
            "year": 2016,
            "venue": "IEEE transactions on industrial electronics (1982. Print)",
            "authors": [
              {
                "authorId": "2521747",
                "name": "S. Schraml"
              },
              {
                "authorId": "1768812",
                "name": "A. Belbachir"
              },
              {
                "authorId": "144746444",
                "name": "H. Bischof"
              }
            ]
          }
        },
        {
          "citedcorpusid": 22330500,
          "isinfluential": false,
          "contexts": [
            "To benefit from sparse and asynchronous computation, neuromorphic processors have been developed [44, 24, 30, 9, 56]."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "A wafer-scale neuromorphic hardware system for large-scale neural modeling",
            "abstract": "",
            "year": 2010,
            "venue": "Proceedings of 2010 IEEE International Symposium on Circuits and Systems",
            "authors": [
              {
                "authorId": "2138905",
                "name": "J. Schemmel"
              },
              {
                "authorId": "2553650",
                "name": "Daniel Brüderle"
              },
              {
                "authorId": "34988192",
                "name": "Andreas Grübl"
              },
              {
                "authorId": "34977416",
                "name": "Matthias Hock"
              },
              {
                "authorId": "36245042",
                "name": "K. Meier"
              },
              {
                "authorId": "48226911",
                "name": "S. Millner"
              }
            ]
          }
        },
        {
          "citedcorpusid": 24007071,
          "isinfluential": true,
          "contexts": [
            "Our implementation uses a pair of synchronized DAVIS240C cameras [10], connected via Ethernet to a cluster of TrueNorth NS1e boards (Fig.",
            "Recently developed event-based cameras such as Dynamic Vision Sensor (DVS) [37, 10] and ATIS [50], inspired by the biological retina, encode pixel illumination changes as events.",
            "Using a cluster of TrueNorth neurosynaptic processors, we demonstrate their ability to process bilateral event-based inputs streamed live by Dynamic Vision Sensors (DVS), at up to 2,000 disparity maps per second, producing high fidelity disparities which are in turn used to reconstruct, at low power, the depth of events produced from rapidly changing scenes.",
            "The proposed event-based disparity method is implemented using a stereo pair of DAVIS sensors [10] (a version of DVS) and nine TrueNorth NS1e boards [53].",
            "A live-feed version of the system running on nine TrueNorth chips is shown to calculate 400 disparity maps per second, and the ability to increase this up to 2,000 disparities per second (subject to certain trade-offs) is demonstrated, for use with high speed event cameras, such as DVS.",
            "When the data in a cycle is sparse, as is the case with a DVS sensor, most neurons would not compute for most of the time, resulting in low active power [44].",
            "Experiments on real-world sequences demonstrate the ability of the system to take full advantage of the asynchronous and sparse nature of DVS sensors for low power depth reconstruction, in environments where conventional frame-based cameras connected to synchronous processors would be inefficient for rapidly moving objects.",
            "The system is highly parameterized and can operate with other event based sensors such as ATIS [50] or DVS [37]."
          ],
          "intents": [
            "['methodology']",
            "['background']",
            "--",
            "['methodology']",
            "--",
            "--",
            "--",
            "--"
          ],
          "cited_paper_info": {
            "title": "A 240 × 180 130 dB 3 µs Latency Global Shutter Spatiotemporal Vision Sensor",
            "abstract": "",
            "year": 2014,
            "venue": "IEEE Journal of Solid-State Circuits",
            "authors": [
              {
                "authorId": "2239977",
                "name": "Christian Brandli"
              },
              {
                "authorId": "144246116",
                "name": "R. Berner"
              },
              {
                "authorId": "1779496",
                "name": "Minhao Yang"
              },
              {
                "authorId": "1704961",
                "name": "Shih-Chii Liu"
              },
              {
                "authorId": "5548576",
                "name": "T. Delbruck"
              }
            ]
          }
        },
        {
          "citedcorpusid": 24236495,
          "isinfluential": false,
          "contexts": [
            "[60, 58] propose DSP implementation of a spatiotemporal similarity method using two live event sensors [37].",
            "Local methods can be parallelized and find corresponding events using either local features over a spatiotemporal window or event-to-event features [13, 58, 52, 32, 57]."
          ],
          "intents": [
            "['methodology']",
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Dynamic stereo vision system for real-time tracking",
            "abstract": "",
            "year": 2010,
            "venue": "Proceedings of 2010 IEEE International Symposium on Circuits and Systems",
            "authors": [
              {
                "authorId": "2521747",
                "name": "S. Schraml"
              },
              {
                "authorId": "1768812",
                "name": "A. Belbachir"
              },
              {
                "authorId": "152832347",
                "name": "Nenad Milosevic"
              },
              {
                "authorId": "1680865",
                "name": "Peter Schön"
              }
            ]
          }
        },
        {
          "citedcorpusid": 30913835,
          "isinfluential": false,
          "contexts": [
            "[60, 58] propose DSP implementation of a spatiotemporal similarity method using two live event sensors [37]."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Smartcam for real-time stereo vision - address-event based embedded system",
            "abstract": "We present a novel real-time stereo smart camera for sparse disparity (depth) map estimation of moving objects at up to 200 frames/sec. It is based on a 128x128 pixel asynchronous optical transient sensor, using address-event representation (AER) protocol. An address-event based algorithm for stereo depth calculation including calibration, correspondence and reconstruction processing steps is also presented. Due to the onchip data pre-processing the algorithm can be implemented on a single low-power digital signal processor.",
            "year": 2007,
            "venue": "International Conference on Computer Vision Theory and Applications",
            "authors": [
              {
                "authorId": "2521747",
                "name": "S. Schraml"
              },
              {
                "authorId": "1680865",
                "name": "Peter Schön"
              },
              {
                "authorId": "152832347",
                "name": "Nenad Milosevic"
              }
            ]
          }
        },
        {
          "citedcorpusid": 34855834,
          "isinfluential": true,
          "contexts": [
            "[17] use six SpiNNaker [24] processor boards to implement the cooperative network for 106 × 106 pixels of stereo event data.",
            "The implemented neuromorphic stereo disparity system achieves these advantages, while consuming ∼ 200× less power per pixel per disparity map compared to the stateof-the-art [17].",
            "However, most of the recent event-based implementations of the cooperative algorithm do not consider depth gradients [47, 48, 23, 17].",
            "Most global methods [40, 17, 49, 45] are derived from the Marr and Poggio cooperative stereo algorithm [42].",
            "The main advantages of the proposed method, compared to the related work [17, 49, 45, 52, 57], are simultaneous end-to-end neuromorphic disparity calculation, low power, high throughput, low latency (9-11 ms), and linear scalability to multiple neuromorphic processors for larger input sizes.",
            "With respect to the most relevant state-of-the-art approach [17], our method uses ∼ 200× less power per pixel per disparity map.",
            "The algorithm converges well when object surfaces are fronto-parallel and candidate matches injected to the network are close together [40, 17]."
          ],
          "intents": [
            "['methodology']",
            "['background']",
            "['background']",
            "['methodology']",
            "['methodology']",
            "['methodology']",
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Spiking Cooperative Stereo-Matching at 2 ms Latency with Neuromorphic Hardware",
            "abstract": "",
            "year": 2017,
            "venue": "Living Machines",
            "authors": [
              {
                "authorId": "46775745",
                "name": "G. Dikov"
              },
              {
                "authorId": "145885214",
                "name": "M. Firouzi"
              },
              {
                "authorId": "1685761",
                "name": "Florian Röhrbein"
              },
              {
                "authorId": "3302681",
                "name": "J. Conradt"
              },
              {
                "authorId": "2053647526",
                "name": "Christoph Richter"
              }
            ]
          }
        },
        {
          "citedcorpusid": 35157264,
          "isinfluential": false,
          "contexts": [
            "models, followed by sparse-to-dense conversions [18, 5]."
          ],
          "intents": [
            "--"
          ],
          "cited_paper_info": {
            "title": "Sparse Stereo Disparity Map Densification Using Hierarchical Image Segmentation",
            "abstract": "",
            "year": 2017,
            "venue": "International Symposium on Mathematical Morphology and Its Application to Signal and Image Processing",
            "authors": [
              {
                "authorId": "10782739",
                "name": "S. Drouyer"
              },
              {
                "authorId": "145992742",
                "name": "S. Beucher"
              },
              {
                "authorId": "2538744",
                "name": "M. Bilodeau"
              },
              {
                "authorId": "1859553",
                "name": "M. Moreaud"
              },
              {
                "authorId": "10736353",
                "name": "L. Sorbier"
              }
            ]
          }
        },
        {
          "citedcorpusid": 45560068,
          "isinfluential": false,
          "contexts": [
            "For example, the TrueNorth neuromorphic chip [44] has been used for high throughput Convolutional neural networks (CNNs) [22], character recognition [53], optic flow [11], saliency [3], and gesture recognition [2]."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Event-based optical flow on neuromorphic hardware",
            "abstract": "Event-based sensing, i.e. the asynchronous detection of luminance changes, promises low-energy, high dynamic range, and sparse sensing. This stands in contrast to whole image frame-wise acquisition using standard cameras. Recently, we proposed a novel biologically inspired efficient motion detector for such event-based input streams and demonstrated how a canonical neural circuit can improve such representations using normalization and feedback. In this contribution, we suggest how such a motion detection scheme is defined by utilizing a canonical neural circuit corresponding to the resolution of cortical columns. In addition, we develop a mapping of key computational elements of this circuit model onto neuromorphic hardware. In particular, we focus on the recently developed TrueNorth chip architecture by IBM to realize a real-time, energy-efficient and adjustable neuromorphic optical flow detector. We demonstrate the function of the computations of the canonical model and its approximate neuromorphic realization.",
            "year": 2015,
            "venue": "International Conference on Bio-inspired Information and Communications Technologies",
            "authors": [
              {
                "authorId": "2256291",
                "name": "T. Brosch"
              },
              {
                "authorId": "144718494",
                "name": "H. Neumann"
              }
            ]
          }
        },
        {
          "citedcorpusid": 121601380,
          "isinfluential": false,
          "contexts": [
            "However, most of the recent event-based implementations of the cooperative algorithm do not consider depth gradients [47, 48, 23, 17]."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Cooperative and asynchronous stereo vision for dynamic vision sensors",
            "abstract": "",
            "year": 2014,
            "venue": "",
            "authors": [
              {
                "authorId": "47105977",
                "name": "Ewa Piatkowska"
              },
              {
                "authorId": "1768812",
                "name": "A. Belbachir"
              },
              {
                "authorId": "1990797",
                "name": "M. Gelautz"
              }
            ]
          }
        },
        {
          "citedcorpusid": 195859047,
          "isinfluential": false,
          "contexts": [
            "Frame-based stereo disparity methods calculate matching cost using a spatial similarity metric [25, 27, 29] or a cost function learned from a dataset (see reviews [62, 55, 34, 63])."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "A Taxonomy and Evaluation of Dense Two-Frame Stereo Correspondence Algorithms",
            "abstract": "",
            "year": 2001,
            "venue": "Proceedings IEEE Workshop on Stereo and Multi-Baseline Vision (SMBV 2001)",
            "authors": [
              {
                "authorId": "1709053",
                "name": "D. Scharstein"
              },
              {
                "authorId": "1717841",
                "name": "R. Szeliski"
              }
            ]
          }
        }
      ]
    },
    "259336126": {
      "citing_paper_info": {
        "title": "X-maps: Direct Depth Lookup for Event-based Structured Light Systems",
        "abstract": "We present a new approach to direct depth estimation for Spatial Augmented Reality (SAR) applications using event cameras. These dynamic vision sensors are a great fit to be paired with laser projectors for depth estimation in a structured light approach. Our key contributions involve a conversion of the projector time map into a rectified X-map, capturing x-axis correspondences for incoming events and enabling direct disparity lookup without any additional search. Compared to previous implementations, this significantly simplifies depth estimation, making it more efficient, while the accuracy is similar to the time map-based process. Moreover, we compensate non-linear temporal behavior of cheap laser projectors by a simple time map calibration, resulting in improved performance and increased depth estimation accuracy. Since depth estimation is executed by two lookups only, it can be executed almost instantly (less than 3 ms per frame with a Python implementation) for incoming events. This allows for real-time interactivity and responsiveness, which makes our approach especially suitable for SAR experiences where low latency, high frame rates and direct feedback are crucial. We present valuable insights gained into data transformed into X-maps and evaluate our depth from disparity estimation against the state of the art time map-based results. Additional results and code are available on the X-maps project page.",
        "year": 2023,
        "venue": "2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)",
        "authors": [
          {
            "authorId": "1442117232",
            "name": "W. Morgenstern"
          },
          {
            "authorId": "51267348",
            "name": "N. Gard"
          },
          {
            "authorId": "2221102711",
            "name": "Simon Baumann"
          },
          {
            "authorId": "3353355",
            "name": "A. Hilsmann"
          },
          {
            "authorId": "2516942",
            "name": "P. Eisert"
          }
        ]
      },
      "citation_summary": {
        "citation_count": 8,
        "unique_cited_count": 7,
        "influential_count": 2,
        "detailed_records_count": 8
      },
      "cited_papers": [
        "244729216",
        "53419489",
        "206767633",
        "206428176",
        "239050401",
        "33388297",
        "18327083"
      ],
      "citation_details": [
        {
          "citedcorpusid": 18327083,
          "isinfluential": false,
          "contexts": [
            "The performance is compared to MC3D and Semi-Global Matching (SGM) [9] algorithms: ESL surpasses both MC3D and SGM in static scenes, demonstrating lower error values and superior noise suppression against the baseline."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Ieee Transactions on Pattern Analysis and Machine Intelligence 1 Stereo Processing by Semi-global Matching and Mutual Information",
            "abstract": "",
            "year": null,
            "venue": "",
            "authors": [
              {
                "authorId": "3335378",
                "name": "H. Hirschmüller"
              }
            ]
          }
        },
        {
          "citedcorpusid": 33388297,
          "isinfluential": false,
          "contexts": [
            "tainment [2], industrial applications [1, 6], advertising [19], cultural heritage [22], and healthcare [5]."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Direct augmented reality computed tomographic angiography technique (ARC): an innovation in preoperative imaging",
            "abstract": "",
            "year": 2018,
            "venue": "European journal of plastic surgery",
            "authors": [
              {
                "authorId": "2993154",
                "name": "Michael P. Chae"
              },
              {
                "authorId": "1394124065",
                "name": "Dasun Ganhewa"
              },
              {
                "authorId": "1399949255",
                "name": "D. Hunter-Smith"
              },
              {
                "authorId": "3736907",
                "name": "W. Rozen"
              }
            ]
          }
        },
        {
          "citedcorpusid": 53419489,
          "isinfluential": false,
          "contexts": [
            "tainment [2], industrial applications [1, 6], advertising [19], cultural heritage [22], and healthcare [5]."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Use of projector based augmented reality to improve manual spot-welding precision and accuracy for automotive manufacturing",
            "abstract": "This paper presents the use of a projector-based spatial augmented reality system in an industrial quality assurance setting to highlight spot-weld locations on vehicle panels for manual welding operators. The aim of this work is to improve the precision and accuracy of manual spot-weld placements with the aid of visual cues as a proactive step by the automotive manufacturer to enhance product quality. The prototype system was deployed at General Motors (GM) Holden plant in Elizabeth, Australia on the production line building Holden Cruze vehicles. Production trials were conducted and techniques developed to analyse and validate the precision and accuracy of spot-welds both with and without the visual cues. A reduction of 52 % of the standard deviation of manual spot-weld placement was observed when using augmented reality visual cues. The average standard deviation with-AR assistance (19 panels and 114 spot-welds) was calculated at 1.94 mm compared to without-AR (45 panels and 270 spot-welds) at 4.08 mm. All welds were within the required specification and panels evaluated in this study were used as the final product made available to consumers. The visual cues enabled operators to spot-weld at a higher degree of precision and accuracy.",
            "year": 2017,
            "venue": "",
            "authors": [
              {
                "authorId": "1728368",
                "name": "A. Doshi"
              },
              {
                "authorId": "2109379677",
                "name": "Ross T. Smith"
              },
              {
                "authorId": "143885004",
                "name": "B. Thomas"
              },
              {
                "authorId": "100613744",
                "name": "Con Bouras"
              }
            ]
          }
        },
        {
          "citedcorpusid": 206428176,
          "isinfluential": false,
          "contexts": [
            "Camera-projector systems can be calibrated with established methods such as [13]."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Simple, Accurate, and Robust Projector-Camera Calibration",
            "abstract": "",
            "year": 2012,
            "venue": "2012 Second International Conference on 3D Imaging, Modeling, Processing, Visualization & Transmission",
            "authors": [
              {
                "authorId": "144714552",
                "name": "Daniel Moreno"
              },
              {
                "authorId": "1690237",
                "name": "G. Taubin"
              }
            ]
          }
        },
        {
          "citedcorpusid": 206767633,
          "isinfluential": true,
          "contexts": [
            "Building on MC3D’s groundwork, Event-based Structured Light (ESL) [15] introduces time maps to establish the temporal projector-camera correspondences.",
            "If the MC3D measurements are averaged over a period of 1 second (60 frames) in MC3D-1s, the depth maps become more dense, but still differ a lot from the smoothed ESL depth maps.",
            "MC3D is not able to capture full frames with a frequency of 60 Hz .",
            "Our approach builds on the findings of previous methods [12,15], which combine structured light technology and",
            "The performance is compared to MC3D and Semi-Global Matching (SGM) [9] algorithms: ESL surpasses both MC3D and SGM in static scenes, demonstrating lower error values and superior noise suppression against the baseline.",
            "In ESL, results from a time-averaged MC3D calculation are used as a baseline to compare against.",
            "Motion Contrast 3D (MC3D) [12] introduces the concept of merging single-shot structured light techniques with event-based cameras and addresses the trade-offs between resolution, robustness, and speed in structured light systems.",
            "In comparison to MC3D, we see that X-maps provides depth maps with much higher fill rate and lower RMSE.",
            "At high sampling rates, MC3D’s correspondence search amplifies noise in the event timestamps, resulting in noisy and patchy stereo correspondences.",
            "We found that the refined ESL results (window size W = 7 and denoised) capture the geometry more cleanly than MC3D.",
            "MC3D employs a projector that scans scenes using a single laser beam in a raster pattern.",
            "We compare against MC3D and the initialization step of ESL ( ESL-init ), which uses a row-wise disparity matching and no further optimization."
          ],
          "intents": [
            "--",
            "--",
            "--",
            "['methodology']",
            "--",
            "--",
            "['background']",
            "--",
            "--",
            "--",
            "--",
            "--"
          ],
          "cited_paper_info": {
            "title": "MC3D: Motion Contrast 3D Scanning",
            "abstract": "",
            "year": 2015,
            "venue": "International Conference on Computational Photography",
            "authors": [
              {
                "authorId": "3271808",
                "name": "N. Matsuda"
              },
              {
                "authorId": "1793812",
                "name": "O. Cossairt"
              },
              {
                "authorId": "49744086",
                "name": "Mohit Gupta"
              }
            ]
          }
        },
        {
          "citedcorpusid": 239050401,
          "isinfluential": false,
          "contexts": [
            "A good discussion of the different sources of noise within the camera timestamps can be found in [17].",
            "Foveated rendering in VR headsets is adapted for depth sensing in [17], where the authors develop a foveating"
          ],
          "intents": [
            "['background']",
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Event Guided Depth Sensing",
            "abstract": "Active depth sensors like structured light, lidar, and time-of-flight systems sample the depth of the entire scene uniformly at a fixed scan rate. This leads to limited spatiotemporal resolution where redundant static information is over-sampled and precious motion information might be under-sampled. In this paper, we present an efficient bio-inspired event-camera-driven depth estimation algorithm. In our approach, we dynamically illuminate areas of interest densely, depending on the scene activity detected by the event camera, and sparsely illuminate areas in the field of view with no motion. The depth estimation is achieved by an event-based structured light system consisting of a laser point projector coupled with a second event-based sensor tuned to detect the reflection of the laser from the scene. We show the feasibility of our approach in a simulated autonomous driving scenario and real indoor sequences using our prototype. We show that, in natural scenes like autonomous driving and indoor environments, moving edges correspond to less than 10% of the scene on average. Thus our setup requires the sensor to scan only 10% of the scene, which could lead to almost 90% less power consumption by the illumination source. While we present the evaluation and proof-of-concept for an event-based structured-light system, the ideas presented here are applicable for a wide range of depth sensing modalities like LIDAR, time-of-flight, and standard stereo.",
            "year": 2021,
            "venue": "International Conference on 3D Vision",
            "authors": [
              {
                "authorId": "30871466",
                "name": "Manasi Muglikar"
              },
              {
                "authorId": "2837457",
                "name": "Diederik Paul Moeys"
              },
              {
                "authorId": "2075371",
                "name": "D. Scaramuzza"
              }
            ]
          }
        },
        {
          "citedcorpusid": 244729216,
          "isinfluential": true,
          "contexts": [
            "The increased accuracy in dynamic scenes and robustness of event-based sensing compared to structured light sensors such as RealSense [4] was performed in [15] and is also applicable to our method.",
            "Building on MC3D’s groundwork, Event-based Structured Light (ESL) [15] introduces time maps to establish the temporal projector-camera correspondences.",
            "Our approach builds on the findings of previous methods [12,15], which combine structured light technology and",
            "A rectified time map is used in ESL [15] to match the recorded time in the camera’s time map to the ideal time in a synthesized projector time map, by searching along epipolar lines to minimize the difference in t.",
            "Matching time entries of the map along epipolar lines with an idealized projector time map to compute scene disparity is computationally expensive [15].",
            "In the data recorded for the ESL [15], the projector displayed a comparatively milder nonlinear behavior, resulting in significantly flatter planes even without time map calibration.",
            "We compare our method to the state of the art approach ESL [15] and use the static scenes of the public dataset provided by them."
          ],
          "intents": [
            "['methodology']",
            "['methodology']",
            "['methodology']",
            "['methodology']",
            "['background']",
            "['methodology']",
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "ESL: Event-based Structured Light",
            "abstract": "Event cameras are bio-inspired sensors providing significant advantages over standard cameras such as low latency, high temporal resolution, and high dynamic range. We propose a novel structured-light system using an event camera to tackle the problem of accurate and high-speed depth sensing. Our setup consists of an event camera and a laser-point projector that uniformly illuminates the scene in a raster scanning pattern during 16 ms. Previous methods match events independently of each other, and so they deliver noisy depth estimates at high scanning speeds in the presence of signal latency and jitter. In contrast, we optimize an energy function designed to exploit event correlations, called spatio-temporal consistency. The resulting method is robust to event jitter and therefore performs better at higher scanning speeds. Experiments demonstrate that our method can deal with high-speed motion and outperform state-of-the-art 3D reconstruction methods based on event cameras, reducing the RMSE by 83% on average, for the same acquisition time. Code and dataset are available at http://rpg.ifi.uzh.ch/esl/.",
            "year": 2021,
            "venue": "International Conference on 3D Vision",
            "authors": [
              {
                "authorId": "30871466",
                "name": "Manasi Muglikar"
              },
              {
                "authorId": "144036711",
                "name": "Guillermo Gallego"
              },
              {
                "authorId": "2075371",
                "name": "D. Scaramuzza"
              }
            ]
          }
        },
        {
          "citedcorpusid": null,
          "isinfluential": false,
          "contexts": [
            "We build our demonstrator using the Nebra Anybeam MEMS Laser Projector [18]."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {}
        }
      ]
    },
    "270045246": {
      "citing_paper_info": {
        "title": "EvGGS: A Collaborative Learning Framework for Event-based Generalizable Gaussian Splatting",
        "abstract": "Event cameras offer promising advantages such as high dynamic range and low latency, making them well-suited for challenging lighting conditions and fast-moving scenarios. However, reconstructing 3D scenes from raw event streams is difficult because event data is sparse and does not carry absolute color information. To release its potential in 3D reconstruction, we propose the first event-based generalizable 3D reconstruction framework, called EvGGS, which reconstructs scenes as 3D Gaussians from only event input in a feedforward manner and can generalize to unseen cases without any retraining. This framework includes a depth estimation module, an intensity reconstruction module, and a Gaussian regression module. These submodules connect in a cascading manner, and we collaboratively train them with a designed joint loss to make them mutually promote. To facilitate related studies, we build a novel event-based 3D dataset with various material objects and calibrated labels of grayscale images, depth maps, camera poses, and silhouettes. Experiments show models that have jointly trained significantly outperform those trained individually. Our approach performs better than all baselines in reconstruction quality, and depth/intensity predictions with satisfactory rendering speed.",
        "year": 2024,
        "venue": "International Conference on Machine Learning",
        "authors": [
          {
            "authorId": "2242768739",
            "name": "Jiaxu Wang"
          },
          {
            "authorId": "2281906381",
            "name": "Junhao He"
          },
          {
            "authorId": "2233873709",
            "name": "Ziyi Zhang"
          },
          {
            "authorId": "2284645461",
            "name": "Mingyuan Sun"
          },
          {
            "authorId": "2243709540",
            "name": "Jingkai Sun"
          },
          {
            "authorId": "2224151598",
            "name": "Renjing Xu"
          }
        ]
      },
      "citation_summary": {
        "citation_count": 15,
        "unique_cited_count": 15,
        "influential_count": 3,
        "detailed_records_count": 15
      },
      "cited_papers": [
        "251040986",
        "23102425",
        "257632404",
        "264886560",
        "189998802",
        "220978548",
        "4766599",
        "244707609",
        "210886473",
        "236469482",
        "246285530",
        "235651771",
        "49877954",
        "257232560",
        "251765179"
      ],
      "citation_details": [
        {
          "citedcorpusid": 4766599,
          "isinfluential": true,
          "contexts": [
            "L p is the perceptual loss (Zhang et al., 2018). β 1 , β 2 aim to balance the L 1 and perceptual loss, we constantly set them to 0.8 and 0.2 for all situations."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "The Unreasonable Effectiveness of Deep Features as a Perceptual Metric",
            "abstract": "While it is nearly effortless for humans to quickly assess the perceptual similarity between two images, the underlying processes are thought to be quite complex. Despite this, the most widely used perceptual metrics today, such as PSNR and SSIM, are simple, shallow functions, and fail to account for many nuances of human perception. Recently, the deep learning community has found that features of the VGG network trained on ImageNet classification has been remarkably useful as a training loss for image synthesis. But how perceptual are these so-called \"perceptual losses\"? What elements are critical for their success? To answer these questions, we introduce a new dataset of human perceptual similarity judgments. We systematically evaluate deep features across different architectures and tasks and compare them with classic metrics. We find that deep features outperform all previous metrics by large margins on our dataset. More surprisingly, this result is not restricted to ImageNet-trained VGG features, but holds across different deep architectures and levels of supervision (supervised, self-supervised, or even unsupervised). Our results suggest that perceptual similarity is an emergent property shared across deep visual representations.",
            "year": 2018,
            "venue": "2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition",
            "authors": [
              {
                "authorId": "2844849",
                "name": "Richard Zhang"
              },
              {
                "authorId": "2094770",
                "name": "Phillip Isola"
              },
              {
                "authorId": "1763086",
                "name": "Alexei A. Efros"
              },
              {
                "authorId": "2177801",
                "name": "Eli Shechtman"
              },
              {
                "authorId": "39231399",
                "name": "Oliver Wang"
              }
            ]
          }
        },
        {
          "citedcorpusid": 23102425,
          "isinfluential": false,
          "contexts": [
            "Traditional explicit representation methods include point cloud(Achlioptas et al., 2018), mesh(Liu et al., 2020a), and voxel(Lombardi et al., 2019; Sitzmann et al., 2019)."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Learning Representations and Generative Models for 3D Point Clouds",
            "abstract": "Three-dimensional geometric data offer an excellent domain for studying representation learning and generative modeling. In this paper, we look at geometric data represented as point clouds. We introduce a deep AutoEncoder (AE) network with state-of-the-art reconstruction quality and generalization ability. The learned representations outperform existing methods on 3D recognition tasks and enable shape editing via simple algebraic manipulations, such as semantic part editing, shape analogies and shape interpolation, as well as shape completion. We perform a thorough study of different generative models including GANs operating on the raw point clouds, significantly improved GANs trained in the fixed latent space of our AEs, and Gaussian Mixture Models (GMMs). To quantitatively evaluate generative models we introduce measures of sample fidelity and diversity based on matchings between sets of point clouds. Interestingly, our evaluation of generalization, fidelity and diversity reveals that GMMs trained in the latent space of our AEs yield the best results overall.",
            "year": 2017,
            "venue": "International Conference on Machine Learning",
            "authors": [
              {
                "authorId": "22199114",
                "name": "Panos Achlioptas"
              },
              {
                "authorId": "1868022",
                "name": "Olga Diamanti"
              },
              {
                "authorId": "3168518",
                "name": "Ioannis Mitliagkas"
              },
              {
                "authorId": "51352814",
                "name": "L. Guibas"
              }
            ]
          }
        },
        {
          "citedcorpusid": 49877954,
          "isinfluential": false,
          "contexts": [
            "Dataset Existing event-based 3D datasets such as (Rudnev et al., 2023; Zhou et al., 2018) only contain a limited number of objects and lack high-quality intensity, depth, and mask groundtruths because they mainly concentrate on single scene reconstruction or sparse vision tasks."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Semi-Dense 3D Reconstruction with a Stereo Event Camera",
            "abstract": "Event cameras are bio-inspired sensors that offer several advantages, such as low latency, high-speed and high dynamic range, to tackle challenging scenarios in computer vision. This paper presents a solution to the problem of 3D reconstruction from data captured by a stereo event-camera rig moving in a static scene, such as in the context of stereo Simultaneous Localization and Mapping. The proposed method consists of the optimization of an energy function designed to exploit small-baseline spatio-temporal consistency of events triggered across both stereo image planes. To improve the density of the reconstruction and to reduce the uncertainty of the estimation, a probabilistic depth-fusion strategy is also developed. The resulting method has no special requirements on either the motion of the stereo event-camera rig or on prior knowledge about the scene. Experiments demonstrate our method can deal with both texture-rich scenes as well as sparse scenes, outperforming state-of-the-art stereo methods based on event data image representations.",
            "year": 2018,
            "venue": "European Conference on Computer Vision",
            "authors": [
              {
                "authorId": null,
                "name": "Yi Zhou"
              },
              {
                "authorId": "144036711",
                "name": "Guillermo Gallego"
              },
              {
                "authorId": "3414274",
                "name": "Henri Rebecq"
              },
              {
                "authorId": "1727013",
                "name": "L. Kneip"
              },
              {
                "authorId": "40124570",
                "name": "Hongdong Li"
              },
              {
                "authorId": "2075371",
                "name": "D. Scaramuzza"
              }
            ]
          }
        },
        {
          "citedcorpusid": 189998802,
          "isinfluential": false,
          "contexts": [
            "In this section, we select three popular image recovery algorithms, i.e. E2VID (Rebecq et al., 2019), FireNet (Scheerlinck et al., 2020), and EVSNN (Barchid et al., 2023).",
            "E2VID(Rebecq et al., 2019) introduced a ConvLSTM-based model, facilitating the recovery of high-dynamic video.",
            "While the recovery of intensity images is effective when interpolating between the given intensity images (Wang et al., 2020), the performance dramatically degrades when only the event stream is available (Rebecq et al., 2019)."
          ],
          "intents": [
            "['methodology']",
            "['methodology']",
            "['background']"
          ],
          "cited_paper_info": {
            "title": "High Speed and High Dynamic Range Video with an Event Camera",
            "abstract": "Event cameras are novel sensors that report brightness changes in the form of a stream of asynchronous “events” instead of intensity frames. They offer significant advantages with respect to conventional cameras: high temporal resolution, high dynamic range, and no motion blur. While the stream of events encodes in principle the complete visual signal, the reconstruction of an intensity image from a stream of events is an ill-posed problem in practice. Existing reconstruction approaches are based on hand-crafted priors and strong assumptions about the imaging process as well as the statistics of natural images. In this work we propose to learn to reconstruct intensity images from event streams directly from data instead of relying on any hand-crafted priors. We propose a novel recurrent network to reconstruct videos from a stream of events, and train it on a large amount of simulated event data. During training we propose to use a perceptual loss to encourage reconstructions to follow natural image statistics. We further extend our approach to synthesize color images from color event streams. Our quantitative experiments show that our network surpasses state-of-the-art reconstruction methods by a large margin in terms of image quality (<inline-formula><tex-math notation=\"LaTeX\">$>\\!20\\%$</tex-math><alternatives><mml:math><mml:mrow><mml:mo>></mml:mo><mml:mspace width=\"-0.166667em\"/><mml:mn>20</mml:mn><mml:mo>%</mml:mo></mml:mrow></mml:math><inline-graphic xlink:href=\"rebecq-ieq1-2963386.gif\"/></alternatives></inline-formula>), while comfortably running in real-time. We show that the network is able to synthesize high framerate videos (<inline-formula><tex-math notation=\"LaTeX\">$>5,000$</tex-math><alternatives><mml:math><mml:mrow><mml:mo>></mml:mo><mml:mn>5</mml:mn><mml:mo>,</mml:mo><mml:mn>000</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href=\"rebecq-ieq2-2963386.gif\"/></alternatives></inline-formula> frames per second) of high-speed phenomena (e.g., a bullet hitting an object) and is able to provide high dynamic range reconstructions in challenging lighting conditions. As an additional contribution, we demonstrate the effectiveness of our reconstructions as an intermediate representation for event data. We show that off-the-shelf computer vision algorithms can be applied to our reconstructions for tasks such as object classification and visual-inertial odometry and that this strategy consistently outperforms algorithms that were specifically designed for event data. We release the reconstruction code, a pre-trained model and the datasets to enable further research.",
            "year": 2019,
            "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
            "authors": [
              {
                "authorId": "3414274",
                "name": "Henri Rebecq"
              },
              {
                "authorId": "2774325",
                "name": "René Ranftl"
              },
              {
                "authorId": "145231047",
                "name": "V. Koltun"
              },
              {
                "authorId": "2075371",
                "name": "D. Scaramuzza"
              }
            ]
          }
        },
        {
          "citedcorpusid": 210886473,
          "isinfluential": true,
          "contexts": [
            "Following (Scheerlinck et al., 2020), we set B = 5 in our experiments.",
            "FireNet(Scheerlinck et al., 2020) employs the GRUs to provide a more rapid and lightweight method for event-based video reconstruction.",
            "E2VID and FireNet rely on the recurrent convolution structure while EVSNN is built upon the spiking neural network.",
            "The intensity reconstruction of E2VID is slightly better than FireNet, especially the reconstruction of the ‘Train’ scene, which is quite close to our joint training strategy.",
            "FireNet did not recover the intensity values correctly, and it can be observed that there are severe color bleeding effects in all five test scenes.",
            "In this section, we select three popular image recovery algorithms, i.e. E2VID (Rebecq et al., 2019), FireNet (Scheerlinck et al., 2020), and EVSNN (Barchid et al., 2023)."
          ],
          "intents": [
            "['methodology']",
            "['methodology']",
            "--",
            "--",
            "--",
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Fast Image Reconstruction with an Event Camera",
            "abstract": "Event cameras are powerful new sensors able to capture high dynamic range with microsecond temporal resolution and no motion blur. Their strength is detecting brightness changes (called events) rather than capturing direct brightness images; however, algorithms can be used to convert events into usable image representations for applications such as classification. Previous works rely on hand-crafted spatial and temporal smoothing techniques to reconstruct images from events. State-of-the-art video reconstruction has recently been achieved using neural networks that are large (10M parameters) and computationally expensive, requiring 30ms for a forward-pass at 640 × 480 resolution on a modern GPU. We propose a novel neural network architecture for video reconstruction from events that is smaller (38k vs. 10M parameters) and faster (10ms vs. 30ms) than state-of-the-art with minimal impact to performance.",
            "year": 2020,
            "venue": "IEEE Workshop/Winter Conference on Applications of Computer Vision",
            "authors": [
              {
                "authorId": "51939028",
                "name": "C. Scheerlinck"
              },
              {
                "authorId": "3414274",
                "name": "Henri Rebecq"
              },
              {
                "authorId": "51152279",
                "name": "Daniel Gehrig"
              },
              {
                "authorId": "1712576",
                "name": "Nick Barnes"
              },
              {
                "authorId": "144814747",
                "name": "R. Mahony"
              },
              {
                "authorId": "2075371",
                "name": "D. Scaramuzza"
              }
            ]
          }
        },
        {
          "citedcorpusid": 220978548,
          "isinfluential": false,
          "contexts": [
            "Traditional explicit representation methods include point cloud(Achlioptas et al., 2018), mesh(Liu et al., 2020a), and voxel(Lombardi et al., 2019; Sitzmann et al., 2019)."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "A General Differentiable Mesh Renderer for Image-Based 3D Reasoning",
            "abstract": "Rendering bridges the gap between 2D vision and 3D scenes by simulating the physical process of image formation. By inverting such renderer, one can think of a learning approach to infer 3D information from 2D images. However, standard graphics renderers involve a fundamental step called rasterization, which prevents rendering to be differentiable. Unlike the state-of-the-art differentiable renderers (Kato et al. 2018 and Loper 2018), which only approximate the rendering gradient in the backpropagation, we propose a natually differentiable rendering framework that is able to (1) directly render colorized mesh using differentiable functions and (2) back-propagate efficient supervisions to mesh vertices and their attributes from various forms of image representations. The key to our framework is a novel formulation that views rendering as an aggregation function that fuses the probabilistic contributions of all mesh triangles with respect to the rendered pixels. Such formulation enables our framework to flow gradients to the occluded and distant vertices, which cannot be achieved by the previous state-of-the-arts. We show that by using the proposed renderer, one can achieve significant improvement in 3D unsupervised single-view reconstruction both qualitatively and quantitatively. Experiments also demonstrate that our approach can handle the challenging tasks in image-based shape fitting, which remain nontrivial to existing differentiable renders.",
            "year": 2020,
            "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
            "authors": [
              {
                "authorId": "47130347",
                "name": "Shichen Liu"
              },
              {
                "authorId": "50290121",
                "name": "Tianye Li"
              },
              {
                "authorId": "2535947",
                "name": "Weikai Chen"
              },
              {
                "authorId": "79482877",
                "name": "Hao Li"
              }
            ]
          }
        },
        {
          "citedcorpusid": 235651771,
          "isinfluential": false,
          "contexts": [
            "We employ V2E(Hu et al., 2021) to generate synthetic event streams maintaining default noise configurations."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "v2e: From Video Frames to Realistic DVS Events",
            "abstract": "To help meet the increasing need for dynamic vision sensor (DVS) event camera data, this paper proposes the v2e toolbox that generates realistic synthetic DVS events from intensity frames. It also clarifies incorrect claims about DVS motion blur and latency characteristics in recent literature. Unlike other toolboxes, v2e includes pixel-level Gaussian event threshold mismatch, finite intensity-dependent bandwidth, and intensity-dependent noise. Realistic DVS events are useful in training networks for uncontrolled lighting conditions. The use of v2e synthetic events is demonstrated in two experiments. The first experiment is object recognition with N-Caltech 101 dataset. Results show that pretraining on various v2e lighting conditions improves generalization when transferred on real DVS data for a ResNet model. The second experiment shows that for night driving, a car detector trained with v2e events shows an average accuracy improvement of 40% compared to the YOLOv3 trained on intensity frames.",
            "year": 2021,
            "venue": "2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)",
            "authors": [
              {
                "authorId": "2175899",
                "name": "Yuhuang Hu"
              },
              {
                "authorId": "1704961",
                "name": "Shih-Chii Liu"
              },
              {
                "authorId": "5548576",
                "name": "T. Delbruck"
              }
            ]
          }
        },
        {
          "citedcorpusid": 236469482,
          "isinfluential": false,
          "contexts": [
            "NeuRay(Liu et al., 2022b) implicitly models visibility to deal with occlusion issues."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Neural Rays for Occlusion-aware Image-based Rendering",
            "abstract": "We present a new neural representation, called Neural Ray (NeuRay), for the novel view synthesis task. Recent works construct radiance fields from image features of input views to render novel view images, which enables the generalization to new scenes. However, due to occlusions, a 3D point may be invisible to some input views. On such a 3D point, these generalization methods will include inconsistent image features from invisible views, which interfere with the radiance field construction. To solve this problem, we predict the visibility of 3D points to input views within our NeuRay representation. This visibility enables the radiance field construction to focus on visible image features, which significantly improves its rendering quality. Meanwhile, a novel consistency loss is proposed to refine the visibility in NeuRay when finetuning on a specific scene. Experiments demonstrate that our approach achieves state-of-the-art performance on the novel view synthesis task when generalizing to unseen scenes and outperforms perscene optimization methods after finetuning. Project page:https://liuyuan-pal.github.io/NeuRay/",
            "year": 2021,
            "venue": "Computer Vision and Pattern Recognition",
            "authors": [
              {
                "authorId": "2143861796",
                "name": "Yuan Liu"
              },
              {
                "authorId": "2072712025",
                "name": "Sida Peng"
              },
              {
                "authorId": "46458089",
                "name": "Lingjie Liu"
              },
              {
                "authorId": "2144298123",
                "name": "Qianqian Wang"
              },
              {
                "authorId": "2155300848",
                "name": "Peng Wang"
              },
              {
                "authorId": "1680185",
                "name": "C. Theobalt"
              },
              {
                "authorId": "145453113",
                "name": "Xiaowei Zhou"
              },
              {
                "authorId": "2108349601",
                "name": "Wenping Wang"
              }
            ]
          }
        },
        {
          "citedcorpusid": 244707609,
          "isinfluential": false,
          "contexts": [
            "ET-Net(Weng et al., 2021) employed a vision transformer to reconstruct videos from events."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Event-based Video Reconstruction Using Transformer",
            "abstract": "Event cameras, which output events by detecting spatio- temporal brightness changes, bring a novel paradigm to image sensors with high dynamic range and low latency. Previous works have achieved impressive performances on event-based video reconstruction by introducing convolutional neural networks (CNNs). However, intrinsic locality of convolutional operations is not capable of modeling long-range dependency, which is crucial to many vision tasks. In this paper, we present a hybrid CNN- Transformer network for event-based video reconstruction (ET-Net), which merits the fine local information from CNN and global contexts from Transformer In addition, we further propose a Token Pyramid Aggregation strategy to implement multi-scale token integration for relating internal and intersected semantic concepts in the token-space. Experimental results demonstrate that our proposed method achieves superior performance over state-of-the-art methods on multiple real-world event datasets. The code is available at https://github.com/WarranWeng/ET-Net.",
            "year": 2021,
            "venue": "IEEE International Conference on Computer Vision",
            "authors": [
              {
                "authorId": "2107017232",
                "name": "Wenming Weng"
              },
              {
                "authorId": "2145912767",
                "name": "Yueyi Zhang"
              },
              {
                "authorId": "2352456",
                "name": "Zhiwei Xiong"
              }
            ]
          }
        },
        {
          "citedcorpusid": 246285530,
          "isinfluential": true,
          "contexts": [
            "The hue of the scenes reconstructed by EVSNN is noticeably lighter compared to the true intensity map.",
            "E2VID and FireNet rely on the recurrent convolution structure while EVSNN is built upon the spiking neural network.",
            "EVSNN performs the best among the methods outside of our joint training strategy, recovering the intensity values well in all scenes except for ’Flower’ and ’Doll’.",
            "EVSNN(Zhu et al., 2022) proposes a hybrid potential-assisted spiking neural network to recover images from events efficiently.",
            "However, it can be observed that EVSNN also has the same issue as E2VID with low contrast.",
            "In this section, we select three popular image recovery algorithms, i.e. E2VID (Rebecq et al., 2019), FireNet (Scheerlinck et al., 2020), and EVSNN (Barchid et al., 2023)."
          ],
          "intents": [
            "--",
            "--",
            "--",
            "['background']",
            "--",
            "--"
          ],
          "cited_paper_info": {
            "title": "Event-based Video Reconstruction via Potential-assisted Spiking Neural Network",
            "abstract": "Neuromorphic vision sensor is a new bio-inspired imaging paradigm that reports asynchronous, continuously perpixel brightness changes called ‘events’ with high temporal resolution and high dynamic range. So far, the event-based image reconstruction methods are based on artificial neural networks (ANN) or hand-crafted spatiotemporal smoothing techniques. In this paper, we first implement the image reconstruction work via deep spiking neural network (SNN) architecture. As the bio-inspired neural networks, SNNs operating with asynchronous binary spikes distributed over time, can potentially lead to greater computational efficiency on event-driven hardware. We propose a novel Event-based Video reconstruction framework based on a fully Spiking Neural Network (EVSNN), which utilizes Leaky-Integrate-and-Fire (LIF) neuron and Membrane Potential (MP) neuron. We find that the spiking neurons have the potential to store useful temporal information (memory) to complete such time-dependent tasks. Further-more, to better utilize the temporal information, we propose a hybrid potential-assisted framework (PAEVSNN) using the membrane potential of spiking neuron. The proposed neuron is referred as Adaptive Membrane Potential (AMP) neuron, which adaptively updates the membrane potential according to the input spikes. The experimental results demonstrate that our models achieve comparable performance to ANN-based models on IJRR, MVSEC, and HQF datasets. The energy consumptions of EVSNN and PAEVSNN are $19.36\\times$ and $7.75\\times$ more computationally ef-ficient than their ANN architectures, respectively. The code and pretrained model are available at https://sites.google.com/view/evsnn.",
            "year": 2022,
            "venue": "Computer Vision and Pattern Recognition",
            "authors": [
              {
                "authorId": "2118939937",
                "name": "Lin Zhu"
              },
              {
                "authorId": "2118448261",
                "name": "Xiao Wang"
              },
              {
                "authorId": "2140493102",
                "name": "Yi Chang"
              },
              {
                "authorId": "1864921313",
                "name": "Jianing Li"
              },
              {
                "authorId": "34097174",
                "name": "Tiejun Huang"
              },
              {
                "authorId": "40161651",
                "name": "Yonghong Tian"
              }
            ]
          }
        },
        {
          "citedcorpusid": 251040986,
          "isinfluential": false,
          "contexts": [
            "NeuMesh(Yang et al., 2022) distills the neural field into a mesh scaffold, enabling field manipulation with the mesh deformation."
          ],
          "intents": [
            "--"
          ],
          "cited_paper_info": {
            "title": "NeuMesh: Learning Disentangled Neural Mesh-based Implicit Field for Geometry and Texture Editing",
            "abstract": "Very recently neural implicit rendering techniques have been rapidly evolved and shown great advantages in novel view synthesis and 3D scene reconstruction. However, existing neural rendering methods for editing purposes offer limited functionality, e.g., rigid transformation, or not applicable for fine-grained editing for general objects from daily lives. In this paper, we present a novel mesh-based representation by encoding the neural implicit field with disentangled geometry and texture codes on mesh vertices, which facilitates a set of editing functionalities, including mesh-guided geometry editing, designated texture editing with texture swapping, filling and painting operations. To this end, we develop several techniques including learnable sign indicators to magnify spatial distinguishability of mesh-based representation, distillation and fine-tuning mechanism to make a steady convergence, and the spatial-aware optimization strategy to realize precise texture editing. Extensive experiments and editing examples on both real and synthetic data demonstrate the superiority of our method on representation quality and editing ability. Code is available on the project webpage: https://zju3dv.github.io/neumesh/.",
            "year": 2022,
            "venue": "European Conference on Computer Vision",
            "authors": [
              {
                "authorId": "1400284746",
                "name": "Bangbang Yang"
              },
              {
                "authorId": "2159002587",
                "name": "Chong Bao"
              },
              {
                "authorId": "7216488",
                "name": "Junyi Zeng"
              },
              {
                "authorId": "1679542",
                "name": "H. Bao"
              },
              {
                "authorId": "1591143181",
                "name": "Yinda Zhang"
              },
              {
                "authorId": "1813796",
                "name": "Zhaopeng Cui"
              },
              {
                "authorId": "32162658",
                "name": "Guofeng Zhang"
              }
            ]
          }
        },
        {
          "citedcorpusid": 251765179,
          "isinfluential": false,
          "contexts": [
            "(Rud-nev et al., 2023; Hwang et al., 2023; Klenk et al., 2023; Wang et al., 2024a) build similar pipelines which integrate the event generation model into NeRF."
          ],
          "intents": [
            "--"
          ],
          "cited_paper_info": {
            "title": "E-NeRF: Neural Radiance Fields From a Moving Event Camera",
            "abstract": "Estimating neural radiance fields (NeRFs) from “ideal” images has been extensively studied in the computer vision community. Most approaches assume optimal illumination and slow camera motion. These assumptions are often violated in robotic applications, where images may contain motion blur, and the scene may not have suitable illumination. This can cause significant problems for downstream tasks such as navigation, inspection, or visualization of the scene. To alleviate these problems, we present E-NeRF, the first method which estimates a volumetric scene representation in the form of a NeRF from a fast-moving event camera. Our method can recover NeRFs during very fast motion and in high-dynamic-range conditions where frame-based approaches fail. We show that rendering high-quality frames is possible by only providing an event stream as input. Furthermore, by combining events and frames, we can estimate NeRFs of higher quality than state-of-the-art approaches under severe motion blur. We also show that combining events and frames can overcome failure cases of NeRF estimation in scenarios where only a few input views are available without requiring additional regularization.",
            "year": 2022,
            "venue": "IEEE Robotics and Automation Letters",
            "authors": [
              {
                "authorId": "1966877",
                "name": "Simone Klenk"
              },
              {
                "authorId": "1990661065",
                "name": "Lukas Koestler"
              },
              {
                "authorId": "2075371",
                "name": "D. Scaramuzza"
              },
              {
                "authorId": "1695302",
                "name": "D. Cremers"
              }
            ]
          }
        },
        {
          "citedcorpusid": 257232560,
          "isinfluential": false,
          "contexts": [
            "( Brebion et al., 2023) fuses information from an event camera and a LiDAR."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Learning to Estimate Two Dense Depths from LiDAR and Event Data",
            "abstract": "Event cameras do not produce images, but rather a continuous flow of events, which encode changes of illumination for each pixel independently and asynchronously. While they output temporally rich information, they lack any depth information which could facilitate their use with other sensors. LiDARs can provide this depth information, but are by nature very sparse, which makes the depth-to-event association more complex. Furthermore, as events represent changes of illumination, they might also represent changes of depth; associating them with a single depth is therefore inadequate. In this work, we propose to address these issues by fusing information from an event camera and a LiDAR using a learning-based approach to estimate accurate dense depth maps. To solve the\"potential change of depth\"problem, we propose here to estimate two depth maps at each step: one\"before\"the events happen, and one\"after\"the events happen. We further propose to use this pair of depths to compute a depth difference for each event, to give them more context. We train and evaluate our network, ALED, on both synthetic and real driving sequences, and show that it is able to predict dense depths with an error reduction of up to 61% compared to the current state of the art. We also demonstrate the quality of our 2-depths-to-event association, and the usefulness of the depth difference information. Finally, we release SLED, a novel synthetic dataset comprising events, LiDAR point clouds, RGB images, and dense depth maps.",
            "year": 2023,
            "venue": "Scandinavian Conference on Image Analysis",
            "authors": [
              {
                "authorId": "2146377003",
                "name": "Vincent Brebion"
              },
              {
                "authorId": "2061063803",
                "name": "Julien Moreau"
              },
              {
                "authorId": "1742818",
                "name": "F. Davoine"
              }
            ]
          }
        },
        {
          "citedcorpusid": 257632404,
          "isinfluential": false,
          "contexts": [
            "Ref-NeuS(Ge et al., 2023) model sign distance field by incorporating explicit reflection scores into NeRF."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Ref-NeuS: Ambiguity-Reduced Neural Implicit Surface Learning for Multi-View Reconstruction with Reflection",
            "abstract": "Neural implicit surface learning has shown significant progress in multi-view 3D reconstruction, where an object is represented by multilayer perceptrons that provide continuous implicit surface representation and view-dependent radiance. However, current methods often fail to accurately reconstruct reflective surfaces, leading to severe ambiguity. To overcome this issue, we propose Ref-NeuS, which aims to reduce ambiguity by attenuating the effect of reflective surfaces. Specifically, we utilize an anomaly detector to estimate an explicit reflection score with the guidance of multiview context to localize reflective surfaces. Afterward, we design a reflection-aware photometric loss that adaptively reduces ambiguity by modeling rendered color as a Gaussian distribution, with the reflection score representing the variance. We show that together with a reflection direction-dependent radiance, our model achieves high-quality surface reconstruction on reflective surfaces and outperforms the state-of-the-arts by a large margin. Besides, our model is also comparable on general surfaces.",
            "year": 2023,
            "venue": "IEEE International Conference on Computer Vision",
            "authors": [
              {
                "authorId": "2054600625",
                "name": "Wenhang Ge"
              },
              {
                "authorId": "144436761",
                "name": "T. Hu"
              },
              {
                "authorId": "2112673645",
                "name": "Haoyu Zhao"
              },
              {
                "authorId": "1779129",
                "name": "Shu Liu"
              },
              {
                "authorId": "104375063",
                "name": "Yingke Chen"
              }
            ]
          }
        },
        {
          "citedcorpusid": 264886560,
          "isinfluential": false,
          "contexts": [
            "(Xu et al., 2022) and (Wang et al., 2023) combine point clouds with NeRF to deliver better reconstruction quality."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Learning to Generate and Manipulate 3D Radiance Field by a Hierarchical Diffusion Framework with CLIP Latent",
            "abstract": "3D‐aware generative adversarial networks (GAN) are widely adopted in generating and editing neural radiance fields (NeRF). However, these methods still suffer from GAN‐related issues including degraded diversity and training instability. Moreover, 3D‐aware GANs consider NeRF pipeline as regularizers and do not directly operate with 3D assets, leading to imperfect 3D consistencies. Besides, the independent changes in disentangled editing cannot be ensured due to the sharing of some shallow hidden features in generators. To address these challenges, we propose the first purely diffusion‐based three‐stage framework for generative and editing tasks, with a series of well‐designed loss functions that can directly handle 3D models. In addition, we present a generalizable neural point field as our 3D representation, which explicitly disentangles geometry and appearance in feature spaces. For 3D data conversion, it simplifies the preparation pipeline of datasets. Assisted by the representation, our diffusion model can separately manipulate the shape and appearance in a hierarchical manner by image/text prompts that are provided by the CLIP encoder. Moreover, it can generate new samples by adding a simple generative head. Experiments show that our approach outperforms the SOTA work in the generative tasks of direct generation of 3D representations and novel image synthesis, and completely disentangles the manipulation of shape and appearance with correct semantic correspondence in the editing tasks.",
            "year": 2023,
            "venue": "Computer graphics forum (Print)",
            "authors": [
              {
                "authorId": "2242768739",
                "name": "Jiaxu Wang"
              },
              {
                "authorId": "2233873709",
                "name": "Ziyi Zhang"
              },
              {
                "authorId": "2224151598",
                "name": "Renjing Xu"
              }
            ]
          }
        }
      ]
    },
    "271769110": {
      "citing_paper_info": {
        "title": "LiDAR-Event Stereo Fusion with Hallucinations",
        "abstract": "Event stereo matching is an emerging technique to estimate depth from neuromorphic cameras; however, events are unlikely to trigger in the absence of motion or the presence of large, untextured regions, making the correspondence problem extremely challenging. Purposely, we propose integrating a stereo event camera with a fixed-frequency active sensor -- e.g., a LiDAR -- collecting sparse depth measurements, overcoming the aforementioned limitations. Such depth hints are used by hallucinating -- i.e., inserting fictitious events -- the stacks or raw input streams, compensating for the lack of information in the absence of brightness changes. Our techniques are general, can be adapted to any structured representation to stack events and outperform state-of-the-art fusion methods applied to event-based stereo.",
        "year": 2024,
        "venue": "European Conference on Computer Vision",
        "authors": [
          {
            "authorId": "2243335739",
            "name": "Luca Bartolomei"
          },
          {
            "authorId": "2509750",
            "name": "Matteo Poggi"
          },
          {
            "authorId": "2145601115",
            "name": "Andrea Conti"
          },
          {
            "authorId": "10261545",
            "name": "Stefano Mattoccia"
          }
        ]
      },
      "citation_summary": {
        "citation_count": 40,
        "unique_cited_count": 40,
        "influential_count": 5,
        "detailed_records_count": 40
      },
      "cited_papers": [
        "216036364",
        "259380779",
        "261031728",
        "44623261",
        "6079544",
        "250374739",
        "258187423",
        "10712214",
        "53082511",
        "226254259",
        "31762881",
        "2658860",
        "24236495",
        "262083814",
        "2430892",
        "260779095",
        "257407018",
        "196183868",
        "208828341",
        "247596980",
        "786967",
        "159040912",
        "233204703",
        "5880703",
        "6262684",
        "225072923",
        "3871029",
        "269983050",
        "703552",
        "3719281",
        "55750",
        "253080413",
        "12047627",
        "34855834",
        "232104918",
        "257495841",
        "4252896",
        "119304432",
        "73729084",
        "6628106"
      ],
      "citation_details": [
        {
          "citedcorpusid": 55750,
          "isinfluential": false,
          "contexts": [
            "[19] proposed an efficient seed-growing algorithm to fuse time-of-flight (ToF) depth data with stereo pairs, while Marin et al ."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "High-resolution depth maps based on TOF-stereo fusion",
            "abstract": "The combination of range sensors with color cameras can be very useful for robot navigation, semantic perception, manipulation, and telepresence. Several methods of combining range- and color-data have been investigated and successfully used in various robotic applications. Most of these systems suffer from the problems of noise in the range-data and resolution mismatch between the range sensor and the color cameras, since the resolution of current range sensors is much less than the resolution of color cameras. High-resolution depth maps can be obtained using stereo matching, but this often fails to construct accurate depth maps of weakly/repetitively textured scenes, or if the scene exhibits complex self-occlusions. Range sensors provide coarse depth information regardless of presence/absence of texture. The use of a calibrated system, composed of a time-of-flight (TOF) camera and of a stereoscopic camera pair, allows data fusion thus overcoming the weaknesses of both individual sensors. We propose a novel TOF-stereo fusion method based on an efficient seed-growing algorithm which uses the TOF data projected onto the stereo image pair as an initial set of correspondences. These initial “seeds” are then propagated based on a Bayesian model which combines an image similarity score with rough depth priors computed from the low-resolution range data. The overall result is a dense and accurate depth map at the resolution of the color cameras at hand. We show that the proposed algorithm outperforms 2D image-based stereo algorithms and that the results are of higher resolution than off-the-shelf color-range sensors, e.g., Kinect. Moreover, the algorithm potentially exhibits real-time performance on a single CPU.",
            "year": 2012,
            "venue": "IEEE International Conference on Robotics and Automation",
            "authors": [
              {
                "authorId": "145091336",
                "name": "Vineet Gandhi"
              },
              {
                "authorId": "2064794036",
                "name": "Jan Cech"
              },
              {
                "authorId": "1794229",
                "name": "R. Horaud"
              }
            ]
          }
        },
        {
          "citedcorpusid": 703552,
          "isinfluential": false,
          "contexts": [
            "…a longstanding open problem, with a large body of literature spanning from traditional approaches grounded on handcrafted features and priors [5, 24, 31, 36, 62, 68, 75, 76, 78] to contemporary deep learning approaches that brought significant improvements over previous methods, starting with [79]."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Non-parametric Local Transforms for Computing Visual Correspondence",
            "abstract": "",
            "year": 1994,
            "venue": "European Conference on Computer Vision",
            "authors": [
              {
                "authorId": "2984143",
                "name": "R. Zabih"
              },
              {
                "authorId": "1803592",
                "name": "J. Woodfill"
              }
            ]
          }
        },
        {
          "citedcorpusid": 786967,
          "isinfluential": false,
          "contexts": [
            "…a longstanding open problem, with a large body of literature spanning from traditional approaches grounded on handcrafted features and priors [5, 24, 31, 36, 62, 68, 75, 76, 78] to contemporary deep learning approaches that brought significant improvements over previous methods, starting with [79]."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "What energy functions can be minimized via graph cuts?",
            "abstract": "",
            "year": 2002,
            "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
            "authors": [
              {
                "authorId": "144653004",
                "name": "V. Kolmogorov"
              },
              {
                "authorId": "2984143",
                "name": "R. Zabih"
              }
            ]
          }
        },
        {
          "citedcorpusid": 2430892,
          "isinfluential": true,
          "contexts": [
            "XI to XVIII) different figures from the M3ED [5] dataset, using raw LiDAR measurements as guidance for all fusion frameworks.",
            "For more details, please refer to the original paper [5].",
            "It is a longstanding open problem, with a large body of literature spanning from traditional approaches grounded on handcrafted features and priors [5, 24, 31, 36, 62, 68, 75, 76, 78] to contemporary deep learning approaches that brought significant improvements over previous methods, starting with…",
            "In particular, we describe the DSEC [7] search split we used for the hyperparameters search concerning our proposals VSH and BTH, the M3ED [5] evaluation split we selected, and how we managed to process both datasets to extract raw LiDAR and, on M3ED dataset [5], for obtaining misaligned LiDAR…",
            "Differently from M3ED [5], this dataset does not provide any ground-truth pose.",
            "…VSH and BTH, the M3ED [5] evaluation split we selected, and how we managed to process both datasets to extract raw LiDAR and, on M3ED dataset [5], for obtaining misaligned LiDAR measurements with respect to the timestamp at which we estimate disparity maps – and thus, at which we have…"
          ],
          "intents": [
            "['methodology']",
            "['background']",
            "['background']",
            "['methodology']",
            "['background']",
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Fast approximate energy minimization via graph cuts",
            "abstract": "",
            "year": 2001,
            "venue": "Proceedings of the Seventh IEEE International Conference on Computer Vision",
            "authors": [
              {
                "authorId": "1692688",
                "name": "Yuri Boykov"
              },
              {
                "authorId": "1922280",
                "name": "Olga Veksler"
              },
              {
                "authorId": "2984143",
                "name": "R. Zabih"
              }
            ]
          }
        },
        {
          "citedcorpusid": 2658860,
          "isinfluential": false,
          "contexts": [
            "In contrast, the latter constructs a feature cost volume from image pair features and estimates the disparity map through 3D convolutions at the cost of substantially higher memory and runtime demands [10,11,13,16,23, 27,28,57,70,73,80]."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "End-to-End Learning of Geometry and Context for Deep Stereo Regression",
            "abstract": "We propose a novel deep learning architecture for regressing disparity from a rectified pair of stereo images. We leverage knowledge of the problem’s geometry to form a cost volume using deep feature representations. We learn to incorporate contextual information using 3-D convolutions over this volume. Disparity values are regressed from the cost volume using a proposed differentiable soft argmin operation, which allows us to train our method end-to-end to sub-pixel accuracy without any additional post-processing or regularization. We evaluate our method on the Scene Flow and KITTI datasets and on KITTI we set a new stateof-the-art benchmark, while being significantly faster than competing approaches.",
            "year": 2017,
            "venue": "IEEE International Conference on Computer Vision",
            "authors": [
              {
                "authorId": "47645184",
                "name": "Alex Kendall"
              },
              {
                "authorId": "9746545",
                "name": "H. Martirosyan"
              },
              {
                "authorId": "5762869",
                "name": "Saumitro Dasgupta"
              },
              {
                "authorId": "1791800",
                "name": "Peter Henry"
              }
            ]
          }
        },
        {
          "citedcorpusid": 3719281,
          "isinfluential": false,
          "contexts": [
            "The former, inspired by the U-Net model [53], adopts an encoder-decoder design [37,42,45,50, 54,59,63,64,74,77]."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "U-Net: Convolutional Networks for Biomedical Image Segmentation",
            "abstract": "There is large consent that successful training of deep networks requires many thousand annotated training samples. In this paper, we present a network and training strategy that relies on the strong use of data augmentation to use the available annotated samples more efficiently. The architecture consists of a contracting path to capture context and a symmetric expanding path that enables precise localization. We show that such a network can be trained end-to-end from very few images and outperforms the prior best method (a sliding-window convolutional network) on the ISBI challenge for segmentation of neuronal structures in electron microscopic stacks. Using the same network trained on transmitted light microscopy images (phase contrast and DIC) we won the ISBI cell tracking challenge 2015 in these categories by a large margin. Moreover, the network is fast. Segmentation of a 512x512 image takes less than a second on a recent GPU. The full implementation (based on Caffe) and the trained networks are available at http://lmb.informatik.uni-freiburg.de/people/ronneber/u-net .",
            "year": 2015,
            "venue": "International Conference on Medical Image Computing and Computer-Assisted Intervention",
            "authors": [
              {
                "authorId": "1737326",
                "name": "O. Ronneberger"
              },
              {
                "authorId": "152702479",
                "name": "P. Fischer"
              },
              {
                "authorId": "1710872",
                "name": "T. Brox"
              }
            ]
          }
        },
        {
          "citedcorpusid": 3871029,
          "isinfluential": false,
          "contexts": [
            "The former, inspired by the U-Net model [53], adopts an encoder-decoder design [37,42,45,50, 54,59,63,64,74,77]."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "EdgeStereo: A Context Integrated Residual Pyramid Network for Stereo Matching",
            "abstract": "Recently convolutional neural network (CNN) promotes the development of stereo matching greatly. Especially those end-to-end stereo methods achieve best performance. However less attention is paid on encoding context information, simplifying two-stage disparity learning pipeline and improving details in disparity maps. Differently we focus on these problems. Firstly, we propose an one-stage context pyramid based residual pyramid network (CP-RPN) for disparity estimation, in which a context pyramid is embedded to encode multi-scale context clues explicitly. Next, we design a CNN based multi-task learning network called EdgeStereo to recover missing details in disparity maps, utilizing mid-level features from edge detection task. In EdgeStereo, CP-RPN is integrated with a proposed edge detector HED$_\\beta$ based on two-fold multi-task interactions. The end-to-end EdgeStereo outputs the edge map and disparity map directly from a stereo pair without any post-processing or regularization. We discover that edge detection task and stereo matching task can help each other in our EdgeStereo framework. Comprehensive experiments on stereo benchmarks such as Scene Flow and KITTI 2015 show that our method achieves state-of-the-art performance.",
            "year": 2018,
            "venue": "Asian Conference on Computer Vision",
            "authors": [
              {
                "authorId": "2118944332",
                "name": "Xiao Song"
              },
              {
                "authorId": "1390869207",
                "name": "Xu Zhao"
              },
              {
                "authorId": "9750713",
                "name": "Hanwen Hu"
              },
              {
                "authorId": "40901159",
                "name": "Liangji Fang"
              }
            ]
          }
        },
        {
          "citedcorpusid": 4252896,
          "isinfluential": false,
          "contexts": [
            "In contrast, the latter constructs a feature cost volume from image pair features and estimates the disparity map through 3D convolutions at the cost of substantially higher memory and runtime demands [10,11,13,16,23, 27,28,57,70,73,80]."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Pyramid Stereo Matching Network",
            "abstract": "Recent work has shown that depth estimation from a stereo pair of images can be formulated as a supervised learning task to be resolved with convolutional neural networks (CNNs). However, current architectures rely on patch-based Siamese networks, lacking the means to exploit context information for finding correspondence in ill-posed regions. To tackle this problem, we propose PSMNet, a pyramid stereo matching network consisting of two main modules: spatial pyramid pooling and 3D CNN. The spatial pyramid pooling module takes advantage of the capacity of global context information by aggregating context in different scales and locations to form a cost volume. The 3D CNN learns to regularize cost volume using stacked multiple hourglass networks in conjunction with intermediate supervision. The proposed approach was evaluated on several benchmark datasets. Our method ranked first in the KITTI 2012 and 2015 leaderboards before March 18, 2018. The codes of PSMNet are available at: https://github.com/JiaRenChang/PSMNet.",
            "year": 2018,
            "venue": "2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition",
            "authors": [
              {
                "authorId": "2936466",
                "name": "Jia-Ren Chang"
              },
              {
                "authorId": "2143438143",
                "name": "Yonghao Chen"
              }
            ]
          }
        },
        {
          "citedcorpusid": 5880703,
          "isinfluential": false,
          "contexts": [
            "…a longstanding open problem, with a large body of literature spanning from traditional approaches grounded on handcrafted features and priors [5, 24, 31, 36, 62, 68, 75, 76, 78] to contemporary deep learning approaches that brought significant improvements over previous methods, starting with [79]."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "A constant-space belief propagation algorithm for stereo matching",
            "abstract": "",
            "year": 2010,
            "venue": "2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition",
            "authors": [
              {
                "authorId": "1777434",
                "name": "Qingxiong Yang"
              },
              {
                "authorId": "40476140",
                "name": "Liang Wang"
              },
              {
                "authorId": "145237406",
                "name": "N. Ahuja"
              }
            ]
          }
        },
        {
          "citedcorpusid": 6079544,
          "isinfluential": false,
          "contexts": [
            "Instead, [47] revisited the cooperative network from [41]."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Asynchronous Stereo Vision for Event-Driven Dynamic Stereo Sensor Using an Adaptive Cooperative Approach",
            "abstract": "",
            "year": 2013,
            "venue": "2013 IEEE International Conference on Computer Vision Workshops",
            "authors": [
              {
                "authorId": "47105977",
                "name": "Ewa Piatkowska"
              },
              {
                "authorId": "1768812",
                "name": "A. Belbachir"
              },
              {
                "authorId": "1990797",
                "name": "M. Gelautz"
              }
            ]
          }
        },
        {
          "citedcorpusid": 6262684,
          "isinfluential": false,
          "contexts": [
            "…a longstanding open problem, with a large body of literature spanning from traditional approaches grounded on handcrafted features and priors [5, 24, 31, 36, 62, 68, 75, 76, 78] to contemporary deep learning approaches that brought significant improvements over previous methods, starting with [79]."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Stereo correspondence by dynamic programming on a tree",
            "abstract": "",
            "year": 2005,
            "venue": "Computer Vision and Pattern Recognition",
            "authors": [
              {
                "authorId": "1922280",
                "name": "Olga Veksler"
              }
            ]
          }
        },
        {
          "citedcorpusid": 6628106,
          "isinfluential": true,
          "contexts": [
            "We use Adam [29] with beta (0.9, 0.999) and weight decay set to 10 − 4 ."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Adam: A Method for Stochastic Optimization",
            "abstract": "We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm.",
            "year": 2014,
            "venue": "International Conference on Learning Representations",
            "authors": [
              {
                "authorId": "1726807",
                "name": "Diederik P. Kingma"
              },
              {
                "authorId": "2503659",
                "name": "Jimmy Ba"
              }
            ]
          }
        },
        {
          "citedcorpusid": 10712214,
          "isinfluential": false,
          "contexts": [
            "However, pseudo-images lose the high temporal resolution of the stream: to face this problem, [8,52] handle events without an intermediate representation using an event-to-event matching approach, where for each reference event, a set of possible matches is given."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Event-based 3D reconstruction from neuromorphic retinas",
            "abstract": "",
            "year": 2013,
            "venue": "Neural Networks",
            "authors": [
              {
                "authorId": "2057119545",
                "name": "J. Carneiro"
              },
              {
                "authorId": "144975525",
                "name": "S. Ieng"
              },
              {
                "authorId": "153466606",
                "name": "C. Posch"
              },
              {
                "authorId": "1750848",
                "name": "R. Benosman"
              }
            ]
          }
        },
        {
          "citedcorpusid": 12047627,
          "isinfluential": false,
          "contexts": [
            "[7] Fig.",
            "We start with DSEC [7], which we use for i) tuning the hyper-parameters in our solutions, and ii) training the models involved in our experiments.",
            "In particular, we describe the DSEC [7] search split we used for the hyperparameters search concerning our proposals VSH and BTH, the M3ED [5] evaluation split we selected, and how we managed to process both datasets to extract raw LiDAR and, on M3ED dataset [5], for obtaining misaligned LiDAR…"
          ],
          "intents": [
            "--",
            "['methodology']",
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "On the use of orientation filters for 3D reconstruction in event-driven stereo vision",
            "abstract": "The recently developed Dynamic Vision Sensors (DVS) sense visual information asynchronously and code it into trains of events with sub-micro second temporal resolution. This high temporal precision makes the output of these sensors especially suited for dynamic 3D visual reconstruction, by matching corresponding events generated by two different sensors in a stereo setup. This paper explores the use of Gabor filters to extract information about the orientation of the object edges that produce the events, therefore increasing the number of constraints applied to the matching algorithm. This strategy provides more reliably matched pairs of events, improving the final 3D reconstruction.",
            "year": 2014,
            "venue": "Frontiers in Neuroscience",
            "authors": [
              {
                "authorId": "1398500621",
                "name": "L. Camuñas-Mesa"
              },
              {
                "authorId": "1397317865",
                "name": "T. Serrano-Gotarredona"
              },
              {
                "authorId": "144975525",
                "name": "S. Ieng"
              },
              {
                "authorId": "1750848",
                "name": "R. Benosman"
              },
              {
                "authorId": "1397317879",
                "name": "B. Linares-Barranco"
              }
            ]
          }
        },
        {
          "citedcorpusid": 24236495,
          "isinfluential": false,
          "contexts": [
            "Similarly to conventional stereo matching, the first approaches focused on developing traditional algorithms by building structured representations, such as voxel grids [56], matched through handcrafted similarity functions [30, 56, 60, 85]."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Dynamic stereo vision system for real-time tracking",
            "abstract": "",
            "year": 2010,
            "venue": "Proceedings of 2010 IEEE International Symposium on Circuits and Systems",
            "authors": [
              {
                "authorId": "2521747",
                "name": "S. Schraml"
              },
              {
                "authorId": "1768812",
                "name": "A. Belbachir"
              },
              {
                "authorId": "152832347",
                "name": "Nenad Milosevic"
              },
              {
                "authorId": "1680865",
                "name": "Peter Schön"
              }
            ]
          }
        },
        {
          "citedcorpusid": 31762881,
          "isinfluential": false,
          "contexts": [
            "The former, inspired by the U-Net model [53], adopts an encoder-decoder design [37,42,45,50, 54,59,63,64,74,77]."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Cascade Residual Learning: A Two-Stage Convolutional Neural Network for Stereo Matching",
            "abstract": "Leveraging on the recent developments in convolutional neural networks (CNNs), matching dense correspondence from a stereo pair has been cast as a learning problem, with performance exceeding traditional approaches. However, it remains challenging to generate high-quality disparities for the inherently ill-posed regions. To tackle this problem, we propose a novel cascade CNN architecture composing of two stages. The first stage advances the recently proposed DispNet by equipping it with extra up-convolution modules, leading to disparity images with more details. The second stage explicitly rectifies the disparity initialized by the first stage; it couples with the first-stage and generates residual signals across multiple scales. The summation of the outputs from the two stages gives the final disparity. As opposed to directly learning the disparity at the second stage, we show that residual learning provides more effective refinement. Moreover, it also benefits the training of the overall cascade network. Experimentation shows that our cascade residual learning scheme provides state-of-the-art performance for matching stereo correspondence. By the time of the submission of this paper, our method ranks first in the KITTI 2015 stereo benchmark, surpassing the prior works by a noteworthy margin.",
            "year": 2017,
            "venue": "2017 IEEE International Conference on Computer Vision Workshops (ICCVW)",
            "authors": [
              {
                "authorId": "33520260",
                "name": "Jiahao Pang"
              },
              {
                "authorId": "8397576",
                "name": "Wenxiu Sun"
              },
              {
                "authorId": "145335572",
                "name": "Jimmy S. J. Ren"
              },
              {
                "authorId": "18306323",
                "name": "Chengxi Yang"
              },
              {
                "authorId": "144479026",
                "name": "Qiong Yan"
              }
            ]
          }
        },
        {
          "citedcorpusid": 34855834,
          "isinfluential": true,
          "contexts": [
            "In the latter case, Concat [6] and Guided+Concat [15] can reduce the error by about 40%, yet far behind the improvement yielded by BTH (more than 70% error rate reduction).",
            "We report additional details about the deep architectures used in our experiments, starting from the baseline network, SE-CFF [12], and then showing the fusion strategies ported from classical deep stereo literature [6,13,15] Baseline Model: SE-CFF.",
            "Guided [13] is almost ineffective, while using both Concat [6] and Guided+Concat [15] leads to 20% error reduction.",
            "The former are often inspired by [41] and typically employ Spiking Neural Networks (SNN) [1, 15, 44]."
          ],
          "intents": [
            "['background']",
            "['methodology']",
            "['methodology']",
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Spiking Cooperative Stereo-Matching at 2 ms Latency with Neuromorphic Hardware",
            "abstract": "",
            "year": 2017,
            "venue": "Living Machines",
            "authors": [
              {
                "authorId": "46775745",
                "name": "G. Dikov"
              },
              {
                "authorId": "145885214",
                "name": "M. Firouzi"
              },
              {
                "authorId": "1685761",
                "name": "Florian Röhrbein"
              },
              {
                "authorId": "3302681",
                "name": "J. Conradt"
              },
              {
                "authorId": "2053647526",
                "name": "Christoph Richter"
              }
            ]
          }
        },
        {
          "citedcorpusid": 44623261,
          "isinfluential": false,
          "contexts": [
            "Similarly to conventional stereo matching, the first approaches focused on developing traditional algorithms by building structured representations, such as voxel grids [56], matched through handcrafted similarity functions [30, 56, 60, 85]."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Address-Event Based Stereo Vision with Bio-Inspired Silicon Retina Imagers",
            "abstract": "Several industry, home, or automotive applications need 3D or at least range data of the observed environment to operate. Such applications are, e.g., driver assistance systems, home care systems, or 3D sensing and measurement for industrial production. State-of-the-art range sensors are laser range finders or laser scanners (LIDAR, light detection and ranging), time-of-flight (TOF) cameras, and ultrasonic sound sensors. All of them are embedded, which means that the sensors operate independently and have an integrated processing unit. This is advantageous because the processing power in the mentioned applications is limited and they are computationally intensive anyway. Another benefits of embedded systems are a low power consumption and a small form factor. Furthermore, embedded systems are full customizable by the developer and can be adapted to the specific application in an optimal way. A promising alternative to the mentioned sensors is stereo vision. Classic stereo vision uses a stereo camera setup, which is built up of two cameras (stereo camera head), mounted in parallel and separated by the baseline. It captures a synchronized stereo pair consisting of the left camera’s image and the right camera’s image. The main challenge of stereo vision is the reconstruction of 3D information of a scene captured from two different points of view. Each visible scene point is projected on the image planes of the cameras. Pixels which represent the same scene points on different image planes correspond to each other. These correspondences can then be used to determine the three dimensional position of the projected scene point in a defined coordinate system. In more detail, the horizontal displacement, called the disparity, is inverse proportional to the scene point’s depth. With this information and the camera’s intrinsic parameters (principal point and focal length), the 3D position can be reconstructed. Fig. 1 shows a typical stereo camera setup. The projections of scene point P are pl and pr. Once the correspondences are found, the disparity is calculated with",
            "year": 2011,
            "venue": "",
            "authors": [
              {
                "authorId": "1824241",
                "name": "J. Kogler"
              },
              {
                "authorId": "1692498",
                "name": "C. Sulzbachner"
              },
              {
                "authorId": "1721721",
                "name": "M. Humenberger"
              },
              {
                "authorId": "2435053",
                "name": "F. Eibensteiner"
              }
            ]
          }
        },
        {
          "citedcorpusid": 53082511,
          "isinfluential": false,
          "contexts": [
            "In contrast, the latter constructs a feature cost volume from image pair features and estimates the disparity map through 3D convolutions at the cost of substantially higher memory and runtime demands [10,11,13,16,23, 27,28,57,70,73,80]."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Anytime Stereo Image Depth Estimation on Mobile Devices",
            "abstract": "Many applications of stereo depth estimation in robotics require the generation of accurate disparity maps in real time under significant computational constraints. Current state-of-the-art algorithms force a choice between either generating accurate mappings at a slow pace, or quickly generating inaccurate ones, and additionally these methods typically require far too many parameters to be usable on power- or memory-constrained devices. Motivated by these shortcomings, we propose a novel approach for disparity prediction in the anytime setting. In contrast to prior work, our end-to-end learned approach can trade off computation and accuracy at inference time. Depth estimation is performed in stages, during which the model can be queried at any time to output its current best estimate. Our final model can process $1242 \\times 375$ resolution images within a range of 10-35 FPS on an NVIDIA Jetson TX2 module with only marginal increases in error – using two orders of magnitude fewer parameters than the most competitive baseline. The source code is available at https://github.com/mileyan/AnyNet.",
            "year": 2018,
            "venue": "IEEE International Conference on Robotics and Automation",
            "authors": [
              {
                "authorId": "47906756",
                "name": "Yan Wang"
              },
              {
                "authorId": "51451501",
                "name": "Zihang Lai"
              },
              {
                "authorId": "143983679",
                "name": "Gao Huang"
              },
              {
                "authorId": "49292472",
                "name": "Brian H. Wang"
              },
              {
                "authorId": "1803520",
                "name": "L. Maaten"
              },
              {
                "authorId": "143903367",
                "name": "M. Campbell"
              },
              {
                "authorId": "7446832",
                "name": "Kilian Q. Weinberger"
              }
            ]
          }
        },
        {
          "citedcorpusid": 73729084,
          "isinfluential": false,
          "contexts": [
            "In contrast, the latter constructs a feature cost volume from image pair features and estimates the disparity map through 3D convolutions at the cost of substantially higher memory and runtime demands [10,11,13,16,23, 27,28,57,70,73,80]."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Group-Wise Correlation Stereo Network",
            "abstract": "Stereo matching estimates the disparity between a rectified image pair, which is of great importance to depth sensing, autonomous driving, and other related tasks. Previous works built cost volumes with cross-correlation or concatenation of left and right features across all disparity levels, and then a 2D or 3D convolutional neural network is utilized to regress the disparity maps. In this paper, we propose to construct the cost volume by group-wise correlation. The left features and the right features are divided into groups along the channel dimension, and correlation maps are computed among each group to obtain multiple matching cost proposals, which are then packed into a cost volume. Group-wise correlation provides efficient representations for measuring feature similarities and will not lose too much information like full correlation. It also preserves better performance when reducing parameters compared with previous methods. The 3D stacked hourglass network proposed in previous works is improved to boost the performance and decrease the inference computational cost. Experiment results show that our method outperforms previous methods on Scene Flow, KITTI 2012, and KITTI 2015 datasets.",
            "year": 2019,
            "venue": "Computer Vision and Pattern Recognition",
            "authors": [
              {
                "authorId": "49932298",
                "name": "Xiaoyang Guo"
              },
              {
                "authorId": "2118048994",
                "name": "Kai Yang"
              },
              {
                "authorId": "3432961",
                "name": "Wukui Yang"
              },
              {
                "authorId": "31843833",
                "name": "Xiaogang Wang"
              },
              {
                "authorId": "47893312",
                "name": "Hongsheng Li"
              }
            ]
          }
        },
        {
          "citedcorpusid": 119304432,
          "isinfluential": false,
          "contexts": [
            "In contrast, the latter constructs a feature cost volume from image pair features and estimates the disparity map through 3D convolutions at the cost of substantially higher memory and runtime demands [10,11,13,16,23, 27,28,57,70,73,80]."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "GA-Net: Guided Aggregation Net for End-To-End Stereo Matching",
            "abstract": "In the stereo matching task, matching cost aggregation is crucial in both traditional methods and deep neural network models in order to accurately estimate disparities. We propose two novel neural net layers, aimed at capturing local and the whole-image cost dependencies respectively. The first is a semi-global aggregation layer which is a differentiable approximation of the semi-global matching, the second is the local guided aggregation layer which follows a traditional cost filtering strategy to refine thin structures. These two layers can be used to replace the widely used 3D convolutional layer which is computationally costly and memory-consuming as it has cubic computational/memory complexity. In the experiments, we show that nets with a two-layer guided aggregation block easily outperform the state-of-the-art GC-Net which has nineteen 3D convolutional layers. We also train a deep guided aggregation network (GA-Net) which gets better accuracies than state-of-the-art methods on both Scene Flow dataset and KITTI benchmarks.",
            "year": 2019,
            "venue": "Computer Vision and Pattern Recognition",
            "authors": [
              {
                "authorId": "2144025110",
                "name": "Feihu Zhang"
              },
              {
                "authorId": "2824784",
                "name": "V. Prisacariu"
              },
              {
                "authorId": "38958903",
                "name": "Ruigang Yang"
              },
              {
                "authorId": "143635540",
                "name": "Philip H. S. Torr"
              }
            ]
          }
        },
        {
          "citedcorpusid": 159040912,
          "isinfluential": false,
          "contexts": [
            "The former, inspired by the U-Net model [53], adopts an encoder-decoder design [37,42,45,50, 54,59,63,64,74,77]."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "AutoDispNet: Improving Disparity Estimation With AutoML",
            "abstract": "Much research work in computer vision is being spent on optimizing existing network architectures to obtain a few more percentage points on benchmarks. Recent AutoML approaches promise to relieve us from this effort. However, they are mainly designed for comparatively small-scale classification tasks. In this work, we show how to use and extend existing AutoML techniques to efficiently optimize large-scale U-Net-like encoder-decoder architectures. In particular, we leverage gradient-based neural architecture search and Bayesian optimization for hyperparameter search. The resulting optimization does not require a large-scale compute cluster. We show results on disparity estimation that clearly outperform the manually optimized baseline and reach state-of-the-art performance.",
            "year": 2019,
            "venue": "IEEE International Conference on Computer Vision",
            "authors": [
              {
                "authorId": "2872102",
                "name": "Tonmoy Saikia"
              },
              {
                "authorId": "3169630",
                "name": "Yassine Marrakchi"
              },
              {
                "authorId": "51109984",
                "name": "Arber Zela"
              },
              {
                "authorId": "144661829",
                "name": "F. Hutter"
              },
              {
                "authorId": "1710872",
                "name": "T. Brox"
              }
            ]
          }
        },
        {
          "citedcorpusid": 196183868,
          "isinfluential": false,
          "contexts": [
            "In contrast, the latter constructs a feature cost volume from image pair features and estimates the disparity map through 3D convolutions at the cost of substantially higher memory and runtime demands [10,11,13,16,23, 27,28,57,70,73,80]."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Hierarchical Deep Stereo Matching on High-Resolution Images",
            "abstract": "We explore the problem of real-time stereo matching on high-res imagery. Many state-of-the-art (SOTA) methods struggle to process high-res imagery because of memory constraints or speed limitations. To address this issue, we propose an end-to-end framework that searches for correspondences incrementally over a coarse-to-fine hierarchy. Because high-res stereo datasets are relatively rare, we introduce a dataset with high-res stereo pairs for both training and evaluation. Our approach achieved SOTA performance on Middlebury-v3 and KITTI-15 while running significantly faster than its competitors. The hierarchical design also naturally allows for anytime on-demand reports of disparity by capping intermediate coarse results, allowing us to accurately predict disparity for near-range structures with low latency (30ms). We demonstrate that the performance-vs-speed tradeoff afforded by on-demand hierarchies may address sensing needs for time-critical applications such as autonomous driving.",
            "year": 2019,
            "venue": "Computer Vision and Pattern Recognition",
            "authors": [
              {
                "authorId": "3408952",
                "name": "Gengshan Yang"
              },
              {
                "authorId": "47027863",
                "name": "Joshua M. Manela"
              },
              {
                "authorId": "2348519",
                "name": "Michael Happold"
              },
              {
                "authorId": "1770537",
                "name": "Deva Ramanan"
              }
            ]
          }
        },
        {
          "citedcorpusid": 208828341,
          "isinfluential": false,
          "contexts": [
            "[48] exploited confidence measures."
          ],
          "intents": [
            "--"
          ],
          "cited_paper_info": {
            "title": "Confidence Estimation for ToF and Stereo Sensors and Its Application to Depth Data Fusion",
            "abstract": "Time-of-Flight (ToF) sensors and stereo vision systems are two widely used technologies for depth estimation. Due to their rather complementary strengths and limitations, the two sensors are often combined to infer more accurate depth maps. A key research issue in this field is how to estimate the reliability of the sensed depth data. While this problem has been widely studied for stereo systems, it has been seldom considered for ToF sensors. Therefore, starting from the work done for stereo data, in this paper, we firstly introduce novel confidence estimation techniques for ToF data. Moreover, we also show how by using learning-based confidence metrics jointly trained on the two sensors yields better performance. Finally, deploying different fusion frameworks, we show how confidence estimation can be exploited in order to guide the fusion of depth data from the two sensors. Experimental results show how accurate confidence cues allow outperforming state-of-the-art data fusion schemes even with the simplest fusion strategies known in the literature.",
            "year": 2020,
            "venue": "IEEE Sensors Journal",
            "authors": [
              {
                "authorId": "2509750",
                "name": "Matteo Poggi"
              },
              {
                "authorId": "27507167",
                "name": "Gianluca Agresti"
              },
              {
                "authorId": "121670758",
                "name": "Fabio Tosi"
              },
              {
                "authorId": "1709035",
                "name": "P. Zanuttigh"
              },
              {
                "authorId": "10261545",
                "name": "Stefano Mattoccia"
              }
            ]
          }
        },
        {
          "citedcorpusid": 216036364,
          "isinfluential": false,
          "contexts": [
            "We build our code base starting from SE-CFF [43] – state-of-the-art for event-based stereo – assuming the same stereo backbone as in their experiments, i.e . derived from AANet [72], and run SBN to generate the event history to be stacked."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "AANet: Adaptive Aggregation Network for Efficient Stereo Matching",
            "abstract": "Despite the remarkable progress made by learning based stereo matching algorithms, one key challenge remains unsolved. Current state-of-the-art stereo models are mostly based on costly 3D convolutions, the cubic computational complexity and high memory consumption make it quite expensive to deploy in real-world applications. In this paper, we aim at completely replacing the commonly used 3D convolutions to achieve fast inference speed while maintaining comparable accuracy. To this end, we first propose a sparse points based intra-scale cost aggregation method to alleviate the well-known edge-fattening issue at disparity discontinuities. Further, we approximate traditional cross-scale cost aggregation algorithm with neural network layers to handle large textureless regions. Both modules are simple, lightweight, and complementary, leading to an effective and efficient architecture for cost aggregation. With these two modules, we can not only significantly speed up existing top-performing models (e.g., 41x than GC-Net, 4x than PSMNet and 38x than GA-Net), but also improve the performance of fast stereo models (e.g., StereoNet). We also achieve competitive results on Scene Flow and KITTI datasets while running at 62ms, demonstrating the versatility and high efficiency of the proposed method. Our full framework is available at https://github.com/haofeixu/aanet.",
            "year": 2020,
            "venue": "Computer Vision and Pattern Recognition",
            "authors": [
              {
                "authorId": "2108835907",
                "name": "Haofei Xu"
              },
              {
                "authorId": "2108487442",
                "name": "Juyong Zhang"
              }
            ]
          }
        },
        {
          "citedcorpusid": 225072923,
          "isinfluential": true,
          "contexts": [
            "We report additional details about the deep architectures used in our experiments, starting from the baseline network, SE-CFF [12], and then showing the fusion strategies ported from classical deep stereo literature [6,13,15] Baseline Model: SE-CFF.",
            "In the former case, Guided [13] is nearly ineffective, whereas both VSH and BTH largely improve the results.",
            "Guided Stereo [13].",
            "In contrast, the latter constructs a feature cost volume from image pair features and estimates the disparity map through 3D convolutions at the cost of substantially higher memory and runtime demands [10,11,13,16,23, 27,28,57,70,73,80].",
            "Following [13], LiDAR points are downsampled through nearest-neighbor interpolation to act at the different resolutions.",
            "Guided [13] is almost ineffective, while using both Concat [6] and Guided+Concat [15] leads to 20% error reduction."
          ],
          "intents": [
            "['methodology']",
            "['result']",
            "--",
            "['background']",
            "['methodology']",
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Hierarchical Neural Architecture Search for Deep Stereo Matching",
            "abstract": "To reduce the human efforts in neural network design, Neural Architecture Search (NAS) has been applied with remarkable success to various high-level vision tasks such as classification and semantic segmentation. The underlying idea for the NAS algorithm is straightforward, namely, to enable the network the ability to choose among a set of operations (e.g., convolution with different filter sizes), one is able to find an optimal architecture that is better adapted to the problem at hand. However, so far the success of NAS has not been enjoyed by low-level geometric vision tasks such as stereo matching. This is partly due to the fact that state-of-the-art deep stereo matching networks, designed by humans, are already sheer in size. Directly applying the NAS to such massive structures is computationally prohibitive based on the currently available mainstream computing resources. In this paper, we propose the first end-to-end hierarchical NAS framework for deep stereo matching by incorporating task-specific human knowledge into the neural architecture search framework. Specifically, following the gold standard pipeline for deep stereo matching (i.e., feature extraction -- feature volume construction and dense matching), we optimize the architectures of the entire pipeline jointly. Extensive experiments show that our searched network outperforms all state-of-the-art deep stereo matching architectures and is ranked at the top 1 accuracy on KITTI stereo 2012, 2015 and Middlebury benchmarks, as well as the top 1 on SceneFlow dataset with a substantial improvement on the size of the network and the speed of inference. The code is available at this https URL.",
            "year": 2020,
            "venue": "Neural Information Processing Systems",
            "authors": [
              {
                "authorId": "47172429",
                "name": "Xuelian Cheng"
              },
              {
                "authorId": "2015152",
                "name": "Yiran Zhong"
              },
              {
                "authorId": "1686714",
                "name": "Mehrtash Harandi"
              },
              {
                "authorId": "1681554",
                "name": "Yuchao Dai"
              },
              {
                "authorId": "144950946",
                "name": "Xiaojun Chang"
              },
              {
                "authorId": "144418842",
                "name": "T. Drummond"
              },
              {
                "authorId": "46382489",
                "name": "Hongdong Li"
              },
              {
                "authorId": "1808390",
                "name": "ZongYuan Ge"
              }
            ]
          }
        },
        {
          "citedcorpusid": 226254259,
          "isinfluential": false,
          "contexts": [
            "A recent trend in this field [34,38,65,71,83,84] introduced innovative deep stereo networks that embrace an iterative refinement paradigm or use Vision Transformers [22,35]."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Revisiting Stereo Depth Estimation From a Sequence-to-Sequence Perspective with Transformers",
            "abstract": "Stereo depth estimation relies on optimal correspondence matching between pixels on epipolar lines in the left and right images to infer depth. In this work, we revisit the problem from a sequence-to-sequence correspondence perspective to replace cost volume construction with dense pixel matching using position information and attention. This approach, named STereo TRansformer (STTR), has several advantages: It 1) relaxes the limitation of a fixed disparity range, 2) identifies occluded regions and provides confidence estimates, and 3) imposes uniqueness constraints during the matching process. We report promising results on both synthetic and real-world datasets and demonstrate that STTR generalizes across different domains, even without fine-tuning.",
            "year": 2020,
            "venue": "IEEE International Conference on Computer Vision",
            "authors": [
              {
                "authorId": "1491078396",
                "name": "Zhaoshuo Li"
              },
              {
                "authorId": "120281016",
                "name": "Xingtong Liu"
              },
              {
                "authorId": "5689165",
                "name": "Francis X Creighton"
              },
              {
                "authorId": "1684883",
                "name": "R. Taylor"
              },
              {
                "authorId": "3197182",
                "name": "M. Unberath"
              }
            ]
          }
        },
        {
          "citedcorpusid": 232104918,
          "isinfluential": false,
          "contexts": [
            "Eventually, contemporary approaches integrated depth from sensors with modern stereo networks, either by concatenating them to images as input [12,46,69,81] or by using them to guide the cost optimization process by modulating existing cost volumes [25, 49, 69, 82].",
            "…of i) concatenating the two modalities and processing them as joint inputs with a stereo network [12,46,69,81], ii) modulating the internal cost volume computed by the backbone itself [25,49,69,82] or, more recently, iii) projecting distinctive patterns on images according to depth hints [4]."
          ],
          "intents": [
            "['background']",
            "--"
          ],
          "cited_paper_info": {
            "title": "S3: Learnable Sparse Signal Superdensity for Guided Depth Estimation",
            "abstract": "Dense depth estimation plays a key role in multiple applications such as robotics, 3D reconstruction, and augmented reality. While sparse signal, e.g., LiDAR and Radar, has been leveraged as guidance for enhancing dense depth estimation, the improvement is limited due to its low density and imbalanced distribution. To maximize the utility from the sparse source, we propose Sparse Signal Superdensity (S3) technique, which expands the depth value from sparse cues while estimating the confidence of expanded region. The proposed S3 can be applied to various guided depth estimation approaches and trained end-to-end at different stages, including input, cost volume and output. Extensive experiments demonstrate the effectiveness, robustness, and flexibility of the S3 technique on LiDAR and Radar signal.",
            "year": 2021,
            "venue": "Computer Vision and Pattern Recognition",
            "authors": [
              {
                "authorId": "2108736469",
                "name": "Yu-Kai Huang"
              },
              {
                "authorId": "1614037619",
                "name": "Yueh-Cheng Liu"
              },
              {
                "authorId": "2142343820",
                "name": "Tsung-Han Wu"
              },
              {
                "authorId": "71309591",
                "name": "Hung-Ting Su"
              },
              {
                "authorId": "1557268505",
                "name": "Yu-Cheng Chang"
              },
              {
                "authorId": "2076736485",
                "name": "T. Tsou"
              },
              {
                "authorId": "2156167389",
                "name": "Yu-An Wang"
              },
              {
                "authorId": "1716836",
                "name": "Winston H. Hsu"
              }
            ]
          }
        },
        {
          "citedcorpusid": 233204703,
          "isinfluential": false,
          "contexts": [
            "In contrast, the latter constructs a feature cost volume from image pair features and estimates the disparity map through 3D convolutions at the cost of substantially higher memory and runtime demands [10,11,13,16,23, 27,28,57,70,73,80]."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "CFNet: Cascade and Fused Cost Volume for Robust Stereo Matching",
            "abstract": "Recently, the ever-increasing capacity of large-scale annotated datasets has led to profound progress in stereo matching. However, most of these successes are limited to a specific dataset and cannot generalize well to other datasets. The main difficulties lie in the large domain differences and unbalanced disparity distribution across a variety of datasets, which greatly limit the real-world applicability of current deep stereo matching models. In this paper, we propose CFNet, a Cascade and Fused cost volume based network to improve the robustness of the stereo matching network. First, we propose a fused cost volume representation to deal with the large domain difference. By fusing multiple low-resolution dense cost volumes to enlarge the receptive field, we can extract robust structural representations for initial disparity estimation. Second, we propose a cascade cost volume representation to alleviate the unbalanced disparity distribution. Specifically, we employ a variance-based uncertainty estimation to adaptively adjust the next stage disparity search space, in this way driving the network progressively prune out the space of unlikely correspondences. By iteratively narrowing down the disparity search space and improving the cost volume resolution, the disparity estimation is gradually refined in a coarse-to-fine manner. When trained on the same training images and evaluated on KITTI, ETH3D, and Middlebury datasets with the fixed model parameters and hyperparameters, our proposed method achieves the state-of-the-art overall performance and obtains the 1st place on the stereo task of Robust Vision Challenge 2020. The code will be available at https://github.com/gallenszl/CFNet.",
            "year": 2021,
            "venue": "Computer Vision and Pattern Recognition",
            "authors": [
              {
                "authorId": null,
                "name": "Zhelun Shen"
              },
              {
                "authorId": "1681554",
                "name": "Yuchao Dai"
              },
              {
                "authorId": "67220664",
                "name": "Zhibo Rao"
              }
            ]
          }
        },
        {
          "citedcorpusid": 247596980,
          "isinfluential": false,
          "contexts": [
            "A recent trend in this field [34,38,65,71,83,84] introduced innovative deep stereo networks that embrace an iterative refinement paradigm or use Vision Transformers [22,35]."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Practical Stereo Matching via Cascaded Recurrent Network with Adaptive Correlation",
            "abstract": "With the advent of convolutional neural networks, stereo matching algorithms have recently gained tremendous progress. However, it remains a great challenge to accurately extract disparities from real-world image pairs taken by consumer-level devices like smartphones, due to practical complicating factors such as thin structures, non-ideal rectification, camera module inconsistencies and various hard-case scenes. In this paper, we propose a set of innovative designs to tackle the problem of practical stereo matching: 1) to better recover fine depth details, we design a hierarchical network with recurrent refinement to update disparities in a coarse-to-fine manner, as well as a stacked cascaded architecture for inference; 2) we propose an adaptive group correlation layer to mitigate the impact of erroneous rectification; 3) we introduce a new synthetic dataset with special attention to difficult cases for better generalizing to real-world scenes. Our results not only rank 1st on both Middlebury and ETH3D benchmarks, outperforming existing state-of-the-art methods by a notable margin, but also exhibit high-quality details for real-life photos, which clearly demonstrates the efficacy of our contributions.",
            "year": 2022,
            "venue": "Computer Vision and Pattern Recognition",
            "authors": [
              {
                "authorId": "2812735",
                "name": "Jiankun Li"
              },
              {
                "authorId": "2855307",
                "name": "Peisen Wang"
              },
              {
                "authorId": "40448951",
                "name": "Pengfei Xiong"
              },
              {
                "authorId": "2072572644",
                "name": "Tao Cai"
              },
              {
                "authorId": "12977189",
                "name": "Zi-Ping Yan"
              },
              {
                "authorId": "2145452797",
                "name": "Lei Yang"
              },
              {
                "authorId": "2119612247",
                "name": "Jiangyu Liu"
              },
              {
                "authorId": "1934546",
                "name": "Haoqiang Fan"
              },
              {
                "authorId": "2108589268",
                "name": "Shuaicheng Liu"
              }
            ]
          }
        },
        {
          "citedcorpusid": 250374739,
          "isinfluential": false,
          "contexts": [
            "The latter adopts data-driven Convolutional Neural Networks (CNNs) to infer dense depth maps [43, 66, 67]."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Unsupervised Deep Event Stereo for Depth Estimation",
            "abstract": "Bio-inspired event cameras have been considered effective alternatives to traditional frame-based cameras for stereo depth estimation, especially in challenging conditions such as low-light or high-speed environments. Recently, deep learning-based supervised event stereo matching methods have achieved significant performance improvements over the traditional event stereo methods. However, the supervised methods depend on ground-truth disparity maps for training, and it is difficult to secure a large amount of ground-truth disparity maps. A feasible alternative is to devise an unsupervised event stereo method that can be trained without ground-truth disparity maps. To this end, we propose the first unsupervised event stereo matching method that can predict dense disparity maps, and is trained by transforming the depth estimation problem into a warping-based reconstruction problem. We propose a novel unsupervised loss function that enforces the network to minimize the feature-level epipolar correlation difference between the ground-truth intensity images and warped images. Moreover, we propose a novel event embedding mechanism that utilizes both temporal and spatial neighboring events to capture spatio-temporal relationships among the events for stereo matching. Experimental results reveal that the proposed method outperforms the baseline unsupervised methods by significant margins (e.g., up to 16.88% improvement) and achieves comparable results with the existing supervised methods. Extensive ablation studies validate the efficacy of the proposed modules and architectural choices.",
            "year": 2022,
            "venue": "IEEE transactions on circuits and systems for video technology (Print)",
            "authors": [
              {
                "authorId": "40621769",
                "name": "S M Nadim Uddin"
              },
              {
                "authorId": "2175478044",
                "name": "Soikat Hasan Ahmed"
              },
              {
                "authorId": "48895895",
                "name": "Yong Ju Jung"
              }
            ]
          }
        },
        {
          "citedcorpusid": 253080413,
          "isinfluential": false,
          "contexts": [
            "A recent trend in this field [34,38,65,71,83,84] introduced innovative deep stereo networks that embrace an iterative refinement paradigm or use Vision Transformers [22,35]."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Context-Enhanced Stereo Transformer",
            "abstract": "Stereo depth estimation is of great interest for computer vision research. However, existing methods struggles to generalize and predict reliably in hazardous regions, such as large uniform regions. To overcome these limitations, we propose Context Enhanced Path (CEP). CEP improves the generalization and robustness against common failure cases in existing solutions by capturing the long-range global information. We construct our stereo depth estimation model, Context Enhanced Stereo Transformer (CSTR), by plugging CEP into the state-of-the-art stereo depth estimation method Stereo Transformer. CSTR is examined on distinct public datasets, such as Scene Flow, Middlebury-2014, KITTI-2015, and MPI-Sintel. We find CSTR outperforms prior approaches by a large margin. For example, in the zero-shot synthetic-to-real setting, CSTR outperforms the best competing approaches on Middlebury-2014 dataset by 11%. Our extensive experiments demonstrate that the long-range information is critical for stereo matching task and CEP successfully captures such information.",
            "year": 2022,
            "venue": "European Conference on Computer Vision",
            "authors": [
              {
                "authorId": "1588153345",
                "name": "Weiyu Guo"
              },
              {
                "authorId": "1491078396",
                "name": "Zhaoshuo Li"
              },
              {
                "authorId": "2047908098",
                "name": "Yongkui Yang"
              },
              {
                "authorId": "11032852",
                "name": "Z. Wang"
              },
              {
                "authorId": "144041441",
                "name": "Russell H. Taylor"
              },
              {
                "authorId": "3197182",
                "name": "M. Unberath"
              },
              {
                "authorId": "145081362",
                "name": "A. Yuille"
              },
              {
                "authorId": "2111160883",
                "name": "Yingwei Li"
              }
            ]
          }
        },
        {
          "citedcorpusid": 257407018,
          "isinfluential": false,
          "contexts": [
            "A recent trend in this field [34,38,65,71,83,84] introduced innovative deep stereo networks that embrace an iterative refinement paradigm or use Vision Transformers [22,35]."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "EAI-Stereo: Error Aware Iterative Network for Stereo Matching",
            "abstract": "",
            "year": 2022,
            "venue": "Asian Conference on Computer Vision",
            "authors": [
              {
                "authorId": "2112674491",
                "name": "Haoliang Zhao"
              },
              {
                "authorId": "2193719588",
                "name": "Huizhou Zhou"
              },
              {
                "authorId": "1591131546",
                "name": "Yongjun Zhang"
              },
              {
                "authorId": "2151269656",
                "name": "Yong Zhao"
              },
              {
                "authorId": "1390863714",
                "name": "Yitong Yang"
              },
              {
                "authorId": "2210993430",
                "name": "Ting Ouyang"
              }
            ]
          }
        },
        {
          "citedcorpusid": 257495841,
          "isinfluential": false,
          "contexts": [
            "A recent trend in this field [34,38,65,71,83,84] introduced innovative deep stereo networks that embrace an iterative refinement paradigm or use Vision Transformers [22,35]."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Iterative Geometry Encoding Volume for Stereo Matching",
            "abstract": "Recurrent All-Pairs Field Transforms (RAFT) has shown great potentials in matching tasks. However, all-pairs correlations lack non-local geometry knowledge and have difficulties tackling local ambiguities in ill-posed regions. In this paper, we propose Iterative Geometry Encoding Volume (IGEV-Stereo), a new deep network architecture for stereo matching. The proposed IGEV-Stereo builds a combined geometry encoding volume that encodes geometry and context information as well as local matching details, and iteratively indexes it to update the disparity map. To speed up the convergence, we exploit GEV to regress an accurate starting point for ConvGRUs iterations. Our IGEV-Stereo ranks 1st on KITTI 2015 and 2012 (Reflective) among all published methods and is the fastest among the top 10 methods. In addition, IGEV-Stereo has strong cross-dataset generalization as well as high inference efficiency. We also extend our IGEV to multi-view stereo (MVS), i.e. IGEV-MVS, which achieves competitive accuracy on DTU benchmark. Code is available at https://github.com/gangweiX/IGEV.",
            "year": 2023,
            "venue": "Computer Vision and Pattern Recognition",
            "authors": [
              {
                "authorId": "2158317969",
                "name": "Gangwei Xu"
              },
              {
                "authorId": "2107956900",
                "name": "Xianqi Wang"
              },
              {
                "authorId": "2117436225",
                "name": "Xiao-Hua Ding"
              },
              {
                "authorId": "2150441002",
                "name": "Xin Yang"
              }
            ]
          }
        },
        {
          "citedcorpusid": 258187423,
          "isinfluential": false,
          "contexts": [
            "Although LiDAR sensors and event cameras have been deployed together for some applications [6,14,20,33,55,58,61], this paper represents the first attempt at combining LiDAR with an event stereo framework."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Event Camera and LiDAR based Human Tracking for Adverse Lighting Conditions in Subterranean Environments",
            "abstract": "In this article, we propose a novel LiDAR and event camera fusion modality for subterranean (SubT) environments for fast and precise object and human detection in a wide variety of adverse lighting conditions, such as low or no light, high-contrast zones and in the presence of blinding light sources. In the proposed approach, information from the event camera and LiDAR are fused to localize a human or an object-of-interest in a robot's local frame. The local detection is then transformed into the inertial frame and used to set references for a Nonlinear Model Predictive Controller (NMPC) for reactive tracking of humans or objects in SubT environments. The proposed novel fusion uses intensity filtering and K-means clustering on the LiDAR point cloud and frequency filtering and connectivity clustering on the events induced in an event camera by the returning LiDAR beams. The centroids of the clusters in the event camera and LiDAR streams are then paired to localize reflective markers present on safety vests and signs in SubT environments. The efficacy of the proposed scheme has been experimentally validated in a real SubT environment (a mine) with a Pioneer 3AT mobile robot. The experimental results show real-time performance for human detection and the NMPC-based controller allows for reactive tracking of a human or object of interest, even in complete darkness.",
            "year": 2023,
            "venue": "IFAC-PapersOnLine",
            "authors": [
              {
                "authorId": "2214624139",
                "name": "Mario A. V. Saucedo"
              },
              {
                "authorId": "2115922518",
                "name": "Akash Patel"
              },
              {
                "authorId": "2247527",
                "name": "Rucha Sawlekar"
              },
              {
                "authorId": "37072537",
                "name": "Akshit Saradagi"
              },
              {
                "authorId": "3491300",
                "name": "Christoforos Kanellakis"
              },
              {
                "authorId": "2068120981",
                "name": "A. Agha-mohammadi"
              },
              {
                "authorId": "1912308",
                "name": "G. Nikolakopoulos"
              }
            ]
          }
        },
        {
          "citedcorpusid": 259380779,
          "isinfluential": false,
          "contexts": [
            "…consisting of i) modulating the cost volume built by the backbone – Guided Stereo Matching [49], ii) concatenating the sparse depth values to the inputs to the stereo network – e.g ., as done by LidarStereoNet [12], iii) a combination of both the previous strategies – in DSEC [21] M3ED [9] Fig.",
            "M3ED [9].",
            "Our strategies outperform existing alternatives inherited from RGB stereo literature on DSEC [21] and M3ED [9] datasets – VSH and BTH can exploit even outdated LiDAR data to increase the event stream distinctiveness and ease matching, preserving the microsecond resolution of event cameras and…"
          ],
          "intents": [
            "['methodology']",
            "--",
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "M3ED: Multi-Robot, Multi-Sensor, Multi-Environment Event Dataset",
            "abstract": "We present M3ED, the first multi-sensor event camera dataset focused on high-speed dynamic motions in robotics applications. M3ED provides high-quality synchronized and labeled data from multiple platforms, including ground vehicles, legged robots, and aerial robots, operating in challenging conditions such as driving along off-road trails, navigating through dense forests, and performing aggressive flight maneuvers. Our dataset also covers demanding operational scenarios for event cameras, such as scenes with high egomotion and multiple independently moving objects. The sensor suite used to collect M3ED includes high-resolution stereo event cameras (1280×720), grayscale imagers, an RGB imager, a high-quality IMU, a 64-beam LiDAR, and RTK localization. This dataset aims to accelerate the development of event-based algorithms and methods for edge cases encountered by autonomous systems in dynamic environments.",
            "year": 2023,
            "venue": "2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)",
            "authors": [
              {
                "authorId": "20728097",
                "name": "Kenneth Chaney"
              },
              {
                "authorId": "3163734",
                "name": "Fernando Cladera Ojeda"
              },
              {
                "authorId": "2117966677",
                "name": "Ziyun Wang"
              },
              {
                "authorId": "30814125",
                "name": "Anthony Bisulco"
              },
              {
                "authorId": "144062128",
                "name": "M. A. Hsieh"
              },
              {
                "authorId": "2311918",
                "name": "C. Korpela"
              },
              {
                "authorId": "37956314",
                "name": "Vijay R. Kumar"
              },
              {
                "authorId": "31589308",
                "name": "C. J. Taylor"
              },
              {
                "authorId": "1751586",
                "name": "Kostas Daniilidis"
              }
            ]
          }
        },
        {
          "citedcorpusid": 260779095,
          "isinfluential": false,
          "contexts": [
            "A recent trend in this field [34,38,65,71,83,84] introduced innovative deep stereo networks that embrace an iterative refinement paradigm or use Vision Transformers [22,35]."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "High-Frequency Stereo Matching Network",
            "abstract": "In the field of binocular stereo matching, remarkable progress has been made by iterative methods like RAFT-Stereo and CREStereo. However, most of these methods lose information during the iterative process, making it difficult to generate more detailed difference maps that take full advantage of high-frequency information. We propose the Decouple module to alleviate the problem of data coupling and allow features containing subtle details to transfer across the iterations which proves to alleviate the problem significantly in the ablations. To further capture high-frequency details, we propose a Normalization Refinement module that unifies the disparities as a proportion of the disparities over the width of the image, which address the problem of module failure in cross-domain scenarios. Further, with the above improvements, the ResNet-like feature extractor that has not been changed for years becomes a bottleneck. Towards this end, we proposed a multi-scale and multi-stage feature extractor that introduces the channel-wise self-attention mechanism which greatly addresses this bottleneck. Our method (DLNR) ranks 1st on the Middlebury leaderboard, significantly outperforming the next best method by 13.04%. Our method also achieves SOTA performance on the KITTI-2015 benchmark for D1-fg. Code and demos are available at: https://github.com/David-Zhao-1997/High-frequency-Stereo-Matching-Network.",
            "year": 2023,
            "venue": "Computer Vision and Pattern Recognition",
            "authors": [
              {
                "authorId": "2112674491",
                "name": "Haoliang Zhao"
              },
              {
                "authorId": "2193719588",
                "name": "Huizhou Zhou"
              },
              {
                "authorId": "1591131546",
                "name": "Yongjun Zhang"
              },
              {
                "authorId": "47740650",
                "name": "Jing Chen"
              },
              {
                "authorId": "1390863714",
                "name": "Yitong Yang"
              },
              {
                "authorId": "2151269656",
                "name": "Yong Zhao"
              }
            ]
          }
        },
        {
          "citedcorpusid": 261031728,
          "isinfluential": false,
          "contexts": [
            "Even so, complex event representations – e.g ., ERGO-12 [87] – can better generalize.",
            "ERGO-12 [87].",
            "Starting from baseline models, we can notice how the different representations have an impact on the accuracy of the stereo backbone, with those modeling complex behaviors – e.g ., Time Surface [32] or ERGO-12 [87] – yielding up to 2% lower 1PE than simpler ones such as Histogram [39]."
          ],
          "intents": [
            "['background']",
            "--",
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "From Chaos Comes Order: Ordering Event Representations for Object Recognition and Detection",
            "abstract": "Today, state-of-the-art deep neural networks that process events first convert them into dense, grid-like input representations before using an off-the-shelf network. However, selecting the appropriate representation for the task traditionally requires training a neural network for each representation and selecting the best one based on the validation score, which is very time-consuming. This work eliminates this bottleneck by selecting representations based on the Gromov-Wasserstein Discrepancy (GWD) between raw events and their representation. It is about 200 times faster to compute than training a neural network and preserves the task performance ranking of event representations across multiple representations, network backbones, datasets, and tasks. Thus finding representations with high task scores is equivalent to finding representations with a low GWD. We use this insight to, for the first time, perform a hyperparameter search on a large family of event representations, revealing new and powerful representations that exceed the state-of-the-art. Our optimized representations outperform existing representations by 1.7 mAP on the 1 Mpx dataset and 0.3 mAP on the Gen1 dataset, two established object detection benchmarks, and reach a 3.8% higher classification score on the mini N-ImageNet benchmark. Moreover, we outperform state-of-the-art by 2.1 mAP on Gen1 and state-of-the-art feed-forward methods by 6.0 mAP on the 1 Mpx datasets. This work opens a new unexplored field of explicit representation optimization for event-based learning.",
            "year": 2023,
            "venue": "IEEE International Conference on Computer Vision",
            "authors": [
              {
                "authorId": "2286866374",
                "name": "Nikola Zubic"
              },
              {
                "authorId": "51152279",
                "name": "Daniel Gehrig"
              },
              {
                "authorId": "8329387",
                "name": "Mathias Gehrig"
              },
              {
                "authorId": "2075371",
                "name": "D. Scaramuzza"
              }
            ]
          }
        },
        {
          "citedcorpusid": 262083814,
          "isinfluential": true,
          "contexts": [
            "[4] followed a different path with Virtual Pattern Projection (VPP).",
            "Inspired by [4], events might be optionally hallucinated in patches rather than single pixels.",
            "…of i) concatenating the two modalities and processing them as joint inputs with a stereo network [12,46,69,81], ii) modulating the internal cost volume computed by the backbone itself [25,49,69,82] or, more recently, iii) projecting distinctive patterns on images according to depth hints [4].",
            "We deploy a generalized version of the random pattern operator A proposed in [4], agnostic to the stacked representation: with S − and S + the minimum and maximum values appearing across stacks S L , S R and U a uniform random distribution.",
            "Following [4], the pattern can either cover a single pixel or a local window.",
            "Inspired by [4], which projects distinctive color patterns on the images consistently with measured depth, we design a hallucination mechanism to generate fictitious events over time to densify the stream collected by the event cameras.",
            "This strategy alone is sufficient already to ensure distinctiveness and to dramatically ease matching across stacks, even more than with color images [4], since acting on semi-dense structures – i.e ., stacks are uninformative in the absence of events.",
            "According to the RGB stereo literature, fusing color information with sparse depth measurements from an active sensor [4,12,49,82] ( e.g ., a LiDAR) considerably softens the weaknesses of passive depth sensing, despite the much lower resolution at which depth points are provided.",
            "It also ensures a straight-forward application of the same principles used on RGB images, e.g ., to combine the original content (color) with the virtual projection (pattern) employing alpha blending [4].",
            "In this paper, starting from the RGB literature [4,12,49,82], we embark on a comprehensive investigation into the fusion of event-based stereo with sparse depth hints from active sensors."
          ],
          "intents": [
            "['methodology']",
            "--",
            "--",
            "['methodology']",
            "['background']",
            "['background']",
            "['background']",
            "['background']",
            "['methodology']",
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Active Stereo Without Pattern Projector",
            "abstract": "This paper proposes a novel framework integrating the principles of active stereo in standard passive camera systems without a physical pattern projector. We virtually project a pattern over the left and right images according to the sparse measurements obtained from a depth sensor. Any such devices can be seamlessly plugged into our framework, allowing for the deployment of a virtual active stereo setup in any possible environment, overcoming the limitation of pattern projectors, such as limited working range or environmental conditions. Experiments on indoor/outdoor datasets, featuring both long and close-range, support the seamless effectiveness of our approach, boosting the accuracy of both stereo algorithms and deep networks.",
            "year": 2023,
            "venue": "IEEE International Conference on Computer Vision",
            "authors": [
              {
                "authorId": "2243335739",
                "name": "Luca Bartolomei"
              },
              {
                "authorId": "2509750",
                "name": "Matteo Poggi"
              },
              {
                "authorId": "121670758",
                "name": "Fabio Tosi"
              },
              {
                "authorId": "2145601115",
                "name": "Andrea Conti"
              },
              {
                "authorId": "10261545",
                "name": "Stefano Mattoccia"
              }
            ]
          }
        },
        {
          "citedcorpusid": 269983050,
          "isinfluential": false,
          "contexts": [
            "The former, inspired by the U-Net model [53], adopts an encoder-decoder design [37,42,45,50, 54,59,63,64,74,77]."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Federated Online Adaptation for Deep Stereo",
            "abstract": "We introduce a novel approach for adapting deep stereo networks in a collaborative manner. By building over principles of federated learning, we develop a distributed framework allowing for demanding the optimization process to a number of clients deployed in different environments. This makes it possible, for a deep stereo network running on resourced-constrained devices, to capitalize on the adaptation process carried out by other instances of the same architecture, and thus improve its accuracy in challenging environments even when it cannot carry out adaptation on its own. Experimental results show how federated adaptation performs equivalently to on-device adaptation, and even better when dealing with challenging environments.",
            "year": 2024,
            "venue": "Computer Vision and Pattern Recognition",
            "authors": [
              {
                "authorId": "2509750",
                "name": "Matteo Poggi"
              },
              {
                "authorId": "121670758",
                "name": "Fabio Tosi"
              }
            ]
          }
        }
      ]
    },
    "56475917": {
      "citing_paper_info": {
        "title": "Unsupervised Event-Based Learning of Optical Flow, Depth, and Egomotion",
        "abstract": "In this work, we propose a novel framework for unsupervised learning for event cameras that learns motion information from only the event stream. In particular, we propose an input representation of the events in the form of a discretized volume that maintains the temporal distribution of the events, which we pass through a neural network to predict the motion of the events. This motion is used to attempt to remove any motion blur in the event image. We then propose a loss function applied to the motion compensated event image that measures the motion blur in this image. We train two networks with this framework, one to predict optical flow, and one to predict egomotion and depths, and evaluate these networks on the Multi Vehicle Stereo Event Camera dataset, along with qualitative results from a variety of different scenes.",
        "year": 2018,
        "venue": "Computer Vision and Pattern Recognition",
        "authors": [
          {
            "authorId": "3385588",
            "name": "A. Z. Zhu"
          },
          {
            "authorId": "36001694",
            "name": "Liangzhe Yuan"
          },
          {
            "authorId": "20728097",
            "name": "Kenneth Chaney"
          },
          {
            "authorId": "1751586",
            "name": "Kostas Daniilidis"
          }
        ]
      },
      "citation_summary": {
        "citation_count": 8,
        "unique_cited_count": 8,
        "influential_count": 1,
        "detailed_records_count": 8
      },
      "cited_papers": [
        "19160323",
        "20619009",
        "3845250",
        "26324573",
        "703552",
        "3328976",
        "11977588",
        "2497402"
      ],
      "citation_details": [
        {
          "citedcorpusid": 703552,
          "isinfluential": false,
          "contexts": [
            "However, the number of events between the two cameras may also differ, and so we apply a similarity loss on the census transforms [21] of the images.",
            "2, and a robust similarity loss between the census transforms [21, 17] of the deblurred event images."
          ],
          "intents": [
            "['methodology']",
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Non-parametric Local Transforms for Computing Visual Correspondence",
            "abstract": "",
            "year": 1994,
            "venue": "European Conference on Computer Vision",
            "authors": [
              {
                "authorId": "2984143",
                "name": "R. Zabih"
              },
              {
                "authorId": "1803592",
                "name": "J. Woodfill"
              }
            ]
          }
        },
        {
          "citedcorpusid": 2497402,
          "isinfluential": false,
          "contexts": [
            "[10], there has been a strong interest in the development of algorithms that leverage the benefits provided by these cameras."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "> Replace This Line with Your Paper Identification Number (double-click Here to Edit) < 1",
            "abstract": "—This paper describes a CMOS vision sensor which is inspired by biological visual systems. Each pixel independently and in continuous time quantizes local relative intensity changes to generate spike events. These events appear at the output of the sensor as an asynchronous stream of digital pixel addresses. These address-events signify scene reflectance change and have sub-millisecond timing precision. The output data rate depends on the dynamic content of the scene and is typically orders of magnitude lower than those of conventional frame-based imagers. By combining an active front-end logarithmic photoreceptor running in continuous time with a self-timed switched-capacitor differencing circuit, the sensor achieves an array mismatch of 2.1% in relative intensity event threshold and a pixel bandwidth of 3 kHz under 1 klux scene illumination. Dynamic range is >120 dB and chip power consumption is 23 mW. Event latency shows weak light dependency and decreases to 15 us at >1 klux pixel illumination. The sensor is built in a 0.35u 4M2P process yielding 40x40 um 2 pixels with 9.4% fill-factor. By providing high pixel bandwidth, wide dynamic range, and precisely-timed sparse digital output, this neuromorphic silicon retina provides an attractive combination of characteristics for low-latency dynamic vision under uncontrolled illumination with low post-processing requirements.",
            "year": null,
            "venue": "",
            "authors": []
          }
        },
        {
          "citedcorpusid": 3328976,
          "isinfluential": false,
          "contexts": [
            "Recently, there have been several works, such as [4, 5, 13, 26, 24], that have shown that optical ﬂow, and other types of motion information, can be estimated from a spatiotemporal volume of events, by propagating the events along the optical ﬂow direction, and attempting to minimize the motion…"
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Accurate Angular Velocity Estimation With an Event Camera",
            "abstract": "",
            "year": 2017,
            "venue": "IEEE Robotics and Automation Letters",
            "authors": [
              {
                "authorId": "144036711",
                "name": "Guillermo Gallego"
              },
              {
                "authorId": "2075371",
                "name": "D. Scaramuzza"
              }
            ]
          }
        },
        {
          "citedcorpusid": 3845250,
          "isinfluential": false,
          "contexts": [
            "[13], who use a loss which minimizes the sum of squares of the average timestamp at each pixel.",
            "[13] for a neural network, by generating a single fully differentiable loss function that allows our networks to learn optical flow and structure from motion in an unsupervised manner."
          ],
          "intents": [
            "['background']",
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Event-Based Moving Object Detection and Tracking",
            "abstract": "Event-based vision sensors, such as the Dynamic Vision Sensor (DVS), are ideally suited for real-time motion analysis. The unique properties encompassed in the readings of such sensors provide high temporal resolution, superior sensitivity to light and low latency. These properties provide the grounds to estimate motion efficiently and reliably in the most sophisticated scenarios, but these advantages come at a price - modern event-based vision sensors have extremely low resolution, produce a lot of noise and require the development of novel algorithms to handle the asynchronous event stream. This paper presents a new, efficient approach to object tracking with asynchronous cameras. We present a novel event stream representation which enables us to utilize information about the dynamic (temporal)component of the event stream. The 3D geometry of the event stream is approximated with a parametric model to motion-compensate for the camera (without feature tracking or explicit optical flow computation), and then moving objects that don't conform to the model are detected in an iterative process. We demonstrate our framework on the task of independent motion detection and tracking, where we use the temporal model inconsistencies to locate differently moving objects in challenging situations of very fast motion.",
            "year": 2018,
            "venue": "IEEE/RJS International Conference on Intelligent RObots and Systems",
            "authors": [
              {
                "authorId": "144559298",
                "name": "A. Mitrokhin"
              },
              {
                "authorId": "1759899",
                "name": "C. Fermüller"
              },
              {
                "authorId": "38904651",
                "name": "Chethan Parameshwara"
              },
              {
                "authorId": "1697493",
                "name": "Y. Aloimonos"
              }
            ]
          }
        },
        {
          "citedcorpusid": 11977588,
          "isinfluential": true,
          "contexts": [
            "As the translation predicted by SFMLearner is only up to a scale factor, we present errors in terms of angular error between both the predicted translation and rotations.",
            "As there is currently no public code to the extent of our knowledge for unsupervised deep SFM methods with a stereo loss, we compare our ego-motion results against SFMLearner by Zhou et al. [22], which learns egomotion and depth from monocular grayscale images, while ac-knowledging that our loss has access to an additional stereo image at training time.",
            "We train the SFMLearner models on the VI-Sensor images from the outdoor day2 sequence, once again cropping out the hood of the car.",
            "[23] show that a network can learn a camera’s egomotion and depth using camera reprojection and a photoconsistency loss.",
            "As there is currently no public code to the extent of our knowledge for unsupervised deep SFM methods with a stereo loss, we compare our ego-motion results against SFMLearner [23], and ECN [20], which learn egomotion and depth from monocular images and events."
          ],
          "intents": [
            "--",
            "--",
            "--",
            "['background']",
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Unsupervised Learning of Depth and Ego-Motion from Video",
            "abstract": "We present an unsupervised learning framework for the task of monocular depth and camera motion estimation from unstructured video sequences. In common with recent work [10, 14, 16], we use an end-to-end learning approach with view synthesis as the supervisory signal. In contrast to the previous work, our method is completely unsupervised, requiring only monocular video sequences for training. Our method uses single-view depth and multiview pose networks, with a loss based on warping nearby views to the target using the computed depth and pose. The networks are thus coupled by the loss during training, but can be applied independently at test time. Empirical evaluation on the KITTI dataset demonstrates the effectiveness of our approach: 1) monocular depth performs comparably with supervised methods that use either ground-truth pose or depth for training, and 2) pose estimation performs favorably compared to established SLAM systems under comparable input settings.",
            "year": 2017,
            "venue": "Computer Vision and Pattern Recognition",
            "authors": [
              {
                "authorId": "1822702",
                "name": "Tinghui Zhou"
              },
              {
                "authorId": "144735785",
                "name": "Matthew A. Brown"
              },
              {
                "authorId": "1830653",
                "name": "Noah Snavely"
              },
              {
                "authorId": "35238678",
                "name": "D. Lowe"
              }
            ]
          }
        },
        {
          "citedcorpusid": 19160323,
          "isinfluential": false,
          "contexts": [
            "1, we can see that our method outperforms EV-FlowNet in almost all experiments, and nears the performance of UnFlow on the short 1 frame sequences.",
            "1, where we compare our results against EV-FlowNet [24] and the image method UnFlow [12].",
            "[12] extend this work by applying a bidirectional census loss to improve the quality of the flow."
          ],
          "intents": [
            "--",
            "['result']",
            "['background']"
          ],
          "cited_paper_info": {
            "title": "UnFlow: Unsupervised Learning of Optical Flow with a Bidirectional Census Loss",
            "abstract": "\n \n In the era of end-to-end deep learning, many advances in computer vision are driven by large amounts of labeled data. In the optical flow setting, however, obtaining dense per-pixel ground truth for real scenes is difficult and thus such data is rare. Therefore, recent end-to-end convolutional networks for optical flow rely on synthetic datasets for supervision, but the domain mismatch between training and test scenarios continues to be a challenge. Inspired by classical energy-based optical flow methods, we design an unsupervised loss based on occlusion-aware bidirectional flow estimation and the robust census transform to circumvent the need for ground truth flow. On the KITTI benchmarks, our unsupervised approach outperforms previous unsupervised deep networks by a large margin, and is even more accurate than similar supervised methods trained on synthetic datasets alone. By optionally fine-tuning on the KITTI training data, our method achieves competitive optical flow accuracy on the KITTI 2012 and 2015 benchmarks, thus in addition enabling generic pre-training of supervised networks for datasets with limited amounts of ground truth.\n \n",
            "year": 2017,
            "venue": "AAAI Conference on Artificial Intelligence",
            "authors": [
              {
                "authorId": "2141442727",
                "name": "Simon Meister"
              },
              {
                "authorId": "2470340",
                "name": "Junhwa Hur"
              },
              {
                "authorId": "145920814",
                "name": "S. Roth"
              }
            ]
          }
        },
        {
          "citedcorpusid": 20619009,
          "isinfluential": false,
          "contexts": [
            "[25] use an EM based feature tracking method to perform visual-inertial odometry, while Rebecq et al."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Event-Based Visual Inertial Odometry",
            "abstract": "Event-based cameras provide a new visual sensing model by detecting changes in image intensity asynchronously across all pixels on the camera. By providing these events at extremely high rates (up to 1MHz), they allow for sensing in both high speed and high dynamic range situations where traditional cameras may fail. In this paper, we present the first algorithm to fuse a purely event-based tracking algorithm with an inertial measurement unit, to provide accurate metric tracking of a cameras full 6dof pose. Our algorithm is asynchronous, and provides measurement updates at a rate proportional to the camera velocity. The algorithm selects features in the image plane, and tracks spatiotemporal windows around these features within the event stream. An Extended Kalman Filter with a structureless measurement model then fuses the feature tracks with the output of the IMU. The camera poses from the filter are then used to initialize the next step of the tracker and reject failed tracks. We show that our method successfully tracks camera motion on the Event-Camera Dataset in a number of challenging situations.",
            "year": 2017,
            "venue": "Computer Vision and Pattern Recognition",
            "authors": [
              {
                "authorId": "3385588",
                "name": "A. Z. Zhu"
              },
              {
                "authorId": "50365495",
                "name": "Nikolay A. Atanasov"
              },
              {
                "authorId": "1751586",
                "name": "Kostas Daniilidis"
              }
            ]
          }
        },
        {
          "citedcorpusid": 26324573,
          "isinfluential": false,
          "contexts": [
            "[9] demonstrate that a Kalman filter can reconstruct the pose of the camera and a local map."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Real-Time 3D Reconstruction and 6-DoF Tracking with an Event Camera",
            "abstract": "",
            "year": 2016,
            "venue": "European Conference on Computer Vision",
            "authors": [
              {
                "authorId": "3227772",
                "name": "Hanme Kim"
              },
              {
                "authorId": "2864731",
                "name": "Stefan Leutenegger"
              },
              {
                "authorId": "2052135690",
                "name": "A. Davison"
              }
            ]
          }
        }
      ]
    },
    "259937070": {
      "citing_paper_info": {
        "title": "Video Frame Interpolation With Stereo Event and Intensity Cameras",
        "abstract": "The stereo event-intensity camera setup is widely applied to leverage the advantages of both event cameras with low latency and intensity cameras that capture accurate brightness and texture information. However, such a setup commonly encounters cross-modality parallax that is difficult to be eliminated solely with stereo rectification especially for real-world scenes with complex motions and varying depths, posing artifacts and distortion for existing Event-based Video Frame Interpolation (E-VFI) approaches. To tackle this problem, we propose a novel Stereo Event-based VFI (SE-VFI) network (SEVFI-Net) to generate high-quality intermediate frames and corresponding disparities from misaligned inputs consisting of two consecutive keyframes and event streams emitted between them. Specifically, we propose a Feature Aggregation Module (FAM) to alleviate the parallax and achieve spatial alignment in the feature domain. We then exploit the fused features accomplishing accurate optical flow and disparity estimation, and achieving better interpolated results through flow-based and synthesis-based ways. We also build a stereo visual acquisition system composed of an event camera and an RGB-D camera to collect a new Stereo Event-Intensity Dataset (SEID) containing diverse scenes with complex motions and varying depths. Experiments on public real-world stereo datasets, i.e., DSEC and MVSEC, and our SEID dataset, demonstrate that our proposed SEVFI-Net outperforms state-of-the-art methods by a large margin.",
        "year": 2023,
        "venue": "IEEE transactions on multimedia",
        "authors": [
          {
            "authorId": "2223593117",
            "name": "Chao Ding"
          },
          {
            "authorId": "2115913728",
            "name": "Mingyuan Lin"
          },
          {
            "authorId": "143922833",
            "name": "Haijian Zhang"
          },
          {
            "authorId": "114462250",
            "name": "Jian-zhuo Liu"
          },
          {
            "authorId": "2109352265",
            "name": "Lei Yu"
          }
        ]
      },
      "citation_summary": {
        "citation_count": 25,
        "unique_cited_count": 25,
        "influential_count": 7,
        "detailed_records_count": 25
      },
      "cited_papers": [
        "253651036",
        "253553798",
        "244499996",
        "4459013",
        "235719472",
        "253761147",
        "206596513",
        "232170230",
        "4766599",
        "10817557",
        "220713296",
        "212675709",
        "13756489",
        "207761262",
        "202782364",
        "12128172",
        "115151433",
        "13697803",
        "254612876",
        "202786778",
        "17839778",
        "255749379",
        "250644220",
        "211731854",
        "251903532"
      ],
      "citation_details": [
        {
          "citedcorpusid": 4459013,
          "isinfluential": false,
          "contexts": [
            "Warping-based methods [17], [18], [19], [20] use optical ﬂow that perceives motion information between consecutive frames and captures dense correspondences.",
            "Some techniques and information have been utilized to enhance the interpolation performance, e.g. , forward warping [17], context [19], depth [20], and deformable convolution [21], [22].",
            "However, due to the lack of motion information between consecutive input keyframes, most Frame-based VFI (F-VFI) methods are built on simpliﬁed assumptions, e.g. , linear motions [17], [18], [19], [20] or local movements [15], [30], leading to performance degradation in real-world scenarios."
          ],
          "intents": [
            "['methodology']",
            "['methodology']",
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Context-Aware Synthesis for Video Frame Interpolation",
            "abstract": "Video frame interpolation algorithms typically estimate optical flow or its variations and then use it to guide the synthesis of an intermediate frame between two consecutive original frames. To handle challenges like occlusion, bidirectional flow between the two input frames is often estimated and used to warp and blend the input frames. However, how to effectively blend the two warped frames still remains a challenging problem. This paper presents a context-aware synthesis approach that warps not only the input frames but also their pixel-wise contextual information and uses them to interpolate a high-quality intermediate frame. Specifically, we first use a pre-trained neural network to extract per-pixel contextual information for input frames. We then employ a state-of-the-art optical flow algorithm to estimate bidirectional flow between them and pre-warp both input frames and their context maps. Finally, unlike common approaches that blend the pre-warped frames, our method feeds them and their context maps to a video frame synthesis neural network to produce the interpolated frame in a context-aware fashion. Our neural network is fully convolutional and is trained end to end. Our experiments show that our method can handle challenging scenarios such as occlusion and large motion and outperforms representative state-of-the-art approaches.",
            "year": 2018,
            "venue": "2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition",
            "authors": [
              {
                "authorId": "39644974",
                "name": "Simon Niklaus"
              },
              {
                "authorId": "40405236",
                "name": "Feng Liu"
              }
            ]
          }
        },
        {
          "citedcorpusid": 4766599,
          "isinfluential": false,
          "contexts": [
            "To achieve better visual quality, we add the perceptual loss [52] to Lrec that is formulated as:"
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "The Unreasonable Effectiveness of Deep Features as a Perceptual Metric",
            "abstract": "While it is nearly effortless for humans to quickly assess the perceptual similarity between two images, the underlying processes are thought to be quite complex. Despite this, the most widely used perceptual metrics today, such as PSNR and SSIM, are simple, shallow functions, and fail to account for many nuances of human perception. Recently, the deep learning community has found that features of the VGG network trained on ImageNet classification has been remarkably useful as a training loss for image synthesis. But how perceptual are these so-called \"perceptual losses\"? What elements are critical for their success? To answer these questions, we introduce a new dataset of human perceptual similarity judgments. We systematically evaluate deep features across different architectures and tasks and compare them with classic metrics. We find that deep features outperform all previous metrics by large margins on our dataset. More surprisingly, this result is not restricted to ImageNet-trained VGG features, but holds across different deep architectures and levels of supervision (supervised, self-supervised, or even unsupervised). Our results suggest that perceptual similarity is an emergent property shared across deep visual representations.",
            "year": 2018,
            "venue": "2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition",
            "authors": [
              {
                "authorId": "2844849",
                "name": "Richard Zhang"
              },
              {
                "authorId": "2094770",
                "name": "Phillip Isola"
              },
              {
                "authorId": "1763086",
                "name": "Alexei A. Efros"
              },
              {
                "authorId": "2177801",
                "name": "Eli Shechtman"
              },
              {
                "authorId": "39231399",
                "name": "Oliver Wang"
              }
            ]
          }
        },
        {
          "citedcorpusid": 10817557,
          "isinfluential": false,
          "contexts": [
            "2) Benchmark: For VFI task, we compare SEVFI-Net with three open-sourced E-VFI methods, i.e. , CBMNet [38], RE-FID [39] and Time Lens [7]; and seven state-of-the-art F-VFI methods, DAIN [20], EMA-VFI [24], FLAVR [29], RIFE [13], RRIN [16], Super Slomo [57] and TTVFI [25]."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Super SloMo: High Quality Estimation of Multiple Intermediate Frames for Video Interpolation",
            "abstract": "Given two consecutive frames, video interpolation aims at generating intermediate frame(s) to form both spatially and temporally coherent video sequences. While most existing methods focus on single-frame interpolation, we propose an end-to-end convolutional neural network for variable-length multi-frame video interpolation, where the motion interpretation and occlusion reasoning are jointly modeled. We start by computing bi-directional optical flow between the input images using a U-Net architecture. These flows are then linearly combined at each time step to approximate the intermediate bi-directional optical flows. These approximate flows, however, only work well in locally smooth regions and produce artifacts around motion boundaries. To address this shortcoming, we employ another U-Net to refine the approximated flow and also predict soft visibility maps. Finally, the two input images are warped and linearly fused to form each intermediate frame. By applying the visibility maps to the warped images before fusion, we exclude the contribution of occluded pixels to the interpolated intermediate frame to avoid artifacts. Since none of our learned network parameters are time-dependent, our approach is able to produce as many intermediate frames as needed. To train our network, we use 1,132 240-fps video clips, containing 300K individual video frames. Experimental results on several datasets, predicting different numbers of interpolated frames, demonstrate that our approach performs consistently better than existing methods.",
            "year": 2017,
            "venue": "2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition",
            "authors": [
              {
                "authorId": "40175280",
                "name": "Huaizu Jiang"
              },
              {
                "authorId": "3232265",
                "name": "Deqing Sun"
              },
              {
                "authorId": "2745026",
                "name": "V. Jampani"
              },
              {
                "authorId": "37144787",
                "name": "Ming-Hsuan Yang"
              },
              {
                "authorId": "1389846455",
                "name": "E. Learned-Miller"
              },
              {
                "authorId": "1690538",
                "name": "J. Kautz"
              }
            ]
          }
        },
        {
          "citedcorpusid": 12128172,
          "isinfluential": false,
          "contexts": [
            ", forward warping [17], transformer [18], context [19], depth [20], patch-based [21], [22] and deformable convolution [23], [24]."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Frame Rate Upconversion Using Optical Flow and Patch-Based Reconstruction",
            "abstract": "",
            "year": 2016,
            "venue": "IEEE transactions on circuits and systems for video technology (Print)",
            "authors": [
              {
                "authorId": "40091697",
                "name": "Hoda Rezaee Kaviani"
              },
              {
                "authorId": "1732935",
                "name": "S. Shirani"
              }
            ]
          }
        },
        {
          "citedcorpusid": 13697803,
          "isinfluential": false,
          "contexts": [
            "Kernel-based methods [15], [30] incorporate both motion estimation and frame reconstruction.",
            "However, due to the lack of motion information between consecutive input keyframes, most Frame-based VFI (F-VFI) methods are built on simpliﬁed assumptions, e.g. , linear motions [17], [18], [19], [20] or local movements [15], [30], leading to performance degradation in real-world scenarios."
          ],
          "intents": [
            "['methodology']",
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Video Frame Interpolation via Adaptive Separable Convolution",
            "abstract": "Standard video frame interpolation methods first estimate optical flow between input frames and then synthesize an intermediate frame guided by motion. Recent approaches merge these two steps into a single convolution process by convolving input frames with spatially adaptive kernels that account for motion and re-sampling simultaneously. These methods require large kernels to handle large motion, which limits the number of pixels whose kernels can be estimated at once due to the large memory demand. To address this problem, this paper formulates frame interpolation as local separable convolution over input frames using pairs of 1D kernels. Compared to regular 2D kernels, the 1D kernels require significantly fewer parameters to be estimated. Our method develops a deep fully convolutional neural network that takes two input frames and estimates pairs of 1D kernels for all pixels simultaneously. Since our method is able to estimate kernels and synthesizes the whole video frame at once, it allows for the incorporation of perceptual loss to train the neural network to produce visually pleasing frames. This deep neural network is trained end-to-end using widely available video data without any human annotation. Both qualitative and quantitative experiments show that our method provides a practical solution to high-quality video frame interpolation.",
            "year": 2017,
            "venue": "IEEE International Conference on Computer Vision",
            "authors": [
              {
                "authorId": "39644974",
                "name": "Simon Niklaus"
              },
              {
                "authorId": "2712573",
                "name": "Long Mai"
              },
              {
                "authorId": "40513795",
                "name": "Feng Liu"
              }
            ]
          }
        },
        {
          "citedcorpusid": 13756489,
          "isinfluential": false,
          "contexts": [
            "Besides, Transformer [23] has also been used in VFI tasks, Zhang et al. [24] design a hybrid CNN and Transformer, and propose to utilize inter-frame attention to extract both motion and appearance information."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Attention is All you Need",
            "abstract": "The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.",
            "year": 2017,
            "venue": "Neural Information Processing Systems",
            "authors": [
              {
                "authorId": "40348417",
                "name": "Ashish Vaswani"
              },
              {
                "authorId": "1846258",
                "name": "Noam M. Shazeer"
              },
              {
                "authorId": "3877127",
                "name": "Niki Parmar"
              },
              {
                "authorId": "39328010",
                "name": "Jakob Uszkoreit"
              },
              {
                "authorId": "145024664",
                "name": "Llion Jones"
              },
              {
                "authorId": "19177000",
                "name": "Aidan N. Gomez"
              },
              {
                "authorId": "40527594",
                "name": "Lukasz Kaiser"
              },
              {
                "authorId": "3443442",
                "name": "I. Polosukhin"
              }
            ]
          }
        },
        {
          "citedcorpusid": 17839778,
          "isinfluential": false,
          "contexts": [
            "Some approaches design complex high-order motion models, such as cubic [26], [27] and quadratic [28], to address non-linear motions."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "MRI-Based Nonrigid Motion Correction in Simultaneous PET/MRI",
            "abstract": "",
            "year": 2012,
            "venue": "Journal of Nuclear Medicine",
            "authors": [
              {
                "authorId": "34971370",
                "name": "S. Chun"
              },
              {
                "authorId": "34463221",
                "name": "T. Reese"
              },
              {
                "authorId": "3320544",
                "name": "J. Ouyang"
              },
              {
                "authorId": "47284569",
                "name": "B. Guérin"
              },
              {
                "authorId": "1734333",
                "name": "C. Catana"
              },
              {
                "authorId": "3340220",
                "name": "Xuping Zhu"
              },
              {
                "authorId": "2735493",
                "name": "N. Alpert"
              },
              {
                "authorId": "144521864",
                "name": "G. El Fakhri"
              }
            ]
          }
        },
        {
          "citedcorpusid": 115151433,
          "isinfluential": false,
          "contexts": [
            "These methods are mainly based on RNN [33], [34], [35] or GAN [36], [37]."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Events-To-Video: Bringing Modern Computer Vision to Event Cameras",
            "abstract": "Event cameras are novel sensors that report brightness changes in the form of asynchronous \"events\" instead of intensity frames. They have significant advantages over conventional cameras: high temporal resolution, high dynamic range, and no motion blur. Since the output of event cameras is fundamentally different from conventional cameras, it is commonly accepted that they require the development of specialized algorithms to accommodate the particular nature of events. In this work, we take a different view and propose to apply existing, mature computer vision techniques to videos reconstructed from event data. We propose a novel, recurrent neural network to reconstruct videos from a stream of events and train it on a large amount of simulated event data. Our experiments show that our approach surpasses state-of-the-art reconstruction methods by a large margin (> 20%) in terms of image quality. We further apply off-the-shelf computer vision algorithms to videos reconstructed from event data on tasks such as object classification and visual-inertial odometry, and show that this strategy consistently outperforms algorithms that were specifically designed for event data. We believe that our approach opens the door to bringing the outstanding properties of event cameras to an entirely new range of tasks.",
            "year": 2019,
            "venue": "Computer Vision and Pattern Recognition",
            "authors": [
              {
                "authorId": "3414274",
                "name": "Henri Rebecq"
              },
              {
                "authorId": "2774325",
                "name": "René Ranftl"
              },
              {
                "authorId": "145231047",
                "name": "V. Koltun"
              },
              {
                "authorId": "2075371",
                "name": "D. Scaramuzza"
              }
            ]
          }
        },
        {
          "citedcorpusid": 202782364,
          "isinfluential": false,
          "contexts": [
            "Some approaches design complex high-order motion models, such as cubic [26], [27] and quadratic [28], to address non-linear motions."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Quadratic video interpolation",
            "abstract": "Video interpolation is an important problem in computer vision, which helps overcome the temporal limitation of camera sensors. Existing video interpolation methods usually assume uniform motion between consecutive frames and use linear models for interpolation, which cannot well approximate the complex motion in the real world. To address these issues, we propose a quadratic video interpolation method which exploits the acceleration information in videos. This method allows prediction with curvilinear trajectory and variable velocity, and generates more accurate interpolation results. For high-quality frame synthesis, we develop a flow reversal layer to estimate flow fields starting from the unknown target frame to the source frame. In addition, we present techniques for flow refinement. Extensive experiments demonstrate that our approach performs favorably against the existing linear models on a wide variety of video datasets.",
            "year": 2019,
            "venue": "Neural Information Processing Systems",
            "authors": [
              {
                "authorId": "48669892",
                "name": "Xiangyu Xu"
              },
              {
                "authorId": "1392433527",
                "name": "Liu Siyao"
              },
              {
                "authorId": "8397576",
                "name": "Wenxiu Sun"
              },
              {
                "authorId": "144059876",
                "name": "Qian Yin"
              },
              {
                "authorId": "37144787",
                "name": "Ming-Hsuan Yang"
              }
            ]
          }
        },
        {
          "citedcorpusid": 202786778,
          "isinfluential": false,
          "contexts": [
            "3) TrainingDetails: WeimplementtheproposedSEVFI-Net in Pytorch [59] and train three models separately on DSEC, MVSEC, and SEID datasets."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "PyTorch: An Imperative Style, High-Performance Deep Learning Library",
            "abstract": "Deep learning frameworks have often focused on either usability or speed, but not both. PyTorch is a machine learning library that shows that these two goals are in fact compatible: it was designed from first principles to support an imperative and Pythonic programming style that supports code as a model, makes debugging easy and is consistent with other popular scientific computing libraries, while remaining efficient and supporting hardware accelerators such as GPUs. In this paper, we detail the principles that drove the implementation of PyTorch and how they are reflected in its architecture. We emphasize that every aspect of PyTorch is a regular Python program under the full control of its user. We also explain how the careful and pragmatic implementation of the key components of its runtime enables them to work together to achieve compelling performance. We demonstrate the efficiency of individual subsystems, as well as the overall speed of PyTorch on several commonly used benchmarks.",
            "year": 2019,
            "venue": "Neural Information Processing Systems",
            "authors": [
              {
                "authorId": "3407277",
                "name": "Adam Paszke"
              },
              {
                "authorId": "39793298",
                "name": "Sam Gross"
              },
              {
                "authorId": "1403239967",
                "name": "Francisco Massa"
              },
              {
                "authorId": "1977806",
                "name": "Adam Lerer"
              },
              {
                "authorId": "2065251344",
                "name": "James Bradbury"
              },
              {
                "authorId": "114250963",
                "name": "Gregory Chanan"
              },
              {
                "authorId": "2059271276",
                "name": "Trevor Killeen"
              },
              {
                "authorId": "3370429",
                "name": "Zeming Lin"
              },
              {
                "authorId": "3365851",
                "name": "N. Gimelshein"
              },
              {
                "authorId": "3029482",
                "name": "L. Antiga"
              },
              {
                "authorId": "3050846",
                "name": "Alban Desmaison"
              },
              {
                "authorId": "1473151134",
                "name": "Andreas Köpf"
              },
              {
                "authorId": "2052812305",
                "name": "E. Yang"
              },
              {
                "authorId": "2253681376",
                "name": "Zachary DeVito"
              },
              {
                "authorId": "10707709",
                "name": "Martin Raison"
              },
              {
                "authorId": "41203992",
                "name": "Alykhan Tejani"
              },
              {
                "authorId": "22236100",
                "name": "Sasank Chilamkurthy"
              },
              {
                "authorId": "32163737",
                "name": "Benoit Steiner"
              },
              {
                "authorId": "152599430",
                "name": "Lu Fang"
              },
              {
                "authorId": "2113829116",
                "name": "Junjie Bai"
              },
              {
                "authorId": "2127604",
                "name": "Soumith Chintala"
              }
            ]
          }
        },
        {
          "citedcorpusid": 206596513,
          "isinfluential": false,
          "contexts": [
            "We also incorporate the edge-aware disparity smoothness loss Lds used in [54] to promote the local smoothness of disparities by computing the cost using the gradients of both disparities and frames, which is represented as:"
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Unsupervised Monocular Depth Estimation with Left-Right Consistency",
            "abstract": "Learning based methods have shown very promising results for the task of depth estimation in single images. However, most existing approaches treat depth prediction as a supervised regression problem and as a result, require vast quantities of corresponding ground truth depth data for training. Just recording quality depth data in a range of environments is a challenging problem. In this paper, we innovate beyond existing approaches, replacing the use of explicit depth data during training with easier-to-obtain binocular stereo footage. We propose a novel training objective that enables our convolutional neural network to learn to perform single image depth estimation, despite the absence of ground truth depth data. Ex-ploiting epipolar geometry constraints, we generate disparity images by training our network with an image reconstruction loss. We show that solving for image reconstruction alone results in poor quality depth images. To overcome this problem, we propose a novel training loss that enforces consistency between the disparities produced relative to both the left and right images, leading to improved performance and robustness compared to existing approaches. Our method produces state of the art results for monocular depth estimation on the KITTI driving dataset, even outperforming supervised methods that have been trained with ground truth depth.",
            "year": 2016,
            "venue": "Computer Vision and Pattern Recognition",
            "authors": [
              {
                "authorId": "31082236",
                "name": "Clément Godard"
              },
              {
                "authorId": "2918822",
                "name": "Oisin Mac Aodha"
              },
              {
                "authorId": "3309893",
                "name": "G. Brostow"
              }
            ]
          }
        },
        {
          "citedcorpusid": 207761262,
          "isinfluential": false,
          "contexts": [
            "The metrics of Peak Signal to Noise Ratio (PSNR) and Structural SIMilarity (SSIM) [58] are used for quantitative evaluation."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Image quality assessment: from error visibility to structural similarity",
            "abstract": "",
            "year": 2004,
            "venue": "IEEE Transactions on Image Processing",
            "authors": [
              {
                "authorId": "41210105",
                "name": "Zhou Wang"
              },
              {
                "authorId": "1747569",
                "name": "A. Bovik"
              },
              {
                "authorId": "2387140",
                "name": "H. Sheikh"
              },
              {
                "authorId": "1689350",
                "name": "Eero P. Simoncelli"
              }
            ]
          }
        },
        {
          "citedcorpusid": 211731854,
          "isinfluential": true,
          "contexts": [
            "We compare the results with other state-of-the-art VFI methods, such as DAIN [20], EMA-VFI [24], FLAVR [29], RIFE [13], RRIN [16], TTVFI [25], CBMNet [38] and Time Lens [7]. in our result.",
            "To relieve the burden, existing VFI approaches commonly rely on inter-frame motion prediction from neighboring frames [16], and thus can be roughly categorized into warping-based and kernel-based.",
            "2) Benchmark: For VFI task, we compare SEVFI-Net with three open-sourced E-VFI methods, i.e. , CBMNet [38], RE-FID [39] and Time Lens [7]; and seven state-of-the-art F-VFI methods, DAIN [20], EMA-VFI [24], FLAVR [29], RIFE [13], RRIN [16], Super Slomo [57] and TTVFI [25].",
            ", Time Lens [7] and four stateof-the-art F-VFI methods, DAIN [20], RIFE [13], RRIN [16],"
          ],
          "intents": [
            "--",
            "['methodology']",
            "--",
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Video Frame Interpolation Via Residue Refinement",
            "abstract": "Video frame interpolation achieves temporal super-resolution by generating smooth transitions between frames. Although great success has been achieved by deep neural networks, the synthesized images stills suffer from poor visual appearance and unsatisfactory artifacts. In this paper, we propose a novel network structure that leverages residue refinement and adaptive weight to synthesize in-between frames. The residue refinement technique is used for optical flow and image generation for higher accuracy and better visual appearance, while the adaptive weight map combines the forward and backward warped frames to reduce the artifacts. Moreover, all submodules in our method are implemented by U-Net with less depths, so the efficiency is guaranteed. Experiments on public datasets demonstrate the effectiveness and superiority of our method over the state-of-the-art approaches.",
            "year": 2020,
            "venue": "IEEE International Conference on Acoustics, Speech, and Signal Processing",
            "authors": [
              {
                "authorId": "49404171",
                "name": "Haopeng Li"
              },
              {
                "authorId": "34567611",
                "name": "Yuan Yuan"
              },
              {
                "authorId": "145346762",
                "name": "Qi Wang"
              }
            ]
          }
        },
        {
          "citedcorpusid": 212675709,
          "isinfluential": false,
          "contexts": [
            "Warping-based methods [17], [18], [19], [20] use optical ﬂow that perceives motion information between consecutive frames and captures dense correspondences.",
            "Some techniques and information have been utilized to enhance the interpolation performance, e.g. , forward warping [17], context [19], depth [20], and deformable convolution [21], [22].",
            "However, due to the lack of motion information between consecutive input keyframes, most Frame-based VFI (F-VFI) methods are built on simpliﬁed assumptions, e.g. , linear motions [17], [18], [19], [20] or local movements [15], [30], leading to performance degradation in real-world scenarios."
          ],
          "intents": [
            "['methodology']",
            "['methodology']",
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Softmax Splatting for Video Frame Interpolation",
            "abstract": "Differentiable image sampling in the form of backward warping has seen broad adoption in tasks like depth estimation and optical flow prediction. In contrast, how to perform forward warping has seen less attention, partly due to additional challenges such as resolving the conflict of mapping multiple pixels to the same target location in a differentiable way. We propose softmax splatting to address this paradigm shift and show its effectiveness on the application of frame interpolation. Specifically, given two input frames, we forward-warp the frames and their feature pyramid representations based on an optical flow estimate using softmax splatting. In doing so, the softmax splatting seamlessly handles cases where multiple source pixels map to the same target location. We then use a synthesis network to predict the interpolation result from the warped representations. Our softmax splatting allows us to not only interpolate frames at an arbitrary time but also to fine tune the feature pyramid and the optical flow. We show that our synthesis approach, empowered by softmax splatting, achieves new state-of-the-art results for video frame interpolation.",
            "year": 2020,
            "venue": "Computer Vision and Pattern Recognition",
            "authors": [
              {
                "authorId": "39644974",
                "name": "Simon Niklaus"
              },
              {
                "authorId": "40513795",
                "name": "Feng Liu"
              }
            ]
          }
        },
        {
          "citedcorpusid": 220713296,
          "isinfluential": false,
          "contexts": [
            "Some approaches design complex high-order motion models, such as cubic [26], [27] and quadratic [28], to address non-linear motions."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "All at Once: Temporally Adaptive Multi-Frame Interpolation with Advanced Motion Modeling",
            "abstract": "Recent advances in high refresh rate displays as well as the increased interest in high rate of slow motion and frame up-conversion fuel the demand for efficient and cost-effective multi-frame video interpolation solutions. To that regard, inserting multiple frames between consecutive video frames are of paramount importance for the consumer electronics industry. State-of-the-art methods are iterative solutions interpolating one frame at the time. They introduce temporal inconsistencies and clearly noticeable visual artifacts. \nDeparting from the state-of-the-art, this work introduces a true multi-frame interpolator. It utilizes a pyramidal style network in the temporal domain to complete the multi-frame interpolation task in one-shot. A novel flow estimation procedure using a relaxed loss function, and an advanced, cubic-based, motion model is also used to further boost interpolation accuracy when complex motion segments are encountered. Results on the Adobe240 dataset show that the proposed method generates visually pleasing, temporally consistent frames, outperforms the current best off-the-shelf method by 1.57db in PSNR with 8 times smaller model and 7.7 times faster. The proposed method can be easily extended to interpolate a large number of new frames while remaining efficient because of the one-shot mechanism.",
            "year": 2020,
            "venue": "European Conference on Computer Vision",
            "authors": [
              {
                "authorId": "35793956",
                "name": "Zhixiang Chi"
              },
              {
                "authorId": "49456126",
                "name": "Rasoul Mohammadi Nasiri"
              },
              {
                "authorId": "2145976533",
                "name": "Zheng Liu"
              },
              {
                "authorId": "150152476",
                "name": "Juwei Lu"
              },
              {
                "authorId": "37864689",
                "name": "Jin Tang"
              },
              {
                "authorId": "1705037",
                "name": "K. Plataniotis"
              }
            ]
          }
        },
        {
          "citedcorpusid": 232170230,
          "isinfluential": true,
          "contexts": [
            "There are two public stereo event datasets, DSEC [5] and MVSEC [6].",
            "The dataset features frames at 60 FPS, which [6] AND DSEC [5] is higher than the previous stereo datasets as shown in Table II.",
            "However, this setup suffers from the cross-modality parallax issue, especially in real-world scenes with complex non-linear motions and varying depths [5], [6].",
            "We use DSEC [5], MVSEC [6], and our SEID datasets for training and evaluation.",
            "Additionally,duetothelimiteddiversityofscenescapturedby current stereo datasets such as the Stereo Event Camera Dataset for Driving Scenarios (DSEC) [5] and the Multi-Vehicle Stereo Event Camera Dataset (MVSEC) [6], they are unable to effectively evaluate the performance of methods in various…",
            "…on synthetic data, restricted by per-pixel spatial alignment and ideal imaging without intense motion and sudden brightness change, which is difﬁcult to fulﬁll in real-world applications since the events and frames are usually captured separately by an event camera and an intensity camera [5], [7].",
            "Although Although existing works [5], [7] directly apply global homography and stereo rectiﬁcation, it is only valid for scenes with a large depth [5] or within a plane [7].",
            "[5], MVSEC [6], AND O UR SEID D ATASETS TABLE V C OMPARISONS ON M ODEL P ARAMETERS AND R UNTIMES continuous multi-frame reconstruction and exhibiting excellent ability in reconstructing dynamic scenes, with the motion patterns closely resembling the ground truth.",
            "[5] and MVSEC [6] are concentrated within a small range, indicatingthatthescenestheycapturearesimilarandhomogeneous."
          ],
          "intents": [
            "--",
            "['methodology']",
            "['background']",
            "['methodology']",
            "['methodology']",
            "['background']",
            "['background']",
            "['background']",
            "['background']"
          ],
          "cited_paper_info": {
            "title": "DSEC: A Stereo Event Camera Dataset for Driving Scenarios",
            "abstract": "Once an academic venture, autonomous driving has received unparalleled corporate funding in the last decade. Still, operating conditions of current autonomous cars are mostly restricted to ideal scenarios. This means that driving in challenging illumination conditions such as night, sunrise, and sunset remains an open problem. In these cases, standard cameras are being pushed to their limits in terms of low light and high dynamic range performance. To address these challenges, we propose, DSEC, a new dataset that contains such demanding illumination conditions and provides a rich set of sensory data. DSEC offers data from a wide-baseline stereo setup of two color frame cameras and two high-resolution monochrome event cameras. In addition, we collect lidar data and RTK GPS measurements, both hardware synchronized with all camera data. One of the distinctive features of this dataset is the inclusion of high-resolution event cameras. Event cameras have received increasing attention for their high temporal resolution and high dynamic range performance. However, due to their novelty, event camera datasets in driving scenarios are rare. This work presents the first high resolution, large scale stereo dataset with event cameras. The dataset contains 53 sequences collected by driving in a variety of illumination conditions and provides ground truth disparity for the development and evaluation of event-based stereo algorithms.",
            "year": 2021,
            "venue": "IEEE Robotics and Automation Letters",
            "authors": [
              {
                "authorId": "8329387",
                "name": "Mathias Gehrig"
              },
              {
                "authorId": "2052356146",
                "name": "Willem Aarents"
              },
              {
                "authorId": "51152279",
                "name": "Daniel Gehrig"
              },
              {
                "authorId": "2075371",
                "name": "D. Scaramuzza"
              }
            ]
          }
        },
        {
          "citedcorpusid": 235719472,
          "isinfluential": false,
          "contexts": [
            "Recently, event cameras have been adopted for high-quality VFI [7], [8], [9], [10], [11], [12].",
            "…Event-based Video Frame Interpolation (E-VFI) approaches, most of which rely on simulation datasets and require per-pixel spatial alignment between events and frames [7], [8], [9], [10], [11], [12], leading to artifacts and distortions with the stereo event-intensity camera setup as shown in Fig."
          ],
          "intents": [
            "--",
            "--"
          ],
          "cited_paper_info": {
            "title": "EFI-Net: Video Frame Interpolation from Fusion of Events and Frames",
            "abstract": "Event cameras are sensors with pixels that respond independently and asynchronously to changes in scene illumination. Event cameras have a number of advantages when compared to conventional cameras: low-latency, high temporal resolution, high dynamic range, low power and sparse data output. However, existing event cameras also suffer from comparatively low spatial resolution and are sensitive to noise. Recently, it has been shown that it is possible to reconstruct an intensity frame stream from an event stream. These reconstructions preserve the high temporal rate of the event stream, but tend to suffer from significant artifacts and low image quality due to the shortcomings of event cameras. In this work we demonstrate that it is possible to combine the best of both worlds, by fusing a color frame stream at low temporal resolution and high spatial resolution with an event stream at high temporal resolution and low spatial resolution to generate a video stream with both high temporal and spatial resolutions while preserving the original color information. We utilize a novel event frame interpolation network (EFI-Net), a multi-phase convolutional neural network which fuses the frame and event streams. EFI-Net is trained using only simulated data and generalizes exceptionally well to real-world experimental data. We show that our method is able to interpolate frames where traditional video interpolation approaches fail, while also outperforming event-only reconstructions. We further contribute a new dataset, containing event camera data synchronized with high speed video. This work opens the door to a new application for event cameras, enabling high fidelity fusion with frame based image streams for generation of high-quality high-speed video. The dataset is available at https://drive.google.com/file/d/1UIGVBqNER_5KguYPAu5y7TVg-JlNhz3-/view?usp=sharing",
            "year": 2021,
            "venue": "2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)",
            "authors": [
              {
                "authorId": "3333723",
                "name": "Genady Paikin"
              },
              {
                "authorId": "1500420093",
                "name": "Y. Ater"
              },
              {
                "authorId": "1781251107",
                "name": "Roy Shaul"
              },
              {
                "authorId": "2083223429",
                "name": "Evgeny Soloveichik"
              }
            ]
          }
        },
        {
          "citedcorpusid": 244499996,
          "isinfluential": true,
          "contexts": [
            "Comparison of continuous frame reconstruction results by RIFE [13], Time Lens [7] and the proposed SEVFI-Net on SEID.",
            ", Time Lens [7] and four stateof-the-art F-VFI methods, DAIN [20], RIFE [13], RRIN [16],",
            ", RIFE [13] and Time Lens [7] to generate intermediate frames and then input them along with the event stream into two existing stereo matching algorithms, i.",
            "Methods EPE (px)↓ > 1px (%)↓ > 2px (%)↓ > 3px (%)↓ DSEC HSM [3] + RIFE [13] 12.",
            "(b) Comparison of video frame interpolation results of RIFE [13], Time Lens [7], and our proposed SEVFI-Net."
          ],
          "intents": [
            "['methodology']",
            "['methodology']",
            "['background']",
            "--",
            "--"
          ],
          "cited_paper_info": {
            "title": "Real-Time Intermediate Flow Estimation for Video Frame Interpolation",
            "abstract": "Real-time video frame interpolation (VFI) is very useful in video processing, media players, and display devices. We propose RIFE, a Real-time Intermediate Flow Estimation algorithm for VFI. To realize a high-quality flow-based VFI method, RIFE uses a neural network named IFNet that can estimate the intermediate flows end-to-end with much faster speed. A privileged distillation scheme is designed for stable IFNet training and improve the overall performance. RIFE does not rely on pre-trained optical flow models and can support arbitrary-timestep frame interpolation with the temporal encoding input. Experiments demonstrate that RIFE achieves state-of-the-art performance on several public benchmarks. Compared with the popular SuperSlomo and DAIN methods, RIFE is 4--27 times faster and produces better results. Furthermore, RIFE can be extended to wider applications thanks to temporal encoding. The code is available at https://github.com/megvii-research/ECCV2022-RIFE.",
            "year": 2020,
            "venue": "European Conference on Computer Vision",
            "authors": [
              {
                "authorId": "14042304",
                "name": "Zhewei Huang"
              },
              {
                "authorId": "2265615827",
                "name": "Tianyuan Zhang"
              },
              {
                "authorId": "2265548468",
                "name": "Wen Heng"
              },
              {
                "authorId": "2265551803",
                "name": "Boxin Shi"
              },
              {
                "authorId": "2261909771",
                "name": "Shuchang Zhou"
              }
            ]
          }
        },
        {
          "citedcorpusid": 250644220,
          "isinfluential": true,
          "contexts": [
            "In terms of runtime on single frame interpolation settings, ours is lower than DAIN [20], TTVFI [25] and CBMNet [39].",
            "We compare the results with other state-of-the-art VFI methods, such as DAIN [20], EMA-VFI [24], FLAVR [29], RIFE [13], RRIN [16], TTVFI [25], CBMNet [38] and Time Lens [7]. in our result.",
            "2) Benchmark: For VFI task, we compare SEVFI-Net with three open-sourced E-VFI methods, i.e. , CBMNet [38], RE-FID [39] and Time Lens [7]; and seven state-of-the-art F-VFI methods, DAIN [20], EMA-VFI [24], FLAVR [29], RIFE [13], RRIN [16], Super Slomo [57] and TTVFI [25].",
            "In addition, Liu et al. [25] focus on regions with motion consistency differences and propose a trajectory-aware Transformer."
          ],
          "intents": [
            "['methodology']",
            "['result']",
            "['methodology']",
            "['background']"
          ],
          "cited_paper_info": {
            "title": "TTVFI: Learning Trajectory-Aware Transformer for Video Frame Interpolation",
            "abstract": "Video frame interpolation (VFI) aims to synthesize an intermediate frame between two consecutive frames. State-of-the-art approaches usually adopt a two-step solution, which includes 1) generating locally-warped pixels by calculating the optical flow based on pre-defined motion patterns (e.g., uniform motion, symmetric motion), 2) blending the warped pixels to form a full frame through deep neural synthesis networks. However, for various complicated motions (e.g., non-uniform motion, turn around), such improper assumptions about pre-defined motion patterns introduce the inconsistent warping from the two consecutive frames. This leads to the warped features for new frames are usually not aligned, yielding distortion and blur, especially when large and complex motions occur. To solve this issue, in this paper we propose a novel Trajectory-aware Transformer for Video Frame Interpolation (TTVFI). In particular, we formulate the warped features with inconsistent motions as query tokens, and formulate relevant regions in a motion trajectory from two original consecutive frames into keys and values. Self-attention is learned on relevant tokens along the trajectory to blend the pristine features into intermediate frames through end-to-end training. Experimental results demonstrate that our method outperforms other state-of-the-art methods in four widely-used VFI benchmarks. Both code and pre-trained models will be released at https://github.com/ChengxuLiu/TTVFI.",
            "year": 2022,
            "venue": "IEEE Transactions on Image Processing",
            "authors": [
              {
                "authorId": "2108119060",
                "name": "Chengxu Liu"
              },
              {
                "authorId": "46402216",
                "name": "Huan Yang"
              },
              {
                "authorId": "3247966",
                "name": "Jianlong Fu"
              },
              {
                "authorId": "6468417",
                "name": "Xueming Qian"
              }
            ]
          }
        },
        {
          "citedcorpusid": 251903532,
          "isinfluential": false,
          "contexts": [
            "Typically, only around 10% of the depth values are valid, while the remaining 90% are empty [55]."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Deep Depth Completion From Extremely Sparse Data: A Survey",
            "abstract": "Depth completion aims at predicting dense pixel-wise depth from an extremely sparse map captured from a depth sensor, e.g., LiDARs. It plays an essential role in various applications such as autonomous driving, 3D reconstruction, augmented reality, and robot navigation. Recent successes on the task have been demonstrated and dominated by deep learning based solutions. In this article, for the first time, we provide a comprehensive literature review that helps readers better grasp the research trends and clearly understand the current advances. We investigate the related studies from the design aspects of network architectures, loss functions, benchmark datasets, and learning strategies with a proposal of a novel taxonomy that categorizes existing methods. Besides, we present a quantitative comparison of model performance on three widely used benchmarks, including indoor and outdoor datasets. Finally, we discuss the challenges of prior works and provide readers with some insights for future research directions.",
            "year": 2022,
            "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
            "authors": [
              {
                "authorId": "1409846329",
                "name": "Junjie Hu"
              },
              {
                "authorId": "1576661870",
                "name": "Chenyu Bao"
              },
              {
                "authorId": "2159942",
                "name": "M. Ozay"
              },
              {
                "authorId": "2047692",
                "name": "Chenyou Fan"
              },
              {
                "authorId": "2164872388",
                "name": "Qing Gao"
              },
              {
                "authorId": "2139103448",
                "name": "Honghai Liu"
              },
              {
                "authorId": "35337770",
                "name": "Tin Lun Lam"
              }
            ]
          }
        },
        {
          "citedcorpusid": 253553798,
          "isinfluential": false,
          "contexts": [
            ", forward warping [17], transformer [18], context [19], depth [20], patch-based [21], [22] and deformable convolution [23], [24].",
            "Warping-based methods [17]–[21] utilize optical flow that perceives motion information between consecutive frames and captures dense correspondences."
          ],
          "intents": [
            "['background']",
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Optical Flow Reusing for High-Efficiency Space-Time Video Super Resolution",
            "abstract": "In this paper, we consider the task of space-time video super-resolution (ST-VSR), which can increase the spatial resolution and frame rate for a given video simultaneously. Despite the remarkable progress of recent methods, most of them still suffer from high computational costs and inefficient long-range information usage. To alleviate these problems, we propose a Bidirectional Recurrence Network (BRN) with the optical-flow-reuse strategy to better use temporal knowledge from long-range neighboring frames for high-efficiency reconstruction. Specifically, an efficient and memory-saving multi-frame motion utilization strategy is proposed by reusing the intermediate flow of adjacent frames, which considerably reduces the computation burden of frame alignment compared with traditional LSTM-based designs. In addition, the proposed hidden state in BRN is updated by the reused optical flow and refined by the Feature Refinement Module (FRM) for further optimization. Moreover, by utilizing intermediate flow estimation, the proposed method can inference non-linear motion and restore details better. Extensive experiments demonstrate that our optical-flow-reuse-based bidirectional recurrent network (OFR-BRN) is superior to state-of-the-art methods in accuracy and efficiency. Codes are available on URL: https://github.com/hahazh/OFR-BRN",
            "year": 2021,
            "venue": "IEEE transactions on circuits and systems for video technology (Print)",
            "authors": [
              {
                "authorId": "2145783565",
                "name": "Yuantong Zhang"
              },
              {
                "authorId": "2044342535",
                "name": "Huairui Wang"
              },
              {
                "authorId": "2115313367",
                "name": "Han Zhu"
              },
              {
                "authorId": "48354614",
                "name": "Zhenzhong Chen"
              }
            ]
          }
        },
        {
          "citedcorpusid": 253651036,
          "isinfluential": true,
          "contexts": [
            ", HSM [3] and SSIE [1], to compare their performance with our method.",
            "Methods EPE (px)↓ > 1px (%)↓ > 2px (%)↓ > 3px (%)↓ DSEC HSM [3] + RIFE [13] 12.",
            "Therefore, we ﬁrst choose two representative VFI methods, i.e. , RIFE [13] and Time Lens [7] to generate intermediate frames, and then input them along with the event stream into two existing stereo matching methods, i.e. , HSM [3] and SSIE [1], to compare their performance with our method.",
            "For stereo matching task, we compare SEVFI-Net with two existing cross-modal methods, i.e. , HSM [3] and SSIE [1].",
            "Due to the fact that event and intensity cameras perceive the same light field, the edge information extracted from events and intensity images can be correlated to calculate the sparse disparity map [3], [4]."
          ],
          "intents": [
            "['result']",
            "--",
            "--",
            "--",
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Real-Time Hetero-Stereo Matching for Event and Frame Camera With Aligned Events Using Maximum Shift Distance",
            "abstract": "Event cameras can show better performance than frame cameras in challenging scenarios, such as fast-moving environments or high-dynamic-range scenes. However, it is still difficult for event cameras to replace frame cameras in non-challenging normal scenarios. In order to leverage the advantages of both cameras, we conduct a study for the heterogeneous stereo camera system which employs both an event and a frame camera. The proposed system estimates the semi-dense disparity in real-time by matching heterogeneous data of an event and a frame camera in stereo. We propose an accurate, intuitive and efficient way to align events with 6-DOF camera motion, by suggesting the maximum shift distance method. The aligned event image shows high similarity to the edge image of the frame camera. The proposed method can estimate poses of an event camera and depth of events in a few frames, which can speed up the initialization of the event camera system. We verified our algorithm in the DSEC dataset. The proposed hetero-stereo matching outperformed other methods. For real-time operation, we implemented our code using parallel computation with CUDA and release our code open source:",
            "year": 2023,
            "venue": "IEEE Robotics and Automation Letters",
            "authors": [
              {
                "authorId": "2108880866",
                "name": "Haram Kim"
              },
              {
                "authorId": "2108077736",
                "name": "Sangil Lee"
              },
              {
                "authorId": "2125035466",
                "name": "Junha Kim"
              },
              {
                "authorId": "2161495857",
                "name": "H. J. Kim"
              }
            ]
          }
        },
        {
          "citedcorpusid": 253761147,
          "isinfluential": false,
          "contexts": [
            "Some techniques and information have been utilized to enhance the interpolation performance, e.g. , forward warping [17], context [19], depth [20], and deformable convolution [21], [22]."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Flow Guidance Deformable Compensation Network for Video Frame Interpolation",
            "abstract": "Flow-based and deformable convolution (DConv)-based methods are two mainstream approaches for solving the video frame interpolation (VFI) problem, which have made remarkable progress with the development of deep convolutional networks over the past years. However, flow-based VFI methods often suffer from the inaccuracy of flow map estimation, especially in dealing with complex and irregular real-world motions. DConv-based VFI methods have advantages in handling complex motions, while the increased degree of freedom makes the training of the DConv model difficult. To address these problems, in this article, we propose a flow guidance deformable compensation network (FGDCN) for the VFI task. FGDCN decomposes the frame sampling process into two steps: a flow step and a deformation step. Specifically, the flow step utilizes a coarse-to-fine flow estimation network to directly estimate the intermediate flows and synthesizes an anchor frame simultaneously. To ensure the accuracy of the estimated flow, a distillation loss and a task-oriented loss are jointly employed in this step. Under the guidance of the flow priors learned in step one, the deformation step designs a new pyramid deformable compensation network to compensate for the missing details of the flow step. In addition, a pyramid loss is proposed to supervise the model in both the image and frequency domains. Experimental results show that the proposed algorithm achieves excellent performance on various datasets with fewer parameters.",
            "year": 2022,
            "venue": "IEEE transactions on multimedia",
            "authors": [
              {
                "authorId": "48956636",
                "name": "Pengcheng Lei"
              },
              {
                "authorId": "152786529",
                "name": "Faming Fang"
              },
              {
                "authorId": "2247191947",
                "name": "Tieyong Zeng"
              },
              {
                "authorId": "1730192",
                "name": "Guixu Zhang"
              }
            ]
          }
        },
        {
          "citedcorpusid": 254612876,
          "isinfluential": true,
          "contexts": [
            "Recently, event cameras have been adopted for high-quality VFI [7]–[12].",
            "[12] analyze the drawbacks of existing methods and introduce a novel method named EVA(2) for E-VFI via cross-modal alignment and aggregation.",
            "To tackle this problem, E-VFI methods are proposed by predicting inter-frame motion information with events [7]–[9], [11], [12], achieving better interpolation performance than FVFI methods.",
            "As a result, it can significantly degenerate the performance of existing Eventbased Video Frame Interpolation (E-VFI) approaches, most of which rely on simulation datasets and require per-pixel spatial alignment between events and frames [7]–[12], leading to"
          ],
          "intents": [
            "--",
            "['methodology']",
            "['methodology']",
            "['background']"
          ],
          "cited_paper_info": {
            "title": "EVA$^{2}$: Event-Assisted Video Frame Interpolation via Cross-Modal Alignment and Aggregation",
            "abstract": "We consider the problem of event-assisted video frame interpolation (VFI), a new track for VFI, by introducing the <italic>event</italic> data, a novel sensing modality, into the process of generating intermediate frames from low-frame-rate videos. This new track challenges existing methods in two aspects: (1) how to utilize the event data to align boundary keyframes to intermediate ones, especially when there are corruptions in scenes (<italic>e.g.</italic>, non-uniform motion, object occlusions, and illumination changes); (2) how to effectively utilize and aggregate cross-modal information for further mitigating corruptions and refining details. In this paper, we propose a novel <underline>E</underline>vent-assisted <underline>V</underline>FI method with cross-modal <underline>A</underline>lignment and <underline>A</underline>ggregation, termed <underline>EVA<inline-formula><tex-math notation=\"LaTeX\">$^{2}$</tex-math></inline-formula></underline>, to address these challenges. First, to handle corruptions during alignment, we devise the cross-modal Event-Guided Alignment (EGA) module, in which the intermediate frames are aligned at both the feature and the image levels. The alignment operation in the EGA module is guided by the offset maps generated from the event data and information extracted from the input boundary keyframes. Second, we propose the cross-modal Event-aware Dynamic Aggregation (EDA) module, in which the event-aware dynamic convolution operation is applied to aggregate the event data with the aligned results adaptively for further improvements. Extensive experiments on both synthetic and real-world datasets validate the effectiveness of our EVA<inline-formula><tex-math notation=\"LaTeX\">$^{2}$</tex-math></inline-formula>.",
            "year": 2022,
            "venue": "IEEE Transactions on Computational Imaging",
            "authors": [
              {
                "authorId": "1993383158",
                "name": "Zeyu Xiao"
              },
              {
                "authorId": "2107017232",
                "name": "Wenming Weng"
              },
              {
                "authorId": "2145912767",
                "name": "Yueyi Zhang"
              },
              {
                "authorId": "2352456",
                "name": "Zhiwei Xiong"
              }
            ]
          }
        },
        {
          "citedcorpusid": 255749379,
          "isinfluential": true,
          "contexts": [
            "Sun et al. [39] propose a framework for solving general E-VFI and event-based image deblurring.",
            "Compared to F-VFI methods, existing E-VFI methods can estimate a more precise inter-frame motion model with the help of events, thus achieving better frame interpolation reconstruction results [7], [8], [9], [38], [39].",
            "2) Benchmark: For VFI task, we compare SEVFI-Net with three open-sourced E-VFI methods, i.e. , CBMNet [38], RE-FID [39] and Time Lens [7]; and seven state-of-the-art F-VFI methods, DAIN [20], EMA-VFI [24], FLAVR [29], RIFE [13], RRIN [16], Super Slomo [57] and TTVFI [25].",
            "In terms of runtime on single frame interpolation settings, ours is lower than DAIN [20], TTVFI [25] and CBMNet [39].",
            "Quantitative comparisons: Quantitative comparisons are given in Tables IV and V. 1 We ﬁrst notice that E-VFI methods, i.e. , CBMNet [38], REFID [39] and Time Lens [7], despite their utilization of events to estimate motion information and the integration of synthesis-based and ﬂow-based approaches,…"
          ],
          "intents": [
            "['background']",
            "['methodology']",
            "['methodology']",
            "['methodology']",
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "A Unified Framework for Event-Based Frame Interpolation With Ad-Hoc Deblurring in the Wild",
            "abstract": "Effective video frame interpolation hinges on the adept handling of motion in the input scene. Prior work acknowledges asynchronous event information for this, but often overlooks whether motion induces blur in the video, limiting its scope to sharp frame interpolation. We instead propose a unified framework for event-based frame interpolation that performs deblurring ad-hoc and thus works both on sharp and blurry input videos. Our model consists in a bidirectional recurrent network that incorporates the temporal dimension of interpolation and fuses information from the input frames and the events adaptively based on their temporal proximity. To enhance the generalization from synthetic data to real event cameras, we integrate self-supervised framework with the proposed model to enhance the generalization on real-world datasets in the wild. At the dataset level, we introduce a novel real-world high-resolution dataset with events and color videos named HighREV, which provides a challenging evaluation setting for the examined task. Extensive experiments show that our network consistently outperforms previous state-of-the-art methods on frame interpolation, single image deblurring, and the joint task of both. Experiments on domain transfer reveal that self-supervised training effectively mitigates the performance degradation observed when transitioning from synthetic data to real-world data. Code and datasets are available at https://github.com/AHupuJR/REFID.",
            "year": 2023,
            "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
            "authors": [
              {
                "authorId": "2110832828",
                "name": "Lei Sun"
              },
              {
                "authorId": "7593607",
                "name": "Christos Sakaridis"
              },
              {
                "authorId": "145270228",
                "name": "Jingyun Liang"
              },
              {
                "authorId": null,
                "name": "Peng Sun"
              },
              {
                "authorId": "32879676",
                "name": "Jiezhang Cao"
              },
              {
                "authorId": "144110274",
                "name": "K. Zhang"
              },
              {
                "authorId": "2113805888",
                "name": "Qi Jiang"
              },
              {
                "authorId": "2148352079",
                "name": "Kaiwei Wang"
              },
              {
                "authorId": "1681236",
                "name": "L. Gool"
              }
            ]
          }
        }
      ]
    }
  }
}