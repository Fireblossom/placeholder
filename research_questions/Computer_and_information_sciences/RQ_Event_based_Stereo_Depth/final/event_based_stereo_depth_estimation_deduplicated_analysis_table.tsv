Name (extracted)	Citing Article	Citied Article	Features	Analysis_Source	Homepage_URL
Multi Vehicle Stereo Event Camera Dataset (MVSEC)	https://doi.org/10.1109/ISCAS51556.2021.9401402 (2021), https://doi.org/10.1609/aaai.v35i2.16171 (2021), https://doi.org/10.1109/TCSVT.2022.3189480 (2022)	https://www.semanticscholar.org/paper/6fe028a54dad296b6d25c50baaa47511df6d4123	The Multi Vehicle Stereo Event Camera (MVSEC) Dataset is used to evaluate event-based stereo algorithms, leveraging its indoor and outdoor sequences with varying illumination and speeds. This dataset enables researchers to test the robustness and accuracy of stereo depth estimation methods under diverse conditions, focusing on algorithm performance in dynamic environments.	cited_context | citing_context	
3planes	https://doi.org/10.1007/978-3-030-01246-5_15 (2018)	https://doi.org/10.1177/0278364917691115 (2016)	The '3planes' dataset is used to simulate event-based data for pose estimation, offering a controlled environment to test algorithms under diverse conditions. This enables researchers to evaluate and refine pose estimation techniques systematically, ensuring robust performance across varying scenarios.	cited_context	
An Event-based Vision Dataset for Visual Navigation Tasks in Agricultural Environments	https://doi.org/10.1109/LRA.2022.3186770 (2022)	https://doi.org/10.1109/ICRA48506.2021.9561741 (2021)		cited_context	
CoSEC	https://doi.org/10.1109/TPAMI.2025.3586559 (2024)	https://doi.org/10.48550/arXiv.2408.08500 (2024)	The CoSEC dataset is used for aligning pixels between frame-based and event cameras to facilitate multi-modal fusion in autonomous driving. This alignment is crucial for integrating data from different camera types, enhancing the accuracy and robustness of perception systems. The dataset supports research focused on technical integration methods, enabling more effective use of diverse sensor inputs in real-world driving scenarios.	citing_context	
D S E C	https://doi.org/10.1109/TPAMI.2025.3586559 (2024)	https://doi.org/10.1007/978-3-030-01246-5_15 (2018)	The D S E C dataset is used for evaluating event-based stereo depth estimation methods, particularly focusing on semi-dense 3D reconstruction with stereo event cameras. Researchers employ this dataset to assess the performance of algorithms in generating accurate depth maps from event data, leveraging its specialized characteristics to advance the field of event-based vision.	citing_context	
Davis	https://doi.org/10.1177/1729881417752759 (2018)	https://doi.org/10.1109/JSSC.2014.2342715 (2014)	The DAVIS dataset is used to capture high-resolution and low-latency event-based data for stereo depth estimation. It employs a quadruplet representation (time, coordinates, polarity) to process event streams, enabling precise and efficient depth estimation in dynamic environments. This dataset facilitates research focused on improving the accuracy and responsiveness of event-based stereo vision systems.; The DAVIS dataset is used for capturing event-based data for stereo depth estimation, specifically focusing on high-resolution and low-latency event streams. It employs a quadruplet representation (time, x, y, polarity) to enable precise and efficient processing of visual events, facilitating research in real-time depth perception and dynamic scene understanding.	cited_context | citing_context	
DAVIS sequences	https://doi.org/10.1109/TIV.2024.3412595 (2023)	https://doi.org/10.1007/978-3-030-01246-5_15 (2018)	The DAVIS sequences dataset is used to evaluate stereo depth estimation methods, particularly focusing on the alignment of event-based and frame-based sensors that share the same photoreceptor pixel array. This dataset enables researchers to assess the performance of algorithms in integrating data from these dual-modal sensors, addressing challenges in sensor synchronization and data fusion.	citing_context	
DAVIS240	https://doi.org/10.1109/ECMR.2019.8870946 (2019)	https://doi.org/10.1109/JSSC.2014.2342715 (2014)	The DAVIS240 dataset is primarily used for calibration purposes in event-based vision research. It provides standard camera frames that ensure accurate setup before applying event-only processing methods. This calibration is crucial for maintaining the precision of subsequent event-based algorithms, enabling reliable and consistent experimental results.	citing_context	
DAVIS346	https://doi.org/10.1080/01691864.2020.1821770 (2020)	https://doi.org/10.1109/JSSC.2014.2342715 (2014)	The DAVIS346 dataset is used to collect stereo event sequences with ground truth pose and depth information. It is specifically employed for evaluating event-based stereo depth estimation algorithms. The dataset's provision of synchronized event data and ground truth enables researchers to accurately assess the performance of these algorithms in real-world scenarios.	citing_context	
DAVISm346b	https://doi.org/10.1109/LRA.2018.2800793 (2018)	https://doi.org/10.1109/ICRA.2014.6906892 (2014)	The DAVISm346b dataset is used to provide event streams from synchronized and calibrated dynamic vision sensors, capturing long indoor and outdoor sequences under varying conditions. It includes depth images and pose data, enabling research in stereo depth estimation. This dataset supports the development and evaluation of algorithms by offering realistic, diverse, and well-calibrated data.	cited_context	
Ddd17	https://doi.org/10.1109/LRA.2018.2800793 (2018)	https://www.semanticscholar.org/paper/e9496ccf44f6ebca4f01c31a012bdab7cac4a65c (2017)	The DDD17 dataset is used to provide a large collection of data from a DAVIS 346B event-based sensor mounted in a car, capturing 12 hours of driving scenarios. It is employed for end-to-end learning of driving-related tasks, leveraging the unique temporal resolution and low latency of event-based sensors to enhance real-time driving applications.; The DDD17 dataset is used to provide a large collection of data from a DAVIS 346B event-based sensor mounted in a car, capturing 12 hours of driving scenarios. It is employed for end-to-end learning of driving-related tasks, leveraging the unique temporal resolution and dynamic range of event-based sensors to enhance the training of machine learning models for real-time driving applications.	cited_context | citing_context	
DENSE	https://doi.org/10.1109/ICRA48891.2023.10160605 (2023)	https://doi.org/10.1109/LRA.2018.2800793 (2018)	The DENSE dataset is primarily used for validating methods in 3D perception and monocular dense depth estimation using synthetic event data. It provides synthetic samples of objects such as trees and bus stations, which are used for both initial training and validation of neural networks. This enables researchers to test and refine algorithms for depth estimation in event-based systems.	citing_context	
depth images and pose data	https://doi.org/10.1109/LRA.2018.2800793 (2018)	https://doi.org/10.1109/ICRA.2014.6906892 (2014)	The dataset of depth images and pose data is used to enhance the quality of event-based stereo depth estimation. It provides accurate depth images and pose information at high frequency, generated from a lidar system and motion capture, which improves the precision and reliability of depth estimation in dynamic environments.	citing_context	
DHP19	https://doi.org/10.1109/ISCAS51556.2021.9401402 (2021)	https://doi.org/10.3389/fnbot.2020.568283 (2020)	The DHP19 dataset is used to evaluate the robustness and performance of event-based approaches for neuromorphic, on-chip depth estimation. Researchers focus on assessing the reliability of these methods, leveraging the dataset's characteristics to test and validate their algorithms in real-world conditions. This enables the development and refinement of more efficient and accurate depth estimation techniques in neuromorphic computing.	citing_context	
Dsec	https://doi.org/10.1007/978-3-031-92460-6_5 (2024), https://doi.org/10.1109/TPAMI.2024.3396116 (2024), https://doi.org/10.1109/TMM.2024.3387690 (2023) (+5) (+3)	https://doi.org/10.1109/LRA.2021.3068942 (2021)	The DSEC dataset is primarily used for evaluating and improving event-based stereo depth estimation, particularly in driving scenarios. It supports research on 3D perception with event cameras, focusing on depth estimation accuracy, robustness under varying conditions, and the integration of motion information. The dataset facilitates cross-validation, benchmarking against state-of-the-art methods, and hyperparameter optimization with raw LiDAR data. Specific applications include assessing the effectiveness of time-aware warps, improving edge and contour accuracy, and evaluating performance in challenging scenes like garage doors.; The DSEC dataset is primarily used to evaluate and compare event-based stereo depth estimation methods in driving scenarios. It focuses on challenges such as forward motion, sparse event generation, dynamic conditions, and varying illumination. Researchers use it to validate the performance of event-based algorithms against intensity frame-based methods, emphasizing accuracy and robustness in diverse and challenging environments. The dataset supports the development and testing of stereo event camera algorithms without the need for LiDAR or IMU data.	cited_context | citing_context	
DSEC dataset	https://doi.org/10.1109/TCSVT.2022.3189480 (2022)	https://doi.org/10.1109/LRA.2018.2800793 (2018)	The DSEC dataset is mentioned in research citations but lacks detailed descriptions of its usage. There is no explicit information on how it is employed in methodologies, specific research questions, or the characteristics that enable its use. Therefore, based on the provided evidence, no specific research application or methodology can be accurately described for this dataset.	cited_context	
DSEC disparity benchmark	https://doi.org/10.1109/TPAMI.2025.3586559 (2024), https://doi.org/10.48550/arXiv.2408.05452 (2024), https://doi.org/10.1109/TIM.2025.3566813 (2025)	https://doi.org/10.1109/ICCV48922.2021.00422 (2021)	The DSEC disparity benchmark dataset is used to evaluate the performance of event-based stereo depth estimation methods. It focuses on metrics such as mean average disparity error (MAE), root mean square error (RMSE), and pixel error rates (1PE, 2PE). The dataset enables researchers to assess the accuracy and robustness of disparity maps generated by different algorithms, facilitating comparisons with state-of-the-art techniques.	citing_context	
DSEC driving dataset	https://doi.org/10.1007/978-3-031-92460-6_5 (2024)	https://doi.org/10.1109/LRA.2021.3068942 (2021)	The DSEC driving dataset is used for event-based stereo depth estimation in driving scenarios. Researchers employ it to compare their proposed strategies with existing methods like MC-EMVS, demonstrating significant improvements. This dataset enables precise evaluation and validation of event-based depth estimation techniques, crucial for enhancing autonomous driving systems.	citing_context	
DSEC flow benchmark	https://doi.org/10.1109/TPAMI.2024.3396116 (2024)		The DSEC flow benchmark dataset is used to evaluate the performance of RAFT in estimating pixel correspondences for large displacements, particularly in the context of event-based visual odometry and stereo depth estimation. This dataset facilitates the assessment of algorithms by providing challenging scenarios with significant motion, enabling researchers to test and improve the accuracy and robustness of their methods in real-world conditions.	citing_context	
DSEC zurich city 04 a	https://doi.org/10.48550/arXiv.2504.15863 (2025)	https://doi.org/10.1109/LRA.2018.2800793 (2018)	The DSEC Zurich City 04 a dataset is used to qualitatively compare depth estimation methods, particularly focusing on the performance of MC-EMVS and DERD-Net on event-based stereo sequences. This dataset enables researchers to evaluate and contrast these methods, providing insights into their effectiveness in depth estimation tasks.	citing_context	
DVS 3D Human Pose Dataset (DHP19)	https://doi.org/10.1109/ISCAS51556.2021.9401402 (2021)	https://www.semanticscholar.org/paper/6fe028a54dad296b6d25c50baaa47511df6d4123	The DVS 3D Human Pose Dataset (DHP19) is used to provide DVS input data and 3D ground-truth information for training small-scale neuromorphic architectures. It focuses on coarse stereo vision tasks, enabling researchers to develop and evaluate models that process event-based data for 3D human pose estimation. The dataset's event-based nature and 3D ground-truth labels are crucial for training and validating these neuromorphic systems.	citing_context	
DVS stereo dataset	https://doi.org/10.1109/ISCAS51556.2021.9401402 (2021), https://doi.org/10.1109/TPAMI.2025.3586559 (2024)	https://doi.org/10.1109/CVPR.2018.00786 (2018)	The DVS stereo dataset is used to evaluate and test event-based stereo depth estimation methods, particularly focusing on dynamic objects in a stationary setup. It employs stereo DAVIS240C cameras to capture real-world sequences, such as a fast rotating fan and a rotating toy butterfly, enabling researchers to assess the performance of event-based stereo algorithms in dynamic environments.	citing_context	
ECD	https://doi.org/10.1109/TPAMI.2024.3396116 (2024)	https://doi.org/10.1177/0278364917691115 (2016)	The ECD dataset is used to evaluate time-aware warp methods, specifically for pose estimation, visual odometry, and SLAM, leveraging event-based data. This dataset enables researchers to test and refine algorithms that process asynchronous events, enhancing the accuracy and efficiency of these critical tasks in robotics and computer vision.	citing_context	
ECD dataset	https://doi.org/10.1109/TPAMI.2024.3396116 (2024)	https://doi.org/10.1177/0278364917691115 (2016)	The ECD dataset is primarily used for depth and ego-motion estimation in event-based stereo depth estimation research. It is utilized in both real and simulated sequences, such as slider_depth and simulation_3planes, to evaluate performance and support studies in pose estimation, visual odometry, and SLAM. The dataset's event-based data is crucial for these applications, enabling precise and efficient estimation techniques.	citing_context	
EMSGC	https://doi.org/10.1109/TPAMI.2024.3396116 (2024)	https://doi.org/10.1109/TNNLS.2021.3124580 (2020)	The EMSGC dataset is used for event-based motion segmentation, specifically employing spatio-temporal graph cuts methodology. It records sequences with a hand-held DAVIS346 camera, enabling researchers to analyze and segment motion events in real-time. This dataset facilitates the development and evaluation of algorithms for dynamic scene understanding in event-based vision systems.	citing_context	
ERGO-12	https://doi.org/10.48550/arXiv.2408.04633 (2024)	https://doi.org/10.1109/ICCV51070.2023.01180 (2023)	The dataset 'ERGO-12' is mentioned in the citation context but lacks detailed descriptions of its usage in research. There is no explicit information regarding its application, methodology, research questions, or specific characteristics. Therefore, it cannot be accurately described as being used for any particular research area or method.	citing_context	
ESVO (Stereo Events)	https://doi.org/10.1109/TIV.2024.3412595 (2023)	https://doi.org/10.1109/TRO.2021.3062252 (2020)	The ESVO (Stereo Events) dataset is used to evaluate event-based stereo visual odometry, specifically focusing on the accuracy and robustness of depth estimation in dynamic environments. This dataset enables researchers to test and validate algorithms designed to handle real-time, high-frequency event data, ensuring reliable performance under varying conditions.	cited_context	
event streams from DAVISm346b	https://doi.org/10.1109/LRA.2018.2800793 (2018)	https://doi.org/10.1109/ICRA.2014.6906892 (2014)	The 'event streams from DAVISm346b' dataset is used to capture synchronized event streams from two calibrated Dynamic Vision and Active Pixel Sensors. It provides long indoor and outdoor sequences under various conditions, enabling research in depth estimation. This dataset supports the development and evaluation of algorithms for event-based stereo depth estimation by offering diverse and realistic sensor data.	citing_context	
Event-Based Stereo Visual Odometry	https://doi.org/10.1109/TIV.2024.3412595 (2023)	https://doi.org/10.1109/TRO.2021.3062252 (2020)	The Event-Based Stereo Visual Odometry dataset is used to evaluate event-based stereo depth estimation methods, specifically focusing on visual odometry performance in dynamic environments. Researchers employ this dataset to assess the accuracy and robustness of algorithms in real-world conditions, leveraging its event-based data to simulate and analyze performance under rapid changes and high dynamics.	citing_context	
event-camera dataset	https://doi.org/10.1007/978-3-030-01246-5_15 (2018)	https://doi.org/10.1177/0278364917691115 (2016)	The event-camera dataset is used to evaluate stereo depth estimation methods, particularly through synthetic sequences generated by a simulator. It is also employed to assess event-based stereo depth estimation techniques, focusing on pose estimation, visual odometry, and SLAM. This dataset enables researchers to test and refine algorithms in these specific areas, leveraging its synthetic data to provide controlled evaluation environments.	cited_context	
EVIMO2	https://doi.org/10.1007/978-3-031-92460-6_5 (2024), https://doi.org/10.1109/TPAMI.2025.3586559 (2024)	https://doi.org/10.48550/arXiv.2205.03467 (2022)	The EVIMO2 dataset is primarily used for motion segmentation, optical flow, structure from motion, and visual inertial odometry in indoor scenes, particularly with monocular and trinocular event cameras. It provides a rich resource for evaluating event-based stereo algorithms and assessing their performance in various indoor environments. The dataset's comprehensive coverage of these areas enables researchers to thoroughly test and validate their methods.	citing_context	
flying1	https://doi.org/10.1109/TPAMI.2025.3586559 (2024)	https://doi.org/10.1109/ICCV.2019.00161 (2019)	The 'flying1' dataset is used to evaluate mean depth error in event-based stereo depth estimation. Researchers focus on specific sequences within the dataset to assess the performance of their algorithms, ensuring accurate and reliable depth estimation in dynamic environments. This dataset enables the rigorous testing and validation of event-based stereo methods by providing challenging and diverse scenarios.	citing_context	
flying2	https://doi.org/10.1109/TPAMI.2025.3586559 (2024)	https://doi.org/10.1109/ICCV.2019.00161 (2019)	The 'flying2' dataset is used to evaluate mean depth error in event-based stereo depth estimation. Researchers focus on specific sequences within the dataset to assess the performance of their algorithms, ensuring accurate and reliable depth estimation in dynamic environments. This dataset enables the rigorous testing and validation of event-based stereo methods by providing challenging and diverse scenarios.	citing_context	
flying3	https://doi.org/10.1109/TPAMI.2025.3586559 (2024)	https://doi.org/10.1109/ICCV.2019.00161 (2019)	The 'flying3' dataset is used to evaluate mean depth error in event-based stereo depth estimation. Researchers focus on specific sequences within the dataset to assess the performance of their algorithms, ensuring accurate and reliable depth estimation in dynamic environments. This dataset enables the rigorous testing and validation of event-based stereo methods by providing challenging and diverse scenarios.	citing_context	
Indoor Flying Dataset	https://doi.org/10.1109/LSP.2024.3398531 (2024), https://doi.org/10.1109/CVPR52688.2022.00848 (2022), https://doi.org/10.1609/aaai.v35i2.16171 (2021)	https://doi.org/10.1109/LRA.2018.2800793 (2018)	The Indoor Flying dataset is used for preprocessing and splitting in event-based stereo depth estimation, particularly for indoor flying scenarios. It is divided into three subsets, with split 1 and split 3 utilized for training and evaluation, respectively. This dataset enables researchers to assess and improve event-based stereo depth estimation methods in controlled indoor environments.; The Indoor Flying dataset is used to train and evaluate event-based stereo depth estimation models, specifically tailored for indoor flying scenarios. This dataset, featuring data from event cameras, enables researchers to focus on the challenges of dynamic indoor environments, enhancing the accuracy and robustness of depth estimation algorithms in such settings.	cited_context | citing_context	
Indoor flying1	https://doi.org/10.1007/978-3-030-01246-5_15 (2018)	https://doi.org/10.1177/0278364917691115 (2016)	The 'Indoor flying1' dataset is used to evaluate event-based visual odometry in indoor flying scenarios. It focuses on accurate motion estimation and assessing the robustness of algorithms in dynamic environments. This dataset enables researchers to test and improve the performance of visual odometry techniques specifically tailored for indoor navigation tasks.	cited_context	
Indoor flying3	https://doi.org/10.1007/978-3-030-01246-5_15 (2018)	https://doi.org/10.1177/0278364917691115 (2016)	The 'Indoor flying3' dataset is used to evaluate the performance of event-based SLAM in complex indoor flying environments. It emphasizes real-time processing and accuracy, enabling researchers to test and refine algorithms for robust navigation and mapping in dynamic indoor settings.	cited_context	
Kitti	https://doi.org/10.3389/fnins.2017.00535 (2017), https://doi.org/10.1080/01691864.2020.1821770 (2020), https://doi.org/10.1109/LRA.2022.3186770 (2022)	https://doi.org/10.1109/CVPR.2012.6248074 (2012)	The KITTI dataset is primarily used to evaluate and demonstrate the performance of stereo depth estimation methods in real-world autonomous driving scenarios. It provides annotated data for assessing stereo vision and scene flow techniques. Additionally, the dataset is utilized to enhance intermediate frame estimation for video interpolation by upsampling standard frames. Its real-world driving scenarios make it valuable for testing and improving algorithms in autonomous vehicle applications.; The KITTI dataset is primarily used to evaluate stereo vision systems in both real-world driving scenarios and general outdoor and indoor environments. It provides stereo image pairs and ground truth data, enabling researchers to benchmark and compare stereo depth estimation methods. The dataset's comprehensive ground truth data and diverse environmental conditions facilitate robust evaluation and improvement of stereo vision algorithms.	cited_context | citing_context	
KITTI dataset	https://doi.org/10.1080/01691864.2020.1821770 (2020)	https://doi.org/10.1109/JSSC.2014.2342715 (2014)	The KITTI dataset is enhanced with event data to evaluate the performance of methods in real-world driving scenarios, specifically focusing on stereo depth estimation accuracy. This dataset enables researchers to test and validate their algorithms under dynamic conditions, ensuring robustness and reliability in practical applications.	citing_context	
M V S E C	https://doi.org/10.1109/TPAMI.2025.3586559 (2024)	https://doi.org/10.1007/978-3-030-01246-5_15 (2018)	The M V S E C dataset is used for evaluating event-based stereo depth estimation methods, particularly focusing on semi-dense 3D reconstruction with stereo event cameras. This dataset enables researchers to assess the performance of algorithms designed for event-based stereo vision, providing a benchmark for advancing techniques in this specialized area.	citing_context	
M3ED	https://doi.org/10.48550/arXiv.2408.04633 (2024), https://doi.org/10.1007/978-3-031-92460-6_5 (2024)	https://doi.org/10.1109/ICCV.1999.791245 (2001)	The M3ED dataset is primarily used for evaluating and generating figures related to event-based stereo depth estimation, with a focus on integrating raw LiDAR measurements and event-based data. It is employed to assess the performance of stereo depth estimation methods, particularly in handling misaligned LiDAR data. The dataset also supports the evaluation of strategies across multiple robots, sensors, and environments, enhancing the robustness and versatility of event-based systems.	citing_context	
Middlebury	https://doi.org/10.3389/fnins.2017.00535 (2017), https://doi.org/10.1109/CVPR.2018.00786 (2018)	https://doi.org/10.1109/CVPR.2015.7298925 (2015)	The Middlebury dataset is primarily used to evaluate stereo vision algorithms by providing ground truth disparity maps. Researchers employ this dataset to benchmark the performance of their depth estimation methods, ensuring accuracy and reliability in stereo vision applications. The dataset's detailed ground truth data enables rigorous testing and comparison of different algorithms.; The Middlebury dataset is referenced in academic citations but lacks detailed descriptions of its usage. There is no explicit evidence provided regarding its application in event-based stereo depth estimation or any other specific research methodologies, questions, or characteristics. Therefore, based on the given information, the actual usage and research enabled by this dataset cannot be accurately specified.	cited_context | citing_context	
Multi Vehicle Stereo Event Camera (Mvsec)	https://doi.org/10.1109/LRA.2023.3311374 (2023), https://doi.org/10.1109/ACCESS.2022.3226484 (2021), https://doi.org/10.1109/CVPR52688.2022.00848 (2022) (+1)	https://doi.org/10.1109/LRA.2018.2800793 (2018)	The Multi Vehicle Stereo Event Camera (MVSEC) dataset is primarily used for evaluating and improving event-based stereo depth estimation techniques. It is employed to assess the accuracy and smoothness of trajectory estimates, train and test neural networks, and evaluate stereo matching methods. The dataset provides real-world scenarios, enabling researchers to compare performance against state-of-the-art methods and focus on criteria such as accuracy and efficiency in 3D perception tasks.; The Multi Vehicle Stereo Event Camera (MVSEC) dataset is primarily used for evaluating and training deep learning networks focused on event-based stereo depth estimation. It provides real-world scenarios to assess the accuracy, smoothness, and performance of trajectory estimates in 3D perception tasks, often compared against state-of-the-art methods. The dataset's event-based camera data enable researchers to enhance and validate algorithms for robust stereo depth estimation.	cited_context | citing_context	
Multi-Vehicle Stereo Camera Dataset (MVSEC)	https://doi.org/10.1109/LSP.2024.3398531 (2024)	https://doi.org/10.1109/LRA.2021.3068942 (2021)	The Multi-Vehicle Stereo Camera Dataset (MVSEC) is used to evaluate methods for stereo depth estimation using event camera data, particularly in multi-vehicle scenarios. Researchers employ this dataset to test and validate their algorithms, focusing on the accuracy and robustness of depth estimation in dynamic environments. The dataset's event-based nature and multi-vehicle context enable detailed analysis of performance in complex, real-world conditions.	citing_context	
Multivehicle Stereo Event Camera Dataset	https://doi.org/10.48550/arXiv.2504.15863 (2025), https://doi.org/10.1109/TMM.2024.3387690 (2023), https://doi.org/10.1109/ICRA48891.2023.10160605 (2023) (+1)	https://doi.org/10.1109/LRA.2018.2800793 (2018)	The Multivehicle Stereo Event Camera Dataset is used to evaluate event-based stereo depth estimation methods, highlighting its importance due to the scarcity of similar datasets with ground truth depth. It assesses method effectiveness across multiple vehicles but notes limitations in scene diversity. The dataset employs mDAVIS346 sensors to capture real-world scenarios, enabling 3D perception research with event cameras.; The Multivehicle Stereo Event Camera Dataset is used to evaluate and develop methods for 3D perception and stereo depth estimation using event-based cameras. Researchers employ the dataset to create and test stereo observations from real-world sequences, particularly those captured by drones. This enables the assessment of stereo depth estimation techniques in dynamic environments, focusing on improving 3D perception accuracy and robustness.	cited_context | citing_context	
MVS	https://doi.org/10.1109/LRA.2023.3269950 (2022)	https://doi.org/10.1109/LRA.2018.2800793 (2018)	The dataset 'MVS' is mentioned in the citation context but lacks detailed descriptions of its usage, methodology, research questions, or specific characteristics. Therefore, there is insufficient evidence to provide a comprehensive description of how this dataset is actually used in research.	cited_context	
Mvsec	https://doi.org/10.1109/LRA.2023.3269950 (2022), https://doi.org/10.1109/ICRA48891.2023.10160605 (2023), https://doi.org/10.1109/CVPR52688.2022.00848 (2022) (+6) (+2)	https://doi.org/10.1109/LRA.2018.2800793 (2018)	The MVSEC dataset is primarily used for evaluating and comparing event-based stereo depth estimation methods. It supports research in 3D perception with event cameras, facilitating the assessment of performance metrics, consistency, and generalization of models. The dataset includes sequences captured in an indoor flying room, enabling benchmarking of optical flow estimation and cross-validation against state-of-the-art techniques.; The MVSEC dataset is primarily used for evaluating event-based stereo depth estimation methods, leveraging event cameras and stereo setups. It provides rich sensor data, including event streams, intensity images, and Lidar measurements, enabling researchers to compare performance metrics between event-based and frame-based methods. The dataset supports 3D perception tasks, particularly in indoor environments, and facilitates the development and evaluation of pipelines for event-based stereo depth estimation.	cited_context | citing_context	
MVSEC dataset	https://doi.org/10.48550/arXiv.2504.15863 (2025), https://doi.org/10.1080/01691864.2020.1821770 (2020), https://doi.org/10.1109/ICRA48891.2023.10160605 (2023)	https://doi.org/10.1109/LRA.2018.2800793 (2018)	The MVSEC dataset is used to evaluate and validate methods for event-based stereo depth estimation, particularly focusing on indoor flying sequences and dynamic environments. It assesses the accuracy, robustness, and temporal-spatial consistency of depth estimation techniques, including Mixed-EF2DNet, using real-world event camera data. This dataset enables researchers to test and improve algorithms in challenging, real-world conditions.	citing_context	
MVSEC indoor flying	https://doi.org/10.48550/arXiv.2504.15863 (2025)	https://doi.org/10.1109/LRA.2018.2800793 (2018)	The MVSEC indoor flying dataset is used to qualitatively compare depth estimation methods, particularly focusing on the performance of MC-EMVS and DERD-Net on event-based stereo sequences. This dataset enables researchers to evaluate and contrast the effectiveness of these methods in generating accurate depth maps from event-based data, providing insights into their strengths and limitations in real-world scenarios.	citing_context	
MVSEC indoor seqs.	https://doi.org/10.1109/TPAMI.2024.3396116 (2024)	https://doi.org/10.1109/IROS55552.2023.10341802 (2023)	The MVSEC indoor seqs. dataset is used to evaluate the generalizability of event-based optical flow methods, particularly focusing on indoor sequences. It tests the performance of these methods beyond driving scenarios, assessing their robustness and adaptability in diverse indoor environments. This dataset enables researchers to validate and refine algorithms designed for event-based visual processing.	citing_context	
MVSEC indoor sequences	https://doi.org/10.1109/TPAMI.2024.3396116 (2024)	https://doi.org/10.15607/RSS.2018.XIV.062 (2018)	The MVSEC indoor sequences dataset is used to evaluate the performance of tile-based methods in event-based stereo depth estimation. It allows researchers to compare their proposed techniques against state-of-the-art methods, focusing on accuracy. This dataset enables precise assessment of stereo depth estimation algorithms in indoor environments.	citing_context	
NSAVP	https://doi.org/10.1109/TPAMI.2025.3586559 (2024)	https://doi.org/10.1177/02783649241273554 (2024)	The NSAVP dataset is used to evaluate stereo event-based depth estimation methods, particularly in scenarios relevant to autonomous driving. It provides synchronized stereo events and images, enabling researchers to test and refine algorithms that enhance depth perception in dynamic environments. This dataset facilitates the assessment of real-time depth estimation techniques crucial for improving the safety and efficiency of autonomous vehicles.	citing_context	
outdoor_day2	https://doi.org/10.1109/TPAMI.2024.3396116 (2024)	https://doi.org/10.1109/IROS45743.2020.9341224 (2020)	The 'outdoor_day2' dataset is used to train supervised-learning methods for event-based stereo depth estimation, specifically focusing on real-world outdoor scenarios. This dataset enables researchers to develop and refine algorithms that can accurately estimate depth from event-based cameras in dynamic outdoor environments.	citing_context	
SEID	https://doi.org/10.1109/TMM.2024.3387690 (2023)	https://www.semanticscholar.org/paper/3c8a456509e6c0805354bd40a35e3f2dbf8069b1 (2019)	The SEID dataset is primarily used for training and evaluating event-based stereo depth estimation models, particularly in indoor environments and static scenes. It supports the development of models like SEVFI-Net, which are tailored for custom event-based stereo camera data. This dataset enables researchers to improve the accuracy and robustness of depth estimation in event-based systems.	citing_context	
slider_depth	https://doi.org/10.1109/TPAMI.2024.3396116 (2024)	https://doi.org/10.1177/0278364917691115 (2016)	The 'slider_depth' dataset is used to evaluate the performance of time-aware warping techniques in generating improved Intensity-Weighted Events (IWEs). This evaluation focuses on achieving higher Frame-Wise Loss (FWL) scores in the context of event-based stereo depth estimation. The dataset enables researchers to assess the effectiveness of these methods in enhancing depth estimation accuracy.	citing_context	
stereo DAVIS recordings	https://doi.org/10.1080/01691864.2020.1821770 (2020)	https://doi.org/10.1109/JSSC.2014.2342715 (2014)	The stereo DAVIS recordings dataset is used to validate methods in event-based stereo depth estimation, specifically focusing on latency and accuracy in controlled settings. Researchers employ custom stereo recordings to test and evaluate the performance of their algorithms, ensuring reliable and timely depth estimation. This dataset enables precise validation by providing synchronized, high-resolution event data.	citing_context	
Stereo DAVIS sequences	https://doi.org/10.1109/TIV.2024.3412595 (2023)	https://doi.org/10.1007/978-3-030-01246-5_15 (2018)	The Stereo DAVIS sequences dataset is used to evaluate stereo depth estimation with event-based cameras, specifically focusing on alignment and reconstruction accuracy in semi-dense 3D environments. This dataset enables researchers to assess the performance of event-based stereo algorithms, ensuring precise and reliable depth maps in dynamic scenes.	cited_context	
Stereo Event Camera Dataset for Driving Scenarios (DSEC)	https://doi.org/10.1109/LSP.2024.3398531 (2024), https://doi.org/10.1109/TMM.2024.3387690 (2023)	https://doi.org/10.1109/LRA.2021.3068942 (2021)	The Stereo Event Camera Dataset for Driving Scenarios (DSEC) is used to evaluate methods for stereo depth estimation using event camera data, particularly in driving contexts. Researchers employ this dataset to assess the performance and limitations of event-based stereo depth estimation techniques, focusing on the diversity of driving scenes captured in the dataset. This enables the identification of specific challenges and improvements needed in real-world applications.	citing_context	
T U M	https://doi.org/10.1109/TPAMI.2025.3586559 (2024)	https://doi.org/10.1007/978-3-030-01246-5_15 (2018)	The T U M dataset is used for evaluating event-based stereo depth estimation methods, particularly focusing on semi-dense 3D reconstruction with stereo event cameras. This dataset enables researchers to assess the performance of algorithms designed for event-based systems, providing a benchmark for advancing real-time 3D reconstruction techniques.	citing_context	
TUM-EV	https://doi.org/10.1109/TIV.2024.3412595 (2023)	https://doi.org/10.1109/ICCV48922.2021.01249 (2021)	The TUM-EV dataset is used to evaluate methods for fusing event-based and frame-based data, specifically focusing on maintaining and tracking features under varying lighting conditions. It demonstrates the robustness of these methods in both bright and dim scenes, enabling researchers to assess the performance of events-frames fusion techniques in real-world environments.	citing_context	
Tum-Vie	https://doi.org/10.1109/TIV.2024.3412595 (2023), https://doi.org/10.48550/arXiv.2210.08927 (2022)	https://doi.org/10.1109/IROS51168.2021.9636728 (2021)	The TUM-VIE dataset is used to evaluate depth estimation algorithms in an egocentric setting, specifically focusing on event-based stereo visual-inertial data. It assesses the accuracy and robustness of these algorithms using visual-inertial event data from 1Mpix cameras. The dataset supports offline optimization processes to estimate the δP value with high-confidence feature matches, enhancing the evaluation of event-based stereo depth estimation methods.; The TUM-VIE dataset is used to evaluate methods in event-based stereo depth estimation, specifically focusing on the accuracy and robustness of these methods. It assesses the ability of proposed techniques to maintain and track features under varying lighting conditions, employing a fusion of events and frames to enhance performance.	cited_context | citing_context	
TUM-VIE mocap-desk sequence	https://doi.org/10.1007/978-3-031-92460-6_5 (2024)		The TUM-VIE mocap-desk sequence dataset is used to evaluate event-based stereo depth estimation methods. It provides high-resolution event data and ground truth, enabling researchers to assess the robustness and efficiency of visual odometry and SLAM algorithms. This dataset facilitates the development and testing of real-time depth estimation techniques by offering precise and detailed event streams.	citing_context	
Tum-Vie Sequences	https://doi.org/10.1109/TIV.2024.3412595 (2023)	https://doi.org/10.1109/TRO.2021.3075644 (2020)	The TUM-VIE sequences dataset is used to evaluate multi-sensor SLAM systems, offering diverse sequences that test performance across various environments. This dataset enables researchers to assess the robustness and accuracy of SLAM algorithms by providing realistic and challenging scenarios, enhancing the reliability of these systems in real-world applications.; The TUM-VIE sequences dataset is mentioned in the citation context but lacks detailed descriptions of its usage in specific research. Therefore, there is no evidence to support claims about its application in event-based stereo depth estimation or any other research areas, methodologies, or specific characteristics.	cited_context | citing_context	
VECDataset	https://doi.org/10.1109/LRA.2023.3269950 (2022)	https://doi.org/10.1109/LRA.2022.3186770 (2022)	The VECDataset is mentioned in the citation context but lacks detailed descriptions of its usage, methodology, research questions, or specific characteristics. Therefore, there is insufficient evidence to provide a comprehensive description of how this dataset is actually used in research.	cited_context	
Vector	https://doi.org/10.1109/LRA.2023.3269950 (2022), https://doi.org/10.1109/TIV.2024.3412595 (2023), https://doi.org/10.1109/TPAMI.2025.3586559 (2024)	https://doi.org/10.1109/LRA.2022.3186770 (2022)	The VECtor dataset is primarily used to evaluate multi-sensor SLAM techniques, particularly focusing on the integration of stereo event cameras with other sensors such as standard cameras, RGB-D sensors, LiDAR, and IMU. It serves as a versatile benchmark for performance comparison, enabling researchers to assess the robustness and accuracy of algorithms in various real-world scenarios. The dataset provides high-quality ground truth values and sensor calibration parameters, facilitating comprehensive evaluations and comparisons between event-based and RGB image-based methods.; The VECtor dataset is primarily used to evaluate and compare algorithms in multi-sensor SLAM and event-based stereo depth estimation. It leverages high-quality ground truth and sensor calibration to assess performance, robustness, and accuracy, particularly in dynamic environments. The dataset supports offline optimization processes and facilitates comparisons between RGB and event-based methods, highlighting its utility in defining method limitations and enhancing system reliability.	cited_context | citing_context	
VECtor benchmark dataset	https://doi.org/10.1109/TPAMI.2025.3586559 (2024)	https://doi.org/10.1109/LRA.2022.3186770 (2022)	The VECtor benchmark dataset is used to evaluate multi-sensor SLAM algorithms, particularly in the context of event-based stereo depth estimation. It provides versatile event-centric data, enabling researchers to test and validate algorithms with high temporal resolution and dynamic range. This dataset supports the development and assessment of real-time 3D mapping and localization techniques.	citing_context	
Vector Sequences	https://doi.org/10.1109/TIV.2024.3412595 (2023)	https://doi.org/10.1109/TRO.2021.3075644 (2020)	The VECtor sequences dataset is used to evaluate multi-sensor SLAM systems, particularly focusing on event-centric data. It enables researchers to enhance the accuracy of localization and mapping by leveraging the unique characteristics of event-based sensors. This dataset supports the development and testing of algorithms that integrate event data for robust SLAM performance.; The VECtor sequences dataset is used to evaluate multi-sensor SLAM systems, particularly focusing on event-centric data. It enables accurate localization and mapping by providing precise event-based information, which enhances the performance and reliability of SLAM algorithms in dynamic environments.	cited_context | citing_context	
visual-inertial dataset	https://doi.org/10.48550/arXiv.2210.08927 (2022)	https://doi.org/10.1109/ISSCC19947.2020.9063149 (2020)	The visual-inertial dataset, featuring 1 Megapixel stereo event cameras, is used to support research in event-based stereo depth estimation and visual-inertial odometry. This dataset enables researchers to develop and test algorithms that integrate visual and inertial data, enhancing the accuracy and robustness of depth estimation and odometry in dynamic environments. The high-resolution event cameras provide detailed temporal information, crucial for real-time applications.	citing_context	
