Name (extracted)	Citing Article	Citied Article	Features
MVSEC	https://doi.org/10.1109/ICRA48891.2023.10160605 (2023), https://doi.org/10.1109/LRA.2023.3269950 (2022), https://doi.org/10.1109/CVPR52688.2022.00848 (2022) (+6)	https://doi.org/10.1109/LRA.2018.2800793 (2018)	The MVSEC dataset is primarily used for evaluating and comparing event-based stereo depth estimation methods. It supports research in 3D perception with event cameras, facilitating the assessment of performance metrics, consistency, and generalization of models. The dataset includes sequences captured in an indoor flying room, enabling benchmarking of optical flow estimation and cross-validation against state-of-the-art techniques.
DSEC	https://doi.org/10.1007/978-3-031-92460-6_5 (2024), https://doi.org/10.1109/TPAMI.2024.3396116 (2024), https://doi.org/10.1109/TMM.2024.3387690 (2023) (+5)	https://doi.org/10.1109/LRA.2021.3068942 (2021)	The DSEC dataset is primarily used for evaluating and improving event-based stereo depth estimation, particularly in driving scenarios. It supports research on 3D perception with event cameras, focusing on depth estimation accuracy, robustness under varying conditions, and the integration of motion information. The dataset facilitates cross-validation, benchmarking against state-of-the-art methods, and hyperparameter optimization with raw LiDAR data. Specific applications include assessing the effectiveness of time-aware warps, improving edge and contour accuracy, and evaluating performance in challenging scenes like garage doors.
VECtor	https://doi.org/10.1109/LRA.2023.3269950 (2022), https://doi.org/10.1109/TIV.2024.3412595 (2023), https://doi.org/10.1109/TPAMI.2025.3586559 (2024)	https://doi.org/10.1109/LRA.2022.3186770 (2022)	The VECtor dataset is primarily used to evaluate multi-sensor SLAM techniques, particularly focusing on the integration of stereo event cameras with other sensors such as standard cameras, RGB-D sensors, LiDAR, and IMU. It serves as a versatile benchmark for performance comparison, enabling researchers to assess the robustness and accuracy of algorithms in various real-world scenarios. The dataset provides high-quality ground truth values and sensor calibration parameters, facilitating comprehensive evaluations and comparisons between event-based and RGB image-based methods.
M3ED	https://doi.org/10.48550/arXiv.2408.04633 (2024), https://doi.org/10.1007/978-3-031-92460-6_5 (2024)	https://doi.org/10.1109/ICCV.1999.791245 (2001)	The M3ED dataset is primarily used for evaluating and generating figures related to event-based stereo depth estimation, with a focus on integrating raw LiDAR measurements and event-based data. It is employed to assess the performance of stereo depth estimation methods, particularly in handling misaligned LiDAR data. The dataset also supports the evaluation of strategies across multiple robots, sensors, and environments, enhancing the robustness and versatility of event-based systems.
EVIMO2	https://doi.org/10.1007/978-3-031-92460-6_5 (2024), https://doi.org/10.1109/TPAMI.2025.3586559 (2024)	https://doi.org/10.48550/arXiv.2205.03467 (2022)	The EVIMO2 dataset is primarily used for motion segmentation, optical flow, structure from motion, and visual inertial odometry in indoor scenes, particularly with monocular and trinocular event cameras. It provides a rich resource for evaluating event-based stereo algorithms and assessing their performance in various indoor environments. The dataset's comprehensive coverage of these areas enables researchers to thoroughly test and validate their methods.
Indoor Flying dataset	https://doi.org/10.1109/LSP.2024.3398531 (2024), https://doi.org/10.1109/CVPR52688.2022.00848 (2022)	https://doi.org/10.1109/LRA.2018.2800793 (2018)	The Indoor Flying dataset is used for preprocessing and splitting in event-based stereo depth estimation, particularly for indoor flying scenarios. It is divided into three subsets, with split 1 and split 3 utilized for training and evaluation, respectively. This dataset enables researchers to assess and improve event-based stereo depth estimation methods in controlled indoor environments.
DENSE	https://doi.org/10.1109/ICRA48891.2023.10160605 (2023)	https://doi.org/10.1109/LRA.2018.2800793 (2018)	The DENSE dataset is primarily used for validating methods in 3D perception and monocular dense depth estimation using synthetic event data. It provides synthetic samples of objects such as trees and bus stations, which are used for both initial training and validation of neural networks. This enables researchers to test and refine algorithms for depth estimation in event-based systems.
Multivehicle Stereo Event Camera Dataset	https://doi.org/10.48550/arXiv.2504.15863 (2025), https://doi.org/10.1109/TMM.2024.3387690 (2023), https://doi.org/10.1109/ICRA48891.2023.10160605 (2023)	https://doi.org/10.1109/LRA.2018.2800793 (2018)	The Multivehicle Stereo Event Camera Dataset is used to evaluate event-based stereo depth estimation methods, highlighting its importance due to the scarcity of similar datasets with ground truth depth. It assesses method effectiveness across multiple vehicles but notes limitations in scene diversity. The dataset employs mDAVIS346 sensors to capture real-world scenarios, enabling 3D perception research with event cameras.
DSEC disparity benchmark	https://doi.org/10.1109/TPAMI.2025.3586559 (2024), https://doi.org/10.48550/arXiv.2408.05452 (2024), https://doi.org/10.1109/TIM.2025.3566813 (2025)	https://doi.org/10.1109/ICCV48922.2021.00422 (2021)	The DSEC disparity benchmark dataset is used to evaluate the performance of event-based stereo depth estimation methods. It focuses on metrics such as mean average disparity error (MAE), root mean square error (RMSE), and pixel error rates (1PE, 2PE). The dataset enables researchers to assess the accuracy and robustness of disparity maps generated by different algorithms, facilitating comparisons with state-of-the-art techniques.
MVSEC dataset	https://doi.org/10.48550/arXiv.2504.15863 (2025), https://doi.org/10.1080/01691864.2020.1821770 (2020), https://doi.org/10.1109/ICRA48891.2023.10160605 (2023)	https://doi.org/10.1109/LRA.2018.2800793 (2018)	The MVSEC dataset is used to evaluate and validate methods for event-based stereo depth estimation, particularly focusing on indoor flying sequences and dynamic environments. It assesses the accuracy, robustness, and temporal-spatial consistency of depth estimation techniques, including Mixed-EF2DNet, using real-world event camera data. This dataset enables researchers to test and improve algorithms in challenging, real-world conditions.
Multi Vehicle Stereo Event Camera (MVSEC)	https://doi.org/10.1109/LRA.2023.3311374 (2023), https://doi.org/10.1109/ACCESS.2022.3226484 (2021), https://doi.org/10.1109/CVPR52688.2022.00848 (2022)	https://doi.org/10.1109/LRA.2018.2800793 (2018)	The Multi Vehicle Stereo Event Camera (MVSEC) dataset is primarily used for evaluating and improving event-based stereo depth estimation techniques. It is employed to assess the accuracy and smoothness of trajectory estimates, train and test neural networks, and evaluate stereo matching methods. The dataset provides real-world scenarios, enabling researchers to compare performance against state-of-the-art methods and focus on criteria such as accuracy and efficiency in 3D perception tasks.
KITTI	https://doi.org/10.1080/01691864.2020.1821770 (2020), https://doi.org/10.3389/fnins.2017.00535 (2017)	https://doi.org/10.1109/CVPR.2012.6248074 (2012)	The KITTI dataset is primarily used to evaluate and demonstrate the performance of stereo depth estimation methods in real-world autonomous driving scenarios. It provides annotated data for assessing stereo vision and scene flow techniques. Additionally, the dataset is utilized to enhance intermediate frame estimation for video interpolation by upsampling standard frames. Its real-world driving scenarios make it valuable for testing and improving algorithms in autonomous vehicle applications.
ECD dataset	https://doi.org/10.1109/TPAMI.2024.3396116 (2024)	https://doi.org/10.1177/0278364917691115 (2016)	The ECD dataset is primarily used for depth and ego-motion estimation in event-based stereo depth estimation research. It is utilized in both real and simulated sequences, such as slider_depth and simulation_3planes, to evaluate performance and support studies in pose estimation, visual odometry, and SLAM. The dataset's event-based data is crucial for these applications, enabling precise and efficient estimation techniques.
DVS stereo dataset	https://doi.org/10.1109/ISCAS51556.2021.9401402 (2021), https://doi.org/10.1109/TPAMI.2025.3586559 (2024)	https://doi.org/10.1109/CVPR.2018.00786 (2018)	The DVS stereo dataset is used to evaluate and test event-based stereo depth estimation methods, particularly focusing on dynamic objects in a stationary setup. It employs stereo DAVIS240C cameras to capture real-world sequences, such as a fast rotating fan and a rotating toy butterfly, enabling researchers to assess the performance of event-based stereo algorithms in dynamic environments.
SEID	https://doi.org/10.1109/TMM.2024.3387690 (2023)	https://www.semanticscholar.org/paper/3c8a456509e6c0805354bd40a35e3f2dbf8069b1 (2019)	The SEID dataset is primarily used for training and evaluating event-based stereo depth estimation models, particularly in indoor environments and static scenes. It supports the development of models like SEVFI-Net, which are tailored for custom event-based stereo camera data. This dataset enables researchers to improve the accuracy and robustness of depth estimation in event-based systems.
TUM-VIE	https://doi.org/10.48550/arXiv.2210.08927 (2022), https://doi.org/10.1109/TIV.2024.3412595 (2023)	https://doi.org/10.1109/IROS51168.2021.9636728 (2021)	The TUM-VIE dataset is used to evaluate depth estimation algorithms in an egocentric setting, specifically focusing on event-based stereo visual-inertial data. It assesses the accuracy and robustness of these algorithms using visual-inertial event data from 1Mpix cameras. The dataset supports offline optimization processes to estimate the Î´P value with high-confidence feature matches, enhancing the evaluation of event-based stereo depth estimation methods.
Stereo Event Camera Dataset for Driving Scenarios (DSEC)	https://doi.org/10.1109/LSP.2024.3398531 (2024), https://doi.org/10.1109/TMM.2024.3387690 (2023)	https://doi.org/10.1109/LRA.2021.3068942 (2021)	The Stereo Event Camera Dataset for Driving Scenarios (DSEC) is used to evaluate methods for stereo depth estimation using event camera data, particularly in driving contexts. Researchers employ this dataset to assess the performance and limitations of event-based stereo depth estimation techniques, focusing on the diversity of driving scenes captured in the dataset. This enables the identification of specific challenges and improvements needed in real-world applications.
VECtor sequences	https://doi.org/10.1109/TIV.2024.3412595 (2023)	https://doi.org/10.1109/TRO.2021.3075644 (2020)	The VECtor sequences dataset is used to evaluate multi-sensor SLAM systems, particularly focusing on event-centric data. It enables researchers to enhance the accuracy of localization and mapping by leveraging the unique characteristics of event-based sensors. This dataset supports the development and testing of algorithms that integrate event data for robust SLAM performance.
TUM-VIE sequences	https://doi.org/10.1109/TIV.2024.3412595 (2023)	https://doi.org/10.1109/TRO.2021.3075644 (2020)	The TUM-VIE sequences dataset is used to evaluate multi-sensor SLAM systems, offering diverse sequences that test performance across various environments. This dataset enables researchers to assess the robustness and accuracy of SLAM algorithms by providing realistic and challenging scenarios, enhancing the reliability of these systems in real-world applications.
DVS 3D Human Pose Dataset (DHP19)	https://doi.org/10.1109/ISCAS51556.2021.9401402 (2021)	https://www.semanticscholar.org/paper/6fe028a54dad296b6d25c50baaa47511df6d4123	The DVS 3D Human Pose Dataset (DHP19) is used to provide DVS input data and 3D ground-truth information for training small-scale neuromorphic architectures. It focuses on coarse stereo vision tasks, enabling researchers to develop and evaluate models that process event-based data for 3D human pose estimation. The dataset's event-based nature and 3D ground-truth labels are crucial for training and validating these neuromorphic systems.
Multi Vehicle Stereo Event Camera (MVSEC) Dataset	https://doi.org/10.1109/ISCAS51556.2021.9401402 (2021)	https://www.semanticscholar.org/paper/6fe028a54dad296b6d25c50baaa47511df6d4123	The Multi Vehicle Stereo Event Camera (MVSEC) Dataset is used to evaluate event-based stereo algorithms, leveraging its indoor and outdoor sequences with varying illumination and speeds. This dataset enables researchers to test the robustness and accuracy of stereo depth estimation methods under diverse conditions, focusing on algorithm performance in dynamic environments.
DSEC zurich city 04 a	https://doi.org/10.48550/arXiv.2504.15863 (2025)	https://doi.org/10.1109/LRA.2018.2800793 (2018)	The DSEC Zurich City 04 a dataset is used to qualitatively compare depth estimation methods, particularly focusing on the performance of MC-EMVS and DERD-Net on event-based stereo sequences. This dataset enables researchers to evaluate and contrast these methods, providing insights into their effectiveness in depth estimation tasks.
DAVIS240	https://doi.org/10.1109/ECMR.2019.8870946 (2019)	https://doi.org/10.1109/JSSC.2014.2342715 (2014)	The DAVIS240 dataset is primarily used for calibration purposes in event-based vision research. It provides standard camera frames that ensure accurate setup before applying event-only processing methods. This calibration is crucial for maintaining the precision of subsequent event-based algorithms, enabling reliable and consistent experimental results.
DDD17	https://doi.org/10.1109/LRA.2018.2800793 (2018)	https://www.semanticscholar.org/paper/e9496ccf44f6ebca4f01c31a012bdab7cac4a65c (2017)	The DDD17 dataset is used to provide a large collection of data from a DAVIS 346B event-based sensor mounted in a car, capturing 12 hours of driving scenarios. It is employed for end-to-end learning of driving-related tasks, leveraging the unique temporal resolution and low latency of event-based sensors to enhance real-time driving applications.
depth images and pose data	https://doi.org/10.1109/LRA.2018.2800793 (2018)	https://doi.org/10.1109/ICRA.2014.6906892 (2014)	The dataset of depth images and pose data is used to enhance the quality of event-based stereo depth estimation. It provides accurate depth images and pose information at high frequency, generated from a lidar system and motion capture, which improves the precision and reliability of depth estimation in dynamic environments.
KITTI dataset	https://doi.org/10.1080/01691864.2020.1821770 (2020)	https://doi.org/10.1109/JSSC.2014.2342715 (2014)	The KITTI dataset is enhanced with event data to evaluate the performance of methods in real-world driving scenarios, specifically focusing on stereo depth estimation accuracy. This dataset enables researchers to test and validate their algorithms under dynamic conditions, ensuring robustness and reliability in practical applications.
event streams from DAVISm346b	https://doi.org/10.1109/LRA.2018.2800793 (2018)	https://doi.org/10.1109/ICRA.2014.6906892 (2014)	The 'event streams from DAVISm346b' dataset is used to capture synchronized event streams from two calibrated Dynamic Vision and Active Pixel Sensors. It provides long indoor and outdoor sequences under various conditions, enabling research in depth estimation. This dataset supports the development and evaluation of algorithms for event-based stereo depth estimation by offering diverse and realistic sensor data.
stereo DAVIS recordings	https://doi.org/10.1080/01691864.2020.1821770 (2020)	https://doi.org/10.1109/JSSC.2014.2342715 (2014)	The stereo DAVIS recordings dataset is used to validate methods in event-based stereo depth estimation, specifically focusing on latency and accuracy in controlled settings. Researchers employ custom stereo recordings to test and evaluate the performance of their algorithms, ensuring reliable and timely depth estimation. This dataset enables precise validation by providing synchronized, high-resolution event data.
DAVIS346	https://doi.org/10.1080/01691864.2020.1821770 (2020)	https://doi.org/10.1109/JSSC.2014.2342715 (2014)	The DAVIS346 dataset is used to collect stereo event sequences with ground truth pose and depth information. It is specifically employed for evaluating event-based stereo depth estimation algorithms. The dataset's provision of synchronized event data and ground truth enables researchers to accurately assess the performance of these algorithms in real-world scenarios.
MVSEC indoor sequences	https://doi.org/10.1109/TPAMI.2024.3396116 (2024)	https://doi.org/10.15607/RSS.2018.XIV.062 (2018)	The MVSEC indoor sequences dataset is used to evaluate the performance of tile-based methods in event-based stereo depth estimation. It allows researchers to compare their proposed techniques against state-of-the-art methods, focusing on accuracy. This dataset enables precise assessment of stereo depth estimation algorithms in indoor environments.
Multi-Vehicle Stereo Camera Dataset (MVSEC)	https://doi.org/10.1109/LSP.2024.3398531 (2024)	https://doi.org/10.1109/LRA.2021.3068942 (2021)	The Multi-Vehicle Stereo Camera Dataset (MVSEC) is used to evaluate methods for stereo depth estimation using event camera data, particularly in multi-vehicle scenarios. Researchers employ this dataset to test and validate their algorithms, focusing on the accuracy and robustness of depth estimation in dynamic environments. The dataset's event-based nature and multi-vehicle context enable detailed analysis of performance in complex, real-world conditions.
slider_depth	https://doi.org/10.1109/TPAMI.2024.3396116 (2024)	https://doi.org/10.1177/0278364917691115 (2016)	The 'slider_depth' dataset is used to evaluate the performance of time-aware warping techniques in generating improved Intensity-Weighted Events (IWEs). This evaluation focuses on achieving higher Frame-Wise Loss (FWL) scores in the context of event-based stereo depth estimation. The dataset enables researchers to assess the effectiveness of these methods in enhancing depth estimation accuracy.
ECD	https://doi.org/10.1109/TPAMI.2024.3396116 (2024)	https://doi.org/10.1177/0278364917691115 (2016)	The ECD dataset is used to evaluate time-aware warp methods, specifically for pose estimation, visual odometry, and SLAM, leveraging event-based data. This dataset enables researchers to test and refine algorithms that process asynchronous events, enhancing the accuracy and efficiency of these critical tasks in robotics and computer vision.
M V S E C	https://doi.org/10.1109/TPAMI.2025.3586559 (2024)	https://doi.org/10.1007/978-3-030-01246-5_15 (2018)	The M V S E C dataset is used for evaluating event-based stereo depth estimation methods, particularly focusing on semi-dense 3D reconstruction with stereo event cameras. This dataset enables researchers to assess the performance of algorithms designed for event-based stereo vision, providing a benchmark for advancing techniques in this specialized area.
outdoor_day2	https://doi.org/10.1109/TPAMI.2024.3396116 (2024)	https://doi.org/10.1109/IROS45743.2020.9341224 (2020)	The 'outdoor_day2' dataset is used to train supervised-learning methods for event-based stereo depth estimation, specifically focusing on real-world outdoor scenarios. This dataset enables researchers to develop and refine algorithms that can accurately estimate depth from event-based cameras in dynamic outdoor environments.
D S E C	https://doi.org/10.1109/TPAMI.2025.3586559 (2024)	https://doi.org/10.1007/978-3-030-01246-5_15 (2018)	The D S E C dataset is used for evaluating event-based stereo depth estimation methods, particularly focusing on semi-dense 3D reconstruction with stereo event cameras. Researchers employ this dataset to assess the performance of algorithms in generating accurate depth maps from event data, leveraging its specialized characteristics to advance the field of event-based vision.
EMSGC	https://doi.org/10.1109/TPAMI.2024.3396116 (2024)	https://doi.org/10.1109/TNNLS.2021.3124580 (2020)	The EMSGC dataset is used for event-based motion segmentation, specifically employing spatio-temporal graph cuts methodology. It records sequences with a hand-held DAVIS346 camera, enabling researchers to analyze and segment motion events in real-time. This dataset facilitates the development and evaluation of algorithms for dynamic scene understanding in event-based vision systems.
T U M	https://doi.org/10.1109/TPAMI.2025.3586559 (2024)	https://doi.org/10.1007/978-3-030-01246-5_15 (2018)	The T U M dataset is used for evaluating event-based stereo depth estimation methods, particularly focusing on semi-dense 3D reconstruction with stereo event cameras. This dataset enables researchers to assess the performance of algorithms designed for event-based systems, providing a benchmark for advancing real-time 3D reconstruction techniques.
MVSEC indoor seqs.	https://doi.org/10.1109/TPAMI.2024.3396116 (2024)	https://doi.org/10.1109/IROS55552.2023.10341802 (2023)	The MVSEC indoor seqs. dataset is used to evaluate the generalizability of event-based optical flow methods, particularly focusing on indoor sequences. It tests the performance of these methods beyond driving scenarios, assessing their robustness and adaptability in diverse indoor environments. This dataset enables researchers to validate and refine algorithms designed for event-based visual processing.
ERGO-12	https://doi.org/10.48550/arXiv.2408.04633 (2024)	https://doi.org/10.1109/ICCV51070.2023.01180 (2023)	The dataset 'ERGO-12' is mentioned in the citation context but lacks detailed descriptions of its usage in research. There is no explicit information regarding its application, methodology, research questions, or specific characteristics. Therefore, it cannot be accurately described as being used for any particular research area or method.
DSEC flow benchmark	https://doi.org/10.1109/TPAMI.2024.3396116 (2024)		The DSEC flow benchmark dataset is used to evaluate the performance of RAFT in estimating pixel correspondences for large displacements, particularly in the context of event-based visual odometry and stereo depth estimation. This dataset facilitates the assessment of algorithms by providing challenging scenarios with significant motion, enabling researchers to test and improve the accuracy and robustness of their methods in real-world conditions.
VECtor benchmark dataset	https://doi.org/10.1109/TPAMI.2025.3586559 (2024)	https://doi.org/10.1109/LRA.2022.3186770 (2022)	The VECtor benchmark dataset is used to evaluate multi-sensor SLAM algorithms, particularly in the context of event-based stereo depth estimation. It provides versatile event-centric data, enabling researchers to test and validate algorithms with high temporal resolution and dynamic range. This dataset supports the development and assessment of real-time 3D mapping and localization techniques.
flying1	https://doi.org/10.1109/TPAMI.2025.3586559 (2024)	https://doi.org/10.1109/ICCV.2019.00161 (2019)	The 'flying1' dataset is used to evaluate mean depth error in event-based stereo depth estimation. Researchers focus on specific sequences within the dataset to assess the performance of their algorithms, ensuring accurate and reliable depth estimation in dynamic environments. This dataset enables the rigorous testing and validation of event-based stereo methods by providing challenging and diverse scenarios.
flying2	https://doi.org/10.1109/TPAMI.2025.3586559 (2024)	https://doi.org/10.1109/ICCV.2019.00161 (2019)	The 'flying2' dataset is used to evaluate mean depth error in event-based stereo depth estimation. Researchers focus on specific sequences within the dataset to assess the performance of their algorithms, ensuring accurate and reliable depth estimation in dynamic environments. This dataset enables the rigorous testing and validation of event-based stereo methods by providing challenging and diverse scenarios.
flying3	https://doi.org/10.1109/TPAMI.2025.3586559 (2024)	https://doi.org/10.1109/ICCV.2019.00161 (2019)	The 'flying3' dataset is used to evaluate mean depth error in event-based stereo depth estimation. Researchers focus on specific sequences within the dataset to assess the performance of their algorithms, ensuring accurate and reliable depth estimation in dynamic environments. This dataset enables the rigorous testing and validation of event-based stereo methods by providing challenging and diverse scenarios.
DSEC driving dataset	https://doi.org/10.1007/978-3-031-92460-6_5 (2024)	https://doi.org/10.1109/LRA.2021.3068942 (2021)	The DSEC driving dataset is used for event-based stereo depth estimation in driving scenarios. Researchers employ it to compare their proposed strategies with existing methods like MC-EMVS, demonstrating significant improvements. This dataset enables precise evaluation and validation of event-based depth estimation techniques, crucial for enhancing autonomous driving systems.
NSAVP	https://doi.org/10.1109/TPAMI.2025.3586559 (2024)	https://doi.org/10.1177/02783649241273554 (2024)	The NSAVP dataset is used to evaluate stereo event-based depth estimation methods, particularly in scenarios relevant to autonomous driving. It provides synchronized stereo events and images, enabling researchers to test and refine algorithms that enhance depth perception in dynamic environments. This dataset facilitates the assessment of real-time depth estimation techniques crucial for improving the safety and efficiency of autonomous vehicles.
CoSEC	https://doi.org/10.1109/TPAMI.2025.3586559 (2024)	https://doi.org/10.48550/arXiv.2408.08500 (2024)	The CoSEC dataset is used for aligning pixels between frame-based and event cameras to facilitate multi-modal fusion in autonomous driving. This alignment is crucial for integrating data from different camera types, enhancing the accuracy and robustness of perception systems. The dataset supports research focused on technical integration methods, enabling more effective use of diverse sensor inputs in real-world driving scenarios.
DAVIS sequences	https://doi.org/10.1109/TIV.2024.3412595 (2023)	https://doi.org/10.1007/978-3-030-01246-5_15 (2018)	The DAVIS sequences dataset is used to evaluate stereo depth estimation methods, particularly focusing on the alignment of event-based and frame-based sensors that share the same photoreceptor pixel array. This dataset enables researchers to assess the performance of algorithms in integrating data from these dual-modal sensors, addressing challenges in sensor synchronization and data fusion.
TUM-VIE mocap-desk sequence	https://doi.org/10.1007/978-3-031-92460-6_5 (2024)		The TUM-VIE mocap-desk sequence dataset is used to evaluate event-based stereo depth estimation methods. It provides high-resolution event data and ground truth, enabling researchers to assess the robustness and efficiency of visual odometry and SLAM algorithms. This dataset facilitates the development and testing of real-time depth estimation techniques by offering precise and detailed event streams.
Middlebury	https://doi.org/10.3389/fnins.2017.00535 (2017)	https://doi.org/10.1109/CVPR.2015.7298925 (2015)	The Middlebury dataset is primarily used to evaluate stereo vision algorithms by providing ground truth disparity maps. Researchers employ this dataset to benchmark the performance of their depth estimation methods, ensuring accuracy and reliability in stereo vision applications. The dataset's detailed ground truth data enables rigorous testing and comparison of different algorithms.
visual-inertial dataset	https://doi.org/10.48550/arXiv.2210.08927 (2022)	https://doi.org/10.1109/ISSCC19947.2020.9063149 (2020)	The visual-inertial dataset, featuring 1 Megapixel stereo event cameras, is used to support research in event-based stereo depth estimation and visual-inertial odometry. This dataset enables researchers to develop and test algorithms that integrate visual and inertial data, enhancing the accuracy and robustness of depth estimation and odometry in dynamic environments. The high-resolution event cameras provide detailed temporal information, crucial for real-time applications.
Event-Based Stereo Visual Odometry	https://doi.org/10.1109/TIV.2024.3412595 (2023)	https://doi.org/10.1109/TRO.2021.3062252 (2020)	The Event-Based Stereo Visual Odometry dataset is used to evaluate event-based stereo depth estimation methods, specifically focusing on visual odometry performance in dynamic environments. Researchers employ this dataset to assess the accuracy and robustness of algorithms in real-world conditions, leveraging its event-based data to simulate and analyze performance under rapid changes and high dynamics.
DAVIS	https://doi.org/10.1177/1729881417752759 (2018)	https://doi.org/10.1109/JSSC.2014.2342715 (2014)	The DAVIS dataset is used to capture high-resolution and low-latency event-based data for stereo depth estimation. It employs a quadruplet representation (time, coordinates, polarity) to process event streams, enabling precise and efficient depth estimation in dynamic environments. This dataset facilitates research focused on improving the accuracy and responsiveness of event-based stereo vision systems.
TUM-EV	https://doi.org/10.1109/TIV.2024.3412595 (2023)	https://doi.org/10.1109/ICCV48922.2021.01249 (2021)	The TUM-EV dataset is used to evaluate methods for fusing event-based and frame-based data, specifically focusing on maintaining and tracking features under varying lighting conditions. It demonstrates the robustness of these methods in both bright and dim scenes, enabling researchers to assess the performance of events-frames fusion techniques in real-world environments.
DHP19	https://doi.org/10.1109/ISCAS51556.2021.9401402 (2021)	https://doi.org/10.3389/fnbot.2020.568283 (2020)	The DHP19 dataset is used to evaluate the robustness and performance of event-based approaches for neuromorphic, on-chip depth estimation. Researchers focus on assessing the reliability of these methods, leveraging the dataset's characteristics to test and validate their algorithms in real-world conditions. This enables the development and refinement of more efficient and accurate depth estimation techniques in neuromorphic computing.
MVSEC indoor flying	https://doi.org/10.48550/arXiv.2504.15863 (2025)	https://doi.org/10.1109/LRA.2018.2800793 (2018)	The MVSEC indoor flying dataset is used to qualitatively compare depth estimation methods, particularly focusing on the performance of MC-EMVS and DERD-Net on event-based stereo sequences. This dataset enables researchers to evaluate and contrast the effectiveness of these methods in generating accurate depth maps from event-based data, providing insights into their strengths and limitations in real-world scenarios.
